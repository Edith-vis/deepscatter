id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
46520d181dfef62960c208bd121e07c7a27cbd42	qualitative models in the design and construction of an autonomous hydraulic press machine	analisis cualitativo;control algorithm;mecatronica;programme commande;simulation;construction industry;qualitative analysis;intelligence artificielle;mobile installation;industrie construction;machine hydraulique;analyse qualitative;installation mobile;control program;qualitative modelling;maquina hidraulica;programa mando;artificial intelligence;mecatronique;inteligencia artificial;industria construccion;qualitative modeling;hydraulic machine;mechatronics;instalacion movil	An important goal in advanced manufacturing research is the ability to construct efficient autonomous machines. In this paper, the design, construction and operation of an autonomous mobile hydraulic press machine are discussed. This machine automates the production of moulded concrete elements for architectural and building projects. The research demonstrates how the design of a state-of-the-art industrial mechatronic system can be facilitated by using qualitative modelling and simulation from artificial intelligence. The research further demonstrates how an efficient knowledge-based control algorithm for operating the machine can be derived from a qualitative modelling exercise.		Georgia Garani;George K. Adam	2009	Control and Intelligent Systems	10.2316/Journal.201.2009.1.201-1957	simulation;mechatronics;computer science;engineering;qualitative research;artificial intelligence;hydraulic machinery;mechanical engineering	Robotics	-24.376325012760535	-4.8029074091409685	51827
327093f5bbe236bfa8974ccbd939a17450692784	semantic overlay network for peer-to-peer hybrid information search and retrieval	p2p system;semantic networks computer network management file organisation information retrieval peer to peer computing;information retrieval;semantics;p2p;maintenance engineering;semantic networks;peer to peer system;artificial neural networks;semantic information;dht peer to peer network overlay network hybrid information search information retrieval keyword matching hierarchical semantic network content based full text search network management p2p;computer network management;semantic overlay network peer to peer system information retrieval;peer to peer computing semantics artificial neural networks maintenance engineering;semantic search;network management;peer to peer computing;peer to peer;information search and retrieval;semantic overlay network;artificial neural network;file organisation	Peer-to-peer (P2P) systems have many important advantages. However, most existing P2P systems are limited to providing resource searches based on simple keyword matching, and do not provide any semantic information about the content of the objects stored and the relationships between those objects. This paper proposes the design of a hierarchical semantic overlay network that can be used for content-based full-text search, and is part of our work to use semantics in network management. Our semantic overlay network is based on creating a semantic cluster of objects that is associated with each node in the P2P DHT to provide semantic search. We validate some research questions in our approach by conducting simulations.	distributed hash table;overlay network;peer-to-peer;semantic search;serial ata;simulation	Sung-Su Kim;John Strassner;James Won-Ki Hong	2011	12th IFIP/IEEE International Symposium on Integrated Network Management (IM 2011) and Workshops	10.1109/INM.2011.5990543	network management;semantic interoperability;semantic similarity;semantic computing;semantic integration;semantic search;semantic grid;computer science;peer-to-peer;semantic web stack;semantic compression;database;semantic network;semantic technology;world wide web;information retrieval;semantic analytics;artificial neural network	Arch	-29.47760380297244	-0.3011561070818129	51828
8b0bada4475984dcdb2afaaba44b3fbae5929c19	a knowledge-intensive approach for semi-automatic causal subgroup discovery		This paper presents a methodological view on knowledge-intensive causal subgroup discovery implemented in a semi-automatic approach. We show how to identify causal relations between subgroups by generating an extended causal subgroup network utilizing background knowledge. Using the links within the network we can identify causal relations, but also relations that are potentially confounded and/or effect-modified by external (confounding) factors. In a semi-automatic approach, the network and the discovered relations are presented to the user as an intuitive visualization. The applicability and benefit of the presented technique is illustrated by examples from a case-study in the medical domain.	causal filter;causal model;embedded system;knowledge base;ontology (information science);refinement (computing);scientific visualization;semiconductor industry;sensor	Martin Atzmüller;Frank Puppe	2009		10.1007/978-3-642-01891-6_2	data mining;visualization;confounding;computer science	AI	-33.0479906798791	-4.602456750827748	51883
04abd834001e2a1e9b0f5359a164bcee3c19a16a	ccam: a connectivity-clustered access method for networks and network computations	modelizacion;computer networks spatial databases road transportation aggregates intelligent vehicles costs computer network management irrigation joining processes intelligent transportation systems;query processing visual databases distributed databases;base donnee;range query;management system;systeme information geographique;geographic information system;query processing;analisis espacial;network computations;intelligent vehicle highway system;heuristic method;interrogation base donnee;database;interrogacion base datos;base dato;access methods;metodo heuristico;access protocol;network analysis;spatial database;modelisation;graph partitioning;clustering;geographic information systems;spatial databases;distributed databases;methode heuristique;systeme gestion base donnee;spatial analysis;spatial networks;protocole acces;access method;analyse circuit;modeling;sistema gestion base datos;database management system;article;spatial access method;acceso protocolo;database query;analyse spatiale;sistema informacion geografica;network computing;analisis circuito;minneapolis road map ccam connectivity clustered access method network computations spatial database management systems sdbms static clustering range queries spatial points line segments polygons spatial access methods geographic proximity expected i o cost weighted connectivity residue ratio connected nodes connectivity clustering get a successor get successors aggregate computations graph partitioning approach;visual databases	Current Spatial Database Management Systems (SDBMS) provide efficient access methods and operators for point and range queries over collections of spatial points, line segments, and polygons. However, it is not clear if existing spatial access methods can efficiently support network computations which traverse line-segments in a spatial network based on connectivity rather than geographic proximity. The expected I/O cost for many network operations can be reduced by maximizing the Weighted Connectivity Residue Ratio (WCRR), i.e., the chance that a pair of connected nodes that are more likely to be accessed together are allocated to a common page of the file. CCAM is an access method for general networks that uses connectivity clustering. CCAM supports the operations of insert, delete, create, and find as well as the new operations, get-A-successor and get-successors, which retrieve one or all successors of a node to facilitate aggregate computations on networks. The nodes of the network are assigned to disk pages via a graph partitioning approach to maximize the WCRR. CCAM includes methods for static clustering, as well as dynamic incremental reclustering, to maintain high WCRR in the face of updates, without incurring high overheads. We also describe possible modifications to improve the WCRR that can be achieved by existing spatial access methods. Experiments with network computations on the Minneapolis road map show that CCAM outperforms existing access methods, even though the proposed modifications also substantially improve the performance of existing spatial access methods.	aggregate data;cluster analysis;computation;graph partition;input/output;range query (data structures);spatial database;spatial network;traverse	Shashi Shekhar;Duen-Ren Liu	1997	IEEE Trans. Knowl. Data Eng.	10.1109/69.567054	computer science;theoretical computer science;data mining;database;geographic information system;access method;world wide web	DB	-26.764175914224722	3.7573984223251253	51945
82b4ce0df4d50e80446c96aeff2984373ed57591	smart-swaps - a decision support system for multicriteria decision analysis with the even swaps method	even swaps;multicriteria decision support systems;modelizacion;outil logiciel;multicriteria analysis;decision support;software tool;decision support tool;systeme intelligent;red www;systeme aide decision;sistema inteligente;preference programming;reseau web;analisis decision;prise de decision;sistema ayuda decision;value analysis;decision maker;decision analysis;analyse valeur;programming model;modelisation;support system;decision support system;internet;intelligent system;analisis valor;preferencia;world wide web;trade offs;preference;analisis multicriterio;analyse multicritere;toma decision;herramienta software;modeling;analyse decision;multicriteria decision analysis	This paper introduces a new web-based decision support tool called Smart-Swaps to support multicriteria decision analysis. The decision maker’s preferences are elicited with the even swaps method, which is an elimination process based on value trade-offs. The software provides a platform for carrying out the elimination process and implements a preference programming model to give suggestions to the decision maker on how to proceed with the process. Such decision support can provide substantial help to the decision maker, especially when the number of alternatives and attributes is large.	academy;backtracking;decision analysis;decision support system;documentation;path dependence;programming model;real life;technical support;the superficial;web application	Jyri Mustajoki;Raimo P. Hämäläinen	2007	Decision Support Systems	10.1016/j.dss.2007.04.004	decision-making;r-cast;the internet;systems modeling;optimal decision;influence diagram;decision support system;intelligent decision support system;decision analysis;decision field theory;decision engineering;trade-off;computer science;artificial intelligence;operations management;decision analysis cycle;decision tree;data mining;decision rule;programming paradigm;evidential reasoning approach;evidential decision theory;operations research;world wide web;weighted sum model;business decision mapping	Web+IR	-26.705205169119857	-4.489986322879978	51947
d1e1a49f115d97cdf40a5df2d0d0d205cc5f492a	modeling and diagnosing problem-solving system behavior	modelizacion;analisis sistema;availability;delta modulation;resolucion problema;modelisation;data analysis;monitoring;problem solving fault diagnosis delta modulation hardware vehicles monitoring system testing artificial intelligence availability data analysis;system analysis;system testing;artificial intelligence;analyse systeme;vehicles;modeling;problem solving;resolution probleme;fault diagnosis;hardware	"""A new component of a problem-solving system, called the diagnosis module (DM), that enables the system to reason about its own behavior is described. The aim of the diagnosis is to identify inappropriate control parameter settings or faulty hardware components as the causes of observed misbehavior. The problem-solving system being diagnosed is a distributed interpretation system, the distributed vehicle monitoring testbed (DVMT), which is based on a blackboard problem-solving architecture. The diagnosis module uses a causal model of the expected behavior of the DVMT to guide the diagnosis. Causal-model-based diagnosis is not new in AI. What is different is the application of this technique to the diagnosis of problem-solving system behavior. Problem-solving systems are characterized by the availability of the intermediate problem-solving state, the large amounts of data to process, and in some cases, the lack of absolute standards for behavior. New diagnostic techniques that exploit the availability of the intermediate problem-solving state and address the combinatorial problem arising from the large amount of data to analyze are described. A technique has also been developed, called comparative reasoning, for dealing with cases where no absolute standard for correct behavior is available. In such cases the diagnosis system selects its own """"correct behavior criteria"""" from objects within the problem-solving system which did achieve some desired situation. The diagnosis module for the DVMT has been implemented and successfully identifies faults."""	causal filter;causal model;dynamic video memory technology;problem solving;software diagnosis;testbed	Eva Hudlicka;Victor R. Lesser	1987	IEEE Transactions on Systems, Man, and Cybernetics	10.1109/TSMC.1987.4309057	availability;delta modulation;simulation;systems modeling;computer science;artificial intelligence;control theory;system analysis;data analysis;system testing	AI	-21.96694192040848	-4.249359680519757	52288
df88cf626a1c099220291bf2412aa71d7698e841	management of uncertainty in top-down, fuzzy logic-based image understanding of natural objects		This paper is concerned with the integration of knowledge intensive methods with low level image processing, in order to achieve a top-down image processing system. The knowledge part contains information about the object to be recognized, the model, expressed here as a part-of hierarchy, and expert knowledge on image processing. Both of these bodies of knowledge are allowed to be imprecise and/or incomplete, in which case fuzzy logic based methods are used for representation and inference. As a consequence, uncertainty management issues, such as partial matching, and evidence combination must be addressed. The approach proposed is most suitable for complex/natural objects and is illustrated for the task of recognizing a human face.	fuzzy logic	Koji Miyajima;Toshio Norita;Anca L. Ralescu	1993	International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems	10.1142/S0218488593000103	computer vision;computer science;artificial intelligence;machine learning;data mining	Robotics	-20.346122179935072	-2.218354012557955	52451
94e351af5fae409b1c6a4201c127f9fcc56c6c71	multiple criteria group decision support through the usage of argumentation-based multi-agent systems: an overview	decision support;multi agent system;multiple criteria;decision analysis;group decision support;intelligent agent;group decision making;multicriteria decision analysis	This work aims to document and present the most significant developments in the usage of negotiation- and argumentation-based multi-agent systems in the field of multiple criteria decision support. Also, this work sets forth the goal of a currently ongoing research for the development of an argument-based intelligent agent platform for the support of group decision making using multicriteria decision analysis methods.	decision support system;multi-agent system	Nikolaos F. Matsatsinis;Konstantinos-Dimitrios Tzoannopoulos	2008	Operational Research	10.1007/s12351-008-0008-4	r-cast;group decision-making;decision support system;intelligent decision support system;decision analysis;decision engineering;computer science;knowledge management;artificial intelligence;decision tree;management science;evidential reasoning approach;intelligent agent;business decision mapping	AI	-31.240142837901868	-9.67781560632643	52529
24e1d60e37812492c220af107d911d440d997056	branch-and-bound processing of ranked queries	databases;database system;computacion informatica;multi criteria decision making;branch and bound algorithm;r tree;multi dimensional;branch and bound algorithms;ciencias basicas y experimentales;indexation;ranked queries;grupo a;access method;branch and bound	Despite the importance of ranked queries in numerous applications involving multi-criteria decision making, they are not efficiently supported by traditional database systems. In this paper, we propose a simple yet powerful technique for processing such queries based on multi-dimensional access methods and branch-and-bound search. The advantages of the proposed methodology are: (i) it is space efficient, requiring only a single index on the given relation (storing each tuple at most once), (ii) it achieves significant (i.e., orders of magnitude) performance gains with respect to the current state-of-theart, (iii) it can efficiently handle data updates, and (iv) it is applicable to other important variations of ranked search (including the support for non-monotone preference functions), at no extra space overhead. We confirm the superiority of the proposed methods with a detailed experimental study. r 2006 Elsevier B.V. All rights reserved.	branch and bound;database;experiment;overhead (computing);single-index model;monotone	Yufei Tao;Vagelis Hristidis;Dimitris Papadias;Yannis Papakonstantinou	2007	Inf. Syst.	10.1016/j.is.2005.12.001	computer science;theoretical computer science;operating system;machine learning;data mining;database;branch and bound;statistics	DB	-27.70329999290902	3.046332401250998	53200
d5e68f5007469b187a2e3935f74948801f342edf	monitoring query processing in mobile robot databases		In this paper, we propose methods for monitoring query processing in mobile robot databases. We assume that a mobile robot can move based on the specified movement plan and perform sensing (e.g., temperature measurements) at the specified points. The purpose of our query processing is to reduce the total travel and measurement time while ensuring the given sensing quality requirements. We develop a framework based on an existing approach in sensor databases. Since features of mobile robots are different from those of sensor networks, we extend the former approach considering our context. We propose four algorithms for planning robot movements and compare these methods in simulation-based experiments.		Kento Sugiura;Arata Hayashi;Tingting Dong;Yoshiharu Ishikawa	2014		10.1007/978-3-662-43984-5_20	computer vision;simulation;computer science;data mining;database	Robotics	-25.382811705666967	0.3112788176391952	53289
5ced0dea84cff0e6a5cd8a143ac00b03002bcec6	learning in a multi-agent approach to a fish bank game	navio;regle inference;multiagent system;learning algorithm;architecture systeme;supervised learning;vertebrata;intelligence artificielle;algorithme apprentissage;inference rule;pisces;artificial intelligence;arquitectura sistema;inteligencia artificial;ship;apprentissage supervise;system architecture;sistema multiagente;aprendizaje supervisado;algoritmo aprendizaje;systeme multiagent;regla inferencia;navire	In this paper application of symbolic, supervised learning in a multi-agent system is presented. As an environment Fish Bank game is used. Agents represent players that manage fishing companies. Rule induction algorithm is applied to generate ship allocation rules. In this article system architecture and learning process are described and preliminary experimental results are presented. Results show that learning agent performance increases significantly when new experience is taken into account.	multi-agent system	Bartlomiej Sniezynski;Jaroslaw Kozlak	2005		10.1007/11559221_62	error-driven learning;simulation;computer science;artificial intelligence;machine learning;supervised learning;active learning;rule of inference	AI	-23.09155966347091	-6.976481704683216	53597
30f97e7e93b431151abf8db68c08345589ce4c7b	connecting the different faces of information	logic;semantics;universiteitsbibliotheek;philosophy;artificial intelligence			Johan van Benthem;Robert van Rooy	2003	Journal of Logic, Language and Information	10.1023/A:1025026116766	natural language processing;philosophy;epistemology;computer science;artificial intelligence;semantics;linguistics;symbolic artificial intelligence;logic;cognitive science	NLP	-27.372605629498228	-9.194780467442158	53775
b7de1b92a244f59073ba5d7e68205e5c97dae9cf	envelope parameter calculation of similarity indexing structure	algoritmo busqueda;multimedia;implementation;algorithme recherche;search algorithm;ejecucion;indexing;indexation;estructura datos;indizacion;pattern recognition;structure donnee;reconnaissance forme;systeme gestion base donnee;information system;reconocimiento patron;multimedia database;high dimension;sistema gestion base datos;database management system;data structure;content based retrieval;systeme information;similarity index;sistema informacion	Similarity Indexing is very important for content-based retrieval on large multimedia databases, and the “tightness” of data set envelope is a factor that influences the performance of index. For equidistant envelope (bounding sphere), calculation of envelope is difficult because of the complexity of direct computation in high dimension space. In this paper we summarize the envelope used in similarity indexing structures, and discuss the envelope parameter calculation problem. We improved theγ-spatial search algorithm proposed by R. Kuniawati and J.S.Jin for equidistant envelope parameter calculation, and apply it to various distance spaces. Basic thoughts and theorem are provided in the paper, also with algorithm implementation.	bounding sphere;computation;random indexing;search algorithm;spatial database	Xuesheng Bai;Guangyou Xu;Yuanchun Shi	1998		10.1007/BFb0016502	search engine indexing;data structure;computer science;theoretical computer science;data mining;mathematics;programming language;implementation;information system;algorithm;search algorithm	DB	-26.01609347198588	1.8838796027911742	54059
3be5dc3b2e17bfaba6d3a455cc1d2294fefa277e	improving space-efficiency in temporal text-indexing	modelizacion;model based reasoning;text;raisonnement base sur modele;base donnee temporelle;texte;modelisation;inclusion expresion;inclusion expression;indexing;indexation;indizacion;temporal databases;texto;modeling;text indexing;analytical model;query containment	Support for temporal text-containment queries is of intere st in a number of contexts. In previous papers we have presented two app r ches to temporal text-indexing, the V2X and ITTX indexes. In this paper, we fir st present improvements to the previous techniques. We then perform a study of t he space usage of the indexing approaches based on both analytical models and results from indexing temporal text collections. These results show for wh at kind of document collections the different techniques should be employed. T he results also show that regarding space usage, the new ITTX/VIDPI technique pr oposed in this paper is in most cases superior to V2X, except in the case of patt erns of high number of new documents relative to number of updated documents.	space–time tradeoff	Kjetil Nørvåg;Albert Overskeid Nybø	2005		10.1007/11408079_72	search engine indexing;systems modeling;computer science;artificial intelligence;model-based reasoning;data mining;database;temporal database;world wide web;information retrieval;algorithm	DB	-25.958599188490865	1.79631871870541	54077
00ee62992ba2bba580852b93a3da4a82283797d3	funser: a functional server for textual information retrieval	data intensive application;programming language;information retrieval	The paper describes a data-intensive application written in a lazy functional language: a server for textual information retrieval. The design illustrates the importance of interoperability, the capability of interacting with code written in other programming languages. Lazy functional programming is shown to be a powerful and elegant means of accomplishing several desirable concrete goals: delivering initial results promptly, using space economically, and avoiding unnecessary I/O. Performance results, however, are mixed.	data-intensive computing;functional programming;information retrieval;interaction;interoperability;lazy evaluation;programming language;server (computing)	Donald A. Ziff;Stephen P. Spackman;Keith Waclena	1995	J. Funct. Program.	10.1017/S0956796800001386	question answering;relevance;cognitive models of information retrieval;computer science;database;adversarial information retrieval;programming language;data retrieval;information retrieval;human–computer information retrieval	PL	-32.9363772010294	3.470869889309184	54405
3b95adfd2a6c58f797ce454e748e37080324018f	how to improve collaborative decision making in the context of knowledge management	knowledge management;multi criteria decision aid;set approach;collaborative decision making;dominance based rough set approach;decision rules;chapitre d ouvrage	In this paper we present a Multi-criteria Classification Methodology for identifying and evaluating crucial knowledge. We aim to identify, on the one hand, the explicit and tacit crucial knowledge to be preserved in the organizational memory, and on the other hand the tacit knowledge that is hard to formalize.	knowledge management	Inès Saad;Michel Grundstein;Camille Rosenthal-Sabroux	2008			engineering;knowledge management;data mining;management science;personal knowledge management	AI	-31.1212275372584	-7.8147829378590306	54464
34ec06cca1929fe41424178ba43abdb36a9e0966	topic 5: parallel and distributed databases, data mining and knowledge discovery		Managing and efficiently analysing the vast amounts of data produced by a huge variety of data sources is one of the big challenges in computer science. The development and implementation of algorithms and applications that can extract information diamonds from these ultra-large, and often distributed, databases is a key challenge for the design of future data management infrastructures. Today's data-intensive applications often suffer from performance problems and an inability to scale to high numbers of distributed data sources. Therefore, distributed and parallel databases have a key part to play in overcoming resource bottlenecks, achieving guaranteed quality of service and providing system scalability. The increased availability of distributed architectures, clusters, Grids and P2P systems, supported by high performance networks and intelligent middleware provides parallel and distributed databases and digital repositories with a great opportunity to cost-effectively support key everyday applications. Further, there is the prospect of data mining and knowledge discovery tools adding value to these vast new data resources by automatically extracting useful information from them.	data mining and knowledge discovery;database	Patrick Valduriez;Wolfgang Lehner;Domenico Talia;Paul Watson	2006		10.1007/11823285_34		ML	-32.22661719956251	-1.1213880025446599	54580
6c458f114d7059d8b446cdf5ac9ed83602b711a2	inexact reasoning in expert systems: a stochastic parallel network approach	expert system		expert system	V. Venkatasubramanian	1985			model-based reasoning;data mining;machine learning;artificial intelligence;expert system;computer science;legal expert system	AI	-27.932772999908384	-7.85791076352327	54597
6b2b861f0736f0d3247e34412634a14c58f4b12a	performance analysis of particle-match search algorithms for bd trees	databases;multidimensional data structures;search algorithm;partial match search;performance analysis;bd trees;k d trees	Abstract#R##N##R##N#Database systems are becoming increasingly popular for answering queries. Partial-match search queries are an important class of queries in such a system. Several storage structures have been proposed to answer these queries efficiently. The BD tree is an example of such a storage structure. A previous study indicated that the k-d tree performance is better than that of the BD tree for partial-match search queries. A recent paper reported some improved algorithms. However, it is unclear whether the improved algorithms show the BD tree in a favourable light for partial-match search queries. This paper explores the performance of these algorithms and compares their performance to that of the k-d tree. Since the BD tree construction process uses some heuristics to make it a better balanced tree, this paper also evaluates the effect of these heuristics on the partial-match search algorithms. The major conclusions of this study are that the BD tree performance for partial-match search is better than that of the k-d tree when an improved algorithm is used for partial-match search, and only the DZ expression rearrangement heuristic has substantial effect on partial-match search performance.	algorithm;blu-ray;profiling (computer programming)	Sivarama P. Dandamudi;Paul G. Sorenson	1988	Softw., Pract. Exper.	10.1002/spe.4380180110	optimal binary search tree;beam search;computer science;theoretical computer science;order statistic tree;tree rearrangement;k-d tree;incremental decision tree;data mining;database;iterative deepening depth-first search;fractal tree index;search tree;ternary search tree;k-d-b-tree;tree traversal;depth-first search;search algorithm	ML	-29.848340012561106	3.7177865628004287	54775
e2bfd30e37a7ec0ae67962483c9e00fbf081262b	goal-oriented multimedia dialogue with variable initiative	goal orientation;multimedia;relacion hombre maquina;man machine relation;intelligence artificielle;systeme conversationnel;resolucion problema;interactive system;sistema conversacional;artificial intelligence;relation homme machine;inteligencia artificial;problem solving;resolution probleme	"""A dialogue algorithm is described that executes Prolog-style rules in an attempt to achieve a goal. The algorithm selects paths in the proof in an attempt to achieve success and proves subgoals on the basis of internally available information where possible. Where \missing axioms"""" are discovered, the algorithm interacts with the user to solve subgoals, and it uses the received information from the user to attempt to complete the proof. A multimedia grammar codes messages sent to and received from the user into a combination of speech, text, and voice tokens. This theory is the result of a series of dialog projects implemented in our laboratory. These will be described including statistics that measure their levels of success. When two individuals collaborate to solve a problem, they undertake a variety of behaviors to enable a fast and eecient convergence to a goal. They reason individually to nd a sequence of actions that will achieve success and if they are successful, the problem will be solved directly. The reasoning, however, could uncover obstacles and they may then communicate to try to address them. One participant may see a solution to the problem if a speciic issue can be solved and bring that to the attention of the other. The other may be able to provide the needed support but may respond with other subproblems to be faced. They hand back and forth subproblems related to the global goal and solutions to them as they nd them. If a suucient set of the subgoals is solved, the global one will be solved also and the interaction will terminate with success. The problematic subgoals are thus the ones that generate most interactions. We will call the set of interactions related to a given subgoal a subdialogue, and every problem solving dialogue seems to be an amalgamation of such sub-dialogues. Thus we do not see a problem solving dialogue as a linear coverage from beginning to end. Rather we see it as a light footed dance in the space of mentionable topics with almost every step aimed at a problematic subgoal. The concept of subdialogues as constituents of dialogue has been studied widely. For example, Grosz and Sidner 5] call them segments and they give an analysis of dialogue phenomena in terms of a three part model. Speciically, they deene the intentional structure to be the reasoning mechanism that supports the interaction, the attentional …"""	algorithm;code;dos;interaction;problem solving;prolog;terminate (software);dialog	Alan W. Biermann;Curry I. Guinn;Michael S. Fulkerson;Greg A. Keim;Zheng Liang;Douglas M. Melamed;Krishnan Rajagopalan	1997		10.1007/3-540-63614-5_1	simulation;computer science;artificial intelligence;machine learning;goal orientation;database;mathematics;distributed computing;computer security;algorithm	AI	-24.64307070420048	-8.184564485644202	54962
3a1abb4ef1947482ff9ee4214723aedb8a40df19	docbase - the inex evaluation experience	database system;information retrieval;system design;database query	Can a system designed primarily for the purpose of databasetype storage and retrieval be used for information-retrieval tasks? This was one of the questions that led us to participate in the INEX 2004 initiative. DocBase, a prototype database system developed initially for SGML, and adapted to work with XML, was used for the purpose of answering the queries. DocBase uses DSQL, an adaptation of SQL to provide a mechanism for querying XML using existing database and indexing technologies. The INEX evaluation experience was encouraging although it did show the limitations of database query languages for classic information retrieval tasks, it also demonstrated that several interesting results can be obtained by using database query languages for information retrieval, especially for queries involving both content and structure. Our results demonstrate the adaptability and scalability of a database system for processing IR queries.	database;information retrieval;prototype;query language;sql;scalability;standard generalized markup language;xml	Sriram Mohan;Arijit Sengupta	2004		10.1007/11424550_21	database theory;database tuning;computer science;data mining;database;view;database schema;information retrieval;alias;database design;query language	DB	-33.61442758433066	3.7530711987545833	55175
02b1e8dde724459f24b05e123057c08b316cd7a0	storage and indexing of relational olap views with mixed categorical and continuous dimensions		Due to the widespread adoption of locationbased services and other spatial applications, data warehouses that store spatial information are becoming increasingly prevalent. Consequently, it is becoming important to extend the standard OLAP paradigm with features that support spatial analysis and aggregation. While traditional OLAP systems are limited to data characterized by strictly categorical feature dimensions, Spatial OLAP systems must provide support for both categorical and spatial feature dimensions. Such spatial feature dimensions are typically represented by continuous data values. In this paper we propose a technique for representing and indexing relational OLAP views with mixed categorical and continuous data. Our method builds on top of an established mechanism for standard OLAP and exploits characteristic properties of space-filling curves. It allows us to effectively represent and index mixed categorical and continuous data, while dynamically adapting to changes in dimension cardinality during updates. We have implemented the proposed storage and indexing methods and evaluated their build, update, and query times using both synthetic and real datasets. Our experiments show that the proposed methods based on Hilbert curves of dynamic resolutions offers significant performance advantages especially for view updates.	data drilling;experiment;feature model;hilbert space;keyboard technology;online analytical processing;programming paradigm;space-filling curve;spatial analysis;spatial reference system;synthetic intelligence	Oliver Baltzer;Andrew Rau-Chaplin;Norbert Zeh	2007	JDIM		online analytical processing;computer science;data science;data mining;database	DB	-31.50647540215787	1.080888115367171	55262
f50440b10052864586e105e39e7eec390842d5e4	data modeling for analytical queries on document-oriented dbms		NoSQL database management systems have emerged as an alternative to increase performance and decrease the hardware costs of applications that use traditional relational databases. However, there are not many works on how to guide data modeling for such DBMS to gain query performance. Specially, in the context of Business Intelligence (BI) applications, data modeling should take into account analytical queries performance. This work highlights the importance of data modeling for this kind of application on NoSQL DBMS. It shows how much alternative modelings can significantly impact on the query performance. Experiments were performed on MongoDB, a popular document-oriented NoSQL DBMS, and show some significant results. In addition, a modeling heuristic is presented for this DBMS, and suggests that more than one document collection, based on alternative data modelings, should be maintained in order to improve query performance. Future work points to query redirection mechanisms for such systems.	aggregate data;archive;benchmark (computing);couchbase server;data modeling;document-oriented database;experiment;heuristic;ibm tivoli storage productivity center;mongodb;nosql;orientdb;pa-risc;relational database	R. A. S. N. Soransso;Maria Cláudia Reis Cavalcanti	2018		10.1145/3167132.3167191	nosql;database;relational database;business intelligence;data modeling;heuristic;computer science	DB	-33.24906039058809	0.5248002496714653	55316
a9f159e61f25e21a459e6d85a32dba60611f281b	reasoning with linguistic preferences using npn logic		Negative-positive-neutral logic provides an alternative framework for fuzzy cognitive maps development and decision analysis. This paper reviews basic notion of NPN logic and NPN relations and proposes adaptive approach to causality weights assessment. It employs linguistic models of causality weights activated by measurement-based fuzzy cognitive maps’ concepts values. These models allow for quasi-dynamical adaptation to the change of concepts values, providing deeper understanding of possible side effects. Since in the real-world environments almost every decision has its consequences, presenting very valuable portion of information upon which we also make our decisions, the knowledge about the side effects enables more reliable decision analysis and directs actions of decision maker.	causality;cluster analysis;cognitive map;decision analysis;fuzzy concept;polish notation;side effect (computer science);software propagation	Goran Devedzic;Danijela Milosevic;Lozica Ivanovic;Dragan Adamovic;Miodrag Manic	2010	Comput. Sci. Inf. Syst.	10.2298/CSIS090223003D	artificial intelligence;machine learning;data mining;algorithm	Web+IR	-19.43590253014781	1.3623242259634831	55364
3907712f40c5a52caaa40775c8cf8a53b929645d	intcare: a knowledge discovery based intelligent decision support system for intensive care medicine	intelligent decision support system;decision support;decouverte de la connaissance;systeme intelligent;aplicacion medical;intensive care unit;systeme aide decision;intelligent information system;sistema inteligente;hospital ward;base connaissance;intelligence artificielle;sistema ayuda decision;mortalite;servicio hospitalario;data mining;intelligent decision support;qualite service;decision support system;mortalidad;mortality;fouille donnee;intensive care;decouverte connaissance;knowledge discovery process;intelligent system;artificial intelligence;descubrimiento conocimiento;base conocimiento;medical application;modele donnee;inteligencia artificial;appui de decision intelligent;information system;intencion;service hospitalier;quality of service;busca dato;systeme information;intention;service quality;data models;soin intensif;application medicale;calidad servicio;sistema informacion;knowledge base;knowledge discovery	This paper introduces the INTCare system, an intelligent information system based on a completely automated Knowledge Discovery process and on the Agents paradigm. The system was designed to work in Hospital Intensive Care Units, supporting the physicians’ decisions by means of prognostic Data Mining models. In particular, these techniques were used to predict organ failure and mortality assessment. The main intention is to change the current reactive behaviour to a pro-active one, enhancing the quality of service. Current applications and experimentations, the functional and structural aspects, and technological options are presented. 2 Title of the journal. Volume X – no X/2002 RÉSUMÉ. Cet article présente INTCare, un système d'information intelligent basé sur un procédé complètement automatisé de découverte de la connaissance et sur le paradigme d'agents. Le système a été conçu pour fonctionner dans les services hospitaliers de réanimation, soutenant les décisions des médecins au moyen de modèles d'extraction de données pronostiques. En particulier, ces techniques ont été employées pour prévoir l'échec d'organes et l'évaluation de la mortalité. La principale intention est de changer le comportement réactif habituel en proactif augmentant la qualité du service. Des applications et les expérimentations courantes, les aspects fonctionnels et structuraux, et des options technologiques sont présentés.	autonomous robot;data mining;experiment;information system;intelligent decision support system;international components for unicode;knowledge acquisition;linear algebra;online and offline;preprocessor;programming paradigm;quality of service;reinforcement learning;situated;software deployment;system call	Pedro Gago;Manuel Filipe Santos;Santos Alvaro Silva;Paulo Cortez;José Neves;Lopes Gomes	2005	Journal of Decision Systems	10.3166/jds.14.241-259	data modeling;simulation;quality of service;decision support system;computer science;artificial intelligence;operations research;service quality;information system	ML	-25.403630396719233	-5.4381733454618315	55581
c553206ac8251e974569cc48219764ed589c69ca	action languages		Action languages are formal models of parts of the natural language that are used for talking about the eeects of actions. This article is a collection of deenitions related to action languages that may be useful as a reference in future publications.	action language;natural language	Michael Gelfond;Vladimir Lifschitz	1998	Electron. Trans. Artif. Intell.			ML	-19.9904791840491	2.617039988120681	55598
c31de7276d3f3912c96e488e4420825528b46164	range and knn query processing for moving objects in grid model	dynamic change;moving object;range query;query processing;k nearest neighbors query;mobile computer;wireless communication;moving objects;k nearest neighbor	With the growing popularity of mobile computing devices and wireless communications, managing dynamically changing information about moving objects is becoming feasible. In this paper, we implement a system that manages such information and propose two query algorithms: a range query algorithm and a k nearest neighbor algorithm. The range query algorithm is combined with an efficient filtering technique which determines if a polyline corresponding to the trajectory of a moving object intersects with a given range. We study the performance of the system, which shows that despite the filtering step, for moderately large ranges, the range query algorithm we propose outperforms the algorithm without filtering.		Hae Don Chon;Divyakant Agrawal;Amr El Abbadi	2003	MONET	10.1023/A:1024535730539	range query;sargable;query optimization;computer science;operating system;data mining;database;mobile computing;k-nearest neighbors algorithm;information retrieval;wireless	DB	-26.58650310444526	0.5988809000462273	55620
98d44d5bf60feebd4435d7aa0f845bb4be1c2785	oid: optimized information discovery using space filling curves in p2p overlay networks	object recognition;range query;resource discovery;query processing;p2p overlay networks;complex queries;query processing peer to peer computing;overlay networks;p2p;query optimization;information discovery;space filling curves;p2p information discovery system;computer architecture;indexes;engines;system design;indexation;tree based query optimizations;multiattribute range queries;overlay network;optimization;space filling curve;peer to peer computing;tree based query optimizations optimized information discovery space filling curves p2p overlay networks p2p information discovery system multiattribute range queries;filling query processing design optimization scalability fault tolerance grid computing;complex queries p2p overlay networks resource discovery space filling curves;optimized information discovery	In this paper, we present the system design and evaluation of a space filling curve (SFC)-based P2P information discovery system OID. The OID system uses multiple SFCs to significantly optimize the performance of multi-attribute range queries, particularly for applications with a large number of data attributes where a single big SFC-based index is inefficient. The basic idea is to have multiple SFC based indices and select the best one to perform a query. We also introduce two tree-based query optimizations that increase the scalability of the system.	algorithm;bottleneck (software);computation;database;discovery system;information discovery;koch snowflake;load balancing (computing);mathematical optimization;overlay network;prototype;range query (data structures);routing;scalability;space-filling curve;systems design	Faraz Ahmed Memon;Daniel Tiebler;Frank Dürr;Kurt Rothermel;Marco Tomsu;Peter Domschitz	2008	2008 14th IEEE International Conference on Parallel and Distributed Systems	10.1109/ICPADS.2008.57	overlay network;computer science;theoretical computer science;data mining;database	DB	-29.292953002404996	1.0349386861028962	55912
3e1bb73ed29556f564ca4ad1c95def3f8f322e7b	generalization of a new definition of consistency for pairwise comparisons	generalizacion;evaluation performance;judgment;sistema experto;performance evaluation;evaluacion prestacion;connaissance;conocimiento;consistencia;knowledge;jugement;generalisation;experts opinions;knowledge acquisition;consistance;acquisition;juicio;systeme expert;judgement inconsistency;generalization;consistency;adquisicion;expert system			Zbigniew Duszak;Waldemar W. Koczkodaj	1994	Inf. Process. Lett.	10.1016/0020-0190(94)00155-3	generalization;computer science;artificial intelligence;data mining;mathematics;expert system	DB	-20.673840855034733	-1.5960312761049331	55966
1b58ef446a28e91efad158e80725a08686a76431	handling frequent updates of moving objects using the dynamic non-uniform grid		For services related with the u-LBS and u-GIS, most previous works had try to solve frequent updates as a lot of moving objects by extending traditional R-tree. In related works, however, processing for a situation occurring partial denseness of many objects is so hard because these haven't considering the non-uniform distribution. Thus, we proposed new scheme to solve problems above by using the dynamic non-uniform grid. Due to its result of split isn't equal, our proposed scheme can allow distributed processing locally for dense moving objects. Also it has several in-memory buffers to handle frequent updates of massively moving objects lazily.		Ki-Young Lee;Jeong Jin Kang;Joung-Joon Kim;Chae-Gyun Lim;Myung-Jae Lim;Kyu-Ho Kim;Jeong-Lae Kim	2011		10.1007/978-3-642-27186-1_22	real-time computing;computer science;database;distributed computing	DB	-25.232420877816697	2.4711415407620274	56051
4ee91f78e4fa95806ffa5b3ee16cfdf711935aca	supporting direction relations in spatio-temporal databases	database indexing;visual databases database indexing geographic information systems mobile computing query processing temporal databases;direction relations;spatial databases roads query processing spatial indexes indexing database systems data engineering geographic information systems maintenance engineering surfaces;geographic information system;query processing;spatio temporal databases;spatial index;time interval directional queries;spatial database;mobile environment;geographic information systems;spatial index structures;indexation;time slice directional queries direction relations spatio temporal databases geographic information systems mobile environment spatial index structures r fnbbx index time interval directional queries;temporal databases;time slice directional queries;mobile computing;r fnbbx index;visual databases	Direction relations constitute an important class of user queries in spatial databases and geographic information systems. However, no work has dealt with the processing of such relations in a mobile environment. Supporting direction queries using traditional spatial index structures do not work well in this environment because of the need to frequently update the index, which results in very poor performance. In this work, a new indexing technique, named the R+FnBBx index, is proposed to efficiently handle time-interval and time-slice directional queries in the past, present and future.	geographic information system;preemption (computing);spatial database	Bibi Annie Oommen;K. A. Abdul Nazeer	2006	2006 1st International Conference on Digital Information Management	10.1109/ICDIM.2007.369355	computer science;data mining;database;geographic information system;spatial database;information retrieval	DB	-26.978963084649628	1.1295882935920145	56340
6d36ac09d9fbbdd6649a633f96dca8e34bf807d5	towards an experience-based negotiation agent	trading agents;commerce electronique;multiagent system;raisonnement base sur cas;razonamiento fundado sobre caso;systeme intelligent;comercio electronico;systeme aide negociation;case base reasoning;e commerce;sistema inteligente;context of use;conceptual framework;trade negotiations;adaptive behaviour;intelligent system;negotiation support systems;information system;case based reasoning;sistema multiagente;experience base;systeme information;electronic trade;systeme multiagent;sistema informacion	Current E-Commerce trading agents with electronic negotiation facilities usually use predefined and non-adaptive negotiation mechanisms [5]. This paper presents a negotiation agent that applies Case-Based Reasoning techniques to capture and re-use previously successful negotiation experiences. This Experience Based Negotiation (EBN) agent provides adaptive negotiation strategies that can be generated dynamically and are context-sensitive. We demonstrate this negotiation agent in the context of used-car trading. This paper describes the negotiation process and the conceptual framework of the EBN agent. It discusses our web based used-car trading prototype, the representation of the used-car trading negotiation experience, and the stages of experience based negotiation. The paper also discusses some experimental observation and illustrates an example of adaptive behaviour exhibited by the EBN agent. We believe that this Experience Based Negotiation framework can enhance the negotiation skills and performance of current trading agents.	adaptive behavior;case-based reasoning;context-sensitive grammar;e-commerce;experience;experiment;problem domain;prototype;real life;synergy	Wai Yat Wong;Dong Mei Zhang;Mustapha Kara-Ali	2000		10.1007/978-3-540-45012-2_13	e-commerce;case-based reasoning;simulation;negotiation theory;computer science;knowledge management;artificial intelligence;conceptual framework;information system	AI	-23.07067517361481	-7.055526693991314	56606
e48615fca12f2f140701f5f474857bce03a6ac01	an inverted file processor for information retrieval	information retrieval;computer systems information retrieval inverted files merging parallel processing;inverted files;computer systems;system performance;merging;document retrieval;parallel processing	Response time in large, inverted file document retrieval systems is determined primarily by the time required to access files of document identifiers on disk and perform the processing associated with a Boolean search request. This paper describes a specialized computer system capable of performing these functions in hardware. Using this equipment, a complicated sample search involving 70 terms and over 67 000 document references can be performed from 13 to 60 times faster than with a conventional machine. Alternatively, many small searches can be processed concurrently with little effect upon system performance. Similar configurations can be applied to standard merging and sorting problems.	boolean algebra;computer;document retrieval;identifier;information retrieval;inverted index;sorting algorithm	William H. Stellhorn	1977	IEEE Transactions on Computers	10.1109/TC.1977.1674787	document retrieval;parallel processing;merge;inverted index;computer science;theoretical computer science;database;information retrieval	DB	-28.481062294341456	3.8008919588344265	56719
b90a96df6919d919867c7df656c3669483a4b90d	towards a ubiquitous semantics of interaction: phenomenology, scenarios, and traces	reponse temporelle;phenomenology;relevance model;representacion conocimientos;realite virtuelle;realidad virtual;phenomenologie;interaction;relacion hombre maquina;virtual reality;semantics;man machine relation;discrete time;fenomenologia;semantica;semantique;qa75 electronic computers computer science;time response;user behaviour;interaccion;relation homme machine;tiempo discreto;knowledge representation;temps discret;respuesta temporal;representation connaissances;timing	This paper begins a process of building a semantic framework to link the many diverse interface notations that are used in more formal communities of HCI. The focus in this paper is on scenarios – single traces of user behaviour. These form a point of contact between approaches that embody very different models of interface abstractions or mechanisms. The paper looks first at discrete time models as these are more prevalent and finds that even here there are substantive issues to be addressed, especially concerning the different interpretation of timing that become apparent when you relate behaviour from different models/notations. Ubiquitous interaction, virtual reality and rich media all involve aspects of more continuous interaction and the relevant models are reviewed. Because of their closer match to the real world, they are found to differ less in terms of ontological features of behaviour.	align (company);human–computer interaction;interactive media;interstitial webpage;language binding;mathematical model;programming paradigm;theory;tracing (software);virtual reality	Alan J. Dix	2002		10.1007/3-540-36235-5_18	discrete time and continuous time;interaction;simulation;computer science;phenomenology;artificial intelligence;semantics;virtual reality	HCI	-25.2243652872482	-7.090678038078794	56734
be5d19622762c075cf2b436d02a16efb59609fca	the dimension-join: a new index for data warehouses	materialized views;data warehouse;indexation	There are several auxiliary pre-computed access structures that allow faster answers by reading less base data. Examples are materialized views, join indexes, B-tree and bitmap indexes. This paper proposes dimension-join, a new type of index especially suited for data warehouses. The dimension-join borrows ideas from several concepts. It is a bitmap index, it is a multi-table join and when being used one of the tables is not read to improve performance. It is a multi-table join because it holds information belonging to two tables, which is similar to the join index proposed by Valduriez. However, instead of being composed by the tables’ primary keys, the dimension-join index is a bitmap index over the fact table using values from a dimension column. The dimension-join index is very useful when selecting facts depending on dimension tables belonging to snowflakes. The dimension-join represents a direct connection between the fact table and a table in the snowflake that can avoid several joins and produce enormous performance improvements. This paper also evaluates experimentally the dimension-join indexes using the TPC-H benchmark and shows that this new index structure can dramatically improve the performance for some queries.	b-tree;benchmark (computing);bitmap index;column (database);concurrency (computer science);database server;experiment;ibm tivoli storage productivity center;join (sql);materialized view;oracle database;pl/sql;precomputation;sql;server (computing);unique key	Pedro Bizarro;Henrique Madeira	2001			database;fact table;data mining;materialized view;performance improvement;bitmap index;computer science;indexation;joins;data warehouse	DB	-28.7852867553459	3.4202075696103504	57169
2826c243fbd8b51f67c50c8dd6a8a59c09c89669	multidimensional data structures: review and outlook	multidimensional data structure	Publisher Summary   This chapter presents simple and efficiently balanced multidimensional tree structures that are useful for real-time applications. It also describes balanced multidimensional and weighted trees as well as their applications in physical database organization, information retrieval, and computational geometry problems. The performance of a multidimensional data structure is measured in terms of three quantities—the preprocessing cost, the storage cost, and the access cost. The multidimensional data structures, such as the k-d trees, quadtrees, and range trees, are comparison-based data structures. Weighted trees and multidimensional trees are related to each other. One can start from an efficient weighted one-dimensional tree structure and use it for constructing efficient multidimensional or even efficient weighted multidimensional tree structures by using it appropriately for implementing nodes of TRIES. Balanced multidimensional and weighted trees have applications in areas, such as physical database organization, information retrieval, self-organizing file structures, and computational geometry.	microsoft outlook for mac	S. Sitharama Iyengar;Rangasami L. Kashyap;Vijay K. Vaishnavi;Nageswara S. V. Rao	1988	Advances in Computers	10.1016/S0065-2458(08)60257-0	computer science;theoretical computer science;data mining;database	NLP	-28.206555298344618	1.2454958439939903	57265
9ac576207b99129295e85fd14e0b9b499c3b855f	recommending xmltable views for xquery workloads	xml database;materialized views	Physical structures, for example indexes and materialized views, can improve query execution performance by orders of magnitude. Hence, it is important to choose the right configuration of these physical structures for a given database. In this paper, we discuss the types of materialized views that are suitable for an XML database. We then focus on XMLTable materialized views and present a procedure to recommend them given an XML database and a workload of XQuery queries. We have implemented our XMLTable View Advisor in a prototype version based on IBM R © DB2 R © V9.7, which supports both relational and XML data, and we experimentally demonstrate the effectiveness of our advisor’s recommendations.	experiment;materialized view;prototype;xml database;xpath;xquery	Iman Elghandour;Ashraf Aboulnaga;Daniel C. Zilio;Calisto Zuzarte	2009		10.1007/978-3-642-03555-5_11	materialized view;xml validation;computer science;data mining;xml database;database;view;information retrieval	DB	-32.10525320652314	3.4182927532559684	57552
80fe472e339d77df9fa78c22b24e63cf385b75eb	application of a hybrid intelligent decision support model in logistics outsourcing	offre service;real time decision making;decision support;outsourcing;raisonnement base sur cas;razonamiento fundado sobre caso;sistema experto;logistique;externalisation;fuzzy programming;service provider;case base reasoning;systeme aide decision;programming environment;real time;modelo hibrido;logique floue;logica difusa;sistema ayuda decision;prise decision;modele hybride;hybrid model;fuzzy logic;medio ambiente programacion;systeme incertain;sous traitance;decision support system;logistics;temps reel;compromise programming;evaluation and selection 3pl service providers;tiempo real;industrial application;systeme expert;programmation floue;case based reasoning;subcontratacion;subcontracting;toma decision;sistema incierto;proposals;uncertain system;environnement programmation;programacion difusa;logistica;rule based reasoning;expert system	Outsourcing is an increasingly important issue pursued by corporations seeking improved efficiency. Logistics outsourcing or third-party logistics (3PL) involves the use of external companies to perform some or all of the firm’s logistics activities. This paper proposes an intelligent decision support framework for effective 3PL evaluation and selection. The proposed framework integrates case-based reasoning, rule-based reasoning and compromise programming techniques in fuzzy environment. This real-time decisionmaking approach deals with uncertain and imprecise decision situations. Furthermore, the integration of different methodologies takes the advantage of their strengths and complements each other’s weaknesses. Consequently, the framework leads to a more accurate, flexible and efficient retrieval of 3PL service providers (alternatives) that are most similar and most useful to the current decision situation. Finally, a real industrial application is given to demonstrate the potential of the proposed framework. 2006 Published by Elsevier Ltd.	case-based reasoning;complement (complexity);decision support system;logic programming;logistics;outsourcing;real-time locating system;third-party software component	Gülfem Isiklar Alptekin;S. Emre Alptekin;Gülçin Büyüközkan	2007	Computers & OR	10.1016/j.cor.2006.01.011	fuzzy logic;service provider;logistics;case-based reasoning;decision support system;computer science;artificial intelligence;operations research;outsourcing	AI	-26.68406387388612	-4.435637865590304	57709
72b2c38c7e024970c62b42b035390f5fb9c1367e	quantitative and qualitative safety analysis of a hemodialysis machine with s#				Johannes Leupolz;Axel Habermaier;Wolfgang Reif	2018	Journal of Software: Evolution and Process	10.1002/smr.1942		Vision	-29.14286642286824	-8.862935638231235	57857
5ab586c472dac612f8ef4c6bc282284d5ab9bf9f	explanation-based learning for planning		Definition Explanation Based Learning (EBL) involves using prior knowledge to explain (“prove”) why the training example has the label it is given, and using this explanation to guide the learning. Since the explanations are often able to pinpoint the features of the example that justify its label, EBL techniques are able to get by with much fewer number of training examples. On the flip side, unlike general classification learners, EBL requires prior knowledge (aka “domain theory/model”) in addition to labeled training examples—a requirement that is not easily met in some scenarios. Since many planning and problem solving agents do start with declarative domain theories (consisting at least descriptions of actions along with their preconditions and effects), EBL has been a popular learning technique for planning.	explanation-based learning;precondition;problem solving	Subbarao Kambhampati;Sung Wook Yoon	2017		10.1007/978-1-4899-7687-1_97	cooperative learning;error-driven learning;experiential learning;machine learning;action learning	AI	-19.95344246473286	-5.164884824109747	57894
87cdb8039e2e1dfe9d9e829636aac3effd2fb753	stratified multi-agent htn planning in dynamic environments	htn planning;dynamic environment;agent systems	In stratified multi-agent planning (SMAP), the parent planning agent and its child planning agents work together to achieve a goal. The parent planning agent executes a rough plan for a goal, and the child planning agents execute detailed plans for subgoals. Although this kind of SMAP is efficient, it is difficult for the parent agent to change the plan while a child planning agent is working. On the other hand, on-line planning, where the agent continuously updates its plan during the plan execution, is very important if we need to implement planning agents working in dynamic environments. This paper shows how to realize on-line planning in SMAP. For this purpose, we extend Dynagent which is an on-line HTN planning agent system.	automated planning and scheduling;hierarchical task network	Hisashi Hayashi	2007		10.1007/978-3-540-72830-6_20	simulation;knowledge management	AI	-19.501865227373678	-7.726034305518583	57915
dce8a9172c46d82fe8479790f9d4d54f52bf4a13	application of sparse matrix techniques to search, retrieval, classification and relationship analysis in large data base systems - sparcom	sparse matrix;large data	In order to meet the requirements of Large Data Base (LDB) Systems for short response time and high throughput a new approach has been investigated. In this paper only certain aspects of a powerful system, called SPARCOM, are described. The unique approach of this system is the conceptual conversion of data into large sparse binary matrices that enables one to apply sophisticated sparse matrix techniques to perform data base operations. Only for the sake of clarity the matrices are presented in their binary form. The high performance of the system is due to the direct conversion and manipulation of the matrices in their compact form as obtained from the application of the sparse matrix algorithms.	sparse matrix	Ron Ashany	1978			sparse matrix;computer science;theoretical computer science;machine learning;sparse approximation;data mining;matrix-free methods	Web+IR	-32.92257662552907	-1.503879961574493	58079
137610c1946d6b515ab101ae30d7fc8c8b8b942d	temporal machine learning for switching control	electroenergetics;production energie electrique;systeme intelligent;systeme apprentissage;temporal variability;sistema inteligente;electroenergetica;electrical switching;commutation electrique;learning systems;electroenergetique;machine learning;complex system;raisonnement temporel;electric power production;intelligent system;produccion energia electrica;conmutacion electrica;discrete event;switching control;temporal reasoning	In this paper, a temporal machine learning method is presented which is able to automatically construct rules allowing to detect as soon as possible an event using past and present measurements made on a complex system. This method can take as inputs dynamic scenarios directly described by temporal variables and provides easily readable results in the form of detection trees. The application of this method is discussed in the context of switching control. Switching (or discrete event) control of continuous systems consists in changing the structure of a system in such a way as to control its behavior. Given a particular discrete control switch, detection trees are applied to the induction of rules which decide based on the available measurements whether or not to operate a switch. Two practical applications are discussed in the context of electrical power systems emergency control.	complex system;ibm power systems;machine learning	Pierre Geurts;Louis Wehenkel	2000		10.1007/3-540-45372-5_43	complex systems;simulation;computer science;artificial intelligence;machine learning	ML	-22.51698191428218	-4.070110949270328	58087
8018b2800888466d6b0fc0d19532ac00fae745ab	skills, rules, and knowledge; signals, signs, and symbols, and other distinctions in human performance models	man machine systems human factors;human performance;performance systeme;man machine relation;system performance;man machine system;modelisation;interface systems qualitative models human performance models man machine interface systems models of human performance performance signals signs symbols quantitative models;human factors;humans knowledge based systems cognition planning feedforward neural networks context;relation homme machine;modeling;man machine systems;systeme homme machine	The introduction of information technology based on digital computers for the design of man-machine interface systems has led to a requirement for consistent models of human performance in routine task environments and during unfamiliar task conditions. A discussion is presented of the requirement for different types of models for representing performance at the skill-, rule-, and knowledge-based levels, together with a review of the different levels in terms of signals, signs, and symbols. Particular attention is paid to the different possible ways of representing system properties which underlie knowledge-based performance and which can be characterised at several levels of abstraction-from the representation of physical form, through functional representation, to representation in terms of intention or purpose. Furthermore, the role of qualitative and quantitative models in the design and evaluation of interface systems is mentioned, and the need to consider such distinctions carefully is discussed.	computer;function representation;human reliability;information sign;principle of abstraction;symbol (formal);user interface	Jens Rasmussen	1983	IEEE Transactions on Systems, Man, and Cybernetics	10.1109/TSMC.1983.6313160	human performance technology;simulation;systems modeling;computer science;artificial intelligence;human factors and ergonomics;machine learning;computer performance	AI	-24.858400902829647	-7.051556856127684	58255
13492172eeca090853363bba5646081fadf67e25	name-tracing using the icl content addressable filestore	icl content addressable filestore	The paper describes the Regional and National Tracing Systems being developed for the UK enabling public service records to be identified and located using unreliable and incomplete name and address information, and explains how CAFS provides an answer to a problem which would otherwise be insoluble. Topics covered include the sizing calculations, the main database organisation, the primary and secondary indexing structures and particular reference is made to the use of CAFS in 'fuzzy' and quorum searches.	content addressable file store;content-addressable memory;icl;quorum sensing	A. G. Ward	1984			data mining;fuzzy logic;search engine indexing;computer science;tracing;public service	Web+IR	-27.976131675745144	2.8190707786100346	58359
60182907182a1e86bab9073730eda8de53b9e40e	an inference model with probabilistic ontologies to support automation in effects-based operations planning			customer support;ontology (information science)	Henrique Costa Marques	2012				Robotics	-29.729527875752808	-7.4564553368651625	58487
81afdac8fee3a9e2db4606bf75c4f83f015fc173	a new indexing scheme for content-based image retrieval	nearest neighbor queries;range query;query processing;image database;indexing scheme;feature space;nearest neighbor query;indexation;content based image retrieval;content based retrieval;multidimensional index	We propose a new efficient indexing scheme, called the HG-tree, to support content-based retrieval in image databases. Image content is represented by a point in a multidimensional feature space. The types of queries considered are the range query and the nearest-neighbor query, both in a multidimensional space. Our goals are twofold: increasing the storage utilization and decreasing the area covered by the directory regions of the index tree. The high storage utilization and the small directory area reduce the number of nodes that have to be touched during the query processing. The first goal is achieved by suppressing node splitting if possible, and when splitting is necessary, converting two nodes into three. This is done by proposing a good ordering on the directory nodes. The second goal is achieved by maintaining the area occupied by the directory region as small as possible. This is done by introducing the smallest interval that encloses all regions of the lower nodes. We note that there is a trade-off between the two design goals, but the HG-tree is so flexible that it can control the trade-off to some extent. We present the design of our indexing scheme and associated algorithms. In addition, we report the results of a series of tests, comparing the proposed index tree with the buddy-tree, which is one of the most successful point indexing schemes for a multidimensional space. The results show the superiority of our method.	algorithm;best, worst and average case;content-based image retrieval;database;directory (computing);experiment;feature vector;linear discriminant analysis;maximal set;nearest neighbor search;performance;range query (database);x-tree	Guang-Ho Cha;Chin-Wan Chung	1998	Multimedia Tools and Applications	10.1023/A:1009608331551	range query;query optimization;query expansion;feature vector;computer science;machine learning;data mining;database;world wide web;information retrieval	DB	-27.848270251396695	2.369401081285884	58846
a883ddafa500070c110f4cdb699654712eca3d59	approximate counting of frequent query patterns over xquery stream	recuento;incremental computation;comptage;evaluation performance;enumeration;management system;performance evaluation;enumeracion;batch production;xml language;evaluacion prestacion;interrogation base donnee;procede discontinu;interrogacion base datos;enumeration counting;cache memory;denombrement;data mining;contaje;activity pattern;pattern mining;antememoria;antememoire;produccion por lote;fouille donnee;decouverte connaissance;estructura datos;production par lot;batch process;counting;descubrimiento conocimiento;procedimiento discontinuo;structure donnee;data structure;busca dato;database query;langage xml;lenguaje xml;knowledge discovery	One efficient approach to improve the performance of XML management systems is to cache the frequently retrieved results. This entails the discovery of frequent query patterns that are issued by users. In this paper, we model user queries as a stream of XML query pattern trees and mine for frequent query patterns in a batch-wise manner. We design a novel data structure called D-GQPT to merge the pattern trees of the batches seen so far, and to dynamically mark the active portion of the current batch. With the D-GQPT, we are able to limit the enumeration of candidate trees to only the currently active pattern trees. We also design a summary data structure called ECTree to incrementally compute the frequent tree patterns over the query stream. Based on the above two constructs, we present the frequent query pattern mining algorithm called AppXQSMiner over the XML query stream. Experiment results show that the proposed approach is both efficient and scalable.	algorithm;data mining;data structure;scalability;web search engine;xml;xquery	Liang Huai Yang;Mong-Li Lee;Wynne Hsu	2004		10.1007/978-3-540-24571-1_6	sargable;query optimization;query expansion;web query classification;xml;boolean conjunctive query;cpu cache;data structure;computer science;data mining;management system;database;programming language;web search query;enumeration;range query;world wide web;counting;batch processing	DB	-30.392262435536736	4.156163031462395	58934
04b67bda49edadf2dc618004582a10d2dc66b6a3	simplifying decision trees	simplifying decision tree;simplifying decision trees;excessive complexity;decision tree;opaque structure;paper discusses technique;information processing;artificial intelligence;accuracy;decision theory;fault tree analysis;test bed	simplifying decision tree;simplifying decision trees;excessive complexity;decision tree;opaque structure;paper discusses technique;information processing;artificial intelligence;accuracy;decision theory;fault tree analysis;test bed	decision tree	J. Ross Quinlan	1987	International Journal of Man-Machine Studies	10.1016/S0020-7373(87)80053-6	computer science;artificial intelligence;data mining;algorithm	Arch	-27.301717578459975	-6.949766315130317	58936
5be5ea04af2b31efb113966093d24878980a82ba	developing a real-time inference approach for rule-based reasoning systems	event condition action rules;real time scheduling;event;rule graph;rule based reasoning	Rule-based reasoning systems play importance roles for many real-time intelligent systems that need to take time-critical actions in response to the continuously arriving events. In this paper, we propose a novel inference approach, called RTINF to make the reasoning system meet its hard deadlines. A series of simulation studies are conducted to evaluate the performance of RTINF.	algorithm;artificial intelligence;inference engine;logic programming;real-time clock;real-time locating system;reasoning system;rule 90;simulation;window of opportunity	Ying Qiao;Chang Leng;Hongan Wang;Jian Liu	2013		10.1145/2513228.2513303	opportunistic reasoning;computer science;artificial intelligence;model-based reasoning;machine learning;data mining;reasoning system	AI	-19.34563969456884	-6.624885142619355	59008
21640bec1f288d5737e267c18cbf5878b0b5156e	robotic process automation of unstructured data with machine learning			automation;machine learning	Anna Wróblewska;Tomasz Stanislawek;Bartlomiej Prus-Zajaczkowski;Lukasz Garncarek	2018		10.15439/2018F373	data mining;machine learning;artificial intelligence;computer science;process automation system;unstructured data	EDA	-29.68362070151665	-7.388338080681964	59252
35b05303c38d991264dd987aaac95c53b47fc14c	distributed secondo: a highly available and scalable system for spatial data processing		Motivation Problem: Database management systems (DBMS) have to deal with rapidly growing amounts of data. Nowadays, distributed key-value stores are often used to store huge amounts of data. The current situation can be roughly summarized as follows. DBMS . . . I . . . scale poorly. I . . . provide a lot of functions (e.g. joins or spatial joins) to analyze data. Key-Value stores . . . I . . . scale well. I . . . provide very limited functions regarding data analyzes. Functions like joins have to be implemented repeatedly in the application code. Solution: Couple a DBMS with a key-value store to create a highly available and scalable data base management system. System overview Secondo is an extensible DBMS developed at FernUniversität Hagen. The system is designed with a focus on supporting spatial and spatio-temporal data.	attribute–value pair;high availability;key-value database;scalability	Jan Kristof Nidzwetzki;Ralf Hartmut Güting	2015		10.1007/978-3-319-22363-6_28	data mining;scalability;computer science;spatial analysis;data modeling;database;extensibility;joins	DB	-32.30768028771082	1.1114717997286465	59265
a4b20c9484d4dd32857fb4781810e266cb45653e	a novel indexing method for improving timeliness of high-dimensional data		Investment in information technology (IT) has been growing rapidly and one key reason for investing in IT is to improve information quality (IQ). Timeliness is an important IQ dimension that often needs to be improved for decision making. Especially in the era of big data, timeliness becomes more valued because of challenges of massive data size and high dimensionality. Many financial analyses require timely data to support time-critical decision making. In this paper, we develop a novel index method and effective query algorithms to reduce latency of querying high-dimensional data. The effectiveness of point, range, and similarity queries implemented using our methods is evaluated using a high-dimensional testbed conducted using real world financial data. Results show that our method outperforms existing methods in query speed of three types of queries frequently used in financial decision making.	algorithm;big data;gnu savannah;information quality;performance;scalability;testbed;window of opportunity	Jian Lu;Huong Pham;Hongwei Zhu;Cindy X. Chen	2014			data mining;information retrieval	DB	-28.720529206154428	-1.2100295038666584	59346
6ae09583da7749e349161ea21548cd892d3b0292	iterative design and evaluation methodology for clinical decision support systems			clinical decision support system;iterative design	Fei Yu;Vincent Carrasco;Ketan K. Mane;Javed Mostafa	2015			clinical decision support system;iterative design;management science;computer science;systems engineering	Metrics	-31.791056225894565	-9.1727653328956	59460
8cca84d98e4b9992ce6b90022ff99c45d5a8bcb3	adding compression to next-generation text retrieval systems	data compression;space time;indexation;text retrieval;next generation	In this chapter we present a combination of several data compression features to provide economical storage, faster indexing, and accelerated searches. We discuss recent methods for compressing the text and the index of text retrieval systems. By compressing both the complete text and the index, the total amount of space is less than half the size of the original text alone. Most surprisingly, the time required to build the index and also to answer a query is much less than if the index and text had not been compressed. This is a rare case where there is no space-time trade-off. Moreover, the text can be kept compressed all the time, allowing updates when changes occur in the compressed text.	document retrieval	Nivio Ziviani;Edleno Silva de Moura	2003	Advances in Computers	10.1016/S0065-2458(03)57004-8	data compression;computer science;data science;space time;data mining;information retrieval	NLP	-29.644537200470275	2.5026070932329088	59511
4b5d3eb4a720f937016581745f50a2dd37ccec4e	graph management to improve querying of health and social data		Large amount of data related to health care are stored in heterogeneous data sources. Independently, social media provides information about people’s environment and activities, such as family relationships or patient’s habits and social interaction. This information could be used to complement patients medical profiles to improve patient’s care. Providing expert users with mechanisms to integrate and query such sources becomes crucial to retrieve information allowing to improve the analysis of patient’s situations. This work contributes to facilitating visualization and querying of data coming from such sources. We adopt a graph data model at the conceptual level as it facilitates the integration of structured and semi-structured data. Our purpose is to go a step forward by providing a conceptual query language intended to allow end users, medical domain experts, to retrieve data from heterogeneous data sources by ad hoc queries. In this paper we introduce a set of operators to query data by transforming a graph and we analyze how they fulfill some design features of the conceptual language. These operators allow successive graph transformation to generate subgraphs with filtered data and to derive new relations representing information that is implicit or that is sparse in the data.	conceptual schema;data model;edge contraction;graph rewriting;graphical user interface;harris affine region detector;high- and low-level;hoc (programming language);mathematical optimization;neo4j;performance evaluation;program optimization;prototype;query language;query plan;sparql;sql;scientific visualization;semi-structured data;semiconductor industry;social media;sparse matrix	María Constanza Pabón;Claudia Roncancio;Martha Millán	2014		10.5220/0004805403430350	data modeling;data model;computer science;data virtualization;artificial intelligence;machine learning;data mining;database;information retrieval	DB	-33.59594072574681	-4.192667798491419	59989
4ca9d33d595d9ea933b851c7a7099f06fd3abea8	exact results on response time distributions in networks of queues	exact results	Following a wish expressed by the organizers of this meeting the aim of this presentation has initially been a survey on the available closed-form results about residence time distributions for networks of queues, more precisely: results providing more information than just expected values. The consent to give such a survey was given lightheartedly, since the scarcity of such results promised an easy job. However, a closer look revealed that there is really only one such result, if one defines a network as something involving more than two nodes. It was therefore felt that the original aim had to be broadened to include the available closed-form results on expected residence times in networks of queues. However, here too, there seems to exist just one such result, excluding again mini-networks. Thus, chapters 2 and 3, which are devoted to the description of these two results, are followed by a chapter 4 on remarks about exact algorithms for computing residence time results for certain mini-networks.		Rolf Schassberger	1985		10.1007/978-3-642-87472-7_9	scarcity;real-time computing;residence time;residence;expected value;response time;wish;queue;mathematics	Metrics	-22.87896000459515	-0.03914223896990974	60096
2078cc2af10c26404a28ec13d2338c6652ce2dc3	application of model-based reasoning to the maintenance of telecommunication networks	model based reasoning;telecommunication management network;qualitative reasoning;knowledge representation;telecommunication networks	This paper describes the application of model-based reasoning techniques to the maintenance of telecommunication networks. Model-based reasoning is used in all major steps of the overall maintenance process. The necessary knowledge is represented as structural knowledge (functional entities and their interdependencies) and as behavioural knowledge. The inference engine uses this knowledge for abductive and deductive reasoning. Experiences with a prototypical application (modelling the BERKOM network) are described and evaluated. In the light of these experiences new concepts and improvements are discussed.	abductive reasoning;entity;inference engine;interdependence;model-based reasoning;modeling language	Walter Kehl;Heiner Hopfmüller;Traytcho Koussev;Mark A. Newstead	1992		10.1007/BFb0024958	knowledge representation and reasoning;opportunistic reasoning;qualitative reasoning;computer science;knowledge management;artificial intelligence;model-based reasoning;management science;reasoning system	AI	-27.5924094848009	-6.118134624920221	60313
47f9fff517fa1483cecda965ddbf675020cf389b	conceptual graphs for corporate knowledge repositories	corporate memory;representacion conocimientos;red semantica;information technology;semantic network;reseau semantique;comparative modeling;conceptual graph;grafo conceptual;knowledge representation;representation connaissances;organization development;graphe conceptuel	The challenge companies will have to meet when making the leap from the industrial era to the knowledge era is the memorization of corporate knowledge and its dissemination to employees throughout the organization. Developing a corporate memory is the means chosen by DMR Consulting Group to capitalize on and manage its expertise in information technology. This paper presents a study conducted to choose a formalism to represent the know-how and methodologies - processes, techniques and learning materials - in corporate memory. It compares modeling formalisms against specific requirements and demonstrates that conceptual graphs are well suited to implement corporate memories. More specifically, we show that conceptual graphs support: (i) classification and partial knowledge, (ii) category or instance in relationship and (iii) category or instance in metamodel.	conceptual graph;information repository	Olivier Gerbé	1997		10.1007/BFb0027892	conceptual graph;knowledge representation and reasoning;homology modeling;computer science;knowledge management;artificial intelligence;machine learning;data mining;database;mathematics;distributed computing;semantic network;information technology;computer security;organization development;algorithm	NLP	-32.60007036027728	-5.620490927818863	60337
8f38b25e84f5d1d162359452c8ef9369414551ad	an advice system for consumer’s law disputes	legal system;electronic commerce;multi agent system;information technology;alternative dispute resolution;artificial intelligent;online dispute resolution;conflict resolution	The development of electronic commerce in the last years results in a new type of trade which the traditional legal systems are not ready to deal with. Moreover, the number of consumer claims increased mainly due to the increase in B2C relations and many of these are not getting a satisfactory response. Having this in mind, together with the slowness of the judicial system and the cost/beneficial relation in legal procedures, there is the need for new suited and effective approaches. In this paper we use Information Technologies and Artificial Intelligence to point out to an alternative way of solving these conflicts online. The work described in this paper results in a consumer advice system aimed at fastening and making easier the conflict resolution process, both for consumers and for legal experts.	artificial intelligence;case-based reasoning;e-commerce;emoticon	Nuno Manuel Silva Costa;Davide Carneiro;Paulo Novais;Diovana Barbieri;Francisco José Cavalcante Andrade	2010		10.1007/978-3-642-19802-1_17	e-commerce;online dispute resolution;computer science;knowledge management;conflict resolution;dispute resolution;dispute mechanism;alternative dispute resolution;law;information technology;computer security	AI	-21.723160193456767	-5.941815558880335	60361
964c8b1f9159a8425aeaddf93358ba7a611454b1	formal specification of information systems requirements	formal specification;information systems;modele mathematique;interaction;design requirements;information needs;conception;modelo matematico;mathematical models;online systems;information processing;mathematical model;design;computer software;information system;systeme information;systems development	An organizational information system can be viewed, in a broad sense, as a computational model of organizational activity and function. Logical requirements of information systems are defined. according to this view, as constraints imposed by the logical structure of such a computational model on the design, implementation and operation of information systems. In this study a formal model for the specification of logical requirements is presented in detail, and its role as a tool for systems development is discussed. Three main advantages of the specification of logical requirements that can be capitalized through the use of this formalism are: (1) logical requirements are invariant with respect to specific means in which information processing is actually accomplished: (2) they can be easily related to relevant aspects of organizational activity and function; and (3) they can be related to basic components of the corresponding software systems, either existing or being devel-	computation;computational model;concurrency (computer science);database trigger;formal language;formal specification;formal system;high- and low-level;information processing;information system;knowledge representation and reasoning;norm (social);parallel computing;requirement;semantics (computer science);software development process;software system	Roberto R. Kampfner	1985	Inf. Process. Manage.	10.1016/0306-4573(85)90086-X	software requirements specification;specification language;information processing;computer science;theoretical computer science;system requirements specification;mathematical model;information system;statistics	SE	-25.098771653211827	-6.959403028673729	60770
2452d970f94e228be7ba1083dac6288a449bb996	enhanced vector space models for content-based recommender systems	high dimensionality;vector space model;large dataset;information retrieval;information filtering;vector space;content based recommender system;personalization;vector space models;recommender system;indexation;scientific communication;experimental evaluation;open source	The use of Vector Space Models (VSM) in the area of Information Retrieval is an established practice within the scientific community. The reason is twofold: first, its very clean and solid formalism allows us to represent objects in a vector space and to perform calculations on them. On the other hand, as proved by many contributions, its simplicity does not hurt the effectiveness of the model. Although Information Retrieval and Information Filtering undoubtedly represent two related research areas, the use of VSM in Information Filtering is much less analzyed.  The goal of this work is to investigate the impact of vector space models in the Information Filtering area. Specifically, I will introduce two approaches: the first one, based on a technique called Random Indexing, reduces the impact of two classical VSM problems, this is to say its high dimensionality and the inability to manage the semantics of documents. The second extends the previous one by integrating a negation operator implemented in the Semantic Vectors1 open-source package. The results emerged from an experimental evaluation performed on a large dataset and the applicative scenarios opened by these approaches confirmed the effectiveness of the model and induced to investigate more these techniques.	applicative programming language;information filtering system;information retrieval;open-source software;random indexing;recommender system;semantics (computer science);viable system model	Cataldo Musto	2010		10.1145/1864708.1864791	vector space;computer science;information filtering system;machine learning;data mining;personalization;world wide web;vector space model;information retrieval;recommender system	Web+IR	-32.158524161627355	4.023373205805326	61031
69e9b0a09f4b3b9c3049b0e22261c05ea1c8eae0	supporting fast search in time series for movement patterns in multiple scales	heterogeneous information access;time series;networked information retrieval;multiple scales;large scale;movement pattern;database systems;mediators	"""An important investigation of time series involves searching for \movement"""" patterns, such as \going up"""" or \going down"""" or some combinations of them. Movement patterns can be in various scales: a large scale pattern may cover a long time period, while a small scale pattern usually covers a short time period. This paper considers such scale requirement. More speciically, a pattern is deened as a regular expression of letters, where each letter describes a movement direction and covers a speciied length of time (called pattern unit length). To nd if a time series (or a part of it) matches a pattern, the time series is rst partitioned into consecutive sub-series of the unit length, and for each sub-series, the direction of its best tting line is taken as the movement direction of the sub-series if the distance between the best tting line and the sub-series is within a speciied tolerance (tolerance requirement). A direct implementation of pattern search will undoubtedly yield poor performance if the number of time series or the length of them is large. This paper introduces a pre-computation and indexing method to facilitate fast evaluation of pattern queries in user-speciied scales. An eecient pre-computation algorithm is given to nd the movement directions for all the sub-series that satisfy the tolerance requirement. Bounding triangles are used to represent clusters of sub-series. Relational database is then used to store these bounding triangles and relational operations are employed to facilitate the evaluation of pattern queries. The paper also reports some experiments performed on a real-life data set to show the eeciency and the scalability of the algorithms. 1 Introduction Time series data are important for many application domains ranging from nancial to scientiic applications. For example, trading prices of a stock market can be interesting to a nancial application and temperatures of the Earth's atmosphere may be crucial in an Earth science application."""	algorithm;computation;experiment;gradient;pattern search (optimization);precomputation;real life;regular expression;relational database;scalability;time series	Yunyao Qu;Changzhou Wang;Xiaoyang Sean Wang	1998		10.1145/288627.288664	computer science;time series;data mining;database;information retrieval;statistics	DB	-27.28838502810222	0.27250395058235044	61102
64c0d377a51ea9b5db2ab35dd0463889e6881b9e	a framework to benchmark nosql data stores for large-scale model persistence		We present a framework and methodology to benchmark NoSQL stores for large scale model persistence. NoSQL technologies potentially improve performance of some applications and provide schema-less data-structures, so are particularly suited to persisting large and heterogeneous models. Recent studies consider only a narrow set of NoSQL stores for large scale modelling. Benchmarking many technologies requires substantial effort due to the disparate interface each store provides. Our experiments compare a broad range of NoSQL stores in terms of processor time and disc space used. The framework and methodology is evaluated through a case study that involves persisting large reverse-engineered models of open source projects. The results give tool engineers and practitioners a basis for selecting a store to persist large models.	benchmark (computing);nosql;persistence (computer science)	Seyyed M. Shah;Ran Wei;Dimitrios S. Kolovos;Louis M. Rose;Richard F. Paige;Konstantinos Barmpis	2014		10.1007/978-3-319-11653-2_36	data mining;database;world wide web	ML	-33.332720763854155	0.3476346511625785	61956
486875ce588d9d28b1f097817ebd5bdad7178acf	a knowledge engineering approach to natural language understanding	computer system;knowledge engineering approach;current capability;semantic network;rule-based system;linguistic knowledge;natural language understanding;computer response;knowledge storage;natural language;knowledge base;programming language;rule based;artificial intelligent;knowledge engineering;rule based system	This paper describes the results of a preliminary study of a Knowledge Engineering approach to Natural Language Understanding. A computer system is being developed to handle the acquisition, representation, and use of linguistic knowledge. The computer system is rule-based and utilizes a semantic network for knowledge storage and representation. In order to facilitate the interaction between user and system, input of linguistic knowledge and computer responses are in natural language. Knowledge of various types can be entered and utilized: syntactic and semantic; assertions and rules. The inference tracing facility is also being developed as a part of the rule-based system with output in natural language. A detailed example is presented to illustrate the current capabilities and features of the system.	computer;knowledge engineering;logic programming;natural language understanding;rule-based system;semantic network	Stuart C. Shapiro;Jeannette G. Neal	1982			rule-based system;natural language processing;language identification;knowledge representation and reasoning;natural language programming;semantic computing;multinet;universal networking language;language primitive;question answering;object language;natural language user interface;computer science;artificial intelligence;mathematical knowledge management;knowledge-based systems;machine learning;open knowledge base connectivity;linguistics;modeling language;natural language;semantic network;commonsense knowledge;language technology;high-level programming language;domain knowledge	AI	-29.33084562456678	-6.238359245959525	62098
8b95c9f425af7b436439d1d220b6f800a8b7f4ab	a peer-to-peer architecture for cloud based data cubes allocation		Large amounts of data are generated daily, according to the wide usage of social media websites and scientific data. These data need to be stored and analyzed to help decision makers but the traditional database concepts are insufficient. Data warehouse and OLAP are useful technologies in the storage and analysis of big data. Using MapReduce will help to save processing time, using cloud computing will help in saving resources and storage. In this paper, we propose a system that integrates the OLAP and MapReduce over cloud (considering workload balance) in order to enhance the performance of query processing over big data. The proposed system is applied to large amounts of data stored in cubes located in a Peer-to-peer cloud; this process is done using an allocation approach to save resources and query processing times. The proposed system achieves enhancements as time saving in query processing and in resources usage.	cubes;peer-to-peer	Mohammed Ezzat;Rasha M. Ismail;Nagwa Lotfy Badr;Mohamed F. Tolba	2015		10.1007/978-3-319-26690-9_35	parallel computing;operating system	DB	-31.918446809188065	-1.1199074784799696	62312
a3ad070de58130035fb0d1d876ed9ee93e7e93ca	an integrated case-based reasoning and mcdm system for web based tourism destination planning	analytic hierarchy process;decision support;case base reasoning;multi criteria decision making;tourism destination planning;case based reasoning;web based framework	The accelerating interaction between technology and tourism has changed radically the efficiency and effectiveness of tourism organizations, as well as how consumers interact with organizations. In this study, a Web based intelligent framework for travel agencies is proposed that offers customers a fast and reliable response service in a less costly manner. The proposed framework integrates case-based reasoning (CBR) system with a well-known multi criteria decision making (MCDM) technique, namely Analytic Hierarchy Process, to enhance the accuracy and speed in case matching in tourism destination planning. The integration of two techniques enables taking advantages of their strengths and complements each other's weaknesses. A case study is performed to demonstrate how this framework can facilitate intelligent decision support by retrieving best-fitted responses for customers.	case-based reasoning	Gülfem Isiklar Alptekin;Gülçin Büyüközkan	2011	Expert Syst. Appl.	10.1016/j.eswa.2010.07.153	case-based reasoning;analytic hierarchy process;decision support system;computer science;knowledge management;artificial intelligence;management science	AI	-32.30240153598696	-8.78539559218222	62450
63f540cfc68ae264c842e4a6eb8bf701313a7a94	meta-case-based reasoning: using functional models to adapt case-based agents	raisonnement base sur cas;razonamiento fundado sobre caso;case base reasoning;numerical technique;mataanalisis;reinforcement learning;agent logiciel;software agents;metaanalysis;apprentissage renforce;agent intelligent;metaanalyse;relative efficiency;intelligent agent;functional model;agente inteligente;case based reasoning;learning artificial intelligence;aprendizaje reforzado;intelligent software agent;apprentissage intelligence artificielle	It is useful for an intelligent software agent to be able to adapt to new demands from an environment. Such adaptation can be viewed as a redesign problem; an agent has some original functionality but the environment demands an agent with a slightly different functionality, so the agent redesigns itself. It is possible to take a case-based approach to this redesign task. Furthermore, one class of agents which can be amenable to redesign of this sort is case-based reasoners. These facts suggest the notion of “meta-case-based reasoning,” i.e., the application of case-based redesign techniques to the problem of adapting a case-based reasoning process. Of course, meta-case-based reasoning is a very broad topic. In this paper we focus on a more specific issue within meta-casebased reasoning: balancing the use of relatively efficient but knowledge intensive symbolic techniques with relatively flexible but computationally costly numerical techniques. In particular, we propose a mechanism whereby qualitative functional models are used to efficiently propose a set of design alternatives to specific elements within a meta-case and then reinforcement learning is used to select among these alternatives. We describe an experiment in which this mechanism is applied to a casebased disassembly agent. The results of this experiment show that the combination of model-based adaptation and reinforcement learning can address meta-case-based reasoning problems which are not effectively addressed by either approach in isolation.	case-based reasoning;disassembler;numerical analysis;reinforcement learning;semantic reasoner;software agent	J. William Murdock;Ashok K. Goel	2001		10.1007/3-540-44593-5_29	meta-analysis;simulation;computer science;knowledge management;artificial intelligence;machine learning;reinforcement learning;intelligent agent;algorithm	AI	-21.071940631195684	-6.0665653871490495	62556
81eed7bf7a198ecaeeec434382712d720afff02f	crib: computer fault-finding through knowledge engineering	mortar;expert systems;knowledge engineering expert systems diagnostic expert systems fault diagnosis system testing database systems indium tin oxide aggregates artificial intelligence mortar;indium tin oxide;aggregates;diagnostic expert systems;database systems;system testing;artificial intelligence;fault diagnosis;knowledge engineering	CRIB, or computer retrieval incidence bank, is one of a small number of expert systems 1,2 designed for computer fault diagnosis. Although it possesses little of the structural or functional knowledge common in more advanced expert systems, this system is a highly flexible, userfriendly, and pattern-directed inference system that is adequate for both hardware and software fault diagnosis. In this article, CRIB is described in terms of its generation, or phases of development, to emphasize the importance of predevelopment and post-development system analysis in building a viable expert system. Three systems development phases (not dissimilar to the development phases of any commercial computer system) have been described:	computer;expert system;incidence matrix;inference engine;knowledge engineering;software development process;system analysis	Roger T. Hartley	1984	Computer	10.1109/MC.1984.1659085	mortar;indium tin oxide;computer science;artificial intelligence;knowledge engineering;data mining;system testing	AI	-28.93796794237829	-6.043968809351208	62617
e0a9a4d8b5fedc8bf49a6ffae25d823ee44a2b6d	a knowledge-based approach for automatic generation of summaries of behavior	informatica;representacion conocimientos;linguistique;base de connaissances;flood;rivers;behavioral analysis;surveillance;hydrologie;generacion automatica;automatic summarization;generation texte;inundacion;pertinencia;abstraction;resumen;dynamic system;intelligence artificielle;abstraccion;automatic generation;dynamical system;systeme dynamique;captador medida;physical characteristic;inondation;hidrologia;linguistica;measurement sensor;vigilancia;generation automatique;capteur mesure;monitoring;generacion texto;resume;rio;pertinence;analyse comportementale;riviere;inferencia;representation connaissance;hydrology;artificial intelligence;base conocimiento;analisis conductual;inteligencia artificial;monitorage;relevance;sistema dinamico;knowledge representation;text generation;monitoreo;abstract;inference;knowledge base;linguistics	Effective automatic summarization usually requires simulating human reasoning such as abstraction or relevance reasoning. In this paper we describe a solution for this type of reasoning in the particular case of surveillance of the behavior of a dynamic system using sensor data. The paper first presents the approach describing the required type of knowledge with a possible representation. This includes knowledge about the system structure, behavior, interpretation and saliency. Then, the paper shows the inference algorithm to produce a summarization tree based on the exploitation of the physical characteristics of the system. The paper illustrates how the method is used in the context of automatic generation of summaries of behavior in an application for basin surveillance in the presence of river floods.	algorithm;automatic summarization;dynamical system;interpretation (logic);knowledge-based systems;relevance;simulation	Martin Molina;Victor Flores	2006		10.1007/11861461_28	knowledge representation and reasoning;knowledge base;computer science;artificial intelligence;dynamical system;automatic summarization;algorithm	AI	-23.01539567924747	-3.7683118525718378	62668
f61a6c4cfeed94a28a861c12b7b250368a44a580	dymond: an active system for dynamic vertical partitioning of multimedia databases	technique evaluation and assessment;v2i;intelligent transportation systems;communication saving;data reduction techniques;multimedia database	In recent years, vertical partitioning techniques have been employed in multimedia databases to achieve efficient retrieval of multimedia objects. These techniques are static because the input to the partitioning process, which includes queries accessing database and their frequency as well as the database schema, is obtained from an earlier analysis stage. This implies that when the system undergoes sufficient changes, a new analysis stage is carried out to re-run the partitioning process. Multimedia databases are accessed by many users simultaneously, therefore queries and their frequency tend to quickly change over time. In this context, dynamic vertical partitioning can significantly improve performance. In this paper we present an active system called DYMOND (DYnamic Multimedia ON line Distribution), which performs a dynamic vertical partitioning in multimedia databases to improve query performance. Experimental results on benchmark multimedia databases clarify the validness of our system.	benchmark (computing);binary space partitioning;database schema;partition (database)	Lisbeth Rodríguez-Mazahua;Xiaoou Li;Jair Cervantes;Farid García	2012		10.1145/2351476.2351485	intelligent transportation system;real-time computing;computer science;data mining;database	DB	-26.73988387622614	0.1316055664828403	62671
6a6859d4bedf40addeb3ec40649dbb54d60ca888	intelligente systeme für den customer-support: fallbasiertes schließen in help-desk- und call-center-anwendungen.	systeme intelligent;raisonnement base cas;sistema inteligente;prise decision;raisonnement;case based reasonning;intelligent system;razonamiento;reasoning;toma decision;call center	Recently the case-based reasoning technology is often used in helpdesk, call center and self-help applications. In this paper we will show some scenarios for the use of CBR will first summarize the different application areas, describe the key topics and give some case studies of implemented and daily used CBR systems.		Stefan Wess	1996	Wirtschaftsinformatik		artificial intelligence;operations research;reason	EDA	-25.661616277945928	-5.5186363210817335	62675
cadacaa3b074d972934ca15710ff4f1fc421f067	indexing textual xml in p2p networks using distributed bloom filters	database system;bloom filter;p2p;indexing method;large scale;p2p network;indexation;xml;xml document;xquery text;p2p networks;information system;data structure	Nowadays P2P information systems can be considered as large scale databases where all peers can store and query data in the network. Keywords and structure indexes must be maintained. However, indexing XML documents with massive set of words brings out a major problem: The number of entries to be shipped in the network is huge.We define Distributed Bloom Filter, a data structure derived from Bloom Filters, a probabilistic data structure to test whether an element is member of a set, to summarize peer XML content and structure. Our strength is to split the traditional Bloom Filter into several segments. We rely on a DHT network to distribute these segments in a P2P network. Our measurements show that our indexing method is scalable for a large number of words, and outperforms similar methods.	bloom filter;xml	Clément Jamard;Georges Gardarin;Laurent Yeh	2007		10.1007/978-3-540-71703-4_93	xml validation;xml encryption;xml;data structure;streaming xml;computer science;document structure description;xml framework;data mining;xml database;xml schema;database;xml signature;world wide web;information retrieval;efficient xml interchange	DB	-29.460256758019703	-0.037069787848159536	62736
ac3caca624e2bc88010c1dc1dc3ff72afe73ecd6	comparison and performance analysis of join approach in mapreduce	analytics;hadoop;join process;mapreduce;performance	MapReduce framework has become a general programming model. MapReduce proved its superiority in fields like sorting, full-text searching. However, as demands become complicated, MapReduce could not directly support relational algebra, typically as join, on heterogeneous data source. We discusses the factors that influence the performance when implementing join both in map function and in reduce function. We also conduct implementation and make analysis. Experimental result shows that the first approach wins in situation that datasets involved in join have significant difference in size and one of them is small enough. In order to get advantages of the first approach, we conduct further discuss when the smaller dataset grows and improve it. © Springer-Verlag Berlin Heidelberg 2013.	mapreduce;profiling (computer programming)	Fuhui Wu;Qingbo Wu;Yusong Tan	2012		10.1007/978-3-642-35795-4_79	parallel computing;data mining;database	DB	-28.78610641403469	-1.3523569515880256	63171
7f8f05f57a1f315422704c9133f012f0ffbe0d63	graph-based data model for the content representation of multimedia data	arbre graphe;graph theory;analisis contenido;methode recursive;hipergrafico;expresion regular;appel procedure;teoria grafo;multimedia;tree graph;ingenierie connaissances;interrogation base donnee;metodo recursivo;interrogacion base datos;recursive method;intelligence artificielle;logical programming;theorie graphe;data model;multigraph;content analysis;llamada procedimiento;programmation logique;multigrafo;directed graph;multimedia data;graphe oriente;expression reguliere;artificial intelligence;grafo orientado;modele donnee;hypergraph;inteligencia artificial;analyse contenu;multigraphe;arbol grafo;programacion logica;procedure call;database query;regular expression;data models;hypergraphe;knowledge engineering	The contents of multimedia data has complex relationships including deeply nested whole-part and the many-to-many relationships. This paper proposes a data model incorporating the concepts of directed graphs, recursive graphs, and hypergraphs in order to represent the contents of multimedia data. In the proposed data model, an instance is represented with a directed recursive hypergraph called an instance graph. The logic-based operation called rewrite is introduced. It gives us powerful querying capability because regular expressions on paths can be specified in retrieving instance graphs from collection graphs.	data model	Teruhisa Hochin	2006		10.1007/11893004_150	pathwidth;graph product;computer science;theoretical computer science;database;modular decomposition;indifference graph;algorithm	DB	-22.692570937652942	4.106666574964136	63476
d5c79bf3d6967ab2736561fcba8e9f7292fbeb65	spkv: a multi-dimensional index system for large scale key-value stores		A number of key-value databases have emerged with the development of cloud computing, which provide the ability of large scale data storage, but they do not efficiently support the multi-dimensional range queries and kNN queries which are important in online applications. Thus, we introduce the Sliced Pyramid Index for Key-Value Stores (SPKV), an index system that bridges the gap between data scale and querying functionality for highly available and scalable distributed key-value databases. SPKV implements a distributed index system with an improved pyramid index scheme called SP-Index, which allows efficient multi-dimensional query processing. In our experiments, SPKV achieves dozens of times faster than other index systems for key-value databases.	attribute–value pair	Qi Wang;Hailong Sun;Yu Tang;Xudong Liu	2014		10.1007/978-3-319-11116-2_32	database;computer science;scalability;range query (data structures);pyramid;cloud computing;computer data storage	HPC	-30.234908970435033	0.9034837140814077	63497
52fd58cdd148448bd888a253ea013adcf077e30f	event ontology reasoning based on event class influence factors		An event ontology is a shared, objective and formal specification of an event class system model. In comparison with the conventional ontology, the event ontology represents knowledge with a higher granularity. For event classes, non-classified relations are more important such as cause-effect relation and follow relation, and there are certain associative strengths between them, called event class influence factor. In this paper, we propose a method of event ontology reasoning based on event class influence factors. After introducing event, event class and event ontology model, we illustrate some related theorems and inferences. The experimental results show that the method proposed has a certain significance for analyzing event scenarios and pre-warning disasters.		Zhaoman Zhong;Zong-tian Liu;Cunhua Li;Yan Guan	2012	Int. J. Machine Learning & Cybernetics	10.1007/s13042-011-0046-8	complex event processing;theoretical computer science;data mining;database;mathematics;event;event tree;event tree analysis;process ontology	AI	-19.74708042552518	1.491861524171893	63664
1a6a8375277ad8625e24d309e6e39b838f9e2cc3	agent transparency for human-agent teaming effectiveness	atmospheric measurements;uncertainty;impact human agent teaming situation awareness based agent transparency model sat model operator situation awareness agent reasoning information autonomous squad member;intelligent agents;cognition uncertainty automation robots intelligent agents decision making atmospheric measurements;robots;cognition;remotely operated vehicles military systems mobile robots;military agent transparency human agent teaming human robot interaction mixed initiative situation awareness unmanned vehicles;automation	We discuss the Situation awareness-based Agent Transparency (SAT) model and its three levels of information to support agent transparency and operator situation awareness of the agent: agent's current action or plan, agent's reasoning information, and agent's projections of future outcomes. Two studies -- Autonomous Squad Member and IMPACT -- are summarized to illustrate the utility of agent transparency for the overall human-agent team performance.	autonomous robot	Jessie Y. C. Chen;Michael J. Barnes	2015	2015 IEEE International Conference on Systems, Man, and Cybernetics	10.1109/SMC.2015.245	robot;simulation;cognition;uncertainty;computer science;knowledge management;artificial intelligence;autonomous agent;automation;intelligent agent	Robotics	-19.43937416499934	-9.448471426306735	63669
3811557e74cd9265d7010bd4293c2c534eb3ef1f	contextualized task modeling	sensibilidad contexto;arbre graphe;modelizacion;procedures and practices;context aware;tree graph;conduccion vehiculo;task model;conduite vehicule;vehicle driving;modelisation;multigraph;contextual graphs;prescribed and effective task;multigrafo;contexto;driver modelling;contexte;sensibilite contexte;multigraphe;arbol grafo;modeling;context	The distinction between what is planned and what is done is well known. One contrasts the prescribed task to the effective task, the logic of functioning to the logic of use, procedures to practices, etc. However, there is no real analysis of the differences because of the large variety of effective tasks for a given prescribed task. Indeed, the large number of effective tasks comes from the multitude of contexts in which one may accomplish the task. Indeed, there is a solution, thanks to a uniform representation of elements of reasoning and of contexts, called Contextual Graphs. We propose in this paper a notion of contextualized task model that is an operational intermediate between prescribed and effective tasks. Using such contextualized task models would lead to developing more robust procedures as shown in four applications.		Patrick Brézillon;Juliette Brézillon	2008	Revue d'Intelligence Artificielle	10.3166/ria.22.531-548	simulation;systems modeling;computer science;artificial intelligence;multigraph;task analysis;tree	NLP	-21.162082856962353	0.8857821908903486	63701
9d9c3570bc622626391441919790dee783f6748d	udekam: a methodology for acquiring declarative structured knowledge from unstructured knowledge resources	analytical models;biological system modeling;diabetes;natural languages;engines;guidelines;load modeling	An effective knowledge representation has always proved its importance for mankind intelligence. Among various kinds of knowledge, declarative knowledge has a vital role in medical domain and is critical for health-care safety and quality. A large volume of declarative knowledge is hidden in multiple knowledge resources such as clinical notes, standard guidelines etc. that can play an important role in decision support systems as well as in health and wellness applications after structured transformation. In this paper, an Unstructured Declarative Knowledge Acquisition Methodology, called UDeKAM, is proposed that acquires and constructs the declarative structured knowledge from unstructured knowledge resources using Documents Clustering, Topic Modeling, and Controlled Natural Language processing techniques. The proposed methodology is designed for different domains to serve a variety of applications. It is an ongoing work and for the realization of UDeKAM, a diabetes scenario is explained through example.	big data;controlled natural language;decision support system;declarative programming;knowledge acquisition;knowledge representation and reasoning;natural language processing;topic model	Maqbool Ali;Sungyoung Lee;Byeong Ho Kang	2016	2016 International Conference on Machine Learning and Cybernetics (ICMLC)	10.1109/ICMLC.2016.7860897	natural language processing;knowledge base;computer science;knowledge management;artificial intelligence;body of knowledge;knowledge-based systems;machine learning;knowledge engineering;open knowledge base connectivity;data mining;procedural knowledge;knowledge extraction;natural language;domain knowledge	AI	-33.60100805064839	-6.321835178094076	63938
844348b91c78f8a50acbc5929d62a33f9cab96bc	cognitive model of memory for mechanical-design problems	concepcion asistida;memoire;computer aided design;raisonnement base cas;concepcion sistema;red semantica;semantic network;base connaissance;stockage donnee;construction mecanique;resolucion problema;mechanical engineering;case based reasonning;construccion mecanica;data storage;reseau semantique;memoria;system design;conception assistee;almacenamiento datos;base conocimiento;sistema cognoscitivo;systeme cognitif;cognitive system;mechanism design;cognitive model;conception systeme;memory;problem solving;resolution probleme;knowledge base	Mechanical-design activities can be categor&ed into two classes: creating new designs for new problems, and modifying old designs to fi t new problems. The vast majority of mechanical-design activities can be associated with the latter class. In most cases, it is more effective to modify the design process that creates a mechanical artifact than it is to modify the mechanical artifact itself. A scheme for design automation that uses analogical problem solving as its intelligent agent offers an effective method for solving both classes of mechanical design. It is particularly suited to the latter class of design activities. It relies heavily on a knowledge base for storing design cases generated while solving design problems, and retrieving design cases that are applicable in a new design-problem context. In the paper, a cognitive model of memory for storing design plans is presented. The memory model is four layers deep: product design plans, assembly design plans, component design plans, and recurring-engineering-problem design plans. The storage and retrieval mechanism is based on some of the more popular work found in case-based reasoning. To alleviate the situation in which the user(s) operate(s) in a restricted vocabulary set, a semantic network is integrated into the memory model for use as an elaboration and crossreference mechanism. Mechanical design plans for products, assemblies, mechanical components and recurring engineering problems can be stored and retrieved from the memory model using information found in the description of the design problem. Sample examples are presented to demonstrate the potential of the memory model. mechanical design, case-based reasoning, analogical problem solving, design plans, memory models, knowledge bases, semantic networks, content frames, case selection, natural language, retrieval, reminding, storage, elaboration, plausible index generation, index fitting, products, assembly, components, gears, rollers		Theodore Bardasz;Ibrahim Zeid	1992	Computer-Aided Design	10.1016/0010-4485(92)90050-K	mechanism design;cognitive model;knowledge base;simulation;probabilistic design;computer science;engineering;artificial intelligence;computer aided design;computer data storage;semantic network;memory;algorithm;systems design;mechanical engineering	AI	-24.381680875804353	-4.503620045412393	63985
8e09e5edc6640f392f6ec59c18a37573647c1d67	architecting a dimensional document warehouse	search engines data warehouses information retrieval;data cube;search engines;information retrieval;search engines data warehouses information retrieval writing law legal factors buildings data analysis file servers voice mail;document retrieval;data warehouses;data warehouse;data cube dimensional document warehouse document retrieval document storage search engine	This paper examines how the concepts of dimensional data warehouses can be applied to document retrieval and storage. It then shows how the specifics of dimensional document warehouses differ from dimensional data warehouse and how these differences make it impractical to use existing engines for building and analyzing data cubes (such as SQL server's analysis manager) in order to build and analyze a document warehouse. The paper further shows that readily available software can be used to build an engine to analyze a dimensional document mart. All of the steps required to design, build, and analyze a dimensional document mart are described and illustrated. Design features are suggested for improving the recall and precision of searches from dimensional document marts	data cube;data mart;document retrieval;microsoft sql server;precision and recall;server (computing)	Gregory Schymik;Karen Corral;David Schuff;Robert D. St. Louis	2007	2007 40th Annual Hawaii International Conference on System Sciences (HICSS'07)	10.1109/HICSS.2007.85	document retrieval;document clustering;dimensional modeling;computer science;data warehouse;data mining;database;world wide web;information retrieval;data cube;design document listing	DB	-33.359909094106854	1.5357251351880183	64314
4f5a3540177f79d2171a1f0a2cd222b01228b7a9	a scalable data structure for real-time estimation of resource availability in build-to-order environments	build to order;mass customization;resource planning;real time;index structure;simulation experiment;due date assignment;large scale;indexation;tree structure;resource availability;efficient estimation;index data structures;empirical evaluation;data structure;random access;binary tree	This paper defines a highly scalable interval index structure called the Temporal Bin tree (TB-tree) that can be embedded in any resource planning application whose algorithms require efficiently estimating either the time that a resource will be available to process a specific task of known length or the net availability of a resource during a specified period of time. It is specifically engineered to meet the real-time response and space efficiency requirements of large-scale resource planning applications that are required for mass customization. Basically, the TB-tree is a binary tree structure that represents availability of a resource across a planning horizon. Representing intervals of availability hierarchically using a tree structure increases the efficiency of search for resource availability when the discretization of time is fine-grained or the planning horizon is long. The tree forms a backbone structure that does not require disruptive rebalancing during update operations, which would mitigate the ability of the tree to respond to queries in real time. Its specific implementation allows for random access at any level of the tree to further improve scalability. An application of planning to real-time promising of order due dates for custom built products provides the context for empirical evaluation. Results of analytical evaluations and simulation experiments clearly demonstrate the scalability of the TB-tree relative to existing index structures in terms of both time and space.	data structure;real-time clock;scalability	Scott A. Moses;Le Gruenwald;Khushru Dadachanji	2008	J. Intelligent Manufacturing	10.1007/s10845-008-0130-4	mathematical optimization;build to order;real-time computing;simulation;data structure;binary tree;mass customization;computer science;tree structure;random access	Robotics	-27.264831161822702	-0.22862734696936413	64322
b5f39a19876943d96d30b73fb781cf9ed3f32076	faster and smaller two-level index for network-based trajectories		Two-level indexes have been widely used to handle trajectories of moving objects that are constrained to a network. The top-level of these indexes handles the spatial dimension, whereas the bottom level handles the temporal dimension. The latter turns out to be an instance of the interval-intersection problem, but it has been tackled by non-specialized spatial indexes. In this work, we propose the use of a compact data structure on the bottom level of these indexes. Our experimental evaluation shows that our approach is both faster and smaller than existing solutions.		Rodrigo Rivera;M. Andrea Rodríguez;Diego Seco	2018		10.1007/978-3-030-00479-8_28	discrete mathematics;theoretical computer science;search engine indexing;mathematics;data structure	DB	-26.969938553180377	0.4667464993163827	64372
54972bc1158e97c9567c1bbc3ba4db49158b4212	explaining evidential analyses	evidential analysis;expert systems;sensitivity analysis;belief functions;evidential reasoning;dempster-shafer theory;reasoning under uncertainty;explanation generation;dempster shafer theory	Abstract One of the most highly touted virtues of knowledge-based expert systems is their ability to construct explanations for their lines of reasoning. However, there is a basic difficulty in generating explanations in expert systems that reason under uncertainty using numeric measures. In particular, systems based on evidential reasoning using the theory of belief functions have lacked all but the most rudimentary facilities for explaining their conclusions. Int this paper we review the process whereby other expert system technologies produce explanations, and present a methodology for augmenting an evidential reasoning system with a versatile explanation facility. The method, which is based on sensitivity analysis, has been implemented, and several examples of its use are described.		Thomas M. Strat;John D. Lowrance	1989	Int. J. Approx. Reasoning	10.1016/0888-613X(89)90020-0	legal expert system;computer science;artificial intelligence;data mining;evidential reasoning approach	Logic	-20.157984951804945	-2.501057633606142	64407
74418aba2f474f3fefdd3c32a46d7a9b2e783bc6	highly scalable algorithm for distributed real-time text indexing	distributed algorithms;group based index construction;mpp architecture;time complexity;real time;ibm intranet data;construction industry;text analysis;financial services;distributed data structure;distributed real time text indexing algorithm;load imbalance reduction;storage class memory;indexing delay throughput algorithm design and analysis petascale computing data analysis data security surveillance parallel architectures real time systems;arrays;data analysis;large scale;time factors;parallel architectures;distributed data structures;indexing;parallel systems;computational complexity;blue gene l machine;data structures;text analysis computational complexity data analysis data structures distributed algorithms indexing merging parallel processing;indexation;distributed indexing algorithm;merging;security surveillance;stream computing research;parallel system;parallel architecture;stock trading;index data structures;data structure;high speed;merge process;text indexing;algorithm design and analysis;parallel processing;scalable algorithm;blue gene l machine scalable algorithm stream computing research data analysis decision making security surveillance financial services stock trading parallel architectures storage class memory distributed data structures distributed real time text indexing algorithm parallel system group based index construction index data structures load imbalance reduction merge process asymptotic parallel time complexity distributed indexing algorithm mpp architecture ibm intranet data;asymptotic parallel time complexity;throughput	Stream computing research is moving from terascale to petascale levels. It aims to rapidly analyze data as it streams in from many sources and make decisions with high speed and accuracy in fields as diverse as security surveillance and financial services including stock trading. We specifically consider real-time text indexing and search with high input data rates (10 GB/s or more) along with small index age-off(expiry) time. This makes it necessary to have maximal indexing rates for large volumes of data as well as minimal latency for indexing (time between start of indexing for a document and its availability for search) while maintaining very-low search response time. In addition, future massively parallel architectures with storage class memories will enable high speed in-memory real-time indexing, where index can be completely stored in a high capacity storage class memory. In this paper, we present the design of distributed data-structures and distributed real-time text indexing algorithm for parallel systems having large (thousands to hundred thousand) number of cores/processors, while simultaneously providing acceptable search performance [1]. The inherent trade-offs involved in index space, indexing throughput and search response time make this problem particularly challenging. Our algorithm uses group-based index construction and leverages novel index data structures that reduce load imbalance and make text indexing and merge process more scalable and efficient. We show analytically that the asymptotic parallel time complexity of our distributed indexing algorithm, is at least Ω(log(P)) factor better than typical indexing approaches, where P is the number of indexing nodes in a group. We further demonstrate the performance and scalability of our distributed indexing algorithm, on an MPP architecture (Blue Gene/L 1) using actual IBM intranet data. We achieved high indexing throughput of around 312 GB/min on an 8K node Blue Gene/L machine. In comparison with parallel indexing implemented using typical approaches like CLucene 2, this is 3x–7x better. To the best of our knowledge, this is the first published result on indexing throughput at such a large scale, with sustained search performance. We further show that our approach is scalable to 128K nodes, giving an estimated indexing throughput of 5 T B/min. We also achieved indexing latency that is around 10x better than typical indexing approaches.	8k resolution;algorithm;algorithm design;blue gene;c syntax;central processing unit;data structure;distributed algorithm;distributed web crawling;extrapolation;gigabyte;goodyear mpp;in-memory database;intranet;maxima and minima;maximal set;min/max kd-tree;multistage interconnection networks;petascale computing;real-time clock;real-time transcription;response time (technology);scalability;search algorithm;search engine indexing;sequential consistency;stream processing;supercomputer;terascale (microarchitecture);terabyte;throughput;time complexity	Ankur Narang;Vikas Agarwal;Monu Kedia;Vijay K. Garg	2009	2009 International Conference on High Performance Computing (HiPC)	10.1109/HIPC.2009.5433193	time complexity;algorithm design;search engine indexing;throughput;parallel computing;data structure;financial services;computer science;theoretical computer science;operating system;massively parallel;data mining;database;distributed computing;data analysis;programming language;computational complexity theory;algorithm	HPC	-28.896590391168683	2.1103296331675874	64476
22b2d09fb26d3d8ac38d5e4a495085922503a6a4	optimize first, buy later: analyzing metrics to ramp-up very large knowledge bases	benchmarking;ncbo;relational data;biomedical ontologies;semantic indexing;ontology evolution;large dataset;data distribution;scaling up;indexation;semantic web;large knowledge bases;weed management;knowledge base	As knowledge bases move into the landscape of larger ontologies and have terabytes of related data, we must work on optimizing the performance of our tools. We are easily tempted to buy bigger machines or to fill rooms with armies of little ones to address the scalability problem. Yet, careful analysis and evaluation of the characteristics of our data—using metrics—often leads to dramatic improvements in performance. Firstly, are current scalable systems scalable enough? We found that for large or deep ontologies (some as large as 500,000 classes) it is hard to say because benchmarks obscure the load-time costs for materialization. Therefore, to expose those costs, we have synthesized a set of more representative ontologies. Secondly, in designing for scalability, how do we manage knowledge over time? By optimizing for data distribution and ontology evolution, we have reduced the population time, including materialization, for the NCBO Resource Index, a knowledge base of 16.4 billion annotations linking 2.4 million terms from 200 ontologies to 3.5 million data elements, from one week to less than one hour for one of the large datasets on the same machine.	benchmark (computing);best, worst and average case;fill device;knowledge base;loader (computing);national center for biomedical ontology;ontology (information science);pubmed;ramp simulation software for modelling reliability, availability and maintainability;scalability;terabyte	Paea LePendu;Natalya Fridman Noy;Clement Jonquet;Paul R. Alexander;Nigam Haresh Shah;Mark A. Musen	2010		10.1007/978-3-642-17746-0_31	open biomedical ontologies;knowledge base;relational database;computer science;artificial intelligence;data science;weed control;semantic web;data mining;database;world wide web;benchmarking	Web+IR	-31.37693236661156	-0.5159065699613976	64753
0d92267fc3838ed6082ab40e4490a83ea48ef164	a distributed approach to detect outliers in very large data sets	distributed data;time scale;large data sets;outlier detection;parallel computer	We propose a distributed approach addressing the problem of distance-based outlier detection in very large data sets. The presented algorithm is based on the concept of outlier detection solving set ([1]), which is a small subset of the data set that can be provably used for predicting novel outliers. The algorithm exploits parallel computation in order to meet two basic needs: (i) the reduction of the run time with respect to the centralized version and (ii) the ability to deal with distributed data sets. The former goal is achieved by decomposing the overall computation into cooperating parallel tasks. Other than preserving the correctness of the result, the proposed schema exhibited excellent performances. As a matter of fact, experimental results showed that the run time scales up with respect to the number of nodes. The latter goal is accomplished through executing each of these parallel tasks only on a portion of the entire data set, so that the proposed algorithm is suitable to be used over distributed data sets. Importantly, while solving the distance-based outlier detection task in the distributed scenario, our method computes an outlier detection solving set of the overall data set of the same quality as that computed by the corresponding centralized method.		Fabrizio Angiulli;Stefano Basta;Stefano Lodi;Claudio Sartori	2010		10.1007/978-3-642-15277-1_32	anomaly detection;parallel computing;computer science;machine learning;data mining;database;distributed computing	ML	-24.26728588874027	2.473146019280321	65047
6ad10174f1a419abedcf647eb9c7897148946e3a	constructing a sensuous judgment system based on conceptual processing	lenguaje natural;representacion conocimientos;linguistica matematica;langage naturel;semantics;base connaissance;semantica;semantique;imperfect information;natural language;informacion imperfecta;linguistique mathematique;base conocimiento;computational linguistics;knowledge representation;representation connaissances;information imparfaite;knowledge base	"""When we humans receive uncertain information, we interpret it suitably, to understand what the speaker is trying to say. This is possible because we have """"commonsense"""" concerning the word, which is built up from knowledge that is stored from long time experience. In order to understand the meaning, we think that the construction of a """"Commonsense Judgment System"""" is necessary. Of the commonsense we use in our every day lives, one concerns the characteristics of words, such as an apple is red. This paper proposes a mechanism to associate the characteristics of a word based on our five senses with a knowledge base consisting of basic words. By using a concept-base, which is automatically constructed from dictionaries, this mechanism can understand a word that does not exist in the knowledge base. This study aims to retrieve the meaning concerning sense, and deepen semantic understanding."""		Atsushi Horiguchi;Seiji Tsuchiya;Kazuhide Kojima;Hirokazu Watabe;Tsukasa Kawaoka	2002		10.1007/3-540-45715-1_7	natural language processing;knowledge base;commonsense reasoning;computer science;artificial intelligence;perfect information;computational linguistics;semantics;linguistics;natural language;commonsense knowledge;algorithm	NLP	-24.14346114015697	-2.614349823765517	65178
028625196200ba0525cbf9b580323bd3e0c125be	definitions of line-line relations for geographic databases		The management of spatial data in geographic information systems (GISs) gained increasing importance during the last decade. Due to the high complexity of objects and queries and also due to extremely large data volumes, geographic database systems impose stringent requirements on their storage and access architecture with respect to efficient spatial query processing. Geographic database systems are used in very different application environments. Therefore, it is not possible to find a compact set of operations fulfilling all requirements of geographic applications. But as described in [BHKS 93], spatial selections are of great importance within the set of spatial queries and operations. They do not only represent an own query class, but also serve as a very important basis for the operations such as the nearest neighbor query and the spatial join. Therefore, an efficient implementation of spatial selections is an important requirement for good overall performance of the complete geographic database system. The two main representatives of spatial selections are the point and the window query (see figure 1): • Point query: Given a query point P and a set of objects M, the point query yields all the objects of M geometrically containing P . • Window query: Given a rectilinear rectangle W and a set of objects M, the window query yields all the objects of M sharing points with W. For the efficient processing of spatial queries, we present a multi-stepprocedure (see figure 5). The main goal of our spatial query processor is to reduce expensive steps by preprocessing operations in the preceding steps which reduce the number of objects investigated in an expensive step. In figure 5 expensive steps are marked with a “$”symbol.	geographic information system;preprocessor;query string;regular grid;requirement;spatial database;spatial query;window function	Max J. Egenhofer	1993	IEEE Data Eng. Bull.		line (geometry);database;data mining;computer science	DB	-27.25799294858401	2.1269255446814785	65359
5844383df192553e0173cc027f502963a1614f94	a structure for modern computer narratives	symbolic computation;narrative;image processing;generic model;narration;speech processing;tratamiento palabra;procesamiento imagen;traitement parole;structure ordinateur;traitement image;calculo simbolico;narracion;pattern recognition;jeu ordinateur;reconnaissance forme;reconocimiento patron;computer structure;computer games;calcul symbolique;computer game;symbolic representation;estructura computadora	In order to analyze or better develop modern computer games it is critical to have an appropriate representation framework. In this paper a symbolic representation of modern computer narratives is described, and related to a general model of operational behaviour. The resulting structure can then be used to verify desirable properties, or as the basis for a narrative development system.	directed acyclic graph;pc game;petri net;relevance;semantics (computer science)	Clark Verbrugge	2002		10.1007/978-3-540-40031-8_21	symbolic computation;simulation;image processing;computer science;artificial intelligence;speech processing;narrative;algorithm	ML	-25.120523352279783	-9.056601034203322	65646
06321ea2206c81c806545108f75a23e6ea3b9bdb	pastrystrings: a comprehensive content-based publish/subscribe dht network	information systems;routing;intrusion detection;computer networks;network topology;equal opportunities;publish subscribe;informatics;information system;peer to peer computing;routing peer to peer computing computer networks large scale systems intrusion detection informatics information systems buildings equal opportunities network topology;buildings;large scale systems	In this work we propose and develop a comprehensive infrastructure, coined PastryStrings, for supporting rich queries on both numerical (with range, and comparison predicates) and string attributes, (accommodating equality, prefix, suffix, and containment predicates) over DHT networks utilising prefix-based routing. As event-based, publish/ subscribe information systems are a champion application class, we formulate our solution in terms of this environment.	distributed hash table;information system;numerical analysis;publish–subscribe pattern;routing	Ioannis Aekaterinidis;Peter Triantafillou	2006	26th IEEE International Conference on Distributed Computing Systems (ICDCS'06)	10.1109/ICDCS.2006.63	computer science;database;distributed computing;world wide web;information system;computer network	DB	-29.62438794880003	-0.3450913741475593	66351
6bda06c2971ddf671a0ef1540a8b8e8b7e48a5ee	on the configuration of the similarity search data structure d-index for high dimensional objects	high dimensionality;information retrieval;digital library;spatial index;indexation;access method;data structure;similarity search;multimedia system	Among similarity search indexes, the D-index introduced by Gennaro et al. in 2001 is regarded as an efficient metric access method. The performance of this index depends on several parameters, and their optimal configuration remains an open problem. We study two performance issues that occur when the D-index handles high dimensional objects. To solve these problems, we introduce an optimization that simplifies the D-index. By doing this, we remove two configuration parameters and improve performance.	search data structure;similarity search	Arnoldo José Müller Molina;Takeshi Shinohara	2010		10.1007/978-3-642-12179-1_37	digital library;data structure;computer science;data mining;database;programming language;access method;spatial database;information retrieval	DB	-27.720102367014743	2.4578670263096893	66498
1093f4dcdde5d4e67dd156721d7cc1b496815cc5	common knowledge sharing model of 24-hour knowledge factory of grid computing based on case based reasoning	case base reasoning;common knowledge;knowledge transfer;cbr;knowledge diffusion;case based reasoning;24 hour knowledge factory;grid computing	In order to improve the level of decision making and competitive advantage, organizations try to learn and develop new knowledge management techniques that are suited for the evolving global economy. Decision makers are becoming increasingly faced with a dilemma: great difficulty arises in sharing knowledge where distributed and heterogeneous data sources are so rich in terms of their information content. In order to address the problem of knowledge sharing, this article proposes a common knowledge sharing modelâ€” the 24-hour knowledge factoryâ€”of grid computing founded on case-based reasoning (CBR). This article begins with a description of the 24-hour knowledge factory, the enterprise common knowledge shared (ECKS) methodology and the time-shift sharing model. Next, the notion of a CBR-adapted approach for 24-hour knowledge factory, based on grid computing, is presented. Third, based on the ECKS concept, this article analyzes the use of the model described in this article in decision evolution and builds upon enterprise sharing structure ECK modeling, focusing on activity-work sharing, passive-work sharing and mix-work sharing. Multiple types of enterprise common knowledge transfer mechanisms are presented in this article.	case-based reasoning;grid computing	Huosong Xia;Amar Gupta	2008	IJKM	10.4018/jkm.2008070101	knowledge representation and reasoning;legal expert system;case-based reasoning;knowledge base;knowledge integration;computer science;knowledge management;data science;model-based reasoning;knowledge-based systems;open knowledge base connectivity;data mining;knowledge extraction;reasoning system;common knowledge;grid computing	AI	-31.662156757396737	-6.973065245531235	66646
dec0fe505b54b7c07779c2ce133eb1b5bc82e04a	an adaptative multi-agent system to co-construct an ontology from texts with an ontologist	intelligence artificielle;systeme multi agents	Ontologies are one of the most used representations to model the domain knowledge. An ontology consists of a set of concepts connected by semantic relations. The construction and evolution of an ontology are complex and time-consuming tasks. This paper presents DYNAMO-MAS, an Adaptive Multi-Agent System (AMAS) that automates these tasks by co-constructing an ontology from texts with an ontologist. Terms and concepts of a given domain are agentified and they act, according to the AMAS approach, by solving the non cooperative situations they locally perceive at runtime. These agents cooperate to determine their position in the AMAS (that is the ontology) thanks to (i) lexical relations between terms, (ii) some adaptive mechanisms enabling addition, removing or moving of new terms, of concepts and of relations in the ontology as well as (iii) feedbacks from the ontologist about the propositions given by the AMAS. This paper focuses on the instantiation of the AMAS approach to this difficult problem. It presents the architecture of DYNAMO-MAS, and details the cooperative behaviors of the two types of agents we defined for ontology evolution. Finally evaluations made on three different ontologies are given in order to show the genericity of our solution.	multi-agent system	Zied Sellami;Valérie Camps	2014	Trans. Computational Collective Intelligence	10.1007/978-3-662-44750-5_6	natural language processing;upper ontology;computer science;knowledge management;ontology;artificial intelligence;ontology-based data integration;process ontology	AI	-22.07041004086684	-9.306914318942312	67019
ef6a1c61bacae815328af534d8b0ceb343197cad	st-hadoop: a mapreduce framework for spatio-temporal data		This paper presents ST-Hadoop; the first full-fledged open-source MapReduce framework with a native support for spatio-temporal data. ST-Hadoop is a comprehensive extension to Hadoop and SpatialHadoop that injects spatio-temporal data awareness inside each of their layers, mainly, language, indexing, and operations layers. In the language layer, ST-Hadoop provides built in spatio-temporal data types and operations. In the indexing layer, ST-Hadoop spatiotemporally loads and divides data across computation nodes in Hadoop Distributed File System in a way that mimics spatio-temporal index structures, which result in achieving orders of magnitude better performance than Hadoop and SpatialHadoop when dealing with spatio-temporal data and queries. In the operations layer, ST-Hadoop shipped with support for two fundamental spatio-temporal queries, namely, spatio-temporal range and join queries. Extensibility of ST-Hadoop allows others to expand features and operations easily using similar approach described in the paper. Extensive experiments conducted on large-scale dataset of size 10 TB that contains over 1 Billion spatio-temporal records, to show that ST-Hadoop achieves orders of magnitude better performance than Hadoop and SpaitalHadoop when dealing with spatio-temporal data and operations. The key idea behind the performance gained in ST-Hadoop is its ability in indexing spatio-temporal data within Hadoop Distributed File System.	apache hadoop;mapreduce	Louai Alarabi;Mohamed F. Mokbel;Mashaal Musleh	2018	GeoInformatica	10.1007/s10707-018-0325-6	distributed file system;data type;temporal database;database;data mining;extensibility;search engine indexing;computer science;computation	DB	-31.138443535035407	0.7692858366659229	67260
70cf4aa3b7a3de50658dcb5cdc7c3867b559540e	making context aware decision from uncertain information in a smart home: a markov logic network approach	reasoning under uncertainty;sensing and reasoning technology;knowledge based systems	This research addresses the issue of building home automation systems reactive to voice for improved comfort and autonomy at home. The focus of this paper is on the context-aware decision process which uses a dedicated Markov Logic Network approach to benefit from the formal logical representation of domain knowledge as well as the ability to handle uncertain facts inferred from real sensor data. The approach has been experiemented in a real smart home with naive and users with special needs.	home automation;markov chain;markov logic network	Pedro Chahuara;François Portet;Michel Vacher	2013		10.1007/978-3-319-03647-2_6	computer science;knowledge management;artificial intelligence;knowledge-based systems;data mining	AI	-19.442888636689407	-2.731915135587798	67387
938870975a4f7ee912730f6298fc9d804da63742	what is the data warehousing problem? (are materialized views the answer?)	data warehousing;materialized views	A view is a derived relation defined in terms of base (stored) relations. A view can be materialized by storing the tuples of the view in the database. A materialized view provides fast access to data; the speed difference is critical in applications where the query rate is high and the views are complex or over data in remote databases, so that it is not feasible to recompute the view for every query.	database;materialized view	Ashish Gupta;Inderpal Singh Mumick	1996			materialized view;computer science;data warehouse;data mining;database	DB	-27.904389469944395	3.784669055094997	68067
08072a4313fb233e258041b2840c0766a70c9d50	using the possibility theory in fuzzy temporal reasoning	logica temporal;incertidumbre;uncertainty;temporal logic;logique floue;intelligence artificielle;logica borrosa;raisonnement;fuzzy logic;razonamiento;artificial intelligence;incertitude;inteligencia artificial;reasoning;logique temporelle;theorie possibilite		possibility theory	Jozef Sajda	1992	Kybernetika		fuzzy logic;uncertainty;qualitative reasoning;temporal logic;artificial intelligence;neuro-fuzzy;machine learning;mathematics;reasoning system;reason;algorithm	AI	-20.824856827281174	-1.7679822760911956	68380
18c377368ded4feb82eabab18f2a6e6258fb2e26	flexible response choice using problem-solving plans and rhetorical relations	rhetorics;lenguaje natural;reconocimiento lenguaje;linguistica matematica;reconnaissance langage;raw materials;langage naturel;intelligence artificielle;language recognition;rhetorique;retorica;natural language;artificial intelligence;linguistique mathematique;heuristic evaluation;computational linguistics;inteligencia artificial;information seeking;problem solving	In this paper we present an architecture for choosing a flexible response in a natural language system involved in information-seeking tasks. Our work considers the crucial issue of choosing what information to provide and how to structure it, considering from the generation perspective a model of dialogue that was previously developed to study the recognition activity of an agent. In such a model the underlying reasoning activity of an agent is represented by means of problem-solving plans, that manage domain and linguistic actions. The correct choice of the information to provide depends on the ability to select the action that best suits the domain situation and an answer that conveys the information in a way appropriate to the user. For this reason we provide heuristic evaluation criteria that consider both participants' goals and some other context-related factors impacting on the evaluation of the action to allow the problem-solving plans to make a choice among the available alternatives. Furthermore, we discuss how to convey the raw material in a suitable and understandable way by using different rhetorical relations. A detailed example illustrates how our approach models in a flexible way aspects of the interaction overlooked by previous systems.	common lisp;heuristic evaluation;information;library (computing);natural language;problem solving;workstation	Paolo Barboni;Dario Sestero	1997		10.1007/3-540-63576-9_97	computer science;artificial intelligence;computational linguistics;raw material;machine learning;mathematics;natural language;heuristic evaluation;computer security;algorithm	AI	-25.07798602356034	-7.690261593221361	68817
e95c98c57505ae310acc85f92083bac8daaef730	a multiple-layer clustering method for real-time decision support in a water distribution system		Machine learning provides a foundation for a new paradigm where the facilities of computing extend to the level of cognitive abilities in the form of decision support systems. In the area of water distribution systems, there is an increased demand in data processing capabilities as smart meters are being installed providing large amounts of data. In this paper, a method for multiple-layer data processing is defined for prioritizing pipe replacements in a water distribution system. The identified patterns provide relevant information for calculating the associated priorities as part of a real-time decision support system. A modular architecture provides insights at different levels and can be extended to form a network of networks. The proposed clustering method is compared to a single clustering of aggregated data in terms of the overall accuracy.		Alexandru Predescu;Catalin Negru;Mariana Mocanu;Ciprian Lupu;Antonio Candelieri	2018		10.1007/978-3-030-04849-5_42	architecture;knowledge management;cluster analysis;computer science;decision support system;machine learning;modular design;data processing;artificial intelligence;cognition	Robotics	-33.32319368634115	-7.856327235206498	69232
108de45cc1413a1729c693d8de7ea3527a6299c4	dima: a distributed in-memory similarity-based query processing system		Data analysts in industries spend more than 80% of time on data cleaning and integration in the whole process of data analytics due to data errors and inconsistencies. It calls for effective query processing techniques to tolerate the errors and inconsistencies. In this paper, we develop a distributed in-memory similarity-based query processing system called Dima. Dima supports two core similarity-based query operations, i.e., similarity search and similarity join. Dima extends the SQL programming interface for users to easily invoke these two operations in their data analysis jobs. To avoid expensive data transformation in a distributed environment, we design selectable signatures where two records approximately match if they share common signatures. More importantly, we can adaptively select the signatures to balance the workload. Dima builds signature-based global indexes and local indexes to support efficient similarity search and join. Since Spark is one of the widely adopted distributed inmemory computing systems, we have seamlessly integrated Dima into Spark and developed effective query optimization techniques in Spark. To the best of our knowledge, this is the first full-fledged distributed in-memory system that can support similarity-based query processing. We demonstrate our system in several scenarios, including entity matching, web table integration and query recommendation.	antivirus software;application programming interface;dima;in-memory database;job stream;mathematical optimization;query optimization;sql;similarity search;sputter cleaning	Ji Sun;Zeyuan Shang;Guoliang Li;Dong Deng;Zhifeng Bao	2017	PVLDB	10.14778/3137765.3137810	web search query;sql;database;data mining;dima;data analysis;computer science;nearest neighbor search;data structure;global information system;query optimization	DB	-31.749744314409874	0.08456970186508708	69878
ec0fd4311cb2e9b83b15f1648dace85debb154cf	temporal conditional preferences over sequences of objects	graph theory;graph based algorithm;data mining;transport system;outlier detection;k nearest neighbor;k nearest neighbor spatial outlier detection graph based algorithm;object detection meteorology image edge detection artificial intelligence computer science data mining transportation diseases military satellites petroleum;spatial outlier detection	Most research on preference elicitation, preference reasoning and preference query languages design focus mainly on preferences over single objects represented by relational tuples. An increasing interest on preferences over more complex structures like sets of objects has arised in recent papers. However, most recent applications deal with more sophisticated complex structured objects, like sequences, trees and graphs. In this paper, we introduce TPref, a formalism to reason with qualitative conditional preferences over sequences of objects indexed in time. TPref generalizes the CP-Nets formalism by allowing temporal conditional preferences besides the static rules used in CP-Nets. An algorithm for computing optimal sequences of objects satisfying a set of temporal constraints is also presented.	algorithm;cp/m;computation;consistency model;graph (discrete mathematics);preference elicitation;query language;relational database;semantics (computer science);temporal logic;tree (data structure)	Sandra de Amo;Arnaud Giacometti	2007	19th IEEE International Conference on Tools with Artificial Intelligence(ICTAI 2007)	10.1109/ICTAI.2007.169	local outlier factor;computer science;graph theory;machine learning;pattern recognition;data mining;k-nearest neighbors algorithm	Vision	-21.82615016304012	2.1349699305910317	69935
137fe30c505e155134c3552ba6c40ae619716fce	decision support systems for knowledge management		This article deal with the development of imitative and mathematical decision making models in knowledge management training systems based on fuzzy modeling and multi-agent technologies. These approaches enable the authors to solve a lot of problems with the help of classifier learning agents and classifier combinational agents connected by informative exchange procedures. Information management is conceived as a combination of systematic attainment, synthesis, knowledge exchange and use.	decision support system;knowledge management	Vladimir V. Kureichik;Y. A. Kravchenko;V. V. Bova	2015		10.1007/978-3-319-18473-9_13	clinical decision support system;r-cast;decision support system;intelligent decision support system;decision analysis;decision engineering;data management;knowledge management;knowledge engineering;management information systems;information management;personal knowledge management;evidential reasoning approach;information system;business decision mapping	DB	-31.05316036867149	-7.443016440666489	70108
145b46a0b1f8dc8f2182290655b9597fba087e78	towards a one size fits all database architecture		We propose a new type of database system coined OctopusDB. Our approach suggests a unified, one size fits all data processing architecture for OLTP, OLAP, streaming systems, and scan-oriented database systems. OctopusDB radically departs from existing architectures in the following way: it uses a logical event log as its primary storage structure. To make this approach efficient we introduce the concept of Storage Views (SV), i.e. secondary, alternative physical data representations covering all or subsets of the primary log. OctopusDB (1) allows us to use different types of SVs for different subsets of the data; and (2) eliminates the need to use different types of database systems for different applications. Thus, based on the workload, OctopusDB emulates different types of systems (row stores, column stores, streaming systems, and more importantly, any hybrid combination of these). This is a feature impossible to achieve with traditional DBMSs.	computer data storage;database;emulator;fits;online analytical processing;online transaction processing;systemverilog	Jens Dittrich;Alekh Jindal	2011			workload;online transaction processing;computer science;online analytical processing;database;architecture;data architecture;data processing	DB	-29.247449116918222	3.3176195923418756	70159
c1f0cbe80e51a8b0a0f95742b49e68752ba71028	case-based reasoning for breast cancer treatment decision helping	decision helping;raisonnement base sur cas;razonamiento fundado sobre caso;aplicacion medical;seno;decision aid;case base reasoning;tumor maligno;aide a la decision;cas regles;ayuda decision;satisfiability;breast;decision;sein;aide decision;traitement du cancer du sein;breast cancer treatment;medical application;tumeur maligne;case based reasoning;breast cancer;malignant tumor;rules as cases;rule based reasoning;application medicale	This paper presents two applications for the breast cancer treatment decision helping. The first one is called CASIMIR/RBR and can be likened to a rule-based reasoning system. In some situations, the application of the rules of this system does not provide a satisfying treatment. Then, the application CASIMIR/CBR-which is not fully implemented-can be used. CASIMIR/CBR uses principles of case-based reasoning in order to suggest solutions by adapting the rules of CASIMIR/RBR. In this framework, the rules are considered as cases: they are adapted rather than used literally.	case-based reasoning	Jean Lieber;Benoît Bresson	2000		10.1007/3-540-44527-7_16	rule-based system;case-based reasoning;computer science;artificial intelligence;breast cancer;algorithm	Logic	-25.506028583469707	-5.681764067958611	70352
1f1029825326c20f9d7f70eb3c90b3be876e6f3f	techniques for design and implementation of efficient spatial access methods	computer aided design;spatial data;hybrid method;design and implementation;point of view;access method;database management system;spatial access method	In order to handle spatial data efficiently, aa required in computer aided design and g-data ap plications, a database management system (DBMS) needs an access method that will help it retrieve data items quickly according to their spatial location. In this paper we present a classification of existing spatial access methods and show that they use one of the following three techniques: clipping, overlapping regions, and transformation. From a practical point of view we provide a tool box supporting simple design of a spatial access method for a given point access method using one of the above techniques. We analyze the technique of transformation in more detail and show that our new concept of asymmetric partitioning is more retrieval efficient than the traditional symmetric approach. Furthermore we suggest a hybrid method combining the techniques of overlapping regions and transformation and provide an analysis and comparison of our new method. For data for which an analysis of Rand R+-trees was available, these comparisons demonstrate a superiority of our scheme.	binary space partitioning;clipping (computer graphics);computer-aided design;rand tablet;rand index;spatial database	Bernhard Seeger;Hans-Peter Kriegel	1988			computer science;theoretical computer science;computer aided design;data mining;database;spatial analysis;access method	DB	-26.754158881043832	2.2713339139601754	70522
2bfb3c7027373c8f2e3f3e2689ea5d8e556f58fe	the idiotypic network with binary patterns matching	modelizacion;sistema experto;sistema inmunitario;rule based;base connaissance;intelligence artificielle;modelisation;stimulation;pattern matching;immune system;idiotype;artificial intelligence;base conocimiento;estimulacion;inteligencia artificial;concordance forme;systeme expert;idiotipo;networked systems;modeling;systeme immunitaire;knowledge base;expert system	Outline of the presentation 1.Introduction 2.Rules of interaction in the net 3.Evaluation of a new concentration level 4.Affinity measures and transformation T operator 5.Applied measures for evaluation of the results 6.Results of simulations 7.Conclusions 3 Introduction ● The model represents a network which consists of a set of antibodies (objects in a binary shape space) and rules of relationship between them, ● every object consists of a paratope and an epitope, ● every object represents a set of antibodies with the same patterns of the paratope and the epitope, ● the quantity of a set represented by an object is defined by the concentration attribute.	simulation	Krzysztof Trojanowski;Marcin Sasin	2006		10.1007/11823940_8	knowledge base;simulation;systems modeling;immune system;stimulation;computer science;artificial intelligence;pattern matching;expert system;algorithm;idiotype	Vision	-21.221940124767098	-1.8072723277687692	70796
76f086e20a78b077d4a3610ca47f2c5ec1ba0392	commas (condition monitoring multi-agent system)	task performance;model based reasoning;decision support;knowledge based system;multi agent system;case base reasoning;agent based;data interpretation;computational intelligence;real time;software systems;data fusion;early diagnosis;power plant;multi agent systems;intelligent agents;condition monitoring;intelligent system;agent technology;intelligent agent;multi agent architecture;real time application;knowledge based systems;artificial neural network;electrical engineering electronics nuclear engineering	The application of intelligent systems for data interpretation and condition monitoring is an advancing field of research. In recent years autonomous intelligent agents and multi-agent systems have gained much attention within different real time applications. The novel idea of COMMAS (COndition Monitoring Multi-Agent System) introduces a hierarchical decentralised multi-agent architecture developed for data interpretation and condition monitoring applications. By definition condition monitoring is concerned with detecting and distinguishing faults occurring in plant that is being monitored [1]; therefore the early diagnosis and identification of faults has a number of benefits (improvement in the plant economy, reduction in operational costs, improving the level of safety etc). A variety of intelligent techniques have been applied in plant monitoring, which resulted in the development of centralised approaches for condition monitoring, e.g., Knowledge Based Systems (KBS) [2], Model Based Reasoning (MBR) Systems [3], Case Based Reasoning (CBR) Systems [4], Artificial Neural Networks (ANN) [5] etc. These approaches tend to be fixed, so they lack flexibility and extensibility. Moving to an agent-based architecture allows simultaneous complex tasks to be performed in real-time; better handling of inaccurate data is achieved and each agent can be independently updated.	multi-agent system	Eleni E. Mangina;Stephen D. J. McArthur;James R. McDonald	2001	Autonomous Agents and Multi-Agent Systems	10.1023/A:1011499912015	power station;simulation;computer science;artificial intelligence;model-based reasoning;multi-agent system;sensor fusion;data analysis;intelligent agent;software system	AI	-21.83320145340347	-4.497340185238497	71087
7bc70188d24efa7be56dd73507af1a0004ebe1c1	qualitative models in medical diagnosis	qualitative modeling;medical diagnosis	Abstract   Diagnostic systems based solely on associative knowledge are able to draw accurate conclusions in acceptable times but they do not capture all the available medical knowledge. Some of this knowledge, even if incomplete, is sufficiently precise to allow the formulation of qualitative models. The aim of this paper is to show how qualitative models can be exploited in a medical diagnostic system. We present a system, NEOANEMIA, that integrates first generation knowledge representation formalisms (frames and production rules) with qualitative pathophysiological models to diagnose hematologic disorders causing anemia. To this end, qualitative models of iron metabolism, erythropoietin metabolism, and red cell production and destruction have been formulated. The key ideas of our work are: abducing diagnostic hypotheses from observed problem features, modeling pathophysiological systems with dynamic qualitative models, predicting pathophysiological behaviours by qualitative model simulation, comparing clinical observations against simulation results, and when necessary, incrementally creating and testing multiple diagnostic hypotheses. In this way the performance of a diagnostic expert system can be highly enhanced.		Liliana Ironi;Mario Stefanelli;Giordano Lanzola	1990	Artificial Intelligence in Medicine	10.1016/0933-3657(90)90031-L	simulation;computer science;medical diagnosis	AI	-20.519986242521753	-4.018422551628761	71127
53aef36273e72801760e9c78d53db63263cb3969	taking decisions in the expert intelligent system to support maintenance of a technical object on the basis information from an artificial neural network	intelligent system;servicing process;system modelling;expert system;artificial neural networks;knowledge base;diagnostics information	In this paper, an intelligent operation system, which consists of an intelligent diagnostic subsystem (with a neural network) and an intelligent maintenance subsystem (with an expert system), was presented and discussed. The artificial neural network and the expert system, which use the information developed in the neural network, perform a special function in this system. The functional combination of the artificial neural network and the expert system together created a new solution in the form of an intelligent system, which was referred to as an intelligent maintenance system. This article also covers decision-making methods that are used in an expert maintenance system and whose purpose is an organization and control of the process of the prevention of technical objects. For this purpose, the method was described of taking decisions by an expert for complex parametric type hypotheses and for simple finished type hypotheses in the set of possible decisions’ hypotheses. A considerable part of this paper covers the presentation of the method to transform diagnostic information into the required form of maintenance information. For this purpose, an algorithm of the work of maintenance system was performed and descried. In the creation process of the maintenance knowledge base, the specialist knowledge of a human specialist was also used. Hence, a skilful and proper taking of decisions by an expert to create this set of information is essential. Two inference methods were characterized and described in this paper. The theoretical results obtained were verified in the examination of the influence of each of these decision-making inference methods on the final results of the process of the prevention treatment of an object.	algorithm;artificial intelligence;artificial neural network;expert system;knowledge base;operating system;requirement	Stanislaw Duer;Konrad Zajkowski	2012	Neural Computing and Applications	10.1007/s00521-012-1169-x	legal expert system;artificial intelligence;machine learning;data mining	AI	-22.518992967517175	-5.115363888043182	71307
dc5bd196a19e2fb0c4a10e1f0fa0796086a470c9	efficient classification of billions of points into complex geographic regions using hierarchical triangular mesh	genome data analysis system;data warehouse	We present a case study about the spatial indexing and regional classification of billions of geographic coordinates from geo-tagged social network data using Hierarchical Triangular Mesh (HTM) implemented for Microsoft SQL Server. Due to the lack of certain features of the HTM library, we use it in conjunction with the GIS functions of SQL Server to significantly increase the efficiency of pre-filtering of spatial filter and join queries. For example, we implemented a new algorithm to compute the HTM tessellation of complex geographic regions and precomputed the intersections of HTM triangles and geographic regions for faster false-positive filtering. With full control over the index structure, HTM-based pre-filtering of simple containment searches outperforms SQL Server spatial indices by a factor of ten and HTM-based spatial joins run about a hundred times faster.	algorithm;decade (log scale);geographic coordinate system;geographic information system;html;hierarchical temporal memory;microsoft sql server;polygon mesh;precomputation;social network	Dániel Kondor;Laszlo Dobos;István Csabai;András Bodor;Gábor Vattay;Tamas Budavari;Alexander S. Szalay	2014		10.1145/2618243.2618245	computer science;data warehouse;data mining;database;world wide web	DB	-27.53754848561974	-0.4588693639295668	71347
ddaf6fdf057b402927ff6c0a1829e08e05d8cbf8	rough sets, guarded command language, and decision rules	command language;intelligence artificielle;regle decision;control protegido;systeme incertain;commande gardee;artificial intelligence;guarded command;inteligencia artificial;regla decision;information system;rough set;sistema incierto;ensemble approximatif;uncertain system;systeme information;decision rule;sistema informacion	The rough set approach is a mathematical tool for dealing with imprecision, uncertainty, and vagueness in data. Guarded command languages provide logical approaches for representing constrained nondeterminacy in an otherwise deterministic system without incorporating probabilistic elements. Although from dramatically different functional and mathematical origins, both approaches attempt to resolve observed or anticipated discontinuities between specific pre- and postcondition states of a given information system. This paper investigates the use of a guarded command language in the generation of rough data from explicit decision rules, and in the extraction of implicit decision rules from rough experimental data. Based on these findings, rough sets and guarded command languages appear to be compatible and complementary in their approaches to imprecision and uncertainty. As the association between rough sets and guarded command language represents a new and heretofore untested research direction, possible research alternatives are suggested.	guarded command language;rough set	Frederick V. Ramsey;James J. Alpigini	2002		10.1007/3-540-45813-1_23	rough set;computer science;artificial intelligence;machine learning;data mining;decision rule;information system;algorithm;dominance-based rough set approach	Logic	-21.930700658656736	-2.191559531254138	71464
1e4959803e595063259ad555b5b4d0911f3a98da	an approach for automatic data virtualization	query processing;sql;virtual reality;meta data geographic information systems distributed databases visual databases virtual reality sql query processing very large databases grid computing;automatic generation;multi dimensional;geographic information systems;data abstraction;distributed databases;meta data;data layout;very large databases;grid computing;grid computing databases biomedical computing distributed computing biomedical informatics information analysis indexing data mining subcontracting laboratories;hand written code automatic data virtualization geographically distributed scientific dataset grid computing binary character flat file data service sql query meta data description language;geographic distribution;visual databases	Analysis of large and/or geographically distributed scientific datasets is emerging as a key component of grid computing. One challenge in this area is that scientific datasets are typically stored as binary or character flat-files, which makes specification of processing much harder. In view of this, there has been recent interest in data virtualization, and data services to support such virtualization. This paper presents an approach for automatically creating data services to support data virtualization. Specifically, we show how a relational table like data abstraction can be supported for complex multidimensional scientific datasets that are resident on a cluster. We have designed and implemented a tool that processes SQL queries (with select and where statements) on multi-dimensional datasets. We have designed a meta-data description language that is used for specifying the data layout. From such description, our tool automatically generates efficient data subsetting and access functions. We have extensively evaluated our system. The key observations from our experiments are as follows. First, our tool can correctly and efficiently handle a variety of different data layouts. Second, our system scales well as the number of nodes or the amount of data is scaled. Third, the performance of the automatically generated code for indexing and contracting functions is quite comparable to the performance of hand-written codes.	abstraction (software engineering);binary file;code;data definition language;experiment;grid computing;sql	Li Weng;Gagan Agrawal;Ümit V. Çatalyürek;Tahsin M. Kurç;Sivaramakrishnan Narayanan;Joel H. Saltz	2004	Proceedings. 13th IEEE International Symposium on High performance Distributed Computing, 2004.	10.1109/HPDC.2004.2	sql;computer science;data virtualization;theoretical computer science;operating system;data mining;database;distributed computing;virtual reality;programming language;metadata;grid computing	HPC	-31.597127240047488	1.7289561636405268	71524
2d2042f38f42682bff35a8067fd9721e445554d4	knowledge engineering by large-scale knowledge reuse - experience from the medical domain.	knowledge reuse;large scale;knowledge engineering			Stefan Schulz;Udo Hahn	2000			knowledge integration;computer science;knowledge management;artificial intelligence;feature-oriented domain analysis;domain engineering;knowledge engineering;open knowledge base connectivity;management science;knowledge extraction;domain knowledge	SE	-32.15943264088165	-6.094263008353947	71536
e7a4c5c4b983645473fe35babc0a855725b01b98	minimizing the molap/rolap divide: you can have your performance and scale it too	olap;analytics;data warehousing	Over the past generation, data warehousing and online analytical processing (OLAP) applications have become the cornerstone of contemporary decision support environments. Typically, OLAP servers are implemented on top of either proprietary array-based storage engines (MOLAP) or as extensions to conventional relational DBMSs (ROLAP). While MOLAP systems do indeed provide impressive performance on common analytics queries, they tend to have limited scalability. Conversely, ROLAP’s table oriented model scales quite nicely, but offers mediocre performance at best relative to the MOLAP systems. In this paper, we describe a storage and indexing framework that aims to provide both MOLAP like performance and ROLAP like scalability by essentially combining some of the best features from both. Based upon a combination of R-trees and bitmap indexes, the storage engine has been integrated with a robust OLAP query engine prototype that is able to fully exploit the efficiency of the proposed storage model. Specifically, it utilizes an OLAP algebra coupled with a domain specific query optimizer, to map user queries directly to the storage and indexing framework. Experimental results demonstrate that not only does the design improve upon more naive approaches, but that it does indeed offer the potential to optimize both query performance and scalability. Category: Convergence computing	bitmap;client-side;compile time;compiler;core storage;data (computing);database engine;decision support system;deployment environment;disk space;email;emoticon;entity;experiment;java;mathematical optimization;online analytical processing;prototype;query optimization;r-tree;scalability;simple xml;sparse matrix;star schema;storage model	Todd Eavis;Ahmad Taleb	2013	JCSE	10.5626/JCSE.2013.7.1.1	computer science;data mining;database;world wide web	DB	-31.75286621340491	1.3922215267632683	72017
afb7e847d8c4628817e5ad1487a8d67619abbd6f	a distributed range query framework for the internet of things	sensors;communication systems;data indexing module internet of things data management mechanism data storage distributed range query framework data reporting module query executor sensed data learning data range prediction abnormal data data migration quantity reduction data migration frequency reduction reported data information collection query request query executor load balancing;computer and information science;internet of things;indexing;kommunikationssystem;peer to peer computing indexing sensors internet of things distributed databases wireless sensor networks;distributed databases;internet of things range query distributed data;peer to peer computing;data och informationsvetenskap;wireless sensor networks;query processing database indexing distributed databases internet of things	The range query referring to the Internet of Things is a tough challenge since the data information is fully distributed. In order to support efficient range query, most existing approaches focused on designing the data management mechanism, which ensures that the similar data are stored nearby within the network. However, it will introduce large extra overhead to each peer in the Internet of Things especially when the peers generate data frequently. In this paper, a distributed range query framework is proposed, which consists of three core modules, reporting and indexing module, along with a query executor. The reporting module learns the sensed data and predicts a data range in which the coming future data is likely to be. Only the abnormal data that exceeds the data range will be detected, which greatly reduces the frequency and quantity of data migration in these data management mechanism. The indexing module is responsible for collecting reported data information and establishing data index used for responding to query request. Based on the above two modules, the range query is processed by the query executor. The experimental results show that this proposal could support range query effectively and efficiently, with load balance among the peers at the same time.	internet of things;load balancing (computing);overhead (computing);range query (database)	Congcong Zhang;Tingting Zhang;Mei Wang	2015	2015 18th International Conference on Intelligence in Next Generation Networks	10.1109/ICIN.2015.7073811	sargable;query optimization;query expansion;web query classification;computer science;query by example;data mining;database;rdf query language;web search query;view;world wide web;query language	DB	-30.71192015117134	-0.7408996145810572	72302
b53d03b3e3e09b5ca237fad299aba5de47ec7d56	a machine learning method for multi-expert decision support	decision support;rule based;decision maker;decision problem;machine learning;expert system	When a decision maker has access to multiple expert systems, each embodying a different expert perspective on analyzing and reasoning about the same kind of decision problem, an important consideration is which to use at what times. We address this issue with a method based on competition among the distinct expert systems (and their respective rules). We begin by reviewing prior research concerned with the coordination of multiple sources of expertise in support of decision making, pointing out potential weaknesses of the proposed methods. Next, we introduce a new coordination method based on the competitive paradigm that has been applied in machine learning. This method involves adjustments to the strengths of expert systems and to their constituent rules based on their performances. A nine-step process for adjusting strengths is described. Advantages and limitations of this new method for expert system coordination are discussed. We outline an approach to testing the coordination method and report on preliminary testing of the performance of a system employing our method versus the performance of individual experts.	decision problem;decision support system;expert system;machine learning;performance;programming paradigm	Clyde W. Holsapple;Anita Lee;Jim Otto	1997	Annals OR	10.1023/A:1018955328719	legal expert system;decision-making;r-cast;optimal decision;decision support system;intelligent decision support system;decision analysis;decision engineering;computer science;knowledge management;artificial intelligence;decision problem;data mining;decision rule;expert system;business decision mapping	AI	-21.46704407823484	-8.32274038849254	72391
eb0428529095373446e281bcc564fcb22a3bba53	optimized data placement for column-oriented data store in the distributed environment	gray code;range query;data processing;data analysis;data storage;distributed environment;theoretical analysis;query evaluation;storage capacity;indexation;data access;compression ratio;distributed file system;high efficiency;data placement	Column-oriented data storage becomes a buzzword nowadays for its high efficiency in massive data access, high compression ratio on individual columns and etc. However, the initial observations turn out to not be trivially true. The seek time and bandwidth of current hard disk drivers (HDD) become the bottleneck for massive data processing day by day, when comparing to other component enhancements of computers during the past four decades. In this paper, we provide a novel data placement strategy for massive data analysis (i.e., readoptimized) based on Gray Code, which enhances the ratio of sequential access to a great extent for diverse query evaluations (e.g., range query, partial match range query, aggregation query and etc). A centralized/distributed structured index is employed in the popularly deployed distributed file systems (e.g., GFS), which achieves the convenient management, efficient accessibility, high extendibility and etc. Detailed theoretical analysis on index extendibility, sequential access improvement and storage capacity usage in terms of proposed data placement strategies are provided as well as specific algorithms. Our extensive experimental studies confirm the efficiency and effectiveness of our proposed data placement methods.		Minqi Zhou;Chen Xu	2011		10.1007/978-3-642-20244-5_42	gray code;data access;range query;query optimization;data processing;computer science;compression ratio;computer data storage;data mining;database;data efficiency;distributed file system;data analysis;world wide web;distributed computing environment	HPC	-27.765287789213307	1.3179285510464864	72505
df9ab0a8a16d640b7e6621a62cb5a71ae996975b	an axiomatic decision aid for marine equipment selection in ship machinery design and installation projects				Selçuk Çebi;Metin Celik;Cengiz Kahraman	2012	Multiple-Valued Logic and Soft Computing		machine learning;artificial intelligence;systems engineering;computer science;axiom	HCI	-30.73377456297738	-9.34185857805752	72998
a9d31503f0aafe7eaf5247dbf21b99d99da9cb44	empirical merging of ontologies - a proposal of universal uncertainty representation framework	distributed system;bottom up method;reseau social;representacion conocimientos;ontologie;systeme reparti;bottom up;conceptualization;metodo ascendente;correspondance ontologie;procesamiento informacion;ontology mapping;web semantique;conceptual analysis;traitement incertitude;uncertainty handling;acquisition connaissances;analisis conceptual;methode ascendante;imperfect information;conceptualizacion;langage dedie;social network;sistema repartido;ontology learning;web semantica;knowledge acquisition;information processing;inferencia;representation connaissance;domain specific language;semantic web;informacion imperfecta;uncertainty representation;ontologia;information system;adquisicion de conocimientos;analyse conceptuelle;knowledge representation;traitement information;ontology;correspondencia ontologia;conceptualisation;systeme information;red social;domain specificity;inference;lenguaje dedicado;information imparfaite;sistema informacion	The significance of uncertainty representation has become obvious in the Semantic Web community recently. This paper presents our research on uncertainty handling in automatically created ontologies. A new framework for uncertain information processing is proposed. The research is related to OLE (Ontology LEarning) — a project aimed at bottom–up generation and merging of domain–specific ontologies. Formal systems that underlie the uncertainty representation are briefly introduced. We discuss the universal internal format of uncertain conceptual structures in OLE then and offer a utilisation example then. The proposed format serves as a basis for empirical improvement of initial knowledge acquisition methods as well as for general explicit inference tasks.	academy;cluster analysis;formal methods;fuzzy set;hash function;information processing;knowledge acquisition;lookup table;markov switching multifractal;mind;object linking and embedding;ontology (information science);ontology learning;semantic web;usability;web ontology language	Vít Novácek;Pavel Smrz	2006		10.1007/11762256_8	conceptualization;semantic integration;information processing;computer science;domain-specific language;artificial intelligence;perfect information;semantic web;ontology;top-down and bottom-up design;data mining;information system;social network	AI	-22.636776350470214	-1.0896997680624854	73228
6f07cc069cabad52ffb1faf663f38319585a7d23	finding ways to get the job done: an affordance-based approach		Adapting plans to changes in the environment by finding alternatives and taking advantage of opportunities is a common human behavior. The need for such behavior is often rooted in the uncertainty produced by our incomplete knowledge of the environment. While several existing planning approaches deal with such issues, artificial agents still lack the robustness that humans display in accomplishing their tasks. In this work, we address this brittleness by combining Hierarchical Task Network planning, Description Logics, and the notions of affordances and conceptual similarity. The approach allows a domestic service robot to find ways to get a job done by making substitutions. We show how knowledge is modeled, how the reasoning process is used to create a constrained planning problem, and how the system handles cases where plan generation fails due to missing/unavailable objects. The results of the evaluation for two tasks in a domestic service domain show the viability of the approach in finding and making the appropriate goal transformations.	description logic;hierarchical task network;intelligent agent;planning;service robot	Iman Awaad;Gerhard K. Kraetzschmar;Joachim Hertzberg	2014			machine learning;hierarchical task network;artificial intelligence;robustness (computer science);affordance;description logic;management science;service robot;computer science	AI	-21.275436678162652	-8.4669756580787	73241
391266b1907e5621ad24261aff5e89bb1b24a70e	data quality through knowledge engineering	data processing;data mining;static and dynamic constraints;business operations databases;domain knowledge;subject matter expert;data quality;business rules;production rule;expert system;knowledge engineering	Traditionally, data quality programs have acted as a preprocessing stage to make data suitable for a data mining or analysis operation. Recently, data quality concepts have been applied to databases that support business operations such as provisioning and billing. Incorporating business rules that drive operations and their associated data processes is critically important to the success of such projects. However, there are many practical complications. For example, documentation on business rules is often meager. Rules change frequently. Domain knowledge is often fragmented across experts, and those experts do not always agree. Typically, rules have to be gathered from subject matter experts iteratively, and are discovered out of logical or procedural sequence, like a jigsaw puzzle. Our approach is to impement business rules as constraints on data in a classical expert system formalism sometimes called production rules. Our system works by allowing good data to pass through a system of constraints unchecked. Bad data violate constraints and are flagged, and then fed back after correction. Constraints are added incrementally as better understanding of the business rules is gained. We include a real-life case study.	data mining;data quality;database;documentation;electronic billing;expert system;knowledge engineering;preprocessor;production (computer science);provisioning;real life;semantics (computer science);subject matter expert turing test;subject-matter expert	Tamraparni Dasu;Gregg T. Vesonder;Jon R. Wright	2003		10.1145/956750.956844	data governance;business logic;semantics of business vocabulary and business rules;data quality;data processing;computer science;knowledge management;artifact-centric business process model;artificial intelligence;data science;machine learning;data validation;knowledge engineering;data mining;subject-matter expert;business rule;expert system;domain knowledge	DB	-31.67482342621366	-6.267519541870462	73302
36f6d0491cb1f70adf07ae6853471976ec35d931	an agent simulation model for the quebec forest supply chain	consumidor;modelizacion;wood;multiagent system;logistique;agent modeling;consommateur;cooperation;intelligence artificielle;cooperacion;game design;bois;modelisation;logistics;consumer;artificial intelligence;supply chain;inteligencia artificial;sistema multiagente;modeling;madera;simulation model;systeme multiagent;logistica	A supply chain is a network of companies producing and distributing products to end-consumers. The Québec Wood Supply Game (QWSG) is a board game designed to teach supply chain dynamics. The QWSG provides the agent model for every company in our simulation. The goal of this paper is to introduce this simulation model. For this purpose, we first outline the QWSG, and then describe with mathematical equations each company in our simulation. Finally, three examples illustrate the use of our simulation to study collaboration in supply chains. More precisely, we study incentives for collaboration at both the supply chain and company level.	game theory;nash equilibrium;open collaboration;random forest;sawmill;simulation;supply chain attack	Thierry Moyaux;Brahim Chaib-draa;Sophie D'Amours	2004		10.1007/978-3-540-30104-2_17	game design;logistics;simulation;systems modeling;consumer;computer science;artificial intelligence;simulation modeling;wood;supply chain;operations research;cooperation	ECom	-23.181262475045926	-7.575472107760824	73365
3930baa2e41393a493743c860e425481acdd8145	a comparative analysis of state-of-the-art sql-on-hadoop systems for interactive analytics	sql-on-hadoop;interactive analytics;perfor- mance evaluation;olap	Hadoop is emerging as the primary data hub in enterprises, and SQL represents the de facto language for data analysis. This combination has led to the development of a variety of SQL-on-Hadoop systems that are in use today. While the various SQL-on-Hadoop systems target the same class of analytical workloads, their different architectures, design decisions and implementations impact query performance. In this work, we perform a comparative analysis of four state-of-the-art SQL-on-Hadoop systems (Impala, Drill, Spark SQL and Phoenix) using the Web Data Analytics micro benchmark and the TPC-H benchmark on the Amazon EC2 cloud platform. The TPC-H experiment results show that, although Impala outperforms other systems (4.41x–6.65x) in the text format, trade-offs exists in the parquet format, with each system performing best on subsets of queries. A comprehensive analysis of execution profiles expands upon the performance results to provide insights into performance variations, performance bottlenecks and query execution characteristics.		Ashish Tapdiya;Daniel Fabbri	2017	2017 IEEE International Conference on Big Data (Big Data)	10.1109/BigData.2017.8258066	sql;data mining;database;implementation;computer science;cloud computing;data analysis;benchmark (computing);data hub;formatted text;analytics	DB	-32.6126325183135	0.8738917303183265	73408
b4d5e9f64d4a3e8177076c9c7449f0e4a722a8cc	incremental relevance feedback	user interface;document retrieval;relevance feedback	Although relevance feedback techniques have been investigated for more than 20 years, hardly any of these techniques has been implemented in a commercial full-text document retrieval system. In addition to pure performance problems, this is due to the fact that the application of relevance feedback techniques increases the complexity of the user interface and thus also the use of a document retrieval system. In this paper we concentrate on a relevance feedback technique that allows easily understandable and manageable user interfaces, and at the same time provides high-quality retrieval results. Moreover, the relevance feedback technique introduced unifies as well as improves other well-known relevance feedback techniques.	document retrieval;relevance feedback;user interface	I Jsbrand Jan Aalbersberg	1992		10.1145/133160.133169	document retrieval;computer science;multimedia;user interface;world wide web;information retrieval	Web+IR	-33.68672154967228	3.753484664413655	73531
6711f6b00ef6dbc261d7763ad4cfcab6e904ad9d	assessment of features technology	concepcion asistida;computer aided design;base donnee;modele geometrique;systeme intelligent;metodologia;sistema inteligente;database;base dato;methodologie;intelligent system;conception assistee;methodology;geometrical model;modelo geometrico	Features encapsulate the engineering significance of portions of the geometry of a part or assembly, and, as such, are important in product design, product definition, and reasoning, for a variety of applications. Feature-based systems have demonstrated some potential in creating attractive design environments and in automating the geometric reasoning required in applications such as process planning and manufacturability evaluation. The paper reviews the major concepts and approaches that are in use in feature-based modeling. Several methodologies are prevalent for creating feature models and databases. These fall broadly into the categories of interactive definition, automatic recognition/extraction, and design by features. Within each, there are several subcategories, which are discussed and compared in the paper. Also presented are several schemes popular for representing features. They include augmented graphs, syntactic strings in grammars, and objects in objectoriented programming. Feature interactions and validation issues are outlined. Attempts at developing feature taxonomies are also summarized.	data validation;database;design for manufacturability;feature model;interaction	Jami J. Shah	1991	Computer-Aided Design	10.1016/0010-4485(91)90027-T	simulation;computer science;engineering;artificial intelligence;computer aided design;methodology;engineering drawing	EDA	-24.73655437986755	-2.55634683616815	73550
3527d9b11e106fa8bcfe4fbad0c5b70a417475aa	geometric information makes spatial query processing more efficient.	query processing;indexation;spatial access method	In order to index complex and heterogeneous cartographic data by means of spatial access methods, one typically uses single, simple geometries to approximate the given geome-tries. Traditionally, no information regarding the quality of the approximation is stored. In this paper, we show that the availability of such information allows us to decide at an early stage that numerous objects fullll the spatial predicate. This leads to signiicant performance improvements for the spatial selection as well as for the spatial intersection join. We present two techniques enabling such an early decision for the z-ordering method, and we demonstrate their eeciency by means of experiments. 1 Introduction Fast access to spatial data stored in databases is essential for answering spatial queries eeciently. As a result, numerous spatial access methods have been proposed and studied in the past. The complexity and heterogeneity of spatial data makes it impossible to store cartographic data directly in the spatial index, since these indices are designed to handle simple geometries such as points, rectangles or (simple) convex polygons. To index cartographic data, we have to approximate complex geometries with a simple geometry that is in turn stored in the spatial index along with a reference to the object geometry. This simple geometry is a representative for the actual geometry. As a result of this approximation scheme, query processing has to be broken up into multiple steps:	approximation algorithm;cartography;experiment;spatial database;spatial query	Volker Gaede	1995			online aggregation;sargable;query optimization;query expansion;web query classification;computer science;data mining;database;rdf query language;information retrieval;query language;spatial query	DB	-27.131522613256255	2.030856826036035	73685
2a2ca2584de4a7a6d41ec126c73d8de0ceb9f46f	analysis and comparison of declustering schemes for interactive navigation queries	random assignment;resource limitation;declustering;range query;spatial data;query processing;navigation tiles information retrieval data analysis satellites performance analysis spatial databases visual databases geoscience delay;navigation queries;latin square;parallel databases;random assignment scheme declustering schemes interactive navigation queries parallel retrievals raster spatial data static range queries data access patterns performance latin square based schemes disk modulo fieldwise exclusive or hilbert curve allocation method;spatial databases;query processing parallel databases visual databases;data access;high performance;hilbert curve;visual databases	Declustering schemes enable parallel retrievals of raster-spatial data by allocating data among multiple disks. Previous work in the literature has focused on static range queries. In this paper, we focus on interactive navigation queries, which exhibit a new class of data access patterns. We analyze and compare the performance of previously known declustering schemes from the perspective of navigation queries. These schemes include a class of latin-square-based schemes, Disk Modulo, Fieldwise Exclusive-OR, Hilbert Curve Allocation Method, and a random assignment scheme. We show that, unlike the case of range queries, disk module outperforms other schemes and gives nearly optimal performance for navigation queries. In addition, we propose a new scheme under the constraint of bounded window size-a common constraint in practice due to resource limitation such as monitor resolution or memory size. Through extensive analysis, we establish guidelines on how these schemes can be tuned to provide guaranteed performance under the constraint of bounded window size.		Chung-Min Chen;Rakesh K. Sinha	2000	IEEE Trans. Knowl. Data Eng.	10.1109/69.877507	data access;range query;computer science;latin square;theoretical computer science;data mining;database;spatial analysis;statistics;random assignment	DB	-27.509925987441438	2.1557961630435636	73698
415617f0523e6d330936ec3ce7757aa94111e644	a comparison of inferences about containers and surfaces in small-scale and large-scale spaces	human cognition;query language;spatial reasoning;human subjects;large scale;scale space;spatial relation;cognitive structure;spatial information	Inference mechanisms about spatial relations constitute an important aspect of spatial reasoning as they allow users to derive unknown spatial information from a set of known spatial relations. When formalized in the form of algebras, spatial-relation inferences represent a mathematically sound definition of the behavior of spatial relations, which can be used to specify constraints in spatial query languages. Current spatial query languages utilize spatial concepts that are derived primarily from geometric principles, which do not necessarily match with the concepts people use when they reason and communicate about spatial relations. This paper presents an alternative approach to spatial reasoning by starting with a small set of spatial operators that are derived from concepts closely related to human cognition. This cognitive foundation comes from the behavior of image schemata, which are cognitive structures for organizing people’s experiences and comprehension. From the operations and spatial relations of a small-scale space, a container-surface algebra is defined with nine basic spatial operators—inside, outside, on, off, their respective converse relationscontains, excludes, supports, separated_from, and the identity relation equal . The container-surface algebra was applied to spaces with objects of different sizes and its inferences were assessed through human-subject experiments. Discrepancies between the container-surface algebra and the human-subject esting appear for combinations of spatial relations that result in more than one possible inference depending on the relative size of objects. For configurations with small-scale and large-scale objects larger discrepancies were found because people use relations such as part of and at in lieu of in. Basic concepts such as containers and surfaces seem to be a promising approach to define and derive inferences among spatial relations that are close to human reasoning.	cognition;depth perception;experiment;organizing (structure);query language;scale space;spaces;spatial query;spatial–temporal reasoning	M. Andrea Rodríguez;Max J. Egenhofer	2000	J. Vis. Lang. Comput.	10.1006/jvlc.2000.0166	spatial relation;scale space;cognition;computer science;database;spatial analysis;spatial intelligence;query language	AI	-21.636250455450135	3.92204690671875	73725
3091ad788effd851d7df8c3a2d32d4e513cb8cdb	probabilistic plan recognition for hostile agents	plan recognition	This paper presents a probabilistic and abductive theory of plan recognition that handles agents that are actively hostile to the inference of their plans. This focus violates a primary assumption of most previous work, namely complete observability of the executed actions.	abductive reasoning	Christopher W. Geib;Robert P. Goldman	2001			computer science;artificial intelligence	AI	-19.146240293227912	-3.790626024294369	73746
faf5bb964e5ff4c056c47cd21a6c3955a2b8a5e0	a fuzzy approximate reasoning model for a rule-based system in laser threat recognition	fuzzy approximate reasoning;model based reasoning;linguistique;raisonnement base sur modele;sistema experto;theorie ensemble flou;rule based system;logique floue;traitement incertitude;logica difusa;uncertainty handling;fuzzy set theory;fuzzy logic;approximate reasoning;linguistica;raisonnement approximatif;pattern recognition;decision process;reconnaissance forme;systeme expert;reconocimiento patron;laser threat;expert system;linguistics	Intrinsically, vague knowledge is typical of the decision process especially when it is based on complex and non-numeric data. Linguistic descriptions play a significant role in decision making for pattern recognition. Fuzzy set theory and fuzzy logic provide an approximate framework for the representation of imprecise concepts and imprecise models of reasoning. In this article, fuzzy logic is proposed as a means of handling uncertainty in an expert system structure for automatic laser threat recognition which is increasingly required in modern war. Fuzzy characteristics of the feature values of laser threat patterns are comprehensively described. A prototype system has been developed and tested on labeled sample data of laser threats. The results are compared to our previous work, a similar recognition system. a;' 1998 Elsevier Science B.V. All rights reserved.	approximation algorithm;expert system;fuzzy logic;fuzzy set;level of measurement;pattern recognition;prototype;rule-based system;set theory;vagueness	Cheng-shung Wang;Xun-kai Gong	1998	Fuzzy Sets and Systems	10.1016/S0165-0114(96)00289-8	fuzzy logic;defuzzification;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;model-based reasoning;machine learning;fuzzy set;expert system;fuzzy set operations;algorithm;fuzzy control system	AI	-20.634711449104774	-1.6844843676272026	74220
e259be8216b029ffc2f23410d8942946b0a2b81b	environmental decision support systems: exactly what are they? - workshop report			decision support system	David A. Swayne	1999			environmental science;decision analysis;r-cast;systems engineering;decision support system;intelligent decision support system;business decision mapping;decision engineering	ECom	-30.90190001181517	-9.33246876368225	74303
a9ecf2ae6b009e80ad5b4a9f81b116c2ee1face7	top-k exploration of query candidates for efficient keyword search on graph-shaped (rdf) data	top k;graph theory;resource description format;optimisation;cyclic graphs;graph shaped data;query processing;optimization technique;database optimization techniques;search algorithm;data mining;data model;data graph graph shaped data keyword search keyword queries search algorithms dedicated indexing techniques rdf data model database optimization techniques query processing top k matching subgraphs cyclic graphs;indexes;keyword search resource description framework databases indexing data models engines query processing tree graphs costs data structures;performance improvement;dedicated indexing techniques;engines;keyword search;top k keyword search rdf;data structures;keyword queries;indexation;data graph;top k matching subgraphs;joining processes;query processing data handling graph theory optimisation;data handling;information need;rdf data model;data structure;search algorithms;rdf;structured data	Keyword queries enjoy widespread usage as they represent an intuitive way of specifying information needs. Recently, answering keyword queries on graph-structured data has emerged as an important research topic. The prevalent approaches build on dedicated indexing techniques as well as search algorithms aiming at finding substructures that connect the data elements matching the keywords. In this paper, we introduce a novel keyword search paradigm for graph-structured data, focusing in particular on the RDF data model. Instead of computing answers directly as in previous approaches, we first compute queries from the keywords, allowing the user to choose the appropriate query, and finally, process the query using the underlying database engine. Thereby, the full range of database optimization techniques can be leveraged for query processing. For the computation of queries, we propose a novel algorithm for the exploration of top-k matching subgraphs. While related techniques search the best answer trees, our algorithm is guaranteed to compute all k subgraphs with lowest costs, including cyclic graphs. By performing exploration only on a summary data structure derived from the data graph, we achieve promising performance improvements compared to other approaches.	computation;data model;data structure;database engine;graph (abstract data type);information needs;information retrieval;mathematical optimization;programming paradigm;resource description framework;search algorithm;structured programming;yahoo! answers	Thanh Tran;Haofen Wang;Sebastian Rudolph;Philipp Cimiano	2009	2009 IEEE 25th International Conference on Data Engineering	10.1109/ICDE.2009.119	data structure;data model;computer science;data mining;database;programming language;information retrieval;search algorithm	DB	-31.356032807469877	3.7080873505174674	74384
c4ae4add074d8d29e6d732f06ed5e7f9d5d1719a	mk-tree: an effective access method for indexing high dimensional data.	indexation;high dimensional data;access method			Guoren Wang;Xiangmin Zhou;Bin Wang;Baiyou Qiao;Donghong Han	2005	JDIM		computer science;data mining;database;programming language;access method;information retrieval;clustering high-dimensional data	DB	-28.25953039964374	0.7092150813018709	74501
34a325ee1fbcc7c3c5ffd0c379422f5f7065b9c7	generating flexible workloads for graph databases		Graph data management tools are nowadays evolving at a great pace. Key drivers of progress in the design and study of data intensive systems are solutions for synthetic generation of data and workloads, for use in empirical studies. Current graph generators, however, provide limited or no support for workload generation or are limited to fixed use-cases. Towards addressing these limitations, we demonstrate gMark, the first domainand query languageindependent framework for synthetic graph and query workload generation. Its novel features are: (i) fine-grained control of graph instance and query workload generation via expressive user-defined schemas; (ii) the support of expressive graph query languages, including recursion among other features; and, (iii) selectivity estimation of the generated queries. During the demonstration, we will showcase the highly tunable generation of graphs and queries through various user-defined schemas and targeted selectivities, and the variety of supported practical graph query languages. We will also show a performance comparison of four state-ofthe-art graph database engines, which helps us understand their current strengths and desirable future extensions.	data-intensive computing;graph (discrete mathematics);graph database;query language;recursion;selectivity (electronic);synthetic intelligence	Guillaume Bagan;Angela Bonifati;Radu Ciucanu;George H. L. Fletcher;Aurélien Lemay;Nicky Advokaat	2016	PVLDB	10.14778/3007263.3007283	wait-for graph;computer science;theoretical computer science;data mining;database;programming language;graph database;query language	DB	-32.70809241311807	1.8725301496151516	74646
01c4376cb874f8a2d637c978b8340614e0824276	gaining reactivity for rich internet applications by introducing client-side complex event processing and declarative rules		Rich Internet Applications provide, in conjunction with Internet push technologies, a powerful framework to bring use cases formerly reserved to server applications to the client, and to ease their use. In this paper we present a novel approach of monitoring and processing event streams directly in the browser. Our proposed general-purpose framework aims at the design of event-driven, reactive and adaptive Rich Internet Applications. We propose to interweave complex event processing with declarative rule execution directly on the client-side. Our work is based on a novel eventcondition-action rule language tailored to the needs of Rich Internet Applications as well as algorithms capable of detecting complex events and executing rules. The whole approach will be illustrated by means of an example originating from the field of algorithmic stock	algorithm;client-side;complex event processing;declarative programming;e-government;event condition action;event-driven programming;general-purpose markup language;json;rich internet application;sensor;server (computing);server-side	Kay-Uwe Schmidt;Roland Stühmer;Ljiljana Stojanovic	2009			rich internet application;the internet;streams;client-side;computer science;data mining;use case;complex event processing	Web+IR	-32.916949059540684	2.6656353514574374	74751
8545f661b5cf60a1d94191ac423d40c0c6e5d944	the role of knowledge modeling techniques in software development: a general approach based on a knowledge management tool	expert systems;road network;building block;knowledge management;traffic control;traffic management;body of knowledge;knowledge modelling;conceptual modelling;object oriented;knowledge structure;knowledge acquisition;software development;knowledge modeling	The aim of the paper is to discuss the use of knowledge models to formulate general applications. First, the paper presents the recent evolution of the software field where increasing attention is paid to conceptual modeling. Then, the current state of knowledge modeling techniques is described where increased reliability is available through the modern knowledge acquisition techniques and supporting tools. The KSM (Knowledge Structure Manager) tool is described next. First, the concept of knowledge area is introduced as a building block where methods to perform a collection of tasks are included together with the bodies of knowledge providing the basic methods to perform the basic tasks. Then, the CONCEL language to define vocabularies of domains and the LINK language for methods formulation are introduced. Finally, the object oriented implementation of a knowledge area is described and a general methodology for application design and maintenance supported by KSM is proposed. To illustrate the concepts and methods, an example of system for intelligent traffic management in a road network is described. This example is followed by a proposal of generalization for reuse of the resulting architecture. Finally, some concluding comments are proposed about the feasibility of using the knowledge modeling tools and methods for general application design.	integrated development environment;kernel same-page merging;knowledge acquisition;knowledge management;knowledge modeling;knowledge representation and reasoning;point of view (computer hardware company);reyes rendering;software development;vocabulary	José Cuena;Martin Molina	2000	Int. J. Hum.-Comput. Stud.	10.1006/ijhc.1999.0232	knowledge base;active traffic management;knowledge integration;software mining;computer science;knowledge management;artificial intelligence;software development;body of knowledge;knowledge-based systems;knowledge engineering;open knowledge base connectivity;management science;procedural knowledge;knowledge extraction;object-oriented programming;expert system;domain knowledge	AI	-31.636148360051585	-4.17372541639431	75417
9c671ea1fa708586164ada274c2e2bba3d8ad92a	plan generation in robotics: state of the art and perspectives	distributed system;computer program;systeme reparti;articulo sintesis;article synthese;base connaissance;ejecucion programa;robotics;program execution;planificacion;sistema repartido;execution programme;robotica;base conocimiento;planning;robotique;planification;review;programa computador;supervision;programme ordinateur;knowledge base	Abstract   In this paper we have endeavored to present the state of the art of Automatic Plan Generation, with special emphasis on its application to Robotics, as well as some personal perspectives on the topics covered.  Thus, we begin by presenting a panoramic view of the first attempts at plan generation, but as yet, not connected with robotics. Further, a survey was made of the main problems to be met when it is intended to plan for a real executor in a real world.  Next, the case of distributed robotic systems and its implications on plan generation is examined. The question of time and its explicit representation is also looked upon.  Finally, a view on learning and specialized plan generators is also given.	robotics	Luis M. Camarinha-Matos	1987	Robotics	10.1016/0167-8493(87)90050-7	planning;knowledge base;simulation;computer science;engineering;artificial intelligence;robotic paradigms;robotics	Robotics	-25.17971451398906	-4.4492272672695306	75525
85333076567a9e329720b00b54c74c4eb0f7e325	xml full-text search: challenges and opportunities	query language;information retrieval;xml document;structured data	An ever growing number of XML repositories are being made available for search. A lot of activity has been deployed in the past few years to query such repositories. In particular, full-text querying of text-rich XML documents has generated a wealth of issues that are being addressed by both the database (DB) and information retrieval (IR) communities. The DB community has traditionally focused on developing query languages and efficient evaluation algorithms for highly structured data. In contrast, the IR community has focused on searching unstructured data, and has developed various techniques for ranking query results and evaluating their effectiveness. Fortunately, recent trends in DB and IR research demonstrate a growing interest in adopting IR techniques in DBs and vice versa [1, 2, 3, 4, 5, 6, 7, 9]. In the past 5 years, the W3C has been putting a lot of effort in designing the XQuery 1.0 and XPath 2.0 languages that provide powerful primitives to navigate in XML documents. Many database researchers and practitioners have influenced the design of these languages and have been developing XQuery prototypes. On the other hand, in IR, INEX, the INitiative for the Evaluation of XML [8] has been created 3 years ago to put together XML documents to assess scoring and ranking methods for XML that accounts for document structure, in the same manner as TREC was designed for keyword retrieval. Several prototypes participate to INEX each year and the basic query language used within this effort is very similar to XPath. The goal of this proposal is to provide a survey on existing research in XML full-text search in DB and IR including languages, appropriate scoring and ranking methods, implementation architectures and query evaluation algorithms and, summarize open research issues such as the joint optimization of queries on both structure and content. We believe that this tutorial is necessary to drive the atten-	algorithm;database;information retrieval;mathematical optimization;open research;query language;xml;xpath;xquery	Sihem Amer-Yahia;Jayavel Shanmugasundaram	2005			query expansion;xml;data model;computer science;data mining;xml database;database;web search query;xml schema editor;information retrieval;query language;efficient xml interchange	DB	-33.621565232199615	3.5227330776248933	75779
e95a1e2cf8e6f7e322abb3f28f7650016b68fb7b	on the signature tree construction and analysis	database indexing;extraction information;computer aided design;hipertexto;signature trees;signature file scanning;disk signature tree;analisis datos;query processing;information extraction;time complexity;information retrieval;digital library;disc storage;metodo arborescente;interrogation base donnee;query processing tree data structures information retrieval testing multimedia databases application software office automation software libraries multimedia systems indexes;index structure;interrogacion base datos;s trees;estructura archivo;tree data structures;data mining;bit string representation;multimedia systems;interrogacion fichero;information retrieval signature files bit slice files s trees signature trees;signature file;set valued attributes;data analysis;biblioteca electronica;arbre signature;signature tree structure;complex data;fouille donnee;computational complexity;file query;query evaluation;tree structure;estructura datos;structure fichier;file structure;tree structured method;analyse donnee;signature files;structure donnee;arbre s;electronic library;methode arborescente;signature tree construction;bit slice files;tree searching;interrogation fichier;index structure signature tree construction data structure set valued attributes bit string representation signature file signature tree structure signature file scanning query evaluation time complexity signature tree search disk signature tree;signature tree search;data structure;busca dato;hypertexte;database query;extraccion informacion;hypertext;office automation;bibliotheque electronique;tree searching computational complexity database indexing disc storage query processing tree data structures	Advanced database application areas, such as computer aided design, office automation, digital libraries, data-mining, as well as hypertext and multimedia systems, need to handle complex data structures with set-valued attributes, which can be represented as bit strings, called signatures. A set of signatures can be stored in a file, called a signature file. In this paper, we propose a new method to organize a signature file into a tree structure, called a signature tree, to speed up the signature file scanning and query evaluation. In addition, the average time complexity of searching a signature tree is analyzed and how to maintain a signature tree on disk is discussed. We also conducted experiments, which show that the approach of signature trees provides a promising index structure	computer-aided design;data mining;data structure;digital library;experiment;hypertext;library (computing);time complexity;tree structure;type signature	Yangjun Chen;Yibin Chen	2006	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2006.146	time complexity;segment tree;database index;digital library;hypertext;data structure;computer science;theoretical computer science;data mining;database;tree structure;tree;file format;data analysis;programming language;computational complexity theory;world wide web;information extraction;complex data type	DB	-26.968938769984526	3.3233710433603014	75849
05f0b284e63e4653ea1ccaffd3e3a6766f6c7016	the theory and experiments of designing cooperative intelligent systems	multiagent system;systeme intelligent;multi agent system;systeme cooperatif;learning;sistema inteligente;production system;systeme production;intelligence artificielle;algoritmo genetico;sistema produccion;multi agent systems;sistema coordenadas;cooperative systems;agent intelligent;distributed artificial intelligence;intelligent system;intelligent agent;algorithme genetique;artificial intelligence;genetic algorithm;genetic algorithms;agente inteligente;inteligencia artificial;production scheduling;systeme coordonnee;agent coordination;sistema multiagente;cooperative intelligent systems;systeme multiagent;coordinate system	In this paper, we identify the business problems that lend themselves to the design of cooperative intelligent systems and empirically demonstrate the design and application of a multi-agent intelligent system for production scheduling. Our experiments suggest that a multi-agent system where agents coordinate their actions generally performs better than a multiagent system where agents do not coordinate their actions. D 2005 Elsevier B.V. All rights reserved.	agent-based model;artificial intelligence;experiment;multi-agent system;scheduling (computing)	Parag C. Pendharkar	2007	Decision Support Systems	10.1016/j.dss.2005.05.028	simulation;genetic algorithm;intelligent decision support system;computer science;artificial intelligence;multi-agent system	AI	-22.81636477315641	-7.141296262058725	75965
bb671c06fd07c05919e2c138f78476b0f89dbf9c	automated repair of scoring rules in constraint-based recommender systems	constraint based recommender systems;development methods;knowledge acquisition	Constraint-based recommender systems support customers in preference construction processes related to complex products and services. In this context, utility constraints scoring rules play an important role. They determine the order in which items products and services are presented to customers. In many cases utility constraints are faulty, i.e., calculate rankings which are not expected and accepted by marketing and sales experts. The adaptation of these constraints is extremely time-consuming and often an error-prone process. We present an approach to the automated adaptation of utility constraint sets which is based on solutions for nonlinear optimization problems. This approach increases the applicability of constraint-based recommendation technologies by allowing the automated reproduction of example item rankings specified by marketing and sales experts.		Alexander Felfernig;Stefan Schippel;Gerhard Leitner;Florian Reinfrank;Klaus Isak;Monika Mandl;Paul Blazek;Gerald Ninaus	2013	AI Commun.	10.3233/AIC-120543	computer science;knowledge management;machine learning;data mining	AI	-28.717453359868916	-3.940845922049751	76007
cdeccdce0d1a75d3f47b3be1c14c3da46fc4b033	specialized merge processor networks for combined sorted lists	database system;sorted list merging;data processing;computer system architecture;inverted file databases;satisfiability;binary tree networks;pipelined networks;nonnumeric processing;backend processors;full text retrieval systems;indexation;text retrieval;time use;system architecture;network computing;binary tree	In inverted file database systems, index lists consisting of pointers to items within the database are combined to form a list of items which potentially satisfy a user's query. This list merging is similar to the common data processing operation of combining two or more sorted input files to form a sorted output file, and generally represents a large percentage of the computer time used by the retrieval system. Unfortunately, a general purpose digital computer is better suited for complicated numeric processing rather than the simple combining of data. The overhead of adjusting and checking pointers, aligning data, and testing for completion of the operation overwhelm the processing of the data. A specialized processor can perform most of these overhead operations in parallel with the processing of the data, thereby offering speed increases by a factor from 10 to 100 over conventional computers, depending on whether a higher speed memory is used for storing the lists. These processors can also be combined into networks capable of directly forming the result of a complex expression, with another order of magnitude speed increase possible. The programming and operation of these processors and networks is discussed, and comparisons are made with the speed and efficiency of conventional general purpose computers.	acm transactions on database systems;central processing unit;computer;database;global variable;inverted index;overhead (computing);pointer (computer programming);response time (technology)	Lee A. Hollaar	1978	ACM Trans. Database Syst.	10.1145/320263.320281	parallel computing;data processing;binary tree;computer science;theoretical computer science;database;programming language;satisfiability	DB	-28.546880457968385	3.891340012191669	76191
399a51bbb7eec66d5316f7532767a3fb0331a86a	conformance analysis of execution traces with clinical guidelines and basic medical knowledge in answer set programming		Clinical Guidelines (CGs) are developed for specifying the “best” clinical procedures for specific clinical circumstances. However, a CG is executed on a specific patient, with her peculiarities, and in a specific context, with its limitations and constraints. Physicians have to use Basic Medical Knowledge (BMK) in order to adapt the general CG to each specific case, even if the interplay between CGs and the BMK can be very complex. In this paper, we focus on a posteriori analysis of conformance, intended as the adherence of an observed CG execution trace to CG and BMK knowledge. A CG description in the GLARE language is mapped to Answer Set Programming (ASP); the BMK and conformance rules are also represented in ASP, to perform conformance analysis, identifying non-adherence situations to CG and/or BMK, which must ultimately be evaluated by a physician in order to assess whether a trace can be considered as conformant or not.	answer set programming;business process;cg (programming language);conformance testing;correctness (computer science);interaction;ontology (information science);process modeling;tracing (software)	Matteo Spiotta;Alessio Bottrighi;Daniele Theseider Dupré	2013			computer science;data mining;programming language;algorithm	AI	-32.62006994599646	-4.353525948002822	76446
927d0fd297c07ffca5b8b1b08c5b2a3c2d457672	multistrategy learning with introspective meta-explanations	introspective meta-explanations;multistrategy learning;knowledge base	Given an arbitrary learning situation, it is dif ficult to determine the most appropriate learning strategy. The goal of this research is to provide a general representation and processing framework for introspective reasoning for strategy selection. The learning framework for an introspective system is to perform some reasoning task. As it does, the system also records a trace of the reasoning itself, along with the results of such reasoning. If a reasoning failure occurs, the system retrieves and applies an introspective explanation of the failure in order to understand the error and repair the knowledge base. A knowledge structure called a Meta-Explanation Pattern is used to both explain how conclusions are derived and why such conclusions fail. If reasoning is represented in an explicit, declarative manner , the system can examine its own reasoning, analyze its reasoning failures, identify what it needs to learn, and select appropriate learning strategies in order to learn the required knowledge without overreliance on the programmer .	knowledge base;programmer;s/pdif	Michael T. Cox;Ashwin Ram	1992			knowledge representation and reasoning;opportunistic reasoning;case-based reasoning;knowledge base;analytic reasoning;qualitative reasoning;verbal reasoning;computer science;knowledge management;artificial intelligence;adaptive reasoning;model-based reasoning;machine learning;reasoning system;deductive reasoning	AI	-20.375074569044152	-4.813710662551346	76454
aad0cbe5fb2de594463d2783503f2583e2519b3b	efficient computation of a proximity matching in spatial databases	database system;urban planning;query processing;spatial query processing and data mining;road traffic accident;swinburne;weather forecasting;data mining;spatial database;spatial data mining;medical image analysis;database systems;algorithms	Spatial data mining recently emerges from a number of real applications, such as real-estate marketing, urban planning, weather forecasting, medical image analysis, road traffic accident analysis, etc. It demands for efficient solutions for many new, expensive, and complicated problems. In this paper, we investigate a proximity matching problem among  clusters  and  features . The investigation involves proximity relationship measurement between clusters and features. We measure proximity in an average fashion to address possible non-uniform data distribution in a cluster. An efficient algorithm is proposed and evaluated to solve the problem. The algorithm applies a standard multi-step paradigm in combining with novel lower and upper proximity bounds. The algorithm is implemented in several different modes. Our experiment results not only give a comparison among them but also illustrate the efficiency of the algorithm.	computation;spatial database	Xuemin Lin;Xiaomei Zhou;Chengfei Liu	2000	Data Knowl. Eng.	10.1016/S0169-023X(99)00045-2	weather forecasting;computer science;data science;data mining;urban planning;database;spatial database	DB	-27.004367518501205	1.570806694693899	76494
d2f40bbeed45f05df8e17b578a1adabe7c7cb22b	obas: an olap benchmark for analysis services	performance evaluation;benchmark;olap;data warehousing	Some benchmarks have been proposed for helping in the design of data warehouse performance tests, such as TPC-H, which was extended by SSB (Star Schema Benchmark). However, we identified the lack of a benchmark for analysis services (OLAP). In this article, we specify a benchmark for analysis services, called OBAS that extends SSB’s multidimensional schema and creates varying-sized data cubes. It enables the evaluation of analysis services through the specification of running configuration parameters, a set of MDX queries and a set of service performance metrics, i.e. response time, rate of execution and service reliability. Using OBAS, we compared two analysis services that adopt XMLA as communication interface: the SQL Server Analysis Services (SSAS) and the Pentaho Mondrian. The results showed that OBAS helped in the comparison of these services with respect to performance and reliability regarding the number of threads.	.mdx;benchmark (computing);central processing unit;data cube;ibm tivoli storage productivity center;microsoft sql server analysis services (ssas);mondrian olap server;multi-core processor;online analytical processing;parallel database;performance evaluation;query language;response time (technology);scalability;star schema;super smash bros.;xml for analysis	Bruno Edson Martins de Albuquerque Filho;Thiago Luís Lopes Siqueira;Valéria Cesário Times	2013	JIDM		computer science;data mining;database;world wide web	DB	-32.929489478736485	1.664596421715159	76544
97cfec08918ac838351dc7041620f84287e147be	the research of fuzzy segment query under the spatial database	r tree;spatial database;n n query;fuzzy segment;fuzzy segmentation	Recently years, the nearest neighbor (N-N) query in the spatial database contracts much public attentions and is studied widely. Because of the kinds of the query objects, there are generally three types, point to point (p-p), point to segment (p-s) and segment to segment (s-s). Most of them are precise data, and related documents are rich. However, the research about querying fuzzy objects with influential region is seldom and the related documents are few. In this paper, we mainly study the N-N query with the fuzzy segment in spatial database. Four located relations based on fuzzy segments are come up with, and we list the N-N algorithm. The experimental result shows high query efficiency.	algorithm;fuzzy logic;spatial database	Guangjun Song;Zhongxiao Hao;Chunjuan Wang	2010	JCP	10.4304/jcp.5.2.290-297	r-tree;query optimization;web query classification;boolean conjunctive query;computer science;data mining;database;view;spatial database;information retrieval;spatial query	DB	-26.99883798429851	1.2872843876579563	76613
a955235c975360dd5a3b9c4e5477e0f6de278710	manual acquisition of uncountable types in closed worlds (research note)	representacion conocimientos;systeme intelligent;traduccion automatica;adquisicion del conocimiento;ingenierie connaissances;sistema inteligente;tratamiento lenguaje;acquisition connaissance;traduction automatique;language processing;linguistique structurale;knowledge acquisition;traitement langage;linguistica estructural;intelligent system;knowledge representation;structural linguistics;representation connaissances;automatic translation;knowledge engineering	The paper considers the problem of classifying countable-uncountable entities during the process of Knowledge Acquisition (KA) from texts. Since one of the main goals of KA is to identify types, means to distinguish new types, instances and individuals become particularly important. We review briefly related studies to show that the distinction countable-uncountable depends on the considered natural language, context usage and the domain; then countability is a perspective to look at a closed world since there is no universal general taxonomy. Finally we propose an internal ontological solution for mass objects which suits to a project 1  for generation of multilingual Natural Language (NL) explanations from Conceptual Graphs (CG).		Galia Angelova	1998		10.1007/BFb0054926	knowledge representation and reasoning;computer science;artificial intelligence;knowledge engineering;structural linguistics;algorithm	Robotics	-24.14062096224169	-1.8360099058928259	76851
327f8317140255b0444499040f4a4bfc56503dac	forward and backward reasoning in automatic abstracting	basic algorithm;original contribution;novel approach;automatic abstracting;experimental system;new approach;particular attention;basic methodology;abstracting activity	bounded scope dialogue with the user in order to collect his suggestions about the structure and content of the texts to be su~mmrized, and his requirements on the summary to be generated. This information is embedded in two different frameworks called working_ text schema and working summmar~, schema which contain the user's suggestions and requirements, respectively. The schemas will constitute a fundamental input for the following phases of the system operation. The working schemas are defined by the user. under the continuous guidance of the schema builder, through three different activities: choosin~ the most appropriate schema from a library of basic text and summary schemas or from a library of working text and summary schemas which contain the schemas utilised in previous surm~arizing sessions; tuning a selected schema by assigning (or reassigning) same parameters contained in it; defining a fully new (basic) schema. It is understood that working schemas are not requested to be always defined at the same level of detail and completeness; they are allowed to embed more or less information according to the adequacy and richness of the specifications supplied by the user. For both text and summary schemas there exist default values to be utilized when the user is unable or unwilling to supply its own	backward chaining;database schema;embedded system;existential quantification;knowledge representation and reasoning;level of detail;natural deduction;parsing;requirement	Danilo Fum;Giovanni Guida;Carlo Tasso	1982			computer science;artificial intelligence;algorithm	Web+IR	-32.93423651489795	-3.393043891450979	76936
ecac494b1a6b8252a5f40a8187140ee73c21b60e	efficient transformation scheme for indexing continuous queries on rfid streaming data	query index;ecspec model;query processing;information technology;continuous query;radiofrequency identification indexing media streaming middleware query processing;epcglobal;radiofrequency identification transformation scheme rfid streaming data rfid middleware query index epcglobal event cycle specification model ecspec model;performance improvement;indexing;event cycle specification model;indexation;indexing radiofrequency identification middleware information filtering information filters aggregates sensor phenomena and characterization data engineering internet application software;media streaming;middleware;experimental evaluation;data consistency;radiofrequency identification;rfid streaming data;rfid middleware;transformation scheme	RFID data are usually regarded as streaming data that are huge and change frequently because they are gathered continuously by numerous readers. RFID middleware filters the data acquired from the readers to process queries from applications. To enhance the performance of the middleware, an index must be built to process the queries efficiently. Several approaches to building an index on queries rather than data records, called query index, have been proposed and are widely used to evaluate continuous queries over streaming data. EPCglobal proposed an event cycle specification (ECSpec) model, which is a standard query interface for RFID applications. The problem with using any of the query indexes in the ECSpec model is that it takes a long time to build the index and to process queries because each data consist of a large number of segments. To solve this problem, we propose an aggregate transformation that converts a group of segments into compressed data. Then, we measure the performance improvement of the index by the proposed technique using experimental evaluation.	aggregate data;congruence of squares;conjunctive query;data compression;experiment;information retrieval;middleware;r* tree;r+ tree;r-tree;radio-frequency identification;stream (computing);streaming media	Jaekwan Park;Bonghee Hong;ChaeHoon Ban	2007	2007 Second International Conference on Systems and Networks Communications (ICSNC 2007)	10.1109/ICSNC.2007.33	search engine indexing;computer science;operating system;middleware;data mining;database;data consistency;information technology;world wide web;computer network	DB	-27.055639182781384	-0.9294830224721017	76943
82d1b76f5b6d193bef6262df57b2ba930a04283f	automatical knowledge representation of logical relations by dynamical neural network			artificial neural network;dynamical system;knowledge representation and reasoning;logical relations	Gang Wang	2017	J. Intelligent Systems	10.1515/jisys-2016-0101	discrete mathematics;artificial intelligence;machine learning	AI	-27.505855075200124	-8.116717341038466	77212
2977a828eb582cb96ff02a30c2294c4a03d5a2b4	integrated uncertainty in knowledge modelling and decision making					2013		10.1007/978-3-642-39515-4		AI	-30.955494928332378	-9.50666262506088	77217
939067ff9d2963264d837596507456e38385110d	using clp to characterise linguistic lattice boundaries in a text mining process	extraction information;programmation logique avec contrainte;linguistique;donnee textuelle;analisis datos;information extraction;text mining;dato textual;programacion logica con restriccion;fouille texte;data mining;semistructured data;data analysis;linguistica;dato semi estructurado;fouille donnee;textual data;analyse donnee;constraint logic programming;structured documents;busca dato;extraccion informacion;donnee semistructuree;linguistics	Applications of Constraint Satisfaction and Programming to Computer Security Morning (9:00 – 13:00) Llevant 1 Constraint Solving under Change and Uncertainty Morning (9:00 – 13:00) Mestral 3 Cooperative Solvers in Constraint Programming Morning (9:00 – 10:45) Llevant 5 Distributed and Speculative Constraint Processing Morning (9:00 – 13:00) Mestral 1 Interval Analysis, Constraint Propagation, Applications Morning (9:00 – 13:00) Mestral 2 Modelling and Reformulating CSPs Morning (9:00 – 13:00) Llevant 4	computer security;constraint programming;constraint satisfaction problem;interval arithmetic;local consistency;software propagation;speculative execution;text mining	Alexandre S. Saidi	2005		10.1007/11562931_37	constraint logic programming;natural language processing;text mining;computer science;data mining;database;data analysis;information extraction	AI	-27.992489280067804	-3.012739051646691	77278
3d8577f59d77a85080134f29f22dee5b79bb3aa4	efficiently support concurrent queries in multiuser cbir systems	data sharing;query processing;index structure;category search;target search;index structures;content based image retrieval;relevance feedback	Various techniques have been developed for different query types in content-based image retrieval systems such as sampling queries, constrained sampling queries, multiple constrained sampling queries, k-NN queries, constrained k-NN queries, and multiple localized k-NN queries. In this paper, we propose a generalized query model suitable for expressing queries of different types, and investigate efficient processing techniques for this new framework. We exploit sequential access and data sharing by developing new storage and query processing techniques to leverage inter-query concurrency. Our experimental results, based on the Corel dataset, indicate that the proposed optimization can significantly reduce average response time in a multiuser environment, and achieve better retrieval precision and recall compared to two recent techniques.	computation;concurrency (computer science);content-based image retrieval;corel linux;database;experiment;k-nearest neighbors algorithm;mathematical optimization;multi-user;precision and recall;prototype;response time (technology);sampling (signal processing);scalability;sequential access;software deployment;unified framework;virtual community;web search query	Danzhou Liu;Kien A. Hua;Ning Yu	2008	Multimedia Tools and Applications	10.1007/s11042-008-0244-x	query expansion;computer science;data mining;database;information retrieval;query language;spatial query	DB	-30.5776540375572	3.3748989970021577	77844
757505d968ef0742babca3c602287599e66d6bb8	an early warning system for vehicle related quality data	extraction information;early warning system;automovil;analisis datos;information extraction;garantie contre risque;gestion production;porcentaje falla;surveillance;unidad control;distributed computing;taux defaillance;almacen dato;unite controle;data mining;production management;grid;data analysis;vigilancia;monitoring;fouille donnee;automobile;rejilla;seasonality;gestion produccion;motor car;warranty;control unit;grille;calidad produccion;calculo repartido;failure rate;analyse donnee;garantia contra riesgo;sequence analysis;audicion;monitorage;entrepot donnee;reseau neuronal;data warehouse;monitoreo;qualite production;grid computing;production quality;early detection;busca dato;calcul reparti;audit;extraccion informacion;red neuronal;neural network	S T R AT E G Y, P R O C E S S & T R A N S F O R M AT I O N | C U S T O M E R R E L AT I O N S H I P M A N A G E M E N T | S U P P LY C H A I N M A N A G E M E N T E N T E R P R I S E S O L U T I O N S | T E C H N O L O G Y I N F R A S T R U C T U R E & I N T E G R AT I O N | M A N A G E D S E RV I C E S spend on warranty claims and vehicle recalls each year—costs that can be avoided or more effectively managed with a robust quality information system.	information system	Matthias Grabert;Markus Prechtel;Tomas Hrycej;Winfried Günther	2004		10.1007/978-3-540-30185-1_10	computer science;data warehouse;sequence analysis;failure rate;data mining;data analysis;grid;audit;operations research;computer security;information extraction;control unit;artificial neural network;grid computing;seasonality	Robotics	-26.063688779813706	-4.621574622974646	77885
9be90b9e6a7f81f1649376e828f93744c799cf32	a cost model for an adaptive cell-based index structure	arbre r;modelizacion;metodo adaptativo;storage access;analisis datos;structure cellulaire;base donnee temporelle;index structure;methode adaptative;cost analysis;r tree;analisis costo;modelisation;arbol r;data analysis;analyse cout;cell structure;spatio temporal data;adaptive method;acces memoire;acceso memoria;analyse donnee;temporal databases;estructura celular;modeling;cost model	In this paper, we describe a cost model for an adaptive cellbased index structure which aims at efficient management of immense amounts of spatio-temporal data. We first survey various methods to estimate the performance of R-tree variants. Then, we present our cost model which accurately estimates the number of disk accesses for the adaptive cell-based index structure. To show the accuracy of our model, we perform a detailed analysis using various data sets. The experimental result shows that our model has the average error ratio from 7% to 13%.	aim alliance;analysis of algorithms;bit error rate;database index;effective method;experiment;mathematical optimization;microsoft windows;query optimization;query plan;r-tree	Wonik Choi;Jinseok Chae;Nam Joong Kim;Mee Young Sung	2006		10.1007/11902140_81	r-tree;systems modeling;computer science;cost–benefit analysis;artificial intelligence;database;temporal database;data analysis;algorithm	DB	-26.021789707036472	2.4447755869298207	77997
bc6e9098c1d0364f15faf0053d116766233e6503	deadlines, travel time, and robot problem solving	travel time;problem solving;partial order	1 Abstract This paper describes some extensions to the reductionist planning paradigm typified by Sacerdoti's NOAH program. Certain inadequacies of the partial ordering scheme used in NOAH are pointed out and a new architecture is detailed which circumvents these problems. An example from the semi-automated factory domain is used to illustrate features of the new planner. Techniques for eliminating unnecessary travel time by the robot and avoiding backtracking due to deadline failures are discussed and their incorporation in the planner is described. 2 Introduction Most planners since NOAH [8] have represented a plan for a set of tasks as a partial (ie., non-linear) ordering of the steps required for carrying out those tasks. NOAH and its successors (eg., NONLIN [9] and DEVISER [10]) employ a partially ordered network of tasks to avoid early and unnecessary commitment to task orderings. The motivation for this is to eliminate backtrack-ing. However, maintaining a consistent partial order is difficult in domains where the fact that tasks actually take time plays an important role: domains in which deadlines or robot travel time are serious considerations. That a partial order leads unavoidably to a deadline failure usually cannot be discovered until an attempt is made to linearize the ordering. However, failure to notice a deadline violation early will require backtracking later. In addition, planning with a partial order is not well suited to efficiently managing factors like travel or machine running time. It's not difficult to represent that moving from one workstation to another takes time proportional to the distance separating the two workstations. However, generating a plan that eliminates unnecessary travel between workstations requires exploring some of the linearized task orderings. It is not until tasks are completely ordered that the source and destination workstations of each movement can be known and the travel time computed with any accuracy. In this paper we describe an approach to planning that combines the use of a partial order with a method for exploring the possible repercussions of that partial order. This approach has been implemented in the FORBIN planner (First Order RoBot INtender). FORBIN is a planner capable of solving a significantly wider class of problems than any of its predecessors. 2.1 The Factory Domain One problem domain that has been used for exploring our approach to planning is what we refer to as the semi-automated factory. In this domain a mobile robot operator wanders about the …	backtracking;interaction;mobile robot;nonlinear system;problem domain;problem solving;programming paradigm;reductionism;semantics (computer science);semiconductor industry;temporal database;time complexity;workstation	David P. Miller;R. James Firby;Thomas Dean	1985			partially ordered set;mathematical optimization;real-time computing;simulation	AI	-19.34349462276237	-4.627551394373496	78151
0d210bcde8ec347d9205a3272e331c866fc1b004	natural language communication between human and artificial agents	distributed system;lenguaje natural;communication process;interfase usuario;langage artificiel;representacion conocimientos;ontologie;linguistique;architecture systeme;systeme reparti;superviseur;user interface;metodo formal;langage naturel;interrogation base donnee;methode formelle;logica descripcion;interrogacion base datos;semantics;lenguaje artificial;interior design;agent logiciel;semantica;semantique;desarrollo verbal;formal method;software agents;proceso comunicacion;processus communication;planificacion;linguistica;sistema repartido;supervisor;agent intelligent;natural language;language development;intelligent agent;household;representation connaissance;menage;developpement verbal;agent systems;human agent interaction;natural language interface;ontologia;arquitectura sistema;interface utilisateur;planning;agente inteligente;information system;description logic;planification;artificial language;knowledge representation;system architecture;familia;ontology;database query;systeme information;logique description;sistema informacion;linguistics	The use of a complex form of language for the purpose of communication with others, to exchange ideas and thoughts, to express statements, wishes, goals, and plans, and to issue questions, commands and instructions, is one of the most important and distinguishing aspects humankind. If artificial agents want to participate in a co-operative way in human-agent interaction, they must be able to a certain degree to understand and interpret natural language, and translate commands and questions of a human user and map them onto a suitable set of their own primitive actions. In this paper, we outline a general framework and architecture for the development of natural language interfaces for artificial agents. We focus in this paper on task-related communication, in a scenario where the artificial agent performs actions in a co-operative work setting with human partners, who serve as instructors or supervisors. The main aim of this research is to provide a consistent and coherent formal framework for the representation of actions, which can be used for planning, reasoning, and action execution by the artificial agent, and at the same time can serve as a basis for analyzing and generating verbal expressions, i.e. commands, instructions and queries, issued by the human user. The suggested framework is derived from formal methods in knowledge representation, in particular description logics, and involves semantic and ontological descriptive elements taken from linguistics, computer science, and philosophy. Several prototypical agent systems have been developed based on this framework, including simulated agents like an interior design system and a household robot, and a speech controlled toy car as example of a physical agent.	coherence (physics);computer science;description logic;domestic robot;executable;formal methods;framenet;gene ontology term enrichment;intelligent agent;knowledge representation and reasoning;natural language;precondition;prototype;statement (computer science);tree (data structure)	Christel Kemke	2006		10.1007/11802372_11	planning;constructed language;interior design;description logic;natural language user interface;computer science;artificial intelligence;software agent;ontology;semantics;natural language;user interface;intelligent agent;information system;algorithm	AI	-24.46991307153449	-8.108709090109516	78163
7e4265969de603c328c856224524d86674c877e4	a qualitative modeling shell for process diagnosis	control systems;medical control systems diseases knowledge engineering taxonomy human factors design engineering control systems medical diagnosis biomedical engineering research and development;design engineering;rule based;research and development;human factors;biomedical engineering;taxonomy;diseases;qualitative modeling;medical diagnosis;medical control systems;knowledge base;knowledge engineering	Removing the knowledge base from a medical diagnosis system, but retaining the rule base, led to a reusable expert shell. This article evaluates Caster, an application to sandcasting.	knowledge base;rule-based system	Timothy F. Thompson;William J. Clancey	1986	IEEE Software	10.1109/MS.1986.232783	knowledge base;health systems engineering;system of systems engineering;computer science;systems engineering;engineering;knowledge management;knowledge engineering;medical diagnosis;biological engineering;taxonomy	Vision	-29.310435383005665	-6.7287298993022775	78422
12c883d88677ecff1ea1217c5384bde7c64c6855	a mib textual convention for language tags		This MIB module defines a textual convention to represent BCP 47 language tags. The intent is that this textual convention will be imported and used in MIB modules that would otherwise define their own representation. Table of Contents	ietf language tag;mebibyte	David McWalter	2007	RFC	10.17487/RFC5131	natural language processing;speech recognition;linguistics	NLP	-28.794845132365747	-2.881621698481062	78446
6cd64f0ee7b83c19b6c836a596069de823f722e9	invariant characters of information systems under some homomorphisms	information systems;rough set theory;superfluous attribute;artificial intelligent;indiscernibility relation;intelligent system;mathematical model;information system;knowledge representation;information system homomorphism	An information system is one of the very important mathematical models in the field of artificial intelligence. The notion of an information system homomorphism as a kind of tool to study the relation between two information systems was introduced in J.W. Grzymala-Busse (Algebraic properties of knowledge representation systems, in: Proceedings of the ACM SIGART International Symposium on Methodologies for Intelligent Systems, Knoxville, 1986, pp. 432–440). In this paper, some invariant characters of an information system under a homomorphism are discussed.	information system	Deyu Li;Yichen Ma	2000	Inf. Sci.	10.1016/S0020-0255(00)00017-7	knowledge representation and reasoning;discrete mathematics;computer science;artificial intelligence;theoretical computer science;machine learning;mathematics;information system;algorithm	DB	-21.780961532030002	-1.467401076737199	78540
d28db283c6fe1b33368a41148879553b440a3213	a scalable distributed data structure for multi-feature similarity search	scalable distributed data structure;similarity search	Similarity search for content-based retrieval (where content can be any combination of text, image, audio/video, etc.) has gained importance in recent years, also because of the advantage of ranking the retrieved results according to their proximity to a query. However, to use similarity search in real world applications, we need to tackle the problem of huge volumes of such mixed multimedia data (e.g., coming from Web sites) and the problem of their distribution on multiple cooperating nodes. The proposed approach is being used in two running projects: SAPIR and NeP4B.In this paper we approach this problem by considering a scenario of a network of autonomous peers maintaining a local collection of metric objects (i.e., mixed mode multimedia content). This network forms a distributed Peer–to–Peer (P2P) search engine for similarity search based on the paradigm of Routing Index. Each peer in the network thus maintains both an index of its local resources and a table for every neighbor, summarizing the objects that are reachable from it. The paper presents techniques that aim to make our P2P similarity-based search system viable, trading approximate results for scalable solutions. Results of simulations that use real collections of images are discussed.	approximation algorithm;autonomous robot;data structure;information retrieval;peer-to-peer;programming paradigm;routing;scalability;similarity search;simulation;web search engine	Claudio Gennaro;Matteo Mordacchini;Salvatore Orlando;Fausto Rabitti	2008			database;information retrieval	Web+IR	-29.801727038401346	-0.7934566790658956	78632
508d279368cbd937287f993a278b9a1292d833b4	high performance data mining using data cubes on parallel computers	distributed memory;data cube;distributed memory systems;interaction analysis;query processing;software performance evaluation;relational database;data mining;multidimensional database;ibm sp2 high performance data mining data cubes relational database online analytical processing data analysis decision support systems multidimensional databases knowledge discovery olap interactive analysis precomputed aggregate calculations query processing distributed memory parallel computers attribute focusing;data analysis;data mining concurrent computing high performance computing multidimensional systems application software data analysis decision support systems databases performance analysis aggregates;deductive databases software performance evaluation knowledge acquisition data structures parallel machines data analysis query processing distributed memory systems relational databases ibm computers;decision support system;parallel systems;data structures;knowledge acquisition;ibm computers;parallel computer;parallel machines;relational databases;efficient query processing;high performance;on line analytical processing;deductive databases;knowledge discovery	On-Line Analytical Processing techniques are used for data analysis and decision support systems. The multidimensionality of the underlying data is well represented by multidimensional databases. For data mining in knowledge discovery, OLAP calculations can be effectively used. For these, high performance parallel systems are required to provide interactive analysis. Precomputed aggregate calculations in a D ta Cubecan provide efficient query processing for OLAP applications. In this article, we present parallel data cube construction on distributed-memory parallel computers from a relational database. Data Cube is used for data mining of associations usingAttribute Focusing. Results are presented for these on the IBM-SP2, which show that our algorithms and techniques are scalable to a large number of processors, providing a high performance platform for such applications.	aggregate data;central processing unit;computer;cubes;data cube;data mining;decision support system;distributed memory;online analytical processing;parallel algorithm;parallel computing;precomputation;relational database;scalability	Sanjay Goil;Alok N. Choudhary	1998		10.1109/IPPS.1998.669979	online analytical processing;computer science;theoretical computer science;data mining;database;data stream mining	DB	-28.854937505142463	2.553928879663287	78836
8e3aebe821a036e2d612dcb99bcc5b23c55c461f	sharkdb: an in-memory column-oriented storage for trajectory analysis	spatial database;trajectory;in-memory;storage	The last decade has witnessed the prevalence of sensor and GPS technologies that produce a high volume of trajectory data representing the motion history of moving objects. However some characteristics of trajectories such as variable lengths and asynchronous sampling rates make it difficult to fit into traditional database systems that are disk-based and tuple-oriented. Motivated by the success of column store and recent development of in-memory databases, we try to explore the potential opportunities of boosting the performance of trajectory data processing by designing a novel trajectory storage within main memory. In contrast to most existing trajectory indexing methods that keep consecutive samples of the same trajectory in the same disk page, we partition the database into frames in which the positions of all moving objects at the same time instant are stored together and aligned in main memory. We found this column-wise storage to be surprisingly well suited for in-memory computing since most frames can be stored in highly compressed form, which is pivotal for increasing the memory throughput and reducing CPU-cache miss. The independence between frames also makes them natural working units when parallelizing data processing on a multi-core environment. Lastly we run a variety of common trajectory queries on both real and synthetic datasets in order to demonstrate advantages and study the limitations of our proposed storage.	automatic parallelization;cpu cache;central processing unit;column-oriented dbms;computer data storage;computer memory;global positioning system;in-memory database;in-memory processing;multi-core processor;page cache;random-access memory;sampling (signal processing);synthetic intelligence;throughput	Bolong Zheng;Haozhou Wang;Kai Zheng;Han Su;Kuien Liu;Shuo Shang	2017	World Wide Web	10.1007/s11280-017-0466-9	throughput;computer science;boosting (machine learning);data mining;asynchronous communication;spatial database;global positioning system;search engine indexing;trajectory;data processing	DB	-28.197148539002367	-0.9525372200280205	79146
d1c47ea0f0be83bbe172764f68054d9e51a28559	analysis of multi-agent activity using petri nets	multiagent system;activity analysis;reconnaissance geste;niveau activite;red petri;multi agent activity analysis;probabilistic approach;trajectories;probabilistic model;enfoque probabilista;approche probabiliste;basketball analysis;temporal properties;activity levels;sistema multiagente;petri net;gesture recognition;activity recognition and evaluation;domain specificity;reseau petri;systeme multiagent;activity recognition	This paper presents the use of place/transition petri nets (PNs) for the recognition and evaluation of complex multi-agent activities. The PNs were built automatically from the activity templates that are routinely used by experts to encode domain-specific knowledge. The PNs were built in such a way that they encoded the complex temporal relations between the individual activity actions. We extended the original PN formalism to handle the propagation of evidence using net tokens. The evaluation of the spatial and temporal properties of the actions was carried out using trajectory-based action detectors and probabilistic models of the action durations. The presented approach was evaluated using several examples of real basketball activities. The obtained experimental results suggest that this approach can be used to determine the type of activity that a team has performed as well as the stage at which the	action potential;agent-based model;closed-circuit television;design review (u.s. government);encode;experiment;multi-agent system;petri net;quantitative structure–activity relationship;robustness (computer science);semantics (computer science);sensor;software performance testing;software propagation	Matej Perse;Matej Kristan;Janez Pers;Gasper Music;Goran Vuckovic;Stanislav Kovacic	2010	Pattern Recognition	10.1016/j.patcog.2009.11.011	statistical model;computer vision;simulation;computer science;artificial intelligence;trajectory;gesture recognition;petri net;statistics;activity recognition	Vision	-22.172098287156558	-7.660962842472209	79360
e23fd38ba9cf851d078918c8b6e4be37c103fb54	review of logical foundations of artificial intelligence	artificial intelligent		artificial intelligence	Drew McDermott	1989	AI Magazine			AI	-27.83663191929789	-9.0849822592233	79377
cdb257a94545997da2f10fd343e26c2cf571e2bc	benchmarking simple database operations	database system;distributed networks;query optimization;object oriented;number of factors;database management system	There are two widely-known benchmarks for database management systems the TP1 benchmarks (Anon et al [1985]), designed to measure transaction throughout, and the Wisconsin benchmarks (Bitton, Dewitt, & Turbyfil [1984]), designed to measure the performance of a relational query processor. In our work with databases on engineering workstations, we found neither of these benchmarks a suitable measure for our applications' needs. Instead, our requirements are for response time for simple queries. We propose benchmark measurements to measure response time, specifically designed for the simple, object-oriented queries that engineering database applications perform. We report results from running this benchmark against some database systems we use ourselves, and provide enough detail for others to reproduce the benchmark measurements on other relational, object-oriented, or specialized database systems. We discuss a number of factors that make an order of magnitude improvement in benchmark performance caching the entire database in main memory, avoiding query optimization overhead, using physical links for prejoins, and using an alternative to the generally-accepted database “server” architecture on distributed networks.	benchmark (computing);computer data storage;database server;mathematical optimization;overhead (computing);query optimization;relational database;requirement;response time (technology);server (computing);workstation	W. Bradley Rubenstein;M. S. Kubicar;R. G. G. Cattell	1987		10.1145/38713.38754	online aggregation;query optimization;database theory;database server;intelligent database;database transaction;database tuning;relational database;computer science;theoretical computer science;database model;data mining;database;object-oriented programming;view;database schema;distributed database;graph database;alias;database testing;database design	DB	-30.020212114806185	3.2678024713652354	79449
ae34b4c7d06e51f7588eb3305966ddbca5b5a903	a study of the model and algorithms for handling location-dependent continuous queries	busqueda informacion;location problem;replication;probleme localisation;base donnee repartie;life style;architecture systeme;location dependent continuous querying;informatique mobile;distributed database;single zq;utilisateur mobile;information retrieval;continuous query;tree architecture;hierarchical database framework;interrogation base donnee;base repartida dato;interrogacion base datos;mobile computer;replicacion;spatial database;multiple zq;location dependent queries;base donnee hierarchique;recherche information;base donnee spatiale;communication cost;arquitectura sistema;base dato especial;problema localizacion;system architecture;mobile computing;database query;mobile user	Advances in wireless and mobile computing environments allow a mobile user to access a wide range of applications. For example, mobile users may want to retrieve data about unfamiliar places or local life styles related to their location. These queries are called location-dependent queries. Furthermore, a mobile user may be interested in getting the query results repeatedly, which is called location-dependent continuous querying. This continuous query emanating from a mobile user may retrieve information from a single-zone (single-ZQ) or from multiple neighbouring zones (multiple-ZQ). We consider the problem of handling location-dependent continuous queries with the main emphasis on reducing communication costs and making sure that the user gets correct current-query result. The key contributions of this paper include: (1) Proposing a hierarchical database framework (tree architecture and supporting continuous query algorithm) for handling location-dependent continuous queries. (2) Analysing the flexibility of this framework for handling queries related to single-ZQ or multiple-ZQ and propose intelligent selective placement of location-dependent databases. (3) Proposing an intelligent selective replication algorithm to facilitate time- and space-efficient processing of location-dependent continuous queries retrieving single-ZQ information. (4) Demonstrating, using simulation, the significance of our intelligent selective placement and selective replication model in terms of communication cost and storage constraints, considering various types of queries.	algorithm;hierarchical database model;information retrieval;lego digital designer;mobile computing;self-replicating machine;simulation;tree structure	Manish Gupta;Manghui Tu;Latifur Khan;Farokh B. Bastani;I-Ling Yen	2005	Knowledge and Information Systems	10.1007/s10115-005-0196-7	replication;computer science;data mining;database;mobile computing;world wide web;spatial database;spatial query	DB	-26.076118423446193	2.36577471623173	79540
b9cf4d666d65e196901658848bbae394a3f1fb2a	anytime diagnosis using model-based methods for satellite diagnostics	model based reasoning;long period;bottom up;air force research laboratory;top down;data stream;small business innovative research;anytime algorithm;fault detection;intelligent system;satellite telemetry;space vehicles;real time systems;device modeling;knowledge base	"""Many satellite anomalies manifest themselves slowly over time and go undetected until they reach critical and possibly unrecoverable status. Because modern satellite systems are relatively reliable, the ground controller must perform the almost impossible task of attending carefully over long periods of time to telemetry readouts which almost always indicate nominal operation, while maintaining a constant readiness for the onset of failure. Humans are notoriously poor at that type of task. On the other hand, intelligent systems have been successfully used to monitor emerging trends in data streams and have been used successfully for tireless monitoring of manufacturing processes, changes in structure configurations, and deterministic failure patterns. The problem then becomes to develop an """"intelligent"""" real-time system for fault detection, diagnosis, and recovery/resolution of anomalies in satellite telemetry streams. This Model Based Reasoning Diagnostic Engine (MBRDE) is aimed at the combination of adaptive and knowledge-based intelligent methodologies that provides a satellite diagnostic assistant for incorporation into future satellite ground stations. We present a framework for a diagnostic methodology that combines characteristics of model-based reasoning with those of anytime algorithms. We illustrate a bottomup modeling method that constructs a hierarchical device model, and a top-down traversal method that constructs a tree of potential component diagnoses for a given anomaly in the device’s observed outputs. These methods combine to form an innovative framework for providing diagnostic assistance based on model-based principles given any amount of processing time. This work was performed under a Small Business Innovation Research contract No. F30602-94-C-0089, Air Force Research Laboratory, Space Vehicles Directorate, Kirtland AFB, NM."""	anomaly detection;anytime algorithm;consistency model;fault detection and isolation;graphical user interface;humans;model-based reasoning;onset (audio);real-time clock;real-time computing;top-down and bottom-up design;universal quantification	Paul Cobb;Eric S. Yager;Charles Jacobus	1999			knowledge base;real-time computing;simulation;computer science;artificial intelligence;machine learning;top-down and bottom-up design	AI	-19.708903418443875	-6.672473958131699	79630
0f7d6608ccab7391b1d38e9be7524d622d5325a5	automatic knowledge acquisition from subject matter experts	automotive engineering;us army war college;mixed initiative reasoning;expert systems;knowledge bases;software agents knowledge acquisition;mixed initiative;rule learning tool;knowledge styles;software agents;scenario elicitation;reasoning styles;automatic knowledge acquisition;engines;knowledge acquisition;ontologies;subject matter experts;knowledge acquisition knowledge engineering knowledge based systems problem solving expert systems engines ontologies automotive engineering laboratories computer science;subject matter expert;learning agent;computer science;knowledge based systems;problem solving;knowledge engineers;rule refinement tool;knowledge base;knowledge engineering;learning agent automatic knowledge acquisition subject matter experts knowledge bases knowledge engineers scenario elicitation rule learning tool rule refinement tool us army war college mixed initiative reasoning knowledge styles reasoning styles subject matter expert	This paper presents current results in developing a practical approach, methodology and tool, for the development of knowledge bases and agents by subject matter experts, with limited assistance from knowledge engineers. This approach is based on mixed-initiative reasoning that integrates the complementary knowledge and reasoning styles of a subject matter expert and a learning agent, and on a division of responsibilities for those elements of knowledge engineering for which they have the most aptitude. The approach was evaluated at the US Army War College, demonstrating very good results and a high potential for overcoming the knowledge acquisition bottleneck.	knowledge acquisition;knowledge base;knowledge engineer;knowledge engineering;subject matter expert turing test;subject-matter expert;aptitude	Mihai Boicu;Gheorghe Tecuci;Bogdan Stanescu;Dorin Marcu;Cristina Cascaval	2001		10.1109/ICTAI.2001.974450	legal expert system;knowledge base;knowledge integration;computer science;knowledge management;artificial intelligence;data science;body of knowledge;model-based reasoning;machine learning;knowledge engineering;procedural knowledge;knowledge extraction;subject-matter expert;knowledge value chain;domain knowledge	AI	-31.065418676113083	-6.5390083463573365	79848
6468714296eb70cadb0085f078acee9a8b12c4a0	two forms of explanations in computational assumption-based argumentation		Computational Assumption-based Argumentation (CABA) has been introduced to model argumentation with numerical data processing. To realize the “explanation power” of CABA, we study two forms of argumentative explanations, argument explanations and CU explanations representing diagnosis and repair, resp.	computation;level of measurement;numerical analysis;reductionism;tip (unix utility)	Xiuyi Fan;Siyuan Liu;Huiguo Zhang;Chunyan Miao;Cyril Leung	2017			machine learning;computer science;argumentative;argumentation theory;artificial intelligence;natural language processing;data processing	AI	-27.25387576612147	-7.378626295285685	79854
691086cd3f5074150d112256f550031bf18f9468	rank-energy selective query forwarding for distributed search systems	query processing time;large-scale query log;cost-efficient query evaluation;non-local site;rank-energy selective query forwarding;hybrid rank-energy query forwarding;forwarding decision;search system;forwarding query;partial index;non-local result;query forwarding	"""Scaling high-quality, cost-efficient query evaluation is critical to search system performance. Although partial indexes reduce query processing times, result quality may be jeopardized due to exclusion of relevant non-local documents. Selectively forwarding queries between geographically distributed search sites may help. The basic idea of query forwarding is that after a local site receives a query, it determines non-local sites to forward the query to and returns an aggregation of the local and non-local results. Nevertheless, electricity costs remain substantial sources of operating expenses. We present a hybrid rank-energy query forwarding model termed """"RESQ."""" The novel contribution is to simultaneously consider both ranking quality and spatially-temporally varying energy prices when making forwarding decisions. Experiments with a large-scale query log, publicly-available electricity price data, and real search site locations demonstrate that query forwarding under RESQ achieves the result scalability of partial indexes with the cost savings of energy-aware approaches (e.g., an 87% ranking guarantee with a 46% savings in energy costs)."""		Amin Y. Teymorian;Ophir Frieder;Marcus A. Maloof	2013		10.1145/2505515.2505710		Web+IR	-24.682120428047327	3.1138965377344676	79870
a8d8f8d32187e4a3fe5086bd0d29e61e831baaa6	denotatum-based models of knowledge creation for monitoring and evaluating r&d program implementation	graphic form denotatum based model knowledge creation r d program implementation semiotic model development stage generation microprocess expert knowledge frege space knowledge acquisition proactive dictionary evaluation system indicators development;semiotics;cognitive informatics;research and development management knowledge acquisition;research and development management;semiotics knowledge based systems knowledge acquisition knowledge representation cognitive informatics research and development management;dictionaries semantics computers computational modeling knowledge based systems;knowledge acquisition;knowledge representation;knowledge based systems	The paper presents two semiotic models for a description of development stages of indicators, including generation microprocesses of expert knowledge about developed indicators. For the description of stages of these microprocesses, a new notion of “Frege's space” is introduced. A related example is an application of two semiotic models to knowledge acquisition for an expert knowledge base named the proactive dictionary, which is a component of an evaluation system. This dictionary enables experts to fix stages of indicators development, to present the results of developing different variants of indicators in graphic form, to compare and evaluate these variants.	dictionary;frege;knowledge acquisition;knowledge base;semiotics	Igor Zatsman	2012	2012 IEEE 11th International Conference on Cognitive Informatics and Cognitive Computing	10.1109/ICCI-CC.2012.6311191	legal expert system;knowledge base;knowledge integration;computer science;knowledge management;data science;body of knowledge;knowledge-based systems;knowledge engineering;open knowledge base connectivity;data mining;procedural knowledge;knowledge extraction;personal knowledge management;domain knowledge	Robotics	-31.537005818930194	-6.501718503222044	80142
1e51af2cc93de29e43fc23ea270aa20e0483ab2a	an intelligent expert systems' approach to layout decision analysis and design under uncertainty	economic efficiency;soft computing;analysis and design;user preferences;decision analysis;fuzzy logic;research framework;artificial neural network;expert system	This chapter describes an intelligent soft computing based approach to layout decision analysis and design. The solution methodology  involves the use of heuristics, metaheuristics, human intuition as well as soft computing tools like artificial neural networks,  fuzzy logic, and expert systems. The research framework and prototype contribute to the field of intelligent decision making  in layout analysis and design by enabling explicit representation of experts' knowledge, formal modeling of fuzzy user preferences,  and swift generation/manipulation of superior layout alternatives to facilitate the cognitive, ergonomic, and economic efficiency  of layout designers.  	decision analysis;expert system	Abdul Rahim Ahmad;Otman A. Basir;Khaled Hassanein;Shahid Azam	2008		10.1007/978-3-540-76829-6_12	legal expert system;intelligent decision support system;computer science;systems engineering;knowledge management;data mining	EDA	-29.791883879916522	-9.450867271838549	80177
8f021dfb855ffcf2373a79ad11c20917ca4c7249	mapping commonkads knowledge models into prr		This paper aims at supporting the knowledge engineering process by proposing an approach to map CommonKADS knowledge models into specifications based on the Production Rule Representation (PRR) language. This approach starts by proposing a metamodel of CommonKADS knowledge models. We define the concept of inference group, required to perform the mapping transformations, and an algorithm that identifies inference groups automatically. We then proceed to the definition of transformation rules. The latter are applied to map CommonKADS knowledge models into a set of PRR production rulesets, combined with UML activity diagrams.	activity diagram;algorithm;knowledge acquisition and documentation structuring;knowledge engineering;knowledge management;knowledge representation and reasoning;metamodeling;production rule representation;unified modeling language	Nicolas Prat;Jacky Akoka;Isabelle Comyn-Wattiau	2011			data mining;computer science	AI	-31.627985875686495	-5.597944898746913	80217
496ee47d44950af08f20b1d0f70c6d2a47dc8a70	knowledge extraction from intelligent electronic devices	microprocessor;time dependent;decision support;substation;analisis datos;systeme aide decision;rough set theory;analisis decision;knowledge extraction;analyse temporelle;sistema ayuda decision;analisis temporal;decision analysis;network analysis;time analysis;sistema reactivo;data analysis;decision support system;fichier log;fichero actividad;puesto electrico;theorie ensemble approximatif;power system;poste electrique;decouverte connaissance;reactive system;systeme reactif;descubrimiento conocimiento;analyse donnee;microprocesseur;rough set;analyse circuit;microprocesador;analyse decision;log file;analisis circuito;knowledge discovery	Most substations today contain a large number of Intelligent Electronic Devices (IEDs), each of which captures and stores locally measured analogue signals, and monitors the operating status of plant items. A key issue for substation data analysis is the adequacy of our knowledge available to describe certain concepts of power system states. It may happen sometimes that these concepts cannot be classified crisply based on the data/information collected in a substation. The paper therefore describes a relatively new theory based on rough sets to overcome the problem of overwhelming events received at a substation that cannot be crisply defined and for detecting superfluous, conflicting, irrelevant and unnecessary data generated by microprocessor IEDs. It identifies the most significant and meaningful data patterns and presents this concise information to a network or regionally based analysis system for decision support. The operators or engineers can make use of the summary of report to operate and maintain the power system within an appropriate time. The analysis is based on time-dependent event datasets generated from a PSCAD/EMTDC simulation. A 132/11 kV substation network has been simulated and various tests have been performed with a realistic number of variables being logged to evaluate the algorithms.	algorithm;decision support system;microprocessor;relevance;rough set;sensor;simulation;traction substation	Ching-Lai Hor;Peter A. Crossley	2005		10.1007/11427834_4	rough set;decision support system;computer science;artificial intelligence;data mining;knowledge extraction;algorithm	AI	-22.49964334282029	-4.120490204211068	80489
ae590fa7cb9c4f757363d62798bd5ff6a39a6c40	estimating block selectivities for physical database design	base relacional dato;eficacia sistema;selection problem;chemin acces;problema seleccion;metodologia;concepcion sistema;database management systems;sistema informatico;clustering algorithms relational databases costs information retrieval parameter estimation system performance database systems data models indexes np hard problem;index documentation;performance systeme;index;computer system;information access;index selection algorithm;relational database;system performance;methodologie;auxiliary access paths;system design;indice;indexation;primary access path;block selectivity physical database design clustered index primary access path auxiliary access paths index selection algorithm;base donnee relationnelle;acces information;clustered index;acceso informacion;systeme informatique;information system;methodology;physical database design;block selectivity;conception systeme;systeme information;sistema informacion;probleme selection	Access path deployment is a critical issue in physical database design. Access paths typically include a clustered index as the primary access path and a set of secondary indexes as auxiliary access paths. To deploy the right access paths requires an effective algorithm and accurate estimation of the parameters used by the algorithm. One parameter central to any index-selection algorithm is the block selectivity of a query. Existing methods for estimating block selectivities are limited by restrictive assumptions. Furthermore, most existing methods produce estimates useful for aiding the selection of secondary indexes only. Little research has been done in the area of estimating block selectivities for supporting the selection of the clustered index. The paper presents a set of methods that do not depend on any specific assumption, produce accurate estimates, and can be used to aid in selecting the clustered index as well as secondary indexes. >	database design	Pai-Cheng Chu	1992	IEEE Trans. Knowl. Data Eng.	10.1109/69.124900	database index;computer science;data mining;database;computer performance	DB	-26.943394334967504	4.074030420744072	80494
b835e5d2c56ef517b74c02fc98c378ce14a5d490	adaptive optimization of very large join queries		The use of business intelligence tools and other means to generate queries has led to great variety in the size of join queries. While most queries are reasonably small, join queries with up to a hundred relations are not that exotic anymore, and the distribution of query sizes has an incredible long tail. The largest real-world query that we are aware of accesses more than 4,000 relations. This large spread makes query optimization very challenging. Join ordering is known to be NP-hard, which means that we cannot hope to solve such large problems exactly. On the other hand most queries are much smaller, and there is no reason to sacrifice optimality there. This paper introduces an adaptive optimization framework that is able to solve most common join queries exactly, while simultaneously scaling to queries with thousands of joins. A key component there is a novel search space linearization technique that leads to near-optimal execution plans for large classes of queries. In addition, we describe implementation techniques that are necessary to scale join ordering algorithms to these extremely large queries. Extensive experiments with over 10 different approaches show that the new adaptive approach proposed here performs excellent over a huge spectrum of query sizes, and produces optimal or near-optimal solutions for most common queries.	adaptive optimization;algorithm;davis–putnam algorithm;dynamic programming;experiment;greedy algorithm;image scaling;iterative method;long tail;lookup table;mathematical optimization;np-hardness;optimization problem;program optimization;query optimization;speedup	Thomas Neumann;Bernhard Radke	2018		10.1145/3183713.3183733	computer science;data mining;scaling;theoretical computer science;adaptive optimization;query optimization;business intelligence;linearization;joins	DB	-30.850820494598906	1.812168441239104	80546
9539df224916d287ddf0954a11fd0a0f5399518c	distributed spatial k-dominant skyline maintenance using computational object preservation		The existing k-dominant skyline solutions are restricted to centralized query processors, limiting scalability, and imposing a single point of failure. To overcome those problems in this paper, we propose the computation and maintenance algorithms for spatial k-dominant skyline query processing in large-scale distributed environment. Where the underlying dataset is partitioned into geographically distant computing core (personal computer) that are connected to the coordinator (server). Our proposed techniques preserve the spatial k-dominant computation object itself into a serialized form. This preservation is done in clientâ€™s core after completing a computational job successfully. When the issue of maintenance comes in action, preserve data object retrieves and use for computation. This procedure eliminates the necessity of intermediate re-send and re-computation of k-dominant skyline for the maintenance issue. Thus, we quantify the gain of data transferring consecutively into different cores to maximize the overall gain as well as the query or balancing the load on different cores fairly. Extensive performance study shows that proposed algorithms are efficient and robust to different data distributions. Â	computation	Md. Anisuzzaman Siddique;Asif Zaman;Md. Mahbubul Islam;Yasuhiko Morimoto	2013	IJNC		computer science;data mining;database;distributed computing	Robotics	-24.52101470738743	2.5904831232231618	80607
8b61ae5cbf4f88c0a308bcbaf7f60427b49fd6b1	a bdi approach to infer students emotions	teoria cognitiva;model based reasoning;raisonnement base sur modele;diagnostico;cognitive theory;intelligence artificielle;theorie cognitive;educational environment;emotion emotionality;artificial intelligence;emotion emotivite;profitability;emocion emotividad;inteligencia artificial;diagnosis;diagnostic	In this article we describe the use of mental stat e pproach, more specifically the BDI model, to implement the process of affective diagno sis in an educational environment. We use the psychological OCC model, which is based on the cogn itive theory of emotions and is possible to be implemented computationally, in order to infer the learner’s emotions from his actions in the system interface. In our work we profit from the reasoning capacity of the BDI approach in order to infer the student’s appraisal (a cognitive evaluation of a pe rson that elicits an emotion), which allows us to d educe student’s emotions. The system reasons about an emo tion-generating situation and tries to infer the us r’s emotion by using the OCC model. Besides, the BDI ap proach is very adequate since the emotions have a dynamic nature. Key-words: architectures for educational technology system, distance education and telelearning, human-computer interface, intelligent tutoring syst ems, interactive learning environments, media in education.	cognition;emo welzl;human–computer interaction;mental state;optimistic concurrency control;pedagogical agent;real-time clock;speech synthesis	Patrícia Augustin Jaques;Rosa Maria Vicari	2004		10.1007/978-3-540-30498-2_90	computer science;artificial intelligence;model-based reasoning;medical diagnosis;profitability index	NLP	-24.555653167221852	-6.90112749786721	80640
86b975a70905b71ac94594bd14d49b86ba855734	optimizing information agents by selectively materializing data	information agent	There is currently great interest in building information gathering agents that provide integrated access to data from a number of distributed Web sources. Some of the prominent research projects in this area include The Internet Softbot, Information Manifold, InfoMaster and InfoSleuth. A critical problem faced by such agents is a high query response time, due to the fact that a lot of data from Web sources has to be retrieved, extracted, and integrated to answer queries. The aim of this work is to develop an approach for improving query response time in information agents. The resulting system is being developed as part of the Ariadne project (Knoblock et al. 1998). Our approach to optimizing an agent is to selectively materialize data from the different sources being integrated. In order to speed up query response time we materialize information that is frequently queried or used to construct answers to user queries. A key problem is to determine the classes of information that must be materialized. There are several factors that must be taken into account in order to determine such classes, including frequency of access by users, cost of getting the information from the actual Web sources, space required to store the data, and the frequency of updates at the original Web sources. We have developed an algorithm that identifies classes of information to materialize by analyzing patterns in user queries. An important constraint is that the number of information classes materialized must be kept small. This is because ach class materialized is defined as another information source that the agent call access. Having a large number of such sources will create performance problems for the query planner that must generate plans to retrieve information requested by the user from the various sources being integrated. Our strategy is to replace whenever possible, a number of fragmented information classes in which all the data is frequently accessed, by a single class that covers the data in all these classes and possibly some extra data not accessed at all. The advantage is that in this manner we are able to keep the number of materialized classes small. This algorithm thus attempts to cover	algorithm;association for computers and the humanities;information source;materialized view;optimizing compiler;response time (technology)	Naveen Ashish	1998			computer science;data mining;database;world wide web	DB	-30.625757400322584	-0.7598783405899889	80824
bb1b66f2fd939004de6b335fd15a0791f9c6566c	seeking evidence for basing the cs theory course on non-decision problems (abstract only)	computability theory;complexity theory;theory course;decision problems	Computational and complexity theory are core components of the computer science curriculum, and in the vast majority of cases they are taught using decision problems as the main paradigm. For experienced practitioners, decision problems are the best tool. But for undergraduates encountering the material for the first time, non-decision problems (such as optimization problems and search problems) may be preferable. This lightning talk will give a brief pointer to some new technical definitions and pedagogical strategies that have been used successfully for teaching the theory course using non-decision problems as the central concept. For example, instead of the familiar complexity classes P and NP, we can define analogous classes of non-decision problems, Poly and NPoly. The key question behind this lightning talk is to ask whether the new definitions and strategies are actually beneficial. Anecdotal evidence and certain theories of learning suggest the new approach should result in superior learning outcomes for students. We are seeking ideas, feedback, and collaborators interested in investigating this hypothesis and obtaining stronger evidence for it. To summarize, our central question is: how can we investigate whether students gain a superior grasp of computational and complexity theory when they are taught primarily using non-decision problems?	computation;computational complexity theory;computer science;decision problem;feedback;mathematical optimization;np (complexity);optimization problem;p (complexity);p versus np problem;pointer (computer programming);programming paradigm;search problem	John MacCormick	2017		10.1145/3017680.3022388	computability theory;computer science;artificial intelligence;decision problem;algorithm;pedagogy	Theory	-22.44312024341027	1.2031177309666732	80986
07b3f566ce5491e57a0ba46d346ec9a414ac7cda	benchmarking fulltext search performance of rdf stores	performance evaluation;indexing and retrieval;data model;performance improvement	More and more applications use the RDF framework as their data model and RDF stores to index and retrieve their data. Many of these applications require both structured queries as well as fulltext search. SPARQL addresses the first requirement in a standardized way, while fulltext search is provided by store-specific implementations. RDF benchmarks enable developers to compare structured query performance of different stores, but for fulltext search on RDF data no such benchmarks and comparisons exist so far. In this paper, we extend the LUBM benchmark with synthetic scalable fulltext data and corresponding queries for fulltext-related query performance evaluation. Based on the extended benchmark, we provide a detailed comparison of fulltext search features and performance of the most widely used RDF stores. Results show interesting RDF store insights for basic fulltext queries (classic IR queries) as well as hybrid queries (structured and fulltext queries). Our results are not only valuable for selecting the right RDF store for specific applications, but also reveal the need for performance improvements for certain kinds of queries.	algorithm;benchmark (computing);data model;data store;database;dhrystone;mathematical optimization;nepomuk;performance evaluation;query plan;resource description framework;sparql;scalability;triplestore	Enrico Minack;Wolf Siberski;Wolfgang Nejdl	2009		10.1007/978-3-642-02121-3_10	data model;computer science;sparql;data mining;database;information retrieval;rdf schema	Web+IR	-31.64649562451396	3.161431702038452	81096
8bcaa7d6bda8c251b943ed4790b63d1e74cab154	presentation of arguments and counterarguments for tentative scientific knowledge	outil logiciel;argumentacion;information communication;optimisation;software tool;multiagent system;argumentation;ucl;optimizacion;discovery;theses;conference proceedings;logical programming;communication information;digital web resources;first order;programmation logique;ucl discovery;logique ordre 1;open access;comunicacion informacion;ucl library;optimization;book chapters;open access repository;sistema multiagente;herramienta software;programacion logica;scientific knowledge;first order logic;systeme multiagent;logica orden 1;ucl research	A key goal for a scientist is to find evidence to argue for or against universal statements (in effect first-order formulae) about the world. Building logic-based tools to support this activity could be potentially very useful for scientists to analyse new scientific findings using experimental results and established scientific knowledge. In effect, these logical tools would help scientists to present arguments and counterarguments for tentative scientific knowledge, and to share and discuss these with other scientists. To address this, in this paper, we explain how tentative and established scientific knowledge can be represented in logic, we show how first-order argumentation can be used for analysing scientific knowledge, and we extend our framework for evaluating the degree of conflict arising in scientific knowledge. We also discuss the applicability of recent developments in optimizing the impact and believability of arguments for the intended audience.	first-order predicate	Anthony Hunter	2005		10.1007/11794578_16	computer science;artificial intelligence;first-order logic;descriptive knowledge;database;mathematics;programming language;algorithm	AI	-23.608177948146846	-3.1486656789809486	81157
d2e46a5e2771d2a9665b66298d2c15739306457d	scalable processing of continuous k-nearest neighbor queries with uncertain velocity	k nearest neighbors;continuous k nearest neighbor query;moving object;index structure;spatio temporal queries;moving objects;k nearest neighbor;moving query object	Continuous K-nearest neighbor (CKNN) query is an important type of spatio-temporal queries. Given a time interval [ts, te] and a moving query object q, a CKNN query is to find the K-nearest neighbors (KNNs) of q at each time instant within [ts, te]. In this paper, we focus on the issue of scalable processing of CKNN queries over moving objects with uncertain velocity. Due to the large amount of CKNN queries that need to be evaluated concurrently, efficiently processing such queries inevitably becomes more complicated. We propose an index structure, namely the CI-tree, to predetermine and organize the candidates for each query issued by the user from anywhere and anytime. When the CKNN queries are evaluated, their corresponding candidates can be rapidly retrieved by traversing the CI-tree so that the processing time is greatly reduced. A comprehensive set of experiments is performed to demonstrate the effectiveness and the efficiency of the CI-tree. 2011 Elsevier Ltd. All rights reserved.	anytime algorithm;database;experiment;k-nearest neighbors algorithm;scalability;velocity (software development)	Lien-Fa Lin;Yuan-Ko Huang	2011	Expert Syst. Appl.	10.1016/j.eswa.2011.01.028	computer science;machine learning;data mining;database;k-nearest neighbors algorithm;information retrieval	DB	-26.568752855862687	0.3477943646170633	81428
6112c714fe7a747a2fdb293875efb861e9a4c884	the cat's cradle network	knowledge network;280109;knowledge management;cat s cradle network;pre2009 decision support and group support systems;context	In this paper we will argue that the representation of context in knowledge management is appropriately served by the representation of the knowledge networks in an historicised form. Characterising context as essentially extra to any particular knowledge representation, we argue that another dimension to these be modelled, rather than simply elaborating a form in its own terms. We present the formalism of the cat’s cradle network, and show how it can be represented by an extension of the Pathfinder associative network that includes this temporal dimension, and allows evolutions of understandings to be traced. Grounding its semantics in communities of practice ensures utility and cohesiveness, which is lost when mere externalities of a representation are communicated in fully fledged forms. The scheme is general and subsumes other formalisms for knowledge representation. The cat’s cradle network enables us to model such community-based social constructs as pattern languages, shared memory and patterns of trust and reliance, by placing their establishment in a structure that shows their essential temporality.	cohesion (computer science);knowledge management;knowledge representation and reasoning;norm (social);pattern language;semantics (computer science);shared memory	Diarmuid Pigott;John G. Gammack;Valerie Hobbs	2003	JIKM	10.1142/S0219649203000097	computer science;knowledge management;artificial intelligence;management;algorithm	AI	-24.20722861589084	-9.614894498981512	81848
93a81995c32b70a24a42388c64db71d5d189c8bc	on-line industrial supervision and diagnosis, knowledge level description and experimental results	on line supervision of industrial processes;knowledge based system;plant evolution;commonkads experience model;diagnosis;supervision;historical data;problem solving	This paper presents a detailed description of a knowledge-based system for on-line supervision and diagnosis of industrial continuous processes: TURBOLID. The system has been developed for a Spanish beet sugar factory, and it is in use in two plants. The system supports three main tasks; monitoring, operation mode and diagnosis. A detailed knowledge level account of the systems is presented in this paper, using CommonKADS methodology. The knowledge level description presented here will allow the reuse of the TURBOLID problem solving approach to supervision and diagnosis in other continuous plants, even in other domains. The main purpose of this paper is to illustrate how the basic tasks proposed are able to cope with supervision and diagnosis of a complex plant. Particularly signi®cant are the separation of operation mode and diagnosis, and the approach of TURBOLID to diagnosis, that obtains on-line causal explanations for detected problems. TURBOLID is able to differentiate among competing hypotheses looking for historical data, current data and even future data, tracking the plant evolution. Results of the activity of the system during a working month are also presented. q 2001 Elsevier Science Ltd. All rights reserved.	causal filter;knowledge acquisition and documentation structuring;knowledge level;knowledge-based systems;online and offline;problem solving	Carlos Alonso González;Belarmino Pulido Junquera;G. Acosta Lazo;César Llamas	2001	Expert Syst. Appl.	10.1016/S0957-4174(00)00053-1	simulation;computer science;artificial intelligence;knowledge-based systems;medical diagnosis;plant evolution	AI	-22.534265700612895	-5.0976022083602945	81910
0fad137adf516210e063ab5b0bfa4278538a0b0c	reasoning about conversation protocols in a logic-based agent language	multiagent system;belief;red www;reseau web;service web;conversacion;intelligence artificielle;web service;modal logic;croyance;internet;contexto;logique modale;conversation;contexte;logica modal;artificial intelligence;world wide web;inteligencia artificial;creencia;sistema multiagente;context;systeme multiagent	We present an approach to reasoning about conversations within the framework of a logic-based agent language. Our agent theory is based on a modal logic of actions and beliefs and permits the representation of communicative acts and conversation protocols, allowing agents to reason about them before their execution. The work is framed in a world wide web context, in which we show how reasoning about the interaction with web service providers can be exploited for personalizing the service fruition.	agent-based model;docs (software);hector levesque;ibm thinkpad 310;intelligent agent;lecture notes in computer science;logic programming;logical framework;mental state;modal logic;multi-agent system;situated;software agent;springer (tank);theory;web service;world wide web	Matteo Baldoni;Cristina Baroglio;Alberto Martelli;Viviana Patti	2003		10.1007/978-3-540-39853-0_25	modal logic;web service;the internet;computer science;artificial intelligence;belief;machine learning;database;world wide web;computer security;algorithm	AI	-25.66718667784455	-8.684271441787349	82124
d558b401e5e5119aa0276c68a76a7b1dbc47445f	heuristics for reconciling independent knowledge bases	expert systems;consensus formation;knowledge acquisition;conflict resolution;knowledge base	One of the major unsolved problems in knowledge acquisition is reconciling knowledge originating from different sources. This paper proposes a technique for reconciling knowledge in two independent knowledge bases, describes a working program built to implement that technique, and discusses an exploratory study for validating the technique. The technique is based on the use of heuristics for identifying and resolving discrepancies between the knowledge bases. Each heuristic developed provides detection and resolution procedures for a distinct variety of discrepancy in the knowledge bases. Sample discrepancies include using synonyms for the same term, conflicting rules, and extra reasoning steps. Discrepancies are detected and resolved through the use of circumstantial evidence available from the knowledge bases themselves and by asking sharply focussed questions to the experts responsible for the knowledge bases. The technique was tested on two independently developed knowledge bases designed to aid novice statisticians in diagnosing problems in linear regression models. The heuristics located a significant number of the discrepancies between the knowledge bases and assisted the experts in creating a consensus knowledge base for diagnosing multicollinearity problems. We argue that the task of identifying discrepancies between independent bodies of knowledge is an inevitable part of any large knowledge acquisition effort. Hence the heuristics developed in this work are applicable even when knowledge acquisition is not done by reconciling two complete knowledge bases. We also suggest that our approach can be extended to other knowledge representations such as frames and database schemas, and speculate about its potential application to other domains involving the reconciliation of knowledge, such as requirements determination, negotiation, and design.	heuristic	Andrew Trice;Randall Davis	1993	Information Systems Research	10.1287/isre.4.3.262	knowledge base;computer science;knowledge management;artificial intelligence;body of knowledge;knowledge-based systems;conflict resolution;data mining;procedural knowledge;knowledge extraction;social psychology;domain knowledge	DB	-21.12668690202953	-5.219862481039567	82329
d6d7397c693e1f7320a668e6261b8870d17d29b4	analysis of heuristic reasoning for the visualization of cad quadratic	heuristic reasoning;cad quadratic	"""In this paper we develop a conceptual model for CAD algorithm visualization. We analyze the knowledge requirements necessary and formalize a visualization model called FOVISTA. FOVISTA has four essential """"functional views"""" which emphasize the visualization of the computational behaviour of CAD heuristics, rather than the final result. The aim is to provide a learning and research tool for the design of heuristics. As an example, we consider a VLSI (Very Large Scale Integration) heuristic, that of detailed (switchbox) routing."""	computer-aided design;heuristic	Fillia Makedon;Malgorzata Marek-Sadowska	1989		10.1007/3-540-51142-3_72	computer science;artificial intelligence;theoretical computer science	EDA	-26.300043934394917	-3.351382313491479	82452
ef31db844808be3a7dbcf3e663181ec4efd19d4c	some applications of pedrycz's operator	representacion conocimientos;sistema experto;logique floue;logica difusa;intelligence artificielle;similitude;fuzzy logic;similarity;artificial intelligence;possibility theory;inteligencia artificial;similitud;systeme expert;knowledge representation;representation connaissances;theorie possibilite;expert system	In this paper we give a general form of the expression of Pedrycz's operator and then we show that this expression is useful to generate possibility and necessity measures and also to represent the vague and uncertain facts under the form of other vague and certain facts.		Ionela Iancu	1998	Computers and Artificial Intelligence		fuzzy logic;knowledge representation and reasoning;possibility theory;similarity;computer science;artificial intelligence;similitude;mathematics;expert system;algorithm	AI	-20.69445503609498	-1.545040967512363	82503
22aacd2c6182808f6b4b7efbd2e9920a4c2c6807	multiversion concurrency control for large-scale service directories	formal specification;index structure;multidimensional data;tree data structures;web service;large scale;data storage;search trees;concurrency control large scale systems web services multidimensional systems context aware services artificial intelligence laboratories data storage systems indexing concurrent computing;internet;indexing;web services multiversion concurrency control large scale service directories generalized search tree index structure data storage system multidimensional data handling concurrent read access service oriented computing service composition;service oriented computing;concurrency control;formal specification concurrency control tree searching tree data structures data handling internet indexing;data handling;tree searching	In this paper we describe the implementation of multiversion concurrency control on the generalized search tree (GiST), an index structure introduced by Hellerstein. For large-scale service directories, the need arises for a data storage system capable of handling substantial amounts of multidimensional data efficiently, as well as being able to support queries which are natural to the type of data stored in the directory. The GiST is an indexing structure that lends itself particularly well to this type of application. However, the solutions that have been proposed to address concurrency control on the GiST do not meet the requirements of large-scale service directories. The solution proposed here optimizes towards highly concurrent read accesses that are far more frequent than updates to the stored data.	blocking (computing);computer data storage;concurrency (computer science);data structure;directory (computing);gist;input/output;multiversion concurrency control;non-blocking algorithm;parallel random-access machine;requirement;scalability;search tree;service composability principle;tree structure;web service;webserver directory index;whole earth 'lectronic link	Walter Binder;Ion Constantinescu;Boi Faltings;Samuel Spycher	2005	Third European Conference on Web Services (ECOWS'05)	10.1109/ECOWS.2005.18	web service;search engine indexing;the internet;isolation;computer science;concurrency control;service-oriented architecture;group method of data handling;computer data storage;formal specification;database;distributed computing;multiversion concurrency control;tree;non-lock concurrency control;law;world wide web	DB	-30.18398511288784	1.4012426826660986	82700
63809ee4a64a2456eafa5ab2f652905ec3f12a4e	evaluating dynamic linking through the query process using the licas test platform		A novel linking mechanism has been described previously [4] that can be used to autonomously link sources that provide related answers to queries executed over an information network. The test query platform has now been re-written resulting in essentially a new test platform using the same basic query mechanism, but with a slightly different algorithm. This paper describes recent test results on the same query test process that supports the original findings and also shows the effectiveness of the linking mechanism in a new set of test scenarios.	algorithm;dynamic linker	Kieran Greer	2011	CoRR		query optimization;query expansion;computer science;data mining;database;distributed computing;world wide web	SE	-29.424063266811633	-1.6681109996610042	82739
59f393148b2b88acaa9a715658d1b658252e08ce	case-based reasoning system with petri nets for induction motor fault diagnosis	case base reasoning;induction motor;electric motor;case retrieval;system performance;condition monitoring;induction motor maintenance;case based reasoning;petri net;diagnosis;fault diagnosis	This paper presents an innovative approach for integrating case-based reasoning (CBR) with Petri net for the fault diagnosis of induction motors. In the CBR system, maintenance engineers can retrieve the information from previous cases which closely resemble the new problem and solve the new problem using the information from the previous cases. The proposed system has been used in fault diagnosis of electric motor to confirm the system performance. The result shows the proposed system performs better than the conventional CBR system.	case-based reasoning;petri net;reasoning system	Bo-Suk Yang;Seok Kwon Jeong;Yong-Min Oh;Andy Chit Chiow Tan	2004	Expert Syst. Appl.	10.1016/j.eswa.2004.02.004	electric motor;case-based reasoning;computer science;artificial intelligence;induction motor;computer performance;petri net	AI	-28.955410683379274	-6.010022635916643	82777
0e8058eaf29bf8092bb5da1d5493fd375c74e42b	towards gpu-accelerated web-gis for query-driven visual exploration		Web-GIS has played an important role in supporting accesses, visualization and analysis of geospatial data over the Web for the past two decades. However, most of existing WebGIS software stacks are not able to exploit increasingly available parallel computing power and provide the desired high performance to support more complex applications on large-scale geospatial data. Built on top our past works on developing high-performance spatial query processing techniques on Graphics Processing Units (GPUs), we propose a novel yet practical framework on developing a GPU-accelerated Web-GIS environment to support Query-Driven Visual Explorations (QDVE) on Big Spatial Data. An application case on visually exploring global biodiversity data is presented to demonstrate the feasibility and the efficiency of the proposed framework and related techniques on both the frontend and backend of the prototype system.	central processing unit;database;geographic information system;graphical user interface;graphics processing unit;parallel computing;prototype;spatial query;world wide web	Jianting Zhang;Simin You;Le Gruenwald	2017		10.1007/978-3-319-55998-8_8	database;stack (abstract data type);visualization;geospatial analysis;data mining;graphics;software;spatial analysis;computer science;exploit;spatial query	HPC	-32.80102210419889	0.2259389774243938	82833
4150e658fa08a62a17319eed4c9f27142eb7b8bf	modeling of creation of the complex on intelligent information systems learning and knowledge control		This paper analyzes a design of intelligent information systems learning and knowledge control IISLKC. This paper also identifies the key components and modules and performs functional design and different interaction methods for the task.	information system	Shahnaz N. Shahbazova	2014	Int. J. Intell. Syst.	10.1002/int.21635	intelligent decision support system;computer science;knowledge management;artificial intelligence;machine learning	AI	-30.80778904273954	-7.343216778086071	82889
f8d15aab24904f25bc33e6c5bdaef167c782897f	exploiting patterns of interaction to achieve reactive behavior	medio ambiente;representacion conocimientos;systeme intelligent;architecture systeme;logica temporal;proceso;generic algorithm;implementation;temporal logic;assisted planning;prolog;sistema inteligente;specification;robotics;procedural modeling;interval temporal logic;algebre;reacteur;automatic generation;action;dynamical system;systeme dynamique;ejecucion;especificacion;algebra;tracking environment model;planification automatisee;agent intelligent;planificacion automatizada;identification;environment;intelligent system;intelligent agent;processus;robotica;identificacion;arquitectura sistema;context dependent;environnement;robotique;process;sistema dinamico;accion;knowledge representation;system architecture;process algebra;representation connaissances;planner reactor;logique temporelle;reactor	This paper introduces an approach that allows an agent to exploir inherent patterns of interaction in its environment, so-called dynamics, to achieve its objectives. The approach extends the standard treatment of planning and (re) action in which part of the input to the plan generation algorithm is a set of basic actions and perhaps some domain axioms. Real world actions are typically difficult to categorize consistently and are highly context dependent. The approach presented here takes as input a procedural model of the agent’s environment and produces as output a set of action descriptions that capture how the agent can exploit the dynamics in the environment. An agent constructed with this approach can utilize context sensitive actions, “servo” style actions, and other intuitively efficient ways to manipulate its environment. A process-algebra based representation, 7X, is introduced to model the environment and the agent’s reactions. The paper demonstrates how to analyze an ‘RS environment model so as to automatically generate a set of potentially useful dynamics and convert these to action descriptions. The output action descriptions are designed to be input to an Interval Temporal Logic based planner. A series of examples of reaction construction drawn from the kitig robot domain is worked through, and the prototype implementation of the approach described.	algorithm;automated planning and scheduling;categorization;context-sensitive grammar;interval temporal logic;process calculus;prototype;servo	Damian M. Lyons;Antonius J. Hendriks	1995	Artif. Intell.	10.1016/0004-3702(94)00058-9	identification;knowledge representation and reasoning;process calculus;simulation;genetic algorithm;temporal logic;interval temporal logic;computer science;artificial intelligence;dynamical system;context-dependent memory;natural environment;robotics;implementation;prolog;procedural modeling;specification;intelligent agent;algorithm;process	AI	-23.87592008024663	-6.895031339106157	82903
cd41972b181ce13efc48c7e8c8edc49970b1b578	belief combination and propagation in a lattice-structured interference network	belief;belief function;incertidumbre;uncertainty;rule based system;inference mechanisms;intelligence artificielle;raisonnement;knowledge based systems artificial intelligence inference mechanisms;croyance;belief propagation;chaining syllogism artificial intelligence interference network belief combination dempster shafer rule based systems interpretations uncertainty belief propagation;inferencia;razonamiento;dempster shafer;artificial intelligence;incertitude;inteligencia artificial;reasoning;intelligent networks interference belief propagation uncertainty robustness bayesian methods knowledge based systems interpolation expert systems;creencia;knowledge based systems;inference	Belief propagation and belief combination procedures based on the Dempster-Shafer belief function for inference in rule-based systems is proposed. The belief combination procedure yields results identical to those of Dempster's rule when pieces of evidence are independent. Dempster's rule is shown to be nonrobust for combining evidence with a high degree of conflict. The cause of the nonrobustness is discussed, and an alternative belief combination procedure to remedy the deficiency is proposed. Although the proposed procedure yields results that are dependent on the interpretations of the rule, it is shown to be an interpolation between total ignorance and the uncertainty associated with the rule regardless of the interpretations. When the rule interpretation yields an associative belief propagation procedure, a corresponding chaining syllogism for it can be derived. The proposed inference procedures are applied to a lattice-structured inference network. >	interference (communication);software propagation	Hai-Yen Hau;Rangasami L. Kashyap	1990	IEEE Trans. Systems, Man, and Cybernetics	10.1109/21.47808	rule-based system;uncertainty;dempster–shafer theory;computer science;belief structure;artificial intelligence;belief;knowledge-based systems;machine learning;pattern recognition;mathematics;reason;belief propagation	Arch	-19.954946348938247	-1.1390480464334831	83214
57b9581ef5dcf1b70d2f8b87396fb9706ea7b192	fuzzy-rule based adaptive data warehouse: an extension of data warehouse as knowledge warehouse	intelligent data warehouse;soft computing;fuzzy logic;artificial neural networks;adaptive neuro fuzzy inference system anfis;data warehouse;rules;knowledge warehouse kw;decision support systems dss	Data Warehouses (DWs) are aimed to empower the knowledge workers with information and knowledge which helps them in decision making. Technically, the DW is a large reservoir of integrated data that does not provide the intelligence or the knowledge demanded by users. The burden of data analysis and extraction of information and knowledge from integrated data still lies upon the analyst's shoulder. The overhead of analysts can be taken off by architecting a new generation data warehouses systems those shall be capable of capturing, organizing and representing knowledge along with the data and information in it. This new generation DW may be called as Knowledge Warehouse (KW) shall exhibit decision making capabilities themselves and can also supplement the Decision Support Systems (DSS) in making decisions quickly and effortlessly. This paper proposes and simulates a fuzzy-rule based adaptive knowledge warehouse with capabilities to learn and represent implicit knowledge by means of adaptive neuro fuzzy inference system (ANFIS).	fuzzy rule	Rajdev Tiwari;Anubhav Tiwari;Manu Pratap Singh	2012	IJAEC	10.4018/jaec.2012010103	fuzzy logic;dimensional modeling;computer science;data science;machine learning;data warehouse;data mining;database;soft computing;knowledge extraction;artificial neural network	DB	-31.849044791822937	-7.188378164923159	83290
efabcdeb296ed8202e78b6e2f0ffe61943d74e27	an agent system using basic emotions as communication method	fuzzy rules;learning system;emotion;agent system;agent systems;fuzzy classifier system			Yosuke Dendo;Katsuari Kamei	2003	JACIII	10.20965/jaciii.2003.p0040	emotion;adaptive neuro fuzzy inference system;artificial intelligence;neuro-fuzzy;machine learning;intelligent agent	Robotics	-26.404724207966794	-8.99101631010966	83351
0fa1a54d045987a31cd91839e1be11f296e5f78e	linguistic modelling and information coarsening based on prototype theory and label semantics	modelizacion;processus gauss;information coarsening;regle inference;analyse amas;linguistique;linguistic tool;rule induction;computacion informatica;uncertain reasoning;funcion densidad probabilidad;informacion incompleta;probability density function;knowledge extraction;semantics;instrumento linguistico;intelligence artificielle;time series;semantica;semantique;classification;semantic interpretation;imperfect information;inference rule;data clustering;modelisation;fonction densite probabilite;incomplete information;linguistica;fonction densite;cluster analysis;gaussian type density function;label semantics;razonamiento incierto;density function;ciencias basicas y experimentales;funcion densidad;decouverte connaissance;information incomplete;serie temporelle;prototype based clustering;serie temporal;extraction connaissances;informacion imperfecta;artificial intelligence;extraccion conocimiento;descubrimiento conocimiento;prototype theory;analisis cluster;inteligencia artificial;gaussian process;linguistic if then rules;grupo a;proceso gauss;mackey glass time series prediction;modeling;outil linguistique;density functional;clasificacion;raisonnement incertain;time series prediction;linguistic modelling;sunspots time series prediction;regla inferencia;information imparfaite;knowledge discovery;linguistics	The theory of prototypes provides a new semantic interpretation of vague concepts. In particular, the calculus derived from this interpretation results in the same calculus as label semantics proposed by Lawry. In the theory of prototypes, each basic linguistic label L has the form ‘about P’, where P is a set of prototypes of L and the neighborhood size of the underlying concept is described by the word ‘about’ which represents a probability density function d on 1⁄20;þ1Þ. In this paper we propose an approach to vague information coarsening based on the theory of prototypes. Moreover, we propose a framework for linguistic modelling within the theory of prototypes, in which the rules are concise and transparent. We then present a linguistic rule induction method from training data based on information coarsening and data clustering. Finally, we apply this linguistic modelling method to some benchmark time series prediction problems, which show that our linguistic modelling and information coarsening methods are potentially powerful tools for linguistic modelling and uncertain reasoning. 2009 Elsevier Inc. All rights reserved.	algorithm;benchmark (computing);cluster analysis;estimation theory;fuzzy logic;inference engine;machine learning;prototype;rule induction;semantic interpretation;software prototyping;time series;vagueness	Yongchuan Tang;Jonathan Lawry	2009	Int. J. Approx. Reasoning	10.1016/j.ijar.2009.01.004	probability density function;computer science;artificial intelligence;machine learning;time series;mathematics;knowledge extraction;cluster analysis;algorithm;statistics	AI	-20.392591720939574	-1.2142405965172824	83447
4156d19f1f9c0342a98129c877575477312221df	propagating uncertainty in rule based cognitive modelling	rule based;cognitive modelling		cognitive model	Thomas R. Shultz	1990			computer science	AI	-27.52369611815343	-7.995750676760628	83671
076dcf7c4ecf0eca0dd16d415cf718e1819fe6ad	geodetic distance queries on r-trees for indexing geographic data	euclidean distance;database index;actual index structure;geodetic distance;nearest neighbor query;geodetic distance query;appropriate geodetic;equirectangular projection;geographic data;efficient nearest neighbor query;accelerated nearest neighbor query	Geographic data have become abundantly available in the recent years due to the widespread deployment of GPS devices for example in mobile phones. At the same time, the data covered are no longer restricted to the local area of a single application, but often span the whole world. However, we do still use very rough approximations when indexing these data, which are usually stored and indexed using an equirectangular projection. When distances are measured using Euclidean distance in this projection, a non-neglibile error may occur. Databases are lacking good support for accelerated nearest neighbor queries and range queries in such datasets for the much more appropriate geodetic (great-circle) distance. In this article, we will show two approaches how a widely known spatial index structure – the R-tree – can be easily used for nearest neighbor queries with the geodetic distance, with no changes to the actual index structure. This allows existing database indexes immediately to be used with low distortion and highly efficient nearest neighbor queries and radius queries as well as window queries.		Erich Schubert;Arthur Zimek;Hans-Peter Kriegel	2013		10.1007/978-3-642-40235-7_9	data mining;database;mathematics;information retrieval	DB	-26.630555289486896	0.671518471481444	83888
434da09b1c0b6ba29b25a1204d8e9a6adb6bf97f	probabilistic similarity networks	normative expert system;david e. heckerman;expert system;probability theory;probabilistic similarity networks;serious battle;certainty factor;book review;probabilistic reasoning;mit press;normative framework	This is a book about normative expert systems, that is, systems which treat uncertainty using the normative framework of probability theory. The gauntlet of probabilistic reasoning in expert systems was thrown down by the developers of MYCIN when they originated certainty factors. It has been picked up by quite a few, and serious battles have been joined. Now one of the owners of the glove has admitted a success by a probabilist.	expert system;gauntlet keyboard;mycin	David Heckerman	1990	Networks	10.1145/140936.1063776	computer science;artificial intelligence;operations research;cognitive science	AI	-29.0606318427064	-9.649859817005531	84032
720bb5f3f464b8ce89b1226d94672d8a3f5f1e89	a data structure for supply chain management systems	management system;data storage;decision support system;binary search tree;data structures;decision support systems;production scheduling;materials management;data structure;supply chain management;design methodology	Purpose – Efficient operation of supply chain management (SCM) software is highly dependent on performance of its data structures that are used for data storage and retrieval. Each module in the software should use data structures that are appropriate for the types of operations performed in that module. The purpose of this paper is to develop and introduce an efficient data structure for storage and retrieval of data related to capacity of resources. Design/methodology/approach – A major aim of supply management systems is timely production and delivery of products. This paper reviews data structures and designs an efficient data structure for storage and retrieval of data that is used in the scheduling module of SCM software. Findings – This paper introduces a new data structure and search and update algorithms. This data structure can be used in SCM software to record the availability of non-storable resources. Originality/value – This is the first paper that discusses the role of data structures in SCM software and develops a data structure that can be used in the scheduling routine of SCM systems. Scheduling is one of the complex modules of SCM software. Some of the special characteristics related to capacity of resources to develop a data structure that can be efficiently searched and updated as part of scheduling routines were used in the new data structure. This data structure is a modified version of threaded height-balanced binary search tree. Each node in the proposed tree has one more key than a node in the ordinary threaded height-balanced binary search tree. The available algorithms in the literature on search and update operations on height-balanced binary search trees are modified to suit the proposed tree.	algorithm;computer data storage;data structure;data system;email;high-level programming language;in-memory database;industrial engineering;information system;nl (complexity);portable document format;regular tree grammar;scheduling (computing);self-balancing binary search tree;threaded code	Ali Ardalan;Roya K. Ardalan	2009	Industrial Management and Data Systems	10.1108/02635570910926636	binary search tree;real-time computing;data quality;decision support system;data structure;system of record;design methods;computer science;systems engineering;computer data storage;management system;database;scheduling;data efficiency	Theory	-33.661627397674145	-2.629143535912247	84035
ffb301ad8a765e7f062ea01bfd2ddc8b6f0c4550	spatial data traversal in road map databases: a graph indexing approach	databases;dimension alignment;geographic information system;spatial data;sql;temporal data;object identity;time cursor;spatio temporal data;object relational databases;geographic information systems;probabilistic analysis;pattern matching;directed graph;indexation;parametric data;active databases;digital mapping	Spatial data are found in geographic information systems such as digital road map databases where city and road attributes are associated with nodes and links in a directed graph. Queries on spatial data are expensive because of the recursive property of graph traversal. We propose a graph indexing technique to expedite spatial queries where the graph topology remains relatively stationary. Using a probabilistic analysis, this paper shows that the graph indexing technique significantly improves the efficiency of constrained spatial queries.	database;directed graph;geographic information system;graph traversal;probabilistic analysis of algorithms;recursion;stationary process;topological graph theory;tree traversal	J. Leon Zhao;Ahmed Zaki	1994		10.1145/191246.191308	sql;probabilistic analysis of algorithms;directed graph;computer science;graph traversal;pattern matching;data mining;spatial network;database;spatial analysis;geographic information system;graph;temporal database;random geometric graph;parametric statistics;graph database;information retrieval	DB	-26.968618498337406	0.11036799149324619	84054
5dd7e9b4247ae935a5906856bb7a4ec736a75c22	ortes: the design of a real-time control expert system	systeme temps reel;representacion conocimientos;systeme intelligent;object oriented methods;adquisicion del conocimiento;real time control;real time;sistema inteligente;real time monitoring;conceptual model;acquisition connaissances;control system;functional decomposition;object oriented;knowledge acquisition;intelligent system;methode orientee objet;real time system;sistema tiempo real;information system;knowledge representation;representation connaissances;systeme information;knowledge modeling;structure analysis;sistema informacion;expert system	We present ORTES, a methodology for conceptual modeling of real-time expert systems. ORTES combines advantages of current analysis methods, including structured analysis, object-oriented analysis and knowledge modeling, and circumvents their weaknesses. ORTES supports two stages, knowledge analysis and knowledge representation, in the process of knowledge acquisition. For knowledge analysis, ORTES applies both object and functional decomposition techniques. To overcome the limitations of functional decomposition, a generic task structure for real-time monitoring and control systems is proposed. For knowledge representation, ORTES makes use of object-oriented techniques which assign functions to objects and represent the system in terms of objects and their relationships. We present steps and strategies of ORTES in modeling a real-time control expert system.	expert system;real-time transcription	Aijun An;Nick Cercone;Christine W. Chan	1999		10.1007/BFb0095118	knowledge representation and reasoning;functional decomposition;simulation;real-time control system;computer science;control system;artificial intelligence;conceptual model;structural analysis;object-oriented programming;expert system;information system;algorithm	Robotics	-24.054472019334458	-4.210837498575414	84282
bed4362852978bfb3bd7b7380ddd51cce42ead41	the inseparability problem in interactive case-based reasoning	case base reasoning;retrieval;incomplete data;precision;recommender system;case based reasoning	In applications of interactive case-based reasoning (CBR) such as help-desk support and on-line decision guides, a problem that often affects retrieval perfonnance is the inability to distinguish between cases that have different solutions. For example, it is not unusual in a recommender system for two distinct products or services to have the same values for all attributes in the case library. While it is unlikely that both solutions are equally suited to the user's requirements, the system cannot help the user to choose between them. This problem, which we refer to as inseparability, can also arise as a result of incomplete data in the target problem presented for solution by a CBR system. We present an in-depth analysis of the inseparability problem, its relationship to the problem of incomplete data, and its impact on retrieval perfonnance.	case-based reasoning	David McSherry	2002	Knowl.-Based Syst.	10.1016/S0950-7051(01)00164-2	case-based reasoning;computer science;knowledge management;artificial intelligence;machine learning;data mining;recommender system	AI	-21.188088837454046	-5.492177475346695	84321
a7168173ef4dc076a8679351092c56bdf6d41764	using consensus methods for determining the representation of expert information in distributed systems	distributed system;information experte;base donnee repartie;systeme intelligent;science and technology;systeme reparti;distributed database;management system;sistema inteligente;base repartida dato;methode consensus;sistema repartido;intelligent system;intelligent agent;consensus problem	By the expert information we mean the information given by a man-expert of a field of science and technology, or by some intelligent program (for example intelligent agents) in solving of some task. We assume that in an intelligent distributed system for the same task the system sites may generate different solutions, and the problem is for the management system to determine a proper one solution for the task. In this paper we propose solving above problem by determining consensus of given solutions and treat it as the final solution. We present a general consensus problem, the postulates for consensus choice and their analysis. This analysis shows that the final solution being the consensus of given solutions should be the most credible solution in the uncertain situation.	distributed computing	Ngoc Thanh Nguyen	2000		10.1007/3-540-45331-8_2	simulation;consensus;computer science;artificial intelligence;data mining;uniform consensus;management system;distributed computing;distributed database;intelligent agent;science, technology and society	HPC	-21.389418984903177	-2.6482112508091507	84341
2342daeb36e95364cb49150c556c0417f19182cc	meta-level reasoning in deliberative agents	decision making;inference mechanisms;learning (artificial intelligence);multi-agent systems;scheduling;software agents;complex real-time decisions;deliberative agents;domain activities;metalevel reasoning;open environments;reinforcement learning;scheduling	Deliberative agents operating in open environments must make complex real-time decisions on scheduling and coordination of domain activities. These decisions are made in the context of limited resources and uncertainty about the outcomes of activities. We describe a reinforcement learning based approach for efficient meta-level reasoning. Empirical results showing the effectiveness of meta-level reasoning in a complex domain are provided.	automated reasoning;real-time transcription;reinforcement learning;scheduling (computing)	Anita Raja;Victor R. Lesser	2004	Proceedings. IEEE/WIC/ACM International Conference on Intelligent Agent Technology, 2004. (IAT 2004).	10.1109/IAT.2004.1342936	computer science;knowledge management;artificial intelligence;model-based reasoning;machine learning;reasoning system	Robotics	-19.55497849634283	-7.762897467525931	84375
6699a2feadb79a9f2a41f30dfc7b58c5ef0bcd89	the metacognitive loop: an architecture for building robust intelligent systems	robust intelligent systems;ontologies;commonsense	What commonsense knowledge do intelligent systems need, in order to recover from failures or deal with unexpected situations? It is impractical to represent predetermined solutions to deal with every unanticipated situation or provide predetermined fixes for all the different ways in which systems may fail. We contend that intelligent systems require only a finite set of anomaly-handling strategies to muddle through anomalous situations. We describe a generalized metacognition module that implements such a set of anomaly-handling strategies and that in principle can be attached to any host system to improve the robustness of that system. Several implemented studies are reported, that support our contention.	anomaly detection;artificial intelligence;commonsense knowledge (artificial intelligence);high-level programming language;macintosh common lisp;monte carlo localization;observable;ontology (information science);robot;wheels	Hamid Haidarian Shahri;Wikum Dinalankara;Scott Fults;Shomir Wilson;Donald Perlis;Matthew D. Schmill;Tim Oates;Darsana P. Josyula;Michael L. Anderson	2010			simulation;intelligent decision support system;computer science;knowledge management;ontology;artificial intelligence;machine learning	AI	-19.23922220814628	-3.9304159662563003	84780
e5a37e5e0161d4742852cad2964fc44fdec84b66	a methodology for knowledge engineering using an interactive graphical tool for knowledge modelling	knowledge modelling;interactive graphics;knowledge engineering	A basic feature of human nature is the propensity to construct boundaries which define territorial possession. Such assumed possessions are often jealously guarded by their owners, and a consequence of this primitive instinct is the emergence of subject specialists who exercise in-depth 'lordship' over their domain of expertise. For many years computer science has made attempts to relate to this phenomenon of expert intellectual property by developing mechanisms in software to emulate reasoning capability. Correspondingly this has resulted in the development of intelligent knowledge-based (or expert) systems, along with their attendant processes of knowledge elicitation, representation and exploitation. This paper attempts to define a context for knowledge engineering, the term being used to define spanning the void between domain expertise and the intelligent knowledge based system. It goes on to describe the systemic development of a particular solution to the knowledge engineering problem which is underpinned by a software environment called VEGAN (a Visual Editor for the Generation of Associative Networks). Many attempts have been made at bridging this gap, and VEGAN represents a significant aid to the knowledge engineering task, in the context of frame-based systems. Rather than attempt to create a unidirectional information path from expert to computer system (or knowledge engineer), VEGAN presents a common forum for discussion about, and exploration of, the expertise of the domain specialist. By doing so it helps the flow of information between the two parties. VEGAN represents an approach to a human-natured design of a software system which:	bridging (networking);computer science;emergence;expert system;exploit (computer security);file spanning;graphical user interface;knowledge engineer;knowledge engineering;knowledge management;knowledge-based systems;software system;the void (virtual reality);visual editor	J. M. Kellett;G. Winstanley;John T. Boardman	1989	AI in Engineering	10.1016/0954-1810(89)90004-6	legal expert system;knowledge base;computer science;engineering;knowledge management;artificial intelligence;body of knowledge;knowledge-based systems;machine learning;knowledge engineering;data mining;procedural knowledge;subject-matter expert;knowledge value chain;domain knowledge	AI	-33.381708529837674	-6.877746723073181	85536
d6f9fa894664158f61f54072ae1ec5045a657e11	on efficient aggregate nearest neighbor query processing in road networks		An aggregate nearest neighbor (ANN) query returns a point of interest (POI) that minimizes an aggregate function for multiple query points. In this paper, we propose an efficient approach to tackle ANN queries in road networks. Our approach consists of two phases: searching phase and pruning phase. In particular, we first continuously compute the nearest neighbors (NNs) for each query point in some specific order to obtain the candidate POIs until all query points find a common POI. Second, we filter out the unqualified POIs based on the pruning strategy for a given aggregate function. The two-phase process is repeated until there remains only one candidate POI, and the remained one is returned as the final result. In addition, we discuss the partition strategies for query points and the approximate ANN query for the case where the number of query points is huge. Extensive experiments using real datasets demonstrate that our proposed approach outperforms its competitors significantly in most cases.	aggregate function;alpha–beta pruning;approximation algorithm;central processing unit;emoticon;experiment;ibm 1400 series;national vulnerability database;nearest neighbor search;point of interest;two-phase commit protocol;universal flash storage	Weiwei Sun;Chunan Chen;Liang Zhu;Yunjun Gao;Yinan Jing;Qing Li	2015	Journal of Computer Science and Technology	10.1007/s11390-015-1560-z	online aggregation;sargable;query optimization;machine learning;data mining;database;world wide web	DB	-27.777126824842508	1.4193746924132267	85570
601de5a3cdf6d99947614ce130a16bde2f11aa03	exploiting cpu simd extensions to speed-up document scoring with tree ensembles	software;settore inf 01 informatica;information systems;ensemble methods;document scoring;learning to rank	Scoring documents with learning-to-rank (LtR) models based on large ensembles of regression trees is currently deemed one of the best solutions to effectively rank query results to be returned by large scale Information Retrieval systems. This paper investigates the opportunities given by SIMD capabilities of modern CPUs to the end of efficiently evaluating regression trees ensembles. We propose V-QuickScorer (vQS), which exploits SIMD extensions to vectorize the document scoring, i.e., to perform the ensemble traversal by evaluating multiple documents simultaneously. We provide a comprehensive evaluation of vQS against the state of the art on three publicly available datasets. Experiments show that vQS provides speed-ups up to a factor of 3.2x.	central processing unit;decision tree;information retrieval;learning to rank;simd;tree traversal	Claudio Lucchese;Franco Maria Nardini;Salvatore Orlando;Raffaele Perego;Nicola Tonellotto;Rossano Venturini	2016		10.1145/2911451.2914758	computer science;theoretical computer science;machine learning;data mining;world wide web;information retrieval;information system;learning to rank	Web+IR	-32.11814846785496	0.9647134632400747	85898
5d8febeafa42b7cf3b7def04c1cc27efd22513f9	filtering algorithms and implementation for very fast publish/subscribe	data compression;query processing;publish subscribe system;satisfiability;database mining;sampling;clustering;publish subscribe;high performance;data structure	Publish/Subscribe is the paradigm in which users express long-term interests (“subscriptions”) and some agent “publishes” events (e.g., offers). The job of Publish/Subscribe software is to send events to the owners of subscriptions satisfied by those events. For example, a user subscription may consist of an interest in an airplane of a certain type, not to exceed a certain price. A published event may consist of an offer of an airplane with certain properties including price. Each subscription consists of a conjunction of (attribute, comparison operator, value) predicates. A subscription closely resembles a trigger in that it is a long-lived conditional query associated with an action (usually, informing the subscriber). However, it is less general than a trigger so novel data structures and implementations may enable the creation of more scalable, high performance publish/subscribe systems. This paper describes an attempt at the construction of such algorithms and its implementation. Using a combination of data structures, application-specific caching policies, and application-specific query processing our system can handle 600 events per second for a typical workload containing 6 million subscriptions.	algorithm;data structure;database;predicate (mathematical logic);programming paradigm;publish–subscribe pattern;relational operator;scalability	Françoise Fabret;Hans-Arno Jacobsen;François Llirbat;João L. M. Pereira;Kenneth A. Ross;Dennis Shasha	2001		10.1145/375663.375677	data compression;sampling;data structure;computer science;data mining;database;publish–subscribe pattern;cluster analysis;programming language;world wide web;satisfiability	DB	-24.188435271269633	3.9144780453623453	85976
e4bed839ed3d832ac034f36597a14908cc6a239a	a meta-data model for knowledge in decision support systems.	classification;decision support techniques;decision support systems clinical;artificial intelligence;information storage and retrieval	Clinical decision support such as alerts, reminders and guidance are driven by rules often distributed among a variety of applications in a healthcare information system. Due to the increasing size of rule bases, there is a growing need to manage this dispersed knowledge in an integrated environment. A system for management of executable clinical knowledge such as rules should (1) assist in the development and maintenance of rules throughout the rules' life-cycles, (2) support search and retrieval of rules in the knowledge base (e.g., rules for diabetes, rules created by a particular individual), and (3) facilitate the analyses of rules in the knowledge base (e.g., identify rules not updated in the last year). In order to create such a clinical knowledge management system it is necessary to model the meta-data of rules. There have been efforts to document meta-data about rules within the Arden Syntax Medical Logical Modules' project. However, the maintenance and library categories in that project allow mainly free-text information about a rule. We have created a comprehensive meta-data structure and taxonomy for describing clinical rules that supports the features of a knowledge management system. We also tested this model using a representative set of rules.		Yaron Denekamp;Aziz A. Boxwala;Gilad J. Kuperman;Blackford Middleton;Robert A. Greenes	2003	AMIA ... Annual Symposium proceedings. AMIA Symposium		clinical decision support system;r-cast;marketing and artificial intelligence;decision support system;intelligent decision support system;decision analysis;decision engineering;machine learning;pattern recognition;management information systems;data mining;evidential reasoning approach;business decision mapping	DB	-33.38579806660481	-8.263583612752653	86208
7f06dc949735087dd8fda617415a824a40ac135b	aggregation in model-based reasoning using prime models: a preliminary report	inference mechanisms;abstract prime model;domain models;first principles reasoning;model aggregations;model-based reasoning;physical systems	In earlier work, we presented representations of physical systems at two levels of abstraction. Domain models represent particular physical systems whereas prime models are the higher level abstraction, representing general knowledge about a class of domain models. For example, a domain model of a circulatory system is a particular system of a corresponding prime model of a class of flow systems. This paper presents how domain models can be aggregated into intermediate levels, each of which can be realized as an instance of a corresponding abstract prime model. One advantage of modelprime model. One advantage of model aggregation is its abiIity to reason from first principle at different levels of decomposition. This is especially desirable when dealing with large and complex physical	domain model;model-based reasoning;principle of abstraction	Rattikorn Hewett	1992			computer science;artificial intelligence;machine learning;algorithm	AI	-20.547519188955683	1.3596314214114937	86224
619314a5736a47cbed438a57c051582ec59e4e1c	powering archive store query processing via join indices		In recent years, the industry landscape surrounding data processing systems has been significantly impacted by Big Data. Core technology and algorithms for data analysis have been adjusted and redesigned to handle the ever increasing amount of data. In this paper we revisit the concept of join index, a base mechanism in relational DBMS to support the expensive join operator, and analyze how it can be effectively integrated and combined with other mechanisms widely deployed for large-scale data processing. In particular, we show how the data store Informatica IDV, originally designed to facilitate backup and archival of application data, can benefit from join indices to give fast SQL-based access to archival data for discovery purposes. Informatica IDV supports both horizontal and vertical partitioning – two mechanisms that are widely used in modern data stores to speed up large-scale data processing. However, this requires us to reexamine join index design and usage. In this paper, we propose a scalable, partitioned, columnar join index that supports parallel execution, ease of maintenance and a late materialization query processing approach which is efficient for column-stores. Our implementation based on Informatica IDV has been evaluated using a TPC-H based benchmark, showing significant performance improvements compared to executions without join index. CCS Concepts •Information systems → Join algorithms;	algorithm;archive;backup;benchmark (computing);big data;binary space partitioning;cache (computing);central processing unit;cluster analysis;column-oriented dbms;data store;electronic data processing;ibm tivoli storage productivity center;idv;join (sql);performance evaluation;query optimization;relational database management system;run time (program lifecycle phase);sql;scalability;tpc-w	Joseph Vinish D'silva;Bettina Kemme;Richard Grondin;Evgueni Fadeitchev	2017		10.5441/002/edbt.2017.85	data mining;database;world wide web;computer science	DB	-30.245687679386123	1.0760232480442353	86534
2bdfdae091bd230b6612c9fb0edac8cc667d9fb3	real time statistical process advisor for effective quality control	modelizacion;statistical data;sistema experto;control estadistico proceso;analisis estadistico;sintesis control;recoleccion dato;systeme aide decision;data gathering;real time;heuristic method;production system;systeme production;statistical process control;base connaissance;metodo heuristico;maitrise statistique processus;sistema ayuda decision;probabilistic approach;sistema produccion;manufacturing execution system;user assistance;modelisation;support system;control chart;control proceso;decision support system;manufacturing execution system mes advisory support system;assistance utilisateur;statistical analysis;manufacturing execution system mes;enfoque probabilista;approche probabiliste;synthese commande;temps reel;asistencia usuario;analyse statistique;process control;donnee statistique;controle qualite;tiempo real;base conocimiento;peritaje;methode heuristique;advisory support system;expertise;systeme expert;dato estadistico;statistical process control spc;quality control;collecte donnee;modeling;commande processus;control synthesis;control calidad;knowledge base;expert system	An advisory decision support system has been presented in this paper. This system helps in collecting statistical data and thereafter analyzes the enormous volume of data and aids in making quality related decisions. In contrary to conventional SPC applications where the analyzed results have to be interpreted by quality control specialists, MES based unmanned manufacturing environments require automation of the interpretation process. The developed advisory system helps in selecting and designing control charts based on various cost, rule or heuristics models. The system also provides interpretation expertise by configuring and applying various rule sets. On violation of these rules, signals are generated by the system and the expert system advices for appropriate remedial actions. Thus the system acts as an advisory support system. D 2006 Published by Elsevier B.V.	chart;decision support system;expert system;heuristic (computer science);interpretation (logic);manufacturing execution system;unmanned aerial vehicle	Shankar Chakraborty;Diganta Tah	2006	Decision Support Systems	10.1016/j.dss.2005.03.008	manufacturing execution system;knowledge base;quality control;control chart;systems modeling;decision support system;computer science;artificial intelligence;operations management;process control;production system;operations research;expert system;statistical process control;data collection	Robotics	-24.342374554947874	-5.122020530350692	87451
9ff7628490454652dd972e24a5d7d61bec6195d6	semantic-based aggregation for statistical disclosure control	agregacion;representacion conocimientos;analisis estadistico;analisis datos;semantics;data fusion;semantica;semantique;classification;aggregation;data analysis;statistical analysis;fusion donnee;statistical disclosure control;analyse statistique;agregation;analyse donnee;knowledge representation;fusion datos;representation connaissances;clasificacion	In recent years, the so-called information explosion has caused the development of new techniques for data analysis and information management. One class of techniques where this improvement can be found is the one related with information fusion and knowledge integration. As the number of available information sources and the amounts of information increase, the need for these techniques also increases. Now, applications of these techniques are as diverse as scientific fields. One of the particular applications of information fusion techniques is Statistical Disclosure Control (SDC). The mission of National Statistical Offices (NSOs) is to collect information from respondents and to their posterior publication. In fact, data dissemination is a requirement for NSOs as is the main justification for the resources spent and of their existence. However, data dissemination usually is a sensitive task because of reidentification risk. NSOs should process data before publication so that published data ensure that particular individuals or organizations cannot be reidentified, i.e., no sensible data are published in a way that can be reidentified with a particular respondent (see Ref. 2 for a review of reidentification methods). Thus, data have to be protected (this is the so-called disclosure control problem) to avoid possible	information explosion;information management;knowledge integration;secure digital container;statistical disclosure control	Aïda Valls;Vicenç Torra;Josep Domingo-Ferrer	2003	Int. J. Intell. Syst.	10.1002/int.10129	biological classification;computer science;artificial intelligence;data mining;semantics;sensor fusion;data analysis;statistics	DB	-20.11218470540771	-0.5050461649209808	87452
09024bea594a096fd4e080a0d65a415638ce5b68	simulation techniques: an effective way for solving decision-making problems	institutional repositories;fedora;vital;simulation technique;vtls;ils		simulation	Fouad Riane;Michel Gourgand	2008	Eng. Appl. of AI	10.1016/j.engappai.2008.03.002	simulation;computer science;data science;management science	AI	-31.041531603137575	-9.241066397063973	87512
19787e7fd00a4ee39aa0765259403c7ec87d4420	applications of entity-relationship model to picture description	grammar;entity relationship model;image processing;description image;image data bank;procesamiento imagen;traitement image;er model;grammaire;banque image;banca imagen;application;grammaire image;gramatica;picture description	The concepts of entity-relationship diagram have been applied to picture description. Primitive picture entities, picture relationships, and picture grammars are presented with illustrative examples. A high-level description of a two-level picture generation system is proposed using either string description or ER diagram description. Illustrative examples are also given. The advantages of ER diagram description together with its comparison to string description are also presented. The results may have useful applications in robotics, artificial intelligence, expert systems, picture processing, pattern recognition, knowledge engineering and pictorial database design.	artificial intelligence;database design;diagram;entity;entity–relationship model;expert system;formal grammar;high- and low-level;image;knowledge engineering;pattern recognition;robotics	Edward T. Lee;P. Chu;C. Peng Wu	1987	Robotica	10.1017/S0263574700015885	computer vision;entity–relationship model;image processing;computer science;artificial intelligence	AI	-22.509269669556655	-3.2454091186530203	87567
65b0d5581d78152bcda3be63e6fa6dcf6312a7a5	high-performance computing with sparsity and structure: challenges and directions for single-core and multicore hardware	query processing;database;query optimization;operations research;structured matrices;materialized view;high performance computer;next generation;progressive query	Computations involving sparse and structured matrices arise frequently in problems in operations research, science and engineering. Exploiting this structure has been a major contributor to the efficient application of past and current computing platforms, and will continue to play an important role for next-generation systems.	computation;multi-core processor;operations research;single-core;sparse matrix	Robert F. Enenkel;Christopher Kumar Anand;Shahadat Hossain	2010		10.1145/1923947.1924027	materialized view;query optimization;query expansion;computer science;theoretical computer science;data mining;database;distributed computing;query language	HPC	-29.765550621204277	2.159125975120801	87584
b4646fe6be77d963beddca1b7150f736aaabb0ff	synthesizing minimal programs from traces of observable behaviour				Christoph Beierle	1982			artificial intelligence;machine learning;computer science;observable	SE	-21.764839443734402	0.7157025920890691	87630
f122e61d58a4bcbc88e145f78fbf688002905535	a decision theory view of the information retrieval situation	information retrieval;decision theory		decision theory;information retrieval	Keats A. Pullen	1974	JASIS	10.1002/asi.4630250111	relevance;cognitive models of information retrieval;decision theory;decision analysis;decision engineering;computer science;decision rule;evidential reasoning approach;information retrieval;statistics;business decision mapping;human–computer information retrieval	Vision	-30.690955271024666	-9.128451045265212	87788
70c15de32bfb32b5aa361448dabfb0593ab0c2fd	uncertainty representation and combination: new results with application to nuclear safety issues. (représentation et combinaison d'informations incertaines : nouveaux résultats avec applications aux études de sûreté nucléaires)		Interests Information modelling and treatment under severe uncertainty. Interests Uncertainty modelling and aid-decision applied to agronomical production chains. Interests Information modelling and treatment under severe uncertainty.	information model	Sébastien Destercke	2008				Robotics	-29.13906908239799	-8.67256148304502	87798
954f417ac6a2d15333c167fbe626bb2377594c81	how to select among alternative knowledge representations for better knowledge engineering	knowledge representation;knowledge engineering			J. E. Caviedas	1988		10.1145/55674.55697	knowledge integration;computer science;knowledge management;artificial intelligence;body of knowledge;mathematical knowledge management;knowledge-based systems;knowledge engineering;open knowledge base connectivity;procedural knowledge;knowledge extraction;personal knowledge management;commonsense knowledge;knowledge value chain;domain knowledge	AI	-31.535006553420047	-6.547311194170325	87813
a205822ab12093ac95f0ef583950a296583dc689	sri@work: efficient and effective routing strategies in a pdms	p2p system;model design;distributed computing;information sharing;indexation;semantic web;peer data management systems;simulation environment;query routing	In recent years, information sharing has gained much benefit by the large diffusion of distributed computing, namely through P2P systems and, in line with the Semantic Web vision, through Peer Data Management Systems (PDMSs). In a PDMS scenario one of the most difficult challenges is query routing, i.e. the capability of selecting small subsets of semantically relevant peers to forward a query to. In this paper, we put the Semantic Routing Index (SRI) distributed mechanism we proposed in [6] at work. In particular, we present general SRI-based query execution models, designed around different performance priorities and minimizing the information spanning over the network. Starting from these models, we devise several SRI-enabled routing policies, characterized by different effectiveness and efficiency targets, and we deeply test them in ad-hoc PDMS simulation environments.	distributed computing;file spanning;hoc (programming language);pdms;routing;semantic web;simulation	Federica Mandreoli;Riccardo Martoglia;Wilma Penzo;Simona Sassatelli;Giorgio Villani	2007		10.1007/978-3-540-76993-4_24	policy-based routing;static routing;computer science;operating system;semantic web;data mining;database;world wide web	DB	-29.320255026205697	-0.48086320258102666	87883
51db1c5ac0614844e0dead7ffe829831788996ef	self maintenance of multiple views in data warehousing	multiple views;graph models;join processing;data warehousing;page access scheduling;heuristics;materialized views;data warehouse	Materialized views (<italic>MV</italic>) at the data warehouse (<italic>DW</italic>) can be kept up to date in response to changes in data sources without accessing data sources for additional information. This process is usually refered to as “self maintenance of views”. A number of algorithms have been proposed for self maintenance of views, which use <italic>auxiliary views (AV)</italic> to keep some additional information in <italic>DW</italic>. In this paper we propose an algorithm for self maintainability of multiple <italic>MVs</italic> using the above approach. Our algorithm generates a simple maintenance query to incrementally maintain an <italic>MV</italic> along with its <italic>AV</italic> at <italic>DW</italic>. The algorithm maintains these views by minimizing the number and the size of the <italic>AVs</italic>. Our approach provides better insight into view maintenance issues by exploiting the dependencies and constraints that might exist in the data sources and multiple <italic>MVs</italic> at <italic>DW</italic>.	algorithm;emoticon;materialized view;warez	Sunil Samtani;Vijay Kumar;Mukesh K. Mohania	1999		10.1145/319950.320018	materialized view;computer science;theoretical computer science;heuristics;data warehouse;data mining;database	DB	-26.39034204637442	4.106368566957533	88010
34efd2dabe4d1cccefa4ad013a7dd2955a25a3b9	first elements on knowledge discovery guided by domain knowledge (kddk)	data mining;classification;domain knowledge;knowledge discovery in database;knowledge representation;formal concept analysis;knowledge discovery	In this paper, we present research trends carried out in the Orpailleur team at loria, showing how knowledge discovery and knowledge processing may be combined. The knowledge discovery in databases process (kdd) consists in processing a huge volume of data for extracting significant and reusable knowledge units. From a knowledge representation perspective, the kdd process may take advantage of domain knowledge embedded in ontologies relative to the domain of data, leading to the notion of “knowledge discovery guided by domain knowledge” or kddk. The kddk process is based on the classification process (and its multiple forms), e.g. for modeling, representing, reasoning, and discovering. Some applications are detailed, showing how kddk can be instantiated in an application domain. Finally, an architecture of an integrated kddk system is proposed and discussed.	application domain;association rule learning;data mining;database;embedded system;knowledge acquisition;knowledge base;knowledge representation and reasoning;numerical analysis;ontology (information science);rule induction;semantic web;symbolic-numeric computation;text mining	Jean Lieber;Amedeo Napoli;Laszlo Szathmary;Yannick Toussaint	2006		10.1007/978-3-540-78921-5_2	knowledge representation and reasoning;knowledge base;software mining;biological classification;computer science;formal concept analysis;knowledge management;data science;body of knowledge;mathematical knowledge management;knowledge-based systems;open knowledge base connectivity;data mining;procedural knowledge;knowledge extraction;commonsense knowledge;domain knowledge	ML	-33.137227218282696	-5.470793072232788	88039
b2ddab4acdca29a77ba238561a81392b2c3440b2	towards a multiagent decision support system for crisis management		The cirsis management is a complex problem raised by the scientific community currently. Decision support systems are a suitable solution for such issues, they are indeed able to help emergency managers to prevent and to manage crisis in emergency situations. However, they should be enough flexible and adaptive in order to be reliable to solve complex problems that are plunged in dynamic and unpredictable environments. The approach we propose in this paper addresses this challenge. We expose here a modelling of information for an emergency environment and an architecture of a multiagent decision support system that deals with these information in order to prevent the occur of a crisis or to manage it in emergency situations. We focus on the first level of the system mechanism which intends to perceive and to reflect the evolution of the current situation. The general approach and experimentations are provided here. INTRODUCTION Natural and man­made disasters are permanent hazards for human beings since they may have harmful consequences for them and their properties. In order to brace such events, people must be efficient in their evaluations, their decision­making and their actions. They must therefore change and perform their classical crisis management methods by using new means. This is already realized and accepted as a high priority task by many organizations, governments and companies in Europe and all over the world (Cutter et al. 2003). In this context, Decision Support Systems (DSS) have proved their ability to resolve such kind of problems. Our research work addresses this challenge. It lies in building a DSS that must be able to help emergency managers to deal with crisist and to provide them emergency management	agent-based model;cutter expansive classification;decision support system;intelligent agent;simulation	Fahem Kebair;Frédéric Serin	2011	J. Intelligent Systems	10.1515/JISYS.2011.004	decision support system	AI	-21.75578288737183	-8.655886291712555	88398
6df11b798c97ad33a41d3cd94cbe425b90208b7c	algorithms and architectures for parallel processing		The Web of Data is widely considered as one of the major global repositories populated with countless interconnected and structured data prompting these linked datasets to be continuously and sharply increasing. In this context the so-called SPARQL Protocol and RDF Query Language is commonly used to retrieve and manage stored data by means of SPARQL endpoints, a query processing service especially designed to get access to these databases. Nevertheless, due to the large amount of data tackled by such endpoints and their structural complexity, these services usually suffer from severe performance issues, including inadmissible processing times. This work aims at overcoming this noted inefficiency by designing a distributed parallel system architecture that improves the performance of SPARQL endpoints by incorporating two functionalities: (1) a queuing system to avoid bottlenecks during the execution of SPARQL queries; and (2) an intelligent relaxation of the queries submitted to the endpoint at hand whenever the relaxation itself and the consequently lowered complexity of the query are beneficial for the overall performance of the system. To this end the system relies on a two-fold optimization criterion: the minimization of the query running time, as predicted by a supervised learning model; and the maximization of the quality of the results of the query as quantified by a measure of similarity. These two conflicting optimization criteria are efficiently balanced by two bi-objective heuristic algorithms sequentially executed over groups of SPARQL queries. The approach is validated on a prototype and several experiments that evince the applicability of the proposed scheme.	communication endpoint;database;expectation–maximization algorithm;experiment;heuristic;linear programming relaxation;linked data;mathematical optimization;population;prototype;rdf query language;sparql;supervised learning;systems architecture;time complexity;world wide web	Jesús Carretero;Javier Garcia-Blas;Ryan K. L. Ko;Peter Müller;Koji Nakano	2016		10.1007/978-3-319-49583-5		Web+IR	-32.11490514164138	3.2077790670683135	88495
adf56393451a4b54260f71cc3fd7f8ef1c94e201	provenance query evaluation: what's so special about it?	and forward;query optimization;query evaluation;indexation;query;provenance;evaluation;access method;data structure	While provenance has been extensively studied in the literature, the efficient evaluation of provenance queries remains an open problem. Traditional query optimization techniques, like the use of general-purpose indexes, or the materialization of provenance data, fail on different fronts to address the problem. Therefore, the need to develop provenance-aware access methods becomes apparent. This paper starts by identifying some key requirements that are to a large extent specific to provenance queries and are necessary for their efficient evaluation. The first such property, called duality, requires that a single access method is used to evaluate both backward provenance queries (which input items of some analysis generate an output item) and forward provenance queries (which outputs of some analysis does an input item generate). The second property, called locality, guarantees that provenance query evaluation times should depend mainly on the size of the provenance query results and should be largely independent of the total size of provenance data. Motivated by the above, we identify proper data structures with the aforementioned properties, we implement them, and through a detailed set of experiments, we illustrate their effectiveness on the evaluation of provenance queries.	central processing unit;data structure;emoticon;experiment;general-purpose modeling;inverted index;locality of reference;mathematical optimization;query optimization;real life;requirement;single-access key;x-fast trie	Anastasios Kementsietsidis;Min Wang	2009		10.1145/1645953.1646040	query optimization;data structure;computer science;evaluation;data mining;database;programming language;access method;world wide web;information retrieval	DB	-28.298610690726655	3.248905801907287	88867
293b4cf117b5a3bfa41f82b2055d07397ecb28af	general system theoretic approach to data mining system	general systems theory;set theory;data mining;unified modeling language;information system		complexity;control theory;data mining;information system;knowledge engineering;prolog;year 10,000 problem	Yasuhiko Takahara;Naoki Shiba;Yongmei Liu	2002	Int. J. General Systems	10.1080/03081070290005195	unified modeling language;computer science;data science;theoretical computer science;data mining;mathematics;systems theory;information system;set theory	ML	-28.055609402185816	-6.0473356633609985	88919
937fa3375e3d9fdcb1aa8a23432362c6f1c83818	planning and resource allocation for hard real-time, fault-tolerant plan execution	engineering;ai architectures;resource utilization;software engineering programming and operating systems;fault tolerant;resource allocation;procedural reasoning system;real time;user interfaces and human computer interaction;humanities;cooperative intelligent real time control architecture;real time scheduling;fault tolerance;philosophy;intelligent agent;artificial intelligence incl robotics;planning;computer science;agent architecture;data structures cryptology and information theory;belief desire intention architecture;system safety;intelligent software agent;hard real time	We describe the interface between a real-time resource allocation system with an AI planner in order to create fault-tolerant plans that are guaranteed to execute in hard real-time. The planner specifies the task set and all execution deadlines required to ensure system safety, then the resource utilization. A new interface module combines information from planning and resource allocation to enforce development of plans feasible for execution during a variety of internal system faults. Plans that over-utilize any system resource trigger feedback to the planner, which then searches for an alternate plan. A valid plan for each specified fault, including the nominal no-fault situation, is stored in a plan cache for subsequent real-time execution. We situate this work in the context of CIRCA, the Cooperative Intelligent Real-time Control Architecture, which focuses on developing and scheduling plans that make hard real-time safety guarantees, and provide an example of an autonomous aircraft agent to illustrate how our planner-resource allocation interface improves CIRCA performance.		Ella M. Atkins;Tarek F. Abdelzaher;Kang G. Shin;Edmund H. Durfee	1999		10.1145/301136.301204	fault tolerance;real-time computing;simulation;resource allocation;computer science;artificial intelligence;intelligent agent	AI	-19.34309937110828	-7.414676294637449	88935
20aaf65cfde9a97de5dde233cd0ad4df8a872e9e	a review of automated feature recognition with rule-based pattern recognition	modelizacion;concepcion asistida;computer aided design;sistema experto;boundary representation;base de connaissances;feature recognition;logic rules;rule based;extraction forme;representation limite;classification;geometric feature;step;preparacion serie fabricacion;modelisation;capp;extraccion forma;pattern matching;feature extraction;pattern recognition;conception assistee;representacion limite;system development;base conocimiento;concordance forme;automated feature recognition;reconnaissance forme;process planning;extraction caracteristique;systeme expert;b rep;reconocimiento patron;preparation gamme fabrication;modeling;pattern extraction;clasificacion;knowledge base;expert system	Automated feature recognition (AFR) has provided the greatest contribution to fully automated CAPP system development. The objective of this paper is to review various approaches for solving three major AFR problems: (i) extraction of geometric primitives from a CAD model; (ii) defining a suitable part representation for form feature identification; and (iii) feature pattern matching/recognition. A novel, detailed classification of developed AFR systems has been introduced. This paper also provides a thorough investigation of methods for geometric feature extraction, emphasizing STEP standard application and, finally, a review of recent research reports in the field of AFR with rule-based feature pattern recognition. We discuss potentials and limitations of these approaches and emphasize directions for further research work.	feature recognition;logic programming;pattern recognition	Bojan R. Babic;Nenad Nesic;Zoran Miljkovic	2008	Computers in Industry	10.1016/j.compind.2007.09.001	feature recognition;computer vision;knowledge base;systems modeling;feature;feature extraction;biological classification;computer science;engineering;artificial intelligence;computer aided design;pattern matching;engineering drawing;expert system;boundary representation;rule of inference	Vision	-24.65839937043137	-2.6678375232408182	89131
5b9ac85e9a9e92875d51f9a5c9f3c6e2e63da288	parallel accessing massive netcdf data based on mapreduce	atmospheric science;parallel access;scientific data;data format;large scale;distributed environment;data intensive;netcdf;mapreduce;access method	As a Network Common Data Format, NetCDF has been widely used in terrestrial, marine and atmospheric sciences. A new paralleling storage and access method for large scale NetCDF scientific data is implemented based on Hadoop. The retrieval method is implemented based on MapReduce. The Argo data is used to demonstrate our method. The performance is compared under a distributed environment based on PCs by using different data scale and different task numbers. The experiments result show that the parallel method can be used to store and access the large scale NetCDF efficiently.	mapreduce;netcdf	Hui Zhao;SiYun Ai;Zhenhua Lv;Bo Li	2010		10.1007/978-3-642-16515-3_53	parallel computing;computer science;data mining;database;netcdf;access method;distributed computing environment;data	DB	-31.232437336949534	0.7360336809193554	89189
d71d2414d919b632ca8c01409a219094bfa828ba	introducing graph-based reasoning into a knowledge management tool: an industrial case study	busqueda informacion;tool management;anotacion;regle inference;query language;gestion des connaissances;representacion conocimientos;industrial case study;ontologie;conjunctive queries;requerimiento conjuntivo;ingenierie connaissances;red semantica;information retrieval;topic maps;web semantique;knowledge management;interrogation base donnee;conceptual analysis;semantic network;interrogacion base datos;semantics;base connaissance;annotation;intelligence artificielle;semantica;semantique;analisis conceptual;lenguaje interrogacion;knowledge representation languages;inference rule;reseau semantique;langage representation connaissance;recherche information;conceptual graph;gestion herramienta;web semantica;gestion outil;requete conjonctive;representation connaissance;grafo conceptual;semantic web;conjunctive query;artificial intelligence;base conocimiento;ontologia;langage interrogation;inteligencia artificial;analyse conceptuelle;knowledge representation;domain ontology;ontology;gestion conocimiento;graphe conceptuel;database query;regla inferencia;knowledge base;knowledge engineering	This paper is devoted to an industrial case study focused on the issue of how to enhance ITM, a knowledge management tool, with reasoning capabilities, primarily by introducing a semantic query mechanism. ITM knowledge representation language is based on topic maps. We show that these topic maps (and especially those describing the domain ontology and the annotation base) can be naturally mapped to the SG family, a sublanguage of conceptual graphs. As this mapping is reversible, ITM can be equipped with a graph-based query language equivalent to conjunctive queries and it can be enriched with inference rules.	conceptual graph;conjunctive query;irish transverse mercator;knowledge management;knowledge representation and reasoning;map;off topic;ontology (information science);query language;semantic query;sublanguage;suicidegirls;topic maps	Olivier Carloni;Michel Leclère;Marie-Laure Mugnier	2006		10.1007/11779568_64	natural language processing;knowledge base;computer science;artificial intelligence;knowledge engineering;ontology;data mining;database;semantics;conjunctive query	AI	-24.389631626608622	-1.7990175802263821	89192
509116082cbb71c0578e956e9008b5b60f8848ab	contextualizing data warehouses with documents	busqueda informacion;gestion integrada;tratamiento datos;gestion integree;base dato multidimensional;text;keyword;base de donnees multidimensionnelle;cube;on line;information retrieval;en linea;xml language;cubo;database;base dato;data processing;integrated management;traitement donnee;palabra clave;olap;texte;almacen dato;mot cle;multidimensional database;info eu repo semantics article;vista materializada;materialized view;hierarchical classification;recherche information;almacenamiento;estructura datos;entreposage;warehousing;base de donnees;classification hierarchique;xml document;en ligne;structure donnee;text rich xml documents;entrepot donnee;data warehouse;texto;clasificacion jerarquizada;data structure;langage xml;lenguaje xml;structured data;vue materialisee	Current data warehouse and OLAP technologies are applied to analyze the structured data that companies store in databases. The context that helps to understand data over time is usually described separately in text-rich documents. This paper proposes to integrate the traditional corporate data warehouse with a document warehouse, resulting in a contextualized warehouse. Thus, the user first selects an analysis context by supplying some keywords. Then, the analysis is performed on a novel type of OLAP cube, called an R-cube, which is materialized by retrieving and ranking the documents and corporate facts related to the selected context. © 2006 Elsevier B.V. All rights reserved.	database;olap cube;online analytical processing	Juan Manuel Vidal Pérez;Rafael Berlanga Llavori;María José Aramburu Cabo;Torben Bach Pedersen	2008	Decision Support Systems	10.1016/j.dss.2006.12.005	xml;data structure;data processing;dimensional modeling;online analytical processing;computer science;data warehouse;data mining;database;world wide web	DB	-27.25247240435851	-2.579756955480155	89195
8648dfb2739777f7f292f0619779b1db45e778cc	software bug ontology supporting semantic bug search on peer-to-peer networks	bug ontology;bug resolution;semantic bug search;peer-to-peer computing;bug tracking system	This article presents a semantic bug search system that assists users in sharing and searching solutions for similar bug reports on peer-to-peer networks. This system features the capability of exploring bug knowledge resources at various sites in distributed environment and exploiting the classification and relationship information of bug reports. The system uses a unified bug schema that not only integrates bug reports from various bug knowledge resources into a single database but also contains several selective properties including package dependencies, bug relationships, bug symptoms and categories to foster a semantic search mechanism. We have implemented several components of the system including bug updater to maintain bug database, query handler to share and search bug reports, and peer controller to manage communication on an appropriate peer-to-peer network. We have experimented the system on a distributed computing testbed and measured its feasibility, scalability and efficiency.	bandwidth (signal processing);bug tracking system;database;distributed computing;gnutella;knowledge management;peer-to-peer;sparql;scalability;semantic search;software bug;testbed	Ha Manh Tran;Son Thanh Le	2014	New Generation Computing	10.1007/s00354-014-0203-1	security bug;computer science;data mining;database;world wide web	OS	-29.366722412336866	-0.7618258130038906	89260
d9c2c2b063d7bcddc396f0c3f4850b38ff6bd1a4	a case-based reasoning system using a control case-base	case base reasoning		case-based reasoning;reasoning system	Isabelle Bichindaritz	1994			deductive reasoning;procedural reasoning system;model-based reasoning;artificial intelligence;opportunistic reasoning;machine learning;psychology of reasoning;computer science;case-based reasoning;reasoning system;qualitative reasoning	Robotics	-27.61895622568542	-7.7996458889439815	89408
69a89a53c6a06cca8e4bddebdd5641cdb5ec8126	abduction and dynamic preference in plan-based dialogue understanding			abductive reasoning	Katashi Nagao	1993			computer science;knowledge management	NLP	-27.03681649000702	-9.669836342669003	89577
f310bd8cc4f18f9eb9c1a0c503c60475bc56b716	"""""""task trees"""" - a hierarchical structure for modelling complex jobshops"""	simulation ordinateur;modelizacion;regle inference;hierarchical structure;inference motor;sistema experto;systeme discret;implementation;intelligence artificielle;prise decision;algorithme;inference rule;modelisation;algorithm;ejecucion;motor inferencia;estudio caso;scheduling;etude cas;artificial intelligence;ordonamiento;simulacion computadora;inteligencia artificial;systeme expert;sistema discreto;toma decision;modeling;computer simulation;moteur inference;ordonnancement;discrete system;regla inferencia;algoritmo;expert system	"""laboratories (invariant case mix, superficial averaging of analyser performance, and indiscriminate scheduling) are no longer acceptable in a decision support system. A novel core engine that implements a """"hierarchical"""" version of the discrete-event simulation approach, """"task trees,"""" is presented. This addresses the fundamental problems of """"interlocking"""" between tasks (both within and without jobshops) and the need to model at a detailed level, i.e., scheduling individual"""	decision support system;scheduling (computing);simulation;the superficial	Ewan W. Crawford;Iain W. Percy-Robb;Barry Clark	1997	Simulation	10.1177/003754979706900501	computer simulation;systems modeling;computer science;artificial intelligence;machine learning;discrete system;implementation;scheduling;expert system;algorithm;rule of inference	Metrics	-23.69127021529394	-4.864329920670611	89635
6fc7ba485dfa051391ded794e5c0eec8c47ee430	abductive problem solving with abstractions	abduction;plan recognition;model based diagnosis;abstractions;observation cost;image interpretation;action cost;problem solving	Several explanation and interpretation tasks, such as diagnosis, plan recognition and image interpretation, can be formalized as abductive and consistency reasoning. The interpretation task is usually executed for the purpose of performing actions, e.g., in diagnosis, repair actions or therapy. In some cases actions are the only or the main way for discriminating among alternative explanations. Some proposals address the problem based on a task-independent representation of a domain which includes an ontology or taxonomy of hypotheses and actions. In this paper we rely on the same type of representation, and we point out the role of abstractions in an iterative process where, like in model-based diagnosis and troubleshooting, further observations or actions are proposed to achieve the overall goal of discriminating among candidate hypotheses and, more importantly, performing the appropriate actions for the case at hand. Discrimination is performed up to an appropriate level which depends on the cost of actions (e.g. repair actions or therapy) to be taken based on the results of abduction, and on the cost of additional observations, which should be balanced with the benefits, in terms of more suitable actions, of better discrimination. Abstractions have a significant impact on this trade-off, given that the cost of observing the same phenomenon at different levels of abstraction may be quite different, and choosing a generic action, without information on which specific instance is most appropriate, is, in general, suboptimal.	abductive reasoning;approximation algorithm;greedy algorithm;iteration;principle of abstraction;problem solving;theory	Gianluca Torta;Daniele Theseider Dupré	2009			epistemology;artificial intelligence;machine learning;mathematics;abstraction;algorithm	AI	-20.027180650590566	-4.375116167358785	90205
22d47cc3dcb99f1b213f8f801147ecf8808ce068	estimation of query-result distribution and its application in parallel-join load balancing	database system;query optimization;parallel database system;data distribution;load balance;efficient estimation	Many commercial database systems use some form of statistics, typically histograms, to summarize the contents of relations and permit efficient estimation of required quantities. While there has been considerable work done on identifying good histograms for the estimation of query-result sizes, little attention has been paid to the estimation of the data distribution of the result, which is of importance in query optimization. In this paper, we prove that the optimal histogram for estimating the size of the result of a join operator is optimal for estimating its data distribution as well. We also study the effectiveness of these optimal histograms in the context of an important application that requires estimates for the data distribution of a query result: load-balancing for parallel Hybrid hash joins. We derive a cost formula to capture the effect of data skew in both the input and output relations on the load and use the optimal histograms to estimate this cost most accurately. We have developed and implemented a load balancing algorithm using these histograms on a simulator for the Gamma parallel database system. The experiments establish the superiority of this approach compared to earlier ones in handling all kinds and levels of skew while incurring negligible overhead. Partially supported by the National Science Foundation under Grants IRI-9113736 and IRI-9157368 (PYI Award) and by grants from DEC, IBM, HP, AT&T, Informix, and Oracle. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large Data Base Endowment. To copy otherwise, or to republish, requires a fee and/or special permission from the Endowment. Proceedings of the 22nd VLDB Conference Mumbai(Bombay), India, 1996	algorithm;experiment;ibm informix;input/output;load balancing (computing);mathematical optimization;overhead (computing);parallel database;query optimization;vldb	Viswanath Poosala;Yannis E. Ioannidis	1996			query optimization;computer science;load balancing;data mining;database;distributed computing	DB	-29.28179578040642	3.5450492775132045	90507
56c74c4e0076ac2a1d6d60a08705c95b7f2dc56a	group decision support systems: an analysis and critique				Liam J. Bannon	1997			decision analysis;r-cast;knowledge management;decision support system;computer science	ECom	-30.767352697293056	-9.235417848201868	90544
0ba300acd3f5f1b1cc3580bfbe905c3eb44a0cbe	adaptive processing of spatial-keyword data over a distributed streaming cluster		The widespread use of GPS-enabled smartphones along with the popularity of micro-blogging and social networking applications, e.g., Twitter and Facebook, has resulted in the generation of huge streams of geo-tagged textual data. Many applications require real-time processing of these streams. For example, location-based ad-targeting systems enable advertisers to register millions of ads to millions of users based on the users' location and textual profile. Existing streaming systems are either centralized or are not spatial-keyword aware, and hence these systems cannot efficiently support the processing of rapidly arriving spatial-keyword data streams. In this paper, we introduce a two-layered indexing scheme for the distributed processing of spatial-keyword data streams. We realize this indexing scheme in Tornado, a distributed spatial-keyword streaming system. The first layer, termed the routing layer, is used to fairly distribute the workload, and furthermore, co-locate the data objects and the corresponding queries at the same processing units. The routing layer uses the Augmented-Grid, a novel structure that is equipped with an efficient search algorithm for distributing the data objects and queries. The second layer, termed the evaluation layer, resides within each processing unit to reduce the processing overhead. The two-layered index adapts to changes in the workload by applying a cost formula that continuously represents the processing overhead at each processing unit. Extensive experimental evaluation using real Twitter data indicates that Tornado achieves high scalability and more than 2x improvement over the baseline approach in terms of the overall system throughput.	baseline (configuration management);blog;centralized computing;distributed computing;global positioning system;lazy evaluation;location awareness;location-based game;overhead (computing);publish–subscribe pattern;real-time locating system;routing;scalability;search algorithm;smartphone;text corpus;throughput;tornado	Ahmed R. Mahmood;Anas Daghistani;Ahmed M. Aly;MingJie Tang;Saleh M. Basalamah;Sunil Prabhakar;Walid G. Aref	2018		10.1145/3274895.3274932	throughput;workload;distributed computing;data stream mining;scalability;tornado;search engine indexing;computer science;popularity;search algorithm	DB	-27.351854581788068	-1.7904873891282955	90545
a53051fd4fbe89b89c78b866cecba9c1fdaa75fc	tolerance spaces and approximative representational structures	representacion conocimientos;multiagent system;distance measure;dato que falta;sensor model;on line;informacion incompleta;software agent;en linea;rough set theory;teoria conjunto;theorie ensemble;intelligence artificielle;robotics;sistema complejo;set theory;agent logiciel;software agents;donnee manquante;incomplete information;approximate reasoning;traditional knowledge;complex data;systeme complexe;complex system;theorie ensemble approximatif;knowledge structure;tolerancia;estructura datos;information incomplete;datavetenskap datalogi;robotica;artificial intelligence;en ligne;structure donnee;tolerance;robotique;missing data;inteligencia artificial;computer science;knowledge representation;sistema multiagente;representation connaissances;data structure;similarity measure;systeme multiagent	In traditional approaches to knowledge representation, notions such as tolerance measures on data, distance between objects or individuals, and similarity measures between primitive and complex data structures are rarely considered. There is often a need to use tolerance and similarity measures in processes of data and knowledge abstraction because many complex systems which have knowledge representation components such as robots or software agents receive and process data which is incomplete, noisy, approximative and uncertain. This paper presents a framework for recursively constructing arbitrarily complex knowledge structures which may be compared for similarity, distance and approximativeness. It integrates nicely with more traditional knowledge representation techniques and attempts to bridge a gap between approximate and crisp knowledge representation. It can be viewed in part as a generalization of approximate reasoning techniques used in rough set theory. The strategy that will be used is to define tolerance and distance measures on the value sets associated with attributes or primitive data domains associated with particular applications. These tolerance and distance measures will be induced through the different levels of data and knowledge abstraction in complex representational structures. Once the tolerance and similarity measures are in place, an important structuring generalization can be made where the idea of a tolerance space is introduced. Use of these ideas is exemplified using two application domains related to sensor modeling and communication between agents.	approximation algorithm;complex systems;data structure;knowledge representation and reasoning;principle of abstraction;recursion;robot;rough set;set theory;software agent;spaces	Patrick Doherty;Witold Lukaszewicz;Andrzej Szalas	2003		10.1007/978-3-540-39451-8_35	data structure;computer science;artificial intelligence;software agent;machine learning;database;mathematics;robotics;algorithm	AI	-21.138482288955185	-2.159992489225083	90635
3b8a172187d456f4022433e651b26e33b3d7167b	prototype validation of the trapezoidal attribute cardinality map for query optimization in database systems	query optimization;query result size estimation	Current business database systems utilize histograms to approximate frequency distributions of attribute values of relations. These are used to efficiently estimate query result sizes and access plan costs and thus minimize the query response time for business (and non-commercial) database systems. In two recent works [10, 11] we proposed two new forms of histogram-like techniques called the Rectangular and Trape­ zoidal Attribute Cardinality Maps (ACM) respectively, that give much smaller estimation errors than the traditional equi-width and equi-depth histograms currently being used by many commercial database systems. In [10, 11] we also provided a fairly extensive mathematical analysis for their average and worst case errors for their frequency estimates - which, in turn, were verified for synthetic data. This paper reports the prototype validation for the Rectangular­ ACM (R-ACM) for query optimization in real-world database sys­ tems. By using an extensive set of experiments using real-life data [1,2], we demonstrate that the R-ACM scheme is much more accu­ rate than the traditional histograms for query result size estimation. We anticipate that it could become an invaluable tool for query opti­ mization in the future.		Murali Thiyagarajah;B. John Oommen	1999		10.1007/978-1-4471-0875-7_21	online aggregation;sargable;query optimization;query expansion;boolean conjunctive query;computer science;data mining;database;view;information retrieval	DB	-26.861857468137444	2.3780579394541346	90678
d3991cdb7deb1f5e387c79756df91e275e20702f	memcache and mongodb based gis web service	cache storage;web services cache storage encyclopedias relational databases geographic information systems;mongodb cached storage gis web service memcache;geographic information systems;relational database insufficiency memcache based gis web service mongodb based gis web service mass gis data storage mass gis data processing distributed cached storage solution concurrent tasks storage mechanism;web services;relational databases;gis web service;memcache;cached storage;mongodb;web services cache storage geographic information systems relational databases	One of challenges in GIS Web service is mass GIS data storage and processing. This paper proposes a distributed cached storage solution, as the mixture of Mongo DB storage and Memcached cache, for GIS Web service. This cached storage solution tends to solve three obstacles: the mismatch between the frequent requirement of concurrent tasks for accessing data in GIS Web service and the poor performance of conventional storage mechanism such as relational databases, the insufficiency of relational databases to manage complex data types in GIS Web service, the horizontal scalability of storage for non-stop GIS Web service evolution. A concise case indicates the feasibility of the cached storage solution.	attribute–value pair;cache (computing);computer data storage;convergence insufficiency;distributed cache;geographic information system;key-value database;memcached;mongodb;nosql;relational database;scalability;thread (computing);web service;world wide web	Jinghong Yang;Wuyang Ping;Lin Liu;Qiping Hu	2012	2012 Second International Conference on Cloud and Green Computing	10.1109/CGC.2012.19	web service;distributed gis;enterprise gis;web mapping;computer science;data mining;database;am/fm/gis;world wide web	HPC	-31.2153815254886	1.202514474194077	90698
21ae6277dbd022e7801bd6786b459251d9347569	efficient query answering in peer data management systems		Peer data management systems (Pdms) consist of a highly dynamic set of autonomous, heterogeneous peers connected with schema mappings. Queries submitted at a peer are answered with data residing at that peer and by passing the queries to neighboring peers. Pdms are the most general architecture for distributed integrated information systems. With no need for central coordination, Pdms are highly flexible. However, due to the typical massive redundancy in mapping paths, Pdms tend to be very inefficient in computing the complete query result as the number of peers increases. Additionally, information loss is cumulated along mapping paths due to selections and projections in the mappings. Users usually accept concessions on the completeness of query answers in largescale data sharing settings. Our approach turns completeness into an optimization goal and thus trades off benefit and cost of query answering. To this end, we propose several strategies that guide peers in their decision to which neighbors rewritten queries should be sent. In effect, the peers prune mappings that are expected to contribute few data. We propose a query optimization strategy that limits resource consumption and show that it can drastically increase efficiency while still yielding satisfying completeness of the query result. To estimate the potential data contribution of mappings, we adopted self-tuning histograms for cardinality estimation. We developed techniques that ensure sufficient query feedback to adapt these statistics to massive changes in a Pdms. Additionally, histograms can serve to maintain statistics on data overlap between alternative mapping paths. Building on them, redundant query processing is reduced by avoiding overlapping areas of the multi-dimensional data space.	autonomous robot;dataspaces;information system;mathematical optimization;prune and search;query optimization;schema evolution;self-tuning	Armin Roth	2011			query optimization;query expansion;web search query;query language	DB	-25.220663140878028	2.82949763044214	90750
0677be04cd4ac7831705eba68c9deafd244f48fd	masive: a case study in multiagent systems	multiagent system;multi agent system;realite virtuelle;realidad virtual;juego cooperativo;virtual reality;cooperative game;systeme conversationnel;interactive system;jeu cooperatif;sistema conversacional;interactive virtual environment;learning artificial intelligence;sistema multiagente;systeme multiagent;apprentissage intelligence artificielle	The project MASIVE (Multi-Agent Systems Interactive Virtual Environments) is the multi-agent extension of our Interactivist-Expectative Theory on Agency and Learning (IETAL). The agents in the environment learn expectations from their interactions with the environment. In addition to that, they are equipped with special sensors for sensing akin agents, and interchange their knowledge of the environment (their intrinsic representations) during their imitation conventions. In this paper we discuss the basics of the theory, and the social consequences of such an environment from the perspective of learning, knowledge dissemination, and emergence of language.	agent-based model;multi-agent system	Goran P. Trajkovski	2002		10.1007/3-540-45675-9_41	simulation;computer science;knowledge management;artificial intelligence;virtual reality	AI	-24.046027351151835	-8.210596939699101	90991
e407fce2233ad5d2487db18285ad8749cc91b70f	computer-aided triz ideality and level of invention estimation using natural language processing and machine learning	machine learning;natural language processing;problem solving;neural network	Patent textual descriptions provide a wealth of information that can be used to understand the underlying design approaches that result in the generation of novel and innovative technology. This article will discuss a new approach for estimating Degree of Ideality and Level of Invention metrics from the theory of inventive problem solving (TRIZ) using patent textual information. Patent text includes information that can be used to model both the functions performed by a design and the associated costs and problems that affect a design’s value. The motivation of this research is to use patent data with calculation of TRIZ metrics to help designers understand which combinations of system components and functions result in creative and innovative design solutions. This article will discuss in detail methods to estimate these TRIZ metrics using natural language processing and machine learning with the use of	machine learning;natural language processing;problem solving	Christopher Adams;Derrick Tate	2009		10.1007/978-3-642-03346-9_4	computer science;artificial intelligence;machine learning;data mining	AI	-33.17634502345021	-9.292439424299278	90998
c521fc23ef1ed38cfaf6ee3d901186b13a69dfdc	analysis of tpc-ds: the first standard benchmark for sql-based big data systems		The advent of Web 2.0 companies, such as Facebook, Google, and Amazon with their insatiable appetite for vast amounts of structured, semi-structured, and unstructured data, triggered the development of Hadoop and related tools, e.g., YARN, MapReduce, and Pig, as well as NoSQL databases. These tools form an open source software stack to support the processing of large and diverse data sets on clustered systems to perform decision support tasks. Recently, SQL is resurrecting in many of these solutions, e.g., Hive, Stinger, Impala, Shark, and Presto. At the same time, RDBMS vendors are adding Hadoop support into their SQL engines, e.g., IBMu0027s Big SQL, Actianu0027s Vortex, Oracleu0027s Big Data SQL, and SAPu0027s HANA. Because there was no industry standard benchmark that could measure the performance of SQL-based big data solutions, marketing claims were mostly based on cherry picked subsets of the TPC-DS benchmark to suit individual companies strengths, while blending out their weaknesses. In this paper, we present and analyze our work on modifying TPC-DS to fill the void for an industry standard benchmark that is able to measure the performance of SQL-based big data solutions. The new benchmark was ratified by the TPC in early 2016. To show the significance of the new benchmark, we analyze performance data obtained on four different systems running big data, traditional RDBMS, and columnar in-memory architectures.	alpha compositing;apache hadoop;apache hive;benchmark (computing);big data;computer cluster;data system;decision support system;ibm tivoli storage productivity center;in-memory database;mapreduce;nosql;open-source software;presto;relational database management system;sap hana;sql;semiconductor industry;technical standard;the void (virtual reality);vortex;web 2.0	Meikel Pöss;Tilmann Rabl;Hans-Arno Jacobsen	2017		10.1145/3127479.3128603	relational database management system;nosql;sql;oracle;computer science;big data;data mining;software;database;unstructured data;query by example	DB	-33.58284251435845	-0.41296960909921004	91059
c5741bfa97691222dfde78c0d93a66c4a1533a2c	determination of residence status for taxation law: development of a rule-based expert system	intelligent law information systems;rule based;information searching;sgml;law information systems;expert system	The purpose of this paper is to present details of the development of a rule-based expert system in an area of taxation law. The system is based on the determination of taxpayer residence within the definition in the Income Tm Assessment Act, the residence concept being one which underlies much of the consideration of assessability in the Act. The paper evaluates the sources of explanation of the concept of residence so as to be able to derive roles which can be used to determine the issue. Details are also provided as to the nature of the operation of the system, with the use of interactive dialogue to elicit details from which a fetermination may be made. While rule extraction and development is not without its difficulties, a prototype system has been developed. To progress the system further will require testing involving practical situations so as to enhance and refine the system for use in a commercial environment.	expert system;logic programming;prototype;rule induction	Rodney Fisher	1997		10.1145/261618.261649	rule-based system;legal expert system;computer science;knowledge management;artificial intelligence;data mining;expert system;sgml	HCI	-33.56864188507495	-8.352137833293833	91505
7fb94f3611ceee711dd31adbd1cf24d913fb0b70	mining thick skylines over large databases	cardinal number;base donnee;analisis estadistico;analisis datos;methode cholesky;metodo cholesky;efficient algorithm;echantillonnage;database;base dato;probabilistic approach;conjugate gradient method;data mining;sampling;multi dimensional;data analysis;nombre cardinal;numero cardinal;statistical analysis;indexing;fouille donnee;metodo gradiente conjugado;enfoque probabilista;approche probabiliste;indexation;decouverte connaissance;analyse statistique;indizacion;descubrimiento conocimiento;analyse donnee;methode gradient conjugue;muestreo;cholesky method;busca dato;knowledge discovery	People recently are interested in a new operator, called skyline [3], which returns the objects that are not dominated by any other objects with regard to certain measures in a multi-dimensional space. Recent work on the skyline operator [3, 15, 8, 13, 2] focuses on efficient computation of skylines in large databases. However, such work gives users only thin skylines, i.e., single objects, which may not be desirable in some real applications. In this paper, we propose a novel concept, called thick skyline, which recommends not only skyline objects but also their nearby neighbors within ε-distance. Efficient computation methods are developed including (1) two efficient algorithms, Sampling-andPruning and Indexing-and-Estimating, to find such thick skyline with the help of statistics or indexes in large databases, and (2) a highly efficient Microcluster-based algorithm for mining thick skyline. The Microclusterbased method not only leads to substantial savings in computation but also provides a concise representation of the thick skyline in the case of high cardinalities. Our experimental performance study shows that the proposed methods are both efficient and effective.	algorithm;birch;cluster analysis;computation;computational geometry;convex function;data mining;database;execution unit;experiment;maximal set;os-tan;parallel algorithm;parallel computing;preparata code;programming paradigm;skyline operator;springer (tank);vldb	Wen Jin;Jiawei Han;Martin Ester	2004		10.1007/978-3-540-30116-5_25	cardinal number;sampling;search engine indexing;computer science;data mining;database;mathematics;conjugate gradient method;knowledge extraction;data analysis;algorithm;statistics	DB	-26.31777070139731	2.849212222173154	91553
bff00a9cf67d32cdc5170dbd3256d13dc877ce5f	design and implementation of an index structure using fixed intervals for tracing of rfid tags	arbre r;modelizacion;trajectoire;identificacion por radiofrecuencia;localization;base donnee temporelle;interrogation base donnee;index structure;interrogacion base datos;tracing;localizacion;rfid tag;identification par radiofrequence;r tree;spatial database;data model;modelisation;arbol r;localisation;trajectory;design and implementation;indexation;tracage;radio frequency identification;spatial data structures;trayectoria;temporal databases;base dato especial;modele donnee;base de donnees spatiale;fixed interval;modeling;database query;trazado;data models;structure donnee spatiale	Recently, there has been a demand for RFID systems that can trace tag locations. For tracing tag locations, trajectories should be modeled and indexed in an RFID system. The trajectory of a tag is represented as a line that connects two spatiotemporal locations that are captured when the tag enters and leaves the vicinity of a reader. If a tag enters but does not leave a reader, its trajectory is represented only as the point captured at entry. When we process a query that finds the tag staying in a reader, it takes a long time to find this tag because it leads to searching the whole index. To solve this problem, we propose a data model in which trajectories of these tags are defined as intentional fixed intervals and a new index scheme called the Fixed Interval R-tree. We also propose a new insert and split policy to process queries efficiently. We evaluated the performance of the proposed index scheme and compared it with other schemes on various datasets and queries.	radio-frequency identification	Sungwoo Ahn;Bonghee Hong;ChaeHoon Ban;Kihyung Lee	2006		10.1007/11751588_19	radio-frequency identification;tag system;computer science;data mining;database	Mobile	-26.264061787024172	1.1766013034946283	91676
01bf799a2fce5b55acae80ace3b473789bee4c95	use of superimposed code words for partial match data retrieval	data retrieval		data retrieval	Robert M. Colomb	1985	Australian Computer Journal		visual word;computer science;pattern recognition;data mining;data retrieval;information retrieval	Theory	-28.233523554257065	-0.07519722950212948	91930
5b5bb8b051e7d911aed7912d94d91f85cd4fb5c8	a strategy to requirements engineering based on knowledge management	pragmatics;formal specification;knowledge management formal specification;cognitive impairment requirements engineering strategy knowledge management informal structured domains software system;tacit knowledge;knowledge management;software systems;requirements engineering;unified modeling language ontologies knowledge management software systems pragmatics;unified modeling language;knowledge management informal structured domains requirements engineering tacit knowledge;ontologies;informal structured domains	In Informal Structured Domains not all concepts and their relations are formally defined, the most of the problems does not have algorithms to obtain solutions, and the domain specialists use large amounts of tacit knowledge to solve problems. These characteristics generate ambiguous, inappropriate and incomplete requirements that could increase the development time of a software system. In order to minimize these problems, we propose a Requirements Engineering strategy based on knowledge management. As a case study, the strategy was used to the development of a software system, which evaluates the cognitive impairment in patients with sclerosis multiple.	algorithm;knowledge management;requirement;requirements engineering;software system;structured programming	Karla Olmos Sanchez;Jorge Enrique Rodas Osollo	2013	2013 Mexican International Conference on Computer Science	10.1109/ENC.2013.10	unified modeling language;requirements analysis;knowledge base;software requirements specification;software mining;computer science;knowledge management;ontology;requirement;software engineering;knowledge-based systems;knowledge engineering;formal specification;management science;requirements engineering;knowledge extraction;personal knowledge management;knowledge value chain;domain knowledge;software system;pragmatics	SE	-32.339547913360654	-5.462765302299835	92077
cefdd2005b3919f39765ed1357c29fa10dc725b1	indexing mobile objects using dual transformations	moving object;red sin hilo;spatiotemporal databases;range query;reseau sans fil;methode acces;localizacion objeto;moving object database;object location;wireless network;systeme gps;interrogation base donnee;interrogacion base datos;access methods;high concentrate;access methods mobile objects;mobile object;gps system;embedded system;spatial database;mobile phone;dynamic environment;indexing;base donnee spatiale;indexation;indizacion;base donnee orientee objet;spatial data structures;base dato especial;object oriented databases;experimental evaluation;external memory;base donnee spatiotemporelle;dual space;access method;localisation objet;radiolocalizacion;radiolocalisation;database query;mobile objects;sistema gps;structure donnee spatiale	With the recent advances in wireless networks, embedded systems, and GPS technology, databases that manage the location of moving objects have received increased interest. In this paper, we present indexing techniques for moving object databases. In particular, we propose methods to index moving objects in order to efficiently answer range queries about their current and future positions. This problem appears in real-life applications such as predicting future congestion areas in a highway system or allocating more bandwidth for areas where a high concentration of mobile phones is imminent. We address the problem in external memory and present dynamic solutions, both for the one-dimensional and the two-dimensional cases. Our approach transforms the problem into a dual space that is easier to index. Important in this dynamic environment is not only query performance but also the update processing, given the large number of moving objects that issue updates. We compare the dual-transformation approach with the TPR-tree, an efficient method for indexing moving objects that is based on time-parameterized index nodes. An experimental evaluation shows that the dual-transformation approach provides comparable query performance but has much faster update processing. Moreover, the dual method does not require establishing a predefined query horizon.	aggregate data;database;embedded system;global positioning system;mobile phone;network congestion;privacy;range query (data structures);real life;the stanley parable;velocity (software development)	George Kollios;Dimitris Papadopoulos;Dimitrios Gunopulos;Vassilis J. Tsotras	2004	The VLDB Journal	10.1007/s00778-004-0139-z	computer vision;computer science;data mining;database;access method	DB	-26.46788876683255	1.1204916835604768	92326
3c6000b323e76680f1cf049b8df50fe637e8ff26	interpretation of implicit parallel structures. a case study with “vice-versa”	sensibilidad contexto;sistema interactivo;lenguaje natural;linguistique;context aware;semantic representation;educational software program;langage naturel;semantics;analogie;didacticiel;semantica;semantique;systeme conversationnel;reconstruction image;linguistica;reconstruccion imagen;interactive system;image reconstruction;natural language;inferencia;analogy;analogia;preferencia;preference;programa didactico;information system;sensibilite contexte;systeme information;inference;sistema informacion;linguistics	Successful participation in task-oriented, inference-rich dialogs requires, among other things, understanding of specifications implicitly conveyed through the exploitation of parallel structures. Several linguistic operators create specifications of this kind, including “the other way (a)round”, “vice-versa”, and “analogously”; unfortunately, automatic reconstruction of the intended specification is difficult due to the inherent dependence on given context and domain. We address this problem by a well-informed reasoning process. The techniques applied include building deep semantic representations, application of categories of patterns underlying a formal reconstruction, and using pragmatically-motivated and domain-justified preferences. Our approach is not only suitable for improving the understanding in everyday discourse, but it specifically aims at extending capabilities in a tutorial dialog system, where stressing generalities and analogies is a major concern.		Helmut Horacek;Magdalena Wolska	2005		10.1007/11428817_20	iterative reconstruction;analogy;computer science;artificial intelligence;semantics;linguistics;natural language;information system;algorithm	NLP	-25.030128539063387	-7.83086865378787	92378
e3760a253fd8eca9724f6a04e3c6bfcbad28ec1d	efficient lca based keyword search in xml data	keyword;low frequency;search;lca;keyword search;indexation;xml;xml document;experimental evaluation;high frequency	Keyword search in XML documents based on the notion of lowest common ancestors (LCAs) and modifications of it has recently gained research interest [2, 3, 4]. In this paper we propose an efficient algorithm called Indexed Stack to find answers to keyword queries based on XRank's semantics to LCA [2]. The complexity of the Indexed Stack algorithm is O(kd|S1|\log|S|) where k is the number of keywords in the query, d is the depth of the tree and |S1 | (|S|) is the occurrence of the least (most) frequent keyword in the query. In comparison, the best worst case complexity of the core algorithms in [2] is O(kd|S|). We analytically and experimentally evaluate the Indexed Stack algorithm and the two core algorithms in [2]. The results show that the Indexed Stack algorithm outperforms in terms of both CPU and I/O costs other algorithms by orders of magnitude when the query contains at least one low frequency keyword along with high frequency keywords.	best, worst and average case;central processing unit;experiment;indexed grammar;input/output;search algorithm;worst-case complexity;xml	Yu Xu;Yannis Papakonstantinou	2007		10.1145/1321440.1321597	xml;computer science;data mining;database;keyword density;information retrieval	DB	-30.82546142273297	3.6985189915947654	92430
6b176ad72363e4c825b49189fa4e21741b612abc	probabilistic time consistent queries over moving objects	uncertain moving object database;probabilistic time consistent query	Recently, the wide usage of inexpensive mobile devices, along with broad deployment of wireless and positioning technology, has enabled many important applications such as Delay Tolerant Networks (DTN). In these applications, the positions of mobile nodes are dynamically changing, and are often imprecise due to the inaccuracy of positioning devices. Therefore, it is crucial to efficiently and effectively monitor mobile nodes (modeled as uncertain moving objects). In this paper, we propose a novel query, called probabilistic time consistent query (PTCQ). In particular, a PTCQ retrieves uncertain moving objects that consistently satisfy query predicates within a future period with high confidence. We present effective pruning methods to reduce the search space of PTCQs, and seamlessly integrate them into an efficient query procedure. Moreover, to facilitate query processing, we specifically design a data structure, namely UC-Grid, to index uncertain moving objects. The structure construction is based on a formal cost model to minimize the query cost. Extensive experiments demonstrate the efficiency and effectiveness of our proposed approaches to answer PTCQs.	time consistency	Xiang Lian;Lei Chen	2011		10.1007/978-3-642-22351-8_10	sargable;query optimization;query expansion;real-time computing;web query classification;computer science;data mining;database	DB	-25.499186288755006	0.9603912624817069	92506
79765585d78116599835fcfed0a55e1b3a121ec6	the use of paraconsistent logic in negotiation among artificial organizations	multivalued decrement list;page description languages;collaborative work;page description languages counting circuits multivalued logic multiagent systems marketing and sales testing collaborative work;testing;counting circuits;multi agent systems;logic programming;multivalued decrement list negotiation strategy paraconsistent logic organization knowledge;negotiation strategy;negotiation support systems;multivalued logic;logic programming multi agent systems negotiation support systems;multiagent systems;marketing and sales;organization knowledge;paraconsistent logic	The mission of an organization stands for its goals and also leads corrections likely to occur in the posture adopted by the organization before the society. In order to fulfill the goals and achieved the mission of the organization, this one needs to interact with other components of the society. One kind of interaction which is very useful for organizations is the negotiation. To obtain a good result in a negotiation the responsible for the negotiation must use a good negotiation strategy; also he must know how to evaluate an offer according to the organization needings. The result of the offer evaluation will determine the actions that will be taken by the negotiator, either the offer will be accepted or a counter offer will be created. The use of paraconsistent logic provides a natural way, when compared to the utility value approach, to represent and evaluate the offer according to the organization knowledge. This paper presents a paraconsistent approach based on a heuristic of multi-valued decrement list followed by formalization into evidential paraconsistent logic to evaluate offers in a negotiation session.	counter (digital);heuristic;increment and decrement operators;paraconsistent logic;poor posture	Fabiano M. Hasegawa;Bráulio Coelho Ávila;Marcos Augusto Hochuli Shmeil	2005	Proceedings of the Ninth International Conference on Computer Supported Cooperative Work in Design, 2005.	10.1109/CSCWD.2005.194141	paraconsistent logic;computer science;knowledge management;artificial intelligence;multi-agent system;database;management science;software testing;logic programming	SE	-19.990098862018222	-8.934499962192671	92512
24355cc9348724fee1da06e07c66507167bfbabb	computational model for conceptual design based on extended function logic	logic design;computer model;conceptual analysis;intelligence artificielle;analisis conceptual;function;value;schema fonctionnel;esquema funcional;conceptual design;conception logique;conceptual;valeur;artificial intelligence;design;function block diagram;inteligencia artificial;analyse conceptuelle;valor;concepcion logica	Function logic methods have been successfully used in Value Analysis (VA) and Value Engineering (VE) for several decades. This functional approach attempts to provide a common language for specialists in multiple domains. This paper describes an extension of function logic that assists in systematic identification of design functions, allocations, and their interrelations. Our approach identifies a three-level function/allocation/component information structure to represent the state of the design. We illustrate new types of links that exist between functions and the effect of these on the representation of the interrelated functions. These linkages provide new pathways for design information and function evaluation through allocation arithmetic and supported functions. A computational model of the conceptual design process is proposed based on the extended function logic design representation. An outline of the inputs, outputs and operations on form and function variables is given as a step prior to the synthesis process. We illustrate, by example, the process of translating functional representations across specialist domains. Finally, a computer-based aid to developing functional models is described.	computation;computational model;functional approach;hyperlink;mental representation	Robert H. Sturges;Kathleen O'Shaughnessy;Mohammed I. Kilani	1996	AI EDAM	10.1017/S089006040000161X	computer simulation;design;computer science;engineering;artificial intelligence;conceptual design;function block diagram;function;algorithm	AI	-23.882121753265302	-1.6673059246150677	92737
32d13cc8f8bacebdb30cdf4715785e1e3da2ada6	modeling processes from timed observations		This paper presents a modelling approach of dynamic process for diagnosis that is compatible with the Stochastic Approach framework for discovering temporal knowledge from the timed observations contained in a database. The motivation is to define a multi-model formalism that is able to represent both the knowledge of these two sources. The aim is to model the process at the same level of abstraction that an expert uses to diagnose the process. The underlying idea is that at this level of abstraction, the model is simple enough to allow an efficient diagnosis. The proposed formalism represents the knowledge in four models: a structural model defining the components and the connection relations of the process, a behavioural model defining the states and the transitions states of the process, a functional model containing the logical relations between the values of the process’s variables, which are defined in the perception model. The models are linked with the process’s variables. This point facilitates the analysis of the consistency of the four models and is the basis of the corresponding knowledge modelling methodology. The formalism and the methodology is illustrated with the model of a hydraulic dam of Cublize (France).	computation;computational problem;formal system;function model;logical relations;medical algorithm;multi-model database;recursion;semantics (computer science)	Marc Le Goc;Emilie Masse;Corinne Curt	2008			data mining;computer science;logical relations;machine learning;algorithm;artificial intelligence;formalism (philosophy);abstraction	AI	-20.81899887790036	1.1294750454865523	92893
b116152762d9a5846897f8b722c9b9dfc82aeed0	practical implications of handling multiple contexts in the principle of polyrepresentation	busqueda informacion;modelizacion;logica booleana;theoretical framework;information retrieval;modelisation;recherche information;logique booleenne;information system;intencion;information need;boolean logic;modeling;information seeking;systeme information;intention;sistema informacion	The principle of polyrepresentation, proposed more than 10 years ago, offers a holistic theoretical framework for handling multiple contexts in Information Retrieval (IR), and allows integration of representation and matching of both documents as well as the information seeker's information need in context. Relatively few empirically based studies have, however, applied the principle explicitly for IR purposes. This paper examines the principle of polyrepresentation, and analyses the practical implications of applying it to multiple contexts in best match IR research. It is concluded that the principle is inherently Boolean in its foundation in spite of its intentions to be applicable to both exact and best match IR. This may constitute a major obstacle for the application of the principle in main stream IR and information seeking research. A polyrepresentation continuum is proposed as an illustration of this problem, and as a model for developing the principle towards greater practical applicability.		Birger Larsen	2005		10.1007/11495222_4	boolean algebra;information needs;systems modeling;computer science;artificial intelligence;machine learning;data mining;mathematics;information retrieval;information system;algorithm	HCI	-23.435323789479025	-1.3128667586260137	92984
7032c42a413dddc5e39c4960d2e08fd93e44c6dd	a query cache tool for optimizing repeatable and parallel olap queries	decision support;decision maker;experimental evaluation;data warehouse;on line analytical processing	On-line analytical processing against data warehouse databases is a common form of getting decision making information for almost every business field. Decision support information oftenly concerns periodic values based on regular attributes, such as sales amounts, percentages, most transactioned items, etc. This means that many similar OLAP instructions are periodically repeated, and simultaneously, between the several decision makers. Our Query Cache Tool takes advantage of previously executed queries, storing their results and the current state of the data which was accessed. Future queries only need to execute against the new data, inserted since the queries were last executed, and join these results with the previous ones. This makes query execution much faster, because we only need to process the most recent data. Our tool also minimizes the execution time and resource consumption for similar queries simultaneously executed by different users, putting the most recent ones on hold until the first finish and returns the results for all of them. The stored query results are held until they are considered outdated, then automatically erased. We present an experimental evaluation of our tool using a data warehouse based on a real-world business dataset and use a set of typical decision support queries to discuss the results, showing a very high gain in query execution time.	database;decision support system;information retrieval;online analytical processing;optimizing compiler;run time (program lifecycle phase)	Ricardo Jorge Santos;Jorge Bernardino	2009		10.1007/978-3-642-03573-9_11	decision-making;computer science;theoretical computer science;data warehouse;data mining;database;world wide web;spatial query	DB	-30.420183898549684	-0.8694788389823355	93056
7d1473253816506cae8f3b5e84a76be1c622fa62	using $\mathcal{soiq}$ (d) to formalize semantics within a semantic decision table	semantic decision table;conceptual modeling;description logics	As an extension to decision tables, Semantic Decision Tables (SDTs) are considered as a powerful tool of modeling processes in various domains. An important motivation of consuming SDTs is to easily validate a decision table during the Validation and Verification (V&V) processes. An SDT contains a set of formal agreements called commitments. They are grounded on a domain ontology and considered as a result from group decision making processes, which involve a community of business stakeholders. A commitment contains a set of constraints, such as uniqueness and mandatory, with which we can analyze a decision table. A vital analysis issue is to detect inconsistency, which can arise within one table or across tables. In this paper, we focus on the formalization of the semantics within one SDT using the Description Logic $\mathcal{SOIQ}$(D). By doing so, we can use existing reasoners to detect inconsistency and thus assist decision modelers (and evaluators) to validate a decision table.	semantic decision table	Yan Tang Demey;Trung-Kien Tran	2012		10.1007/978-3-642-32689-9_18	decision table;description logic;decision analysis;decision engineering;computer science;artificial intelligence;conceptual model;data mining;decision rule;database;algorithm;business decision mapping	NLP	-30.83830172573728	-4.493607866711847	93103
be9ddb6017b8f2cde324191f6a2c40c643a99e8f	a novel modeling methodology : generalized nets	modelizacion;distributed system;discrete dynamical system;systeme reparti;neural networks;red petri;model generation;soft computing;intelligence artificielle;calculo flexible;dynamical system;modelisation;systeme dynamique;sistema repartido;calcul souple;artificial intelligence;inteligencia artificial;generalized nets;sistema dinamico;modeling methodology;reseau neuronal;knowledge representation;petri net;modeling;red neuronal;reseau petri;neural network	A Case for on-line data analysis for large-scale scientific simulations Alok Choudhary .............................................................................................................5 Development and execution of HPC applications on Clusters and Grids by P-GRADE Peter Kacsuk ...................................................................................................................6 Modelling Physics from Materials to Environmental Problems Lucilla de Arcangelis......................................................................................................14	alok r. chaturvedi;computer cluster;grid computing;online and offline;simulation	Maciej Krawczak	2006		10.1007/11785231_121	systems modeling;computer science;artificial intelligence;dynamical system;machine learning;process architecture;petri net;artificial neural network;algorithm	HPC	-27.63003301828087	-5.61379662857516	93106
0fb0659e148c8f063bb41013c888762c670e3955	the failure of bayes system reliability inference based on data with multi-level applicability	bayes procedure;system reliability;fiabilidad;reliability;fiabilite systeme;aerospace;bayes methods;decision bayes;inconsistance;bayes decision;bayes methods reliability theory failure analysis missiles aerospace;reliability theory;fiabilidad sistema;missiles;failure analysis;model error;fiabilite;bayes procedure restructuring bayes system reliability inference failure multi level applicability operation experience data test data failure rate occurrence analysis missile launches space launches;uncertainty vehicles failure analysis reliability engineering information analysis system testing hazards missiles electric breakdown aerospace engineering	When test or operation-experience data are used that apply at once to two or more system-levels, Bayes reliability inferences at different levels generally give inconsistent results. This inconsistency, quite counter-intuitive when it is first recognized, is particularly important in major analyses of occurrence rates of failures and consequent hazards in space and missile launches. It arises as the lack of perfect aggregation in other applications, as well. Others might consider this inconsistency merely a modeling error, and then attempt, with negligible success, to find means for ameliorating it. The author believes it represents a fundamental breakdown in the usual Bayes methodology in the circumstances noted. A basic restructuring of the Bayes procedure is required (e.g. by exchanging the roles of engineering and experience data). The author gives an illustrative example.		L. L. Philipson	1996	IEEE Trans. Reliability	10.1109/24.488918	reliability engineering;failure analysis;reliability theory;engineering;errors-in-variables models;data mining;reliability;aerospace;forensic engineering;statistics	EDA	-19.608285206376227	-3.2094754552763307	93133
604f8034b5ec2f0a9e26233b74f85fb8c7d8adfd	performance issues of heterogeneous hadoop clusters in cloud computing		Nowadays most of the cloud applications process large amount of data to provide the desired results. Data volumes to be processed by cloud applications are growing much faster than computing power. This growth demands new strategies for processing and analyzing information. Dealing with large data volumes requires two things: 1) Inexpensive, reliable storagee 2) New tools for analyzing unstructured and structured data. Hadoop is a powerful open source software platform that addresses both of these problems. The current Hadoop implementation assumes that computing nodes in a cluster are homogeneous in nature. Hadoop lacks performance in heterogeneous clusters where the nodes have different computing capacity. In this paper we address the issues that affect the performance of hadoop in eterogeneous clusters and also provided some guidelines on how to overcome these	apache hadoop;cloud computing;computer cluster;open-source software	B. Thirumala Rao;N. V. Sridevi;V. Krishna Reddy;L. S. S. Reddy	2011	CoRR		parallel computing;computer science;data-intensive computing;database;distributed computing	HPC	-32.31778943546349	-1.1178348006808538	93145
6e4d8acba115366e32031988634f0de8601187f4	an agent-based approach to robust switching between abstraction levels for fault diagnosis	multiagent system;systeme intelligent;abstraction hierarchy;systeme apprentissage;agent based;sistema inteligente;production system;systeme production;productique;sistema produccion;artificial intelligent;learning systems;diagnostic panne;fault diagnostic;diagnostico pana;intelligent system;robotica;sistema multiagente;computer integrated manufacturing;systeme multiagent;fault diagnosis	Many artificial intelligence approaches to automated fault diagnosis employ functional or symptomatic abstraction hierarchies in their reasoning process. However, these approaches fail to provide rapid response and adaptability comparable to humans experts. This paper presents an approach which allows robust, unstructured switching between abstraction levels and types using agents that examine the problem domain from different perspectives. This approach was implemented and tested with promising results.		Terrence P. Fries;James H. Graham	2000		10.1007/3-540-44914-0_20	simulation;computer science;artificial intelligence;computer-integrated manufacturing;production system	AI	-23.463803837525898	-4.933619026472355	93180
892050d938de14ff3c598e988dde501dc9f2b38c	fuzzy loss less decompositions in databases	fuzzy relational database;base relacional dato;projection operator;fuzzy rules;fuzzy join;fuzzy relation;relational database;satisfiability;functional dependency;fuzzy projection;fuzzy functional dependencies;base donnee relationnelle;fuzzy databases;relation floue;fuzzy database;relacion difusa	In this paper we deal with the problem of deening projection and join operators in the framework of a fuzzy relational database. We study the properties of such an operators relating to the concept of fuzzy functional dependency. There are two main properties to achieve: On the one hand, if a relation r satisses a dependency between two sets of attributes, X and Y , then we want to preserve this dependency in the fuzzy projection over XY of r. On the other hand, we want to test a fuzzy dependency just by looking at this projection. Therefore, such a projection operator must allow us to store the information conveyed in r into a separate relation with fewer tuples. In order to deene the criterion for fuzzy dependency, we shall need to extend a resemblance relation deened for crisp data to the fuzzy case. Finally, we shall introduce a fuzzy join operator which will allow us to recover the information given in a relation, from its projections 1. In the rst section we present the notation we are going to follow, and introduce basic concepts about fuzzy relational databases. We also introduce the fuzzy extension of a resemblance relation, which will be applied in Sections two and three to deene a criterion of fuzzy dependency and projection, respectively. In the nal section we address the issue of fuzzy loss less decompositions. 1 Preliminaries In this section we are going to introduce some basic concepts and the notation we are going to follow throughout this paper. First of all, we very brieey revise some fuzzy concepts as well as the classical relational database model. Then, we shall introduce the concept of fuzzy database which constitutes the basic framework to deene other concepts such as fuzzy functional dependencies.	classical xy model;database model;functional dependency;fuzzy concept;fuzzy logic;fuzzy set;relational database;relational model	Juan C. Cubero;Juan Miguel Medina;Olga Pons;M. Amparo Vila	1998	Fuzzy Sets and Systems	10.1016/S0165-0114(96)00366-1	discrete mathematics;defuzzification;projection;type-2 fuzzy sets and systems;fuzzy mathematics;relational database;fuzzy classification;computer science;fuzzy subalgebra;fuzzy number;join dependency;data mining;mathematics;functional dependency;fuzzy set operations;algorithm;multivalued dependency;satisfiability	AI	-22.20898321348546	3.1656895964309943	93413
b37c69523dbaa84c08602f68a758725cef5571fd	intelligent paraconsistent logic controller and autonomous mobile robot emmy ii	robot movil;systeme aide decision;ingenierie connaissances;logique paraconsistante;autonomous system;prise de decision;intelligence artificielle;logical programming;sistema ayuda decision;autonomous mobile robot;sistema autonomo;automatisme logique;logic controller;systeme incertain;decision support system;robot mobile;programmation logique;systeme autonome;logica paraconsistente;artificial intelligence;inteligencia artificial;automatismo logico;toma decision;sistema incierto;programacion logica;uncertain system;moving robot;paraconsistent logic;knowledge engineering	In this work we present a logic controller based on Paraconsistent Annotated Logic named Paracontrol, which can be applied to resolve conflicts and to deal with contradictions and/or paracompleteness, by implementing a decision-making in the presence of uncertainties. Such controller was implemented in a real autonomous mobile robot Emmy II.		Jair Minoro Abe;Cláudio Rodrigo Torres;Germano Lambert-Torres;Kazumi Nakamatsu;Michiro Kondo	2006		10.1007/11893004_108	control engineering;engineering;artificial intelligence;algorithm	Robotics	-23.058692800502655	-5.811239307893856	93468
db7b23b1847671b2bfbcf032cd67533520fbf313	sharding by hash partitioning - a database scalability pattern to achieve evenly sharded database clusters	pattern;hash partitioning;database sharding;scalability	With the beginning of the 21st century, web applications requirements dramatically increased in scale. Applications like social networks, ecommerce, and media sharing, started to generate lots of data traffic, and companies started to track this valuable data. The database systems responsible for storing all this information had to scale in order to handle the huge load. With the emergence of cloud computing, scaling out a database system has became an affordable solution, making data sharding a viable scalability option. But to benefit from data sharding, database designers have to identify the best manner to distribute data among the nodes of shared cluster. This paper discusses database sharding distribution models, specifically a technique known as hash partitioning. Our objective is to catalog in the format of a Database Scalability Pattern the best practice that consists in sharding the data among the nodes of a database cluster using the hash partitioning technique to nicely balance the load between the database servers. This way, we intend to make the mapping between the scenario and its solution publicly available, helping developers to identify when to adopt the pattern instead of other sharding techniques.	best practice;cloud computing;database server;e-commerce;emergence;high-availability cluster;image scaling;partition (database);requirement;scalability;shard (database architecture);social network;web application	Caio H. Costa;João Vianney B. M. Filho;Paulo Henrique M. Maia;Francisco Carlos M. B. Oliveira	2015		10.5220/0005376203130320	scalability;computer science;operating system;data mining;database;pattern;world wide web	DB	-32.121488159219616	-1.0075954118311885	93557
9ee749e79aa97207a172ddbd036319264927d0f2	an in-network forwarding index for processing historical location query in object-tracking sensor networks	forwarding;wireless sensor network;spatio-temporal data management;index design.;query processing	  Object tracking in wireless sensor networks is critical for many applications, e.g., military and biology, and has been studied  in past years. Based on the object tracking technology, many services become feasible in wireless sensor networks, such as  location management and nearest query. However, most services in the related work are for retrieving the current information  of a moving object or sensor network. In this paper, we investigate the processing of historical location query (HLQ) in the  wireless sensor network. We propose the two-tier HLQ processing architecture to management the object locations: the first  tier, the database server, is a query gateway and maintains volumes of object location history; the second tier, in-network forwarding index, maintains the recent location history of a moving object to save communication cost by using aggregation techniques. Then,  we invent the forwarding index-based query processing scheme for history location queries. We conduct a primary version of experiments to observe the performance characteristic of our  proposed method. The results reveal that the proposed method is indeed effective for processing HLQ.    		Chao-Chun Chen;Chia‑Da Hsu	2011		10.1007/978-3-642-20975-8_26	data mining;database;computer network	Mobile	-27.076818597886565	0.9703283572667567	94209
7908addea8c85027246fbb174db88e6fed5f4a7b	automatized systems for knowledge acquisition			knowledge acquisition	Gennady L. Andrienko;Natalia V. Andrienko;Gennady Ginkul;Sergey Soloview	1993	The Computer Science Journal of Moldova		discrete mathematics;mathematics;theoretical computer science;knowledge acquisition	Logic	-28.008271953473017	-7.568941038535439	94598
13b2c572321959aee14d4cbb3f59cf1245bc1d2f	deciding when to commit to action during observation-based coordination		We have developed a multiagent scheme which utilizes plan recognition as its primary means of acquiring the information ecessary to coordinate the activities of agents. Pr~llmlnary research has demonstrated that the plan recognition system developed makes coordination of multiple agents possible. An important issue that arises when observation is the primary means of information acquisition is the introduction of uncertainty into the coordination process. We have explored the issue of early versus late commitment to he uncertain information thus gained and the resulting tradeof between time and effort as the commitment level is changed. Our results show that while in some situations it is worthwhile delaying commitment u il uncertainty is reduced, in other situations it is important to act even when uncertainty is high. The longterm goal of the research is to develop the notion of coordination through observation, where agents utilize plan recognition to acquire coordination i formation.	agent-based model	Marcus J. Huber;Edmund H. Durfee	1995			simulation;engineering;knowledge management;operations management	AI	-19.718036619339006	-8.828596801901613	94647
0726d43d8e61eb37a717d2b2cb797a679f6c4cf5	a distributed quadtree index for peer-to-peer settings	database indexing;distributed databases peer to peer computing quadtrees query processing visual databases database indexing;information systems;real estate;query processing;p2p;client server;peer to peer computing computer science software engineering educational institutions application software client server systems indexing testing query processing;indexation;distributed databases;hash function;query computation distributed quadtree index peer to peer settings p2p networks chord method p2p routing protocol fault tolerant hashing methods load distribution;peer to peer computing;peer to peer;quadtrees;conference proceeding;visual databases	We describe a distributed quadtree index for enabling more powerful access on complex data over P2P networks. It is based on the Chord method. Methods such as Chord have been gaining usage in P2P settings to facilitate exact-match queries. The Chord method maps both the data keys and peer addresses. Our work can be applied to higher dimensions, to various data types, i.e., other than spatial data, and to different types of quadtrees. Finally, we can use other key-based methods than the Chord method as our base P2P routing protocol and index scale well. The index also benefits from the underlying fault-tolerant hashing-based methods by achieving a nice load distribution among many peers. We can seamlessly execute a single query on multiple branches of the index hosted by a dynamic set of peers.	fault tolerance;load balancing (computing);map;peer-to-peer;quadtree;routing	Egemen Tanin;Aaron Harwood;Hanan Samet	2005	21st International Conference on Data Engineering (ICDE'05)	10.1109/ICDE.2005.7	database index;hash function;computer science;theoretical computer science;peer-to-peer;data mining;database;world wide web;distributed database;information system;client–server model;real estate	DB	-29.554808355671764	0.8133047948734432	94672
8415892e30bc07de38b4c42ea5492d06f7f42260	some approaches to handle noise in concept learning	representacion conocimientos;quantization noise;sistema informatico;apprentissage conceptuel;base connaissance;computer system;intelligence artificielle;raisonnement;reduccion ruido;aprendizaje conceptual;noise reduction;reduction bruit;razonamiento;concept learning;artificial intelligence;base conocimiento;systeme informatique;inteligencia artificial;reasoning;knowledge representation;representation connaissances;bruit quantification;ruido cuantificacion;knowledge base	Noise is a veritable problem in almost all real-world systems. Noise is something the system should effectively recognize and gracefully handle. In this paper, we focus on the effects of attribute and quantization noise on concept learning. Different noise-handling methodologies can be employed depending on whether inductive approaches or theory-based approaches are being employed. We find that associating weights and thresholds with concepts hypothesized using conventional inductive approaches can help combat noise. In an explanation-baaed framework, we suggest the use of a meta-domain theory for controlling the effects of quantization noise and for capturing meta-level notions such as the amount of precision called for in the domain. Based on the idea of near-misses, two new algorithms which can insulate standard explanation-based learning from the effects of noise are presented. These approaches have been validated within the framework of a system that models the process of learning and reasoning about everyday objects and their interrelationships.	algorithm;concept learning;domain theory;explanation-based learning;quantization (signal processing);world-system	Ganesh Mani	1992	International Journal of Man-Machine Studies	10.1016/0020-7373(92)90012-A	knowledge base;concept learning;quantization;computer science;artificial intelligence;machine learning;noise reduction;reason;algorithm	AI	-21.657487517577245	-3.395714774799715	94881
f4f06d3bd08428aabeb3764604a2cf504b63346e	a framework for robust sensing in multi-agent systems	intelligent sensor;robocup 2000;inf;multiagent system;systeme intelligent;multi agent system;sistema inteligente;robotics;intelligent robot;intelligent system;robotica;robotique;robot inteligente;sistema multiagente;robot intelligent;systeme multiagent	We present the framework we have adopted to implement robust sensing in the Milan Robocup F–2000 Team. The main issue concerns the definition of symbolic models both for the single agent and for the whole multi–agent system. Information about physical objects is collected by intelligent sensors and anchored to symbolic concepts. These are used both to control the robots through a behavior–based system, and to improve the global knowledge about the environment.	multi-agent system;robot;sensor	Andrea Bonarini;Matteo Matteucci;Marcello Restelli	2001		10.1007/3-540-45603-1_32	simulation;computer science;artificial intelligence;robotics;intelligent sensor	AI	-22.10767379762384	-6.932908119609829	94912
71cd6c60325e8393e05a2ce97e4c99e265c62643	application of simulation approaches to creation of decision support system for it service management	decision support;slo;stochastic process;simulation;support system;sla;stochastic analysis;it service management	The paper presents a simulation-based approach to creation of decision support system for IT Service Management. The presented approach includes monitoring of stochastic data IT Services and calculation of measure of alignment to business goals with its characteristic basing on SLA/SLO. The approach combines the benefits of two kinds of models: analytical and simulation ones. The key idea of the paper is to demonstrate how modern methods of stochastic process analysis may enhance trustworthiness and quality of decision making along business goals within IT Services.	decision support system;simulation	Yuri Karpov;Rostislav I. Ivanovsky;Kirill A. Sotnikov	2007		10.1007/978-3-540-73940-1_55	stochastic process;decision support system;second-language acquisition;computer science;knowledge management;management science	EDA	-32.61425699533235	-8.461549545852941	95144
40474de65eb75d22f32b9b7ced9472e68b59181c	three main components of experience base in linguistic description of data	ontologies pragmatics maintenance engineering computational modeling temperature measurement vectors context;ontologies artificial intelligence;ontologies artificial intelligence computational linguistics natural language processing;data linguistic description ontology computational representation textual meaning interpersonal meaning ideational meaning natural language computational systems experience base component experience representation;granular linguistic model of phenomena ontology computing with words and perceptions;computational linguistics;natural language processing	In this paper, we present a contribution to solve the problem of organizing the representation of experience in computational systems which are able to generate relevant linguistic descriptions of data for specific users and contexts. We claim, that, typically, the expert knowledge modeled in these systems is limited to one of the dimensions of the meaning of natural language. Here, we model the experience base distinguishing among three types of meaning, namely, Ideational meaning concerning with the technical, impersonal description of the specific phenomenon; Interpersonal meaning concerning with the role of the partners involved in the communication process and Textual meaning concerning with the contribution to the meaning of the specific realization with natural language of both previous types of meaning. In order to organize these types of meaning (also called components of experience base) in a practical computational representation, we have built an ontology that will help designers to model their experience in the application domain. Using this ontology, the computational system is able of identifying the most suitable linguistic descriptions for describing the input data. Our approach is presented with the support of a practical example in the domain of the maintenance of comfort in a room.	application domain;natural language;organizing (structure)	Carmen Martínez-Cruz;Daniel Sánchez;Gracián Triviño	2013	2013 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)	10.1109/FUZZ-IEEE.2013.6622413	natural language processing;computer science;artificial intelligence;computational linguistics;computational semantics	NLP	-30.361121142493836	-6.596056999487406	95487
57e12a774ec35c3dc0ff82f5e99b745241313fa9	information retrieval for evidence-based decision making	aplicacion;dominio investigacion;decision aid;information retrieval system;research field;information retrieval;evidence based practice;evidence based medicine;medicina;ayuda decision;satisfiability;medical products;medecine;recherche information;aide decision;transfer of training;medicine;domaine recherche;ejemplo;recuperacion informacion;information system;application;information seeking;example;medecine factuelle;systeme information;problem solving;sistema informacion;exemple	The emergence of evidence‐based medicine has implications for the use and development of information retrieval systems which are not restricted to the area of medicine. ‘Evidence‐based’ practice emphasises the retrieval and application of high quality knowledge in order to solve real‐world problems. However, information seeking to support such evidence‐based approaches to decision making and problem solving makes demands on retrieval systems which they are not well suited at present to satisfy. A number of approaches have been developed in the field of medicine that seek to address these limitations. The extent to which such approaches may be applied to other areas is discussed, as are their limitations.	information retrieval	Nigel Ford;Dave Miller;Alan J. O'Rourke;Jane Ralph;Edward Turnock;Andrew Booth	1999	Journal of Documentation	10.1108/EUM0000000007152	evidence-based medicine;cognitive models of information retrieval;computer science;artificial intelligence;operations research;information retrieval;information system;satisfiability;evidence-based practice	Vision	-25.282260355629898	-2.952385141198038	95710
f6042ca1a891919c67d97c3bb331593dae555e22	attribute pattern-matching equations: a diagnostic technique for intelligent tutoring systems	systeme intelligent;intelligent tutoring system;sistema inteligente;raisonnement;pattern matching;intelligent system;razonamiento;concordance forme;reasoning	The diagnostic process of an intelligent tutoring system is the procedure by which the system determines a student’s level of performance based upon his or her problem solving behavior. In order for a system to insure the satisfactory performance of this process, three key issues must be addressed: (1) how to assign credit or blame to individual skills, (2) how to handle the problem of noisy data, and (3) how to handle the problem of combinatorial explosion inherent in many AI applications. In this paper we describe an informal reasoning technique, known as attribute pattern matching equations (APMEs), which is used in the diagnostic process of the TAPS tutoring system. This technique addresses all three of the foregoing issues in assessing the performance levels of individual skills in the TAPS system. In addition, it will be demonstrated that APMEs are an effective method for reasoning with uncertainty on three different levels.	algorithm;artificial neural network;effective method;expert system;importance sampling;information;knowledge acquisition;knowledge engineering;pattern matching;portable database image;problem domain;problem solving;production (computer science);repertory grid;signal-to-noise ratio	Dawn J. Holmes;Lois Wright Hawkes	1994	Inf. Sci.	10.1016/0020-0255(94)90053-1	simulation;computer science;artificial intelligence;machine learning;pattern matching;programming language;reason;algorithm	AI	-21.64217037926166	-3.854167674673596	96117
76179d9cbd935e96253f74e7958d1f48bc012ed9	a soft computing approach for privacy requirements engineering: the pris framework	system modeling;pris method;soft computing;conceptual model;fuzzy logic;security and privacy;security requirements;number of factors;decision making process;requirement engineering;system development;privacy requirements engineering	Soft computing continuously gains interest in many fields of academic and industrial domain; among the most notable characteristics for using soft computing methodological tools is the ability to handle with vague and imprecise data in decision making processes. Similar conditions are often encountered in requirements engineering. In this paper, we introduce the PriS approach, a security and privacy requirements engineering framework which aims at incorporating privacy requirements early in the system development process. Specifically, PriS provides a set of concepts for modelling privacy requirements in the organisation domain and a systematic way-of-working for translating these requirements into system models. The conceptual model of PriS uses a goal hierarchy structure. Every privacy requirement is either applied or not on every goal. To this end every privacy requirement is a variable that can take two values [0,1] on every goal meaning that the requirements constraints the goal (value 1) or not (value 0). Following this way of working PriS ends up suggesting a number of implementation techniques based on the privacy requirements constraining the respective goals. Taking into account that the mapping of privacy variables to a crisp set consisting of two values [0,1] is constraining, we extend also the PriS framework so as to be able to address the degree of participation of every privacy requirement towards achieving the generic goal of privacy. Therefore, we propose a fuzzification of privacy variables that maps the expression of the degree of participation of each privacy variable to the [0,1] interval. We also present a mathematical framework that allows the concurrent management of combined independent preferences towards the necessity of a privacy measure; among the advantages of the presented extended framework is the scalability of the approach in such a way that the results are not limited by the number of independent opinions or by the number of factors considered while reasoning for a specific selection of privacy measures.		Christos Kalloniatis;Petros Belsis;Stefanos Gritzalis	2011	Appl. Soft Comput.	10.1016/j.asoc.2010.10.012	fuzzy logic;decision-making;privacy software;systems modeling;privacy by design;computer science;knowledge management;artificial intelligence;conceptual model;machine learning;data mining;soft computing	SE	-31.388993409289803	-4.963556280566857	96368
b8d3a4e610dde6c5984a74f54959fa63aefda61c	possibilistic medical knowledge representation model		Physician is the direct responsible for health and life of his patients. Therefore, diagnosis delivering is an extremely critical, although difficult task. Furthermore, diagnosis delivering is an error-prone task [3]. Medical Decision Support Systems such as Knowledge Based Systems, Case Based Reasoning Systems, Machine Learning Systems and Medical Data Mining Systems, have been constructed in order to reduce diagnosis error risks, as well as to help physicians making high quality and reliable medical decisions [4]. These systems involve two main issues: the medical knowledge representation and adapted reasoning mechanisms. The medical knowledge, in general terms, has to be considered from two points of view: Expert Knowledge related to the physician’s description of different relationships between symptoms and diagnoses, symptoms and symptoms, and diagnoses and diagnoses. Patient Information is collected from each patient (patient data collecting and structuring). The first is crucial in order to establish Medical Knowledge Base, while the second leads to establish the patient database (i.e., Medical Case Base). Experts use their own experience of the medical cases as well as references knowledge sources to define the structure of the medical knowledge base. Medical knowledge often suffers from different forms of information imperfections (i.e., uncertainty, imprecision, ambiguity, etc.). In addition to the different types of information imperfections, the information can be quantitative (numerical or binary) or qualitative (nominal and ordinal) [17, 29]. Thus, the heterogeneity and imperfection of medical knowledge must be taken into consideration while the construction of a Medical Decision Support System. In other words, Medical Decision Support System has to be able to deal with heterogeneous and imperfect knowledge. In [27] R. Seising et al. defined the Medical knowledge as follows:	blocking (computing);case-based reasoning;clinical decision support system;cognitive dimensions of notations;data mining;display resolution;image analysis;knowledge base;knowledge representation and reasoning;knowledge-based systems;machine learning;numerical analysis;ordinal data;possibility theory;relevance;usability	Mohammad Homam Alsun;Laurent Lecornu;Bassel Solaiman	2012	J. Adv. Inf. Fusion		knowledge representation and reasoning;knowledge management;artificial intelligence;model-based reasoning;machine learning;data mining;mathematics	ML	-29.438573325635947	-8.140925914294865	96410
767ffcfb5e7c8dd251d20408c6bde555a6d8ff70	recadvisor: criteria-based ph.d. supervisor recommendation		This demo presents RecAdvisor, a prototype recommender system for finding and recommending potential Ph.D. supervisors for students by identifying different criteria to consider when selecting a supervisor.	common criteria;prototype;recommender system	Mir Anamul Hasan;Daniel G. Schwartz	2018		10.1145/3209978.3210178	multiple-criteria decision analysis;recommender system;data mining;information retrieval;computer science;supervisor	AI	-31.789163614439904	-9.69793833167486	96585
6e0f855d0fe8f91b54dcd628de14f529f5173873	context-based prefetch for implementing objects on relations	performance measure;complex objects;relational database	When implementing persistent objects on a relational database, a major performance issue is prefetching data to minimize the number of roundtrips to the database. This is especially hard with navigational applications, since future accesses are unpredictable. We propose using the context in which an object is loaded as a predictor of future accesses, where context can be a stored collection of relationships, a query result, or a complex object. When an object O’s state is loaded, similar state for other objects in O’s context is prefetched. We present a design for maintaining context and using it to guide prefetch. We give performance measurements of its implementation in Microsoft Repository, showing up to a 70% reduction in running time. We describe variations that selectively apply the technique, exploit asynchronous access, and use application-supplied performance hints.	benchmark (computing);cpu cache;ct scan;depth-first search;graphical user interface;kerrison predictor;link prefetching;mathematical optimization;object-relational database;paging;prefetch input queue;relational database;relational database management system;traverse;time complexity;tree traversal	Philip A. Bernstein;Shankar Pal;David Shutt	1999			object-based spatial database;relational database;computer science;knowledge management;database;distributed computing;object-relational impedance mismatch	DB	-30.184474340791677	4.054461409790622	96650
146ab253a1a1f38b98278de231375fe3848a36bf	data fusion to improve trajectory tracking in a cooperative surveillance multi-agent architecture	video surveillance;multi agent system;software agent;data fusion;interface agent;sensor network;distributed vision;software agents;coalition formation;autonomous agent;fusion center;surveillance application;multi agent architecture;trajectory tracking;sensor network architecture;coordination	1566-2535/$ see front matter 2009 Elsevier B.V. A doi:10.1016/j.inffus.2009.09.002 * Corresponding author. E-mail addresses: fcastane@inf.uc3m.es (F. Castane García), mpatrici@inf.uc3m.es (M.A. Patricio), molina@ URL: http://www.giaa.inf.uc3m.es (M.A. Patricio). In this paper we present a Cooperative Surveillance Multi-Agent System (CS-MAS) architecture extended to incorporate dynamic coalition formation. We illustrate specific coalition formation using fusion skills. In this case, the fusion process is divided into two layers: (i) a global layer in the fusion center, which initializes the coalitions and (ii) a local layer within coalitions, where a local fusion agent is dynamically instantiated. There are several types of autonomous agent: surveillance–sensor agents, a fusion center agent, a local fusion agent, interface agents, record agents, planning agents, etc. Autonomous agents differ in their ability to carry out a specific surveillance task. A surveillance–sensor agent controls and manages individual sensors (usually video cameras). It has different capabilities depending on its functional complexity and limitations related to sensor-specific aspects. In the work presented here we add a new autonomous agent, called the local fusion agent, to the CS-MAS architecture, addressing specific problems of on-line sensor alignment, registration, bias removal and data fusion. The local fusion agent is dynamically created by the fusion center agent and involves several surveillance–sensor agents working in a coalition. We show how the inclusion of this new dynamic local fusion agent guarantees that, in a video-surveillance system, objects of interest are successfully tracked across the whole area, assuring continuity and seamless transitions. 2009 Elsevier B.V. All rights reserved.	agent architecture;algorithm;autonomous agent;autonomous robot;cs-cipher;experiment;interdependence;multi-agent system;online and offline;scott continuity;seamless3d;sensor;visual sensor network	Federico Castanedo;Jesús Caja García;Miguel A. Patricio;José M. Molina López	2010	Information Fusion	10.1016/j.inffus.2009.09.002	computer vision;simulation;computer science;artificial intelligence;software agent;multi-agent system;computer security	AI	-22.015076730813302	-7.008217473849377	96721
8df07437d32cc3c1c4a38546eac3c7419a5d93d3	cost modeling and estimation for olap-xml federations	protocolo acceso;business to business;base donnee;systeme aide decision;on line;en linea;xml language;database;xml database;base dato;query optimization;sistema ayuda decision;access protocol;decision support system;efficient implementation;en ligne;protocole acces;langage xml;lenguaje xml;cost model;on line analytical processing	The ever-changing data requirements of today’s dynamic businesses are not handled well by current OnLine Analytical Processing (OLAP) systems. Physical integration of unexpected data into OLAP systems is a long and time-consuming process, making logical integration, or federation, the better choice in many cases. The increasing use of XML, e.g. in business-to-business (B2B) applications, suggests that the required data will often be available in XML format. Thus, federations of OLAP and XML databases will be very attractive in many situations. However, a naive implementation of OLAP-XML federations will not perform well enough to be useful. In an efficient implementation, cost-based optimization is a must, creating a need for an effective cost model for OLAP-XML federations. However, existing cost models do not support such systems. In this paper we present a cost model for OLAP-XML federations, along with techniques for estimating the cost model parameters in a federated OLAP-XML environment. The paper also present the cost models for the OLAP and XML components in the federation on which the federation cost model is built. The cost model has been used as the basis for effective cost-based query optimization in OLAP-XML federations. Experiments show that the cost model is precise enough to make a substantial difference in the query optimization process.	analysis of algorithms;mathematical optimization;online analytical processing;query optimization;requirement;xml database;xml namespace	Dennis Pedersen;Karsten Riis;Torben Bach Pedersen	2002		10.1007/3-540-46145-0_24	relevant cost;query optimization;xml;decision support system;computer science;operating system;data mining;xml database;database;world wide web;computer security	DB	-26.144944809174003	4.082463496173775	96967
1f5ac4bbaa43bf53399355fae3d9d8dfac3e583c	a content-addressable network for similarity search in metric spaces	metric space;a content addressable;content addressable network;computer networks;similarity join;index structures;similarity search scalable distributed data structure;metric spaces;similarity search;network	In this paper we present a scalable and distributed access structure for similarity search in metric spaces. The approach is based on the Content– addressable Network (CAN) paradigm, which provides a Distributed Hash Table (DHT) abstraction over a Cartesian space. We have extended the CAN structure to support storage and retrieval of more generic metric space objects. We use pivots for projecting objects of the metric space in an N -dimensional vector space, and exploit the CAN organization for distributing the objects among computer nodes of the structure. We obtain a Peer–to–Peer network, called the MCAN, which is able to search metric space objects by means of the similarity range queries. Experiments conducted on our prototype system confirm full scalability of the approach.	access structure;cartesian closed category;content addressable network;distributed hash table;peer-to-peer;programming paradigm;prototype;range query (data structures);scalability;similarity search;spaces	Fabrizio Falchi;Claudio Gennaro;Pavel Zezula	2005		10.1007/978-3-540-71661-7_9	metric space;computer science;theoretical computer science;data mining;database;computer network	DB	-29.58843406470575	0.1803692212966951	97037
9902350093b873c0404a412a352a8421ba8c53a8	integrating behavioral, perceptual, and world knowledge in reactive navigation	commande reactive;navegacion;robot movil;comportement;autonomous system;schemas;espacio 3 dimensiones;simulation;base connaissance;simulacion;mobile robots;intelligence artificielle;robotics;percepcion;sistema autonomo;reactive control;computer architecture;navigation;architecture ordinateur;robot mobile;conducta;espace 3 dimensions;systeme autonome;three dimensional space;robotica;artificial intelligence;base conocimiento;arquitectura ordenador;robotique;inteligencia artificial;perception;behavior;experimentation;knowledge based systems;moving robot;experimentacion;knowledge base	Reactive navigation based on task decomposition is an effective means for producing robust navigation in complex domains. By incorporating various forms of knowledge, this technique can be made considerably more flexible. Behavioral and perceptual strategies which are represented in a modular form and configured to meet the robot's mission and environment add considerable versatility. A priori world knowledge, when available, can be used to configure these strategies in an efficient form. Dynamically acquired world models can be used to circumvent certain pitfalls that representationless methods are subject to. The Autonomous Robot Architecture (AuRA) is the framework within which experiments in the application of knowledge to reactive control are conducted. Actual robot experiments and simulation studies demonstrate the flexibility and feasibility of this approach over a wide range of navigational domains.	commonsense knowledge (artificial intelligence)	Ronald C. Arkin	1990	Robotics and Autonomous Systems	10.1016/S0921-8890(05)80031-4	computer vision;knowledge base;navigation;simulation;computer science;autonomous system;artificial intelligence;robotics;perception;mobile robot navigation;behavior	Robotics	-22.857884434872762	-7.774433606921821	97443
3e241294397256e1ee74ee0f108de42052547e6b	mathematical techniques for efficient record segmentation in large shared databases	assignment problem;records;information retrieval;data management;data processing;time sharing;data bases;mathematical models;mathematical programming;average cost;network flows;fortran;transport costs;network flow;branch and bound;analytical model	It is possible to significantly reduce the average cost of information retrieval from a large shared database by partitioning data items stored within each record into a primary and a secondary record segment. An analytic model, based upon knowledge of data item lengths, transportation costs, and retrieval patterns, is developed to assist an analyst with this assignment problem. The model is generally applicable to environments in which a database resides in secondary storage, and is useful for both uniprogramming and multiprogramming systems. A computationally tractable record design algorithm has been implemented as a Fortran program and applied to numerous problems. Realistic examples are presented which demonstrate a potential for reducing total system cost by more than 65 percent.	algorithm;assignment problem;auxiliary memory;cobham's thesis;computer data storage;computer multitasking;data item;fortran;glossary of computer graphics;information retrieval	Mark J. Eisner;Dennis G. Severance	1976	J. ACM	10.1145/321978.321982	mathematical optimization;combinatorics;flow network;system of record;data processing;data management;computer science;theoretical computer science;data mining;database;programming language;algorithm;statistics	DB	-26.856306094062685	3.8583735905411833	97496
5c1b685db6dd4f6be2a3be8e13f22a0f8371781f	an intelligent agent to support city policies decisions	intelligent agent	In last year there has been a growing interest in computational intelligence techniques applied to economics, providing support for financial decisions. In this paper we propose an intelligent decision support system, aimed at suggesting the best managing strategies for a game-based model of a virtual city. Two knowledge representation areas characterize the intelligent agent. The first one is a “deterministic” area, which deals with descriptions and deterministic events. The second one is a “decisional” area, which deals with decisions taken under conditions of uncertainty. The agent is capable of reasoning in order to prospect the future evolutions of particular choices taken by the user. The interaction is conducted through a natural language interface built as an Alice-based conversational agent.	intelligent agent	Agnese Augello;Giovanni Pilato;Salvatore Gaglio	2009		10.3233/978-1-60750-072-8-163	simulation;intelligent decision support system;computer science;knowledge management;artificial intelligence;management science;intelligent agent	HCI	-30.7300755058236	-9.682260103263886	97497
372ca4714230db2e895ca9e920238cd2b408ae91	optimal clustering of relations to improve sorting and partitioning for joins	base relacional dato;sorting;relational database;systeme base donnee;algorithme;database systems;triage;base donnee relationnelle;algorithms;partitionnement	"""The sorting or partitioning of relations is very common in relational database systems. Implementations of the join operation include the sortmerge join algorithm, which sorts both relations, and the hash join algorithm, which usually partitions both relations. We describe how clustering records using an optimal multi-attribute hash (MAH) """"le, taking the query pattern and distribution into account, reduces the average cost of sorting or partitioning. We demonstrate that maintaining multiple copies of a data """"le, each with a different clustering organization, further reduces the average cost of sorting or partitioning. We describe an inexpensive method for determining a good partitioning index (MAH """"le organization). Our analysis and experiments show that the partitioning indexes we """"nd are usually optimal and can often partition a relation more than ten times faster than by not using any clustering. We also show that a signi""""cant change in the query pattern or distribution is required before a reorganization of a data """"le is necessary, and that such a reorganization is, in general, an inexpensive operation."""	algorithm;cluster analysis;computer cluster;experiment;hash join;join (sql);relational database;sorting	Evan P. Harris;Kotagiri Ramamohanarao	1997	Comput. J.	10.1093/comjnl/40.7.416	relational database;computer science;sorting;theoretical computer science;database;distributed computing;algorithm	DB	-27.271992331452275	4.107272023065748	97533
4f75b756d296c29c588982d6d284483a83c8ea46	how connectionism can change ai and the way we think about ourselves	modelizacion;parallelisme;connectionist models;representacion conocimientos;linguistique;connectionism;conexionismo;logic;intelligence artificielle;artificial intelligent;modelisation;connexionnisme;parallelism;linguistica;paralelismo;metafora;cognition;cognicion;artificial intelligence;inteligencia artificial;reseau neuronal;knowledge representation;metaphor;representation connaissances;modeling;red neuronal;logique;logica;neural network;metaphore;linguistics	Abstract In this paper we discuss some fundamentals of “conventional” or “symbolic ” artificial intelligence (AI) and how connectionism, if done properly, can provide some genuinely novel aspects. We show some evidence against the traditional approach when viewed as a plausible model of animate intelligence. We also show that connectionist models appear to be much closer to an adequate model of “natural” behavior. Furthermore, we discuss possible impacts of connectionism on AI as afield of research and, beyond that, on how we think about our own mind. Many questions, however, remain open to permit a clear forecast of where the research will lead us.	connectionism	Georg Dorffner	1993	Applied Artificial Intelligence	10.1080/08839519308949976	natural language processing;connectionism;cognition;computer science;artificial intelligence;machine learning;logic;cognitive science	AI	-25.09280357454788	-9.317238069966683	97600
aba9186156bc716a29adbcf74982dc27193dc475	maximizing future options: an on-line real-time planning method	simulation ordinateur;competition;methode planification temps reel;soccer;real time;juego cooperativo;cooperative game;robocup;deporte equipo;sport equipe;dynamic environment;jeu cooperatif;football;simulacion computadora;computer simulation;team sport;competencia;futbol	In highly dynamic environments with uncertainty the elaboration of long or rigid plans is useless because the constructed plans are frequently dismissed by the arrival or new unexpected situations; in these cases, a “second-best” plan could rescue the situation. We present a new real-time planning method where we take into consideration the number and quality of future options of the next action to choose, in contrast to most planning methods that just take into account the intrinsic value of the chosen plan or the maximum valued future option. We apply our method to the Robocup simulated soccer competition, which is indeed highly dynamic and involves uncertainty. We propose a specific architecture for implementing this method in the context of a player agent in the Robocup competition, and we present experimental evidence showing the potential of our method.	aerial photography;real-time clock;real-time transcription;search tree	Ramón F. Brena;Emmanuel Martinez	2005		10.1007/11579427_99	computer simulation;simulation;competition;computer science;artificial intelligence	AI	-22.098520637371333	-7.461779430795818	97719
2653057556365fbaf8bd81d5430d5e0c1b2beaf1	comparing two hybrid expert system shells	expert systems;expert system shells;knowledge based systems;expert system	This paper describes in full detail an analysis of two expert system shells: Level 5 Object and Kappa PC. The major components of these tools (knowledge representation, inference and control, developer interface, user interface and explanation facility, interface to external data sources, support and documentation) were studied and tested by means of small prototypes. Results and experiences of this work are given together with some software engineering remarks.	expert system;two-hybrid screening	Koen Vanhoof;Jerzy Surma	1994	International Journal of Software Engineering and Knowledge Engineering	10.1142/S0218194094000088	legal expert system;computer science;systems engineering;engineering;knowledge management;artificial intelligence;data mining;subject-matter expert;expert system	SE	-33.6390463275292	-8.754637244322847	97749
19c203ef3090e64d21bf4689c0a0580c966e0d24	towards next generation business process model repositories - a technical perspective on loading and processing of process models		Business process management repositories manage large collections of process models ranging in the thousands. Additionally, they provide management functions like e.g. mining, querying, merging and variants management for process models. However, most current business process management repositories are built on top of relation database management systems (RDBMS) although this leads to performance issues. These issues result from the relational algebra, the mismatch between relational tables and object oriented programming (impedance mismatch) as well as new technological developments in the last 30 years as e.g. more and cheap disk and memory space, clusters and clouds. The goal of this paper is to present current paradigms to overcome the performance problems inherent in RDBMS. Therefore, we have to fuse research about data modeling along database technologies as well as algorithm design and parallelization for the technology paradigms occurring nowadays. Based on these research streams we have shown how the performance of business process management repositories could be improved in terms of loading performance of processes (from e.g. a disk) and the computation of management techniques resulting in even faster application of such a technique. Exemplarily, applications of the compiled paradigms are presented to show their applicability.	algorithm design;business process;characteristic impedance;compiler;computation;dspace;data mining;data modeling;parallel computing;performance tuning;relational algebra;relational database management system	Hanns-Alexander Dietrich	2013			artifact-centric business process model;management science;business process model and notation;process management;business process discovery;business process modeling	DB	-33.06365180288725	0.8120653257174751	97805
7ddd98cdee21595c2cdfecef0ac56839f4e68e43	accelerating big data analytics with collaborative planning in teradata aster 6	planning contracts collaboration context optimization runtime big data;collaboration;contracts;runtime;sql big data groupware query processing;big data;teradata aster 6 system collaborative planning innovative data processing software large scale big data analysis user defined table operators sql query optimizer query plan operators collaborative exchange;planning;optimization;context	The volume, velocity, and variety of Big Data necessitate the development of new and innovative data processing software. A multitude of SQL implementations on distributed systems have emerged in recent years to enable large-scale data analysis. User-Defined Table operators (written in procedural languages) embedded in these SQL implementations are a powerful mechanism to succinctly express and perform analytic operations typical in Big Data discovery workloads. Table operators can be easily customized to implement different processing models such as map, reduce and graph execution. Despite an inherently parallel execution model, the performance and scalability of these table operators is greatly restricted as they appear as a black box to a typical SQL query optimizer. The optimizer is not able to infer even the basic properties of table operators, prohibiting the application of optimization rules and strategies. In this paper, we introduce an innovative concept of “Collaborative Planning”, which results in the removal of redundant operations and a more optimal rearrangement of query plan operators. The optimization of the query proceeds through a collaborative exchange between the planner and the table operator. Plan properties and context information of surrounding query plan operations are exchanged between the optimizer and the table operator. Knowing these properties also allows the author of the table operator to optimize its embedded logic. Our main contribution in this paper is the design and implementation of Collaborative Planning in the Teradata Aster 6 system. Using real-world workloads, we show that Collaborative Planning reduces query execution times as much as 90.0% in common use cases, resulting in a 24x speedup.	advanced spaceborne thermal emission and reflection radiometer;application programming interface;benchmark (computing);big data;black box;database;distributed computing;embedded system;karl steinbuch;map;mapreduce;mathematical optimization;query optimization;query plan;sql;scalability;select (sql);speedup;velocity (software development)	Aditi Pandit;Derrick Kondo;David E. Simmen;Anjali Norwood;Tongxin Bai	2015	2015 IEEE 31st International Conference on Data Engineering	10.1109/ICDE.2015.7113378	planning;sargable;query optimization;big data;computer science;query by example;theoretical computer science;data mining;database;programming language;collaboration	DB	-32.558265613046075	0.8143159690467552	97818
93718a8c2f391bab147ebbcb4f003e76d7ec5452	an uncertainty management calculus for ordering searches in distributed dynamic databases		MINDS is a distributed system of cooperating query engines that customize, document retrieval for each user in a dynamic environment. It improves its performance and adapts to changing patterns of document distribution by observing system-user interactions and modifying the appropriate certainty factors, which act as search control parameters. It argued here that the uncertainty management calculus must account for temporal precedence, reliability of evidence, degree of support for a proposition, and saturation effects. The calculus presented here possesses these features. Some results obtained with this scheme are discussed.	distributed computing;document retrieval;expert system;interaction;uncertainty quantification	Uttam Mukhopadhyay	2013	CoRR		computer science;artificial intelligence;theoretical computer science;machine learning;data mining;mathematics	DB	-21.386232007959237	-6.493141468467909	97839
f56260e4a7c0dc29374a41a137945e486d5249e2	an efficient stream-based join to process end user transactions in real-time data warehousing		In the field of real-time data warehousing semistream processing has become a potential area of research since last one decade. One important operation in semi-stream processing is to join stream data with a slowly changing diskbased master data. A join operator is usually required to implement this operation. This join operator typically works under limited main memory and this memory is generally not large enough to hold the whole disk-based master data. Recently, a seminal join algorithm called MESHJOIN (Mesh Join) has been proposed in the literature to process semistream data. MESHJOIN is a candidate for a resource-aware system setup. However, MESHJOIN is not very selective. In particular, MESHJOIN does not consider the characteristics of stream data and its performance is suboptimal for skewed stream data. In this paper we propose a novel Semi-Stream Join (SSJ) using a new cache module. The algorithm is more appropriate for skewed distributions, and we present results for Zipfian distributions of the type that appears in many applications. We present the cost model for our SSJ and validate it with experiments. Based on the cost model we also tune the algorithm up to a maximum performance. We conduct a rigorous experimental study to test our algorithm. Our experiments show that SSJ outperforms MESHJOIN significantly. Subject Categories and Descriptors: H.2.7 [Database Administration]; Data Warehouse and Repository: H.2.4 [Systems]; Transaction Processing I.2H.2 General Terms: Data Warehousing, Data Processing	algorithm;analysis of algorithms;computer data storage;experiment;foreign key;input/output;join (sql);real-time data;semiconductor industry;stream processing;table (information);transaction processing;zipf's law	M. Asif Naeem;Noreen Jamil	2014	JDIM			DB	-29.98646777222743	3.126683779702868	98120
cbea01e4f7ce31a8b8c50b2003404cd22e28958b	a framework for real-time fault detection and diagnosis using temporal data	propulsion navale;tratamiento datos;detection erreur;deteccion error;sistema experto;maintenance;surveillance;turbina gas;real time;temporal data;data processing;traitement donnee;intelligence artificielle;lubricacion;captador medida;gas turbine;measurement sensor;vigilancia;capteur mesure;diagnostic panne;fault diagnostic;diagnostico pana;lubrication;mantenimiento;artificial intelligence;machine damage;avarie;donnee temporelle;inteligencia artificial;systeme expert;error detection;propulsion naval;turbine gaz;supervision;marine propulsion;fault detection and diagnosis;diagnostico error;lubrification;diagnostic erreur;error diagnostic;expert system	Abstract Successful real-time sensor-based fault detection and diagnosis in large and complex systems is seldom achieved by operators. The lack of an effective method for handling temporal data is one of several key problems in this area. A methodology is introduced which advantageously uses temporal data in performing fault diagnosis in a subsystem of a Navy ship propulsion system. The methodology is embedded in a computer program designed to be used as a decision aid to assist the operator. It utilizes machine learning, is able to cope with uncertainty at several levels, and works in real-time. Program performance data is presented and analysed. The approach illustrates how relatively simple existing techniques can be assembled into more powerful real-time diagnostic tools.	fault detection and isolation;real-time clock	Donald B. Malkoff	1987	AI in Engineering	10.1016/0954-1810(87)90144-0	lubrication;marine propulsion;simulation;error detection and correction;data processing;computer science;engineering;artificial intelligence;temporal database;forensic engineering;expert system	AI	-22.922353046942217	-4.454565927334647	98164
148d1b86258f91801f1940f22e77c4226679d382	cloudvista: interactive and economical visual cluster analysis for big data in the cloud	selected works;bepress	Analysis of big data has become an important problem for many business and scientific applications, among which clustering and visualizing clusters in big data raise some unique challenges. This demonstration presents the CloudVista prototype system to address the problems with big data caused by using existing data reduction approaches. It promotes a whole-big-data visualization approach that preserves the details of clustering structure. The prototype system has several merits. (1) Its visualization model is naturally parallel, which guarantees the scalability. (2) The visual frame structure minimizes the data transferred between the cloud and the client. (3) The RandGen algorithm is used to achieve a good balance between interactivity and batch processing. (4) This approach is also designed to minimize the financial cost of interactive exploration in the cloud. The demonstration will highlight the problems with existing approaches and show the advantages of the CloudVista approach. The viewers will have the chance to play with the CloudVista prototype system and compare the visualization results generated with different approaches.	algorithm;apache hadoop;batch processing;big data;cloud computing;cluster analysis;data visualization;interactivity;mapreduce;parallel computing;prototype;provisioning;scalability	Huiqi Xu;Zhen Li;Shumin Guo;Keke Chen	2012	PVLDB	10.14778/2367502.2367529	simulation;computer science;data mining;database;world wide web	DB	-32.50150287440365	-0.9249300003914098	98208
77d4277e53bfa78b42e2cf26e834d75a4ff91c88	a method for product line scoping based on a decision-making framework	concepcion modular;systeme intelligent;intelligent transport system;systeme aide decision;sistema inteligente;computer software reusability;developpement produit;product line;sistema ayuda decision;prise decision;decision support system;product line scoping;product line architecture;intelligent system;reutilisation logiciel;modular design;point of view;systeme transport intelligent;toma decision;decision making framework;desarrollo producto;conception modulaire;product development	It is indispensable for strategic product line development to define the proper scope of the product line. Once the scope has been defined, we examine the corresponding product line architecture to realize systematic reuse for the product line. Therefore, in defining the scope, we have to decide whether or not it is appropriate to share the same architecture for the products in the product line. The appropriateness of sharing the same architecture among multiple products has to be examined from two points of view. One is from the point of view of the individual optimality (i.e., whether it is good for each product to use the shared architecture), and the other is from the point of view of the whole optimality (i.e., whether it is good for the product line as a whole to share the architecture). In this paper, we propose a method for product line scoping. We consider scoping as a decision-making activity in which we evaluate multiple candidates for the scope and then select the proper one after examining the appropriateness from the two points of view. In order to demonstrate its applicability, we applied the method to the actual problem picked up from Japanese ITS (Intelligent Transport Systems) projects.	scope (computer science)	Tomoji Kishi;Natsuko Noda;Takuya Katayama	2002		10.1007/3-540-45652-X_22	simulation;decision support system;computer science;operating system;modular design;new product development	Theory	-22.76957649916622	-7.164478134871817	98466
437e71ab1f9d9837b9efaf5db9c7cbf4c27e8256	a knowledge based decision support system for bioinformatics and system biology	meta reasoning;workflow management;decision support system;knowledge base	In this paper, we present a new Decision Support System for Bioinformatics and System Biology issues. Our system is based on a Knowledge base, representing the expertise about the application domain, and a Reasoner. The Reasoner, consulting the Knowledge base and according to the user's request, is able to suggest one or more strategies in order to resolve the selected problem. Moreover, the system can build, at different abstraction layers, a workflow for the current problem on the basis of the user's choices, freeing the user from implementation details and assisting him in the correct configuration of the algorithms. Two possible application scenarios will be introduced: the analysis of protein-protein interaction networks and the inference of gene regulatory networks.		Antonino Fiannaca;Salvatore Gaglio;Massimo La Rosa;Daniele Peri;Riccardo Rizzo;Alfonso Urso	2010		10.1007/978-3-642-21946-7_17	workflow;knowledge base;decision support system;computer science;knowledge management;artificial intelligence;data mining	Comp.	-31.60457673480774	-5.201771593621448	98678
63cb90cc758ff9d3e96d4128854ffee8b26b1cc8	the transbase hypercube rdbms: multidimensional indexing of relational tables	query optimization;indexation	Only few multidimensional access methods have made their way into commercial relational DBMS. Even if a RDBMS ships with a multidimensional index, the multidimensional index usually is an add-on like Oracle SDO, which is not integrated into the SQL interpreter, query processor and query optimizer of the DBMS kernel. Our demonstration shows TransBase HyperCube, a commercial RDBMS, whose kernel fully integrates the UB-Tree, a multidimensional extension of the B-Tree. This integration was performed in an ESPRIT project funded by the European Commission. We put the main emphasis of our demonstration on the application of UB-Tree indexes in realworld databases for OLAP. However, we also address general issues of UB-Trees like creation, spacerequirements, or comparison to other indexing methods.	add-ons for firefox;b-tree;kernel (operating system);mathematical optimization;online analytical processing;oracle nosql db;query optimization;relational database management system;sql;transbase;ub-tree;undefined behavior	Volker Markl;Frank Ramsak;Roland Pieringer;Robert Fenk;Klaus Elhardt;Rudolf Bayer	2001			data mining;database;relational database management system;search engine indexing;information retrieval;hypercube;query optimization;computer science;indexation	DB	-31.874503098218547	1.906309639469993	98863
940a8135f44598288b904da09076557cd3393641	performance bottleneck in time-series subsequence matching	time series;subsequence matching;indexation;optimization;time series databases;performance bottleneck;experience base	This paper addresses a performance bottleneck in time-series subsequence matching. First, we analyze the disk access and CPU processing times required during the index searching and post-processing steps of subsequence matching through preliminary experiments. Based on their results, we show that the post-processing step is a main performance bottleneck in subsequence matching. In order to resolve the performance bottleneck, we propose a simple yet quite effective method that processes the post-processing step. By rearranging the order of candidate subsequences to be compared with a query sequence, our method completely eliminates the redundancies of disk accesses and CPU processing occurring in the post-processing step. We show that our method is optimal and also does not incur any false dismissal. Also, we justify the effectiveness of our method by extensive experiments.	central processing unit;effective method;experiment;time series;video post-processing	Sang Wook Kim;Byeong-Soo Jeong	2005		10.1145/1066677.1066786	theoretical computer science;time series;longest common subsequence problem;distributed computing;algorithm;statistics	DB	-30.44090109505778	-0.7321791972313735	98900
c5c3f32855058fa70a31bfba8506dc3c4cb15afd	spate: compacting and exploring telco big data	data engineering;visualization;time factors;big data;indexing;real time systems	In this demonstration paper, we present SPATE, an innovative telco big data exploration framework whose objectives are two-fold: (i) minimizing the storage space needed to incrementally retain data over time, and (ii) minimizing the response time for spatiotemporal data exploration queries over stored data. Our framework deploys lossless data compression to ingest streams of telco big data in the most compact manner retaining full resolution for data exploration tasks. We augment our storage structures with decaying principles that lead to the progressive loss of detail as information gets older. Our framework also includes visual and declarative interfaces for a variety of telco-specific data exploration tasks. We demonstrate SPATE in two modes: (i) Visual Mode, where attendees will be able to interactively explore synthetic telco traces we will provide, and (ii) SQL Mode, where attendees can submit custom SQL queries based on a provided schema.	big data;data compression;interactivity;response time (technology);sql;synthetic intelligence;tracing (software)	Constantinos Costa;Georgios Chatzimilioudis;Demetrios Zeinalipour-Yazti;Mohamed F. Mokbel	2017	2017 IEEE 33rd International Conference on Data Engineering (ICDE)	10.1109/ICDE.2017.203	search engine indexing;big data;visualization;computer science;data mining;database;world wide web	DB	-32.49914300610481	-0.5415068461882789	99511
38a43fabf97259b252f9273abb7262a54a1144b3	methodologies of solution synthesis in distributed expert systems	sistema experto;architecture systeme;intelligence artificielle;analytical method;artificial intelligence;arquitectura sistema;inteligencia artificial;systeme expert;system architecture;expert system	In this paper, several methodologies for designing synthesis strategies in distributed expert systems are investigated. They are analytic methods, inductive methods, and analogical methods. Firstly, synthesis problems are formally described. Secondly, the measurements for synthesis strategies are formally defined. After that, all methodologies are analyzed thoroughly and corresponding examples are introduced. Purthermore, all methodologies are compared and we conclude that they compensate each other. 1 I n t r o d u c t i o n A distributed expert system (DES) consists of different expert systems (ESs) which are connected by computer networks. In a DES, each expert system (ES) can work individually for solving some specific problems, and can also cooperate with other ESs when dealing with complex problems [2]. Due to the limited knowledge and the ability of problem solving of single ESs and the uncertainty features of tasks, some tasks need to be allocated to more than one ESs (multi-allocation) so as to increase the reliability of solutions. If more than one ES solves the same task, each ES could obtain a solution. It is important to synthesize these multiple solutions to the same task (called inputs) from different ESs in order to obtain the desired final solution to the task (called outputs'). For example, experts in grant agency (e.g. ARC council) normally distribute any grant proposal to 3-5 domain experts to assess the proposal, then they make a final decision (score) of the proposal based on the scores and comments from different domain experts. Some synthesis strategies have been developed. These strategies include: uncertainty management strategy developed by Khan [3], a synthesis strategy for heterogeneous distributed expert systems (DESs) introduced by Zhang [7], and computational synthesis strategies for both conflict and non-conflict cases proposed by Zhang [9, 12]. These strategies were mainly based on the mathematical analysis of the characteristics of the inputs.		Minjie Zhang;Chengqi Zhang	1996		10.1007/BFb0030087	legal expert system;computer science;artificial intelligence;operations research	AI	-21.67373978687741	-6.492061804116687	99812
302ed6e1b35aed007e705ec7a0175fd841633bab	comparaison d'organisations spatiales agricoles. le système rosa		This article describes the use of a case-based reasoning approach on spatial organization of farms and their management. Spatial organization graphs (sog) model observed data. They can be recorded within a computer device and automatically compared. Functioning explanations are associated to sog and are organized within a case base. Thus, according to graph similarity, explanations can be proposed and adapted to new sog. The rosa system, still in progress, has been used to evaluate the similarity calculation and the proposed explanations. The inter-disciplinarity framework of this study allows to draw first lessons on knowledge formalization and reasoning on spatial organization of farms. Key-words. case-based reasoning, graph, choreme, spatial structures.	case-based reasoning;spatial organization	Jean-Luc Metzger;Sylvie Lardon;Florence Le Ber	2006	Revue Internationale de Géomatique	10.3166/rig.16.195-210		AI	-23.231458748519806	-2.0925671688540346	99873
dcfb0dfbcb58397e94c1a657aa0f6a2c219bb5c9	designing web-based courses with a case-based reasoning component	case base reasoning		case-based reasoning;web application	Gary Schmidt;Donna E. LaLonde	2000			knowledge representation and reasoning;opportunistic reasoning;case-based reasoning;model-based reasoning;reasoning system	AI	-28.048596290584253	-7.750998901457176	100325
ebe12aa4e9803d3d866a03f2ba5d9a68c4652906	agent-based decision-support framework for credit risk early warning	financial data processing;decision support;design principle;decision support systems buildings intelligent systems intelligent agent intelligent structures decision making process design predictive models information analysis risk analysis;prediction decision support systems;agent based;early warning;multi agent systems;decision support system;decision making process;decision support systems;risk prediction;intelligent system;credit risk early warning;cooperative intelligent systems;knowledge based systems;multiagent decision support system;credit risk;risk prediction credit risk early warning prediction decision support systems cooperative intelligent systems multiagent decision support system decision making;multi agent systems decision making decision support systems financial data processing knowledge based systems	Credit risk early warning is a complicated process. Early studies of prediction decision support systems (DSS) offer a passive form of support, where users needed to have full knowledge on how to use the data sources and other tools to perform all necessary operations. Agents are considered have capabilities of observe the environment and learn from previous experience to make their own decision, and could building active cooperative intelligent systems. This paper proposes a framework of a multi-agent decision support system (MADSS) used in credit risk prediction. The structure of a single agent is defined, and the mechanism of decision making process is designed. The framework supports model training, risk prediction and results analyzing based on financial information of customer. We outline the design principles and develop architecture for MADSS	decision support system;gene prediction;multi-agent system	Yan Peng;DiMing Ai;Like Zhuang	2006	Sixth International Conference on Intelligent Systems Design and Applications	10.1109/ISDA.2006.253737	decision-making;decision support system;credit risk;computer science;knowledge management;artificial intelligence;warning system;data mining;management science	Robotics	-31.208298610628596	-7.989098713112159	100685
e3d85250808b8b2de549d4e1ea4dcc496cc53b93	a pareto model for olap view size estimation	pareto distribution;olap;view size estimation;materialized view selection	On-Line Analytical Processing (OLAP) aims at gaining useful information quickly from large amounts of data residing in a data warehouse. To improve the quickness of response to queries, pre-aggregation is a useful strategy. However, it is usually impossible to pre-aggregate along all combinations of the dimensions. The multi-dimensional aspects of the data lead to combinatorial explosion in the number and potential storage size of the aggregates. We must selectively pre-aggregate. Cost/benefit analysis involves estimating the storage requirements of the aggregates in question. We present an original algorithm for estimating the number of rows in an aggregate based on the Pareto distribution model. We test the Pareto Model Algorithm empirically against four published algorithms, and conclude the Pareto Model Algorithm is consistently the best of these algorithms for estimating view size.	online analytical processing;pareto efficiency	Thomas P. Nadeau;Toby J. Teorey	2001		10.1145/782096.782109	materialized view;pareto analysis;online analytical processing;computer science;cost–benefit analysis;marketing;pareto distribution;operations management;data warehouse;data mining;database;management science;management	DB	-26.174675587409947	3.697757582799651	100744
dd95d80c824442680afa441a81fd4410c2c6228d	universal knowledge processing systems: a conceptual view and architecture	knowledge base;knowledge based system	In this paper we present the architectural concepts of highly complex knowledge based system dubbed as the Universal Knowledge Processing System (UKPS) could be thought of as a Wisdom Machine (WM) capable of solving any problem simple or difficult with the help of its knowledge bases. The Universal Knowledge Processing Machine is in turn formed of several Knowledge Processing Systems (KPS) connected together through a global network The paper also discusses the key block level components which go on to make the architecture of this highly complex based system. Some of the concepts presented here are at an abstract level and needs further investigation in the cycle of evolution and development.	global network;knowledge-based systems	Ajit Reddy;Syed V. Ahamed	2006			architecture;knowledge engineering;domain knowledge;knowledge base;knowledge-based systems;knowledge integration;knowledge management;open knowledge base connectivity;computer science;systems engineering;legal expert system	AI	-31.02279080042432	-5.937090776732036	100814
f4ed9233e58c89840858647899a8bcfe6f5b9a43	efficient xpath query processing in native xml databases	native xml databases;multiple view;xml query;efficient xpath query processing;native xml databases;xml query representation;join-based approach;general xml index;xml index;efficient xml query processing;xml encodings;efficient xml query evaluation	As XML (eXtensible Markup Language) becomes a universal medium for data exchange over the Internet, efficient XML query processing is now the focus of considerable research and development activities. This thesis describes works toward efficient XML query evaluation and optimization in native XML databases.#R##N#A XML query can be decomposed to a sequence of structural joins (e.g., parent/child and ancestor/descendant) and content joins. Thus, structural join optimization is a key to improving join-based evaluation. We optimize structural join with two orthogonal methods: partition-based method exploits the spatial specialities of XML encodings by projecting them on a plane; and location-based method improves structural join by accurately pruning all irrelevant nodes, which cannot produce results.#R##N#XML indexes are widely studied to evaluate XML queries and in particular to accelerate join-based approaches. Index-based approaches outperform join-based approaches (e.g., holistic twig join) if the queries match the index. Existing XML indexes can only support a small set of XML queries because of the varieties in XML query representations. A XML query may involve child-axis only, both child-axis and branches, or additional descendant-or-self-axis but only in the query root. We propose novel indexes to efficiently support a much wider range of XML queries (with /, //, [], *).#R##N#A general XML index can itself be sizable leading to low efficiency. To alleviate this predicament, frequently asked queries can be indexed by a database system. They are referred to as views. Answering queries using materialized views is always cheaper than evaluating over the base data. Traditional techniques solve this problem by considering only a single view. We approach this problem by exploiting the potential relationships of multiple views, which can be used together to answer a given query. Experiments show that significant performance gain can be achieved from multiple views.	xml database;xpath	Nan Tang	2007			xml catalog;xml validation;binary xml;xml encryption;xml namespace;simple api for xml;xml schema;streaming xml;computer science;document structure description;xml framework;data mining;xml database;xml schema;database;xml signature;xml schema editor;information retrieval;efficient xml interchange	DB	-31.124258444258142	3.7971782960173437	101453
f2568b567009a604acfe5bf880896bf70a89e4f2	tagged mapreduce: efficiently computing multi-analytics using mapreduce	data analysis;mapreduce;on line analytical processing	MapReduce is a programming paradigm for effective processing of large datasets in distributed environments, using the map and reduce functions. The map process creates (key, value) pairs, while the reduce phase aggregates same-key values. In other words, a MapReduce application defines and reduces one set of values for each key, which means that the user only knows one aspect of the key. Advanced OLAP applications however, require multiple sets to be defined and reduced for the same key, not necessarily mutually disjoint. The challenge is to extend MapReduce to support this in a syntactically simple and computationally efficient way. We propose an extension to the classic MapReduce model, called Tagged MapReduce, where data is represented as (key, value, tag) triplets. Users map triplets and reducing takes place for each key and for each tag. For example, given a set of pages, one may want to count words' occurrences per page type. The page type is represented by the tag. While the classic MapReduce can handle this class of queries, it requires effort and possibly advanced programming skills for efficient implementations. For example, should the tag form a compound object with the key or the value? Our formalism makes it simpler for the programmer to use and easier for the system to identify and apply efficient algorithms.	mapreduce	Andreas Williams;Pavlos Mitsoulis-Ntompos;Damianos Chatziantoniou	2011		10.1007/978-3-642-23544-3_18	computer science;machine learning;data mining;database;distributed computing;data analysis	DB	-30.035440182224555	3.564090786370529	101461
1864afd3372bebaa6084926dbf68ff4976594fef	the universe of online databases; reality and model(s)	banque donnee;modelizacion;base integrada dato;on line processing;articulo sintesis;article synthese;databank;universe of discourse;base connaissance;tratamiento en linea;modelisation;banco dato;mathematical model;base conocimiento;integrated database;traitement en ligne;review;modeling;data structure;formal language;knowledge base;base donnee integree	The primary purposes of this article is to describe the range of currently available online databases. This will be done by analysing the modelling effect during database construction, that is by looking at the universes of discourse, the targets of modelling and the resulting data structures. As tools for mod elling, the following information types are differentiated: prop erty space, formatted fields, texts, formal languages (e.g., in chemistry) and mathematical models (in physics). Finally, some remarks are made concerning the online database of tomorrow which should be 'integrated' (integrating different information types) and ' knowledge-based' (with meta-knowledge about in formation types, the universe of discourse and the database).	database	Josef L. Staud	1988	J. Information Science	10.1177/016555158801400303	knowledge base;formal language;systems modeling;data structure;computer science;artificial intelligence;mathematical model;domain of discourse;database;algorithm	NLP	-25.786889588258003	-2.0665979247711213	101571
c2129fa269e26332ab70336f4758ed1cdf098d2a	dimensions based data clustering and zone maps		In recent years, the data warehouse industry has witnessed decreased use of indexing but increased use of compression and clustering of data facilitating efficient data access and data pruning in the query processing area. A classic example of data pruning is the partition pruning, which is used when table data is range or list partitioned. But lately, techniques have been developed to prune data at a lower granularity than a table partition or sub-partition. A good example is the use of data pruning structure called zone map. A zone map prunes zones of data from a table on which it is defined. Data pruning via zone map is very effective when the table data is clustered by the filtering columns. The database industry has offered support to cluster data in tables by its local columns, and to define zone maps on clustering columns of such tables. This has helped improve the performance of queries that contain filter predicates on local columns. However, queries in data warehouses are typically based on star/snowflake schema with filter predicates usually on columns of the dimension tables joined to a fact table. Given this, the performance of data warehouse queries can be significantly improved if the fact table data is clustered by columns of dimension tables together with zone maps that maintain min/max value ranges of these clustering columns over zones of fact table data. In recognition of this opportunity of significantly improving the performance of data warehouse queries, Oracle 12c release 1 has introduced the support for dimension based clustering of fact tables together with data pruning of the fact tables via dimension based zone maps.	boolean algebra;cluster analysis;column (database);data access;database;map;maxima and minima;snowflake schema	Mohamed Ziauddin;Andrew Witkowski;You Jung Kim;Janaki Lahorani;Dmitry Potapov;Murali Krishna	2017	PVLDB	10.14778/3137765.3137769	database;data mining;fact table;search engine indexing;cluster analysis;table (information);data warehouse;data access;computer science;snowflake schema;granularity	DB	-27.940803021922896	1.7118026655269292	101580
830b989faa0afb3aeb099726f86045b7738e39c1	dynamic temporal interpretation contexts for temporal abstraction	drugs;protocols;knowledge based system;formal knowledge based framework;temporal logic;interpretation contexts;traffic control;runtime;computerized monitoring;temporal abstraction;temporal interpretation;medical treatment computerized monitoring protocols biomedical informatics context modeling runtime ontologies medical diagnostic imaging drugs patient monitoring;knowledge acquisition;ontologies;patient monitoring;traffic engineered;time stamped data;knowledge representation;medical treatment;biomedical informatics;context modeling;temporal abstraction knowledge;knowledge based systems;context sensitive;temporal reasoning;medical diagnostic imaging;interpretation contexts temporal interpretation temporal abstraction time stamped data context sensitive formal knowledge based framework temporal abstraction knowledge;knowledge base	Temporal abstraction is the task of abstracting higher-level concepts from time-stamped data in a contextsensitive manner. We have developed and implemented a formal knowledge-based framework for decomposing and solving that task that supports acquisition, maintenance, reuse, and sharing of temporalabstraction knowledge. We present the logical model underlying the representation and runtime formation of interpretation contexts. Interpretation contexts are relevant for abstraction of time-oriented data and are induced by input data, concluded abstractions, external events, goals of the temporal-abstraction process, and certain combinations of interpretation contexts. Knowledge about interpretation contexts is represented as a context ontology and as a dynamic induction relation over interpretation contexts and other proposition types. Induced interpretation contexts are either basic, composite, generalized, or nonconvex. We provide two examples of applying our model using an implemented system; one in the domain of clinical medicine (monitoring of diabetes patients) and one in the domain of traffic engineering (evaluation of traffic-control actions). We discuss several distinct advantages to the explicit separation of interpretation-context propositions from the propositions inducing them and from the abstractions created within them.		Yuval Shahar	1996		10.1109/TIME.1996.555683	natural language processing;computer science;knowledge management;data mining	AI	-21.377079933861303	1.8715508164939425	101613
03270098385eeec9da4d3e592f2ed2746320fa15	big data technology literature review		"""Storing and manipulating Big Data relies on various data structures, algorithms and technologies. Some of these are new, while others have existed for quite a while (the Bloom filter was presented in 1970) and are now making their way into mainstream software engineering. We present those algorithms and technologies briefly, and provide citations for further reading. This paper is organized as follows: First we discuss relevant theoretical background, including algorithms and design approaches. Then we look at concrete technologies and a few example products. Many Big Data products exist today, and new ones are being added constantly. Thus, products mentions below should be treated as concrete examples, rather than recommendations. Each topic ends with a """" further reading """" section, for those who wish to deepen their knowledge. Algorithms and Architectures Traditionally, when discussing an algorithm, there is an underlying assumption that the data can fit in the memory. That is not to say that memory consumption is ignored-on the contrary, big-O space analysis is part of any introduction to algorithm class. But in the """" big data """" world, even algorithms that need O(n) space have to be distributed. Sharding and Consistent Hashing When the dataset is too large to be stored on a single machine, is has to be divided between a few machines. Such division is called horizontal partitioning, or """" sharding """". Each partial dataset stored on a single machine is called """" a shard """". Consistent Hashing is a good algorithm for sharding data. It is simple, scalable, and allows for easy fault tolerance. Consistent hashing is based on a hash function that maps each data point to a point on an edge of a circle. Shards are assigned points on the same circle. Each shard is then responsible for all data points in the arc immediately clockwise adjacent to it. To allow fault tolerance, data are from shard s are backed up to shards further from the arc s is responsible for. This way, when s fails, the system can continue without losing data or dealing with a """" corner case """". Shards can be moved along the circle in order to balance their load."""	algorithm;backup;big data;bloom filter;consistent hashing;corner case;data point;data structure;fault tolerance;hash function;map;partition (database);scalability;shard (database architecture);software engineering;the circle (file system);while	Michael Bar-Sinai	2015	CoRR		computer science;data mining;database;world wide web	ML	-33.39744995322294	-0.6342768177680823	101673
89d8d0f56a98a1015390a8146d20b2041af1b8e2	p-luposdate: using precomputed bloom filters to speed up sparql processing in the cloud		Increasingly data on the Web is stored in the form of Semantic Web data. Because of today’s information overload, it becomes very important to store and query these big datasets in a scalable way and hence in a distributed fashion. Cloud Computing offers such a distributed environment with dynamic reallocation of computing and storing resources based on needs. In this work we introduce a scalable distributed Semantic Web database in the Cloud. In order to reduce the number of (unnecessary) intermediate results early, we apply bloom filters. Instead of computing bloom filters, a time-consuming task during query processing as it has been done traditionally, we precompute the bloom filters as much as possible and store them in the indices besides the data. The experimental results with data sets up to 1 billion triples show that our approach speeds up query processing significantly and sometimes even reduces the processing time to less than half. TYPE OF PAPER AND	big data;bloom filter;cloud computing;database;information overload;precomputation;sparql;scalability;semantic web;world wide web	Sven Groppe;Thomas Kiencke;Stefan Werner;Dennis Heinrich;Marc Stelzner;Le Gruenwald	2014	OJSW		computer science;database;distributed computing;world wide web	DB	-31.235942345853243	-0.3882122707068052	101730
c8e26d2ade5d125ef654b0e21c287622b6810416	scalability and efficiency challenges in large-scale web search engines	query processing;caching;efficiency;indexing;scalability;web search engines;crawling	The main goals of a web search engine are quality, efficiency, and scalability. In this tutorial, we focus on the last two goals, providing a fairly comprehensive overview of the scalability and efficiency challenges in large-scale web search engines. In particular, the tutorial provides an in-depth architectural overview of a web search engine, mainly focusing on the web crawling, indexing, and query processing components. The scalability and efficiency issues encountered in these components are presented at four different granularities: at the level of a single computer, a cluster of computers, a single data center, and a multi-center search engine. The tutorial also points at open research problems and provides recommendations to researchers who are new to the field.	scalability;web search engine	Ricardo A. Baeza-Yates;Berkant Barla Cambazoglu	2014		10.1145/2567948.2577271	search engine indexing;web modeling;web query classification;scalability;computer science;crawling;distributed web crawling;database;efficiency;search analytics;web search query;world wide web;information retrieval;search engine	Web+IR	-33.63909756450898	2.3251182556237837	102082
6df1722570bd7635d6c84448332cac36c1ff3532	machine learning methods for intelligent decision support: an introduction	decision support;machine learning			Michael J. Shaw	1993	Decision Support Systems	10.1016/0167-9236(93)90031-W	robot learning;error-driven learning;r-cast;decision support system;intelligent decision support system;decision engineering;computer science;machine learning;pattern recognition;active learning	AI	-30.215035058987365	-8.356983473375132	102099
16d54ed0c17eca8fe816cc666ab378875cd68fcb	notification of data-stream events in publish/subscribe systems using fuzzy matching	distributed processing;fuzzy sets data stream event notification publish subscribe systems fuzzy matching distributed paradigm;pattern matching data handling distributed processing fuzzy set theory message passing middleware;fuzzy set theory;decision support systems intelligent systems mercury metals;fuzzy matching data streams event detection publish subscribe;pattern matching;message passing;middleware;data handling	Data streams are vast sources of information. In such streams only certain episodic points called events are interesting. The number of events occurring in a data stream can be very large, and a given user would not be interested in all of them. Thus, there is a need to selectively disseminate information about the occurrence of events to users who express interest in them. Publish/Subscribe is a distributed paradigm of messaging in which events published by publishers are automatically sent to subscribers who have expressed interest in them. In this work, we apply the publish/subscribe paradigm to notify a user when an event of interest occurs. Furthermore, user subscriptions may often be vague and non-specific. To capture this uncertainty, we use fuzzy set theory in the modeling of the subscriptions. The use of fuzzy sets gives a user more expressive power in subscriptions and gives better notifications of events.	fuzzy set;programming paradigm;publish–subscribe pattern;set theory;vagueness	H Prabhu RahulPrabhu;Shrisha Rao	2012	2012 12th International Conference on Intelligent Systems Design and Applications (ISDA)	10.1109/ISDA.2012.6416538	message passing;computer science;pattern matching;group method of data handling;middleware;data mining;database;distributed computing;fuzzy set	DB	-23.973280713010045	3.882922485783845	102237
cb6f1aa77852ac496604ef9f12b5aefea0bb199d	efficient vsm-based parallel text retrieval on a pc-cluster environment using mpi.				Dimitris Kehagias;Basilis Mamalis;Grammati E. Pantziou	2005			data mining;database;information retrieval	HPC	-30.325352642144026	-1.9601843600679163	102333
6fc1ff9264f4e50e60da19d5c82489104ff4fa20	intelligent knowledge based systems for diagnosis of machinery	knowledge based system		knowledge-based systems	Christian Steinebach	1988			intelligent decision support system;domain knowledge;knowledge-based systems;computer science;knowledge management	AI	-29.50558459301724	-7.094446201261613	102491
8bf11e69b2db124c3f15179fe20a491a0a425964	a situated classification solution of a resource allocation task represented in a visual language	heuristique;representacion conocimientos;user interface;resource allocation;intelligence artificielle;classification;resolucion problema;probleme sisyphus;knowledge structure;langage visuel;classification rules;classification system;acquisition;visual language;problem solving method;artificial intelligence;asignacion recurso;inteligencia artificial;knowledge representation;allocation ressource;representation connaissances;subsumption;domain ontology;clasificacion;adquisicion;domain specificity;problem solving;resolution probleme	The Sisyphus room allocation problem solving example has been solved using a situated classification approach. A solution was developed from the protocol provided in terms of three heuristic classification systems, one classifying people, another rooms, and another tasks on an agenda of recommended room allocations. The domain ontology, problem data, problem-solving method, and domain-specific classification rules, have each been represented in a visual language. These knowledge structures compile to statements in a term subsumption knowledge representation language, and are loaded and run in a knowledge representation server to solve the problem. The user interface has been designed to provide support for human intervention in under-determined and overdetermined situations, allowing advantage to be taken of the additional choices available in the first case, and a compromise solution to be developed in the second.	compiler;heuristic;knowledge representation and reasoning;ontology (information science);problem solving;server (computing);situated;subsumption architecture;user interface;visual language	Brian R. Gaines	1994	Int. J. Hum.-Comput. Stud.	10.1006/ijhc.1994.1012	knowledge representation and reasoning;human–computer interaction;biological classification;resource allocation;computer science;artificial intelligence;user interface;algorithm	AI	-25.261795308187033	-7.681119926257973	102534
3c580d448b43f8eb45e178bad85bc5e92f2d5b51	recognizing probabilistic opponent movement models	multiagent system;futbol americano;soccer;abstraction;robotics;distributed team;probabilistic approach;abstraccion;football americain;preparacion serie fabricacion;planificacion;enfoque probabilista;approche probabiliste;football;robotica;planning;adaptive response;robotique;process planning;robot soccer;planification;sistema multiagente;preparation gamme fabrication;systeme multiagent;futbol	In multiagent adversarial domains, team agents should adap t to the environment and opponent. We introduce a model representat io s part of a planning process for a simulated soccer domain. The planning is c entralized, but the plans are executed in a multi-agent environment, with teamm te and opponent agents. Further, we present a recognition algorithm where t he model which most closely matches the behavior of the opponents can be selecte d from few observations of the opponent. Empirical results are presented to verify that important information is maintained through the abstraction the mode ls provide.	agent-based model;algorithm;multi-agent system	Patrick Riley;Manuela M. Veloso	2001		10.1007/3-540-45603-1_59	planning;simulation;computer science;artificial intelligence;machine learning;adaptive response;abstraction;robotics;fictitious play	AI	-22.207717927523365	-7.691766106349189	102551
04061b440a93a527ed266f4167bb0e039bc8dad9	intelligent cad systems i		"""Assumptions for CAD are discussed, pointing to a distinction between human knowledge and machine representations of knowledge. Implications for future useful CAD systems are considered. A strategy for """"mechanistic"""" symbol processors is presented, employing """"mechanisms"""" of formal logic to manipulate written and drawn expressions of designers' knowledge."""	central processing unit;computer-aided design	Tetsuo Tomiyama	1987		10.1007/978-3-642-72945-4		AI	-26.14641169204299	-3.7264658592931963	102690
9658470a5c00bb201b3299c4cfa51fb2da9b2a6f	cognitive modeling and dynamic probabilistic simulation of operating crew response to complex system accidents: part 3: idac operator response model	fiabilite humaine;modelo dinamico;modelizacion;analyse risque;operating room;dynamic probabilistic risk assessment;procesamiento informacion;bloc operatoire;systeme aide decision;risk analysis;dynamic model;accident;diagnostico;performance influencing factors;sistema complejo;sistema ayuda decision;prise decision;probabilistic approach;cognitive;modelisation;analisis riesgo;human reliability;decision support system;systeme complexe;complex system;ingeniero consejero;enfoque probabilista;approche probabiliste;modele dynamique;information processing;personal de navegacion;personnel navigant;quirofano;risk assessment;consultant;crew;nuclear power plant;centrale nucleaire;accidente;ingenieur conseil;human reliability analysis;traitement information;toma decision;diagnosis;modeling;central nuclear;performance shaping factors;evaluation risque;fiabilidad humana;diagnostic	This is the third in a series of five papers describing the IDAC (Information, Decision, and Action in Crew context) model for human reliability analysis. An example application of this modeling technique is also discussed in this series. The model is developed to probabilistically predict the responses of the nuclear power plant control room operating crew in accident conditions. The operator response spectrum includes cognitive, emotional, and physical activities during the course of an accident. This paper discusses the modeling components and their process rules. An operator’s problem-solving process is divided into three types: information preprocessing (I), diagnosis and decision-making (D), and action execution (A). Explicit and context-dependent behavior rules for each type of operator are developed in the form of tables, and logical or mathematical relations. These regulate the process and activities of each of the three types of response. The behavior rules are developed for three generic types of operator: Decision Maker, Action Taker, and Consultant. This paper also provides a simple approach to calculating normalized probabilities of alternative behaviors given a context. r 2006 Elsevier Ltd. All rights reserved.	cognitive model;complex system;context-sensitive language;human reliability;preprocessor;problem solving;simulation;table (database)	Y. H. J. Chang;Ali Mosleh	2007	Rel. Eng. & Sys. Safety	10.1016/j.ress.2006.05.013	reliability engineering;risk assessment;simulation;systems modeling;cognition;risk analysis;decision support system;information processing;engineering;artificial intelligence;human reliability;operations research	AI	-24.37367247222938	-6.701338336061087	102907
a536833a1babb5f2a4b907b34c1c93d3af332e5a	strategies for distributing goals in a team of cooperative agents	cooperative agents;autre	This paper addresses the problem of distributing goals to individual agents inside a team of cooperative agents. It shows that several parameters determine the goals of particular agents. The first parameter is the set of goals allocated to the team; the second parameter is the description of the real actual world; the third parameter is the description of the agents’ ability and commitments. The last parameter is the strategy the team agrees on: for each precise goal, the team may define several strategies which are orders between agents representing, for instance, their relative competence or their relative cost. This paper also shows how to combine strategies. The method used here assumes an order of priority between strategies.	intelligent agent;possible world	Laurence Cholvy;Christophe Garion	2004		10.1007/11423355_13	knowledge management;operations management;political science;management science	AI	-20.522956907595642	-9.189236676446813	103039
d5d5867424b71ee578038b38120abe4a446539eb	planning using a temporal world model	temporal logic;problem solving	Current problem solving systems are constrained in their applicability by inadequate world models. We suggest a world model based on a temporal logic. This approach allows the problem solver to gather constraints on the ordering of actions without having to commit to an ordering when a conflict is detected. As such, it generalizes the work on nonlinear planning by Sacerdoti and Tate. In addition, it allows more general descriptions of actions that may occur simultaneously or overlap, and appears promising in supporting reasoning about external events and actions caused by other agents.	action potential;backtracking;nonlinear system;precondition;problem solving;semantics (computer science);solver;temporal logic	James F. Allen;Johannes A. G. M. Koomen	1983			simulation;temporal logic;computer science;artificial intelligence;machine learning;mathematics;algorithm;temporal logic of actions	AI	-19.446261683332164	-4.375708738357314	103144
0e47ceae09890f8e09189227e81c288bd8b5d5ad	systematic analysis and modelling of diagnostic errors in medicine				Shijing Guo	2016				NLP	-29.150532627399258	-8.8136217489342	103378
fde7b31917a4f1f96d1b64d80075fedd2c566c15	an approach to articulating expert system rule bases	logical elements;expert systems;rule based;hierarchies;failure analysis;systems analysis;failure modes;algorithms;knowledge representation;expert system			K. Arbernethy	1988		10.1145/55674.55695	rule-based system;legal expert system;systems analysis;failure analysis;conflict resolution strategy;failure causes;computer science;knowledge management;artificial intelligence;machine learning;data mining;expert system;hierarchy	ML	-26.421754605459604	-6.509529148060417	103507
0a646e080a810af064261f245873826932f53cf6	doco: an efficient event matching algorithm in content-based publish/subscribe systems	index structure;publish subscribe systems;event matching;double combination	The content-based publish/subscribe systems are attracting more and more attention in Internet applications due to their intrinsic time, space, and synchronization decoupling properties. With the increase in system scale, the efficiency of event matching becomes more critical for system performance. However, most existing methods suffer significant performance degradation when the system has large volumes of subscriptions. This paper presents DOCO (DOuble COmbination event matching algorithm) to improve the efficiency of event matching in content-based publish/subscribe systems. Via assembling the attributes in the attribute space by pairs, a novel index structure is built up for classification of the subscriptions. On the arrival of an event, the event matching process is only carried out on some related units of the index structure, thus, the number of subscriptions that involved in the event matching process is reduced. A series of experiments are designed to verify the performance of the proposed algorithm, and a comparison with other event matching algorithms is also carried out. The experimental results show that DOCO can improve the efficiency of event matching in content-based publish/subscribe systems.	algorithm;coupling (computer programming);database index;elegant degradation;experiment;publish–subscribe pattern	Jingli Yang;Jing Fan;Shouda Jiang	2016	2016 IEEE 22nd International Conference on Parallel and Distributed Systems (ICPADS)	10.1109/ICPADS.2016.0035	real-time computing;computer science;data mining;database;distributed computing	DB	-26.65271646432329	-0.062422439262084385	103993
b764b68ce7fc9efb3f14cbc6471adb5b8d20b598	processing knowledge-based systems containing rules with complex bodies	knowledge based system		knowledge-based systems	Jonghoon Chun;Lawrence J. Henschen	1992				AI	-28.936720806911698	-7.357775968548267	104131
2fe713e744d2ec5139d0eec5eb4e412a9a2ebbef	building smart material knowledge system based on ontology bayesian network and grid model	bayesian network;domain ontology;knowledge system;smart material	A smart material using piezoelectric ceramics to have the function of both the sensor and actuator is developed for such a vibration suppression system. This ontology is a guide, which provides a vocabulary for the visual description of domain classes. But traditional smart material system construction is time-consuming and costly procedure. This paper present a novel smart material system construction method based on ontology Bayesian network and grid model. The experimental results indicate that the Bayesian network building smart material algorithm achieves significant performance improvement. © 2011 Springer-Verlag Berlin Heidelberg.	bayesian network;knowledge-based systems	Jing Li;Xiuying Sun	2011		10.1007/978-3-642-23777-5_33	knowledge management;data science;data mining	AI	-31.178353401787277	-5.8035534627651355	104172
8cae696c4d9ef44399e0a2a3a333e751f5e0c2b6	an organizational decision support system for effective r&d project selection	decision models;organization;project management;object oriented methods;systeme aide decision;social decision;r d project selection;project manager;recherche developpement;natural science foundation of china;sistema ayuda decision;prise decision;decision maker;support system;decision support system;selection projet;research and development;odss application;investigacion desarrollo;decision making process;methode orientee objet;gestion projet;decision colectiva;decision collective;toma decision;organisation;organizacion;seleccion proyecto;project selection;odss architecture;gestion proyecto;odss	Research and development (R&D) project selection is an important task for organizations with R&D project management. It is a complicated multi-stage decision-making process, which involves groups of decision makers. Current research on R&D project selection mainly focuses on mathematical decision models and their applications, but ignores the organizational aspect of the decision-making process. This paper proposes an organizational decision support system (ODSS) for R&D project selection. Object-oriented method is used to design the architecture of the ODSS. An organizational decision support system has also been developed and used to facilitate the selection of project proposals in the National Natural Science Foundation of China (NSFC). The proposed system supports the R&D project selection process at the organizational level. It provides useful information for decision-making tasks in the R&D project selection process. D 2004 Elsevier B.V. All rights reserved.	decision support system	Qijia Tian;J. Ma;Jiazhi Liang;Ron Chi-Wai Kwok;Ou Liu	2005	Decision Support Systems	10.1016/j.dss.2003.08.005	project management;decision-making;decision support system;organization;knowledge management;management;operations research	SE	-26.971911658289475	-4.625008852137832	104209
3797406ae6695d9a8a431813f0042d5064c1e461	using preference based heuristics to control abductive reasoning	abductive reasoning;partial order	Explanations for symptoms can be selected using a variety of criteria. When numerical information is scarce, approaches which depend on partial orders could be of value. The problem of generating explanations for symptoms when only preference relationships between possible causes is considered. The lack of information leads to large number of possible solutions and control of the reasoning process is very important. A computational approach based on focusing the reasoning using a cost bounded ATMS is described. 1 I n t r o d u c t i o n In diagnosis an explanation can be considered as a set of assumptions which if true either predict the symptoms or are at least consistent with the symptoms. A preference based ordering of assumptions can be used to induce an ordering over sets of assumptions. An abductive explanation for a set of symptoms is a set of assumptions which the model of the system can use to deduce the symptoms. In the context of explanations for symptoms, if preferences between assumptions are represented by explicit preferability relations, then an ordering over the explanations can be deduced. More specifically, suppose A is a set of assumptions with an associated binary preferability relation < induced by some preference criterion on A, then a < b iff assumption a e A is less preferable to assumption b e A. There are many ways of defining a preference relation << on the power set of A, i.e. on the explanations. 2 Using heuristics to generate preferred explanations: a simple example In this section a simple example is presented which will be used to motivate and illustrate the subsequent sections. Figure 1 shows a functional model, built out of functional entities called units. Each unit corresponds to a specific functionality of the modelled system (telecommunication network, or medical system, or space craft.) A unit is for example a multiplexing functionality or the functionality of transmitting data from one unit to another unit, or the behaviour of a test to produce test results. Units are connected to other units by using ports. A port defines how one unit should connect to other units, and has an associated domain which defines the nature of the interaction. A port connects to the port of the other unit. An important point is that no logic is represented in the connections between ports. A line between two ports just establishes the equivalence of the ports. An input port can have only one input connection from another unit, though an output port can be connected to any number of input ports of	abductive reasoning;computer port (hardware);entity;function model;heuristic (computer science);multiplexing;numerical analysis;telecommunications network;transmitter;turing completeness	John Bigham	1994		10.1007/BFb0035975	partially ordered set;abductive reasoning;computer science;artificial intelligence;non-monotonic logic;model-based reasoning;machine learning;reasoning system;deductive reasoning;algorithm;abductive logic programming	AI	-19.18654616864789	0.7542826577493651	104212
6301583a55defab6cb247a48931d3c6286d757e4	numeric and symbolic knowledge representation of cortex anatomy using web technologies	analyse symbolique;inference motor;informatica biomedical;representacion conocimientos;ontologie;biomedical data processing;red www;systeme nerveux central;dato numerico;genie biomedical;flexibilidad;variacion anatomica;analisis simbolico;variation anatomique;xml language;informatique biomedicale;reseau web;hombre;langage java;donnee numerique;encefalo;anatomia;sistema nervioso central;motor inferencia;biomedical engineering;encephale;numerical data;human;xml;parietal lobe;world wide web;lenguaje java;flexibilite;evaluation;ingenieria biomedica;brain atlas;symbolic analysis;information system;evaluacion;knowledge representation;anatomical variation;anatomie;representation connaissances;web technology;ontology;anatomy;systeme information;moteur inference;langage xml;lenguaje xml;central nervous system;flexibility;java language;homme;sistema informacion;knowledge base;brain vertebrata	We propose an ontology of the brain cortex anatomy representing both numeric and symbolic knowledge. Implementation with web technologies facilitates the ontology's dissemination and reuse in different application contexts. This model references the main terminological sources and can be integrated into larger scale models. It was instantiated as an XML file encompassing anatomical features of the frontal, temporal and parietal lobes. A standalone Java application and an HTML based one have been developed in order to illustrate the knowledge base flexibility. They are both fully interactive with 3D VRML scenes representing the shapes of brain sulci.		Olivier Dameron;Bernard Gibaud;Xavier Morandi	2001		10.1007/3-540-48229-6_49	knowledge base;xml;computer science;artificial intelligence;ontology;algorithm	ML	-26.034937242871766	-2.944730238745566	104261
d0efff004529da6e7c73a2fe8d588236cab52299	u-mart project: learning economic principles from the bottom by both human and software agents	computer program;programme recherche;concepcion sistema;agent based;software agent;economie marche;test bed;agent logiciel;trading strategy;software agents;market economy;system design;u mart system;programa investigacion;learning artificial intelligence;programa computador;research program;economia mercado;conception systeme;programme ordinateur;apprentissage intelligence artificielle	U-Mart is an interdisciplinary research program of agent-based artificial market. U-Mart proposes an open-type test bed to study trading strategies of agents, behavior of the market and their relationship. An experiment open to public (Pre U-Mart 2000) using the proposed system is held in August 2000. More than 40 software agents (computer programs for trading) from 11 teams participated in this experiment. This paper reports the outline of the experiment, the trading strategies of the participated agents and the results of the experiment. While Pre U-Mart 2000 treated only software agents, the U-Mart system is designed considering participation of the human players as well as the software agents. A gaming simulation by human using the U-Mart system held in Kyoto University is also introduced briefly.	data mart;software agent	Hiroshi Sato;Hiroyuki Matsui;Isao Ono;Hajime Kita;Takao Terano;Hiroshi Deguchi;Yoshinori Shiozawa	2001		10.1007/3-540-45548-5_15	simulation;computer science;artificial intelligence;software agent	AI	-23.247019150061078	-7.5352520944051875	104804
126efc110692d2fc428d6023efb912c1c4aaf731	optimum storage allocation for a file in steady state	steady state	A file of fixed-length records in auxiliary storage using a key-to-address transformation to assign records to addresses is considered. The file is assumed to be in steady state, that is that the rates of additions to and of deletions from the file are equal. The loading factors that minimize file maintenance costs in terms of storage space and additional accesses are computed for different bucket sizes and different operational conditions. Introduction This paper describes a study of auxiliary storage allocation for a key-to-address transformation file organization [ 1 31. In this organization, there is a primary storage area divided into buckets. Each bucket can contain s records of the file. The transformation is applied to the key of a record to be loaded and it produces the address of the bucket into which that record will be stored. If more records are assigned to a bucket than it can hold, the excess records are stored in a separate overflow area. The overflow records belonging to a particular bucket are organized in a chain with address pointers. To retrieve a record with a given key, a single access to auxiliary storage is sufficient if that record is stored in the primary storage area. If it is in the overflow area, one or more additional accesses following the pointers from bucket to record and from record to record-are required. The number of accesses depends on the position of the record in the chain. The average length of the chains and, therefore, the average number of additional accesses depends on the bucket size and the storage capacity that is provided in the primary area relative to the number of records to be stored. Studies [3 -51 have been made of the optimal storage allocation for a file that does not change after initial loading. Lum, et al. [3], made performance studies using actual files and different key-to-address transformations. As performance indicators, the authors use the average number of accesses per record and the number of overflow records. Under certain assumptions, formulae can be derived for these quantities [4]. Using these formulae it is possible to compute, for each bucket size, the load factors (that is, the ratio between the number of records to be loaded and the capacity of primary storage) that makes file-handling costs minimal [ 5 ] . These costs consist of storage cost for primary and overflow area, plus the cost for additional accesses. In the study reported in this paper, a different situation is analyzed. A file is considered to which records are added and from which records are deleted. Olsen [6] points out the difference between this situation and the initial loading problem. He considers a steady-state situation in which the number of additions and deletions per time period are equal, and he derives a formula for the average overflow. In this paper, a method is developed to compute also the average number of additional accesses. With these results, the optimal loading for the steady-state situation can be computed in a similar fashion to that for the initial loading model [5]. Analytical approach The file contains II records. The primary storage area consists of b buckets, each of size s. The average loading -number of records assigned to a bucket-is thus: rn = nlb. The load factor is It is assumed that the overflow area has unlimited capacity. AI1 records have equal probability to be de27 STEADY STATE FILE STORAGE	auxiliary memory;bucket (computing);computer data storage;hash table;memory management;offset binary;single-access key;steady state	Jan A. van der Pool	1973	IBM Journal of Research and Development	10.1147/rd.171.0027	real-time computing;computer science;operating system;database;steady state;physics	DB	-27.591112895992598	3.7601055172577746	104822
307a7841caf402a6e86cd652be02439b85577a56	tutorial: sql-on-hadoop systems		Enterprises are increasingly using Apache Hadoop, more specifically HDFS, as a central repository for all their data; data coming from various sources, including operational systems, social media and the web, sensors and smart devices, as well as their applications. At the same time many enterprise data management tools (e.g. from SAP ERP and SAS to Tableau) rely on SQL and many enterprise users are familiar and comfortable with SQL. As a result, SQL processing over Hadoop data has gained significant traction over the recent years, and the number of systems that provide such capability has increased significantly. In this tutorial we use the term SQL-on-Hadoop to refer to systems that provide some level of declarative SQL(-like) processing over HDFS and noSQL data sources, using architectures that include computational or storage engines compatible with Apache Hadoop. It is important to note that there are important distinct characteristics of this emerging eco-system that are different than traditional relational warehouses. First, in the world of Hadoop and HDFS data, complex data types, such as arrays, maps, structs, as well as JSON data are more prevalent. Second, the users utilize UDFs (user-defined-functions) very widely to express their business logic, which is sometimes very awkward to express in SQL itself. Third, often times there is little control over HDFS. Files can be added or modified outside the tight control of a query engine, making statistics maintenance a challenge. These factors complicate the query optimization further in the Hadoop system. There is a wide variety of solutions, system architectures, and capabilities in this space, with varying degree of SQL support and capabilities. The purpose of this tutorial is to provide an overview of these options, discuss various different approaches, and compare them to gain insights into open research problems. In this tutorial, we will examine the SQL-on-Hadoop systems along various dimensions. One important aspect is their data storage. Some of these systems support all native Hadoop formats, and do not impose any propriety data for-	apache hadoop;array data structure;business logic;computer data storage;database engine;erp;ecosystem;json;map;mathematical optimization;method of analytic tableaux;nosql;open research;operational system;query optimization;sas;sql;sensor;smart device;social media;traction teampage	Daniel J. Abadi;Shivnath Babu;Fatma Özcan;Ippokratis Pandis	2015	PVLDB	10.14778/2824032.2824137	computer science;data mining;database;programming language;world wide web	DB	-32.77734932318136	1.8107363092860636	105016
2adb27d820338eeab6a6f6ddec0ef711ba6599c9	automated legislative drafting: generating paraphrases of legislation	legislation;knowledge based system	In this paper, we describe which roles deep structures of law play in (automatic) drafting legislation. Deep structures contain a formal description of the intended normative effects of a new regulation. We discuss mechanisms that can be used to generate different paraphrases of regulations. Since it is possible to test the paraphrases on legal knowledge based systems, we have provided two extra design steps in legislative drafting which can be supported by automated tools. Deep structures are straightforward descriptions of the normative effects of regulations. Each deep structure distinguishes desired and undesired behaviour, and has no further internal structure, such ss paragraphs or exception structures. This paper describes methods to translate a deep structure into representations of different types of codes, i.e. paraphrases. Each representation of a code hsa a different surface structure, according to the choice we make during the translation regarding: a) the initial assumptions of the regulation, i.e. modelling from desired or undesired behaviour, b) the level of abstraction, c) the viewpoint of the law, i.e. the category of norm subjects, and d) the type of deontic modalities the regulation largely uses. All paraphrases have the same ‘effects’ sa the deep structure, but with different features, and are suitable for different gords.	code;data structure;deontic logic;emoticon	Radboud Winkels;Nienke den Haan	1995		10.1145/222092.222202	computer science;artificial intelligence;knowledge-based systems;data mining;computer security	SE	-20.78917624211111	3.2187373946940236	105581
0849494a53215abcc49ff385a7bf523c00f64c54	kat-med : a task and model driven knowledge acquisition tool for medical diagnosis			knowledge acquisition;sql/med	James Fisher Amuah	1992			medical diagnosis;knowledge acquisition;data mining;data science;computer science;text mining	NLP	-31.789481382106167	-6.2167616917170925	105709
22e9b4f8b83a7b69659c36dab5e856f387d22dc6	knowledge management by reusing experience (short paper)	representacion conocimientos;systeme aide decision;ingenierie connaissances;sistema ayuda decision;systeme base connaissances;decision support system;risk assessment;knowledge representation;representation connaissances;knowledge based systems;evaluation risque;knowledge engineering	The paper presents a study of the use of past experience in emergency management with an application to Forest Fire-Fighting Management. Our objective is the sharing of experience between the emergency managers in order to support the re-planning task, regarded as the relevant task of Forest Fire-Fighting Management. Therefore it is necessary to establish a common working language to facilitate sharing. First, we present the methodology used to determine the key element of this language, which is called a unit of experience. We then introduce a method to capitalize such units. Next, we describe the utilization of units of experience to improve emergency management. The reasoning method provides several levels of support to fire managers.	knowledge management	Sabine Delaître;Sabine Moisan	2000		10.1007/3-540-39967-4_23	risk assessment;decision support system;computer science;knowledge management;artificial intelligence;knowledge engineering;operations research	DB	-31.701927159475577	-7.752148521580833	105758
2b60ca1b4b625f51d3ad88ea5428bf3df582f8a4	increasing assignment motivation using a game al tournament	assignment motivation;game al tournament	Introduction Programming assignments are usually strict, which might be demotivating. In this note we describe a rather successful attempt at giving students in a secondary algorithms course a somewhat open assignment that required them to evaluate and use a broad range of programming concepts and ideas. Othello is a zero-sum board game with simple rules but advanced strategy, and AIs for this game is a well researched topic [3, 1]. Creating a successful AI involves considerations of programming topics ranging from data structure optimization, through sorting, hashing and recursion to tree pruning and genetic programming.	algorithm;data structure;genetic programming;hash function;mathematical optimization;recursion;reversi;sorting	Øyvind Kolås;Ivar Farup	2003		10.1145/961511.961634	simulation	AI	-22.445403445213078	1.2298001604402247	105792
fb34e4c41f5a17d1eb4a335d1205b3b6cce69627	the study of fault tolerant system design using complete evolution hardware	neural network network reconfiguration evolvable hardware sensor validation;fault tolerant;neural nets;system modeling;real time;performance index;evolvable hardware;sensor fusion fault tolerant computing neural nets vlsi network topology;linear functionals;process design;chip;network topology;control system;fault tolerant system;fault tolerant computing;information processing;vlsi;process model;control system fault tolerant system complete evolution hardware process engineering process design process simulation process supervision process fault detection process fault diagnosis information processing multiple sensor coordinator based sensor validation neural network sensor failures programmable vlsi chip process modeling formalism network topology nonlinear functional relationship evolvable hardware based process model;sensor fusion;process engineering;fault tolerant systems hardware sensor phenomena and characterization sensor systems circuit testing design engineering process design process control fault detection fault diagnosis;fault detection and diagnosis;neural network	Process engineering, process design and simulation, process supervision, control and estimation, process fault detection and diagnosis rely on the effective processing of unpredictable and imprecise information. A majority of applications require cooperation of two or more independently designed, separately located, but mutually affecting subsystems. In addition to good behavior of each of the subsystems, an effective coordination is very important to achieve the desired overall performance. Such a co-ordination can permit the use of commercially designed subsystems to perform more sophisticated tasks than at present and improve the operational reliability. However, such a co-ordination is very difficult to attain mainly due to the lack of precise system models and/or dynamic parameters. In such situations, the evolvable hardware (EHW) techniques, which can achieve the sophisticated level of information processing the brain is capable of, can excel. In this paper, a new multiple-sensor coordinator based sensor validation scheme combining the techniques of evolvable hardware and neural networks, is presented. The idea of this work is to develop a system that is resistant or tolerant to sensor failures (fault tolerance) by utilizing multiple sensor inputs connected to a programmable VLSI chip. The proposed system can be views as process modeling formalism and given the appropriate network topology; is capable of characterizing non linear functional relationships. The structure of the resulting evolvable hardware based process model can be considered generic, in the sense that little prior process knowledge is required. The knowledge about the plant dynamics and mapping characteristics are implicitly stored within the network. The proposed system help in extending the range of operation of the conventional control systems with respect to sensor validation at no extra (hardware) costs. The proposed design algorithms focus on using the characteristics that evolved systems present like, for example adaptation, auto-regulation and learning. The proposed sensor was tested for its effectiveness by introducing different sensor failures such as: sensor fails as open circuit, sensor fails as short circuit, multiple sensor failure etc. on a real time plant and in each case the performance index was computed and found to be acceptable.	algorithm;artificial neural network;control system;data validation;evolvable hardware;fault detection and isolation;fault tolerance;information processing;network topology;process modeling;semantics (computer science);sensor;short circuit;simulation;systems design;very-large-scale integration	P. Subbiah;B. Ramamurthy	2005	2005 IEEE International Conference on Granular Computing	10.1109/GRC.2005.1547370	embedded system;fault tolerance;real-time computing;computer science;machine learning;artificial neural network	Robotics	-21.69936223059602	-4.250722802476054	106118
311f1b2027749875a1bc06a452850c27d90fa012	diagnostic reasoning in action	medical diagnostic imaging cognitive science psychology hazards medical control systems control systems medical treatment process control artificial intelligence decision theory;procesamiento informacion;articulo sintesis;ergonomia;punishment diagnostic reasoning mental strategies natural decision making explanation compensation repair;maintenance;article synthese;medicina;causal reasoning;diagnostico;hombre;estrategia;inference mechanisms;ergonomie;prise decision;raisonnement;medecine;strategy;control proceso;causalite;cognition;information processing;human;process control;razonamiento;mantenimiento;cognicion;medicine;reasoning;traitement information;toma decision;review;diagnosis;strategie;ergonomics;commande processus;mental model;homme;causality;causalidad;diagnostic	The task of diagnosis is a very important topic in many different contexts. In highly complex technical installations involving high hazards, such as process plants, diagnosis is a crucial part of disturbance control; in technical maintenance, diagnosis is necessary to locate the root cause of system failures; and in medicine, diagnosis is the basis for any patient treatment. The paper presents a discussion of the basic nature of causal reasoning as applied for diagnosis and the mental strategies applied when diagnosis is viewed as an integrated part of “natural decision making” for interaction with the environment. A typology is suggested to characterize diagnosis in different domains such as process control, maintenance and medicine. In addition, an attempt is made to distinguish between the features of diagnosis depending on the ultimate aim, whether it is explanation, compensation, repair, or punishment and the difference in the context of the task, “the causal field,” related to the mental model involved in the different cases is outlined.	biological anthropology;causal system;causality;mental model	Jens Rasmussen	1993	IEEE Trans. Systems, Man, and Cybernetics	10.1109/21.247883	cognition;causal reasoning;causality;information processing;strategy;artificial intelligence;process control;reason	AI	-25.31317085241684	-5.89022017931836	106151
17c65880b4b6eb32b35a0a8c16c7c7b8d013f403	ibidas: querying flexible data structures to explore heterogeneous bioinformatics data		Nowadays, bioinformatics requires the handling of large and diverse datasets. Analyzing this data demands often significant custom scripting, as reuse of code is limited due to differences in input/output formats between both data sources and algorithms. This recurring need to write data-handling code significantly hinders fast data exploration.	bioinformatics	Marc Hulsman;Jan Bot;Arjen P. de Vries;Marcel J. T. Reinders	2013		10.1007/978-3-642-39437-9_2	bioinformatics;data mining;database	Logic	-33.13278531585837	1.0545897811899756	106167
8a4ebf123b17e210d7e1b2d4eb3004233b9d8a32	efficient aggregation algorithms on very large compressed data warehouses	data compression;efficient algorithm;olap;aggregation;decision procedure;data warehouse;on line analytical processing	Multidimensional aggregation is a dominant operation on data warehouses for on-line analytical processing (OLAP). Many efficient algorithms to compute multidimensional aggregation on relational database based data warehouses have been developed. However, to our knowledge, there is nothing to date in the literature about aggregation algorithms on multidimensional data warehouses that store datasets in multidimensional arrays rather than in tables. This paper presents a set of multidimensional aggregation algorithms on very large and compressed multidimensional data warehouses. These algorithms operate directly on compressed datasets in multidimensional data warehouses without the need to first decompress them. They are applicable to a variety of data compression methods. The algorithms have different performance behavior as a function of dataset parameters, sizes of outputs and main memory availability. The algorithms are described and analyzed with respect to the I/O and CPU costs. A decision procedure to select the most efficient algorithm, given an aggregation request, is also proposed. The analytical and experimental results show that the algorithms are more efficient than the traditional aggregation algorithms.	algorithm;central processing unit;computer data storage;cube;data compression;decision problem;direct manipulation interface;input/output;online analytical processing;online and offline;relational database;social network aggregation	Jianzhong Li;Yingshu Li;Jaideep Srivastava	2000	Journal of Computer Science and Technology	10.1007/BF02948809	data compression;online analytical processing;computer science;data science;data warehouse;data mining;database	DB	-28.939650870474093	2.337371786883226	106319
a6757909ba9e0024be7ce607cad9283f2e66b98b	the use of normal multiplication tables for information storage and retrieval	information retrieval;space economy;inverted files;normal multiplication table;rapid retrieval;multiattribute retrieval;multilist file;information system;information storage and retrieval;queries	This paper describes a method for the organization and retrieval of attribute based information systems, using the normal multiplication table as a directory for the information system. Algorithms for the organization and retrieval of information are described. This method is particularly suitable for queries requesting a group of information items, all of which possess a particular set of attributes (and possibly some other attributes as well). Several examples are given; the results with respect to the number of disk accesses and disk space are compared to other common approaches. Algorithms evaluating the appropriateness of the above approach to a given information system are described. For a certain class of information systems, the normal multiplication table method yields far more rapid retrieval with a more economical space requirement than conventional systems. Moreover this method incorporates an improved modification of the inverted file technique.	algorithm;directory (computing);disk space;information system;inverted index;norm (social)	Dalia Motzkin	1979	Commun. ACM	10.1145/359080.359091	document retrieval;standard boolean model;computer science;theoretical computer science;database;vector space model;information retrieval;information system	DB	-28.005286956726252	3.206835056522428	106464
5b1979a4fb906bf5674cfda9101b5684f0814618	a reusable and single-interactive model for secure approximate k-nearest neighbor query in cloud		In sensitive database applications (e.g., time series, scientific databases, and biometric), where database is encrypted and outsourced to a public cloud, secure approximate k-nearest neighbor (SANN) query is a fundamental research topic, aiming at retrieving high-dimensional objects that are similar to a given query from encrypted database. To process such queries without ever decrypting the data in cloud is still a challenging task. Existing works encounter various inherent limitations, such as query distinguishability, low-level efficiency and non-recoverability. All of them lead to either fragile security or low accuracy. Hence, the majority of existing works in this field are impractical for industrial applications.In this work, we present a novel model to remove the above limitations. Specifically, a reusable and single-interactive SANN paradigm is proposed in Euclidean high-dimensional space. Firstly, we present a secure variation of B+-tree (i.e., Bc-tree) to quickly locate high-dimensional candidates in cloud by leveraging on comparable encryption. Based on that, an arbitrary query requestor acquires approximate k-nearest neighbors by linearly scanning over candidates. Meanwhile, two refinements, multi-index strategy and boosting refinement strategy, are proposed to further improve the accuracy of search result and overcome the high-dependency of bandwidth, respectively. In the end, through extensive evaluations on four data sets, the proposed mechanisms are demonstrated to be superior in the tradeoff between accuracy and security.	approximation algorithm;k-nearest neighbors algorithm;nearest neighbor search	Yanguo Peng;Jiangtao Cui;Hui Li;Jianfeng Ma	2017	Inf. Sci.	10.1016/j.ins.2016.07.069	computer science;machine learning;data mining;database;world wide web	DB	-25.172777869304724	0.12883457292499606	106646
b166af874022a7db2f72e75d7d6351152afd5920	the general motors variation-reduction adviser: evolution of a cbr system	ontologie;industrie automobile;raisonnement base sur cas;razonamiento fundado sobre caso;user feedback;intelligence artificielle;assembly;industria automovil;artificial intelligence;ontologia;montage;inteligencia artificial;case based reasoning;general motors;montaje;ontology;automobile industry	"""The GM Variation-Reduction Adviser (VRA) was originally conceived and prototyped as a CBR system. Feedback from the users led to a variety of changes in the system. It is emerging now as an application destined for all GM assembly plants. This is a fine """"success story."""" However, the VRA has lost so much of its CBR character that it might be best characterized as a """"CBR inspired"""" system rather than a CBR system. In this paper, we describe the original concept and the user feedback that guided its evolution into its current form. Every """"real application"""" has interesting stories about """"what worked"""" and """"what didn't."""" Here, we share some of these stories about our project."""		Alexander P. Morgan;John A. Cafeo;Diane I. Gibbons;Ronald M. Lesperance;Gülcin H. Sengir;Andrea M. Simon	2003		10.1007/3-540-45006-8_25	case-based reasoning;simulation;computer science;artificial intelligence;ontology;assembly;computer security;algorithm	Robotics	-26.426193890577128	-6.048430510450547	106912
156f9b5788761b83febd8620bed132e6c09298ff	design of knowledge-based systems in environmental engineering	knowledge based system;fuzzy set;project manager;information technology;dynamic model;decision support system;design knowledge;environmental engineering;knowledge base	The presentation includes description of the concept, its formalization, the use of the experimental data, as well as methods of modeling and designing knowledge-based decision support systems based on self-adjusting and dynamic models that can be utilized in developing of knowledge-based information technology products for environmental engineering. The proposed models have been placed within a practicable setting of fuzzy sets philosophy and a feedback structure of project management.	knowledge-based systems	Zdzislaw Kowalczuk;Cezary Orlowski	2004	Cybernetics and Systems	10.1080/01969720490451869	knowledge base;decision support system;knowledge integration;intelligent decision support system;computer science;systems engineering;knowledge management;artificial intelligence;knowledge-based systems;knowledge engineering;management science;fuzzy set;knowledge extraction;information technology;domain knowledge	AI	-31.266804602535682	-6.746907444281989	107377
b0189a6531bf6e3f0d3011f4e6cf5dc6b33aace8	fast matching of twig patterns	institute for integrated and intelligent systems;faculty of science environment engineering and technology;data processing;database management;journal article;pattern matching;080604;data structure	Twig pattern matching plays a crucial role in xml data processing. Existing twig pattern matching algorithms can be classified into two-phase algorithms and one-phase algorithms. While the two-phase algorithms (e.g., TwigStack) suffer from expensive merging cost, the onephase algorithms (e.g., TwigList, Twig2Stack, HolisticTwigStack) either lack efficient filtering of useless elements, or use over-complicated data structures. In this paper, we present two novel one-phase holistic twig matching algorithms, TwigMix and TwigFast, which combine the efficient selection of useful elements (introduced in TwigStack) with the simple lists for storing final solutions (introduced in TwigList). TwigMix simply introduces the element selection function of TwigStack into TwigList to avoid manipulation of useless elements in the stack and lists. TwigFast further improves this by introducing some pointers in the lists to completely avoid the use of stacks. Our experiments show TwigMix significantly and consistently outperforms TwigList and HolisticTwigStack (up to several times faster), and TwigFast is up to two times faster than TwigMix.	algorithm;benchmark (computing);bottom-up parsing;data structure;experiment;holism;pattern matching;schmidt decomposition;top-down and bottom-up design;twig;two-phase commit protocol;two-phase locking;wide area augmentation system;xml	Jiang Li;Junhu Wang	2008		10.1007/978-3-540-85654-2_45	data structure;data processing;computer science;pattern matching;data mining;database;programming language;algorithm	DB	-30.188984366510457	4.170678085003056	107466
8a28815c42ef99fa9c40221037312ca48d2287e9	distance constraint arrays: a model for reasoning on intervals with qualitative and quantitative distances	modelizacion;representacion conocimientos;linguistica matematica;spatial reasoning;raisonnement qualitatif;modelisation;raisonnement temporel;raisonnement spatial;linguistique mathematique;razonamiento calitativo;computational linguistics;qualitative reasoning;knowledge representation;representation connaissances;modeling;temporal reasoning;partial order	We outline a model of one-dimensional reasoning on interval relations with quantitative and qualitative distances. At the core of this model lie constraints on interval boundaries, partial ordering and subsumption relations on interval relations and interval boundary constraints, and the transformation of interval relations to interval boundary constraints and vice versa. By way of subsumption and approximation criteria on distance constraint arrays, a signiicant level of conceptual abstraction from the underlying interval boundary constraints is realized when new relations are inferred. Abstract. We outline a model of one-dimensional reasoning on interval relations with quantitative and qualitative distances. At the core of this model lie constraints on interval boundaries, partial ordering and subsumption relations on interval relations and interval boundary constraints , and the transformation of interval relations to interval boundary constraints and vice versa. By way of subsumption and approximation criteria on distance constraint arrays, a signiicant level of conceptual abstraction from the underlying interval boundary constraints is realized when new relations are inferred.	approximation;subsumption architecture	Steffen Staab;Udo Hahn	1998		10.1007/3-540-64575-6_62	partially ordered set;knowledge representation and reasoning;systems modeling;qualitative reasoning;computer science;artificial intelligence;computational linguistics;machine learning;mathematics;spatial intelligence;algorithm	AI	-21.742958581079954	-0.5654764329449069	107482
e96bb99fef8e71ab63b2e7c48c0b521889abffef	the role of computational intelligence in data mining	computational intelligence data mining competitive intelligence machine learning artificial intelligence fuzzy logic learning systems speech recognition usa councils information systems;computational adaptation computational intelligence data mining machine learning randomization soft computing type ii fuzzy logic;computational intelligence;soft computing;data mining;fuzzy logic;machine learning;random operator;fuzzy logic data mining learning artificial intelligence;learning artificial intelligence;domain specificity	In this paper, we explore the interdependent roles of computational intelligence, data mining, and machine learning. We explain how these three seemingly independent AI specialties are inextricably linked through the science of randomization and reuse. Computational intelligence is shown to imply soft computing, whose bounds are soft or fuzzy only because they need be adoptively formed (e.g., type II fuzzy logic). Data mining implies the compression of data (including anomalous data) to its simplest form. Such randomization operations are inherently incomplete and thus are necessarily domain-specific or adaptive as a consequence. Finally, machine learning is the study of computational adaptation. For example, a machine that plays master-level chess is not necessarily computationally intelligent - unless it is capable of adaptive improvement in its play. This paper will attempt to make it clear that intelligence and the capability for learning are in fact synonyms for an AI.	artificial intelligence;artificial neural network;black hole;computation;computational intelligence;data mining;fuzzy logic;information revolution;intelligent agent;interdependence;machine learning;randomized algorithm;soft computing;synergy;world wide web	Stuart Harvey Rubin;Marion G. Ceruti;Wei Dai	2005	IRI -2005 IEEE International Conference on Information Reuse and Integration, Conf, 2005.	10.1109/IRI-05.2005.1506472	fuzzy logic;fuzzy electronics;reactive search optimization;artificial architecture;marketing and artificial intelligence;computer science;artificial intelligence;neuro-fuzzy;machine learning;computational intelligence;data mining;artificial intelligence system;symbolic artificial intelligence;soft computing;computational learning theory;ai-complete;artificial intelligence, situated approach;hyper-heuristic;intelligent control	AI	-27.505103648786623	-9.247924668576635	107760
69a98cbde818bb0d04561b1ab8c7dd4fb1b9efa9	sense-think-act framework for intelligent building energy management	ucl;discovery;theses;conference proceedings;digital web resources;ucl discovery;open access;ucl library;book chapters;open access repository;ucl research	The realization of smart and energetically efficient buildings is contingent upon the successful implementation of two tasks that occur on distinct phases of the building life cycle: in the design and subsequent retrofitting phases, the selection and implementation of an effective energy concept, and, during the operation phase, the actuation of energy systems to ensure parsimonious energy use while retaining acceptable end-user thermal comfort. Operational efficiencies are achieved through the use of Building Energy Management Systems tasked to deliver core Sense, Think, Act (STA) functionalities: Sense, using sensing modalities installed in the building; Think, utilizing, typically a rule-based decision system; and Act, by sending actuation commands to controllable building elements. Providing the intelligence in this STA process can be a formidable task due to the complex interplay of many systems and occurrence of disturbances. In this article, an architectural and algorithmic framework is presented to provide streamlined implementation of this process. Important ingredients in this framework are: (S) a data access component capable of collecting and aggregating information from a number of heterogeneous sources (sensors, weather stations, weather forecasts); (T) a model-based optimization methodology to generate intelligent operational decisions; and (A) an assessment and actuation ∗To whom correspondence should be addressed. E-mail: dimitrios. rovas@ibp.fraunhofer.de. component. An illustrative application of the proposed methodology in an office building is provided.	contingency (philosophy);data access;decision support system;logic programming;management system;mathematical optimization;occam's razor;operational amplifier;sense;sensor	K. I. Katsigarakis;Georgios Kontes;G. I. Giannakis;D. V. Rovas	2016	Comp.-Aided Civil and Infrastruct. Engineering	10.1111/mice.12173	computer science;data science;world wide web	Robotics	-31.016017323103696	-3.4881341200379055	108012
7df45c33162e212818052ff5b072d94095c8ed73	a modular fuzzy expert system architecture for data and event streams processing		In many decision making scenarios, fuzzy expert systems have been useful to deduce a more conceptual knowledge from data. With the emergence of the Internet of Things and the growing presence of cloud-based architectures, it is necessary to improve fuzzy expert systems to support higher level operators, large rule bases and an abundant flow of inputs.	expert system;systems architecture	Jean-Philippe Poli;Laurence Boudet	2016		10.1007/978-3-319-40581-0_58	real-time computing;complex event processing;data mining;database	DB	-32.40080372187778	-7.2949150107656004	108231
581107db3df90e220e4c11b3dbc7abc30901ddc9	determination of possible minimal conflict sets using constraint databases technology and clustering	query language;model based reasoning;analyse amas;entrada salida;raisonnement base sur modele;observable;classification non supervisee;model based diagnosis;grobner basis;diagnostico;base connaissance;intelligence artificielle;base grobner;lenguaje interrogacion;base donnee avec contrainte;input output;constrained database;captador medida;measurement sensor;cluster analysis;capteur mesure;clasificacion no supervisada;unsupervised classification;artificial intelligence;base conocimiento;analisis cluster;langage interrogation;base dato forzada;inteligencia artificial;modele amas;cluster model;diagnosis;entree sortie;knowledge base;diagnostic	Model-based Diagnosis allows the identification of the parts which fail in a system. The models are based on the knowledge of the system to diagnose, and can be represented by constraints associated to components. Inputs and outputs of components are represented as variables of those constraints, and they can be observable and non-observable depending on the situation of sensors. In order to obtain the minimal diagnosis in a system, an important issue is to find out the possible minimal conflicts in an efficient way. In this work, we propose a new approach to automate and to improve the determination of possible minimal conflict sets. This approach has two phases. In the first phase, we determine components clusters in the system in order to reduce drastically the number of contexts to consider. In the second phase, we construct a reduced context network with the possible minimal conflicts. In this phase we use Gröbner bases reduction. A novel logical architecture of Constraint Databases is used to store the model, the components clusters and possible minimal conflict sets. The necessary information in each phase is obtained by using a standard query language.		María Teresa Gómez López;Rafael Ceballos;Rafael M. Gasca;Sergio Pozo Hidalgo	2004		10.1007/978-3-540-30498-2_94	input/output;knowledge base;observable;computer science;gröbner basis;artificial intelligence;model-based reasoning;data mining;mathematics;cluster analysis;algorithm;query language	AI	-21.62131117338951	-1.149472293429843	108524
2d0a961491a9678e82392efe2fa3e2778b146fac	a quantitative analysis of the robustness of knowledge-based systems through degradation studies	knowledge based system;dato que falta;informacion incompleta;analisis cuantitativo;base connaissance;systeme base connaissances;donnee manquante;feasibility;incomplete information;article letter to editor;analyse quantitative;robustesse;information incomplete;classification system;controle qualite;quantitative analysis;base conocimiento;robustness;validation;missing data;quality measures;quality control;practicabilidad;faisabilite;knowledge based systems;robust behaviour;control calidad;robustez;knowledge base	The overall aim of this paper is to provide a general setting for quantitative quality measures of knowledge-based system behaviour that is widely applicable to many knowledge-based systems. We propose a general approach that we call degradation studies: an analysis of how system output changes as a function of degrading system input, such as incomplete or incorrect data or knowledge. To show the feasibility of our approach, we have applied it in a case study. We have taken a large and realistic vegetation-classification system, and have analysed its behaviour under various varieties of incomplete and incorrect input. This case study shows that degradation studies can reveal interesting and surprising properties of the system under study.	elegant degradation;knowledge-based systems	Perry Groot;Annette ten Teije;Frank van Harmelen	2003	Knowledge and Information Systems	10.1007/s10115-003-0140-7	quality control;missing data;computer science;quantitative analysis;artificial intelligence;knowledge-based systems;data mining;operations research;complete information;robustness	SE	-20.876339668702208	-2.4237321786770893	108617
1d6853470ff153b6e2941c2ccda57f201380ef54	a knowledge-based system for course scheduling	sistema experto;knowledge based system;planificacion integral;prolog;integrated planning;base connaissance;higher education;intelligence artificielle;horario;satisfiability;resolucion problema;ensenanza universitaria;schedule;artificial intelligence;base conocimiento;inteligencia artificial;systeme expert;ordonnancement;industrial engineering;horaire;problem solving;resolution probleme;enseignement universitaire;knowledge base;expert system	Abstract A knowledge-based scheduling system has been developed for the domain of university class scheduling. The problem addressed is how to schedule courses during the various time periods throughout the day. The class schedule must satisfy a variety of appropriate constraints. The system, written in Prolog, resolves conflicting assignments through backtracking. The inefficiency of Prolog's backtracking feature, with respect to this application, is partly circumvented by the use of a dynamic circular array. The system is now being used to help schedule industrial engineering classes at the Pennsylvania State University.		Udaya Gunasena;Soundar R. T. Kumara;Allen L. Soyster	1989	Applied Artificial Intelligence	10.1080/08839518908949938	knowledge base;simulation;computer science;artificial intelligence;integrated business planning;higher education;prolog;schedule;expert system;algorithm;satisfiability	AI	-23.048011521188307	-5.181740185401202	108892
6b7f1c6ea70bedb5374fe69b1806eb2713b5f3b4	benchmarking elastic query processing on big data		Existing analytical query benchmarks, such as TPC-H, often assess database system performance on on-premises hardware installations. On the other hand, some benchmarks for cloud-based analytics deal with flexible infrastructure, but often focus on simpler queries and semi-structured data. With our benchmark draft we attempt to bridge the gap by challenging analytical platforms to answer complex queries on structured business data while leveraging the elastic infrastructure of the cloud to satisfy performance requirements.	big data	Dimitri Vorona;Florian Funke;Alfons Kemper;Thomas Neumann	2014		10.1007/978-3-319-20233-4_5	query optimization;query expansion;data mining;database;information retrieval	DB	-33.63165741112941	0.9515247596633234	108978
a51207af2c1456b107983b72590f6f41aea9d692	sherlock: hypothetical reasoning in an expert system shell	expert system		expert system	Marie-Odile Cordier	1988			model-based reasoning;machine learning;artificial intelligence;data mining;computer science;expert system;legal expert system	AI	-27.636950190347072	-7.691771250867291	109038
1404afcba6e15a625273153f30a9ceefcf902d54	autonomous planning framework for distributed multiagent robotic systems	technical science	In this paper a creative action planning algorithm (CAPA) is presented for solving multiagent planning problems and task allocation. The distributed multiagent system taken in consideration is a system of m autonomous agents. Agents workspace contains simplified blocks which form different space structures. By employing the planning algorithm and through interaction agents allocate tasks which they execute in order to assemble the required space structure. The planning algorithm is based on an inductive engine. From a given set of objects which can differ from the initial set agents need to reach a solution in the anticipated search space. A multiagent framework for autonomous planning is developed and implemented on an actual robotic system consisting of three 6 DOF industrial robots.	agent-based model;algorithm;automated planning and scheduling;autonomous robot;industrial robot;multi-agent system;workspace	Marko Švaco;Bojan Sekoranja;Bojan Jerbic	2011		10.1007/978-3-642-19170-1_16	computer science;knowledge management;artificial intelligence	AI	-19.540986713358397	-8.45495502479773	109046
231d56dfa563f569fb119eae1f6b1d2837284455	towards a possibility-theoretic approach to uncertainty in medical data interpretation for text generation	medical decision support system;data interpretation;data uncertainty;real world application;neonatal intensive care unit;possibility theory;causal relation;text generation	Many real-world applications that need to reason about events obtained from raw data must deal with the problem of temporal uncertainty, w hich arises due to error or inaccuracy in data. Uncertainty also compromises reasoning where relationships between events need to be inferred. This paper d iscusses an approach to dealing with uncertainty in temporal and causal relations us i g Possibility Theory, focusing on a family of medical decision support sys tem that aim to generate textual summaries from raw patient data in a Neonatal Inten sive Care Unit. We describe a theoretical framework within which to capture temp oral uncertainty, and discuss the way it informs the process of text generatio n.	causal filter;decision support system;experiment;institute for operations research and the management sciences;list comprehension;modal logic;natural language generation;pattern recognition;possibility theory;precision and recall;semantics (computer science);test and evaluation master plan	François Portet;Albert Gatt	2009		10.1007/978-3-642-11808-1_13	possibility theory;uncertainty analysis;computer science;artificial intelligence;data science;machine learning;data mining;data analysis;statistics	ML	-19.4252008550086	-0.42952455612231527	109193
7daa42ba5176a8245b2e7bcc94670d7d7e41d910	where's downtown? behavioral methods for determining referents of vague spatial queries	geographic information system;geographic information;behavioral science;spatial relation;information system	Humans think and talk about regions and spatial relations imprecisely, in terms of vague concepts that are fuzzy or probabilistic (e.g., downtown, near). The functionality of geographic information systems will be increased if they can interpret vague queries. We discuss traditional and newer approaches to defining and modeling spatial queries. Most of the research on vague concepts in information systems has focussed on mathematical and computational implementation. To complement this, we discuss behavioral-science methods for determining the referents of vague spatial terms, particularly vague regions. We present a study of the empirical determination of downtown Santa Barbara. We conclude with a discussion of prospects and problems for integrating vague concepts into geographic information systems.	fuzzy logic;geographic information system;humans;spatial query;vagueness	Daniel R. Montello;Michael F. Goodchild;Jonathon Gottsegen;Peter Fohl	2003	Spatial Cognition & Computation	10.1080/13875868.2003.9683761	spatial relation;artificial intelligence;data mining;mathematics;geographic information system;management science;information system;remote sensing	DB	-21.413297223126534	-1.2239460182549802	109300
a24fbfa6593df0c73fe0aaa2bd1b240500a22dd8	i-ssa: interaction-situated semantic alignment	agent communication;semantic heterogeneity;semantic relations	We tackle semantic heterogeneity in multi-agent communication by looking at semantics related to interaction in order to avoid dependency on a priori semantic agreements. Our underlying claim is that semantic alignment is often relative to the particular interaction in which agents are engaged, and that in such cases the interaction state should be taken into account and brought into the alignment mechanism. We provide a formal foundation along with an alignment mechanism and protocol based on it.	entity;interaction;international ergonomics association;multi-agent system;semantic heterogeneity;situated;static single assignment form;vocabulary	Manuel Atencia;Marco Schorlemmer	2008		10.1007/978-3-540-88871-0_31	semantic interoperability;semantic similarity;semantic integration;semantic search;semantic grid;computer science;knowledge management;data mining;semantic compression;semantic property;communication	AI	-22.38670958526991	-9.877651273417122	109395
030bc70bd9999d169718e694bb31c0a253046e1c	an alternative approach in approximate reasoning using fuzzy production rules in expert systems	degree of dissimilarity;modus ponens generalise;sistema experto;incertidumbre;uncertainty;common sense reasoning;logique floue;intelligence artificielle;logica borrosa;raisonnement;implication strength;fuzzy logic;inferencia;razonamiento;generalised modus ponens;artificial intelligence;incertitude;inteligencia artificial;systeme expert;reasoning;properties of the inference;inference;fuzzy weighted mean;expert system	This paper outlines an alternative approach using fuzzy production rules to handle approximate reasoning in expert systems without using the Generalised Modus Ponens (GMP) model by Zadeh. The proposed approach provides a method whereby a collection of rules is used to specify the properties of the inference as required by the expert. An example on the medical diagnosis of Acute Rheumatic Fever is used for illustration.	expert system	M. Halim;K. M. Ho;A. Liu	1991	IJPRAI	10.1142/S0218001491000272	fuzzy logic;legal expert system;commonsense reasoning;uncertainty;computer science;artificial intelligence;model-based reasoning;machine learning;data mining;mathematics;expert system;reason	AI	-20.637561588300578	-1.7056367655910107	109870
f27429ebc135209ef8dc002170597a3c218c7f82	the user centred knowledge model - t-uck	decision models;rule based systems;information science;human aspects of ict;systemvetenskap;user centred modelling;knowledge modelling;computer systems sciences;data och systemvetenskap;visualisation;graphic modelling;mansklig interaktion med ikt;graphical representation;system development;expert knowledge;knowledge modeling;knowledge engineering	In knowledge engineering, modelling knowledge is the process of structuring knowledge before implementation. A crucial part of system development depends on the acquiring and structuring, since the quality of system’s contents is of decisive importance for making good decisions. Models are needed to assure that all the required knowledge is present. However, the current models tend to be large and this makes it hard to get a grip on the knowledge presented by the model. Also, many models are difficult to use and the users have to be experts on the models before using them. To avoid these problems, we introduce the User-Centred Knowledge Model (t-UCK) for modelling knowledge. The model supports different users, i.e., domain experts, knowledge engineers and end-users, to model, implement, test, consult, and educate through the use of graphic representation and visualisation.	knowledge engineer;knowledge engineering;knowledge management;knowledge representation and reasoning;one-to-one (data model)	Anne Håkansson	2008		10.1007/978-3-540-85567-5_97	computer science;systems engineering;knowledge management;body of knowledge;knowledge-based systems;data mining;procedural knowledge;knowledge extraction;personal knowledge management;knowledge value chain;domain knowledge	AI	-31.464745090606037	-6.537550187851764	110122
42c66631cfc94d2ef19322d2a96e22cde26a1cdf	storage efficiency of lob structures for free rdbmss on example of postgresql and oracle platforms		The article is a study upon storage efficiency of LOB structures for systems consisting of PostgreSQL or Oracle relational database management systems. Despite the fact that recently several NoSQL DBMS concepts were born (in particular document-oriented or key-value), relational databases still do not lose their popularity. The main content is focused on sparse data such as XML or base64 encoded data that stored in raw form consume significant volume of data storage. Studies cover both performance (data volume stored per unit of time) and savings (ability to save data storage) aspects.	postgresql;relational database management system;storage efficiency	Lukasz Wycislik	2017		10.1007/978-3-319-58274-0_18	xml;relational database management system;storage efficiency;nosql;cloud computing;relational database;database;oracle;computer data storage;computer science	Logic	-29.722452133656088	1.8354583539256568	110130
661ff1ee9379352c41d7dbf465e0df6b5f23632a	gis as a tool in emergency management process	emergency management	— This paper shows how GIS could be used to support decision making in emergency risk management. Paper describes GinisEmergency tool, which is based on existing semantic interoperability platform called GeoNis. The proposed prototype is intended to function as an intelligent, computer-based consultant in assisting diagnostic and decision making processes in potentially hazardous situations. Intelligent integration of heterogeneous data sources based on semantics (represented by ontologies) is important part of this research.	gis applications;geographic information system;ontology (information science);prototype;redundancy (engineering);risk management;semantic integration;semantic interoperability;web service	Leonid Stoimenov;Aleksandar Milosavljevic;Aleksandar S. Stanimirovic	2007			computer science;systems engineering;engineering;knowledge management;data mining;emergency management	AI	-33.36710940411518	-8.96256191294218	110369
c719502b8cecd016cbdae816da25cc2bd20e388e	a pomdp design framework for decision making in assistive robots		This paper proposes a theoretical framework that determines the high-level cognitive functions for multipurpose assistive service robots, required to autonomously complete their tasks. It encompasses a probabilistic POMDP based decision-making strategy that provides constant situation awareness about the human and the environment by associating the robot awareness about the user with specific clusters of robotic actions. To achieve this, a method for designing POMDP models is presented herein ample to define decision making policies suitable to resolve assistive tasks through a series of robotic actions. The proposed POMDP design methodology compensates the partial and noisy sensor input acquired from the robot sensors by foreseen mitigation strategies on the robot’s decisions when a software component fails. The theoretical work presented herein is assessed over well defined robotic tasks and proved capable to operate in realistic assistive robotic scenarios.	partially observable markov decision process;robot	Ioannis Kostavelis;Dimitrios Giakoumis;Sotiris Malassiotis;Dimitrios Tzovaras	2017		10.1007/978-3-319-58071-5_35	component-based software engineering;robot;computer science;simulation;probabilistic logic;design methods;partially observable markov decision process;robotic sensing;situation awareness;cognition	Robotics	-19.527020573349155	-8.951980706309127	110380
4df5628f08663a559df60c3e492b7ad41a9672ea	an efficient workload-based data layout scheme for multidimensional data	efficient algorithm;multidimensional data;data model;data clustering;affinity graphs;data warehousing;data access;load balance;data layout;materialized views;graph model;large data	Abstract   Physical data layout is a crucial factor in the performance of queries and updates in large data warehouses. Data layout enhances and complements other performance features such as materialized views and dynamic caching of aggregated results. Prior work has identified that the multidimensional nature of large data warehouses imposes natural restrictions on the query workload. A method based on a “uniform” query class approach has been proposed for data clustering and shown to be optimal. However, we believe that realistic query workloads will exhibit data access skew. For instance, if time is a dimension in the data model, then more queries are likely to focus on the most recent time interval. The query class approach does not adequately model the possibility of multidimensional data access skew. We propose the  affinity graph  model for capturing workload characteristics in the presence of access skew and describe an efficient algorithm for physical data layout. Our proposed algorithm considers declustering and load balancing issues which are inherent to the multidisk data layout problem. We demonstrate the validity of this approach experimentally.		Kazi A. Zaman;Sriram Padmanabhan	2001	Data Knowl. Eng.	10.1016/S0169-023X(01)00043-X	materialized view;data access;data model;computer science;load balancing;theoretical computer science;data warehouse;data mining;database;cluster analysis	DB	-27.71437496150917	3.0362445081409635	110441
4a792dcc8ca9e9bd69e63ff3100aaee9c5e66759	a decision-support system to improve damage survivability of submarine	adquisicion del conocimiento;informacion electronica;automatic system;systeme aide decision;base connaissance;acquisition connaissances;sistema ayuda decision;prise decision;domain knowledge;information electronique;support system;decision support system;fault tolerant system;sistema automatico;knowledge acquisition;sous marin;submarine;sistema tolerando faltas;systeme automatique;electronic information;base conocimiento;systeme tolerant les pannes;submarino;toma decision;knowledge base	Any small leakage in the submarines can lead to serious consecutive damages since it operates under high water pressure. Such leakage including damages on pipe and hull eventually incur human casualties and loss of expensive equipments as well as the loss of combat capabilities. In such cases, a decision-making system is necessary to respond immediately to the damages in order to maintain the safety or the survival of the submarine. So far, human decision has been the most important one based on personal experience, existing data, and any electronic information available. However, it is well recognized that such decisions may not be enough in certain emergency situations. The system that depends on only human experience may cause serious mistakes in devastating and scared situations. So it is necessary to have an automatic system that can generate responses and give advice the operator how to make decisions to maintain the survivability of the damaged vessel. In this paper, a knowledge-based decision support system for submarine safety is developed. The domain knowledge is acquired from the submarine design documents, design expertise, and interviews with operator. The knowledge consists of the responses regarding damage on pressure hull and piping system. Expert Elements are deduced to obtain the decision from the knowledge base, and for instance, the system makes recommendations on how the damages on hull and pipes decision and whether to stay in the sea or to blow. It is confirmed that developed system is well simulated to the real situation throughout sample applications.	decision support system;expert system;knowledge base;pipeline (unix);simulation;spectral leakage	Dongkon Lee;Jae Yong Lee;K. H. Lee	2002		10.1007/3-540-48035-8_40	knowledge base;fault tolerance;decision support system;computer science;artificial intelligence;operations research;computer security;domain knowledge;submarine	AI	-24.271738497054546	-5.482338103429633	110793
41fc221e5c9bdf9550e47e33ae4d9c2c214ce686	using logarithmic code-expansion to speedup index access and maintenance	index structure;space time;data storage;complex data;binary search tree;indexation;access method;binary tree	This paper showcases real life examples in making enterprise SAS ® systems run faster for organizations creating thousands of reports. By diligently testing a series of advanced SAS programming techniques such as macros, and indexing against gigabyte level datasets, programs were optimized to run thousands of times faster. In the era of information explosion, the examples presented in this paper will help business users gain quicker access to processed data and reports needed for analytical decision making. INTRODUCTION Performance is usually measured in units of time. Time savings often translate into cost savings and productivity improvements for an organization. Time includes not only CPU time and real time, which can be measured from the log file, but also programmer’s time, which may be much more expensive. In the real world, time savings may sometimes be obtained at the cost of hardware upgrade such as memory or disk space expansion. Using the technology in a correct way is the key for success. Every technology has limitations or weaknesses. This paper briefly discusses the art of balancing the contradictory factors to derive optimal performance. BE SMART, USE MACRO SAS Macro is very useful for repetitive work. The benefits of using a macro include reducing program length, easy maintenance of changes and system automation. How can we use a macro to help us streamline reports production? MACRO PROGRAMMING TAKES ADVANTAGE OF TODAY’S COMPUTING POWER – PARALELL PROCESSING If your enterprise invested millions of dollars buying a powerful multi-CPU server, you certainly want big return on investment. If the CPUs are always close to 90% idle, you are probably not very happy. Suppose you need to produce 10,000 reports for 10 different teams every week or every month. You can write a simple program to do it: Proc sql; Select id into :id1 :id99999 From idfile; /* idfile includes one ID per report */ Quit; %do i=1 to &sqlobs; data outsource; set insource; if id=”&&id&I” then do; file “file&i..txt”; put var1 var2 var3; end; run; %end; The problem with the program above is that it may take too long to complete especially when there is a large insource file. To improve the performance without changing the hardware, the recommendation here is to break up your program into groups, teams, or silos. Then you can run the broken up programs simultaneously to take advantage of multiple CPUs. 1 Applications NESUG 2006	a new kind of science;adobe streamline;central processing unit;computer hardware;computer memory;disk space;gigabyte;information explosion;outsourcing;programmer;real life;real-time computing;sas;smart;sql;server (computing);speedup	Martin L. Kersten	1989		10.1007/3-540-51295-0_131	random binary tree;b-tree;optimal binary search tree;cartesian tree;segment tree;red–black tree;tree rotation;computer science;treap;theoretical computer science;order statistic tree;self-balancing binary search tree;k-d tree;data mining;interval tree;database;fractal tree index;ternary search tree;threaded binary tree;tree traversal	PL	-31.172780161442063	-2.122447311649423	110849
2c27881ab1ef801964b9ea79d5fa3773a9010a82	vhr satellite image time series analysis using expert knowledge modeling and user assistance	semantics;knowledge based systems semantics satellites monitoring remote sensing time series analysis satellite broadcasting;satellite broadcasting;monitoring;time series analysis;remote sensing;satellites;user assistance spatio temporal analysis vhr sits knowledge modeling semantic sub graph;stis analysis vhr satellite image time series analysis expert knowledge modeling user assistance land cover region monitoring remote sensing data image interpretation evolution graph representation sits subgraph;time series environmental monitoring geophysics geophysical image processing land cover remote sensing;knowledge based systems	In this paper, we address land cover regions monitoring issue by introducing prior knowledge about the studied scene. Actually, remote sensing data growing volumes lead to increase the complexity of direct images interpretation. So, we attempt to overcome this problem by formalizing expert knowledge. The proposed method extends an expert knowledge formalism and temporal evolution graph representation to handle sub-graphs similarity. For this purpose, a user interaction through the proposition of a time window and concepts evolution is introduced. Hence, the proposed approach extracts the most similar temporal evolution to user request over the generated SITS subgraphs. Experiments are performed on synthesized and real STIS and compared to previously presented approaches for STIS analysis.	expert system;graph (abstract data type);knowledge modeling;semantics (computer science)	Safa Rejichi;Ferdaous Chaabane;Florence Tupin	2016	2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)	10.1109/IGARSS.2016.7730348	computer vision;time series;data mining;semantics;physics;satellite;remote sensing	Robotics	-21.693792720157088	1.9835698617618025	110882
d21622ab7a521003e8583fa7e40eba17d139594d	crowdsourcing computing resources for shortest-path computation	shortest path;graph separators;preprocessing;crowdsourcing	Crowdsourcing road network data, i.e., involving users to collect data including the detection and assessment of changes to the road network graph, poses a challenge to shortest-path algorithms that rely on preprocessing. Hence, current research challenges lie with improving performance by adequately balancing preprocessing with respect to fast-changing road networks. In this work, we take the crowdsourcing approach further in that we solicit the help of users not only for data collection, but also to provide us their computing resources. A promising approach is parallelization, which splits the graph into chunks of data that may be processed separately. This work extends this approach in that small-enough chunks allow us to use browser-based computing to solve the pre-computation problem. Essentially, we aim for a Web-based navigation service that whenever users request a route, the service uses their browsers for partially preprocessing a large, but changing road network. The paper gives performance studies that highlight the potential of the browser as a computing platform and showcases a scalable approach, which almost eliminates the computing load on the server.	algorithm;browser-based computing;computation;computational problem;crowdsourcing;parallel computing;precomputation;preprocessor;scalability;server (computing);shortest path problem;web application;world wide web	Alexandros Efentakis;Dimitris Theodorakis;Dieter Pfoser	2012		10.1145/2424321.2424383	computer science;theoretical computer science;machine learning;data mining;database;shortest path problem;programming language;world wide web;preprocessor;crowdsourcing	Web+IR	-32.614996245662006	-0.5736763733234651	110911
efc18667060f89ce4a5db20ad3c3d724b379cdfd	modeling and verifying probabilistic multi-agent systems using knowledge and social commitments	verification;social commitments;interpreted systems;knowledge;multi agent systems;modal logic;probabilistic model checking	http://dx.doi.org/10.1016/j.eswa.2014.04.008 0957-4174/ 2014 Elsevier Ltd. All rights reserved. ⇑ Corresponding author. Tel.: +1 514 848 2424x5382; fax: +1 514 848 3171. E-mail addresses: k_sultan@encs.concordia.ca (K. Sultan), bentahar@encs. concordia.ca (J. Bentahar), w_wan@encs.concordia.ca (W. Wan), f_alsaqq@encs. concordia.ca (F. Al-Saqqar). Khalid Sultan, Jamal Bentahar ⇑, Wei Wan, Faisal Al-Saqqar	agent-based model;algorithm;artificial neural network;autonomous robot;business domain;business process;complex systems;deadlock;el torito (cd-rom standard);electronic business;fax;formal verification;fuzzy logic;interaction;like button;liveness;logical framework;model checking;multi-agent system;prism (surveillance program);prism model checker;probabilistic ctl;robotics;scalability;verification and validation;web service;yang;zhi-li zhang	Khalid Sultan;Jamal Bentahar;Wei Wan;Faisal Al-Saqqar	2014	Expert Syst. Appl.	10.1016/j.eswa.2014.04.008	modal logic;verification;probabilistic ctl;probabilistic relevance model;computer science;artificial intelligence;multi-agent system;data mining;knowledge;probabilistic logic;probabilistic argumentation;algorithm	AI	-25.923757008018626	-8.830092139371974	110923
38a69aa01f86c6c1d57c92d6b169082cc6dbefb9	vor-tree: r-trees with voronoi diagrams for efficient processing of spatial nearest neighbor queries	nearest neighbor queries;voronoi diagram	A very important class of spatial queries consists of nearestneighbor (NN) query and its variations. Many studies in the past decade utilize R-trees as their underlying index structures to address NN queries efficiently. The general approach is to use R-tree in two phases. First, R-tree’s hierarchical structure is used to quickly arrive to the neighborhood of the result set. Second, the R-tree nodes intersecting with the local neighborhood (Search Region) of an initial answer are investigated to find all the members of the result set. While R-trees are very efficient for the first phase, they usually result in the unnecessary investigation of many nodes that none or only a small subset of their including points belongs to the actual result set. On the other hand, several recent studies showed that the Voronoi diagrams are extremely efficient in exploring an NN search region, while due to lack of an efficient access method, their arrival to this region is slow. In this paper, we propose a new index structure, termed VoR-Tree that incorporates Voronoi diagrams into R-tree, benefiting from the best of both worlds. The coarse granule rectangle nodes of R-tree enable us to get to the search region in logarithmic time while the fine granule polygons of Voronoi diagram allow us to efficiently tile or cover the region and find the result. Utilizing VoR-Tree, we propose efficient algorithms for various Nearest Neighbor queries, and show that our algorithms have better I/O complexity than their best competitors.	algorithm;granule (oracle dbms);input/output;r-tree;result set;time complexity;vhf omnidirectional range;voronoi diagram	Mehdi Sharifzadeh;Cyrus Shahabi	2010	PVLDB	10.14778/1920841.1920994	combinatorics;voronoi diagram;computer science;machine learning;data mining;database;mathematics	DB	-25.176609121970632	1.2454787445377986	111020
795ea21bf4d9a4dde98443d8e8aa238d53919c15	generic inverted index on the gpu	databases;paper;cuda;hashing;nvidia;nearest neighbour;computer science;nvidia geforce gtx titan x;tesla k80	Data variety, as one of the three Vs of the Big Data, is manifested by a growing number of complex data types such as documents, sequences, trees, graphs and high dimensional vectors. To perform similarity search on these data, existing works mainly choose to create customized indexes for different data types. Due to the diversity of customized indexes, it is hard to devise a general parallelization strategy to speed up the search. In this paper, we propose a generic inverted index on the GPU (called GENIE), which can support similarity search of multiple queries on various data types. GENIE can effectively support the approximate nearest neighbor search in different similarity measures through exerting Locality Sensitive Hashing schemes, as well as similarity search on original data such as short document data and relational data. Extensive experiments on different reallife datasets demonstrate the efficiency and effectiveness of our system.	approximation algorithm;big data;experiment;genie;graphics processing unit;inverted index;locality of reference;locality-sensitive hashing;nearest neighbor search;parallel computing;similarity search	Jingbo Zhou;Qi Guo;H. V. Jagadish;Wenhao Luan;Anthony K. H. Tung;Yueji Yang;Yuxin Zheng	2015	CoRR		hash function;computer science;theoretical computer science;data mining;database	DB	-31.536230773706304	1.3163707552510537	111035
6fbe28e41c0ddf59c91bdfe791ebe7ed17b281c8	spatiotemporal aggregate computation: a survey	spatiotemporal databases;base donnee;aggregation function;spatiotemporal phenomena aggregates application software biodiversity information analysis resource management roads moon spatial databases biological system modeling;aggregate function;query processing;base donnee temporelle;index terms spatiotemporal databases;interrogation base donnee;database;interrogacion base datos;base dato;spatial database;query languages;aggregate function index terms spatiotemporal databases aggregation queries;base donnee spatiale;base donnee spatiotemporel;spatiotemporal phenomena;spatial data structures;temporal databases;base dato especial;relational databases;spatiotemporal aggregate computation spatiotemporal databases aggregate queries evaluation aggregate function;database query;query languages spatiotemporal phenomena visual databases temporal databases query processing relational databases;visual databases;structure donnee spatiale;aggregation queries	Spatiotemporal databases are becoming increasingly more common. Typically, applications modeling spatiotemporal objects need to process vast amounts of data. In such cases, generating aggregate information from the data set is more useful than individually analyzing every entry. In this paper, we study the most relevant techniques for the evaluation of aggregate queries on spatial, temporal, and spatiotemporal data. We also present a model that reduces the evaluation of aggregate queries to the problem of selecting qualifying tuples and the grouping of these tuples into collections on which an aggregate function is to be applied. This model gives us a framework that allows us to analyze and compare the different existing techniques for the evaluation of aggregate queries. At the same time, it allows us to identify opportunities for research on types of aggregate queries that have not been studied.	aggregate data;aggregate function;algorithm;computation;data structure;database;granule (oracle dbms);information;max;rank (j programming language);spatiotemporal database;xslt/muenchian grouping	Inés Fernando Vega López;Richard T. Snodgrass;Bongki Moon	2005	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2005.34	relational database;computer science;data mining;database;temporal database;spatial database;information retrieval;query language	DB	-27.272692023188572	2.4546182172425977	111052
6ceb051c03825e31ad46e03191044c2b5e96e025	semantic overlay network for large-scale spatial information indexing	semantic similarity;spatial information indexing;overlay networks;self organizing algorithms;distributed systems	The increased demand for online services of spatial information poses new challenges to the combined filed of Computer Science and Geographic Information Science. Amongst others, these include fast indexing of spatial data in distributed networks. In this paper we propose a novel semantic overlay network for large-scale multi-dimensional spatial information indexing, called SON_LSII, which has a hybrid structure integrating a semantic quad-tree and Chord ring. The SON_LSII is a small world overlay network that achieves a very competitive trade-off between indexing efficiency and maintenance overhead. To create SON_LSII, we use an effective semantic clustering strategy that considers two aspects, i.e., the semantic of spatial information that peer holds in overlay network and physical network performances. Based on SON_LSII, a mapping method is used to reduce the multi-dimensional features into a single dimension and an efficient indexing algorithm is presented to support complex range queries of the spatial information with a massive number of concurrent users. The results from extensive experiments demonstrate that SON_LSII is superior to existing overlay networks in various respects, including scalability, maintenance, rate of indexing hits, indexing logical hops, and adaptability. Thus, the proposed SON_LSII can be used for large-scale spatial information indexing. Published by Elsevier Ltd.	algorithm;cluster analysis;computer science;e-services;experiment;geographic information science;internet;overhead (computing);overlay network;performance;quadtree;range query (data structures);raster data;scalability	Zhiqiang Zou;Yue Wang;Kai Cao;Tianshan Qu;Zhongmin Wang	2013	Computers & Geosciences	10.1016/j.cageo.2013.04.019	semantic similarity;overlay network;computer science;database;world wide web;information retrieval	DB	-29.486488282696648	0.5374689068121192	111109
ad783694a374a8747f8f8af6065bf0af9223c757	sequential knowledge acquisition: combining models and cases	knowledge acquisition	Without Abstract	knowledge acquisition	Birgit Brigl;Armin Grau;Peter Ringleb;Thorsten Steiner;Werner Hacke;Reinhold Haux	1995		10.1007/3-540-60025-6_158	computer science	NLP	-27.926508461219928	-7.747344082373918	111201
a63de314a6125429647f484e6d5fd9bef1dd1fdb	a comprehensive method for comparing mental models of dynamic systems	modelo dinamico;modelizacion;aspect dynamique;business studies;problem structuring mental models dynamic systems feedback;tiempo diferido;caracteristica dinamica;systeme aide decision;estructuracion;dynamic model;static properties;prise de decision;dynamic system;propriete statique;sistema n niveles;dynamic systems;sistema ayuda decision;operations research;time delay;dynamical system;resolucion problema;modelisation;systeme dynamique;decision support system;feedback;mental models;retroaccion;transit time;delayed time;polaridad;dynamic aspect;retroaction;causalite;systeme n niveaux;feedback loop;modelo mental;modele dynamique;cognition;multilevel system;feedback regulation;problem structuring;temps parcours;caracteristique dynamique;modele mental;cognicion;structuration;propiedad estatica;model management;aspecto dinamico;temps retard;delay time;dynamic characteristic;polarity;sistema dinamico;boucle reaction;toma decision;retroalimentacion;tiempo recorrido;modeling;tiempo retardo;polarite;temps differe;mental model;problem solving;resolution probleme;causality;causalidad	Mental models are the basis on which managers make decisions even though external decision support systems may provide help. Research has demonstrated that more comprehensive and dynamic mental models seem to be at the foundation for improved policies and decisions. Eliciting and comparing such models can systematically explicate key variables and their main underlying structures. In addition, superior dynamic mental models can be identified. This paper reviews existing studies which measure and compare mental models. It shows that the methods used to compare such models lack to account for relevant aspects of dynamic systems, such as, time delays in causal links, feedback structures, and the polarities of feedback loops. Mental models without those properties are mostly static models. To overcome these limitations of the methods to compare mental models, we enhance the widely used distance ratio approach (Markóczy and Goldberg, 1995) so as to comprehend these dynamic characteristics and detect differences among mental models at three levels: the level of elements, the level of individual feedback loops, and the level of the complete model. Our contribution lies in a new method to compare explicated mental models, not to elicit such models. An application of the method shows that this previously non-existent information is essential for understanding differences between managers’ mental models of dynamic systems. Thereby, a further path is created to critically analyze and elaborate the models managers use in real world decision making. We discuss the benefits and limitations of our approach for research about mental models and decision making and conclude by identifying directions for further research for operational researchers. 2010 Elsevier B.V. All rights reserved.	bluetooth;causal filter;centrality;decision support system;dynamical system;feedback;ldraw;memory data register;mental model;multichannel multipoint distribution service;noise shaping;operations research;simulation;system dynamics;tree accumulation	Martin F. G. Schaffernicht;Stefan N. Groesser	2011	European Journal of Operational Research	10.1016/j.ejor.2010.09.003	simulation;decision support system;computer science;artificial intelligence;dynamical system;mathematics;operations research;business studies	SE	-23.246283312876212	-8.128092437951567	111693
3b6b642f24511619ddd6c49ae36e14f40c03e557	an automated explanation approach for a decision support system based on mcda	decision support system	In a military context, the process of planning operations involves the assessment of the situation, the generation of Courses of Action (COAs), and their evaluation according to significant points of view, in order to select the course of action that represents the best possible compromise. Since several conflicting and quite incommensurable criteria need to be considered and balanced to make wise decisions, MultiCriterion Decision Aid (MCDA) has been used to develop decision support systems. Defence Research and Development Canada – Valcartier (DRDC Valcartier) has developed an advisor tool to assist the Air Operation Centre staff in managing events and their related COAs, as well as prioritizing these COAs according to different evaluation criteria by means of a MCDA procedure. Following this development, an investigation has been conducted to provide this decision support system with explanation facilities. This paper describes the suggested approach for the automated generation of explanations of a ranking proposed by a decision support system based on a MultiCriterion Aggregation Procedure (MCAP).	best, worst and average case;canned response;decision analysis;decision support system;decision theory;expect;graphics;mathematical model;principle of good enough;prototype;refinement (computing);semiconductor industry;verification and validation;virtual community	Micheline Bélanger;Jean-Marc Martel	2005			r-cast;decision analysis;management science;decision support system;business decision mapping;evidential reasoning approach;decision engineering;intelligent decision support system;computer science;compromise	AI	-31.730760509952546	-9.71823307322342	111759
a1e650301188b18774b5fa0ea354fc76d3fcc306	grid query optimizer to improve query processing in grids	resource selection;database system;computational grid;query processing;optimization technique;search space;query optimization;global computing;distributed query processing;database systems;overall response;grid computing;parallel processing	The emergence of computational grids, as global computing infrastructures, calls for development of new and advanced database techniques. While there exist algorithms and tools that facilitate database operations in grids, currently query optimization techniques are scant. In this paper, a query optimization technique, Grid Query Optimizer (GQO), that improves overall response time for grid-based query processing is presented. GQO features a resource selection strategy and a generic parallelism processing algorithm to balance optimization cost and query execution. GQO is tested using a simulated grid environment and compared with two other optimization techniques. Experiment results show that GQO provides better-than-average performance and is especially suitable for queries with large search spaces.	database;mathematical optimization;query optimization	Shuo Liu;Hassan A. Karimi	2008	Future Generation Comp. Syst.	10.1016/j.future.2007.06.003	online aggregation;parallel processing;sargable;query optimization;query expansion;computer science;query by example;theoretical computer science;data mining;database;web search query;view;grid computing;spatial query	DB	-29.116763714564048	2.460131279415843	111847
ed3f97b3afad82a8543885f0e61be2d6b7186bb6	an inverted file cache for fast information retrieval	memory management;cache;information retrieval system;information retrieval;hashing;hash table;inverted file;indexation;text retrieval;distributed simulation;article	The inverted file is the most popular indexing mechanism used for document search in an information retrieval system (IRS). However, the disk I/O for accessing the inverted file becomes a bottleneck in an IRS. To avoid using the disk I/O, we propose a caching mechanism for accessing the inverted file, called the inverted file cache (IF cache). In this cache, a proposed hashing scheme using a linked list structure to handle collisions in the hash table speeds up entry indexing. Furthermore, the replacement and storage mechanisms of this cache are designed specifically for the inverted file structure. We experimentally verify our design, based on documents collected from the TREC (Text REtrieval Conference) and search requests generated by the Zipf-like distribution. Simulation results show that the IF cache can improve the performance of a test IRS by about 60% in terms of the average searching response time.	block size (cryptography);experiment;hash table;information retrieval;input/output;inverted index;linked list;response time (technology);simulation;text retrieval conference;zipf's law;zipf–mandelbrot law	Wann-Yun Shieh;Jean Jyh-Jiun Shann;Chung-Ping Chung	2003	J. Inf. Sci. Eng.		hash table;cache coloring;hash function;page cache;inverted index;torrent file;cache;computer science;cache invalidation;database;open;programming language;cache algorithms;world wide web;information retrieval;memory management	Web+IR	-28.56129519112048	3.260425574702923	111864
4b1f3a55de9dfe3c66ddca5170d58c06c6cf426f	supporting decisions in complex, uncertain domains with declarative languages	lenguaje programacion;metodo analitico;analisis estadistico;programming language;incertidumbre;uncertainty;system dynamics;langage declaratif;statistical method;sistema complejo;probabilistic approach;decision maker;systeme asservi;dynamical system;systeme dynamique;statistical analysis;systeme complexe;complex system;enfoque probabilista;approche probabiliste;declarative languages;analytical method;analyse statistique;declarative language;langage programmation;servomecanismo;methode analytique;incertitude;information system;feedback system;sistema dinamico;simulation model;lenguaje declarativo;systeme information;sistema informacion	Domains with high levels of complexity and uncertainty pose significant challenges for decision-makers. Complex systems have too many linkages and feedbacks among system elements to easily apply analytical methods, and yet are too structured for statistical methods. Uncertainty, in terms of lacking information on system conditions and inter-relationships as well as inherent stochasticity, makes it difficult to predict outcomes from changes to system dynamics. In such situations, simulation models are key tools to integrate knowledge and to help improve understanding of systems responses in order to guide decisions.		Andrew Fall	2004		10.1007/978-3-540-24836-1_2	decision-making;complex systems;declarative programming;uncertainty;computer science;artificial intelligence;dynamical system;simulation modeling;feedback;system dynamics;programming language;information system;algorithm;statistics	DB	-22.177587174050217	-2.662940877370065	111922
0cc96cb1c0ad3799e829e7109c88b1a5f14bcfdc	scalable multi-query optimization for exploratory queries over federated scientific databases	data processing;query optimization;scientific database;proceedings paper;communication cost;natural science	The diversity and large volumes of data processed in the Natural Sciences today has led to a proliferation of highlyspecialized and autonomous scientific databases with inherent and often intricate relationships. As a user-friendly method for querying this complex, ever-expanding network of sources for correlations, we propose exploratory queries. Exploratory queries are loosely-structured, hence requiring only minimal user knowledge of the source network. Evaluating an exploratory query usually involves the evaluation of many distributed queries. As the number of such distributed queries can quickly become large, we attack the optimization problem for exploratory queries by proposing several multi-query optimization algorithms that compute a global evaluation plan while minimizing the total communication cost, a key bottleneck in distributed settings. The proposed algorithms are necessarily heuristics, as computing an optimal global evaluation plan is shown to be np-hard. Finally, we present an implementation of our algorithms, along with experiments that illustrate their potential not only for the optimization of exploratory queries, but also for the multiquery optimization of large batches of standard queries.	algorithm;autonomous robot;brute-force search;cache (computing);database;experiment;exploratory testing;heuristic (computer science);mathematical optimization;np-hardness;optimization problem;query optimization;transmitter;usability;web search query	Anastasios Kementsietsidis;Frank Neven;Dieter Van de Craen;Stijn Vansummeren	2008	PVLDB	10.14778/1453856.1453864	query optimization;natural science;data processing;computer science;data science;data mining;database;spatial query	DB	-25.75124854372898	3.964500707807615	112020
8721e90e869384a50359c0e133da8fe0d4ebd6bf	autonomous learning agent system for spacecraft telemetry processing	automatic control;telemetry processing;adaptability;rivers;prototypes;inference mechanisms;software agents spacecraft telemetry processing autonomous systems learning agent system online mechanism off line architecture inference mechanism adaptability;intelligent control;software agents;learning systems;space vehicles telemetry satellites automation artificial intelligence automatic control prototypes rivers ground support costs;satellites;inference mechanism;agent systems;artificial intelligence;inference mechanisms space vehicles intelligent control learning systems telemetry software agents;telemetry;off line architecture;ground support;autonomous systems;online mechanism;spacecraft;autonomous learning;intelligent software agent;space vehicles;learning agent system;automation	Automation of spacecraft operations can provide a substantial reduction in ground support and program costs. Here, we develop an autonomous spacecraft telemetry processing system consisting of an off-line architecture for the generation and analysis of learned patterns in the telemetry stream and an on-line mechanism for automated event and alert notification. Our system draws on the embedded knowledge, inferencing power, and learning and adaptability of intelligent software agents.	automation;autonomous robot;autonomous system (internet);embedded system;intelligent agent;online and offline;software agent	Paul Gonsalves;Alex Ivanov;Paul Zetocha	1998		10.1109/ICSMC.1998.726479	embedded system;adaptability;simulation;autonomous system;computer science;artificial intelligence;software agent;automation;automatic control;spacecraft;telemetry;prototype;satellite;intelligent control	Robotics	-19.72367464558718	-6.743983497980683	112083
707461bd1bdc7fafe9016dd37f04387427f9f867	diagnostic problem solving using first principles and heuristics	component 3;first principle;problem solving	This paper proposes an approach to diagnostic reasoning with the following distinct features: 1 A diagnostic system is formulated in FOL with equality, particularly in the form of program clauses; 2 The abnormality of system components is determined in terms of either experiential knowledge of domain experts or behavioral description of components; 3 Heuristics is fully used not only to assist in judging the abnormality of system components, but also to guide the diagnosis; 4 A unique diagnosis will be computed for a given observation, provided that certain essential I-O information is supplemented when demanded.	first-order logic;heuristic (computer science);problem solving	Yidong Shen;Mei Rong;Fu Tong	1996	Journal of Computer Science and Technology	10.1007/BF02948481	mathematical optimization;first principle;artificial intelligence;algorithm	AI	-20.202096684594327	-4.333562261218402	112135
0675766a69d595f6c21a67b5307d13f80d2a5dc0	maintaining footprint-based retrieval for case deletion	case based maintenance;case base reasoning;competence models;footprint based retrieval;case based reasoning;conference proceeding	The effectiveness and efficiency of case-based reasoning (CBR) systems depend largely on the success of case-based retrieval. The case-base maintenance (CBM) issues become imperative and important especially for modern societies. This paper proposes a new competence model and a new maintenance procedure for the proposed competence model. Based on the proposed competence maintenance procedure, footprint-based retrieval (FBR), a competence-based case base retrieval method, is able to preserve its own retrieval effectiveness and efficiency.	case-based reasoning;imperative programming	Ning Lu;Jie Lu;Guangquan Zhang	2009		10.1007/978-3-642-10684-2_35	case-based reasoning;computer science;artificial intelligence	AI	-20.54227139095676	-6.449431619372748	112174
2009b3c05dd9084a0a1c609abcedd81713ed7150	subzero: a fine-grained lineage system for scientific databases	array cell level;genomics;query processing;api;database management systems;cost reduction;runtime;arrays;application program interfaces;lineage query cost reduction fine grained lineage system scientific databases array cell level user defined operators api astronomy genomics;lineage query cost reduction;arrays runtime benchmark testing genomics bioinformatics payloads optimization;payloads;optimization;astronomy;scientific databases;fine grained lineage system;user defined operators;query processing application program interfaces cost reduction database management systems;benchmark testing;bioinformatics	Data lineage is a key component of provenance that helps scientists track and query relationships between input and output data. While current systems readily support lineage relationships at the file or data array level, finer-grained support at an array-cell level is impractical due to the lack of support for user defined operators and the high runtime and storage overhead to store such lineage. We interviewed scientists in several domains to identify a set of common semantics that can be leveraged to efficiently store fine-grained lineage. We use the insights to define lineage representations that efficiently capture common locality properties in the lineage data, and a set of APIs so operator developers can easily export lineage information from user defined operators. Finally, we introduce two benchmarks derived from astronomy and genomics, and show that our techniques can reduce lineage query costs by up to 10× while incuring substantially less impact on workflow runtime and storage.	application programming interface;benchmark (computing);black box;database;deterministic algorithm;experiment;input/output;interactivity;lineage (evolution);locality of reference;mathematical optimization;overhead (computing);universal disk format	Eugene Wu;Samuel Madden;Michael Stonebraker	2013	2013 IEEE 29th International Conference on Data Engineering (ICDE)	10.1109/ICDE.2013.6544881	benchmark;payload;genomics;application programming interface;computer science;bioinformatics;data mining;database;programming language	DB	-33.13376158891141	1.464199184441219	112343
0b79aa3681dd577064556091bcd91038d291f3c6	on estimating the cost of accessing records in blocked database organizations		The estimation of the cost of processing a query using a particular access path under a given physical organization has important applications in integrated database environments. When records in a file are stored in fixed-length physical blocks in secondary storage, and mechanisms are available whereby a query can be resolved without the accessing of all of the records, an important measure of the cost of using a particular access path is the number of blocks that have to be accessed in referencing the records of interest. In this paper, a general formula is derived for the expected number of blocks on which a random sample of r records from a file containing n records (which may be of arbitrary lengths, and which may extend across block boundaries) will reside. The specialization of this formula to the case of fixedlength records is discussed. An approximation to this formula which is highly accurate for a wide range of parameters and which can be computed very efficiently is also provided.	approximation;auxiliary memory;computer data storage;partial template specialization	Arvola Chan;Bahram Niamir	1982	Comput. J.	10.1093/comjnl/25.3.368	data mining;database;world wide web	DB	-27.555019268579642	3.4456900424768078	112650
6cdda63bfe5ee3d2bad1b482e208889b9824960f	leveraging problem classification in online meta-cognition		Open environments are characterized by their uncertainty and non-determinism. This poses an inevitable challenge to the construction of agents operating in such environments. The agents need to adapt their processing to available resources, deadlines, the goal criteria specified by the clients as well as their current problem solving context in order to survive. Our research focuses on constructing a meta-cognition framework that will enable agents to adapt to their dynamic environment. This will include deciding which environmental changes to address, how quickly they should be addressed and which of the different planning, scheduling and negotiation modes to use to address these changes. In this paper, we also describe how the classification of environmental changes plays a pivotal role in making the meta-level decisions.	automated planning and scheduling;client (computing);cognition;nondeterministic algorithm;problem solving;scheduling (computing)	Anita Raja;George Alexander;Verghese Mappillai	2006			machine learning;computer science;artificial intelligence;management science;scheduling (computing);metacognition;negotiation	AI	-19.56007130925794	-7.453392413362484	112942
6de83cdf60400305e3593588b5244dc5d0ce8dce	multiple generation text files using overlapping tree structures	tree structure			F. Warren Burton;Matthew M. Huntbach;John G. Kollias	1985	Comput. J.	10.1093/comjnl/28.4.414	segment tree;computer science;theoretical computer science;pattern recognition;data mining;fractal tree index;tree structure;search tree;tree;tree traversal	NLP	-26.61215146456704	2.669891439112103	113128
644cb630c73231ddec2b10372631959dc8799dfd	knowledge-driven systems for episodic decision support	decision support systems;ontologies;knowledge engineering	Decision support systems rely on collaborative and episodic participation.Large decision knowledge bases can include varying knowledge representations.We introduce an approach to implement such systems in a coherent manner.The approach is based on semantic technologies and web standards.A case study describes experiences with a decision support system in industrial use for years. The paper describes a new approach of developing and maintaining state-of-the-art decision support systems. Such systems are able to capture the collaborative work on decision problems over time. Due to the complexity of large problem spaces a multi-modal knowledge representation is proposed. For the realization of a multi-modal knowledge base we integrate semantic technologies as a fundamental layer by combining the W3C ontologies PROV-O and SKOS. The approach is demonstrated by an implementation report of an industrially deployed decision support system.	decision support system	Joachim Baumeister;Albrecht Striffler	2015	Knowl.-Based Syst.	10.1016/j.knosys.2015.08.008	r-cast;decision support system;intelligent decision support system;decision analysis;decision engineering;computer science;knowledge management;ontology;artificial intelligence;knowledge engineering;data mining;management science;evidential reasoning approach;business decision mapping	DB	-31.65952305621494	-7.189335350891194	113190
bb04dbabcd75b21ae8f49f014a9b044783e6c6b9	dynamic file structure for partial match retrieval based on overflow bucket sharing	physical design database management file organization information storage and retrieval;file organisation data structures;file organization;almacenamiento de informacion;overflow computer arithmetics;space efficient hashing partial match retrieval overflow bucket sharing dynamic file structure data buckets storage utilization neighboring directory entries retrieval costs file organization;information retrieval;performance;large hadron collider;simulation;recuperacion de informacion;pregunta documental;simulacion;physical design;database management;question documentaire;algorithme;manganese;algorithm;estructura del archivo;indexes;information storage;algorritmo;vectors;data base management system;recherche information;data structures;structure fichier;file structure;rebasamiento capacidad;query;stockage information;depassement capacite;organizations vectors indexes computer science educational institutions manganese large hadron collider;organizations;computer science;systeme gestion base donnee;rendimiento;sistema gestion base datos;information storage and retrieval;hâchage;file organisation	A hashing-based dynamic file structure is introduced for partial match retrieval using overflow bucket sharing. The sharing of overflow buckets is dynamic in the sense that an overflow bucket is shared by a varying number of primary buckets according to the local conditions of the file. The use and sharing of overflow buckets defers splitting of the data buckets, thereby increasing the storage utilization. For the same reason, plus the fact that the sharing is dynamic, the growth of the directory is slowed down. Under the proposed organization, the records are stored more compactly in the data buckets, and for those partial match queries in which few attributes are specified, groups of neighboring directory entries have high probability of being referenced together, so that the retrieval costs for these types of partial match queries are reduced. This file organization is found to be space efficient and is also time efficient for queries in which the number of specified attributes is small.	buffer overflow;directory (computing);stack overflow	Tak-Sun Yuen;David Hung-Chang Du	1986	IEEE Transactions on Software Engineering	10.1109/TSE.1986.6312983	physical design;large hadron collider;data structure;performance;computer science;manganese;operating system;data mining;database;programming language;world wide web	DB	-27.6233481826181	4.0140205069559265	113402
d5251bae87375e2e5d6f501286c5b1b7f53fb3b6	uniform organization of inverted files	case perfect uniformity;range attribute;uniform number;file designer;x record;inverted file;uniform organization;pascal program	A range attribute is defined as an attribute that may assume a range of values. Examples might be Age = (1--10, 11--14, 15--16, ...) or Salary = (0-1000, 1001-1500, ...). This paper is concerned with the selection of ranges that will produce reasonably uniform numbers of records in each range. A set of algorithms has been developed to enable the file designer to obtain a set of ranges such that records are distributed uniformly between the ranges. Although in a given case perfect uniformity may not be achievable, the algorithms can find ranges such that for a set of X records in a range, bounds a and b may be given so that a ≤ X ≤ b for all ranges. The algorithms have been tested with a PASCAL program.	algorithm;attribute grammar;circuit complexity;inverted index;pascal	Dalia Motzkin;Kenneth Williams;Karl Chang	1984		10.1145/1499310.1499385	computer science;theoretical computer science;data mining;algorithm	DB	-28.589593597557116	3.9960201368403414	113447
5ce5b60c2d48b8ab1c0ef90fcf314ef88051fff8	enhancing clinical practice guideline compliance by involving physicians in the decision process	terminologie;medecin;clinical practice guideline;decision support;informatica biomedical;biomedical data processing;hipertexto;terminologia;decision tree;formal model;physician;systeme aide decision;informatique biomedicale;base connaissance;recommandation;sistema ayuda decision;arbol decision;interpretacion informacion;interpretation information;decision support system;medico;recomendacion;base conocimiento;recommendation;terminology;decision process;information interpretation;breast cancer;arbre decision;hypertexte;hypertext;point of care;knowledge base	Despite the proliferation of implemented clinical practice guidelines (CPGs) as decision support systems, there is still little evidence of changes in physicians behavior. The reasons usually evoked to explain the low physicians compliance consider the incompleteness of guidelines knowledge, the impreciseness of the terms used and the physicians psychological reluctance. Another reason comes from the original verbal design of CPGs as well as the impossibility to enumerate all the contexts in which a guideline applies, which avoid the automatised control of all CPGs interpretations and therefore the design of robust formal models. The ONCODOC approach proposes a decision support framework for implementing guidelines where the context-based interpretation is controlled by clinicians. The first application deals with breast cancer therapy. Experimented in real size at the point of care, the system demonstrated significantly high scores of theoretical agreement with CPGs recommendations and compliance.		Brigitte Séroussi;Jacques Bouaud;Éric-Charles Antoine	1999		10.1007/3-540-48720-4_6	point of care;hypertext;decision support system;computer science;knowledge management;artificial intelligence;breast cancer;decision tree;data mining;terminology	HCI	-25.51493369801923	-4.815499460243591	113540
6b07c9772ec683c0aff845f6e8d27fc66435e592	aiding the operator during novel fault diagnosis	aiding the operator during novel fault diagnosis;expert systems;fault diagnosis humans network address translation psychology synthetic aperture sonar wide area networks algebra intelligent systems appraisal probability distribution;eromonomics society of korea;vol 6 no 1;failure analysis;w c yoon;component level model decision support systems failure analysis novel fault diagnosis intelligent aid human operator causal knowledge;operational problems;대한인간공학회지 제6권 제1호;decision support systems;대한인간공학회;computer techniques;j h hammer;mental performance;task complexity;diagnosis;man machine systems;failure analysis decision support systems;operator performance;fault diagnosis;human computer interface	The design and philosophy are presented for an intelligent aid for a human operator who must diagnose a novel fault in a physical system. A novel failure is defined as one that the operator has not experienced in either real system operation or training. Because the fault is novel, the human must reason using causal knowledge. The aid contains unique features that support such reasoning. One of these is a qualitative, component-level model of the physical system. The model can reflect the operator's hypothesis when it is requested. Both the aid and the human are able to reason causally about the system in a cooperative search for a diagnosis. >		Wan Chui Yoon;John M. Hammer	1988	IEEE Trans. Systems, Man, and Cybernetics	10.1109/21.87062	failure analysis;simulation;decision support system;computer science;artificial intelligence;machine learning;operations research;expert system	Robotics	-26.258895597920908	-6.717338637034643	113567
1f4c3f3460d1afefbe853b78d086f2f34e535c31	constructing the information management system based on ontology and concept lattices	concept lattice;domain ontology;formal concept analysis;information management	Concept lattice, the main data structure in formal concept analysis theory, has been widely applied to many fields such as machine learning, knowledge discovery, information retrieval and software engineering. Ontology has become increasingly popular because it promises a shared and common understanding of knowledge domains that can be communicated between people and application systems. The main contributions of this paper is presenting applying formal concept analysis theory and ontology to construct information system of management knowledge, and the experiments shows the CPU Time in the attribute numbers, indicating that FCA is superior to ontology in building the information management system. © 2011 Springer-Verlag Berlin Heidelberg.		Guo Li;Qihua Peng	2011		10.1007/978-3-642-23756-0_123	upper ontology;knowledge management;ontology;data mining;ontology-based data integration;information retrieval;process ontology;suggested upper merged ontology	AI	-33.5243609726663	-5.217607437757998	113612
79f5198e186dd33f2870deeee177cafb90630e99	mga: a decision support system for complex, incompletely defined problems	modelizacion;operateur humain;operador humano;selected works;systeme aide decision;prise decision;decision maker;teoria decision;modelisation;support system;decision support systems humans mathematical model mathematical programming civil engineering power system modeling laboratories associate members logic programming man machine systems;decision support system;methode modeling to generate alternatives;mathematical programming;sensitivity analysis;theorie decision;sensitivity analysis complex problem solving decision support system mathematical programming models human machine decision making system;decision theory;decision support systems;human operator;mathematical model;bepress;user interfaces decision support systems mathematical programming;toma decision;modeling;programmation mathematique;user interfaces;programacion matematica	Modeling-to-generate alternatives (MGA) is a technique for using mathematical programming models to generate a small number of different solutions for the decision maker to consider when dealing with complex, incompletely defined problems. The logic of MGA is presented in the context of concerns about the limitations of mathematical models and limitations of the human decisionmakers who use them. Arguments and experimental evidence are presented to support the assumption that the human-machine decisionmaking system will perform better when the human is presented with a few, different alternatives than when presented a homogeneous set of alternatives as might result from sensitivity analysis.	decision support system;hercules graphics card;mathematical model;mathematical optimization	Downey Brill;John M. Flach;Lewis D. Hopkins;S. Ranji Ranjithan	1990	IEEE Trans. Systems, Man, and Cybernetics	10.1109/21.105076	decision-making;systems modeling;decision support system;decision theory;computer science;artificial intelligence;mathematical model;user interface;operations research;sensitivity analysis;algorithm;statistics	Theory	-24.399243401848977	-6.433111852317568	113824
7371b9139f0f5dec636a5fee0926751714e3b43a	simulating social grouping: an interactive team-building tool (itbt)	systeme intelligent;simulation systeme;systeme cooperatif;systeme aide decision;sistema inteligente;sistema ayuda decision;productique;team building;decision support system;cooperative systems;intelligent system;robotica;social groups;system simulation;computer integrated manufacturing;simulacion sistema	The paper presents the concept of an interactive team-building tool for simulating social grouping. It shows how decisions whom to assign which task can be supported from a game-theoretic perspective. Thus the group member are modeled as social entities following their preferences and being able to resist against work distributions they do not want. The features of ITBT are deduced from the decision situation of a team leader being in charge for the work distribution and are described in more detail in the paper.		Edeltraud Hanappi-Egger	1999		10.1007/10720123_26	social group;simulation;decision support system;computer science;artificial intelligence;computer-integrated manufacturing;operations research	HCI	-23.072111490743538	-7.732162329896906	114064
05fd3b4ab24029ac76c05d68488d0a06abfc8bd4	upper and lower images of a fuzzy set induced by a fuzzy relation: applications to fuzzy inference and diagnosis	representacion conocimientos;sistema experto;fuzzy set;concepcion sistema;incertidumbre;uncertainty;sistema informatico;fuzzy relation;logique floue;diagnostico;computer system;intelligence artificielle;logica borrosa;raisonnement;fuzzy logic;system design;relacion;fuzzy inference;razonamiento;artificial intelligence;systeme informatique;incertitude;inteligencia artificial;systeme expert;reasoning;knowledge representation;diagnosis;representation connaissances;relation;conception systeme;diagnostic;expert system	Abstract   In fuzzy set theory the image of a fuzzy set induced by a fuzzy relation is usually obtained by a sup-t-norm composition. This corresponds to the upper image which gathers the elements in relation with at  least one  element of the fuzzy set. However the dual point of view, leading to the definition of the lower image as the fuzzy set of elements in relation with  all  the elements of the fuzzy set whose image is computed, is not often considered. Fuzzy sets and fuzzy relations, depending on the situations, can be interpreted either in a conjunctive manner or as subsets of mutually exclusive possible values for variables whose precise values are ill-known (disjunctive view). The applications of upper and lower images are investigated in both interpretations. The generalized modus ponens, used in fuzzy rule-based systems, corresponds to the disjunctive view. The interest of upper and lower images is also emphasized for diagnosis problems where the conjunctive interpretation is encountered.	fuzzy logic;fuzzy set	Didier Dubois;Henri Prade	1992	Inf. Sci.	10.1016/0020-0255(92)90101-D	fuzzy logic;t-norm fuzzy logics;uncertainty;membership function;defuzzification;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;relation;artificial intelligence;fuzzy subalgebra;fuzzy number;fuzzy measure theory;mathematics;fuzzy set;fuzzy associative matrix;expert system;fuzzy set operations;reason;algorithm;fuzzy control system;systems design	AI	-20.810608983143744	-1.8319599638533115	114128
433c0b8340dabf262b0a217f5d3e2208da44c13e	parallel high-dimensional index structure for content-based information retrieval	microprocessors;filtering;parallel high dimensional index structure;range query;high dimensionality;query processing;information retrieval;index structure;cell based filtering scheme;computer architecture;indexes;distance measurement;content based information retrieval;servers;va file parallel high dimensional index structure content based information retrieval dimensional curse problem cell based filtering scheme k nn query processing algorithms cluster based parallel architecture;indexes servers distance measurement computer architecture microprocessors filtering program processors;dimensional curse problem;cluster based parallel architecture;parallel architecture;va file;parallel processing content based retrieval filtering theory;k nn query processing algorithms;content based retrieval;program processors;parallel processing;filtering theory	To solve dasiadimensional cursepsila problem, the cell-based filtering scheme has been proposed, but it shows a linear decrease in performance as the dimensionality is increased. In this paper, we propose a parallel high-dimensional index structure for content-based information retrieval so as to cope with the linear decrease in retrieval performance. In addition, we devise data insertion, range query and k-NN query processing algorithms which are suitable for a cluster-based parallel architecture. Finally, we show that our parallel index structure achieves good retrieval performance in proportion to the number of servers in the cluster-based architecture and it outperforms a parallel version of the VA-File when the dimensionality is over 10.	cell (microprocessor);database index;information retrieval;k-nearest neighbors algorithm;parallel computing;range query (database)	Jaewoo Chang;Ahreum Lee	2008	2008 8th IEEE International Conference on Computer and Information Technology	10.1109/CIT.2008.4594657	filter;database index;range query;parallel processing;computer science;theoretical computer science;operating system;database;information retrieval;server	DB	-29.013050869240516	1.4163123525525845	114577
aaf4e56d1fc3b4630e4fb59da631bbee7a0333e1	intelligent debugging of utility constraints in configuration knowledge bases	knowledge base	Knowledge-based configurators support customers in preference construction processes related to complex products and services. In this context utility constraints (scoring rules) play an important role. They determine the order in which configurations are presented to customers. In many cases utility constraints are faulty, i.e., calculate configuration rankings which are not expected and accepted by marketing and sales experts. The repair of these constraints is extremely time-consuming and often an error-prone task. In this paper we present an approach which effectively supports automated debugging and repair of faulty utility constraint sets. This approach allows us to automatically take into account intended rankings of configurations specified by marketing and sales experts.	cognitive dimensions of notations;debugging;knowledge engineer;knowledge engineering;model-based definition	Alexander Felfernig;Monika Mandl;Monika Schubert	2010			debugging;knowledge base;data mining;database;knowledge-based systems;open knowledge base connectivity;computer science	AI	-28.750788378083918	-3.9397633052742145	114631
5dba51c186ad722091e48312d58fb674892f74c1	evaluation of probabilistic threshold queries in mcdb	database system;satisfiability;algorithms;stochastic model;statistical query;uncertain data	"""MCDB is a prototype database system for managing stochastic models for uncertain data. In this paper, we study the problem of how to use MCDB to answer statistical queries that search for database objects which satisfy some filter condition with greater (or less than) a user-specified probability. For example: """"Which packages will arrive late with > 5% probability?"""" """"Which regions will see more than a 2% decline in sales with > 50% probability?"""" """"What items will be out of stock by Friday with > 20% probability?"""" We consider both the systems aspects and the statistical aspects of the problem."""	database;prototype;stochastic process;uncertain data	Luis Leopoldo Perez;Subramanian Arumugam;Chris Jermaine	2010		10.1145/1807167.1807242	computer science;stochastic modelling;data science;data mining;database;satisfiability	DB	-25.85262860477468	3.016734168600067	114854
1e1d37a7262c5d754846456934842b5dcc6efebb	symbolic constraints in constructive geometric constraint solving	computer aided design;constructive geometric constraint;geometric constraint solving;cad;external research report;geometric constraints	In design and manufacturing applications, users of computer aided design systems want to de ne relationships between dimension variables, since such relationships express design intent very exibly. This work reports on a technique developed to enhance a class of constructive geometric constraint solvers with the capability of managing functional relationships between dimension variables. The method is shown to be correct.	algorithm;bigraph;computer cluster;computer-aided design;constraint graph;constraint satisfaction problem;programming paradigm;rewriting;solver	Christoph M. Hoffmann;Robert Joan-Arinyo	1997	J. Symb. Comput.	10.1006/jsco.1996.0089	mathematical optimization;combinatorics;discrete mathematics;computer aided design;cad;mathematics	EDA	-26.40936710321206	-3.179505472220176	114898
5f1b29fcaff0b512ef6d2c568b954607a735ffab	temporal semantics of meta-level architectures for dynamic control of reasoning	system modelling;dynamic control	Meta-level architectures for dynamic control of reasoning processes are quite powerful. In the literature many applications in reasoning systems modelling complex tasks are described, usually in a procedural manner. In this paper we present a declarative framework based on temporal (partial) logic that enables one to describe the dynamics of reasoning behaviour by temporal models. Using these models the semantics of the behaviour of the whole (meta-level) reasoning system can be described by a set of (intended) temporal models.		Jan Treur	1994		10.1007/3-540-58792-6_22	computer science;physics	AI	-20.99316140999436	1.5200341622874882	115037
a8870cfd5665dfd3949dece324e42d09eb82eb16	pytrec_eval: an extremely fast python interface to trec_eval		We introduce pytrec_eval, a Python interface to the trec_eval information retrieval evaluation toolkit. pytrec_eval exposes the reference implementations of trec_eval within Python as a native extension. We show that pytrec_eval is around one order of magnitude faster than invoking trec_eval as a sub process from within Python. Compared to a native Python implementation of NDCG, pytrec_eval is twice as fast for practically-sized rankings. Finally, we demonstrate its effectiveness in an application where pytrec_eval is combined with Pyndri and the OpenAI Gym where query expansion is learned using Q-learning.	information retrieval;python;q-learning;query expansion	Christophe Van Gysel;Maarten de Rijke	2018		10.1145/3209978.3210065	eval;learning to rank;query expansion;computer science;information retrieval;ir evaluation;python (programming language)	OS	-32.96735929335241	3.670698495366181	115383
264bd05419bcce03cfdea88141d46ad2dedefcfb	a repository of rules and lexical resources for discourse structure analysis: the case of explanation structures		In this paper, we present an analysis method, a set of rules, l exica resources dedicated to discourse relation identific ation, in particular for explanation analysis. The following relations are desc ribed with prototypical rules: instructions, advice, warn ings, illustration, restatement, purpose, condition, circumstance, concessi on, contrast and some forms of causes. Rules are developed fo r French and English. The approach used to describe the analysis of such r elations is basically generative and also provides a concep tual view of explanation. The implementation is realized in Dislog, usi ng the<TextCoop> logic-based platform, that also allows for the integration of knowledge and reasoning into rules describing the struct u e of explanation.	discourse relation;fo (complexity);struct (c programming language)	Sarah Bourse;Patrick Saint-Dizier	2012			natural language processing;computer science;knowledge management;linguistics	AI	-24.314579987166983	-1.4310211271734845	115543
6a2f99e638aaeef68b8fa413e2f2d6ef69ef63c7	flexible techniques for storage and analysis of large continuing surveys	efficient processing;flexible technique;common characteristic;statistics canada;processing requirement;processing need;data retrieval;data storage;micro data;large continuing survey;labor statistics;large-scale continuing survey	A common characteristic of continuing surveys is that they tend to have a long history of evolutionary development. Questions asked may change over time, and the accumulated body of data becomes increasingly large and complex. This characteristic leads to the need for computer systems capable of dealing with complexity while providing efficient processing and data storage. Two generalized computer systems are examined to determine their effectiveness in meeting the processing needs of large-scale continuing surveys. Both systems were developed with a major goal being the processing of large and complex collections of micro data. One of the systems, called RAPID, was developed by Statistics Canada for data storage and maintenance. The other, called TPL for Table Producing language, was developed in the United States by the Bureau of Labor Statistics to do data retrieval, reduction, and table generation. Recent work has resulted in the linking of these two systems. A file design is discussed which takes advantage of the special features of these systems and demonstrates that their combined use is weil-suited to the processing requirements of large-scale continuing surveys.		Pamela L. Weeks;Stephen E. Weiss;Peter B. Stevens	1981			computer science;data science;data mining;database;operations research	HPC	-31.47115265123557	-2.216652567893427	115766
6f19006a10224febde404bb43779096962253b66	situation awareness meets ontologies: a context spaces case study		Efficiency and appeal of pervasive computing systems strongly depends on how well and robustly they represent and reason about context and situations. Populating situation search space and inferring situations from context which, in turn, is computed from fusing sensor data and observations remains a major research challenge. This paper proposes to use ontologies as representation of domain knowledge to generate situation search space and then match context with already defined situations. To illustrate the feasibility, a context spaces approach is used to represent, generate and reason about situations as abstractions in a multidimensional space. The proposed approach is evaluated and discussed.	ontology (information science);spaces	Andrey Boytsov;Arkady B. Zaslavsky;Elif Eryilmaz;Sahin Albayrak	2015		10.1007/978-3-319-25591-0_1	knowledge management;data mining;management science	HCI	-21.632099391233933	-8.714048512559696	115906
eee643e9b5267ed8a5455ba3765aee24e5cc5906	user-centered planning - a discussion on planning in the presence of human users		AI planning forms a core capability of intelligent systems. It enables goal directed behavior and allows systems to react adequately and flexibly to the current situation. Further, it allows systems to provide advice to a human user on how to reach his or her goals. Though the process of finding a plan is, by itself, a hard computational problem, some new challenges arise when involving a human user into the process. Plans have to be generated in a certain way, so that the user can be included into the plan generation process in case he or she wishes to; the plans should be presented to the user in an adequate way to prevent confusion or even rejection; to improve the trust in the system, it needs to be able to explain its behavior or presented plans. Here, we discuss these challenges and give pointers on how to solve them.	automated planning and scheduling;computation;computational problem;rejection sampling	Pascal Bercher;Daniel Höller;Gregor Behnke;Susanne Biundo-Stephan	2015			computer science	AI	-21.23415585860905	-8.426478379273291	115987
ac2e2f00dece4d44ad6d0817710da0458e6b3c82	parallel data mining for very large relational databases	data parallel;qa 76 software;relational database;data mining;computer programming;knowledge discovery in database	Data mining, or Knowledge Discovery in Databases (KDD), is of little benefit to commercial enterprises unless it can be carried out efficiently on realistic volumes of data. Operational factors also dictate that KDD should be performed within the context of standard DBMS. Fortunately, relational DBMS have a declarative query interface (SQL) that has allowed designers of parallel hardware to exploit data parallelism efficiently. Thus, an effective approach to the problem of efficient KDD consists of arranging that KDD tasks execute on a parallel SQL server. In this paper we devise generic KDD primitives, map these to SQL and present some results of running these primitives on a commercially-available parallel SQL server.	data mining;relational database management system	Alex Alves Freitas;Simon H. Lavington	1996		10.1007/3-540-61142-8_542	data modeling;data definition language;database theory;sql;relational database management system;entity–relationship model;data model;relational database;computer science;probabilistic database;data science;data administration;database model;data mining;computer programming;database;change data capture;data stream mining;programming language;view;database design;component-oriented database	ML	-32.13454440364242	1.5457762953414143	116223
fdf81cf967c844cb42dbe724834ae3219c7118a5	knowledge engineering of intelligent sales-recruitment system using multi-layered agent methodology	extraction information;commercial use;image recognition;interes comercial;reconocimiento imagen;optimisation;multiagent system;alarm;red www;optimizacion;information extraction;ingenierie connaissances;image databank;informacion incompleta;soft computing;distributed processing;reseau web;distributed computing;contratacion;data mining;recrutement;incomplete information;internet;interet commercial;fouille donnee;banco imagen;alarme;banque image;information incomplete;reconnaissance image;calculo repartido;web mining;world wide web;optimization;vente;sales;venta;alarma;sistema multiagente;busca dato;calcul reparti;extraccion informacion;medical diagnosis;recruitment;problem solving;systeme multiagent;expert system;knowledge engineering	In this paper we outline a multi-layered multi-agent distributed soft computing architecture for providing support to practitioners and problem solvers at four levels, namely, distributed processing, technology, optimization and problem solving level respectively. The architecture has been applied in complex problems like image recognition, web mining, alarm processing, and medical diagnosis. We illustrate some aspects of the architecture with help of e-Sales recruitment (e-SRS) application. An earlier version of e-SRS (expert system ) is also in commercial use.		Rajiv Khosla;Tharanga Goonesekera;Yasue Mitsukura	2003		10.1007/978-3-540-39592-8_93	space-based architecture;web mining;the internet;simulation;computer science;applications architecture;artificial intelligence;operating system;knowledge engineering;medical diagnosis;data mining;database;solution architecture;soft computing;computer security;expert system;complete information;information extraction	Robotics	-26.850317821171892	-4.488805394964593	116346
ab6dea75c8dd4c915923dd6cd142fc4d71160093	a selectivity model for fragmented relations: applied in information retrieval	databases;selection model;information retrieval;storage management;query optimization;data distribution;zipf;distributed databases;zipfian distributed ir databases selectivity model fragmented relations information retrieval data fragmentation main memory computing query types database design query optimization mathematically derived selectivity model data distribution;selectivity;database design;fragmentation;storage management distributed databases information retrieval;information retrieval distributed databases predictive models distributed computing mathematical model context modeling concurrent computing query processing cost function computer applications	New application domains cause today's database sizes to grow rapidly, posing great demands on technology. Data fragmentation facilitates techniques (like distribution, parallelization. and main-memory computing) meeting these demands. Also, fragmentation might help to improve efficient processing of query types such as top N. Database design and query optimization require a good notion of the costs resulting from a certain fragmentation. Our mathematically derived selectivity model facilitates this. Once its two parameters have been computed based on the fragmentation, after each (though usually infrequent) update, our model can forget the data distribution, resulting in fast and quite good selectivity estimation. We show experimental verification for Zipfian distributed IR databases.	application domain;computer data storage;database design;estimation theory;fork (software development);fragmentation (computing);information retrieval;mathematical optimization;parallel computing;query optimization;selectivity (electronic);zipf's law	Henk Ernst Blok;Sunil Choenni;Henk M. Blanken;Peter M. G. Apers	2004	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2004.1277824	zipf's law;query optimization;query expansion;selectivity;computer science;data mining;database;fragmentation;world wide web;distributed database;information retrieval;database design;query language	DB	-29.842770663887666	1.7408754862520615	116572
ad329214db010eb839b2dcb3722b72be1e9228d1	an inverted index for mass spectra similarity query and comparison with a metric-space method: case study	metric space;inverted index;mass spectra;indexing method;general methods;indexation;similarity query;sparse matrix;metric space indexing;domain specificity	Query performance is a determining factor in the adoption of an indexing method for similarity query. Metric space indexing methods take great pride in their general applicability. However, it is usually hard for a general method to perform well for every domain. Therefore, it is of interest to investigate the performance of metric-space methods, comparing with domain specific methods, on a particular domain. This paper describes such an investigation for proteomic mass spectra. An inverted index method that exploits the sparsity of mass spectra binary format data and acts as a coarse filter before fine ranking is proposed and empirically compared with an existing metric-space indexing method. Results show that the inverted index method yields greater search efficiency and outperforms the metric-space method in query speed and index size.	binary file;inverted index;proteomics;sparse matrix	Rui Mao;Smriti R. Ramakrishnan;Glen Nuckolls;Daniel P. Miranker	2010		10.1145/1862344.1862359	inverted index;mass spectrum;sparse matrix;metric space;computer science;data mining;database;mathematics;information retrieval	Web+IR	-31.18296087188041	2.942092203580263	116680
819e417c5f65ec6e12fc2e0de9d355d300f43a43	modelling from knowledge versus modelling from rules using uml	information science;dynamic model;kommunikationsvetenskap;systemvetenskap;domain knowledge;unified modelling language uml;knowledge acquisition;graphical representation;graphical model;communication studies;production rule;knowledge base	Modelling support for knowledge acquisition is a tool for modelling domain knowledge. However, during the implementation of the knowledge new knowledge is created. Event though this knowledge is found in the knowledge base, the model usually is not updated with the new knowledge and do, therefore, not contain all the knowledge in the system. This paper describes how different graphical models support the complex knowledge acquisition process of handling domain knowledge and how these models can be extended by modelling knowledge from rules in a knowledge base including probability. Thus, the models are designed from domain knowledge to create production rules but the models are also extended with new generated knowledge, i.e., generated rules. The paper also describes how different models can support the domain expert to grasp this new generated knowledge and to understand the uncertainty calculated from rules during consultation. To this objective, graphic representation and visualisation is used as modelling support through the use of diagrams of Unified Modelling Language (UML), which is used for modelling production rules. Presenting rules in a static model can make the contents more comprehensible and in a dynamic model can make the uncertainty more evident.	unified modeling language	Anne Håkansson	2005		10.1007/11552451_52	natural language processing;knowledge base;idef3;computer science;knowledge management;body of knowledge;mathematical knowledge management;knowledge-based systems;open knowledge base connectivity;data mining;procedural knowledge;knowledge extraction;commonsense knowledge;domain knowledge	NLP	-31.493625632827385	-6.376239328622987	116754
2ae08afeec2421ef87acd5424ff1a1950007d847	the strip rule system for efficiently maintaining derived data	database system;real time;option pricing;active databases	Derived data is maintained in a database system to correlate and summarize base data which records real world facts. As base data changes, derived data needs to be recomputed. This is often implemented by writing active rules that are triggered by changes to base data. In a system with rapidly changing base data, a database with a standard rule system may consume most of its resources running rules to recompute data. This paper presents the rule system implemented as part of the STandard Real-time Information Processor (STRIP). The STRIP rule system is an extension of SQL3-type rules that allows groups of rule actions to be batched together to reduce the total recomputation load on the system. In this paper we describe the syntax and semantics of the STRIP rule system, present an example set of rules to maintain stock index and theoretical option prices in a program trading application, and report the results of experiments performed on the running system. The experiments verify that STRIP's rules allow much more efficient derived data maintenance than conventional rules without batching.	central processing unit;computer data storage;database;event condition action;experiment;information processor;mathematical optimization;real-time data;rule 90;window function	Brad Adelberg;Hector Garcia-Molina;Jennifer Widom	1997		10.1145/253260.253287	real-time computing;computer science;valuation of options;data mining;database	DB	-30.7586522425344	2.5338927670479734	117128
968ae898fbf251c7d55a8e7a2e8a8a4d8a626188	query processing using negative and temporal tuples in stream query engines	temporal tuples;developed stream scheme;stream monotonicity classification;primary key constraint;stream monotonicity;full query plan;different type;query processing;negative event;stream engine;stream query engine;individual stream operator	In this paper, we analyze how stream monotonicity classification can be adopted for the introduced developed model, which processes both temporal and negative events. As we show, information about stream monotonicity can be easily used to optimize individual stream operators as well as a full query plan. Comparing our stream engine with such engines as CEDR, STREAM and PIPES we demonstrate how a primary key constraint can be used in different types of the developed stream schemes. We implemented all of the above techniques in StreamAPAS.	data model;mathematical optimization;named pipe;query plan;relational database;stream cipher;streaming media;temporal database;unique key	Marcin Gorawski;Aleksander Chrószcz	2009		10.1007/978-3-642-28038-2_6	data stream clustering;computer science;multiple description coding;data mining;database;data stream mining;information retrieval	DB	-31.613768658790082	2.8090574128409305	117518
938fa830829a6871da1cadb57c0d2f1c2c179a85	capability criteria for marketing decision support systems			common criteria;marketing decision support system	Martin D. Goslar	1986	J. of Management Information Systems			HPC	-30.90934531753504	-9.311314571410197	117629
5f4463b224e1e1d0b679f31f5d1b92cf5e59e8cd	location fingerprint database filter algorithm based on multi-mapping data structure		The key to improve the precision of location fingerprinting algorithm is to reduce the errors between the position vectors in online locating process and the tag vector of the database in offline process. In view of the large errors caused by the construction of locating tags built from a single vector in traditional offline database, this paper proposed a kind of locating mapping tags for establishing the multi-mapping data structure and the offline database, which adapt better the distribution characteristics of RSSI signals in WIFI. Experiments showed that the offline database constructed by the methods effectively match the current actual position, reduce errors, and significantly improve the matching accuracy.	application programming interface;data structure;database;fingerprint (computing);laptop;network interface controller;online and offline;peterson's algorithm	Xun Li;E Teston	2018	Computer and Information Science	10.5539/cis.v11n4p29	data mapping;database;algorithm;data structure;computer science;position (vector)	DB	-26.0441451042465	-0.42884651138203217	117638
3e17018496f909d80e8daf26d826812178e522b6	environmental emergency management supported by knowledge modelling techniques	emergency management;knowledge modelling	At present, complex telematics infrastructures are used to get information about the performance of natural or artificial installations that need to be supervised. However, a forward step towards intelligent assistance to emergency management requires a strategic processing layer complementing the services of the available information systems. This additional processing layer may elaborate on the basic information and provide conclusions and explanations understandable by the human experts. This paper proposes the use of advanced knowledge modelling techniques to develop systems with such an intelligent assistance capability. Starting from the identification of the userdsystem interaction needs, a generic knowledge model is described integrating the knowledge pieces required to perform the reasoning processes, while a simulator of the physical system is necessary to calibrate the model. The feasibility of the approach is illustrated with an example in the domain of emergencies caused by industrial accidents.		Josefa Z. Hernández;Juan Manuel Serrano	2001	AI Commun.		simulation;computer science;knowledge management;artificial intelligence;machine learning;management science;emergency management	AI	-31.558449545154215	-7.513388692348028	117844
f88ff6f814dbb0fd8b549b02c17e8c68723ec0a0	performance aspects of migrating a web application from a relational to a nosql database		There are many studies which discuss the problem of using various NoSQL databases and compare their efficiency thus confirming their usefulness and performance quality. However, there are very few studies dealing with the problem of replacing data storage for systems currently working. This lack has become a motivating factor to examine how difficult and laborious it is to move an existing, regularly used application, based on the relational environment to a non-relational data structure. The difficulty of carrying out a data migration process, the scope of changes which would have to be done in the existing environment and the efficiency of an application while using new data structures were considered in the presented research. As an example one of on–line games, being a good representative for popular web applications, was chosen.	computer data storage;data structure;database;nosql;web application	Katarzyna Harezlak;Robert Skowron	2015		10.1007/978-3-319-18422-7_9	data mining;xml database;database;world wide web	DB	-32.89456210943398	0.7274521121598864	117929
9124991bc560c3ac6ab12001fbb2765056975ee1	an artificial intelligence approach to understanding natural language	artificial intelligent;natural language		artificial intelligence;natural language	Jacques Pitrat	1988			rule-based system;natural language processing;artificial architecture;artificial intelligence;artificial intelligence system;linguistics;symbolic artificial intelligence;commonsense knowledge;artificial psychology;artificial intelligence, situated approach	AI	-27.549514992894835	-8.398941238455816	117936
e5a6ef3f660fc203b98de1081d938d727a5d1a95	efficient historical r-trees	window query retrieval;interval query performance;history;timestamps;query processing;information retrieval;efficiency;software performance evaluation;tree data structures;hr trees;efficiency historical r trees hr trees hr tree spatio temporal access method window query retrieval timestamps tree branch sharing updates space requirements interval query performance;software performance evaluation tree data structures temporal databases query processing;system performance;windows;historical r trees;indexing;updates;tree branch sharing;history spatial databases information retrieval windows indexing computer science system performance concrete;spatial databases;space requirements;temporal databases;experimental evaluation;computer science;spatio temporal access method;access method;hr tree;concrete	The Historical R-tree is a spatio-temporal access method aimed at the retrieval of window queries in the past. The concept behind the method is to keep an R-tree for each timestamp in history, but allow consecutive trees to share branches when the underlying objects do not change. New branches are only created to accommodate updates from the previous timestamp. Although existing implementations of HR-trees process timestamp (window) queries very efficiently, they are hardly applicable in practice due to excessive space requirements and poor interval query performance. This paper addresses these problems by proposing the HR+-tree, which occupies a small fraction of the space required for the corresponding HR-tree (for typical conditions about 20%), while improving interval query performance several times. Our claims are supported by extensive experimental evaluation.	algorithm;interval arithmetic;online and offline;overhead (computing);r-tree;requirement;temporal database	Yufei Tao;Dimitris Papadias	2001		10.1109/SSDM.2001.938554	timestamp;search engine indexing;concrete;computer science;theoretical computer science;data mining;database;computer performance;efficiency;temporal database;tree;access method	DB	-27.00596523398791	1.4339464036601455	118019
ea43dc263fb084f5de42864ebf5ec4ae70876165	a computing with words framework for ambient intelligence	home computing;ambient intelligence vision type 2 fuzzy logic case based reasoning fuzzy composite concept life style improvement cooperative guide home human communication human like reasoning cwws framework computing with words framework intelligent space high level communication home environment;computing with words;ambient intelligence;fuzzy logic;qa75 electronic computers computer science;general type 2 fuzzy logic systems ambient intelligence computing with words;general type 2 fuzzy logic systems;home computing ambient intelligence case based reasoning fuzzy logic;pragmatics frequency selective surfaces cognition mathematical model fuzzy sets fcc fuzzy logic;case based reasoning	One of the challenges for the fast advancing ambient intelligence vision is to maintain the perception of the future home being a safe place where the inhabitants relax, enjoy and feel comfortable. In a home environment, the role of technology is approached skeptically, hence, there is a need to provide high-level communication between the users and the intelligent space so that the users get accustomed to what technology has to offer. In this paper, we introduce a Computing with Words (CWWs) framework which provides human-like reasoning via abstraction and high-level description of thoughts, feelings, etc. This framework can be considered as an exocortex as it aids the human thinking outside the bio-brain. The CWWs paradigm aims to establish high level home-human communication, which is necessary for people to perceive the technology as a cooperative guide and an improvement on their life styles.	ambient intelligence;british informatics olympiad;case-based reasoning;commonsense reasoning;composite data type;computing with words and perceptions;exocortex;high- and low-level;high-level programming language;jaccard index;natural language;programming paradigm;rule-based system;similarity measure;utf-8	Aysenur Bilgin;Hani Hagras;Areej Malibari;Daniyal M. Al-Ghazzawi;J. Mohammed	2013	2013 IEEE International Conference on Systems, Man, and Cybernetics	10.1109/SMC.2013.492	fuzzy logic;case-based reasoning;fuzzy electronics;ambient intelligence;computer science;knowledge management;artificial intelligence;neuro-fuzzy;machine learning	Robotics	-23.95056673030409	-9.434194634549645	118110
facedf6f1efc79b4b92356fc4b1ca14141f3c9b1	combining heterogeneous temporal information: a case study	temporal representation;temporal interval;quantitative constraint;quantitative temporal information;specialized system;constraint propagation;algorithmic level;homogeneous framework;heterogeneous temporal information;metric information;case study;temporal reasoning	In this paper we present a specialized system for temporal reasoning representing possibly uncertain qualitative and quantitative constraints relative to temporal intervals in a homogeneous framework. The underlying logic is that of Allen, which we have extended to include quantitative temporal information. Intervals are still assumed as primitive entities while, to represent metric information, time points are introduced only at the algorithmic level. We illustrate the properties of the temporal representation, the algorithms used for constraint propagation, and their complexity. We show how the system can be used to schedule manufacturing operations in a small factory.		Silvana Badaloni	1999	AI EDAM		temporal logic;computer science;artificial intelligence;process control;scheduling;expert system;algorithm	AI	-21.2057725857577	1.8926700749484087	118233
c24b45cc23dc815d10062dbab683eb954ceb044a	using brandom's framework to do peirce's normative science: pragmatism as the game of harmonizing assertions?	abduction;interfase usuario;user interface;conceptual analysis;analisis conceptual;abduccion;interface utilisateur;analyse conceptuelle	I introduce Robert Brandom's Inferentialism, which he calls a rationalist expressivist form of pragmatism, relating it to C.S. Peirce's unfinished project of Normative Science with its method of pragmatism as the logic of abduction, and suggest how Brandom's notion of a game (which I call Harmonizing Assertions) might serve as an effective methodological instrument in conceptual structures research, for its ultimate challenge of the human-computer tool interface.		Mary Keeler	2004		10.1007/978-3-540-27769-9_16	computer science;artificial intelligence;operating system;mathematics;user interface;algorithm	Logic	-24.67097350076843	-7.215923006261318	118276
465bd97a520552fb4b529563aa6fa84f91814d3e	knowledge-based anytime computation	real time decision making;knowledge based system;real time;anytime algorithm;intelligent system;knowledge base	This paper describes a real-time decision-making model that combines the expressiveness and exibility of knowledge-based systems with the real-time advantages of anytime algorithms. Anytime algorithms ooer a simple means by which an intelligent system can trade oo computation time for quality of results. Previous attempts to develop knowledge-based anytime algorithms failed to produce consistent, predictable improvement of quality over time. Without performance prooles, that describe the output quality as a function of time, it is hard to exploit the exibility of anytime algorithms. The model of progressive reasoning that is presented here is based on a hierarchy of reasoning units that allow for gradual improvement of decision quality in a predictable manner. The result is an important step towards the application of knowledge-based systems in time-critical domains.	anytime algorithm;artificial intelligence;computation;decision quality;knowledge-based systems;quality of results;real-time locating system;real-time transcription;time complexity;window of opportunity	Abdel-Illah Mouaddib;Shlomo Zilberstein	1995			knowledge base;computer science;knowledge management;artificial intelligence;machine learning	AI	-19.48087324306773	-6.637671594355155	118417
439ae02582e839ef6866280e5e6c5416ded3c02e	subject-oriented semantic knowledge warehouse (sskw) to support cognitive dss	journal article	The communication between cognitive DSS and data warehouse tends to be inefficient due to their contradictory knowledge/data oriented nature. Data-to-knowledge conversion requires specialized techniques, whereas knowledge-to-data conversion results in loss of knowledge. To address these issues, a subject-oriented semantic knowledge warehouse (SSKW) is proposed, to provide relevant and precise knowledge to CDSS. The SSKW consists of: a) object/process/event/relationship (OPER) model to store domain knowledge in a unified fashion; and, b) a subjective view database, containing opinions of stakeholders about various OPER knowledge elements. A case study to compare the performance of the SSKW-based CDSS against a DW-based CDSS is presented. The results show that SSKW improves communication efficiency, provides relevant and precise domain knowledge to CDSS in less decision cycles, minimizes the loss of knowledge, and helps decision maker to quickly grasp the decision situation through its human-centric nature.		Tasneem Memon;Jie Lu;Farookh Khadeer Hussain;Rajan Rauniyar	2013		10.1007/978-3-642-41030-7_20	computer science;knowledge management;data science;data mining	HCI	-33.30189336260553	-7.423347152594379	118586
3848c1e25b4c086d7dd6c58c07a55569e57835dc	customizing cooperative office procedures by planning	suitable office knowledge representation;special attention;office worker;knowledge base;electronic organization handbook;cooperative office procedures;maintainable structure;cooperative office procedure	Cooperative office procedures coordinate the flow of information in complex distributed work, in which several office workers are involved. An approach for planning cooperative office procedures is presented, focussing on the requirements, possibilities and limitations of planning activities. Special attention is given to the elaboration of a suitable office knowledge representation and a maintainable structure of the knowledge base, the electronic organization handbook (ELO).	automated planning and scheduling;knowledge base;knowledge representation and reasoning;requirement	Rainer Lutze	1988		10.1145/45410.45418	knowledge base;computer science;knowledge management;office administration;artificial intelligence	AI	-32.793482269783325	-9.620979390111914	118614
dea3f9e44c75bac9b6d53420bd393d7dd3a6614e	cross-national decision making in a group support systems environment	group support;cross-national decision;systems environment			Nasrin Rahmati	1997			r-cast;computer science	ECom	-30.968677477386333	-9.313873580180752	118782
cd83ca3b85ab079df84d947865b24563fe7662f1	investigations on path indexing for graph databases	computer science and information systems	Graph databases have become an increasingly popular choice for the management of the massive network data sets arising in many contemporary applications. We investigate the effectiveness of path indexing for accelerating query processing in graph database systems, using as an exemplar the widely used open-source Neo4j graph database. We present a novel path index design which supports efficient ordered access to paths in a graph dataset. Our index is fully persistent and designed for external memory storage and retrieval. We also describe a compression scheme that exploits the limited differences between consecutive keys in the index, as well as a workload-driven approach to indexing. We demonstrate empirically the speed-ups achieved by our implementation, showing that the path index yields query run-times from 2x up to 8000x faster than Neo4j. Empirical evaluation also shows that our scheme leads to smaller indexes than using general-purpose LZ4 compression. The complete stand-alone implementation of our index, as well as supporting tooling such as a bulk-loader, are provided as open source for further research and development.	general-purpose modeling;graph database;lz4;neo4j;open-source software;persistent data structure	Jonathan M. Sumrall;George H. L. Fletcher;Alexandra Poulovassilis;Johan Svensson;Magnus Vejlstrup;Chris Vest;Jim Webber	2016		10.1007/978-3-319-58943-5_43	wait-for graph;computer science;theoretical computer science;data mining;database;distributed computing;graph database	DB	-32.134355214003286	1.0475843878646698	118790
1bc73a4d1a5d47c0fdde209b21ccd6d83dd4fc43	sut-miner: a knowledge mining and managing system for medical databases	databases;semi automatic trigger creation;association mining;management system;medical administrative data processing;knowledge mining;association mining engine knowledge mining knowledge management system medical databases semi automatic trigger creation sut miner system;association rules knowledge mining medical databases triggers;diabetes;association rules;data mining;rapid prototyping;association rule;triggers;cognition;knowledge management system;sut miner system;knowledge management databases data mining medical diagnostic imaging association rules application software technology management data engineering prototypes engines;medical databases;encoding;medical administrative data processing data mining deductive databases;association mining engine;medical diagnostic imaging;deductive databases	Knowledge is a valuable asset to most organizations as a substantial source to support better decisions. Recently there has been an increasing interest in devising database and data mining technologies to automatically induce knowledge from biomedicine, clinical and health data. Most work had adopted a single technique in the knowledge induction process. We propose a knowledge mining system as an integrated environment storing a repertoire of tools for discovering strong and useful knowledge. We demonstrate the usefulness aspect through the semi-automatic trigger creation for the medical database. A rapid prototyping of association mining engine is also presented in the paper.	association rule learning;comparison of command shells;data mining;database;rapid prototyping;semiconductor industry;system under test	Kittisak Kerdprasop;Nittaya Kerdprasop	2009	2009 20th International Workshop on Database and Expert Systems Application	10.1109/DEXA.2009.24	association rule learning;computer science;knowledge management;data science;knowledge-based systems;knowledge engineering;open knowledge base connectivity;data mining;database;knowledge extraction;domain knowledge	ML	-32.89907926658041	-6.4340213312653365	118851
4d8f76b5f88a41970a4d601273adab1fa9a4c453	adaptive selection of reactive/deliberate planning for a dynamic environment	systeme temps reel;robot movil;integrating reactive and deliberate planning;vision ordenador;spreading activation model;multiagent system;real world agent;autonomous system;real time;intelligence artificielle;sistema autonomo;computer vision;autonomic system;dynamic environment;planificacion;robot mobile;systeme autonome;multi agent planning;spreading activation;artificial intelligence;planning;vision ordinateur;real time system;sistema tiempo real;inteligencia artificial;planification;sistema multiagente;moving robot;systeme multiagent;active vision	This paper proposes mad evaluates a new real-time reactive planning approach for a dynamic environment. In addition to having the features of conventional real-time reactive planning, which can react in a dynamic environment, our planning can perform deliberate planning appropriately. The proposed planning uses three kinds of agents: behavior agents that control simple behavior, planning agents that make plans to achieve their own goals, and behavior-selection agents that intermediate between behavior agents and planning agents. They coordinate a plan in an emergent way for the planning system as a whole. We confirmed the effectiveness of our planning by means of a simulation. Furthermore, we implemented an active-vision system and used it to verify the real-world efficiency of our planning. © 1998 Elsevier Science B.V. All rights reserved.	active vision;emergence;norm (social);reactive planning;real-time clock;real-time operating system;simulation	Satoshi Kurihara;Shigemi Aoyagi;Rikio Onai;Toshiharu Sugawara	1998	Robotics and Autonomous Systems	10.1016/S0921-8890(98)00030-X	computer vision;simulation;real-time operating system;computer science;artificial intelligence;spreading activation;reactive planning	AI	-22.358489129743738	-7.632458632475993	118940
c99c601d7fb07bbefc69059c11e128330e10890b	big r: large-scale analytics on hadoop using r	user interfaces data analysis data visualisation pattern clustering public domain software query languages;partitioned execution large scale analytics data analysis big r data visualization data manipulation hadoop cluster r user interface r semantics r primitives data exploration query language;machine learning;big data;big data delays data visualization data mining vectors semantics database languages;machine learning big data mapreduce;mapreduce	As the volume of available data continues to rapidly grow from a variety of sources, scalable and performant analytics solutions have become an essential tool to enhance business productivity and revenue. Existing data analysis environments, such as R, are constrained by the size of the main memory and cannot scale in many applications. This paper introduces Big R, a new platform which enables accessing, manipulating, analyzing, and visualizing data residing on a Hadoop cluster from the R user interface. Big R is inspired by R semantics and overloads a number of R primitives to support big data. Hence, users will be able to quickly prototype big data analytics routines without the need of learning a new programming paradigm. The current Big R implementation works on two main fronts: (1) data exploration, which enables R as a query language for Hadoop and (2) partitioned execution, allowing the execution of any R function on smaller pieces of a large dataset across the nodes in the cluster.	algorithm;apache hadoop;apache spark;big data;computation;computer data storage;data science;experiment;machine learning;mapreduce;parallel computing;programming paradigm;prototype;query language;r language;scalability;software repository;usability;user experience;user interface	Oscar D. Lara Yejas;Weiqiang Zhuang;Adarsh Pannu	2014	2014 IEEE International Congress on Big Data	10.1109/BigData.Congress.2014.88	programming with big data in r;analytics;computer science;data science;data mining;database	DB	-32.9323588573201	-0.22131784577825803	118945
7341e4f8e9ddb3a47a0155a3d131fbbdfd3b496c	putting it all together: improving display integration in ecological displays	computadora;abstraction hierarchy;ergonomia;ordinateur;integration information;hombre;large scale system;ergonomie;computer;scaling up;ecran visualisation;power plant;pantalla visualizacion;information integration;fault detection;integracion informacion;human;process control;network management;display screen;ergonomics;fault detection and diagnosis;problem solving;fault diagnosis;homme;time integration	Computer displays are being designed for increasingly larger industrial systems. As the application domain scales up, maintaining integration across different kinds of views becomes more challenging. This paper presents the results of a study of three different approaches to integration based on the spatial and temporal proximity of related information objects. The domain used for evaluation was a simulation of an industry-scale conventional power plant. All three displays were ecological displays developed using an abstraction hierarchy analysis. Views were integrated in a high-space/low-time, low-space/high-time, and high-space/high-time integration of means-end related objects. During a fault detection and diagnosis task, it was found that a low level of integration, high-space/ low-time, provided the fastest fault detection time. However, the most integrated condition, high-space/high-time, resulted in the fastest and most accurate fault diagnosis performance. Actual or potential applications of this research include computer displays for large-scale systems such as network management or process control, for which problem solving is critical and integration must be maintained.	application domain;computer monitor;dna integration;fastest;fault detection and isolation;large;numerical methods for ordinary differential equations;physical object;power plants;problem solving (mental process);simulation	Catherine M. Burns	2000	Human factors	10.1518/001872000779656471	psychology;network management;power station;simulation;human–computer interaction;computer science;engineering;artificial intelligence;human factors and ergonomics;information integration;process control;fault detection and isolation;mechanical engineering	HCI	-23.042744876154824	-4.519526281675481	119027
8444ab8cbc7089c406e7f38ab9e0f688fcb7deb6	datometry hyper-q: bridging the gap between real-time and historical analytics	query processing;financial services;big data;data analytics;data virtualization	Wall Street's trading engines are complex database applications written for time series databases like kdb+ that uses the query language Q to perform real-time analysis. Extending the models to include other data sources, e.g., historic data, is critical for backtesting and compliance. However, Q applications cannot run directly on SQL databases. Therefore, financial institutions face the dilemma of either maintaining two separate application stacks, one written in Q and the other in SQL, which means increased IT cost and increased risk, or migrating all Q applications to SQL, which results in losing the inherent competitive advantage on Q real-time processing. Neither solution is desirable as both alternatives are costly, disruptive, and suboptimal. In this paper we present Hyper-Q, a data virtualization plat- form that overcomes the chasm. Hyper-Q enables Q applications to run natively on PostgreSQL-compatible databases by translating queries and results on the fly. We outline the basic concepts, detail specific difficulties, and demonstrate the viability of the approach with a case study.	backtesting;bridging (networking);coupling (computer programming);data infrastructure;database;federation (information technology);hyper-heuristic;hyper-threading;on the fly;plug-in (computing);postgresql;query language;real-time clock;real-time transcription;sql;time series;vendor lock-in;kdb+	Lyublena Antova;Rhonda Baldwin;Derrick Bryant;Tuan Cao;Michael Duller;John Eshleman;Zhongxian Gu;Entong Shen;Mohamed A. Soliman;F. Michael Waas	2016		10.1145/2882903.2903739	big data;financial services;computer science;data virtualization;query by example;data mining;database;data analysis;programming language;world wide web	DB	-33.198774245061564	0.2956133481034772	119221
240bc0758560ec09724718defc35be5020b024d3	refiner: a case-based differential diagnosis aide for knowledge acquisition and knowledge refinement			knowledge acquisition;refinement (computing)	Sunil Sharma;Derek H. Sleeman	1988			knowledge acquisition;differential diagnosis;data mining;computer science;knowledge management	AI	-28.221254107604963	-7.534620081617364	119364
26dd163ee6a0e75659a7f1485d1d442852d74436	using knowledge representation and reasoning tools in the design of robots		The paper describes the authors’ experience in using knowledge representation and reasoning tools in the design of robots. The focus is on the systematic construction of models of the robot’s capabilities and its domain at different resolutions, and on establishing a clear relationship between the models at the different resolutions.	diagram;knowledge representation and reasoning;non-monotonic logic;partially observable markov decision process;refinement (computing);robot	Mohan Sridharan;Michael Gelfond	2016			artificial intelligence;machine learning;model-based reasoning;knowledge representation and reasoning;robot;computer science	AI	-27.161598580040213	-7.986899955320329	119370
1113aa87579bc7a80a3a9be929e4ab6a5b63ff51	mining modern repositories with elasticsearch	elasticsearch;scalability;agile data	"""Organizations are generating, processing, and retaining data at a rate that often exceeds their ability to analyze it effectively; at the same time, the insights derived from these large data sets are often key to the success of the organizations, allowing them to better understand how to solve hard problems and thus gain competitive advantage. Because this data is so fast-moving and voluminous, it is increasingly impractical to analyze using traditional offline, read-only relational databases.   Recently, new """"big data"""" technologies and architectures, including Hadoop and NoSQL databases, have evolved to better support the needs of organizations analyzing such data. In particular, Elasticsearch - a distributed full-text search engine - explicitly addresses issues of scalability, big data search, and performance that relational databases were simply never designed to support. In this paper, we reflect upon our own experience with Elasticsearch and highlight its strengths and weaknesses for performing modern mining software repositories research."""	apache hadoop;big data;dash (cryptocurrency);elasticsearch;nosql;online and offline;read-only memory;real-time clock;relational database;scalability;software repository;web search engine	Oleksii Kononenko;Olga Baysal;Reid Holmes;Michael W. Godfrey	2014		10.1145/2597073.2597091	scalability;computer science;operating system;data mining;database;world wide web	ML	-33.35339651906185	0.13864758273085254	119431
2936b953c3e0cbc06719f13f7b1a610f531b3eaa	an early detection of semantic conflicts between aspects: a model analysis based technique.	model analysis;early detection			F. Tessier;Mourad Badri;Linda Badri	2004			computer science;artificial intelligence;data mining;operations research	SE	-30.474700447785242	-9.005691048535121	119870
453cbf069c3615ed354ec1bd20da8598598904c3	the power of modeling - a response to pddl2.1	domain model;artificial intelligent	In this commentary I argue that although pddl2.1 is a very useful standard for the planning competition, its design does not properly consider the issue of domain modeling. Hence, I would not advocate its use in specifying planning domains outside of the context of the planning competition. Rather, the field needs to explore different approaches and grapple more directly with the problem of effectively modeling and utilizing all of the diverse pieces of knowledge we typically have about planning domains.	am broadcasting;algorithm;automated planning and scheduling;grapple;specification language	Fahiem Bacchus	2003	J. Artif. Intell. Res.	10.1613/jair.1993	simulation;computer science;artificial intelligence;domain model	AI	-21.001632516694002	-7.8888326731502545	120007
bf1e9698ab1afd3b3c487e651c51a4536bf6df06	study on key technology of hpsin-based vector geo-data online service	linking mechanism;vector geo data;lossless reconstruction algorithm;hpsin	"""P2P technology can avoid """"single point of failure"""" and """"hot spots bottleneck"""" problems, which exist in traditional centralized system of spatial information. Hybrid P2P Spatial Indexing Network combines distributed Quad-Tree with DHT-based Chord network to maintain both query efficiency and system load balance. In this paper, key technology of HPSIN based distributed vector geo-data online service is studied, the pattern of vector geo-data organization based on Linking Mechanism, segmentation and lossless reconstruction are proposed. This novel organization pattern can be used to form a loosely and globally distributed topology. Segmentation and lossless reconstruction method takes advantage of linking information, to reconstruct the damaged Geometries lossless. Comparative experiment shows that the organization and the associated algorithms are lossless and reliable."""		Jiali Feng;Nan Jiang;Bin Hu;Jiagao Wu;Zhiqiang Zou	2012	Trans. Edutainment	10.1007/978-3-642-29050-3_25	computer science;theoretical computer science;data mining;distributed computing	HCI	-29.393891373711224	-0.05841417092142387	120042
d4e7d6c80024871d7e54b22e87dab5c728b3b01b	fuzzy logic-based event notification in sparse manets	filtering;disruption tolerant networking;fuzzy reasoning;fuzzy logic based event notification;computational intelligence;fuzzy reasoning fuzzy logic based event notification sparse manet ad hoc info ware project delay tolerant event notification service mobile ad hoc networks complex matching semantics;fuzzy set theory;mobile radio ad hoc networks fuzzy logic fuzzy set theory;computer networks;fuzzy logic;delay tolerant event notification service;mobile ad hoc networks;data structures;mobile radio;event notification;intelligent systems;complex matching semantics;subscriptions;ad hoc networks;mobile ad hoc network;sparse manet;fuzzy logic subscriptions disruption tolerant networking ad hoc networks filtering intelligent systems computational intelligence fuzzy reasoning computer networks data structures;data structure;ad hoc info ware project	In the ad-hoc info ware project, we develop a delay tolerant event notification service for sparse mobile ad-hoc networks for emergency and rescue operations. In most event notification solutions, subscriptions are formed with crisp values or crisp value ranges. Filtering mechanisms do not take into account more expressive subscriptions in terms of approximate predicates and complex aggregating relations among them. However, in emergency scenarios subscribers' interests often have gradual nature and subjective measure. Therefore, we design an intelligent event notification system allowing uncertainties to be modeled and complex matching semantics to be processed by fuzzy reasoning. Requiring more computational efforts, fuzzy logic introduces performance penalties in the whole network. We have developed a new subscription data structure and filtering algorithms, and evaluated and optimized it for runtime and space efficiency.	approximation algorithm;data structure;event (computing);fuzzy logic;hoc (programming language);notification service;notification system;predicate (mathematical logic);sparse matrix;warez	Anna K. Lekova;Katrine Stemland Skjelsvik;Thomas Plagemann;Vera Goebel	2007	21st International Conference on Advanced Information Networking and Applications Workshops (AINAW'07)	10.1109/AINAW.2007.194	mobile ad hoc network;data structure;computer science;artificial intelligence;theoretical computer science;data mining;database;distributed computing;computer security;computer network	DB	-23.845245103150415	3.994200220704258	120186
4eb666c13c6c13f61f6150bc97c7c9675cfbe5d0	indexing continuously changing data with mean-variance tree	sensibilidad contexto;arbre r;location based services;dato observacion;reseau capteur;streaming;context aware;mise a jour;informatique mobile;location based service;query processing;index updating;data stream;interrogation base donnee;index structure;database;interrogacion base datos;base dato;spatial index;r tree;sensor network;mean variance;actualizacion;arbol r;transmission en continu;red sensores;data updates;indexing;sensor networks;mean variance tree;indexation;indizacion;update processing;base de donnees;sensor array;traitement de la requete;query and update processing;tratamiento pregunta;donnee observation;transmision fluyente;sensibilite contexte;mobile computing;database query;observation data;queries;data streaming;updating;variance;variancia	Traditional spatial indexes like R-tree usually assume the database is not updated frequently. In applications like location-based services and sensor networks, this assumption is no longer true since data updates can be numerous and frequent. As a result these indexes can suffer from a high update overhead, leading to poor performance. In this paper we propose a novel index structure, the Mean Variance Tree (MVTree), which is built based on the mean and variance of the data instead of the actual data values that can change continuously. Since the mean and variance are relatively stable features compared to the actual values, the MVTree significantly reduces the index update cost. The mean and the variance of the data item can be dynamically adjusted to match the observed fluctuation of the data. Our experiments show that the MVTree substantially improves index update performance while maintaining satisfactory query performance.		Yuni Xia;Reynold Cheng;Sunil Prabhakar;Shan Lei;Rahul Shah	2008	IJHPCN	10.1504/IJHPCN.2008.022302	wireless sensor network;computer science;location-based service;data mining;database;mobile computing;world wide web	ML	-25.837246566686794	2.881557380281416	120278
dc3ff16419c8e94520550419348226174cffb798	local information processing for decision making in decentralised sensing networks	sistema experto;procesamiento informacion;robotics;data fusion;prise decision;decentralized system;information flow;process monitoring;fusion donnee;information processing;sistema descentralizado;sensor nodes;robotica;robotique;systeme decentralise;systeme expert;traitement information;fusion datos;toma decision;expert system	This paper describes consequences of local information processing for decision making in decentralised systems of sensor nodes. In decentralised data fusion systems, nodes take decisions based on information acquired locally. The ability of such nodes to fuse or combine information is linked to network organisation. Earlier work demonstrates a problem of inconsistency which arises given cyclic information flow in decentralised systems where nodes combine global information. This work shows how this inconsistency limits the decision making capabilities of the sensor nodes. Consequences for real-world systems using decentralised processes for decision making in process monitoring, tracking and aviation are discussed.	information processing	Simukai W. Utete	1998		10.1007/3-540-64582-9_799	information flow;information processing;decentralised system;computer science;artificial intelligence;data mining;sensor fusion;robotics;operations research;expert system	Robotics	-21.054906899581802	-2.708574575738799	120287
7d1d9c8a559e0a643358d9b43ebfe7c0deec5ee0	a web-based spatial data access system using semantic r-trees	spatial data;system performance;semantic information;web based gis;real world application;spatial data access;semantic r tree;web based system;world wide web;system architecture;high performance;data structure;spatial access method;proxy server;graphic information system	With the increasing use of geographical data in real-world applications, Geographic Information Systems (GISs) have recently emerged as a fruitful area for research. In order to provide information to a multitude of users, the World Wide Web (WWW) techniques have been integrated into GISs. A high-performance webbased GIS, called TerraFly, has been developed in order to provide web-based GIS accesses to the general public. The design of TerraFly considers two major aspects: (1) the system architecture including client, database server, proxy server and information server; and (2) the semantic R-tree data structure and semantic queries. The system architecture utilizes the existing resources to achieve maximum performance by using the “Internally Distributed Multithreading (IDMT)” technique. The spatial access method, semantic R-trees, is used to search for an object, based on both spatial and semantic information. System performance results are presented and analyzed. Reducing network traffic to achieve faster response to users is also discussed.	data access;data structure;database server;experiment;geographic information system;ibm notes;map;network traffic control;proxy server;r-tree;range query (data structures);requirement;server (computing);simultaneous multithreading;spatial database;systems architecture;terrafly;thread (computing);tree (data structure);www;web application;world wide web	Shu-Ching Chen;Xinran Wang;Naphtali Rishe;Mark Allen Weiss	2004	Inf. Sci.	10.1016/j.ins.2003.07.019	web service;enterprise gis;semantic computing;data web;web mapping;data structure;semantic search;semantic grid;web standards;computer science;semantic web;web navigation;social semantic web;web page;data mining;semantic web stack;database;spatial analysis;web intelligence;programming language;world wide web;semantic analytics;web server	DB	-31.677086011270777	1.1344139968474443	120471
17aa2a67839e967b9a5c5ac70bff86c85b5e21e6	tailoring the presentation of plans to users' knowledge and capabilites	user needs;user preferences;preparacion serie fabricacion;structural complexity;planificacion;planning;process planning;planification;preparation gamme fabrication;user model	Tailoring advice to a user means finding a plan by which she can reach her goal, and supplying the missing knowledge that she needs to successfully execute the plan. The paper presents a method to determine the kind and amount of this missing knowledge for an already generated domain plan. We show that both a user’s knowledge and his capabilities to perform actions must be taken into account when deciding on a plan presentation that is suitable for him. We also argue that it may be useful to consider issues of plan presentation already during the planning process, and show how this can be accomplished in a planning system.	automated planning and scheduling	Detlef Küpper;Alfred Kobsa	2003		10.1007/978-3-540-39451-8_44	planning;structural complexity;simulation;user modeling;computer science;knowledge management;database	AI	-29.602266331462122	-5.002243416711041	120655
d31e01bfd2e0a32832f5a3bd957e821d5d8970af	ecolang - communications language for ecological simulations network	language use;multiagent system;relatorio tecnico;artificial intelligent;intelligent agent;tecnologia de software tecnologia de agentes engenharia electrotecnica electronica e informatica	This document describes the communication language used in one multiagent system environment for ecological simulations, based on EcoDynamo simulator application linked with several intelligent agents and visualisation applications, and extends the initial definition of the language. The agents actions and perceptions are translated into messages exchanged with the simulator application and other agents. The concepts and definitions used follow the BNF notation (Backus et al. 1960) and is inspired in the Coach Unilang language (Reis and Lau 2002).	agent-based model;beta normal form;environment variable;intelligent agent;multi-agent system;simulation	Antánio Pereira	2008	CoRR		computer science;artificial intelligence;intelligent agent	AI	-24.14788651847408	-8.164977081478717	120750
01e027c5c2e5375b88e9776c8d606675f51be827	a distributed graph engine for web scale rdf data	native graph form;sparql query;random walk;rdf data;sparql query processing;bitmap matrix;web scale rdf data;rdf graph;index data;memory-based graph engine	Much work has been devoted to supporting RDF data. But state-of-the-art systems and methods still cannot handle web scale RDF data effectively. Furthermore, many useful and general purpose graph-based operations (e.g., random walk, reachability, community discovery) on RDF data are not supported, as most existing systems store and index data in particular ways (e.g., as relational tables or as a bitmap matrix) to maximize one particular operation on RDF data: SPARQL query processing. In this paper, we introduce Trinity.RDF, a distributed, memory-based graph engine for web scale RDF data. Instead of managing the RDF data in triple stores or as bitmap matrices, we store RDF data in its native graph form. It achieves much better (sometimes orders of magnitude better) performance for SPARQL queries than the state-of-the-art approaches. Furthermore, since the data is stored in its native graph form, the system can support other operations (e.g., random walks, reachability) on RDF graphs as well. We conduct comprehensive experimental studies on real life, web scale RDF data to demonstrate the effectiveness of our approach.	attribute–value pair;bitmap;distributed memory;in-memory database;key-value database;mathematical optimization;reachability;real life;resource description framework;sparql;scalability;trinity;triplestore;usb flash drive	Kai Zeng;Jiacheng Yang;Haixun Wang;Bin Shao;Zhongyuan Wang	2013	PVLDB	10.14778/2535570.2488333	rdf/xml;cwm;turtle;computer science;sparql;rdf;linked data;data mining;database;rdf query language;blank node;information retrieval;rdf schema	DB	-31.56161326949128	3.101280540704792	120830
b51f89671105e50318a0749af20cc9ce1bf3713f	argumentation-based policy analysis for drone systems		The use of drone systems is increasing especially in dangerous environments where manned operations are too risky. Different entities are involved in drone systems' missions and they come along with their vast varieties of specifications. The behaviour of the system is described by its set of policies that should satisfy the requirements and specifications of the different entities and the system itself. Deciding the policies that describe the actions to be taken is not trivial, as the different requirements and specifications can lead to conflicting actions. We introduce an argumentation-based policy analysis that captures conflicts for which properties have been specified. Our solution allows different rules to take priority in different contexts. We propose a decision making process that solves the detected conflicts by using a dynamic conflict resolution based on the priorities between rules. We apply our solution to two case studies where drone systems are used for military and disaster rescue operations.	apply;autonomy;entity;requirement;unmanned aerial vehicle	Erisa Karafili;Emil C. Lupu;Saritha Arunkumar;Elisa Bertino	2017	2017 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computed, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)	10.1109/UIC-ATC.2017.8397414	redundancy (engineering);distributed computing;argumentation theory;computer science;drone;management science;decision-making;conflict resolution;cognition;policy analysis	SE	-19.89891590521557	-8.728312994530599	120868
ca2b8b2a662a64586d521570f3878ee9cb4aba3e	data warehouse benchmarking with dweb		Performance evaluation is a key issue for designers and users of Database Management Systems (DBMSs). Performance is generally assessed with software benchmarks that help, e.g., test architectural choices, compare different technologies or tune a system. In the particular context of data warehousing and On-Line Analytical Processing (OLAP), although the Transaction Processing Performance Council (TPC) aims at issuing standard decision-support benchmarks, few benchmarks do actually exist. We present in this chapter the Data Warehouse Engineering Benchmark (DWEB), which allows generating various ad-hoc synthetic data warehouses and workloads. DWEB is fully parameterized to fulfill various data warehouse design needs. However, two levels of parameterization keep it relatively easy to tune. We also expand on our previous work on DWEB by presenting its new Extract, Transform, and Load (ETL) feature as well as its new execution protocol. A Java implementation of DWEB is freely available on-line, which can be interfaced with most existing relational DMBSs. To the best of our knowledge, DWEB is the only easily available, up-to-date benchmark for data warehouses.	benchmark (computing);bitmap;data drilling;database;experiment;high- and low-level;hoc (programming language);ibm tivoli storage productivity center;java;keyboard technology;management system;many-to-many;mathematical optimization;metamodeling;online analytical processing;online and offline;performance evaluation;relevance;response time (technology);selectivity (electronic);software portability;synthetic data;transaction processing;word lists by frequency	Jérôme Darmont	2017	CoRR	10.4018/978-1-60566-232-9.ch015	computer science;operating system;data mining;database	DB	-33.27702805781985	1.5285032070275524	121704
7834337791d1203945b3f22b23b299f6f816baf2	logical ontology validation using an automatic theorem prover	theorem prover	Ontologies are utilized for a wide range of tasks, like information retrieval/extraction or text generation, and in a multitude of domains, such as biology, medicine or business and commerce. To be actually usable in such real-world scenarios, ontologies usually have to encompass a large number of factual statements. However, with increasing size, it becomes very difficult to ensure their complete correctness. This is particularly true in the case when an ontology is not hand-crafted but constructed (semi)automatically through text mining, for example. As a consequence, when inference mechanisms are applied on these ontologies, even minimal inconsistencies oftentimes lead to serious errors and are hard to trace back and find. This paper addresses this issue and describes a method to validate ontologies using an automatic theorem prover and MultiNet axioms. This logic-based approach allows to detect many inconsistencies, which are difficult or even impossible to identify through statistical methods or by manual investigation in reasonable time. To make this approach accessible for ontology developers, a graphical user interface is provided that highlights erroneous axioms directly in the ontology for quicker fixing.	automated theorem proving;computer;correctness (computer science);display resolution;graphical user interface;information retrieval;knowledge base;microsoft outlook for mac;multinet;natural language generation;ontology (information science);peano axioms;sensor;text mining;textual entailment	Tim vor der Brück;Holger Stenzhorn	2010		10.3233/978-1-60750-606-5-491	computer science;artificial intelligence;theoretical computer science;machine learning;data mining;automated theorem proving;process ontology;algorithm	AI	-30.616822476761225	-4.089607244572432	121706
3ba4494df228f45de485e29cf1423f161f2505a6	on macrostates in complex multi-scale systems	macrostates;complexity;stability;partitions;multi scale systems;contextual emergence;information	A characteristic feature of complex systems is their deep structure, meaning that the definition of their states and observables depends on the level, or the scale, at which the system is considered. This scale dependence is reflected in the distinction of microand macro-states, referring to lower and higher levels of description. There are several conceptual and formal frameworks to address the relation between them. Here, we focus on an approach in which macrostates are contextually emergent from (rather than fully reducible to) microstates and can be constructed by contextual partitions of the space of microstates. We discuss criteria for the stability of such partitions, in particular under the microstate dynamics, and outline some examples. Finally, we address the question of how macrostates arising from stable partitions can be identified as relevant or meaningful.	complex systems;emergence;formal methods;observable	Harald Atmanspacher	2016	Entropy	10.3390/e18120426	combinatorics;discrete mathematics;complexity;information;stability;mathematics;statistics	AI	-20.488691473070446	1.9900235033667681	121852
3af48905c9ff1f8e70c247a7e773602d95fa206e	performance evaluation of a temporal database management system	temporal query language;management system;performance evaluation;temporal database	A prototype of a temporal database management system was built by extending Ingres. It supports the temporal query language TQuel, a superset of Quel, handling four types of database static, rollback, historical and temporal. A benchmark set of queries was run to study the performance of the prototype on the four types of databases. We analyze the results of the benchmark, and identify major factors that have the greatest impact on the performance of the system. We also discuss several mechanisms to address the performance bottlenecks we encountered.	benchmark (computing);ingres;performance evaluation;prototype;quel;query language;relational database management system;temporal database	Ilsoo Ahn;Richard T. Snodgrass	1986		10.1145/16894.16864	database theory;computer science;data mining;management system;database;temporal database;world wide web	DB	-28.763632949448287	3.8517478372622245	121884
208286d01065ad0ad71dea3e37df2feb2d329fd3	distributed query processing using partitioned inverted files	workstations;client server;indexes;information retrieval;distributed computing;filtering;indexation;distributed databases;col;computer science;computer networks;vector space model;distributed processing;distributed architecture;distributed system;information management	In this paper, we study query processing in a distributed text database. The novelty is a real distributed architecture implementation that offers concurrent query service. The distributed system adopts a network of workstations model and the client-server paradigm. The document collection is indexed with an inverted file. We adopt two distinct strategies of index partitioning in the distributed s ystem, namely local index partitioning and global index parti tioning. In both strategies, documents are ranked using the vector space model along with a document filtering technique for fast ranking. We evaluate and compare the impact of the two index partitioning strategies on query processin g performance. Experimental results on retrieval efficiency show that, within our framework, the global index partitioning outperforms the local index partitioning.	algorithm;archive;auxiliary memory;binary space partitioning;central processing unit;client–server model;computer cluster;computer data storage;computer multitasking;database;database index;distributed computing;inverted index;load balancing (computing);parallel computing;programming paradigm;scheduling (computing);server (computing);speedup;workstation	Claudine Santos Badue;Ricardo A. Baeza-Yates;Berthier A. Ribeiro-Neto;Nivio Ziviani	2001		10.1109/SPIRE.2001.10011	filter;database index;sargable;query optimization;workstation;computer science;theoretical computer science;data mining;database;information management;world wide web;vector space model;information retrieval;client–server model	DB	-29.5520655293284	1.3509491420993311	122186
d2cb7987d438f9265a526599cfd81b7f84684d2e	formal knowledge engineering methods for knowledge discovery	knowledge management;data mining;knowledge discovery;knowledge engineering	This paper presents a framework for the integrated use of formal knowledge engineering methods and data mining based knowledge discovery methods. Knowledge is a key enterprise asset, and organizations are adopting both knowledge engineering and knowledge discovery paradigms for better knowledge management and enhanced decision support capability. Although there exists a useful interdependence between these endeavors, not much effort has been focused on using the full potential of one for the other. This paper presents a framework for the integrated use of established formal knowledge engineering methods and knowledge discovery processes with the ultimate intent of better managing the enterprise knowledge life cycle. It provides a brief overview of the knowledge discovery processes, and introduces a class of formal knowledge engineering methods and the perceived role of these methods in supporting the integration between the two worlds of knowledge discovery and knowledge engineering.	knowledge engineering	Satheesh Ramachandran	2002	JIKM	10.1142/S0219649202000455	knowledge base;organizational learning;knowledge integration;software mining;computer science;knowledge management;data science;body of knowledge;mathematical knowledge management;knowledge-based systems;knowledge engineering;open knowledge base connectivity;data mining;procedural knowledge;knowledge extraction;personal knowledge management;commonsense knowledge;knowledge value chain;domain knowledge	ML	-32.32250538251952	-6.442885600010114	122272
71da295c16c6f3d0a25ebe27b2d819bccf1be838	efficient evaluation of generalized tree-pattern queries with same-path constraints	directed acyclic graph;scientific application;query language;xml data;evaluation algorithm;path constraint;asymptotic optimality;partial trees;structural pattern;structural fragments;asymptotically optimal;theoretical analysis;polynomial time;directed acyclic graphs;scientific applications;pattern query;evaluation models;experimental evaluation;flexible querying;inverted list;main memory;evaluation model	Querying XML data is based on the specification of structural patterns which in practice are formulated using XPath. Usually, these structural patterns are in the form of trees (Tree-Pattern Queries – TPQs). Requirements for flexible querying of XML data including XML data from scientific applications have motivated recently the introduction of query languages that are more general and flexible than TPQs. These query languages correspond to a fragment of XPath larger than TPQs for which efficient non-main-memory evaluation algorithms are not known. In this paper, we consider a query language, called Partial Tree-Pattern Query (PTPQ) language, which generalizes and strictly contains TPQs. PTPQs represent a broad fragment of XPath which is very useful in practice. We show how PTPQs can be represented as directed acyclic graphs augmented with “samepath” constraints. We develop an original polynomial time holistic algorithm for PTPQs under the inverted list evaluation model. To the best of our knowledge, this is the first algorithm to support the evaluation of such a broad structural fragment of XPath. We provide a theoretical analysis of our algorithm and identify cases where it is asymptotically optimal. In order to assess its performance, we design two other techniques that evaluate PTPQs by exploiting the state-of-theart existing algorithms for smaller classes of queries. An extensive experimental evaluation shows that our holistic algorithm outperforms the other ones.	asymptotically optimal algorithm;computation;computer data storage;directed acyclic graph;holism;inverted index;materialized view;polynomial;query language;recursion;stack-oriented programming language;structural pattern;time complexity;xml database;xpath	Xiaoying Wu;Dimitri Theodoratos;Stefanos Souldatos;Theodore Dalamagas;Timos K. Sellis	2009		10.1007/978-3-642-02279-1_27	computer science;theoretical computer science;data mining;database;directed acyclic graph	DB	-30.122962287710212	3.8922469690137302	122492
21d85e1d18e5933b71152f757c4a5e40a71e77b5	"""review of """"reasoning about uncertainty by joseph y. halpern."""" the mit press, 2003"""	intelligent system	Dealing with uncertain and imprecise information has been one of the major issues in almost all intelligent systems. Uncertainty is a fundamental and unavoidable feature of our daily life. It arises because agents almost never have access to the whole truth about their environment. It can also occur when the agents have incomplete and/or incorrect information about the properties of their environment.	artificial intelligence	Wenzhong Zhao	2004	SIGACT News	10.1145/1027914.1027920	computer science;artificial intelligence;operations research;algorithm	AI	-28.870461182814303	-9.245044147078685	122513
429bf67ddc48eee276225c2f93ff6bcc229905c0	multidimensional indexing in an oodbms - a case study	database indexing;multidimensional indexing;odbms;linearity;query processing;search space;government;data view;knowledge management;data type;tree data structures;decision maker;feature space;vldb;hybrid approach;data location multidimensional indexing object oriented database management system query terabyte;oodbms;computer aided software engineering;data location;shape;object oriented database management system;indexing;indexation;multidimensional systems indexing tree data structures linearity computer aided software engineering shape government energy management knowledge management robustness;robustness;object oriented databases;query processing database indexing object oriented databases;data view multidimensional index feature space odbms oodbms vldb;query terabyte;multidimensional systems;multidimensional index;energy management	When decision makers are required to query terabytes or petabytes of datasets, narrowing the search space helps in shortening the time for querying. We investigated the use of a feature space multidimensional index to narrow the search space. When studying different indexing techniques we found that each fits certain data spaces. We identified a hybrid approach that adapts to different types of indexed data with the goal to optimize both insertion and query performance. We also investigated how the multidimensional index could be used to define a complex feature space from different data types, which we call a 'composite' data view. Such an index is independent of the data location and distribution. We believe our approach could address the problem of dealing with massive dynamic datasets for optimum query performance	dataspaces;fits;feature vector;petabyte;terabyte;view (sql)	Ibrahim Sallam	2006	2006 Canadian Conference on Electrical and Computer Engineering	10.1109/CCECE.2006.277747	multidimensional systems;data type;shape;computer science;data mining;database;linearity;information retrieval;government;robustness;energy management	DB	-27.53050454280675	1.467178903390922	122704
18aad6102541c26e2112398a707b8ba9322f94da	the hierarchical ordering in multiattribute files	hierarchical;multiattribute;article	In this paper, we have shown an interesting phenomenon in multiattribute files. If a file becomes significantly more randomized, its performance will deteriorate. Conversely, if a file becomes significantly more structured, its performance will definitely improve. That is, there is a hierarchical structure in multiattribute files.	randomized algorithm	Chin-Chen Chang;M. W. Du;Richard C. T. Lee	1983	Inf. Sci.	10.1016/0020-0255(83)90022-1	computer science;artificial intelligence;data mining;database;hierarchy	DB	-28.48470122667026	3.4843699332567764	123135
81ed4bfe3541caf230f15f39abc95903abc97e16	diagrammatic acquisition of functional knowledge for product configuration systems with the unified modeling language	configuration fonctionnel;metodo diagramatico;adquisicion del conocimiento;life cycle;teoria unificada;base connaissance;conceptual model;acquisition connaissances;software engineering;diagramme;dibujo esquematico;diagram;methode diagrammatique;diagram method;product cycle;knowledge acquisition;unified modeling language;unified theory;base conocimiento;theorie unifiee;product configuration;dessin schematique;diagrammatic sketche;knowledge base;diagrama	Shorter product cycles, lower prices of products, and the production of goods that are tailored to the customers needs made knowledge based product configuration systems a great success of AI technology. However, configuration knowledge bases tend to become large and complex. Therefore, knowledge acquisition and maintenance are crucial phases in the life-cycle of a configuration system. We will show how to meet this challenge by extending a standard design language from the area of Software Engineering with classical description concepts for expressing configuration knowledge. We automatically translate this graphical depiction into logical sentences which can be exploited by a general inference engine to solve the configuration task. In order to cope with usability restrictions of diagrammatic notations for large applications, we introduce the usage of contextual diagrams. This mechanism makes the conceptual model more readable and understandable and supports intuitively the acquisition of functional configuration knowledge.	diagram;knowledge-based configuration;unified modeling language	Alexander Felfernig;Markus Zanker	2000		10.1007/3-540-44590-0_31	unified modeling language;configuration management database;biological life cycle;knowledge base;computer science;artificial intelligence;conceptual model;diagram;product lifecycle;database;unified field theory;algorithm	Logic	-24.67303591739179	-3.5070442267073383	123253
4587b9e637e128481be52ec876f14dec802ba1e2	ijade content management system (cms) - an intelligent multi-agent based content management system with chaotic copyright protection scheme	content management;tratamiento datos;extraction information;analisis contenido;representacion conocimientos;ontologie;multiagent system;systeme intelligent;concentration effect;systeme protection;inverse document frequency;encryption;analisis datos;intellectual property;information extraction;droit auteur;agent based;ingenierie connaissances;chaos;information retrieval;sistema inteligente;logique floue;caos;semantics;data processing;copyright;logica difusa;traitement donnee;gestion contenido;cifrado;intelligence artificielle;logical programming;efecto concentracion;chino;term frequency;data mining;semantica;semantique;information content;copyright protection;fuzzy logic;data analysis;content analysis;cryptage;digital media;programmation logique;fouille donnee;agent intelligent;propiedad intelectual;intelligent system;agent technology;intelligent agent;representation connaissance;gestion contenu;protection system;web mining;content management system;artificial intelligence;analyse donnee;ontologia;effet concentration;agente inteligente;inteligencia artificial;analyse contenu;methode domaine temps frequence;knowledge representation;sistema proteccion;sistema multiagente;chinois;chinese;programacion logica;propriete intellectuelle;busca dato;ontology;metodo dominio tiempo frecuencia;extraccion informacion;time frequency domain method;systeme multiagent;derecho autor;knowledge engineering	In this paper, an Intelligent Content Management System with Chaotic Copyright Protection scheme called iJADE CMS is presented. iJADE CMS focuses on how web mining techniques can be effectively applied on Chinese content, with the integration of various AI techniques including intelligent agents, agent ontology and fuzzy logic based data mining scheme. Through the adoption of chaotic encryption scheme, iJADE CMS demonstrates how agentbased copyright protection can be successfully applied to digital media publishing industry. From the application perspective, iJADETM CMS provides the state-of-the-art content management function by the integration of iJADETM Technology with the Ontological Agent Technology. iJADETM CMS assists user to organize the content in the most semantic way. Moreover, the web mining information retrieval method such as Term Frequency Times Inverse Document Frequency (TFIDF) scheme is adopted to mine the linguistic meaning of the information content.	agent-based model;categorization;content management system;data mining;digital media;encryption;fuzzy logic;information retrieval;intelligent agent;self-information;tf–idf;web mining	Raymond S. T. Lee;Eddie C. L. Chan;Raymond Y. W. Mak	2006		10.1007/11893004_84	engineering;artificial intelligence;algorithm;cartography	AI	-23.183254186188194	-0.9833124643288098	123392
906cc359e565d9adc7c574533760fa1e0f5f0f34	multi-dimensional dynamic bucket index based on mobile agent system architecture	workload;modelizacion;distributed system;base donnee;sistema experto;architecture systeme;systeme reparti;mise a jour;range query;informatique mobile;agent mobile;geometrie algorithmique;base donnee temporelle;agente movil;interrogation base donnee;computational geometry;index structure;database;interrogacion base datos;base dato;intelligence artificielle;commande repartie;probabilistic approach;data distribution;spatial database;indexing method;actualizacion;modelisation;multi dimensional;busquedas dentro de un rango;mobile agent system;sistema repartido;requete a intervalle;indexing;enfoque probabilista;approche probabiliste;base donnee spatiale;indexation;mobile architecture;estructura datos;indizacion;charge travail;on the fly;artificial intelligence;arquitectura sistema;geometria computacional;structure donnee;temporal databases;base dato especial;prediction model;architecture mobile;inteligencia artificial;control repartido;systeme expert;mobile agent;carga trabajo;system architecture;mobile computing;modeling;data structure;distributed control;database query;updating;arquitectura movil;expert system	This paper describes the idea of a bucket index employed for processing of intensive reading streams coming from huge telemetry networks. This data structure answers approximate spatio-temporal range queries concerning utility usage in user selected region and time. The index structure continuously adjusts to data distribution changes and, as opposed to traditional indexing methods, is capable of processing of the updates on the fly. A stochastic prediction model is also used to estimate utility usage in the near future. The presented indexing technique is implemented in a distributed system based on mobile agents. The mobile architecture is used to control the workload of network hosts.	mobile agent;systems architecture	Marcin Gorawski;Adam Dyga	2006		10.1007/11827405_90	range query;search engine indexing;simulation;systems modeling;data structure;computational geometry;computer science;artificial intelligence;mobile agent;database;predictive modelling;temporal database;mobile computing;expert system;spatial database	Robotics	-26.00514745332785	2.326262087734736	123433
fa9f5594b943fd68efc87e7019f3b8340535b7d4	energy and latency efficient access of wireless xml stream	wireless technologies;query processing;indexing;xml	In this article, we address the problem of delayed query processing raised by tree-based index structures in wireless broadcast environments, which increases the access time of mobile clients. We propose a novel distributed index structure and a clustering strategy for streaming XML data that enables energy and latencyefficient broadcasting of XML data. We first define the DIX node structure to implement a fully distributed index structure which contains the tag name, attributes, and text content of an element, as well as its corresponding indices. By exploiting the index information in the DIX node stream, a mobile client can access the stream with shorter latency. We also suggest a method of clustering DIX nodes in the stream, which can further enhance the performance of query processing in the mobile clients. Through extensive experiments, we demonstrate that our approach is effective for wireless broadcasting of XML data and outperforms the previous methods.	interrupt latency;xml	Jun Pyo Park;Chang-Sup Park;Yon Dohn Chung	2010	J. Database Manag.	10.4018/jdm.2010112303	xml encryption;search engine indexing;xml;streaming xml;computer science;data mining;database;xml signature;world wide web	DB	-27.92914952846512	0.24543945511430065	123513
382e5b9c36ea3fa4cd93ec9d100cb10ed23e8dff	cognitive modelling approach to diagnose over-simplification in simulation-based training	systeme commande;sistema control;simplification;supervisory control;engineering and machine engineering;adquisicion del conocimiento;model based approach;acquisition connaissances;psychology;proceso adquisicion;acquisition process;simulator;sistema reactivo;control system;simulador;cognitive modelling;knowledge acquisition;modelo mental;simplificacion;reactive system;simulateur;systeme reactif;modele mental;learning artificial intelligence;computer science internet;supervision;processus acquisition;mental model;apprentissage intelligence artificielle	"""Simulation-based training has become a standard in operational knowledge training for supervisory control in safety-critical environments. But traditional simulators do not support mental model formation of automated systems though these systems are a dominant part in modern control systems. Recently various """"intelligent components"""" for this support have been suggested. But these approaches neglect the dynamic character of mental models. They focus on building a normative model at the beginning of the training but do not consider how it evolves due to knowledge acquisition processes. In this paper we present a model-based approach to diagnose success-driven learning in simulator training and to predict dangerous over-simplifications. Our research focuses on pilot training for automated cockpits."""	cognitive model;simulation;text simplification	Andreas Lüdtke;Claus Möbus;Heinz-Jürgen Thole	2002		10.1007/3-540-47987-2_52	simulation;reactive system;computer science;control system;artificial intelligence;supervisory control;operations research;simplification	Vision	-24.273269731537347	-5.495479726039319	123811
ee3ca17c040c908849af012a5a67651fc4bee6a3	towards unifying advances in twig join algorithms	semi structured data;indexing;twig join	Twig joins are key building blocks in current XML indexing systems, and numerous algorithms and useful data structures have been introduced. We give a structured, qualitative analysis of recent advances, which leads to the identification of a number of opportunities for further improvements. Cases where combining competing or orthogonal techniques would be advantageous are highlighted, such as algorithms avoiding redundant computations and schemes for cheaper intermediate result management. We propose some direct improvements over existing solutions, such as reduced memory usage and stronger filters for bottom-up algorithms. In addition we identify cases where previous work has been overlooked or not used to its full potential, such as for virtual streams, or the benefits of previous techniques have been underestimated, such as for skipping joins. Using the identified opportunities as a guide for future work, we are hopefully one step closer to unification of many advances in twig join algorithms.	algorithm;bottom-up parsing;computation;data structure;database index;join (sql);open-source software;structured analysis;twig;unification (computer science);xml	Nils Grimsmo;Truls Amundsen Bjørklund	2010			search engine indexing;semi-structured data;computer science;theoretical computer science;data mining;database	DB	-32.431146885354885	0.939875126456153	124019
3e0e8c780fe74a3b01850a297f7a7ff96e905c64	stjlbm-1 expert system generating a part classification and coding system	expert system		expert system	Chun-Ping Zeng;Guan-Xun Yang	1991			legal expert system	NLP	-28.395659127284663	-7.597836779407423	124295
89e9133a8ab188c393f6601dc95c92253a0e6c7c	integrating planning, execution, and learning to improve plan execution	cognitive architectures;relational reinforcement learning;symbolic planning	Algorithms for planning under uncertainty require accurate action models that explicitly capture the uncertainty of the environment. Unfortunately, obtaining these models is usually complex. In environments with uncertainty, actions may produce countless outcomes and hence, specifying them and their probability is a hard task. As a consequence, when implementing agents with planning capabilities, practitioners frequently opt for architectures that interleave classical planning and execution monitoring following a replanning when failure paradigm. Though this approach is more practical, it may produce fragile plans that need continuous replanning episodes or even worse, that result in execution dead-ends. In this paper, we propose a new architecture to relieve these shortcomings. The architecture is based on the integration of a relational learning component and the traditional planning and execution monitoring components. The new component allows the architecture to learn probabilistic rules of the success of actions from the execution of plans and to automatically upgrade the planning model with these rules. The upgraded models can be used by any classical planner that handles metric functions or, alternatively, by any probabilistic planner. This architecture proposal is designed to integrate off-the-shelf interchangeable planning and learning components so it can profit from the last advances in both fields without modifying the architecture.	algorithm;programming paradigm	Sergio Jiménez Celorrio;Fernando Fernández;Daniel Borrajo	2013	Computational Intelligence	10.1111/j.1467-8640.2012.00447.x	real-time computing;simulation;computer science;artificial intelligence;machine learning	AI	-19.590264100159665	-6.633322991187618	124315
70e75c9d7164777f633d99352dbadddf33a470c4	conditional deduction under uncertainty	modelizacion;belief;incertidumbre;uncertainty;conditional reasoning;intelligence artificielle;probabilistic approach;conditional inference;modelisation;croyance;enfoque probabilista;approche probabiliste;razonamiento condicional;inferencia;artificial intelligence;incertitude;inteligencia artificial;creencia;modeling;modus ponens;inference;raisonnement conditionnel	Conditional deduction in binary logic basically consists of deriving new statements from an existing set of statements and conditional rules. Modus Ponens, which is the classical example of a conditional deduction rule, expresses a conditional relationship between an antecedent and a consequent. A generalisation of Modus Ponens to probabilities in the form of probabilistic conditional inference is also well known. This paper describes a method for conditional deduction with beliefs which is a generalisation of probabilistic conditional inference and Modus Ponens. Meaningful conditional deduction requires a degree of relevance between the antecedent and the consequent, and this relevance can be explicitly expressed and measured with our method. Our belief representation has the advantage that it is possible to represent partial ignorance regarding the truth of statements, and is therefore suitable to model typical real life situations. Conditional deduction with beliefs thereby allows partial ignorance to be included in the analysis and deduction of statements and hypothesis.	approximation;natural deduction;real life;relevance	Audun Jøsang;Simon Pope;Milan Daniel	2005		10.1007/11518655_69	systems modeling;uncertainty;modus ponens;deduction theorem;computer science;artificial intelligence;belief;machine learning;mathematics;sequent calculus;algorithm	AI	-20.308037067807174	-1.241003676762984	124390
21ca7835db9daa7b79501416b403b81d7c3b40ad	the design and implementation of a high-efficiency distributed web crawler	web pages;distributed databases;crawlers;uniform resource locators;information filters	With the rapid development of the Internet, the amount of data on the Internet become more and more huge, and the website technology is constantly changing. Faced with the huge and complex data on the global Internet, how to crawl and use this information has become a major challenge. Traditional stand-alone web crawler is difficult to cope with the challenges brought by the rapid growth of information, and it is difficult to grab huge amounts of data quickly and effectively. In this paper, we research to use the distributed technology to design and implement an efficient, configurable, load balancing and scalable distributed web crawler system.	apache hadoop;apache nutch;distributed web crawling;heritrix;internet;load balancing (computing);multipoint ground;prototype;requirement;scalability;simultaneous multithreading;thread (computing);web crawler;web page;web search engine;xml	Qiumei Pu	2016	2016 IEEE 14th Intl Conf on Dependable, Autonomic and Secure Computing, 14th Intl Conf on Pervasive Intelligence and Computing, 2nd Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress(DASC/PiCom/DataCom/CyberSciTech)	10.1109/DASC-PICom-DataCom-CyberSciTec.2016.34	data web;computer science;data mining;database;focused crawler;world wide web	HPC	-31.405572964285067	-1.060351150775548	124414
2777eea2b97b7e24e45fa31b5d0241ab68d6763e	optimal linear hashing files for orthogonal range retrieval	partial match retrieval orthogonal range retrieval performance expressions optimal recursive linear hashing files greedy method mmi minimum marginal increase;orthogonal range retrieval;database management systems;information retrieval;minimum marginal increase;performance expressions;database management systems information retrieval data structures;design optimization;optimal recursive linear hashing files;mmi;data structures;partial match retrieval;computer science databases performance analysis;greedy method	In this paper, we are concerned with the problem of designing optimal linear hashing files for orthogonal range retrieval. Through the study of performance expressions, we show that optimal basic linear hashing files and optimal recursive linear hashing files for orthogonal range retrieval can be produced, in certain cases, by a greedy method called the MMI (minimum marginal increase) method; and it is pointed out that optimal linear hashing files for partial match retrieval need not be optimal for orthogonal range retrieval.	approximation algorithm;greedy algorithm;linear hashing;marginal model;range searching;recursion	C. Y. Chen;Chin-Chen Chang;Richard C. T. Lee;D. C. Lin	1996		10.1109/CMPSAC.1996.544601	greedy algorithm;multidisciplinary design optimization;data structure;computer science;theoretical computer science;data mining;database;programming language	Theory	-27.629401496387636	2.8171987296247782	124866
5930a8827b267a57a802b5ec948a3cb30b05d565	animacy information in human sentence processing: an incremental optimization of interpretation approach	language comprehension;modelizacion;utilisation information;complex objects;optimisation;uso informacion;linguistica matematica;procesamiento informacion;optimizacion;information use;frase;verbe;sentence comprehension;hombre;constraint satisfaction;modelisation;refinement method;satisfaction contrainte;sentence;polisemia;information processing;ambiguity resolution;human;polysemy;verbo;polysemie;linguistique mathematique;optimization;comprension lenguaje;phrase;comprehension langage;computational linguistics;satisfaccion restriccion;methode raffinement;traitement information;discriminacion;sentence processing;modeling;metodo afinamiento;article in monograph or in proceedings;discrimination;homme;verb	To formalize and analyze the role of animacy information in on-line sentence comprehension, results of several on-line studies are compared and analyzed according to a new model of incremental optimization of interpretation. This model makes use of violable ranked constraints. To analyze the use of animacy information a set of four constraints is needed, namely Case, Selection, Precedence, and Prominence. It is shown that the pattern of constraint violations of these four constraints provide sufficient information to reflect the on-line effects of language comprehension studies in which animacy information played a crucial role. More specifically, the evaluation of sentences in which either case information or animacy information in combination with the selection restrictions of the verb were used, showed that the model can account for the ambiguity resolution with both sorts of information. The model was also successfully applied to the on-line processing of a more complex object relative structure in English.		Monique Lamers;Helen de Hoop	2004		10.1007/11424574_10	natural language processing;mathematical optimization;discrimination;systems modeling;constraint satisfaction;information processing;computer science;computational linguistics	NLP	-23.01174116769628	-1.3829095495054493	125039
448245c18040a2a30697ea2f748a6c0383aba483	sql in the clouds	databases;libraries;mapreduce algorithm;sql cloud algorithms;parallel algorithm;query processing;sql cloud algorithms cloud computing distributed computing dataflow architectures database structures database query evaluation;web services data mining parallel algorithms query processing relational databases sorting sql;cloud computing environment;sorting;sql;probability density function;dataflow architectures;distributed computing;construction industry;data mining;data distribution;data mining cloud computing maintenance automobiles databases multiprocessor interconnection networks computer networks surveillance parallel processing search engines;supervisory sorting process;database query evaluation;sql database query;database structures;clouds;calculus;web services;supervisory sorting process sql database query cloud computing environment mapreduce algorithm parallel algorithm data distribution process job supervisor infrastructure data mining;data flow computing;relational databases;database query;data distribution process;cloud computing;job supervisor infrastructure;parallel algorithms	In a cloud computing context, the MapReduce algorithm comprises two massively parallel operations linked by a generic sorting and data-distribution process. Although this algorithm is the workhorse in most cloud computing strategies, it's a special case of a more general dataflow. In place of the two cloud operations, the proposed method substitutes longer sequences and then lets the user direct outputs to any subsequent downstream operation. However, the method retains the job-supervisor infrastructure, which performs the necessary sorting, collating, and distributing of these outputs prior to initiating operations. To evaluate SQL database queries, particularly those with correlated subqueries, a computation identifies and aligns data elements from widely separated storage locations, suggesting cloud algorithms that exploit the supervisory sorting process to achieve the desired alignments. Exploring such algorithms reveals that a few customizable templates, assembled recursively as necessary, can handle a wide class of SQL data-mining queries.	algorithm;cloud computing;computation;data mining;database;dataflow;downstream (software development);mapreduce;recursion;sql;sorting	James L. Johnson	2009	Computing in Science & Engineering	10.1109/MCSE.2009.127	parallel computing;computer science;data mining;database;distributed computing;parallel algorithm	DB	-30.540016977512735	1.650103460734664	125106
84a945c605a75132a116aa397b34a11165a9845c	scalable social graph analytics using the vertica analytic platform		Social Graph Analytics has become very popular these days, with companies like Zynga, Linkedin, and Facebook seeking to derive the most value from their respective social networks. It is common belief that relational databases are ill-equipped to deal with graph problems, resulting in the use of MapReduce implementations or special purpose graph analysis engines. We challenge this belief by presenting a few use-cases that Vertica has very successfully solved with simple SQL over a high-performance relational database engine.	social graph	Shilpa Lawande;Lakshmikant Shrinivas;Rajat Venkatesh;Stephen Walkauskas	2011		10.1007/978-3-642-33500-6_7	data science;database;world wide web	DB	-33.498063702814804	-0.3925099028241024	125177
52e1f2ef8edb1624b84861a665a78d4013d10898	adaptation-guided retrieval: questioning the similarity assumption in reasoning	case base reasoning;case retrieval;indexing terms;artificial intelligent;case adaptation;case based reasoning;problem solving;similarity based reasoning	One of the major assumptions in Artificial Intelligence is that similar experiences can guide future reasoning, problem solving and learning; what we will call, the similarity assumption. The similarity assumption is used in problem solving and reasoning systems when target problems are dealt with by resorting to a previous situation with common conceptual features. In this article, we question this assumption in the context of case-based reasoning (CBR). In CBR, the similarity assumption plays a central role when new problems are solved, by retrieving similar cases and adapting their solutions. The success of any CBR system is contingent on the retrieval of a case that can be successfully reused to solve the target problem. We show that it is unwarranted to assume that the most similar case is also the most appropriate from a reuse perspective. We argue that similarity must be augmented by deeper, adaptation knowledge about whether a case can be easily modified to fit a target problem. We implement this idea in a new technique, called adaptation-guided retrieval (AGR), which provides a direct link between retrieval similarity and adaptation needs. This technique uses specially formulated adaptation knowledge, which, during retrieval, facilitates the computation of a precise measure of a case’s adaptation requirements. In closing, we assess the broader implications of AGR and argue that it is just one of a growing number of methods that seek to overcome the limitations of the traditional, similarity assumption in an effort to deliver more sophisticated and scaleable reasoning systems. Smyth & Keane 3 Adaptation-Guided Retrieval	advanced graphics riser;artificial intelligence;case-based reasoning;closing (morphology);computation;contingency (philosophy);experience;inline linking;problem solving;profile-guided optimization;reasoning system;requirement	Barry Smyth;Mark T. Keane	1998	Artif. Intell.	10.1016/S0004-3702(98)00059-9	case-based reasoning;qualitative reasoning;computer science;artificial intelligence;adaptive reasoning;machine learning;data mining;reasoning system	AI	-21.130700100718517	-5.791193501542277	125185
79732e0d839d6267a2976117f6102ce71cb70c14	using multi-bucket data leaves with overflow chains-performance analysis	file layout;organisation fichier;information retrieval;gestion fichier;search strategy;overflow;file management;multibucket;recherche information;organizacion fichero;analyse performance;manejo archivos;performance analysis;strategie recherche;recuperacion informacion;estrategia investigacion;analisis eficacia	Abstract   Many file organizations consist of an index or a directory, which is used for search purposes, and a set of data leaves in which the data records are stored. In order to decrease the index size so that it can fit into main memory, such file organizations may use  multi-bucket  leaves (instead of “ordinary” data leaves), resulting, however, in a reduction of the average disk space utilization. This can be prevented by postponing leaf splits, either by performing partial expansions before splitting or by using overflow chains.  In this paper we analyze (under random insertions) file organizations that use multi-bucket leaves with overflow chains, and compare them to file organizations that use multi-bucket leaves and perform partial expansions before splitting. The analysis can be used to investigate both the dynamic and asymptotic behaviors.	computer data storage;directory (computing);disk space;markov chain;profiling (computer programming)	Gabriel Matsliach	1991	Inf. Syst.	10.1016/0306-4379(91)90038-B	computer science;operating system;database;world wide web	DB	-27.54543827334889	3.9260999917398576	125186
65cfb644d13216d2606ff3cc255402651f3742d5	seed-config: a case-based reasoning system for conceptual building design	conceptual building design;case-based design functionality;design case;design environment;design tool;design process;design knowledge;case-based design capability;case-based reasoning system;design solution;case library;case based reasoning;building design;case base reasoning	A case-based design functionality is a natural and intuitive addition to a design tool that can augment human capabilities and help designers remember and retrieve appropriate cases. SEED-Config, a design environment for conceptual building design, was developed to incorporate a case-based reasoning functionality to provide designers with initial potential solutions. The case representation in SEED-Config is the BENT information model, which records design knowledge, supports the hierarchical decomposition of design cases, offers multiple views, and encapsulates the outcome of the design in addition to the problem specification and the design solution. The case library was implemented in an object-oriented database management system to accumulate cases automatically and to provide efficient query facilities. The case retrieval aspect of SEED-Config offers three different methods to find the most useful cases stored in the case library: task-based, lineage-based, and customized. Case retrieval responds to the exploratory nature of the design process and supports versatile case retrieval by providing multiple paths to each case. The case adaptation aspect, which adjusts the selected case to the new problem to provide a complete solution, uses an adaptation method called derivational replay. The case-based design capabilities are completely integrated within the design environment from which the cases originate.	case-based reasoning;reasoning system	Hugues Rivard;Steven J. Fenves	2000	AI EDAM		case-based reasoning;simulation;computer science;systems engineering;engineering;artificial intelligence;building design;data mining;conceptual design	AI	-29.480529986545996	-4.167843568416887	125209
34fb664158d82872106560d573d1898fc8069ffd	competitive intelligence: history, importance, objectives, process and issues	empirical solutions;data collection;technical data extraction history competitive intelligence domain multicriteria decision aid methods artificial intelligence technical data management;competitive intelligence artificial intelligence;data collection competitive intelligence process empirical solutions;competitive intelligence;process;competitive intelligence decision making artificial intelligence cognition history companies	Competitive intelligence deals with the competitive environment of a company. Several studies have been conducted on competitive intelligence domain but there is no empirical work that gives a complete implemented competitive intelligence solution. This paper presents an overview of competitive intelligence studies and highlights the issues towards developing a complete CI solution. A new conceptual model which details the collection phase and incorporates the anticipation of the competitor decisions is proposed. To better solve the CI problem, the paper proposes to integrate the ability of multi-criteria decision aid methods to manage conflicting criteria in a complex environment, with the ability of artificial intelligence in managing and extracting large amount of technical data/information in such a context.	artificial intelligence;automated reasoning	Dhekra Ben Sassi;Anissa Frini;Wahiba Ben Abdessalem Karaa;Naoufel Kraïem	2015	2015 IEEE 9th International Conference on Research Challenges in Information Science (RCIS)	10.1109/RCIS.2015.7128910	intelligence cycle;artificial architecture;marketing and artificial intelligence;competitive intelligence;computer science;engineering;artificial intelligence;data mining;military intelligence;management science;web intelligence;management;operations research;artificial psychology;statistics;process;data collection	AI	-32.06934556883836	-8.224265301236777	125252
6d204aef585025d75055a283a8eae81f66bc40d0	complex knowledge system modeling based on hierarchical fuzzy petri net	hierarchical fuzzy petri nets (hfpn);knowledge base system;modeling approach;knowledge systems;knowledge base;petri net	The difficulties of modeling complex knowledge system lie in a large quantity of knowledge rules and the difficulty in organizing rules and grasping their mutual logical relationships. This article proposed a concept of hierarchical fuzzy Petri nets (HFPN), which is more suitable for modeling complex knowledge system than other fuzzy Petri Net models. In addition, the concepts of abstract place and abstract transition in HFPN are allowed to describe and analyze the knowledge system at diverse abstract levels. Therefore, using HFPN, the iterative and incremental methods can be applied to modeling complex knowledge system. In addition, structured approach can be naturally applied to the process of modeling knowledge system.	iterative and incremental development;iterative method;knowledge-based systems;organizing (structure);petri net;systems modeling	Hongjun Pan;Jigui Sun	2007	2007 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology - Workshops		computer science;knowledge management;artificial intelligence;knowledge-based systems;machine learning;process architecture;petri net	AI	-30.206546830811888	-5.447649596311612	125457
b790e9d1fd51e45ccd684e388cc2f04554f075f2	a framework for design of an integrated system for decision support and training	decision support;training;cognitive systems engineering;work domain analysis;naturalistic decision making;recognition primed decision model;emerging work domain	This paper introduces a cognitive engineering approach for requirements definition and design of an integrated system for decision support and training for critical situations. Effective decision support and training should be provided not only for predefined, routine work situations, but also for a range of situations, including emergencies and other unanticipated events where decision support can be most valuable. This applies in particular to the case of decision-making in dynamic, complex environments. The design of decision support systems (DSS) should take into account the cognitive demands of decision makers to support them in their work. A DSS that can support a decision maker in critical situations can also be used for training of expertise to handle these critical situations; hence, there can be a large advantage to combining a DSS and training system into one application. A methodological approach for the design of an integrated system for decision support and training was developed using methods from Cognitive Systems Engineering and incorporating the Naturalistic Decision Making approach. A special case of novel or emerging work domains was considered.	artificial intelligence;cognitive engineering;decision support system;requirement;systems engineering	Elena Dalinger	2013		10.1145/2501907.2501942	r-cast;decision support system;intelligent decision support system;decision analysis;decision engineering;computer science;systems engineering;engineering;knowledge management;decision tree;naturalistic decision-making;management science;evidential reasoning approach;business decision mapping	HCI	-31.36697548872353	-8.362896019358356	125617
794dc0ed9dfac6467cd03b0245bd99a399df67fd	progressive distributed top-k retrieval in peer-to-peer networks	database indexing;database indexing peer to peer computing query processing;peer to peer network;query processing;information infrastructure;distributed processing;peer to peer system;query statistics progressive distributed top k retrieval peer to peer network query processing information management system database object query predicate distributed peer to peer information infrastructure hypercup topology index update query routing data traffic;information management;indexation;peer to peer computing;peer to peer;peer to peer computing query processing telecommunication traffic information management information retrieval databases distributed processing spine network topology statistics;matching model;dynamic networks;query routing	Query processing in traditional information management systems has moved from an exact match model to more flexible paradigms allowing cooperative retrieval by aggregating the database objects' degree of match for each different query predicate and returning the best matching objects only. In peer-to-peer systems such strategies are even more important, given the potentially large number of peers, which may contribute to the results. Yet current peer-to-peer research has barely started to investigate such approaches. In this paper we discuss the benefits of best match/top-k queries in the context of distributed peer-to-peer information infrastructures and show how to extend the limited query processing in current peer-to-peer networks by allowing the distributed processing of top-k queries, while maintaining a minimum of data traffic. Relying on a super-peer backbone organized in the HyperCuP topology we show how to use local indexes for optimizing the necessary query routing and how to process intermediate results in inner network nodes at the earliest possible point in time cutting down the necessary data traffic within the network. Our algorithm is based on dynamically collected query statistics only, no continuous index update processes are necessary, allowing it to scale easily to large numbers of peers, as well as dynamic additions/deletions of peers. We show our approach to always deliver correct result sets and to be optimal in terms of necessary object accesses and data traffic. Finally, we present simulation results for both static and dynamic network environments.	algorithm;cluster analysis;computer cluster;database;digital library;distributed computing;information management;information system;internet backbone;microsoft outlook for mac;peer-to-peer;routing;scalability;simulation;web search engine	Wolf-Tilo Balke;Wolfgang Nejdl;Wolf Siberski;Uwe Thaden	2005	21st International Conference on Data Engineering (ICDE'05)	10.1109/ICDE.2005.115	information infrastructure;database index;query optimization;query expansion;computer science;data mining;database;information management;world wide web;information retrieval	DB	-29.249867149602995	-0.3816921802028565	125718
5eac41afb0a4585ce593cfab1c2514e9336c47b5	numerical and syntactic tools for fusion and diagnosis provided with the topmuss-system	information processing;development time	A toolbox for the development of diagnostic systems is described. The information processing structure chosen for TO PMUSS(Tools for Processing of MultiSensorial Signals) relies on five building stages from sensing to decision inside which application-specific tools can be exchanged easily. This is achieved by using an objectoriented approach. For scheduling a reasoning mechanism is used and therefore all tools conform to a comnmn functionality definition. Three short examples of numerical and syntactic tools explain the functionality of the tool box. Its usage will reduce the development time needed for a new diagnostic system.	information processing;numerical analysis;scheduling (computing)	F. Quante;H. Kirsch;M. Ruckhäberle	1992		10.1007/BFb0025024	natural language processing;speech recognition;computer science	HPC	-29.37204352010668	-6.398164776297301	126244
b1f34f49752759e2d402b84c69d889cb1b82b17f	integrated agent architecture: execution and recognition of mental-states	plan recognition;agent architecture	Recognizing the mental-state — the beliefs, desires, plans, and intentions — of other agents situated in the environment is an important part of intelligent activity. Doing this with limited resources and in a continuously changing environment, where agents are also changing their own mental-state, is a challenging task. Following the relative success of reactive planning, as opposed to classical planning, we introduce the notion of reactive plan recognition. We integrate reactive planning and reactive plan recognition and embed them within the framework of an agent's mental-state. This results in a powerful architecture for agents that can handle executions based on mental-states and recognition of the mental-states of other agents.	agent architecture	Anand S. Rao	1995		10.1007/3-540-61314-5_28	embedded system;computer architecture;real-time computing	AI	-19.506192015753964	-8.068666613528093	126330
9f54c07e905ba362c439bd5f3a0732d0a4876916	collaborative ontology development approach for multidisciplinary knowledge: a scenario-based knowledge construction system in life cycle assessment			ontology (information science)	Akkharawoot Takhom;Sasiporn Usanavasin;Thepchai Supnithi;Mitsuru Ikeda	2018	IEICE Transactions		computer science;life-cycle assessment;systems engineering;computer vision;artificial intelligence;multidisciplinary approach;ontology	AI	-32.44302767756115	-6.568436744814351	126556
1002b31ed19a0e7373908fb620859a00bcdb1c90	small-world phenomena and the dynamics of information	small world;indexation;world wide web	The problem of searching for information in networks like the World Wide Web can be approached in a variety of ways, ranging from centralized indexing schemes to decentralized mechanisms that navigate the underlying network without knowledge of its global structure. The decentralized approach appears in a variety of settings: in the behavior of users browsing the Web by following hyperlinks; in the design of focused crawlers [4, 5, 8] and other agents that explore the Web’s links to gather information; and in the search protocols underlying decentralized peer-to-peer systems such as Gnutella [10], Freenet [7], and recent research prototypes [21, 22, 23], through which users can share resources without a central server.	centralized computing;focused crawler;freenet;gnutella;hyperlink;peer-to-peer;server (computing);world wide web	Jon M. Kleinberg	2001			computer science;data science;data mining;world wide web	ML	-29.88187515207455	-1.7672157252852003	126592
1899c3dbdc8062c0c6a550fa62a8650ab3c27060	skyline queries for sets of spatial objects by utilizing surrounding environments		A skyline query finds objects that are not dominated by another object from a given set of objects. Skyline queries help us to filter unnecessary information efficiently and provide us clues for various decision making tasks. However, conventional skyline query algorithms do not consider the surrounding environments for skyline computations though surrounding environments are as important as other attributes. Moreover, they can not protect the privacy of the individual’s and are not well suited for group choice.		Mohammad Shamsul Arefin;Yasuhiko Morimoto	2013		10.1007/978-3-642-37134-9_22	data mining;computer science;convex hull;skyline;computation	DB	-24.802361474611622	2.73057303484306	126698
ef61af90431c98fc2fc39bc6e9ac41cf4d5da2df	unified problem modeling language for knowledge engineering of complex systems	unified problem modeling language;nuclear power;multilayered agents;layered architecture;knowledge based system;image processing;e commerce;soft computing;real time;unified problem modelling language;real time alarm processing;agent oriented software engineering;component based software;modeling language;model complexity;large scale;complex data;operating system;modelling language;complex system;power system;hybrid architecture;multi agent architecture;genetic algorithm;soft computing agents;real time application;flight control;power system control;fuzzy system;problem solving;artificial neural network;distributed architecture;real time systems;expert system;knowledge engineering;time constraint	The 90’s has seen the emergence of hybrid configurations of four most commonly used intelligent methodologies, namely, symbolic knowledge based systems (e.g. expert systems), artificial neural networks, fuzzy systems and genetic algorithms. These hybrid configurations are used for different problem solving tasks/situations. In this paper we describe unified problem modeling language at two different levels, the task structure level for knowledge engineering of complex data intensive domains, and the computational level of the task level hybrid architecture. Among other aspects, the unified problem modeling language considers various intelligent methodologies and their hybrid configurations as technological primitives used to accomplish various tasks defined at the task structure level. The unified problem modeling language is defined in the form of five problem solving adapters. The problem solving adapters outline the goals, tasks, percepts/inputs, and hard and soft computing methods for modeling complex problems. The task structure level has been applied in modeling several applications in e-commerce, image processing, diagnosis, and other complex, time critical, and data intensive domains. We also define a layered intelligent multi-agent, operating system processes, intelligent technologies with the task structure level associative hybrid architecture. The layered architecture also facilitates component based software modeling process.	artificial neural network;complex systems;data-intensive computing;e-commerce;emergence;expert system;fuzzy control system;genetic algorithm;image processing;knowledge engineering;modeling language;multi-agent system;operating system;problem solving;soft computing	Rajiv Khosla;Qiubang Li	2004	Soft Comput.	10.1007/s00500-003-0307-x	nuclear power;simulation;genetic algorithm;image processing;computer science;artificial intelligence;theoretical computer science;multitier architecture;machine learning;knowledge engineering;soft computing;electric power system;modeling language;expert system;artificial neural network;complex data type	AI	-27.643508410390353	-5.954204632918108	126763
6de2f02cfcc10d514431953a623898bfa61c1580	the yin and yang of processing data warehousing queries on gpu devices	databases;paper;nvidia geforce gtx 680;nvidia geforce gtx 580;ati radeon hd 7970;ati;nvidia geforce gtx 480;cuda;package;nvidia;computer science;opencl;analytical model	Database community has made significant research efforts to optimize query processing on GPUs in the past few years. However, we can hardly find that GPUs have been truly adopted in major warehousing production systems. Preparing to merge GPUs to the warehousing systems, we have identified and addressed several critical issues in a threedimensional study of warehousing queries on GPUs by varying query characteristics, software techniques, and GPU hardware configurations. We also propose an analytical model to understand and predict the query performance on GPUs. Based on our study, we present our performance insights for warehousing query execution on GPUs. The objective of our work is to provide a comprehensive guidance for GPU architects, software system designers, and database practitioners to narrow the speed gap between the GPU kernel execution (the fast mode) and data transfer to prepare GPU execution (the slow mode) for high performance in processing data warehousing queries. The GPU query engine developed in this work is open source to the public.	database;graphics processing unit;open-source software;production system (computer science);software system;yang	Yuan Yuan;Rubao Lee;Xiaodong Zhang	2013	PVLDB	10.14778/2536206.2536210	parallel computing;computer science;operating system;database;package	DB	-33.324756757447936	0.253279416570631	127115
8c1a1dd93081c5c7f2f6db74e37d05cd71028b31	fuzzy decision support system for risk analysis in e-commerce development	analyse risque;commerce electronique;factor riesgo;electronic commerce;project management;comercio electronico;fuzzy set;red www;risk factor;design and development;systeme aide decision;risk analysis;fuzzy decision support system;e commerce;project manager;decision borrosa;gestion risque;risk management;reseau web;conjunto difuso;ensemble flou;decision floue;sistema ayuda decision;facteur risque;analisis riesgo;support system;risk factors;decision support system;internet;risk assessment;world wide web;gestion projet;gestion riesgo;electronic trade;gestion proyecto;fuzzy decision;evaluation risque	This paper describes the development of a fuzzy decision support system (FDSS) for the assessment of risk in e-commerce (EC) development. A Web-based prototype FDSS is designed and developed to assist EC project managers in identifying potential EC risk factors and the corresponding project risks. A risk analysis model for EC development using a fuzzy set approach is proposed and incorporated into the FDSS. The results of an evaluation indicate that the prototype performs to expectations. D 2004 Elsevier B.V. All rights reserved.	decision support system;e-commerce;fuzzy set;it risk management;prototype	Eric W. T. Ngai;F. K. T. Wat	2005	Decision Support Systems	10.1016/j.dss.2003.12.002	e-commerce;computer science;operations management;data mining;management;operations research;world wide web;risk factor	AI	-26.84657304436048	-4.507634863683659	127396
1ba838231723e7e51481f490c4bee341145cbefd	representation requirements for supporting decision model formulation	relevant set;broad domain;analysis result;knowledge type;representational support;inference pattern;knowledgebased decision-modeling;context-sensitive manner;uncertain knowledge;representation requirement;design approach;decision model formulation;knowledge base;decision models	This paper outlines a methodology for analyzing the representational support for knowledge-based decision-modeling in a broad domain. A relevant set of inference patterns and knowledge types are identified. By comparing the analysis results to exist­ ing representations, some insights are gained into a design approach for integrating cate­ gorical and uncertain knowledge in a context­ sensitive manner.	knowledge-based systems;requirement	Tze-Yun Leong	1991			knowledge base;decision model;computer science;knowledge management;artificial intelligence;body of knowledge;machine learning;data mining	ML	-32.12053837392407	-6.91008196712715	127615
d25b58c527da19b3c24d295522c3a5b307c050d7	intelligent architectures for knowledge sharing: a soar example and general issues	knowledge sharing	In this talk we present a model of knowledge assessment based on architecture-specific metrics and present a method of knowledge sharing called Direct Knowledge eXchange (DKX). Although embryonic from a knowledge management perspective, the notion of DKX between homogeneous knowledge engines and knowledge metric assessment represents a small step to examining how distribution of knowledge can lead to remote “knowledge invocation on demand” types of distributed problem solving systems and practical mechanisms to assess their resource value. Summary Soar is a production system that characterizes all symbolic goal-oriented behavior as search in problem spaces and serves as an architecture for general intelligent behavior (Laird, Rosenbloom & Newell 1987). A problem space defines a set of states that can be reached within that problem space, and an associated collection of operators. Operator applications move Soar from state to state and, consequently, define search in the problem space. Decisions are the primitive acts of the system used for search (i.e., generation and selection) of appropriate problem spaces, states, and operators, as well as the application of operators for new state configuration, in the pursuit of goals specified in a goal hierarchy. To achieve problem-solving goals, Soar operates in terms of a two-phase decision cycle. Each cycle starts with an elaboration phase followed by a decision phase. Together, these phased mechanisms, coupled with an embedded set of primitive preferences, allow for problem solving to ensue through the specification of appropriate sequences of operators. Soar is impasse-driven. In situations where the operator selection cannot unambiguously proceed (e.g., via incomplete or inconsistent preferences), then an impasse occurs, and a new subgoal is established with an associated problem space, and the process recurses. Copyright © 2002, American Association for Aritificial Intelligence (www.aaai.org). All rights reserved. The resolution of a subgoal in Soar is achieved by finding knowledge that resolves higher-level impasses, allowing problem solving to proceed. When this occurs, Soar learns. Specifically, “chunks” are produced that are productions that map working memory elements defining impasse situations (as antecedent conditions) into the results of subgoals (as consequent conditions). Chunking can be viewed as a form of explanation-based learning, but it is at a level articulated in specific and uniformly applied cognitive mechanisms. Subsequent encounters with similar impasse conditions can thus be resolved more directly (and with less deliberation) with the newly acquired chunks. Soar has learned. Key to learning in general and generalized learning in particular is the ability to transfer knowledge. Soar has three basic forms of transfer: within-trial (chunks can be used as soon as they are built), between-trial (chunks are improved with repeated trials on a task), and across-task (chunks can apply to similar problems). We are exploring another form of transfer. Imagine a set of distributed Soar knowledge engines. What would happen if they could exchange chunks? In other words, what would problem solving look like if a set of affiliated and distributed Soar problem solving engines could directly exchange their chunks – their intimate knowledge of a task? Base Task The task selected was the 8-puzzle (see Figure 1). In this task, there is a 3 x 3 matrix of eight randomly assigned numbers (the ninth being a space) and the problem is to find the series of operators (moves) that shuffle the set into a properly ascending sequence, by moving the tiles, oneby-one, to the open space. This task has 9! initial states and, depending on how you define it, can generate an uninformed search expansion of about 3.4 x 10. 1 Also called the N x N sliding tile problem, often appearing as a hand-held game where tiles are adjacently slid and interchanged until the right sequence/pattern is achieved. 318 FLAIRS 2002 From: FLAIRS-02 Proceedings. Copyright © 2002, AAAI (www.aaai.org). All rights reserved.	15 puzzle;box counting;brute-force search;embedded system;explanation-based learning;knowledge management;mobile device;problem domain;problem solving;production system (computer science);randomness;search algorithm;shallow parsing;soar (cognitive architecture);two-phase locking;zero-knowledge proof	Dan Zhu;Michael J. Prietula	2002			simulation;computer science;knowledge management;artificial intelligence;machine learning;procedural knowledge	AI	-21.077164882609292	-9.143410433834472	127826
9cb82fc5d10869b976d0faead1ac64d9fbf63186	path-based constrained nearest neighbor search in a road network	location-based services;path-based constrained nearest neighbor query;peer-to-peer sharing	Nearest Neighbor (NN) queries are frequently used for location-dependent information services. In this paper, we study a new NN query called Path-based Constrained Nearest Neighbor (PCNN) query, which involves the additional constraints on non-spatial attribute values of data objects on processing a continuous NN search along a path. For PCNN query processing, we propose an efficient PCNN query method based on transformation idea. The proposed method transforms a continuous NN search into static NN queries at discrete intersection nodes. We further leverage peer-to-peer sharing to improve the proposed method. Extensive experiments are conducted, and the results demonstrate the effectiveness of our methods. © 2012 Springer-Verlag.	nearest neighbor search	Yingyuan Xiao;Yan Shen;Tao Jiang;Heng Wang	2012		10.1007/978-3-642-32600-4_37	nearest-neighbor chain algorithm;r-tree;ball tree;nearest neighbor graph;best bin first;cover tree;nearest neighbor search;fixed-radius near neighbors	AI	-26.02181718947176	0.08992168953840646	128005
07e25e40777c615597c9c257892540ce11063efa	predicate result range caching for continuous queries	bond markets;frequencies;quantiles;sorting;continuous query;sliding windows;data streams;graphics processors;memory bandwidth;large classes	Many analysis and monitoring applications require the repeated execution of expensive modeling functions over streams of rapidly changing data. These applications can often be expressed declaratively, but the continuous query processors developed to date are not designed to optimize queries with expensive functions. To speed up such queries, we present CASPER: the CAching System for PrEdicate Result ranges. CASPER computes and caches predicate result ranges, which are ranges of stream input values where the system knows the results of expensive predicate evaluations. Over time, CASPER expands ranges so that they are more likely to contain future stream values. This paper presents the CASPER architecture, as well as algorithms for computing and expanding ranges for a large class of predicates. We demonstrate the effectiveness of CASPER using a prototype implementation and a financial application using real bond market data.	algorithm;central processing unit;experiment;mathematical optimization;memoization;numerical analysis;predicate (mathematical logic);prototype;scheduling (computing);stream (computing);stress testing	Matthew Denny;Michael J. Franklin	2005		10.1145/1066157.1066231	bond market;real-time computing;quantile;computer science;sorting;theoretical computer science;frequency;database;data stream mining;programming language;memory bandwidth	DB	-30.98084349790197	2.4108590416728766	128363
75c4e37dd340c96e72ca46cb8aa468877dc3fc2d	application challenges: system health management for complex systems	aide diagnostic;systeme temps reel;eficacia sistema;system reliability;fiabilite systeme;architecture systeme;operant conditioning;real time;sistema informatico;mantenimiento sistema;performance systeme;computer system;real time data;system performance;fiabilidad sistema;health management;complex system;high performance computer;arquitectura sistema;real time system;systeme informatique;sistema tiempo real;system architecture;condition based maintenance;system maintenance;diagnostic aid;maintenance systeme;ayuda diagnostica	System Health Management (SHM) is an example of the types of challenging applications facing embedding high-performance computing environments. SHM systems monitor real-time sensors to determine system health and performance. Performance, economics, and safety are all at stake in SHM, and the emphasis on health management technology is motivated by all these considerations. This paper describes a project focusing on condition-based maintenance (CBM) for naval ships. Condition-based maintenance refers to the identification of maintenance needs based on current operational conditions. In this project, system architectures and diagnostic and prognostic algorithms are being developed that can efficiently undertake real-time data analysis from appropriately instrumented machinery aboard naval ships and, based on the analysis, provide feedback to human users regarding the state of the machinery – such as its expected time to failure, the criticality of the equipment for current	algorithm;average-case complexity;complex systems;content management system;criticality matrix;norm (social);real-time data;real-time locating system;sensor;super high material cd;supercomputer	George D. Hadden;Peter Bergstrom;Tariq Samad;Bonnie Holte Bennett;George J. Vachtsevanos;Joe Van Dyke	2000		10.1007/3-540-45591-4_108	real-time data;complex systems;simulation;real-time operating system;computer science;operant conditioning;health management system;computer performance;operations research;systems architecture	AI	-22.772426546351657	-4.493553597251791	128468
a13c628b3374f43071a1af8de1b2c14900906fd2	iceberg: exploiting context in information brokering agents	systeme intelligent;query reformulation;integration information;sistema inteligente;multidatabase;systeme base connaissances;systeme recherche information;information integration;quality of information;base multiple donnee;integracion informacion;intelligent system;information retrieval systems;intermediaire information;information broker;query expansion;knowledge based systems;intermediario informacion	The research reported in this paper has both a scie ntific and a commercial aim. The scientific interest is to explo re the use of contexts in order to improve the quality of information brok ering. In this paper it is shown that contexts in information brokering can be exploited to enable four directions of query reformulation: up a nd down (standard query expansion) and sideway reformulations of the user request that can even involve going from one context to another.	query expansion	Catholijn M. Jonker;Arjen Vollebregt	2000		10.1007/978-3-540-45012-2_4	query expansion;computer science;information integration;knowledge-based systems;data mining;database;information quality;information retrieval	NLP	-25.525434179037386	-2.437365155116339	128491
bfc18065d8a49b3661144f2f8d6d32bcac3a44f5	extending clips to support temporal representation and reasoning	temporal expert systems;clips;workflow;temporal representation;truth maintenance	Applications using expert systems for monitoring and control problems often require the ability to represent temporal knowledge and to apply reasoning based on that knowledge. Incorporating temporal representation and reasoning into expert systems leads to two problems in development: dealing with an implied temporal order of events using a non-procedural tool; and maintaining the large number of temporal relations that can occur among facts in the knowledge base. In this paper we explore these problems by using an expert system shell, CLIPS (C Language Integrated Production System), to create temporal relations using common knowledge-based constructs. We also build an extension to CLIPS through a user-defined function which generates the temporal relations from those facts. We use the extension to create and maintain temporal relations in a workflow application that monitors and controls an engineering design change review process. We also propose a solution to ensure truth maintenance among temporally related facts that links our temporal extension to the CLIPS facility for truth maintenance.	clips;engineering design process;expert system;knowledge base;production system (computer science);reason maintenance;regular expression;user-defined function	Susan J. Chinn;Gregory R. Madey	1999	Expert Systems	10.1111/1468-0394.00097	workflow;computer vision;computer science;artificial intelligence;data mining	AI	-21.505851460224527	2.069849279226058	128666
799340ff123462bab6991b99b21960d51aa8affb	planning using multiple execution architectures		We discuss two techniques used by the RALPHMEA agent architecture to facilitate decision making in complex, real-time domains. Multiple execution architectures are four implementations of the agent function, a function that receives percepts from the environment as input and outputs an action choice. The four execution architectures are defined by the different knowledge types that each uses. Depending on the domain and agent capabilities, each execution architecture has different speed and correctness properties. Metalevel control of planning computes the value of information of planning to compare to the utility of executing the current default plan. Examples are presented from an autonomous, underwater vehicle domain.	agent architecture;autonomous robot;correctness (computer science);real-time clock;software performance testing	Gary H. Ogasawara;Stuart J. Russell	1993			real-time computing;simulation;computer science;distributed computing	AI	-19.341544964220947	-8.299618781751004	128751
5fd6c4c8e631309594d75456c376e9a34c719ccf	a cognitive approach to real-time rescheduling using soar-rl	informatica;manufacturing systems;object recognition;learning;objeto de conferencia;real time;reinforcement learning;cognitive architecture;soar;manufacturing;ciencias informaticas;rescheduling	Ensuring flexible and efficient manufacturing of customized products in an increasing dynamic and turbulent environment without sacrificing cost effectiveness, product quality and on-time delivery has become a key issue for most industrial enterprises. A promising approach to cope with this challenge is the integration of cognitive capabilities in systems and processes with the aim of expanding the knowledge base used to perform managerial and operational tasks. In this work, a novel approach to real-time rescheduling is proposed in order to achieve sustainable improvements in flexibility and adaptability of production systems through the integration of artificial cognitive capabilities, involving perception, reasoning/learning and planning skills. Moreover, an industrial example is discussed where the SOAR cognitive architecture capabilities are integrated in a software prototype, showing that the approach enables the rescheduling system to respond to events in an autonomic way, and to acquire experience through intensive simulation while performing repair tasks.	autonomic computing;cognitive architecture;knowledge base;prototype;real-time clock;real-time transcription;simulation;soar (cognitive architecture);software prototyping;turbulence	Juan Cruz Barsce;Jorge A. Palombarini;Ernesto C. Martínez	2013	CoRR		simulation;engineering;artificial intelligence;operations management	Robotics	-19.692037436685126	-6.876506162852388	128998
6109c80c0314e458c426e63e2971221fd3108c91	stream: the stanford stream data manager	data management;image database;classification;cluster merging;content based image retrieval;relevance feedback	STREAM is a general-purpose relational Data Stream Management System (DSMS). STREAM supports a declarative query language and flexible query execution plans. It is designed to cope with high data rates and large numbers of continuous queries through careful resource allocation and use, and by degrading gracefully to approximate answers as necessary. A description of language design, algorithms, system design, and implementation as of late 2002 can be found in [3]. The demonstration focuses on two aspects:	approximation algorithm;declarative programming;general-purpose macro processor;query language;relational data stream management system;stream (computing);systems design	Arvind Arasu;Brian Babcock;Shivnath Babu;Mayur Datar;Keith Ito;Itaru Nishizawa;Justin Rosenstein;Jennifer Widom	2003		10.1145/872757.872854	biological classification;data management;computer science;data mining;database;data stream mining;information retrieval	DB	-32.07343690031844	1.5360629158362276	129063
bc3929e055a940ca2bdf3abf5cb76fe9c497191a	handling topical metadata regarding the validity and completeness of multiple-source information: a possibilistic approach		We study the problem of aggregating metadata about the validity and/or completeness, with respect to given topics, of information provided by multiple sources. For a given topic, the validity level reflects the certainty that the information stored is true. The completeness level of a source on a given topic reflects the certainty that a piece of information that is not stored is false. We propose a modeling based on possibility theory which allows the fusion of such multi-source information into a graded belief base.	datalog;knowledge base;location (geography);multi-source;possibility theory;taxonomy (general)	Célia da Costa Pereira;Didier Dubois;Henri Prade;Andrea Tettamanzi	2017		10.1007/978-3-319-67582-4_26	completeness (statistics);data mining;computer science;metadata;information retrieval;possibility theory;certainty	DB	-19.51640350530859	0.18411184623141977	129114
4a5b375fb7ce42de22f07e5a7820391dcfa5c0f2	algorithmic debugging for intelligent tutoring: how to use multiple models and improve diagnosis	inproceedings	Intelligent tutoring systems (ITSs) are capable to intelligently diagnose learners’ problem solving behaviour only in limited and well-defined contexts. Learners are expected to solve problems by closely following a single prescribed problem solving strategy, usually in a fixedorder, step by step manner. Learners failing to match expectations are often met with incorrect diagnoses even when human teachers would judge their actions admissible. To address the issue, we extend our previous work on cognitive diagnosis, which is based on logic programming and meta-level techniques. Our novel use of Shapiro’s algorithmic debugging now analyses learner input independently against multiple models. Learners can now follow one of many possible algorithms to solve a given problem, and they can expect the tutoring system to respond with improved diagnostic quality, at negligible computational costs.	admissible heuristic;algorithm;chaos theory;computation;debugging;expert system;failure;irreducibility;logic programming;observable;problem solving;prolog	Claus Zinn	2013		10.1007/978-3-642-40942-4_24	simulation;computer science;artificial intelligence;algorithm	AI	-20.16098050223576	-4.7937319505958875	129196
50ab5161bb70bbc79e762fc113656fae7c19b9fb	navigation for digital actors based on synthetic vision, memory, and learning	moving image;navegacion;animacion por computador;vision ordenador;representacion conocimientos;systeme intelligent;visual memory representation;learning;3d visualization;local navigation system;programming environment;information retrieval;three dimensional shape;sistema inteligente;base connaissance;raisonnement;imagen movil;forma tridimensional;image mobile;computer vision;sintesis imagen;aprendizaje;image synthesis;medio ambiente programacion;dynamic environment;navigation;apprentissage;obstacle avoidance;forme tridimensionnelle;recherche information;intelligent system;razonamiento;synthese image;base conocimiento;visual memory;vision ordinateur;synthetic vision;recuperacion informacion;reasoning;learning artificial intelligence;computer animation;knowledge representation;representation connaissances;octrees;environnement programmation;knowledge base;computerised navigation;animation par ordinateur	This paper describes an animation approach where synthetic vision is used for navigation by a digital actor. The vision is the only channel of information between the actor and its environment and offers a universal approach to pass the necessary information from the environment to an actor in the problems of path searching, obstacle avoidance, and internal knowledge representation with learning and forgetting characteristics. For the general navigation problem, we propose a local and a global approach. In the global approach, a dynamic octree serves as global 3D visual memory and allows an actor to memorize their environment that he sees and to adapt it to a changing and dynamic environment. His reasoning process allows him to find 3D paths based on his visual memory by avoiding impasses and circuits. In the local approach, low level vision based navigation reflexes, normally performed by intelligent actors, are simulated. The local navigation model uses the direct input information from his visual environment to reach goals or subgoals and to avoid unexpected obstacles.	knowledge representation and reasoning;motion planning;obstacle avoidance;octree;synthetic vision system	Hansrudi Noser;Olivier Renault;Daniel Thalmann;Nadia Magnenat-Thalmann	1995	Computers & Graphics	10.1016/0097-8493(94)00117-H	computer vision;knowledge base;navigation;simulation;visualization;visual memory;computer science;artificial intelligence;computer animation;obstacle avoidance;synthetic vision system;mobile robot navigation;reason	Robotics	-23.09928446951092	-7.781114720026456	129243
949a17e9e91798dc0b5a8da54443416a5d7067d8	an application of knowledge based modelling using scripts	reusability;case base reasoning;knowledge management;scripts;knowledge modelling;knowledge based engineering;case based reasoning;knowledge base	Knowledge management (KM) takes an increasingly significant place in the companies. The field of the KM aims to answer the problems of memory within companies by proposing methodologies to formalise know-how during the different steps of production. The KM is a domain with many ramifications and applications. One of them, so called Knowledge Based Engineering, search how to record knowledge from experts to put them in CAD software. Our study relates to the possibility of using the knowledge of an expert in modelling and, more particularly, on the automatic modelling of filling systems in foundry.		Nicolas Gardan;Yvon Gardan	2003	Expert Syst. Appl.	10.1016/S0957-4174(03)00096-4	legal expert system;case-based reasoning;reusability;knowledge base;knowledge integration;computer science;knowledge management;artificial intelligence;body of knowledge;model-based reasoning;knowledge-based systems;knowledge engineering;open knowledge base connectivity;data mining;procedural knowledge;knowledge extraction;personal knowledge management;knowledge value chain;domain knowledge	DB	-31.444096482468158	-6.496599333569806	129245
83aa3b3d2eff030581178640e2ac80834ba2f404	taxonomy of scheduling systems as a basis for the study of strategic behavior	operateur humain;factor humano;human machine systems;man machine interaction;operator;operador humano;comportement;behavioral research;ergonomia;analisis ocupacional;job analysis;performance;hombre;estrategia;systematique;sistema complejo;ergonomie;psychology;strategic planning;man machine system;strategy;strategic behavior;sequencing;modelo;systeme complexe;sistematica;conducta;complex system;scheduling;human factor;theory;taxonomy;human;planificacion trabajo;human operator;sistema hombre maquina;production schedule;modele;behavior;analyse travail;operators persons;strategie;facteur humain;ergonomics;models;planification travail;homme;systeme homme machine	"""Strategic behavior is frequently characterized by the need to decide among several courses of action, each of which may lead to a desired goal, subject to time constraints. Often strategic behavior can be regarded as a series of answers to the question, """"In what sequence should I perform the set of actions required, and when should I start and stop each of them?"""" Scheduling theory, which is usually used to determine the sequencing of operations in such settings as transportation and manufacturing, provides normative answers to such a question. The authors introduce the concepts and terminology of scheduling theory and show how these can be identified with aspects of human operator behavior. Scheduling theory can provide a systematic conceptual framework for planning research on behavior in complex human-machine settings, both in and beyond laboratory contexts. It can be used to discover optimal or satisficing strategies and to provide norms against which to measure the quality of strategic decision making and performance in complex systems. The use of scheduling theory is one example of the many well-developed quantitative models available in operations research that are applicable to the analysis of behavior, well beyond the discrete trials paradigm that often characterizes human factors laboratory research."""	evolutionary taxonomy	M. I. Dessouky;Neville Moray;Brian Kijowski	1995	Human Factors	10.1518/001872095779049282	performance;strategy;computer science;engineering;artificial intelligence;human factors and ergonomics;operator;sequencing;operations research;scheduling;job analysis;theory;behavior	HCI	-24.890381486236116	-6.612854380676034	129252
1ba250d66f1f70adf76080ae4da3db88d25eaa58	query optimizations over decentralized rdf graphs	query processing;resource description framework;time factors;pattern matching;life sciences;scalability;parallel processing	Applications in life sciences, decentralized social networks, Internet of Things, and statistical linked dataspaces integrate data from multiple decentralized RDF graphs via SPARQL queries. Several approaches have been proposed to optimize query processing over a small number of heterogeneous data sources by utilizing schema information. In the case of schema similarity and interlinks among sources, these approaches cause unnecessary data retrieval and communication, leading to poor scalability and response time. This paper addresses these limitations and presents Lusail, a system for scalable and efficient SPARQL query processing over decentralized graphs. Lusail achieves scalability and low query response time through various optimizations at compile and run times. At compile time, we use a novel locality-aware query decomposition technique that maximizes the number of query triple patterns sent together to a source based on the actual location of the instances satisfying these triple patterns. At run time, we use selectivity-awareness and parallel query execution to reduce network latency and to increase parallelism by delaying the execution of subqueries expected to return large results. We evaluate Lusail using real and synthetic benchmarks, with data sizes up to billions of triples on an in-house cluster and a public cloud. We show that Lusail outperforms state-of-the-art systems by orders of magnitude in terms of scalability and response time.	cloud computing;compile time;compiler;data retrieval;database;dataspaces;degree of parallelism;internet of things;locality of reference;parallel computing;qr decomposition;resource description framework;response time (technology);run time (program lifecycle phase);sparql;sql;scalability;selectivity (electronic);social network;synthetic data;triplestore	Ibrahim Abdelaziz;Essam Mansour;Mourad Ouzzani;Ashraf Aboulnaga;Panos Kalnis	2017	2017 IEEE 33rd International Conference on Data Engineering (ICDE)	10.1109/ICDE.2017.59	parallel processing;sargable;query optimization;query expansion;web query classification;scalability;computer science;theoretical computer science;pattern matching;rdf;data mining;database;distributed computing;rdf query language;programming language;web search query;query language	DB	-31.013677285923464	1.7581478588723352	129271
25cccf4be616214b1d3e2d52d18006bb0b93f982	a framework for selecting between knowledge-based and traditional systems design	systems design;estudio comparativo;base connaissance;conception;etude comparative;knowledge based language;system design;comparative study;diseno;base conocimiento;design;information system;systeme information;sistema informacion;knowledge base;expert system	Extensive coverage of knowledge-based languages has appeared in the recent literature. However, there has been no discussion of the criteria to be used in selecting between a knowledge-based approach and a traditional, that is, non- knowledge-based, approach for a particular application. This paper presents a framework of application-based criteria to assist in this selection. It also applies this decision framework to a number of real and hypothetical applications.		Tom Murray;Mohan Tanniru	1987	J. of Management Information Systems	10.1080/07421222.1987.11517785	knowledge base;computer science;artificial intelligence;expert system;systems design	AI	-24.722882768212713	-2.8902434482531643	129428
4b05390c4ec4a34701672146f12f40a6cac86324	best effort query processing in dht-based p2p systems	p2p system;nearest neighbor searches;query processing nearest neighbor searches computer science buildings large scale systems peer to peer computing robustness automation application software internet;distributed data management;query processing;distributed hash table;application software;best effort;large scale;internet;query evaluation;robustness;nearest neighbor search;computer science;peer to peer computing;buildings;large scale systems;structured data;automation	Structured P2P systems in the form of distributed hash tables (DHT) are a promising approach for building massively distributed data management platforms. However, for many applications the supported key lookup queries are not sufficient. Instead, techniques for managing and querying (relational) structured data are required. In this paper, we argue that in order to cope with the dynamics in large-scale P2P systems such query techniques should be work in a best effort manner. We describe such operations (namely grouping/ aggregation, similarity and nearest neighbor search) and discuss appropriate query evaluation strategies.	approximation algorithm;best-effort delivery;database;distributed hash table;experiment;load balancing (computing);lookup table;nearest neighbor search;peer-to-peer;query optimization;range query (data structures);sql;social network aggregation	Philipp Rösch;Kai-Uwe Sattler;Christian von der Weth;Erik Buchmann	2005	21st International Conference on Data Engineering Workshops (ICDEW'05)	10.1109/ICDE.2005.200	best-effort delivery;query optimization;application software;the internet;data model;computer science;chord;automation;data mining;database;nearest neighbor search;world wide web;robustness	DB	-29.753136430888897	0.1929329227992147	129483
f76c85ad0d17debe7a980e70c0d32d9b8d7e117b	a communication efficient probabilistic algorithm for mining frequent itemsets from a peer-to-peer network	probabilistic algorithm;sampling;data mining	Data intensive large-scale distributed systems like peer-to-peer (P2P) networks are becoming increasingly popular where centralization of data is impossible for mining and analysis. Unfortunately, most of the existing data mining algorithms work only when data can be accessed in its entirety. Finding all the network-wide frequent itemsets is computationally difficult and usually has large communication overhead in such environment. This paper focuses on developing a communication efficient algorithm for discovering frequent itemsets from a P2P network. A sampling-based approach is adopted to find approximate solution instead of an exact solution with probabilistic guarantee. The benefit of approximation technique is reflected in the low communication overhead in discovering majority of frequent itemsets with probabilistic guarantee. The main principal followed by the algorithm assumes that an independent and identically distributed (iid) sample of the entire data is available at one location to generate a set of candidate itemsets. Collecting iid sample from a P2P network is a challenging problem because of varying degrees of connectivity and sizes of data shared. The paper first addresses this issue and shows how an iid sample of nodes and data can be collected from a P2P network using random walk. It applies the proposed sampling technique to identify most of the frequent itemsets from a P2P network. Theoretical analysis shows how to decide about optimum sample size and minimize communication to compute the results. Experimental results show that the proposed algorithm discovers all of the network-wide frequent itemsets using communication that scales sublinearly with network and datasize. © 2009 Wiley Periodicals, Inc. Statistical Analysis and Data Mining 2: 48-69, 2009	peer-to-peer;randomized algorithm	Souptik Datta;Hillol Kargupta	2009	Statistical Analysis and Data Mining	10.1002/sam.10033	sampling;computer science;data science;data mining;database;mathematics;randomized algorithm	ML	-24.00663153664599	2.7978212736494568	129489
688ef3247342fb1a2292503125bb3b1ccde6a1ea	distributed query processing in an ad-hoc semantic web data sharing system	database indexing;optimisation;ubiquitous computing database indexing file organisation optimisation peer to peer computing query processing relational algebra semantic web;query processing;peer to peer computing resource description framework indexes query processing distributed databases optimization;query optimization;distributed query processing;hybrid p2p;semantic web data sharing;semantic web data sharing ad hoc distributed query processing hybrid p2p query optimization;semantic web;ubiquitous computing;peer to peer computing;ad hoc;relational algebra;ad hoc semantic web data sharing system resource description framework relational algebra optimization database systems optimization techniques distributed sparql query processing two level distributed hashing techniques two level distributed index techniques data sharing scenario hybrid p2p architecture personal users dynamic ad hoc settings distributed computing technologies decentralized environment rdf triples proprietary datasets;file organisation	Sharing the Semantic Web data in proprietary datasets in which data is encoded in RDF triples in a decentralized environment calls for efficient support from distributed computing technologies. The highly dynamic ad-hoc settings that would be pervasive for Semantic Web data sharing among personal users in the future, however, pose even more demanding challenges for the enabling technologies. We extend previous work on a hybrid P2P architecture for an ad-hoc Semantic Web data sharing system which better models the data sharing scenario by allowing data to be maintained by its own providers and exhibits satisfactory scalability owing to the adoption of a two-level distributed index and hashing techniques. Additionally, we propose efficient distributed processing of SPARQL queries in such a context and explore optimization techniques that build upon distributed query processing for database systems and relational algebra optimization. We anticipate that our work will become an indispensable, complementary approach to making the Semantic Web a reality by delivering efficient data sharing and reusing in an ad-hoc environment.	computation;database;distributed computing;experiment;hoc (programming language);lookup table;mathematical optimization;peer-to-peer;performance evaluation;pervasive informatics;programming paradigm;query language;query optimization;relational algebra;response time (technology);ring network;sparql;scalability;self-organization;semantic web	Jing Zhou;Gregor von Bochmann;Zhongzhi Shi	2013	2013 IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum	10.1109/IPDPSW.2013.53	semantic computing;data web;computer science;sparql;social semantic web;linked data;semantic web stack;database;world wide web;information retrieval;semantic analytics	DB	-30.023717354266463	0.7769610128318789	129722
279559b4638434166490144cdeb587cd48fcb0bd	index-based query processing methods for text and spatial databases			database	Xiaoling Zhou	2017				DB	-28.046344087956594	0.2399911397623389	129821
044d3d66cf88c9fcdda54a6ec9270ffba13fad69	an emusim technique and its components in cloud computing- a review	cloud computing;parallel	As with the development of the IT technologies, the amount of accumulated data is also increasing. Thus the role of data mining comes into picture. Association rule mining becomes one of the significant responsibilities of descriptive technique which can be defined as discovering meaningful patterns from large collection of data. The frequent pattern mining algorithms determine the frequent patterns from a database. Mining frequent itemset is very fundamental part of association rule mining. Many algorithms have been proposed from last many decades including majors are Apriori, Direct Hashing and Pruning, FP-Growth, ECLAT etc. The aim of this study is to analyze the existing techniques for mining frequent patterns and evaluate the performance of them by comparing Apriori and DHP algorithms in terms of candidate generation, database and transaction pruning. This creates a foundation to develop newer algorithm for frequent pattern mining. Keywords— Association rule, Frequent pattern mining, Apriori, DHP, Foundation Implementation Study	apriori algorithm;association rule learning;cloud computing;data mining	Rahul Malhotra;Prince Jain	2013	CoRR	10.14445/2231-2803	parallel computing;simulation;cloud computing;operating system;distributed computing;world wide web;computer security	DB	-33.05550188426953	-2.4678358127438256	129887
e44b3910d11911a210f2f7d5ae2fa3974dd73ee5	probabilistic databases for decision analysis	base relacional dato;intervalo;representacion conocimientos;base donnee;mise a jour;incertidumbre;uncertainty;selection;database;base dato;intelligence artificielle;prise decision;relational database;intervalle;base donnee probabiliste;decision analysis;probabilistic database;interval;probability theory;base donnee relationnelle;theorie probabilite;artificial intelligence;teoria probabilidad;puesta al dia;incertitude;inteligencia artificial;seleccion;knowledge representation;toma decision;representation connaissances;base dato probabilistica;updating	Abstract#R##N##R##N#A probabilistic database is defined in a previous article [R. Cavallo and M. Pittarelli, Proc. 13th Int. Conf. on Very Large Databases (VLDB); 1987; see Ref. 9] as a collection of probability distributions over Cartesian products of finite variable domains. the concept is extended here to accommodate interval-valued probabilities. Algebraic operations for both real- and interval-valued probabilities databases analogous to those for relational databases are defined. Techniques for making inferences regarding joint distributions on subsets of the variables over which a probabilistic database is defined are developed. These are illustrated through application to a problem of decision analysis under partial uncertainty. Connections between the probabilistic database formalism and other forms of data representation are discussed.	decision analysis;probabilistic database	Michael Pittarelli	1990	Int. J. Intell. Syst.	10.1002/int.4550050206	interval;probability theory;selection;database theory;uncertainty;decision analysis;relational database;computer science;probabilistic database;artificial intelligence;data mining;mathematics;algorithm	DB	-21.090403493909033	-1.4448696930195783	130025
e8498aaa63e1ae6614f628a8b4181c02e3c2e7a6	a constraint-based robotic soccer team	hybrid dynamical system;robot movil;representacion conocimientos;multiagent system;constraint nets;systeme evenement discret;sistema hibrido;systeme discret;real time control;soccer;integrable system;cooperation;real time;juego cooperativo;continuous system;robotics;systeme integre;sistema integrado;constraint satisfaction;cooperacion;cooperative game;deporte equipo;dynamical system;systeme continu;sistema acontecimiento discreto;systeme dynamique;sport equipe;satisfaction contrainte;semantic model;discrete event system;ubc;robot mobile;jeu cooperatif;sistema continuo;football;robotic architecture;hybrid system;robotica;algorithme evolutionniste;algoritmo evolucionista;robotique;satisfaccion restriccion;robot soccer;evolutionary algorithm;sistema dinamico;sistema discreto;knowledge representation;sistema multiagente;multi agent collaboration;representation connaissances;integrated system;team sport;moving robot;discrete system;systeme multiagent;robot team;systeme hybride;futbol	It is a challenging task for a team of multiple fast-moving robots to cooperate with each other and to compete with another team in a dynamic, real-time environment. For a robot team to play soccer successfully, various technologies have to be incorporated including robotic architecture, multi-agent collaboration and real-time reasoning. A robot is an integrated system, with a controller embedded in its plant. A robotic system is the coupling of a robot to its environment. Robotic systems are, in general, hybrid dynamic systems, consisting of continuous, discrete and event-driven components. Constraint Nets (CN) provide a semantic model for modeling hybrid dynamic systems. Controllers are embedded constraint solvers that solve constraints in real-time. A controller for our robot soccer team, UBC Dynamo98, has been modeled in CN, and implemented in Java, using the Java Beans architecture. A coach program using an evolutionary algorithm has also been designed and implemented to adjust the weights of the constraints and other parameters in the controller. The results demonstrate that the formal CN approach is a practical tool for designing and implementing controllers for robots in multi-agent real-time environments. They also demonstrate the effectiveness of applying the evolutionary algorithm to the CN-modeled controllers.	abstraction layer;agent-based model;control system;controller (computing);dynamical system;embedded system;event-driven programming;evolutionary algorithm;java platform, enterprise edition;multi-agent system;nondeterministic algorithm;partially observable system;real-time clock;real-time transcription;robot;robot control;state space;utility	Yu Zhang;Alan K. Mackworth	2002	Constraints	10.1023/A:1017936325308	semantic data model;integrable system;simulation;real-time control system;constraint satisfaction;computer science;artificial intelligence;dynamical system;discrete system;evolutionary algorithm;robotics;cooperation;hybrid system	Robotics	-22.63398600607533	-6.550171392328131	130194
e81f70e10e0da2a802f820bad2491e668610845f	the design and implementation of a real time visual search system on jd e-commerce platform		We present the design and implementation of a visual search system for real time image retrieval on JD.com, the worldu0027s third largest and Chinau0027s largest e-commerce site. We demonstrate that our system can support real time visual search with hundreds of billions of product images at sub-second timescales and handle frequent image updates through distributed hierarchical architecture and efficient indexing methods. We hope that sharing our practice with our real production system will inspire the middleware communityu0027s interest and appreciation for building practical large scale systems for emerging applications, such as e-commerce visual search.		Jie Li;Haifeng Liu;Chuanghua Gui;Jianyu Chen;Zhenyuan Ni;Ning Wang;Yuan Chen	2018		10.1145/3284028.3284030	architecture;multimedia;visual search;e-commerce;search engine indexing;scalability;image retrieval;middleware;computer science	Graphics	-30.412095499768697	-1.7352033140575647	130348
433f593ca2d22cbd93aa35c694091413867a0f9f	study on the development of design rationale management system for chemical process safety	modelizacion;analyse risque;presentation information;proceso concepcion;controleur logique programmable;concepcion ingenieria;engineering design;ciclo desarrollo;design process;management system;controlador logica programable;formacion;reseau electrique;life cycle;securite;electrical network;risk analysis;conception ingenierie;gestion sistema;red electrica;information layout;safety systems;systeme recherche;intelligence artificielle;formation;process design;preparacion serie fabricacion;search system;modelisation;analisis riesgo;saber hacer;programmable logical controller;sistema quimico;chemical system;sistema investigacion;know how;savoir faire;cognition;safety;cycle developpement;design rationale;systeme securite;cognicion;risk assessment;artificial intelligence;inteligencia artificial;process planning;systeme chimique;safety management;seguridad;preparation gamme fabrication;modeling;system management;presentacion informacion;gestion systeme;evaluation risque;processus conception	Chemical process design is one of the most important activities in the plant life cycle (PLC). To manage the safety of process plant through the PLC, the system environment to enable recording and accessing design rationale (DR) of the current process and/or plant is indispensable. Theoretical DR information which is discussed on process design activity is very important for safety management activities. However, conventional process risk assessment and safety design mainly depends on engineers experience, know-how, sense of value etc, and it has not been recorded explicitly and could not be shared for other PLC activities. This paper proposes a DR management model (DR model) for the chemical process design and a simplified DR search system for managing the process design information.	design rationale;environment variable;inference engine;management system;prolog;risk assessment	Yukiyasu Shimada;Takashi Hamaguchi;Tetsuo Fuchino	2005		10.1007/11552413_29	risk assessment;process design;biological life cycle;electrical network;systems management;systems modeling;cognition;design process;risk analysis;management system;operations research;system safety;design rationale;engineering design process	HCI	-24.20863418451347	-5.260385630036725	130365
723fa1a2b95395c80bf08b205749b2e579c4144e	high-performance geospatial analytics in hyperspace	geospatial data processing;indexing schemes	In the past few years, massive amounts of location-based data has been captured. Numerous datasets containing user location information are readily available to the public. Analyzing such datasets can lead to fascinating insights into the mobility patterns and behaviors of users. Moreover, in recent times a number of geospatial data-driven companies like Uber, Lyft, and Foursquare have emerged. Real-time analysis of geospatial data is essential and enables an emerging class of applications. Database support for geospatial operations is turning into a necessity instead of a distinct feature provided by only a few databases. Even though a lot of database systems provide geospatial support nowadays, queries often do not consider the most current database state. Geospatial queries are inherently slow given the fact that some of these queries require a couple of geometric computations. Disk-based database systems that do support geospatial datatypes and queries, provide rich features and functions, but they fall behind when performance is considered: specifically if real-time analysis of the latest transactional state is a requirement. In this demonstration, we present HyPerSpace, an extension to the high-performance main-memory database system HyPer developed at the Technical University of Munich, capable of processing geospatial queries with sub-second latencies.	computation;in-memory database;real-time clock;real-time web	Varun Pandey;Andreas Kipf;Dimitri Vorona;Tobias Mühlbauer;Thomas Neumann;Alfons Kemper	2016		10.1145/2882903.2899412	computer science;data science;data mining;database;spatial database	DB	-32.938873398268214	-1.4717494561074966	130499
1de68185800bfa1cdfc9d6ad3d778acb34266896	case-base maintenance in a multimodal reasoning system	case base maintenance;competence models;utility problem	The definition of suitable case-base maintenance policies is widely recognized as a major key to success for case-based systems; underestimating this issue may lead to systems that either do not fulfill their role of knowledge management and preservation or that do not perform adequately under performance dimensions, namely, computation time and competence and quality of solutions. The goal of this article is to analyze some automatic case-base management strategies in the context of a multimodal architecture combining case-based reasoning and model-based reasoning. We propose and compare two different methodologies, the first one, called replace, is a competence-based strategy aimed at replacing a set of stored cases with the current one, if the latter exhibits an estimated competence comparable with the estimated competence of the considered set of stored cases. The second one, called learning by failure with forgetting (LFF), is based on incremental learning of cases interleaved with off-line processes of forgetting (deleting) cases whose usage does not fulfill specific utility conditions. Results from an extensive experimental analysis in an industrial plant diagnosis domain are reported, showing the usefulness of both strategies with respect to the maintenance of suitable performance levels for the target system.	multimodal interaction;reasoning system	Luigi Portinale;Pietro Torasso	2001	Computational Intelligence	10.1111/0824-7935.00144	artificial intelligence;management science	AI	-20.45922915094681	-6.532562562929222	130510
07744eb58b2e15d0c1b5546caa0a029f2eef36d6	using vtree indices for queries over objects with complex motions	database indexing;query processing;space exploration costs delay spatiotemporal phenomena computer science query processing algorithm design and analysis tree data structures performance gain data engineering;index structure;indexation;spatiotemporal phenomena;spatiotemporal phenomena query processing database indexing;efficient query processing;vtree join vtree index structure spatiotemporal query processing	We introduce the vTree, an index structure for efficient processing of spatiotemporal queries over sets of objects moving along complex trajectories. The vTree is a tiered structure, and partitions space at different granularities at different tiers. It uses two novel strategies to enhance the performance of spatiotemporal queries. First, it groups objects by velocity, and indexes objects from each group at an appropriate tier in the vTree, to localize the loss of precision induced by fast objects. Second, it accommodates complex trajectories by controlled replication of object descriptors at each tier. These features permit vTree indices to remain useful for longer time durations, and to support very efficient query processing. Our algorithms for vTree joins are designed to limit the portions of index and data space explored, as well as to maximize locality within the portion of space explored.	algorithm;database;dataspaces;locality of reference;multitier architecture;recursive join;velocity (software development)	Sandeep Gupta;Chinya V. Ravishankar	2004	Proceedings. 20th International Conference on Data Engineering	10.1109/ICDE.2004.1320062	database index;query optimization;computer science;theoretical computer science;data mining;database;spatiotemporal database	DB	-26.89641146009217	1.3224075858815776	130569
b15c62988413d2a9d7c054d4e2e15ddbcd45661d	an interface analysis method of complex information system by introducing error factors		With the rapid developments of computer technology and information technology, human-machine interfaces of aircrafts, ships, nuclear power plants, battlefield command system, and other complex information systems have evolved from the traditional control mode to digital control mode with visual information interface. This paper studies error factors of information interface in human-computer interaction based on visual cognition theory. A feasible error-cognition model is established to solve some design problems which result in serious failures in information recognition and analysis, and even in operation and execution processes. Based on Rasmussen, Norman, Reason and other error types as well as the HERA and CREAM failure identification models, we performed classification and cognitive characterization for error factors according to information search, information recognition, information identification, information selection and judgment as well as the decision-making process and obtained the comprehensive error-cognition model for complex information interface.	information system	Xiaoli Wu;Feng Zhou	2016		10.1007/978-3-319-40030-3_13	human–computer interaction;software engineering	Robotics	-25.90725060682989	-6.629435850105792	131018
84bee244e7498d17b7e30d2f017a90445d61cdd2	an efficient indexing technique for location prediction of moving objects	moving object;probability matrix;location based service;maintenance cost;cell;indexation;pct tree;moving objects;long range;location prediction;future index;future trajectory	The necessity of the future index is increasing to predict the future location of moving objects promptly for various location-based services. However, the prediction performance of most future indexes is lowered by the heavy load of extensive future trajectory search in long-range future queries, and their index maintenance cost is high due to the frequent update of future trajectories. Thus, this paper proposes the Probability Cell Trajectory-Tree (PCT-Tree), a cell-based future indexing technique for efficient long-range future location prediction. The PCT-Tree reduces the size of index by building the probability of extensive past trajectories in the unit of cell, and predicts reliable future trajectories using information on past trajectories. Therefore, the PCT-Tree can minimize the cost of communication in future trajectory prediction and the cost of index rebuilding for updating future trajectories. Through experiment, we proved the superiority of the PCT-Tree over existing indexing techniques in the performance of long-range future queries.		Dong-Oh Kim;Kang-Jun Lee;Dong-Suk Hong;Ki-Joon Han	2007		10.1007/978-3-540-74827-4_1	simulation;geography;operations management;data mining	HCI	-26.78142169210893	0.38730448362817077	131054
b368d58f507783e37ff565a2ac5c0157d7330ea3	analysis of distributed data base processing strategies	distributed data;decomposition;information retrieval;data management;multiprocessors;computer communications;data bases;searching;computer logic;computer architecture;interrogation;algorithms	In this paper we report on query processing experiments that were performed in one distributed data base environment. In this environment we compared the strategy produced by a collection of algorithms on the basis of number of bytes moved. Among other conclusions we found that limited search algorithms do not perform very well compared to algorithms which exhaust all possible processing plans.	distributed database	Robert S. Epstein;Michael Stonebraker	1980			data management;computer science;theoretical computer science;data mining;database;decomposition	DB	-28.07621787268121	3.6362034821192872	131055
0e9ec193a1e2b79ec43bb945817e0f6662fb510e	enhanced qualitative physical reasoning system: qupras	objet;representacion conocimientos;sistema experto;heritage;cambio;analysis and design;object;inhentance computer;raisonnement qualitatif;discontinuous;changement discontinu;raisonnement;hereucia;change;representation object;discontinuo;razonamiento;changement;razonamiento calitativo;discontinu;qualitative physics;qualitative reasoning;systeme expert;reasoning;knowledge representation;representation connaissances;objeto;expert system	There are many expert systems that use experimental knowledge for diagnostic analysis and design. However, there are two problems for systems using only experiential knowledge: (1) unexpected problems cannot be solved and (2) acquiring experiential knowledge from human experts is difficult. To solve these problems, general principles or basic knowledge must be added to expert systems in addition to the experimental knowledge. In response, we previously proposed Qupras (Qualitative physical reasoning system) as a framework for basic knowledge. This system has two knowledge representations, one related to physical laws and the other to objects. By using this knowledge, Qupras reasons about the relations among physical objects, and predicts the next state of a physical phenomenon. unexpected problems cannot be solved and acquiring experiential knowledge from human experts is difficult. Recently, we have improved some of Qupras’ features, and this pater desctibes the following main enhancements: (1) inheritance for representation of objects, (2) new primitive representations to describe discontinuous change, and (3) control features for effective reasoning. inheritance for representation of objects, new primitive representations to describe discontinuous change, and control features for effective reasoning.	effective method;expert system;reasoning system;regular expression	Masaru Ohki;Kiyokazu Sakane;Jun Sawamoto	1992	New Generation Computing	10.1007/BF03037480	knowledge representation and reasoning;legal expert system;qualitative reasoning;computer science;knowledge management;artificial intelligence;explicit knowledge;object;body of knowledge;model-based reasoning;knowledge-based systems;procedural knowledge;expert system;domain knowledge;reason	AI	-23.83833649818073	-4.106414027622358	131059
36b8f99417fa1a33e90786fd8912fe5aa5589d37	efficient and robust database support for data-intensive applications in dynamic environments	databases;query processing database management systems peer to peer computing;histograms;data intensive application;database system;query processing;database management systems;distributed computing;distributed database systems;p2p;data mining;robust database support;distributed query processing;grid database;data intensive applications;computer architecture;dynamic environment;grid database p2p distributed database systems;distributed database system;dynamic allocation;grid networks;database systems;distributed databases;distributed computations;dynamic refragmentation;p2p based distributed database system;peer to peer computing;distributed query processing robust database support data intensive applications distributed computations grid networks p2p based distributed database system dynamic refragmentation dynamic allocation;context;robustness database systems grid computing application software computer applications computer networks distributed computing data engineering memory computer science	Requirements from new types of applications call for new database system solutions. Computational science applications performing distributed computations on Grid networks with requirements for efficient storage and query solutions are now emerging. For this purpose we have developed DASCOSA-DB, a P2P-based distributed database system, which in addition to providing location-transparent storage and querying, also includes novel features like efficient partial restartof queries and redistribution of query operators in the context of failure, dynamic refragmentation and allocation, and distributed semantic caching. In this demo, the novel features will be demonstrated, combined with a more general description of the architecture and demonstration of the distributed query processing capabilities.	computation;computational science;decibel;distributed database;requirement	Jon Olav Hauglid;Kjetil Nørvåg;Norvald H. Ryeng	2009	2009 IEEE 25th International Conference on Data Engineering	10.1109/ICDE.2009.12	query optimization;computer science;peer-to-peer;data mining;database;histogram;distributed computing;distributed database	DB	-29.51799578057837	1.236849734082616	131224
20acee86576becfc64977f3e2411476d2fbed39e	a distributed fuzzy constraint satisfaction system with context-based reasoning	constraint satisfaction;theorem proving;distributed environment;formal logic;fuzzy constraints;printed wiring board	This paper presents a fuzzy constraint satisfaction system which can be used in a distributive environment and, through an example, identifies contexts which exist within the constraint satisfaction system. The fuzzy constraint satisfaction system utilizes value propagation on constraints through the use of formal logic and theorem proving. The system has been designed to work in a distributive environment such that large problems can be broken down into smaller constraint networks for easier processing. Context-based reasoning is identified both within and among constraint networks. The paper begins with the motivation behind this research, followed by a description of the fuzzy constraint satisfaction system FuzCon. It concludes by identifying three mappings of the context-based reasoningistoperator to fuzzy constraints and by showing an example of designing a printed wiring board.	constraint satisfaction	David A. Ress;Robert E. Young	1998	Int. J. Hum.-Comput. Stud.	10.1006/ijhc.1997.0177	constraint logic programming;concurrent constraint logic programming;constraint programming;constraint satisfaction;constraint learning;computer science;constraint graph;artificial intelligence;fuzzy number;constraint satisfaction dual problem;complexity of constraint satisfaction;automated theorem proving;constraint;printed circuit board;constraint satisfaction problem;logic;algorithm;hybrid algorithm;local consistency;distributed computing environment;backtracking	AI	-24.559618253776616	-3.9026783356699695	131351
95b15cca83fb4dfcade53142050a12f692e5c48f	corealmlib: an alm library translated from the component library		This paper presents COREALMLIB, an ALM library of commonsense knowledge about dynamic domains. The library was obtained by translating part of the COMPONENT LIBRARY (CLIB) into the modular action language ALM. CLIB consists of general reusable and composable commonsense concepts, selected based on a thorough study of ontological and lexical resources. Our translation targets CLIB states (i.e., fluents) and actions. The resulting ALM library contains the descriptions of 123 action classes grouped into 43 reusable modules that are organized into a hierarchy. It is made available online and of interest to researchers in the action language, answer-set programming, and natural language understanding communities. We believe that our translation has two main advantages over its CLIB counterpart: (i) it specifies axioms about actions in a more elaboration tolerant and readable way, and (ii) it can be seamlessly integrated with ASP reasoning algorithms (e.g., for planning and postdiction). In contrast, axioms are described in CLIB using STRIPS-like operators, and CLIB’s inference engine cannot handle planning nor postdiction. Under consideration for publication in TPLP.	action language;action potential;algorithm;answer set programming;application lifecycle management;commonsense knowledge (artificial intelligence);fluent (artificial intelligence);human-readable medium;inference engine;internationalized domain name;library (computing);natural language understanding;precondition;strips;upper ontology	Daniela Inclezan	2016	TPLP	10.1017/S1471068416000363	computer science;artificial intelligence;programming language;algorithm	AI	-30.216392605126337	-3.9544437675068176	131568
44ae4e7e3636d9f1494eaa8821c3bcd372e94c01	learning processes based on data sources with certainty levels in linked open data	object recognition;uncertainty;resource description framework;learning systems;urban areas;knowledge based systems	Linked Open Data (LOD) consists of numerous data stores that are highly interconnected. LOD stores use Resource Description Framework (RDF) as a data representation format. A graph-based nature of RDF brings an opportunity to develop new approaches for accumulating data from multiple sources characterized by different levels of confidence in them. Recently, a participatory learning mechanism has been extended to cope with RDF. It is an attractive way of integrating new pieces of information with already known ones. Further, it has been recognized that pieces of information describing entities can have a disjunctive or conjunctive form. This paper uses an RDF-based participatory learning process to aggregate information obtained from multiple data stores. This process provides mechanisms that determine overall certainty in combined data based on levels of confidence in already known pieces of information and new ones. The behavior of such a process used for integrating information equipped with different levels of uncertainty is presented, and a simple case study is included.	aggregate data;data (computing);data assimilation;data store;disjunctive normal form;entity;information;linked data;resource description framework	Jesse Xi Chen;Marek Reformat;Ronald R. Yager	2016	2016 IEEE/WIC/ACM International Conference on Web Intelligence (WI)	10.1109/WI.2016.0068	uncertainty;computer science;knowledge management;artificial intelligence;cognitive neuroscience of visual object recognition;knowledge-based systems;machine learning;rdf;linked data;data mining;world wide web	AI	-31.47676792715317	-4.547444427306463	131660
083e258d9773bad5ee4f5c6de5b28064d8ba3644	use of model-based qualitative icons and adaptive windows in workstations for supervisory control systems	modelizacion;error detection codes;systeme commande;sistema control;human performance;affichage graphique;high resolution;control systems design;supervisory control;superviseur;detection panne;display devices;failure detection;real time;adaptive control;programmable control;poste travail;graphic display;windows;modelisation;deteccion pana;computerized monitoring;control system;haute resolution;visualizacion alfanumerica;supervisor;fenetre;displays;temps reel;workstations;fault detection;alta resolucion;interactive control;tiempo real;ventana;visualizacion grafica;affichage alphanumerique;puesto trabajo;workplace layout;modeling;alphanumeric display;programmable control adaptive control workstations supervisory control displays real time systems hardware computerized monitoring fault detection fault diagnosis;fault diagnosis;human computer interface;hardware;human factors engineering;real time systems	Model-based qualitative icons and adaptive window display interfaces may be valuable tools to enhance the effectiveness of operators in real-time data-intensive supervisory control systems. Qualitative icons may be used to integrate low-level quantitative data into high-level qualitative error detection mechanisms. Using windowing technology, multiple data sources that reflect different aspects of system state can be displayed simultaneously on a single screen. Both technologies were combined and implemented to design an operator interface to the Georgia Tech-Multisatellite Operation Control Center (GT-MSOCC). An operator function model for GT-MSOCC was used to derive workstation features, including hardware configuration, the function of qualitative icons for monitoring, fault detection and identification, and the contents and placement of computer windows. The model also determined sets of windows needed by the operator to undertake major operator control functions. An experiment was performed to evaluate the effectiveness of a workstation incorporating model-based qualitative icons and dynamic operator function window sets. Subjects controlled GT-MSOCC via either a conventional operator interface or the model-based interface. Eleven measures that reflected operator performance were analyzed. Subjects using the model-based workstation operated the system significantly better on nine of these measures. On all measures, performance with the model-based workstation was uniformly better on average and had less variability than performance with the conventional workstation.	control function (econometrics);control system;data-intensive computing;error detection and correction;fault detection and isolation;function model;high- and low-level;logitech driving force gt;microsoft windows;real-time data;real-time locating system;spatial variability;workstation	Christine M. Mitchell;Donna L. Saisi	1987	IEEE Transactions on Systems, Man, and Cybernetics	10.1109/TSMC.1987.289348	embedded system;real-time computing;simulation;adaptive control;computer science;control system;artificial intelligence;control theory;supervisory control;display device	Visualization	-25.86762096830021	-6.56376752126127	131670
c52d116ea074dc31021cc557976a73ecddbe1ff4	a view on rough set concept approximations	raisonnement base sur cas;razonamiento fundado sobre caso;concept approximations;rough set theory;base connaissance;intelligence artificielle;concept hierarchy;artificial intelligence;base conocimiento;inteligencia artificial;case based reasoning;rough set;ensemble approximatif;knowledge base	The concept of approximation is one of the most fundamental i rough set theory. In this work we examine this basic notion as well as its extensio ns and modifications. The goal is to construct a parameterized approximation mechanism maki ng it possible to develop multi–stage multi–level concept hierarchies that are capable of mainta ining acceptable level of imprecision from input to output.	approximation;rough set;set theory	Jan G. Bazan;Hung Son Nguyen;Andrzej Skowron;Marcin S. Szczuka	2003	Fundam. Inform.	10.1007/3-540-39205-X_23	knowledge base;rough set;computer science;artificial intelligence;machine learning;mathematics;algorithm	Theory	-21.16130954124744	-1.845749929380794	131824
c4a4f5d472709857354b500f5b9debe4365797a0	a general-purpose compression scheme for large collections	search engine;prior knowledge;sampling;data extraction;query evaluation;general xrays;random access;phrase based compression	Compression of large collections can lead to improvements in retrieval times by offsetting the CPU decompression costs with the cost of seeking and retrieving data from disk. We propose a semistatic phrase-based approach called xray that builds a model offline using sample training data extracted from a collection, and then compresses the entire collection online in a single pass. The particular benefits of xray are that it can be used in applications where individual records or documents must be decompressed, and that decompression is fast. The xray scheme also allows new data to be added to a collection without modifying the semistatic model. Moreover, xray can be used to compress general-purpose data such as genomic, scientific, image, and geographic collections without prior knowledge of the structure of the data. We show that xray is effective on both text and general-purpose collections. In general, xray is more effective than the popular gzip and compress schemes, while being marginally less effective than bzip2. We also show that xray is efficient: of the popular schemes we tested, it is typically only slower than gzip in decompression. Moreover, the query evaluation costs of retrieval of documents from a large collection with our search engine is improved by more than 30% when xray is incorporated compared to an uncompressed approach. We use simple techniques for obtaining the training data from the collection to be compressed and show that with just over 4% of data the entire collection can be effectively compressed. We also propose four schemes for phrase-match selection during the single pass compression of the collection. We conclude that with these novel approaches xray is a fast and effective scheme for compression and decompression of large general-purpose collections.	central processing unit;data compression;general-purpose modeling;netbsd gzip / freebsd gzip;online and offline;web search engine;bzip2	Adam Cannane;Hugh E. Williams	2002	ACM Trans. Inf. Syst.	10.1145/568727.568730	sampling;computer science;data mining;database;programming language;world wide web;information retrieval;random access;search engine	Web+IR	-28.527122418166666	1.5954342171724323	131893
dea885826736f17855b0c62086545cb59c5c964f	fmams: fuzzy mapping approach for mediation systems	fuzzy logic;similarity;global as view gav;mapping;data integration	To access a unified way to different information sources while hiding the user autonomy, heterogeneity, distribution and evolution of these sources, the authors thought to integrate its different sources of information, one of the existing integration approaches is mediation or virtual approach. The integration in mediation approach is done with a schema, called schema mediation, and a set of mapping (links) associating schemas of sources to integrate with the global schema. The problem arises in creating the set of correspondences between the elements of the schema global and all the elements of local schemas, in this paper the authors present their approach FMAMS: Fuzzy Mapping Approach for Mediation Systems, a new approach based on fundamental principles of the theory of fuzzy sets. The authors’ approach is to define and associate to each link between two elements a weight that reflects the degree of its existence, it is present in an analysis using two components: syntactic and semantic. FMAMS: Fuzzy Mapping Approach for Mediation Systems		Moulay Hafid El Yazidi;Ahmed Zellou;Ali Idri	2013	IJAEC	10.4018/jaec.2013070104	fuzzy logic;similarity;computer science;artificial intelligence;data integration;machine learning;data mining;mathematics	DB	-29.62012501606211	-3.2103292000414276	131930
15d216994fd272099b3f703d13a4ede9e468724e	formulating diagnostic problem solving using an action language with narratives and sensing	action language	Given a system and unexpected observations about the system, a diagnosis is often viewed as a fault assignment to the various components of the system that is consistent with (or that explains) the observations. If the observations occur over time, and if we allow the occurrence of (deliberate) actions and (exoge-nous) events, then the traditional notion of a candidate diagnosis must be modiied to consider the possible occurrence of actions and events that could account for the unexpected system behavior. In the presence of multiple candidate diagnoses , we may need to perform actions and observe their impact on the system, to be able to narrow the list of possible diagnoses, and possibly even initiate some repair. A plan that guarantees such narrowing will be referred to as a diagnostic plan, and if this plan also guarantees that at the end of the execution of the plan, the system has no faults then we refer to it as a repair plan. Since actions and narrative play a central role in diagnostic problem solving, we characterize diagnosis, diagnostic planning and repair with respect to the existing action language L, extended to include static constraints , sensing actions, and the notion of observable uents. This language is used to provide a uniform account of diagnostic problem solving.	action language;observable;problem solving	Chitta Baral;Sheila A. McIlraith;Tran Cao Son	2000			natural language processing;action language;narrative;computer science;artificial intelligence	AI	-19.4773153271764	-3.7030358436064157	131945
be912c0742b4ccf4f14d169fe2495a27a41ddb9d	the determination of the optimum database maintenance points	fixed time;database search	The initial database search cost deteriorates due to record additions, deletions and updates performed during the system operation. Previous studies proposed strategies for selecting the optimum maintenance points by making various assumptions about the rate of database deterioration, the database planning period, the maintenance interval, etc. However, the studies did not express the optimum maintenance points as a function of the database physical structure. The paper assumes (a) that the rate of additions to the database equals the rate of deletions for each database record type, and (b) that maintenance is performed for the entire database at fixed time intervals and it shows how the maintenance points may be determined for various primitive physical database structures. The results are subsequently ultilized to select the optimum maintenance points for arbitrary database structures.	emoticon;row (database)	Michael Hatzopoulos;John G. Kollias	1982	Comput. J.	10.1093/comjnl/25.1.126	database index;database search engine;computer science;data mining;database	DB	-27.446361313886868	3.8982446136075897	131974
681daa63494ccb702fd525f46b117f31f3c9a0ac	exploring the design space of a gpu-aware database architecture		The vast amount of processing power and memory bandwidth provided by modern graphics cards make them an interesting platform for data-intensive applications. Unsurprisingly, the database research community has identified GPUs as effective co-processors for data processing several years ago. In the past years, there were many approaches to make use of GPUs at different levels of a database system. In this paper, we summarize the major findings of the literature on GPU-accelerated data processing. Based on this survey, we present key properties, important trade-offs and typical challenges of GPU-aware database architectures, and identify major open research questions.	graphics processing unit	Sebastian Breß;Max Heimel;Norbert Siegmund;Ladjel Bellatreche;Gunter Saake	2013		10.1007/978-3-319-01863-8_25	enterprise architecture framework;reference architecture;website architecture;applications architecture;information architecture;database design;data architecture	DB	-33.35028215244853	0.048742837209037125	132080
c47570856799a5dd56aac70aacae6b51d030d442	synthesis of conversion rules by expanding knowledge representation	knowledge representation		knowledge representation and reasoning	Hiroshi Mabuchi;Kiyoshi Akama;Hidekatsu Koike;Yoshinori Shigeta	2000			knowledge representation and reasoning;artificial intelligence;machine learning;natural language processing;computer science;knowledge-based systems	NLP	-28.033170676363017	-7.468869992053959	132179
6e5a55a2cbb5d732490d5dca542ef340619e8d9a	trustsets: using trust to detect deceitful agents in a distributed information collecting system		This paper presents a study on how to improve a distributed information collecting system in which information is collected by a multi-agent system constituted by communica ting agents, assuming the hypothesis that some agents of this sys tem can deliberately (liar agents) or in good faith (defective agents) produce or communicate incorrect information. To ensure the coherence of the information system under these constraint s, we aim to gradually limit the impact of the incoming perturbati ons. To reach this goal, we propose that each agent develops its own communication strategy from a TrustSet it builds using information collected by itself and information received from agents it communicates with.	information system;multi-agent system;software agent	Quang-Anh Nguyen Vu;Richard Canal;Benoit Gaudou;Salima Hassas;Frederic Armetta	2012	J. Ambient Intelligence and Humanized Computing	10.1007/s12652-012-0140-0	information filtering system;distributed computing;computer security	AI	-21.003205568332003	-9.796167136538443	132415
f93562f454ac664526d0ece72a9de9811b8a8ef7	transforming descriptions and diagrams to sketches in information system design	system configuration;sketches;topological diagrams;problem representation;cognitive skills;information systems design;diagrammatic reasoning;descriptions;information system design	Sketching is integral to information systems design. Designers need to become fluent in translating verbal descriptions of systems to a variety of kinds of sketches, notably sequential and logical, and to translate among the kinds. Here, we investigated these cognitive skills in design students, asking them to design a system configuration starting from either a sequential diagram or a sequential description. Although the two source descriptions were logically equivalent, the diagram led to designs that corresponded more closely to the source description – that is, designs with fewer omissions of crucial components and links. Text descriptions led to more variable and less accurate designs, most likely because they require more cognitive steps from problem representation to problem solution.	diagram;hyperlink;information system;system configuration;systems design	Barbara Tversky;James E. Corter;Jeffrey V. Nickerson;Doris Zahner;Yun Jin Rho	2008		10.1007/978-3-540-87730-1_23	computer science;artificial intelligence;theoretical computer science;algorithm	AI	-27.9217152523746	-4.495082607129294	132509
164e26b9df0cfb28cfdd8dc5b2d23bda41910af4	ontology-based empirical knowledge verification for professional virtual community	community;decision support;information science;knowledge management;virtual community;professional virtual community;classification;knowledge worker;empirical knowledge;process development;expertise;ontology;knowledge verification;verification model;audits verification	A professional virtual community provides an interactive platform for enterprise experts to create and share their empirical knowledge cooperatively, and the platform contains a tremendous amount of hidden empirical knowledge that knowledge experts have preserved in the discussion process. Therefore, enterprise knowledge management highly prioritises how to verify the empirical knowledge effectively before archiving it into enterprise knowledge repository for reuse. This work develops a novel scheme of ontology-based empirical knowledge verification for professional virtual community to assist domain experts in a professional virtual community to verify the logics of empirical knowledge, thus ensuring the quality of empirical knowledge and providing accurate knowledge decision support for knowledge workers. In particular, this work has the following objectives: propose an empirical knowledge verification model for a professional virtual community, design an ontology-based empirical knowledge verification process, develop techniques related to the ontology-based empirical knowledge verification and implement an ontology-based empirical knowledge verification mechanism with an illustrative example of securities trading. Results of this study facilitate efforts within the professional virtual community to verify empirical knowledge in order to provide knowledge workers with logic-correct empirical knowledge for decision support.	virtual community	Yuh-Jen Chen	2011	Behaviour & IT	10.1080/0144929X.2010.549512	knowledge base;community;organizational learning;empirical evidence;process development execution system;knowledge integration;biological classification;information science;computer science;systems engineering;knowledge management;body of knowledge;knowledge-based systems;knowledge engineering;ontology;open knowledge base connectivity;data mining;procedural knowledge;knowledge extraction;personal knowledge management;knowledge value chain;domain knowledge	HCI	-32.44357558600996	-6.106053504696235	132856
ca639a2c753a3f05fb48d8db5b6d7289beb11d44	applying incremental graph transformation to existing models in relational databases	larger system model;incremental techniques side;incremental graph transformation;transformation rule;translates graph pattern;graph pattern;incremental pattern matching;relational databases;graph transformation;favorable performance characteristic;large graph	As industrial practice demands larger and larger system models, the efficient execution of graph transformation remains an important challenge. Additionally, for real-world applications, compatibility and integration with already well-established technologies is highly desirable. Therefore, relational databases have been investigated before as off-the-shelf environments for graph transformation, since they are already widely used for storing, processing and querying large graphs.#R##N##R##N#The graph pattern matching phase of graph transformation typically dominates in cost due to its combinatorial complexity. Therefore significant attempts have been made to improve this process; incremental pattern matching is an approach that has been shown to exert favorable performance characteristics in many practical use cases. To this day, however, no solutions are available for applying incremental techniques side by side with already deployed systems built over relational databases.#R##N##R##N#In the current paper, we propose an approach that translates graph patterns and transformation rules into event-driven (trigger-based) SQL programs that seamlessly integrate with existing relational databases to perform incremental pattern matching. Additionally, we provide experimental evaluation of the performance of our approach.		Gábor Bergmann;Dóra Horváth;Ákos Horváth	2012		10.1007/978-3-642-33654-6_25	wait-for graph;theoretical computer science;machine learning;data mining;mathematics;graph database	DB	-32.890598460606846	1.6377236939303632	132967
0601a7560364fb160e33a4d28be6c002adebb73e	sensitivity analysis of rough classification	digestive diseases;vagotomie;pronostic;ulcera;appareil digestif pathologie;duodeno;ulcer;hombre;duodenum;vagotomy;ulcere;classification;vagotomia;elective;aparato digestivo patologia;electivo;pronostico;sensitivity analysis;chirurgie;human;surgery;cirugia;prognosis;clasificacion;electif;homme	"""Rough classification of patients after highly selective vagotomy (HSV) for duodenal ulcer is analysed from the viewpoint of sensitivity of previously obtained results to minor changes in the norms of attributes. The norms translate exact values of pre-operating quantitative attributes into some qualitative terms, e.g. """"low"""", """"medium"""" and """"high"""". An extensive computational experiment leads to the general conclusion that original norms following from medical experience were well defined, and that the results of analysis of the considered information system using rough sets theory are robust in the sense of low sensitivity to minor changes in the norms of attributes."""	computation;information system;rough set	Krzysztof Slowinski;Roman Slowinski	1990	International Journal of Man-Machine Studies	10.1016/S0020-7373(05)80108-7	biological classification;artificial intelligence;sensitivity analysis	Vision	-25.488208983595115	-4.971122595324761	132998
75ae4b22b808ad43e839349586f62a14ecd618cb	knowledge elicitation to prototype the value of information		From Wall Street to the streets of Baghdad, informa tion drives action. Confounding this edict for the mili tary is not only the unprecedented increase in the types and am ount of information available, but the ability to separate the important information from the routine. Termed the value of information (VOI), the modern military commander and his staff require improved methodologies for assess ing the applicability and relevance of information to a par ticular operation. This paper presents the approach used t o licit the knowledge necessary to value information for mi litary analysis and enable the construction of a fuzzy-bas ed prototype system for automating this valuation.	prototype;relevance;value (ethics)	Timothy Hanratty;Eric Heilman;John Dumer;Robert J. Hammell	2012			information retrieval;value of information;computer science	AI	-33.105346261996736	-8.316330367012025	133005
a5f2845f02515c2341ac15d39684b7b512894350	process control by an expert system at the grandpuits refinery	expert system;process control		expert system	F. Hartmann	1993			systems engineering;refinery;process control;machine learning;artificial intelligence;computer science;expert system	Robotics	-29.3095361232578	-7.2789447280761905	133425
0176c184f4dd799022045f3c5bd43f78d64aec52	cognitive modeling and dynamic probabilistic simulation of operating crew response to complex system accidents. part 4: idac causal model of operator problem-solving response	fiabilite humaine;modelo dinamico;modelizacion;analyse risque;operating room;operateur humain;modeling technique;justification;procesamiento informacion;operador humano;bloc operatoire;systeme aide decision;risk analysis;dynamic model;accident;diagnostico;performance influencing factors;physical activity;spectrum;sistema complejo;sistema ayuda decision;prise decision;probabilistic approach;probabilistic risk assessment;modelisation;analisis riesgo;human reliability;decision support system;systeme complexe;complex system;causalite;enfoque probabilista;approche probabiliste;modele dynamique;information processing;personal de navegacion;personnel navigant;human operator;quirofano;risk assessment;crew;nuclear power plant;centrale nucleaire;cognitive and dynamic probabilistic risk assessment;accidente;justificacion;human reliability analysis;traitement information;toma decision;diagnosis;causal models;modeling;cognitive model;central nuclear;performance shaping factors;problem solving;evaluation risque;fiabilidad humana;causality;causalidad;diagnostic	This is the fourth in a series of five papers describing the  I nformation,  D ecision, and  A ction in  C rew context (IDAC) operator response model for human reliability analysis. An example application of this modeling technique is also discussed in this series. The model has been developed to probabilistically predicts the responses of a nuclear power plant control room operating crew in accident conditions. The operator response spectrum includes cognitive, emotional, and physical activities during the course of an accident. This paper assesses the effects of the performance-influencing factors (PIFs) affecting the operators’ problem-solving responses including information pre-processing ( I ), diagnosis and decision making ( D ), and action execution ( A ). Literature support and justifications are provided for the assessment on the influences of PIFs.	causal model;cognitive model;complex system;problem solving;simulation	Y. H. J. Chang;Ali Mosleh	2007	Rel. Eng. & Sys. Safety	10.1016/j.ress.2006.05.011	reliability engineering;risk assessment;cognitive model;spectrum;systems modeling;risk analysis;causality;decision support system;information processing;engineering;artificial intelligence;human reliability;physical fitness;probabilistic risk assessment;operations research;causal model	AI	-24.409817813763503	-6.721606458797758	133660
80ea92166a4a0af2e853f8cdf6f013b5169a43f7	an attempt of the heuristic evaluation of visualization in searching economic information in topic maps		The usefulness of economic indicators in assessing functioning of an enterprise depends on comprehension by decision-makers of semantic connections existing between ratios. Analysis of economic indicators with regard to semantic relations often has essential impact on formulating accurate conclusions. However, the knowledge of semantic connections and resulting from them information concerning functioning of an enterprise is often possessed only by experienced financial analysts. Topic map standard allows creating a model of knowledge representation, searching and acquiring unique information. The creation of topic map for the ontology of economic indicators facilitates inter alia analyzing economic ratios on account of semantic connections between them. In the semantic search in topic map the visualization plays important role. We have audited the heuristic evaluation of the visualization in searching information on economic indicators based application for Return on Investment indicator. The research with participation of users was carried out.	heuristic evaluation;map;topic maps	Helena Dudycz	2012		10.1007/978-3-642-31069-0_12	data mining;heuristic evaluation;knowledge representation and reasoning;topic maps;visualization;semantic search;computer science;economic indicator;comprehension;information visualization	HCI	-33.02090288254733	-8.45216896046195	133663
e2650e424a763ed2df58dd0f11cee23f64b656a2	communication and shared mental models for teams performing interdependent tasks		Research shows that performance of human teams improves when members have a shared understanding of their task; that is, when teams develop and use a shared mental model (SMM). An SMM can contain different types of information or components and this paper investigates the influence on team performance of sharing different components. We consider two components of an SMM: intentions (e.g goals) and world knowledge (e.g beliefs) and investigate which component(s) contribute most to team performance across different forms of interdependent tasks. We performed experiments using a Blocks World for Team (BW4T) testbed for artificial agent teams and our results show that with high levels of interdependence in tasks, communicating intentions contributes most to team performance, while for low levels of interdependence, communicating world knowledge contributes more. Additionally, as is the case with human teams, higher sharedness correlated with improved team performance for the artificial agent teams. These insights can assist in the design of communication protocols that improves team performance when team members are engaged in interdependent tasks and help design artificial agents that can communicate effectively when working with humans as teammates.		Ronal Singh;Liz Sonenberg;Tim Miller	2016		10.1007/978-3-319-46882-2_10	knowledge management;computer science;testbed;interdependence;distributed computing;communications protocol;blocks world	HCI	-23.130070474361602	-8.693266073768712	133704
e69416a7e3a0725a737e884701a07931e91ecc47	towards a multimodeling approach of dynamic systems for diagnosis	dynamic system	This paper presents the basis of a multimodeling methodology that uses a CommonKADS conceptual model to interpret the diagnosis knowledge with the aim of representing the system with three models: a structural model describing the relations between the components of the system, a functional model describing the relations between the values the variables of the system can take (i.e. the functions) and a behavioural model describing the states of the system and the discrete events firing the state transitions. The relation between these models is made with the notion of variable: a variable used in a function of the functional model is associated with an element of the structural model and a discrete event is defined as the affectation of a value to a variable. This methodology is presented in this paper with a toy but pedagogic problem: the technical diagnosis of a car. The motivating idea is that using the same level of abstraction that the expert can facilitate the problem solving reasoning.	abstraction layer;behavioral modeling;dynamical system;function model;knowledge acquisition and documentation structuring;knowledge base;medical algorithm;problem solving;simulation	Marc Le Goc;Emilie Masse	2007			computer science;dynamical system	AI	-20.99787761675556	0.9886195861562165	133711
053a5909a71ce3a7805db40f0fd83390ac17e999	a scalable xslt processing framework based on mapreduce	xslt;mapreduce;xml transformation;cloud computing	The eXtensible Stylesheet Language Transformation (XSLT) is a de-facto standard for XML data transforming and extracting. Efficient processing of large amounts of XML data brings challenges to conventional XSLT processors, which are designed to run in a single machine context. To solve these data-intensive problems, MapReduce paradigm in the cloud computing domain has received a comprehensive attention in both academia and IT industry recently. In this paper, a novel MapReduce-based XSLT distributed processing framework named CloudXSLT is proposed to implement efficient and scalable XML data transforming. First, the architecture of CloudXSLT framework is outlined. Subsequently, several XML data and XSLT rule representation models which are suitable for MapReduce paradigm are defined, and several MapReduce-based XSLT distributed processing algorithms are proposed. Finally, an experiment on a simulation environment with real XML datasets shows our framework is more efficient and scalable than conventional XSLT processors when processing large size of XML data.	algorithm;central processing unit;cloud computing;data-intensive computing;distributed computing;mapreduce;programming paradigm;scalability;simulation;xml;xslt	Ren Li;Dan Yang;Haibo Hu;Ling Chen	2013	JCP	10.4304/jcp.8.9.2175-2181	xml validation;processing instruction;xslt;cloud computing;streaming xml;computer science;xml framework;data mining;xml database;xml schema;database;schematron;world wide web;efficient xml interchange	DB	-33.17497232654578	-1.429979062147251	133898
c0289617b12d5b39086e5fc8bb8d95e0b8bb9aaf	batch-optimistic test-cases generation using genetic algorithms	medical informatics;medline repository;data mining;association rule mining;domain knowledge;large transaction database;medical information systems;knowledge acquisition;data mining association rules transaction databases biomedical informatics statistics aggregates artificial intelligence computer science data engineering knowledge engineering;background knowledge;medical informatic;electronic medical record;very large databases;medline repository association rule mining large transaction database subjective rule evaluation knowledge acquisition medical informatics electronic medical record database;transaction processing;very large databases data mining medical information systems transaction processing;subjective rule evaluation;electronic medical record database	This paper proposes a dynamic software testing framework, which is able to analyse the source code of a program, create the necessary data structures for automatic testing, such as control flow graphs, and generate a near to optimum set of test cases with reference to a test coverage criterion. The framework consists of two sub-systems: the first is a program analysis system that identifies the type of statements and the complexity of conditions, performs analysis of variables, extracts code paths and creates the control flow graph (CFG) of the program under testing. The second is a test system that uses the CFG for automatically generating test data based on evolutionary computing. The latter system utilises a specially designed genetic algorithm to produce the set of test cases satisfying the selected coverage criterion. The efficacy and performance of the proposed testing approach is assessed and validated using a variety of sample programs.	context-free grammar;control flow graph;data structure;evolutionary computation;fault coverage;genetic algorithm;program analysis;software testing;test case;test data	Anastasis A. Sofokleous;Andreas S. Andreou	2007	19th IEEE International Conference on Tools with Artificial Intelligence(ICTAI 2007)	10.1109/ICTAI.2007.113	health informatics;association rule learning;transaction processing;computer science;artificial intelligence;data science;data mining;information retrieval;domain knowledge	SE	-32.29451699589325	-3.6415623185849046	133967
7f8417f63fa612c966f112c61d960dda39bd8145	comparison of different disk searching methods	data organization;disk files;access methods;search method;searching	The efficiency of data base management and file handling systems depends very much on the organization of data and on the access methods used. I n spite of this, most users rely on some ‘rule of thumb’ or on some previous practical experience when selecting a particular organization and access method, or when it is time to reorganize a much modified data set. On the other hand, the theory of data bases deals with very abstract models which are usually far removed from reality and it is difficult for the practitioner to draw useful conclusions from these general theories. I n this paper an attempt is made to obtain some formulae which can be used to improve the efficiency of data retrieval by selecting the most efficient organization and access method. Numerical results are shown for ES 5052, 5061, IBM 2311, 2314 and 3330 disks and for their Siemens and I C L equivalents. If a file consists of N records, then the average access time is given by the formula	access time;data retrieval;database;history of ibm magnetic disk drives;numerical method;theory	Páe Quittner;D. Kotsis	1978	Softw., Pract. Exper.	10.1002/spe.4380080603	computer science;theoretical computer science;data mining;database;programming language;access method	DB	-28.304896822462965	3.9217589969831996	133968
0822d27aeb5c6a872dc8a5d3635ab9e9a5d790ae	solving large configuration problems efficiently by clustering the conbacon model	representacion conocimientos;systeme intelligent;constraint based configuration;sistema inteligente;systeme base connaissances;constraint satisfaction;satisfaction contrainte;industrial production;intelligent system;constraint programming;satisfaccion restriccion;industrial expert systems;knowledge representation;constraint based modeling;representation connaissances;knowledge based systems;expert system	In this paper, we outline our constraint-based model for configuring and reconfiguring industrial products as well as some aspects of its prototypical implementation ConBaCon (Constraint-Based Configuration). ConBaCon overcomes the deficits of existing commercial configuration systems. Problems remain in the case of large products/ technical systems. As one way of tackling this, we present a clustering approach within the ConBaCon model that allows large configuration problems to be efficiently solved by substantially reducing the amount of constraint variables and constraints needed.		Ulrich John	2000		10.1007/3-540-45049-1_48	industrial production;constraint programming;constraint satisfaction;computer science;artificial intelligence;mathematics;expert system;algorithm	Theory	-24.40666295978127	-3.8798103937358093	134011
31f6b1f8eeca1c99eb77aefe2efcfec170e7da8f	multi-level description directed generation	clear semantic basis;message-directed framework;correct syntactic form;multi-level description;message-directed control;direct replacement;message-directed control flow;direct replacement approach;adequate representation;grammar-directed control scheme;computational property;control flow	The principal deficit of the direct replacement approach is its difficulties with 8ramatar, i.e. the awkwardness of maintaining an adequate representation of the grnmm~tical context, or of carrying out grammatically mediated text-level actions such as producing the correct syntactic form for an embedded clause. In other respects, however. the message-directed control flow that drives direct replacement has a great deal to recommend it. Compared with grammar-directed control schemes, message-directed control is more efficient, since every action will contribute to the eventual production of the text. Message-directed control also gives a pt*nner a very clear semantic basis for its communication to the resfization component, since the message can be viewed simply as a set of instructions to accomplish specific goals. The question then becomes: is there a way off elaborating the basic, message-directed framework so as to overcome the deficits that plague direct replacement approaches while still keeping the computational properties that have made it attractive?	capacitor plague;computation;control flow;embedded system;multi-level governance	David D. McDonald	1986			computer science;artificial intelligence;communication;algorithm	PL	-20.857858721045442	3.148459263583048	134568
944d9a3c7963aa4f609146789ab328e504092bda	dimension-based analysis of hypotheticals from supreme court oral argument	legal defense;hypotheses;computer aided diagnosis;case base reasoning;searching;computer programs;federal law	In this paper we examine a sequence of hypotheticals taken from a Supreme Court oral argument. We use the idea of a “dimension,” developed previously in our case-based reasoning system HYPO, to analyze the hypotheticals and to speculate on how the Justices might have arrived at them. The case we consider is taken from the area of Fourth Amendment law concerning warrantless search and seizure.	case-based reasoning;reasoning system;station hypo	Edwina L. Rissland	1989		10.1145/74014.74030	hypothesis;computer science;artificial intelligence;algorithm	AI	-21.256729814838142	-7.478900143034144	134805
9b6baa76ee1648a94116a1fd8f32bd1218f41224	knowledge representation using fuzzy petri nets - revisited	data abstraction knowledge representation antecedent consequence relationship fuzzy proposition fuzzy reasoning algorithm hierarchical fuzzy petri nets;representacion conocimientos;fuzzy reasoning;red petri;logique floue;adjacency;logica difusa;inference mechanisms;intelligence artificielle;uncertainty handling;raisonnement;fuzzy set theory;algorithme;fuzzy logic;algorithm;knowledge representation petri nets fuzzy systems fuzzy reasoning production systems knowledge based systems;data abstraction;compound transition;razonamiento;artificial intelligence;inference mechanisms knowledge representation petri nets fuzzy set theory fuzzy logic uncertainty handling;backward arc;inteligencia artificial;reasoning;petri nets;knowledge representation;petri net;representation connaissances;reseau petri;algoritmo	In the paper by S. Chen et al. (see ibid., vol.2, no.3, p.311-19, 1990), the authors proposed an algorithm which determines whether there exists an antecedent-consequence relationship from a fuzzy proposition d/sub s/ to proposition d/sub j/ and if the degree of truth of proposition d/sub s/ is given, then the degree of truth of proposition d/sub j/ can be evaluated. The fuzzy reasoning algorithm proposed by S. Chen et al. (1990) was found not to be working with all types of data. We propose: (1) a modified form of the algorithm, and (2) a concept of hierarchical fuzzy Petri nets for data abstraction.	knowledge representation and reasoning;petri net	T. V. Manoj;John Leena;Rajan B. Soney	1998	IEEE Trans. Knowl. Data Eng.	10.1109/69.706063	knowledge representation and reasoning;type-2 fuzzy sets and systems;computer science;artificial intelligence;machine learning;mathematics;petri net;algorithm	DB	-20.143248100640967	-1.2132381392654241	134816
3f7e69f986a209fb9ba84420318b806233f8ea0a	indexing for progressive skyline computation	decision support;answer sets;dominate;interest points;multiple criteria;skyline;b tree index;progressive;indexation;bitmap	Many decision support applications are characterized by several features: (1) the query is typically based on multiple criteria; (2) there is no single optimal answer (or answer set); (3) because of (2), users are typically looking for satisficing answers; (4) for the same query, different users, dictated by their personal preferences, may find different answers meeting their needs. As such, it is important for the DBMS to present all interesting answers that may fulfill a user's need. In this paper, we focus on the set of interesting answers called the skyline. Given a set of points, the skyline comprises the points that are not dominated by other points. A point dominates another point if it is as good or better in all dimensions and better in at least one dimension. We present two novel indexing schemes to compute the skyline of a set of points progressively. Unlike most existing algorithms that require at least one pass over the dataset to return the first interesting point, our algorithms return interesting points gradually as they are identified. The first algorithm, Bitmap, is completely non-blocking and exploits a bitmap structure to quickly identify whether a point is an interesting point or not. The second method, Index, exploits a transformation mechanism and a B+-tree index to return skyline points in batches. Our extensive performance study shows that the proposed algorithms provide quick initial response time as compared to existing algorithms. Moreover, both schemes can also outperform existing techniques in terms of total response time. While Index is superior in most cases, Bitmap is effective when the number of distinct values per dimension is small as well as when the number of skyline points is large.	computation	Pin-Kwang Eng;Beng Chin Ooi;Kian-Lee Tan	2003	Data Knowl. Eng.	10.1016/S0169-023X(02)00208-2	b-tree;decision support system;computer science;data mining;database;programming language;bitmap;algorithm	DB	-25.31108088712223	3.1540889302611057	134896
d8674d0767043363b8dc050b443f94540c5b1541	information access and control operations in multi-agent system based process automation	modelizacion;iterative method;task performance;controle acces;multidisciplinaire;adaptability;adaptabilite;multiagent system;supervisory control;multi agent system;information source;surveillance;source information;teoria sistema;securite informatique;level control;automatisation;intelligence artificielle;laboratory tests;information access;ensayo laboratorio;productique;automatizacion;commande niveau;controle information;holon;adaptabilidad;laboratory test;metodo iterativo;computer security;modelisation;control proceso;vigilancia;monitoring;control informacion;systems theory;methode iterative;bdi agents;theorie systeme;seguridad informatica;process control;situation awareness;acces information;robotica;artificial intelligence;multidisciplinary;acceso informacion;access control;multidisciplinar;inteligencia artificial;monitorage;information control;sistema multiagente;essai laboratoire;monitoreo;modeling;computer integrated manufacturing;supervision;commande processus;fuente informacion;systeme multiagent;automation	An approach to combine information access and control operations in a process automation system extended with multi-agent system technology is presented in this paper. According to this approach a multi-agent system supervises an ordinary process automation system by performing higher-level information access and control operations. The information access operations are aimed for actively combining information from different sources depending on the monitoring tasks of the users. The control operations of the multi-agent system are supervisory control tasks performed either in sequential or iterative fashion. The expected benefit of the multi-agent system is enhanced adaptability of the automation system and increased situation awareness of its users. The architecture of the multi-agent system is based on the BDI agent model and utilization of so-called ontologies. An approach for engineering applications for this kind of a multi-agent system is also discussed. The approach is demonstrated with results from experiments performed with industrial test data and a laboratory test process.	automation;information access;multi-agent system	Ilkka Seilonen;Teppo Pirttioja;Antti Pakonen;Pekka Appelqvist;Aarne Halme;Kari Koskinen	2005		10.1007/11537847_13	situation awareness;adaptability;simulation;systems modeling;computer science;artificial intelligence;process automation system;access control;automation;process control;iterative method;computer-integrated manufacturing;supervisory control;multidisciplinary approach;systems theory;totally integrated automation	EDA	-23.85819169572462	-4.680452421150762	135303
127bd2e6a5ffdcfe9a855eb53a887a43c77ad2be	sketching probabilistic data streams	data stream;data streams;probabilistic database;large scale;information integration;real world application;probability distribution;data cleaning;stream processing;uncertain data;possible worlds;environmental monitoring	"""The management of uncertain, probabilistic data has recently emerged as a useful paradigm for dealing with the inherent unreliabilities of several real-world application domains, including data cleaning, information integration, and pervasive, multi-sensor computing. Unlike conventional data sets, a set of probabilistic tuples defines a probability distribution over an exponential number of possible worlds (i.e., """"grounded"""", deterministic databases). This """"possibleworlds"""" interpretation allows for clean query semantics but also raises hard computational problems for probabilistic database query processors. To further complicate matters, in many scenarios (e.g., large-scale process and environmental monitoring using multiple sensor modalities), probabilistic data tuples arrive and need to be processed in a streaming fashion; that is, using limited memory and CPU resources and without the benefit of multiple passes over a static probabilistic database. Such probabilistic data streams raise a host of new research challenges for stream-processing engines that, to date, remain largely unaddressed.  In this paper, we propose the first space- and time-efficient algorithms for approximating complex aggregate queries (including, the number of distinct values and join/self-join sizes) over probabilistic data streams. Following the possible-worlds semantics, such aggregates essentially define probability distributions over the space of possible aggregation results, and our goal is to characterize such distributions through efficient approximations of their key moments (such as expectation and variance). Our algorithms offer strong randomized estimation guarantees while using only sublinear space in the size of the stream(s), and rely on novel, concise streaming sketch synopses that extend conventional sketching ideas to the probabilistic streams setting. Our experimental results verify the effectiveness of our approach."""	aggregate data;approximation algorithm;central processing unit;computational problem;join (sql);pervasive informatics;plasma cleaning;possible world;probabilistic database;programming paradigm;randomized algorithm;stream processing;time complexity	Graham Cormode;Minos N. Garofalakis	2007		10.1145/1247480.1247513	probability distribution;probabilistic analysis of algorithms;stream processing;computer science;probabilistic database;information integration;theoretical computer science;data mining;database;environmental monitoring;possible world;probabilistic logic;divergence-from-randomness model	DB	-23.996819216910446	3.169093621757225	135344
e3566e5e4f7bdd8b92ad1476e253e968ede2facd	a model of the mechanical design process based on empirical data	modelizacion;concepcion asistida;computer aided design;sistema experto;cambridge university press;intelligence artificielle;recherche developpement;construction mecanique;modelisation;mechanical engineering;construccion mecanica;research and development;investigacion desarrollo;conception assistee;artificial intelligence;inteligencia artificial;systeme expert;mechanism design;modeling;expert system			David G. Ullman;Thomas G. Dietterich;Larry A. Stauffer	1988	AI EDAM	10.1017/S0890060400000536	mechanism design;systems modeling;computer science;engineering;artificial intelligence;computer aided design;operations research;expert system	AI	-24.31811700371599	-4.8057706483280205	135537
11ba612149e18575eabd2d01ff39c98c7ea5e4f5	adaptive string dictionary compression in in-memory column-store database systems		Domain encoding is a common technique to compress the columns of a column store and to accelerate many types of queries at the same time. It is based on the assumption that most columns contain a relatively small set of distinct values, in particular string columns. In this paper, we argue that domain encoding is not the end of the story. In real world systems, we observe that a substantial amount of the columns are of string types. Moreover, most of the memory space is consumed by only a small fraction of these columns. To address this issue, we make three main contributions: First we survey several approaches and variants for dictionary compression, i. e., data structures that store the dictionary of domain encoding in a compressed way. As expected, there is a trade-off between size of the data structure and its access performance. This observation can be used to compress rarely accessed data more than frequently accessed data. Furthermore the question which approach has the best compression ratio for a certain column heavily depends on specific characteristics of its content. Consequently, as a second contribution, we present non-trivial sampling schemes for all our dictionary formats, enabling us to estimate their size for a given column. This way it is possible to identify compression schemes specialized for the content of a specific column. Third, we draft how to fully automate the decision of the dictionary format. We sketch a compression manager that selects the most appropriate dictionary format based on column access and update patterns, characteristics of the underlying data, and costs for set-up and access of the different data structures. We evaluate an off-line prototype of a compression manager using a variation of the TPC-H benchmark [15]. The compression manager can configure the database system to be anywhere in a large range of the space / time trade-off with a fine granularity, providing significantly better trade-offs than any fixed dictionary format.	benchmark (computing);column (database);column-oriented dbms;dspace;data dictionary;data structure;database;dictionary coder;ibm tivoli storage productivity center;online and offline;prototype;random-access memory;sampling (signal processing);string (computer science);world-system	Ingo Müller;Cornelius Ratsch;Franz Färber	2014		10.5441/002/edbt.2014.27	dictionary coder;computer science;theoretical computer science;data mining;database;incremental encoding	DB	-28.599687042668503	1.6080803513317499	135663
d6c436547781792f361637497945999a60b7ab3c	planning for manufacturing workpieces by storing, indexing and replaying planning decisions		Planning for manufacturing workpieces is a complex task that requires the interaction of a domain-specific reasoner and a generic planning mechanism. In this paper we present an architecture for organizing the case base that is based on the information provided by a generic problem solver. A retrieval procedure is then presented that uses the information provided by the domain-specific reasoner in order to improve the accuracy of the cases retrieved. However, it is not realistic to suppose that the case retrieved will entirely fit into the new problem. We present a replay procedure to obtain a partial solution that replays not only the valid decisions taken for solving the case, but also justifications of rejected decisions made during the problem solving process. As a result, those completion alternatives of the partial solution are discarded that are already known to be invalid from the case.	backtracking;domain-specific language;naruto shippuden: clash of ninja revolution 3;organizing (structure);problem solving;semantic reasoner;solver	Hector Muñoz-Avila;Frank Weberskirch	1996			computer science;artificial intelligence;operations management;engineering drawing	AI	-20.45411280134008	-5.01128663693083	135665
1e7032bb2461b3182ab1103f6db46dc9e821def8	a heterogeneous multi-agent modelling for distributed simulation of supply chains	teoria cognitiva;multiagent system;representacion sistema;multi agent system;logistique;behavioral analysis;agent based;systeme aide decision;agent modeling;cognitive theory;sistema ayuda decision;prise decision;theorie cognitive;decision support system;logistics;representation systeme;analyse comportementale;system representation;supply chain;analisis conductual;sistema multiagente;distributed simulation;toma decision;systeme multiagent;logistica	This paper presents a heterogeneous (cognitive/reactive) agent based approach to model supply chains. The proposed model based on an actors' representation introduce the behavioural studies of active entities constituting the logistics organization. Supply chains member's behaviours are split up into two categories: deliberative and operational. The design and exploitation of distributed simulation model with multi-agents systems permits to support the representation of entities realizing decision-making and operational activities. To facilitate the design of such models, a dedicated agent model is proposed for each category of behaviour: the Decision Agent and Simulation Agent.	simulation	Olivier Labarthe;Erwan Tranvouez;Alain Ferrarini;Bernard Espinasse;Benoît Montreuil	2003		10.1007/978-3-540-45185-3_13	simulation;engineering;artificial intelligence;operations management	AI	-23.007291752929785	-7.641647667456739	135743
190905af866dd34050c63f9474b6d12b1cc3de4e	experimental evaluation of big data analytical tools		Due to the extensive use of SQL, the number of SQL-on-Hadoop systems has significantly increased, transforming Big Data Analytics in a more accessible practice and allowing users to perform ad-hoc querying and interactive analysis. Therefore, it is of upmost importance to understand these querying tools and the specific contexts in which each one of them can be used to accomplish specific analytical needs. Due to the high number of available tools, this work performs a performance evaluation, using the well-known TPC-DS benchmark, of some of the most popular Big Data Analytical tools, analyzing in more detail the behavior of Drill, Hive, HAWQ, Impala, Presto, and Spark.		Mário Rodrigues;Maribel Yasmina Santos;Jorge Bernardino	2018		10.1007/978-3-030-11395-7_12	drill;data science;spark (mathematics);sql;big data;computer science	DB	-32.99468253720222	0.019781533776016064	135880
26f2d3fafd1d9e332b5e0821ab1a253d9d41ec29	dempster-shafer's theory of evidence applied to structured documents: modelling uncertainty	theory of evidence;dempster shafer theory of evidence;indexing and retrieval;structure determination;model development;formal logic;information need;structured documents	Documents ojlen display a structure determined by the author, e.g., several chapters, each with several sub-chapters and so on. Taking into account the structure of a document allows the retrieval process to focus on those parts of the documents that are most relevant to an information need. Chiaramella et al advanced a model for indexing and retrieving structured documents. Their aim was to express the model within a framework based on formal logics with associated theories. They developed the logical formalism of the model. This paper adds to this model a theory of uncertainty, the Dempster-Shafer theory of evidence. It is shown that the theory provides a rule, the Dempster’s combination rule, that a[lows the expression of the uncertainty with respect to parts of a document, and that is compatible with the iogica{ model developed by Chiaramella et al.	formal system;information needs;theory	Mounia Lalmas	1997		10.1145/258525.258546	natural language processing;information needs;computer science;pattern recognition;data mining;logic	NLP	-22.811348084670083	3.2817479366899853	136268
c0e9ef97b88bb5b6513c6379a77e60690c617c7a	using fuzzy decision tree to handle uncertainty in context deduction	decision tree;fuzzy set theory	In context-aware systems, one of the main challenges is how to tackle context uncertainty well, since perceived context always yields uncertainty and ambiguity with consequential effect on the performance of context-aware systems. We argue that uncertainty is mainly generated by two sources. One is sensor’s inherent inaccuracy and unreliability. The other source is deduction process from low-level context to high-level context. Decision tree is an appropriate candidate for reasoning. Its distinct merit is that once a decision tree has been constructed, it is simple to convert it into a set of humanunderstandable rules. So human can easily improve these rules. However, one inherent disadvantage of decision tree is that the use of crisp points makes the decision trees sensitive to noise. To overcome this problem, we propose an alternative method, fuzzy decision tree, based on fuzzy set theory.	context-aware pervasive systems;decision tree;fuzzy set;high- and low-level;natural deduction;set theory	Donghai Guan;Weiwei Yuan;Andrey Gavrilov;Sungyoung Lee;Young-Koo Lee;Sangman Han	2006		10.1007/11816171_7	fuzzy logic;optimal decision;decision tree learning;computer science;artificial intelligence;machine learning;decision tree;incremental decision tree;data mining;decision rule;mathematics;fuzzy set;mobile computing;id3 algorithm	ML	-21.053062434913958	-0.7071631882087118	136277
e46ce26dbaccc1b64b213a5b5862878d1633f8ca	extending temporal ontology with uncertain historical time	historical periods;ontology;inference;temporal reasoning	Temporal ontology for representing uncertainly specified time periods is presented. Mature approaches like Allen interval relations are combined with introduction of time granularity and time uncertainty concepts. The ontology is applicable both as a static data representation and for logical data inference. Logical conclusions can be derived using an automated inference system. Uncertainty parametrization was developed for handling the domain specific uncertainty characteristics. Temporal statements containing the most frequent expressions in the domain of cultural heritage preservation are identified and categorized with respect to their accuracy. A temporal inference system is implemented using OCML language. Consistency checks can find non-causal data clusters and lead to improving current event data. Finally, resource annotation with Dynamic Narrative Authoring Tool utilizing temporal inference is presented.	algorithm;allen's interval algebra;categorization;causal filter;computer;context-sensitive language;data (computing);entity;inference engine;knowledge management;markov switching multifractal;ocml;organizing (structure);temporal expressions;temporal logic;uncertainty principle;web ontology language	Kamil Matousek;Matrin Falc;Zdenek Kouba	2007	Computing and Informatics		computer science;artificial intelligence;machine learning;ontology;data mining;database;information retrieval;algorithm	NLP	-21.46918068578604	1.7974193348303251	136445
079cf490692972a4b136edbc79871d1873b2ad5e	representational content and the reciprocal interplay of agent and environment	modelizacion;lenguaje programacion;analisis contenido;agent programming languages and environmentsagent architectures;cognitive science;multiagent system;horses artificial intelligence turning;programming language;turning;langage declaratif;philosophy of mind;intelligence artificielle;horses;theorem proving;modelisation;demonstration theoreme;content analysis;ciencias cognitivas;declarative language;langage programmation;artificial intelligence;inteligencia artificial;analyse contenu;demostracion teorema;sistema multiagente;part of book or chapter of book;modeling;simulation model;lenguaje declarativo;systeme multiagent;sciences cognitives	Declarative modelling approaches in principle assume a notion of representation or representational content for the modelling concepts. The notion of representational content as discussed in literature in cognitive science and philosophy of mind shows complications as soon as agent and environment have an intense reciprocal interaction. In such cases an internal agent state is affected by the way in which internal and external aspects are interwoven during (ongoing) interaction. In this paper it is shown that the classical correlational approach to representational content is not applicable, but the temporalinteractivist approach is. As this approach involves more complex temporal relationships, formalisation was used to define specifications of the representational content more precisely. These specifications have been validated by automatically checking them on traces generated by a simulation model. Moreover, by mathematical proof it was shown how these specifications are entailed by the basic local properties.	cognitive science;formal system;philosophy of mind;simulation;tracing (software)	Tibor Bosse;Catholijn M. Jonker;Jan Treur	2004	Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems, 2004. AAMAS 2004.	10.1109/AAMAS.2004.229	declarative programming;systems modeling;philosophy of mind;content analysis;computer science;artificial intelligence;simulation modeling;automated theorem proving;algorithm	AI	-23.432758001055372	-3.150874018953127	136711
78bd130e622f329182268a63b638f9c53203a01d	the multi-entity decision graph decision ontology: a decision ontology for fusion support		Aiding decision-makers is a key function of a fusion system. In designing decision-aiding modules for fusion systems, it is necessary to understand the elements of the decision model and the dependencies that connect them. An ontology is a disciplined means to codify that understanding. Many fusion systems have a Bayesian Network (BN) component to support probabilistic reasoning under uncertainty. Decision graphs (DG) are an extension that adds decision aiding to BNs. Both BNs and DGs have limited logical expressivity, able to model propositions, but cannot directly model variable numbers of entities or variations in their attributes and relationships. This important capability is called first-order expressivity. Multi-Entity Bayesian Network (MEBN) was developed to provide first-order logic expressivity to BNs. We are developing Multi-Entity Decision Graph (MEDG) to do the same for decision graphs. We found that a decision ontology is useful to our efforts. The literature has a limited discussion of decision ontologies. Almost all focus on the entities and the entity hierarchy. But BNs and DGs emphasize relationships and the dependencies between relationships. The key for probabilistic first-order expressivity is to identify the relationships that enable dependencies between entity instances. We developed a MEDG Decision Ontology that highlights both the entities and key relationships that any decision model needs to address. It is designed to support decision model developers, including fusion model developers, in building comprehensive decision aiding capabilities.	bayesian network;decision tree;discontinuous galerkin method;entity;expressive power (computer science);first-order logic;first-order predicate;ontology (information science)	Mark Locher;Paulo C. G. Costa	2017	2017 20th International Conference on Information Fusion (Fusion)	10.23919/ICIF.2017.8009877	artificial intelligence;computer science;decision analysis;machine learning;evidential reasoning approach;decision tree;ontology (information science);business decision mapping;decision engineering;data mining;decision model;influence diagram	DB	-19.908322731024736	1.3751142322743906	136733
dc5eef831b03a2ef073924bb3d9ff4f792d1f4c4	spatial databases	spatial databases	Spatial database research has continued to advance greatly since three decades ago, addressing the growing data management and analysis needs of spatial applications. This research has produced a taxonomy of models for space, conceptual models, spatial query languages and query processing, spatial file organization and indexes, and spatial data mining. However, emerging needs for spatial database systems include the handling of 3D spatial data, temporal dimension with spatial data, and spatial data visualization. In addition, the rise of new systems such as sensor networks and multi-core processors is likely to have an impact in spatial databases. The goal of this paper is to provide a broad overview of the recent advancements in spatial databases and research needs in each area.	central processing unit;data mining;data visualization;multi-core processor;object-based spatial database;query language;spatial query;taxonomy (general)	Vijay Gandhi;James M. Kang;Shashi Shekhar	2008		10.1002/9780470050118.ecse408	information retrieval;computer science	DB	-30.865833214260974	-0.032860791772252596	136765
2f52cbef51a6a8a2a74119ad821526f9e0b57b39	sap hana database: data management for modern business applications	relational data;text processing;data management;data processing;spectrum;distributed query processing;flash memory ssds;sorted index scan;domain specific language;relational database management system;relational database system;partitioned sort	The SAP HANA database is positioned as the core of the SAP HANA Appliance to support complex business analytical processes in combination with transactionally consistent operational workloads. Within this paper, we outline the basic characteristics of the SAP HANA database, emphasizing the distinctive features that differentiate the SAP HANA database from other classical relational database management systems. On the technical side, the SAP HANA database consists of multiple data processing engines with a distributed query processing environment to provide the full spectrum of data processing -- from classical relational data supporting both row- and column-oriented physical representations in a hybrid engine, to graph and text processing for semi- and unstructured data management within the same system.  From a more application-oriented perspective, we outline the specific support provided by the SAP HANA database of multiple domain-specific languages with a built-in set of natively implemented business functions. SQL -- as the lingua franca for relational database systems -- can no longer be considered to meet all requirements of modern applications, which demand the tight interaction with the data management layer. Therefore, the SAP HANA database permits the exchange of application semantics with the underlying data management platform that can be exploited to increase query expressiveness and to reduce the number of individual application-to-database round trips.	attribute–value pair;canonical account;database engine;distributed computing;document retrieval;domain-specific language;floor and ceiling functions;graph traversal;nosql;relational database management system;requirement;sap hana;sql;throughput;tree traversal	Franz Färber;Sang Kyun Cha;Jürgen Primsch;Christof Bornhövd;Stefan Sigg;Wolfgang Lehner	2011	SIGMOD Record	10.1145/2094114.2094126	relational database management system;data processing;data management;relational database;computer science;database model;data mining;database;programming language;view;world wide web;database design	DB	-32.10168719344366	1.6749888570434983	137196
7b02efb38cbc3aa3ad30b75d323420ec7e0baa4a	on possible rules and apriori algorithm in non-deterministic information systems	no determinismo;possible rules;sistema experto;max max strategy;non deterministic information;metodo minimax;rough set theory;rule based;minimax method;base connaissance;probabilistic approach;systeme non deterministe;systeme deterministe;non deterministic system;non determinism;sistema determinista;non determinisme;theorie ensemble approximatif;enfoque probabilista;approche probabiliste;methode minimax;rough sets;base conocimiento;apriori algorithm;sistema no determinista;information system;systeme expert;rough set;systeme information;min max strategy;deterministic system;sistema informacion;knowledge base;expert system	A framework of rule generation in Non-deterministicInfor- mationSystems (NISs), which follows rough sets based rule generation in DeterministicInformationSystems (DISs), is presented. We have already coped with certainrules and minimalcertainrules, which are characterized by the concept of consistency, in NISs. We also introduced discernibilityfunctions into NISs. In this paper, possiblerules in NISs are focused on. Because of the information incompleteness, huge number of possiblerules may exist, and we introduce Min-Maxstrategy and Max-Maxstrategy into possible rule generation in NISs. Possible rules based on these strategies are characterized by the criteria minimumsupport, maximumsupport, minimumaccuracy and maximumaccuracy, and Apriori based algorithm is applied.	apriori algorithm;information system	Hiroshi Sakai;Michinori Nakata	2006		10.1007/11908029_29	knowledge base;rough set;computer science;artificial intelligence;machine learning;data mining;mathematics;expert system;algorithm	DB	-20.91000039638982	-2.156519142094893	137486
86d63a29962a1f87e7375e1dfb5c1a632a176a3b	query processing in main memory database management systems	database system;query processing;index structure;relational database;difference set;database management system;processing in memory	Most previous work in the area of main memory database systems has focused on the problem of developing query processing techniques that work well with a very large buffer pool. In this paper, we address query processing issues for memory resident relational databases, an environment with a very different set of costs and priorities. We present an architecture for a main memory DBMS, discussing the ways in which a memory resident database differs from a disk-based database. We then address the problem of processing relational queries in this architecture, considering alternative algorithms for selection, projection, and join operations and studying their performance. We show that a new index structure, the T Tree, works well for selection and join processing in memory resident databases. We also show that hashing methods work well for processing projections and joins, and that an old join method, sort-merge, still has a place in main memory.	algorithm;bendix g-15;computer data storage;in-memory database;oracle database;relational database;t-tree	Tobin J. Lehman;Michael J. Carey	1986		10.1145/16894.16878	online aggregation;sargable;query optimization;database theory;relational model;database tuning;relational database;computer science;in-memory processing;database model;data mining;database;flat memory model;view;database schema;sort-merge join;information retrieval;alias;database design;difference set;memory map;memory management	DB	-28.496164945588156	3.804658575688441	137648
2f57156d5581a7d9293588c5098eabf5f043a2e9	towards visual slam with memory management for large-scale environments		Memory consumption of visual SLAM systems grows rapidly with their operation ranges increase. A well-designed organization scheme of map data is important for the scalability of visual SLAM in large-scale environments. In this paper, we present a novel visual SLAM system with an efficient memory management method to manage the map data using a spatial database. Experimental results on two popular public datasets demonstrate the efficiency and accuracy of our system.	memory management	Fu Li;Shaowu Yang;Xiaodong Yi;Xuejun Yang	2017		10.1007/978-3-319-77383-4_76	artificial intelligence;computer vision;memory management;computer science;scalability;spatial database	Robotics	-30.053212808758122	0.43833274418298995	137690
1efcde96823669cc915cdf0bd0cd5050a5a84b8a	reasoning from imperfect knowledge	obstacles;knowledge;reasoning;ontology	One reason why there is a lack of cross-references between articles on knowledge representation in the Cognitive and the Information Sciences is that cognitive scientists are interested in descriptive models of how people reason whereas information scientists are interested in prescriptive models to help people reason. Formal ontologies such as the Suggested Upper Merged Ontology aid human reasoning by providing (1) an accurate knowledge base, (2) a formalization of the knowledge base as axioms, and (3) a logic to derive new information through deductive reasoning. However, all systems confront obstacles when reasoning from imperfect knowledge consisting of ambiguous, conditional, contradictory, fragmented, inert, misclassified, or uncertain knowledge. We use work from the Cognitive and the Information Sciences to analyze obstacles for both computers and people when confronted with ambiguous, contradictory, misclassified, and uncertain knowledge. A concluding section considers both practical and theoretical applications of ontologies in the Cognitive Sciences.		Stephen K. Reed;Adam Pease	2017	Cognitive Systems Research	10.1016/j.cogsys.2016.09.006	knowledge representation and reasoning;computer science;knowledge management;artificial intelligence;model-based reasoning;knowledge-based systems;machine learning;ontology;descriptive knowledge;data mining;knowledge;psychology of reasoning;deductive reasoning	AI	-28.814203363523337	-8.781555929416404	137735
5734315252dd3a477f08a60e5ef3ffcf701e3058	creative design: reasoning and understanding	raisonnement base sur cas;razonamiento fundado sobre caso;learning algorithm;intelligence artificielle;algorithme apprentissage;artificial intelligence;inteligencia artificial;case based reasoning;algoritmo aprendizaje;problem solving	本稿では,事例に基づく推論 (Case-Based Reasoning: CBR)の立場を取って,長期的な創造的問題 解決とデザイン活動に影響する記憶の問題について調査する.これは,詳細に記録された例・Alexander Graham Bellによる電話の発明に基づき,長期の創造的デザインにおいて繰り返し現れた,Bellのメカニ ズムの推論と理解について要約する.事例に基づくデザインに対し,メカニズムを理解することは,デザ インの制約と類推的評価の類推的予想の元であるとみなされる.しかし,既に理解されたデザインは,背 後ではアクティブであり,ある機会に保留されたデザインの問題を満たす.新しいメカニズムは,計算機 モデル ALEC で統合される.これは,事例に基づくデザインにおいて,ある創造的な振る舞いを説明す る. 1. ALECは Alexander G. Bellのニックネームだったが,偶然にも Analogical Learning by Explaining Casesの頭 文字を取った略語でもある	case-based reasoning;graham scan	Marin Simina;Janet L. Kolodner	1997		10.1007/3-540-63233-6_527	case-based reasoning;simulation;computer science;artificial intelligence;machine learning;mathematics;algorithm	AI	-27.49047699724508	-8.159785238480282	137772
f21c028c3c5356f220e48fef34cd4c54f1a7994e	ontology-guided markerless navigation and situational awareness for endoscopic surgery.	surgical navigation;biomedical ontologies;endoscopic surgery;formal ontology;knowledge representation;situational awareness	Optical navigation systems help surgeons find their way through the complex anatomy of a patient. However, such systems are accident-sensitive, time-consuming and difficult to use because of their complicated technical requirements such as the setting of optical markers and their intraoperative registration. The BIOPASS project, therefore, provides an innovative localisation system for markerless navigation in endoscopic surgery to support medical decision making. This system comprises several machine learning classifiers to recognise anatomical structures visible in the endoscopic images. To verify the data provided by these classifiers and to alert medical staff about surgical risk situations, we developed a new ontology-based software called OntoSun. Our software improves the precision and the sustainable traceability of the classifiers' results and also provides warning messages that increase situational awareness during surgical interventions.		Kais Tahar;Alexandr Uciteli;Philip Röppischer;Heinrich Herre;Sebastian Siemoleit	2018	Studies in health technology and informatics	10.3233/978-1-61499-896-9-83	knowledge management;ontology;situation awareness;computer science	HCI	-33.40613232004542	-9.207375161926159	137786
fa23234d7d25934a884428c32970b8bbf51151ca	effectiveness of using expert support technology for individual and small group decision making.				Fiona Fui-Hoon Nah;Ji-Ye Mao;Izak Benbasat	1998			r-cast;decision support system;knowledge management;management science;business decision mapping	HCI	-30.994981206939862	-9.22760915086604	138217
9dca9d89a1fbd4a194d64cb1f875f7598fba7c70	simple-clips ongoing research: more information with less data by implementing inheritance	generative lexicon	This paper presents the application of inheritance to the fo rmal taxonomy ( is-a) of a semantically rich Lexical Resource (LR) based on the Generative Lexicon theory, S IMPLE-CLIPS. The aim is to lighten the representation of its semantic lay er b reducing the number of encoded relations. A prediction calculation on the impact o f introducing inheritance as regards space occupancy is car ried out, which yields a significant space reduction of 22%. This is corroborated by its actual application that reduce s th number of explicitly encoded relations in this lexicon by18.4%. Later on, we study the issues that inheritance poses to the Le xical Resources and discuss sensitive solutions, illustrated by examples, to tackle each of them. Finally, we present a discussion on the application of inher itance, from which two advantages arise: consistency enhancement and inferen ce capabilities.	clips;generative lexicon;is-a;lr parser	Riccardo Del Gratta;Nilda Ruimy;Antonio Toral	2008			artificial intelligence;natural language processing;clips;generative lexicon;lexicon;computer science;inference	AI	-20.165731933567546	4.002644744583422	138413
c587221fe3a2b39c329d99bdfe1257d0f65035ab	evaluating graph traversal algorithms for distributed sparql query optimization	graph theory;linked date;distributed query processing;sparql	Distributed SPARQL queries enable users to retrieve information by exploiting the increasing amount of linked data being published. However, industrial-strength distributed SPARQL query processing is still at its early stage for efficiently answering queries. Previous research shows that it is possible to apply methods from graph theory to optimize the performance of distributed SPARQL. In this paper we describe a framework that can simulate arbitrary RDF data networks to evaluate different approaches of distributed SPARQL query processing. Using this framework we further explore the graph traversal algorithms for distributed SPARQL optimization. We present an implementation of a Minimum-Spanning-Tree-based (MST-based) algorithm for distributed SPARQL processing, the performance of which is compared to other approaches using this evaluation framework. The contribution of this paper is to show that a MST-based approach seems to perform much better than other non graph-traversal-based approaches, and to provide an evaluation framework for evaluating distributed SPARQL processing.	graph traversal;query optimization;sparql;tree traversal	Xin Wang;Thanassis Tiropanis;Hugh C. Davis	2011		10.1007/978-3-642-29923-0_14	named graph;computer science;sparql;data mining;database;rdf query language;information retrieval	DB	-31.789706999810974	3.34362265376947	138449
bb98cfb54237128ff59eb1c17b52dee832ce6846	a hybrid approach to learn, retrieve and reuse qualitative cases		The application of Artificial Intelligence methods is becoming indispensable in several domains, for instance in credit card fraud detection, voice recognition, autonomous cars and robotics. However, some methods fail in performances or solving some problems, and hybrid approaches can outperform the results when compared to traditional ones. In this paper we present a hybrid approach, named qualitative case-based reasoning and learning (QCBRL), that integrates three well-known AI methods: Qualitative Spatial Reasoning, Case-Based Reasoning and Reinforcement Learning. QCBRL system was designed to allow an agent to learn, retrieve and reuse qualitative cases in the robot soccer domain. We applied our method in the Half-Field Offense and we have obtained promising results.	algorithm;artificial intelligence;automated planning and scheduling;autonomous car;autonomous robot;case-based reasoning;credit card fraud;humanoid robot;performance;reasoning system;reinforcement learning;robotics;spatial–temporal reasoning;universal quantification	Thiago Pedro Donadon Homem;Danilo H. Perico;Paulo E. Santos;Anna Helena Reali Costa;Reinaldo A. C. Bianchi;Ramón López de Mántaras	2017	2017 Latin American Robotics Symposium (LARS) and 2017 Brazilian Symposium on Robotics (SBR)	10.1109/SBR-LARS-R.2017.8215309	machine learning;credit card fraud;spatial intelligence;reuse;robot;cognition;case-based reasoning;reinforcement learning;artificial intelligence;robotics;computer science	AI	-26.86884283124069	-8.132649277771478	138748
768a917325abf193384ebbf59dc55166fee09a56	the end of an architectural era for analytical databases		Traditional enterprise warehouse solutions center around an analytical database system that is monolithic and inflexible: data needs to be extracted, transformed, and loaded into the rigid relational form before analysis. It takes years of sophisticated planning to provision and deploy a warehouse; adding new hardware resources to an existing warehouse is an equally lengthy and daunting task. Additionally, modern data analysis employs statistical methods that go well beyond the typical roll-up and drill-down capabilities provided by warehouse systems. Although it is possible to implement such methods using a combination of SQL and UDFs [1], query engines in relational databases are ill-suited for these. The Hadoop ecosystem introduces a suite of tools for data analytics that overcome some of the problems of traditional solutions. These systems, however, forgo years of warehouse research. Memory is significantly underutilized in Hadoop clusters, and execution engine is naive compared with its relational counterparts. It is time to rethink the design of data warehouse systems and take the best from both worlds. The new generation of warehouse systems should be modular, high performance, fault-tolerant, easy to provision, and designed to support both SQL query processing and machine learning applications. This paper references the Shark system developed at Berkeley as an initial attempt [2]. BODY Data warehouse systems should be modular, flexible, easy to provision, and support machine learning. It’s time to rethink the system design.	apache hadoop;data drilling;ecosystem;fault tolerance;keyboard technology;machine learning;relational database;shark;sql;select (sql);systems design	Reynold Xin	2012	TinyToCS		dimensional modeling;computer science;data mining;database;programming language;world wide web	DB	-33.35031262860018	0.7384531206323394	138753
0f3679c08ad65c6370473e34b96987778a0fc51d	dynamically maintaining frequent items over a data stream	data stream;algorithm;dynamic environment;frequent items;range extension;stream	It is challenge to maintain frequent items over a data stream, with a small bounded memory, in a dynamic environment where both insertion/deletion of items are allowed. In this paper, we propose a new novel algorithm, called hCount, which can handle both insertion and deletion of items with a much less memory space than the best reported algorithm. Our algorithm is also superior in terms of precision, recall and processing time. In addition, our approach does not request the preknowledge on the size of range for a data stream, and can handle range extension dynamically. Given a little modification, algorithm hCount can be improved to hCount*, which even owns significantly better performance than before.	algorithm;approximation algorithm;cpu cache;computer programming;dspace;david gries;distributed database;ieee transactions on software engineering;ieee/acm transactions on networking;misra c;michael j. fischer;perf (linux);relational algebra;sampling (signal processing);scalability;sethi–ullman algorithm;vldb;web cache	Cheqing Jin;Weining Qian;Chaofeng Sha;Jeffrey Xu Yu;Aoying Zhou	2003		10.1145/956863.956918	data stream clustering;real-time computing;computer science;data mining;database;stream	DB	-30.01676991341937	3.980135859974097	138754
450760d7b13b5d3fbfd331b425d30c99ac9628b7	multiple reward criterion for cooperative behavior acquisition in a muliagent environment	simulation ordinateur;dynamic change;multicriteria analysis;multiagent system;fonction valeur;recompense;soccer;cooperation;reinforcement learning;weighting;simulacion numerica;estrategia;intelligence artificielle;robotics;ponderacion;cooperacion;deporte equipo;strategy;sport equipe;cooperative behavior;recompensa;reward;apprentissage renforce;weighted sums;football;simulation numerique;acquisition;robotica;artificial intelligence;coordinacion;value function;robotique;analisis multicriterio;simulacion computadora;ponderation;inteligencia artificial;analyse multicritere;sistema multiagente;aprendizaje reforzado;strategie;computer simulation;team sport;adquisicion;systeme multiagent;coordination;numerical simulation;futbol	An extended value function is discussed in the context of multiple behavior coordination, especially in a dynamically changing multiagent environment. Unlike the traditional weighted sum of several reward functions, we define a vectorized value function which evaluates the current action strategy by introducing a discounted matrix to integrate several reward functions. Owing to the extension of the value function, the learning robot can estimate the future multiple rewards from the environment appropriately not suffering from the weighting problem. The proposed method is applied to a simplified soccer game. Computer simulations are shown and a discussion is given.		Eiji Uchibe;Minoru Asada	1999		10.1007/3-540-45327-X_44	computer simulation;simulation;reward-based selection;strategy;computer science;artificial intelligence;weighting;bellman equation;operations research;cooperation	Robotics	-22.44536248993203	-7.210209254607856	138826
9897536e80de9262cc86f3cd23bc17e8b6095d68	integration of fault detection and diagnosis in a probabilistic logic framework	dynamique processus;systeme commande;sistema control;fault detection and isolation;multiagent system;alarm;reasoning models;model based diagnosis;uncertainty management;continuous system;dinamica proceso;discrete observation;systeme continu;artificial intelligent;control system;detecteur phase;intelligent agents;detection defaut;diagnostic panne;sistema continuo;agent intelligent;fault diagnostic;fault detection;alarme;nonlinear dynamics;diagnostico pana;intelligent agent;phase detector;agente inteligente;logic programs;probabilistic logic;detector fase;alarma;sistema multiagente;process dynamics;deteccion imperfeccion;fault detection and diagnosis;systeme multiagent;defect detection	In this paper we formalize an approach to detect and diagnose faults in dynamic industrial processes using a probabilistic and logic multiagent framework. We use and adapt the Dynamic Independent Choice Logic (DICL) for detection and diagnosis tasks. We specialize DICL by introducing two types of agents: the alarm processor agent, that is a logic program that provides reasoning about discrete observations, and the fault detection agent that allows the diagnostic system to reason about continuous data. In our framework we integrate artificial intelligence model-based diagnosis with fault detection and isolation, a technique used by the control systems community. The whole diagnosis task is performed in two phases: in first phase, the alarm processor agent reasons with definite symptoms and produces a subset of suspicious components. In second phase, fault detection agents analyze continuous data of suspicious components, in order to discriminate between faulty and non-faulty components. Our approach is suitable to diagnose large processes with discrete and continuous observations, nonlinear dynamics, noise and missing information.		Luis E. Garza-Castañón;Francisco J. Cantú Ortiz;Salvador Acevedo	2002		10.1007/3-540-36131-6_27	real-time computing;computer science;artificial intelligence;intelligent agent;fault detection and isolation	AI	-21.04810332103734	-3.781549019703037	138879
8d18cd1fd7bcbfcc4d30b96575b7436a79dac1ae	stepq: spatio-temporal engine for complex pattern queries	spatio-temporal data;spatio-temporal databases;scalable spatio-temporal engine;spatio-temporal application;special-purpose processing technology;complex event processing;traditional spatio-temporal databases;powerful spatio-temporal pattern query;complex pattern query;processing capability;query processing requirement	With the increasing complexity and wide diversity of spatiotemporal applications, the query processing requirements over spatiotemporal data go beyond the traditional query types, e.g., range, kNN, and aggregation queries along with their variants. Most applications require support for evaluating powerful spatio-temporal pattern queries (STPQs) that form higher-order correlations and compositions of sequences of events to infer real-world semantics of importance to the targeted application. STPQs can be supported by neither traditional spatio-temporal databases (STDBs) nor by modern complex-event-processing systems (CEP). While the former lack the expressiveness and processing capabilities for handling such complex sequence pattern queries, the later mostly focus on the Time dimension as the driving dimension, and hence lack the power of the special-purpose processing technologies established in STDBs over the past decades. In this paper, we propose an efficient and scalable spatio-temporal engine for complex pattern queries (STEPQ). STEPQ has several innovative features and ideas that will open the research in the area of integration between spatiotemporal databases and complex event processing.	complex event processing;requirement;scalability;spatiotemporal pattern;temporal database	Dongqing Xiao;Mohamed Y. Eltabakh	2013		10.1007/978-3-642-40235-7_22	computer science;theoretical computer science;data mining;database;spatial query	DB	-30.940658388577212	0.3791771801989048	138891
7da929a086914f89a07f9f5fc4a63f9f1d6148f8	rcube: parallel multi-dimensional rolap indexing	high performance computing;efficiency;multi dimensional;indexing;indexation;data warehousing;algorithmic complexity;scalability issues	This paper addresses the query performance issue for Relational OLAP (ROLAP) datacubes. We present RCUBE, a distributed multi-dimensional ROLAP indexing scheme which is practical to implement, requires only a small communication volume, and is fully adapted to distributed disks. Our solution is efficient for spatial searches in high dimensions and scalable in terms of data sizes, dimensions, and number of processors. Our method is also incrementally maintainable. Using “surrogate” group-bys, it allows for the efficient processing of arbitrary OLAP queries on partial cubes, where not all of the group-bys have been materialized. Our experiments with RCUBE show that the ROLAP advantage of better scalability, in comparison to MOLAP, can be maintained while providing, at the same time, a fast and flexible index for OLAP queries.	central processing unit;experiment;olap cube;online analytical processing;relational database;scalability	Frank Dehne;Todd Eavis;Andrew Rau-Chaplin	2008	IJDWM	10.4018/jdwm.2008070101	search engine indexing;computer science;theoretical computer science;data warehouse;data mining;database;efficiency	DB	-29.548471289790683	1.6774061295233804	138910
16b5aaa76cc98457fcbcc936d40033f156135ace	cognitive agents for sense and respond logistics	modelizacion;distributed system;teoria cognitiva;decision support;aplicacion militar;network design;multiagent system;architecture systeme;systeme reparti;application militaire;multi agent system;logistique;systeme aide decision;cognitive agents;relacion orden;automatisation;ordering;cognitive theory;intelligence artificielle;sistema ayuda decision;prise decision;automatizacion;controle information;marqueur;theorie cognitive;synchronisation;modelisation;relation ordre;decision support system;sistema repartido;marcador;logistics;machine learning;control informacion;synchronization;military application;artificial intelligence;coordinacion;arquitectura sistema;accommodation;sincronizacion;inteligencia artificial;marker;system architecture;information control;sistema multiagente;toma decision;modeling;simulation environment;acomodo;systeme multiagent;coordination;logistica;automation	We present a novel cognitive agent architecture and demonstrate its effectiveness in the Sense and Respond Logistics (SRL) domain. Effective applications to support SRL must anticipate and adapt to emerging situations and other dynamic military operations. SRL transforms the static, hierarchical architectures of traditional military models into re-configurable networks designed to encourage coordination among small peer units. Multi-agent systems are ideal for SRL because they can provide valuable automation and decision support from low-level control to high-level information synchronization. In particular, agents can be aware of and adapt to changes in the environment that may affect control and decision making. Our architecture, the Engine for Composable Logical Agents with Intuitive Reorganization (ECLAIR) is based on cognitive theories for motivation and adaptation [6, 13, 21]. Agents respond to external stimuli and internal perception of wellbeing. In normal situations they act logically, using plans, or workflows, when there is a known strategy to accomplish a task. However, when quick reaction is needed, motivation for action is intuitive or reflexive. Adaptation using machine learning techniques improves both logical and reflexive behaviors in ECLAIR. To demonstrate and evaluate our approach, we implemented a small simulation environment where our agents handle the ordering and delivery of supplies among operational and supply units in several scenarios requiring adaptation of default behavior. Work done while at ATL	adaptive system;agent architecture;agile software development;cognition;cognitive architecture;cognitive model;data synchronization;decision support system;distress (novel);eclair;high- and low-level;logistics;machine learning;mathematical optimization;multi-agent system;net-centric;optimization problem;piaget's theory of cognitive development;seven bridges of königsberg;simulation	Kshanti A. Greene;David G. Cooper;Anna L. Buczak;Michael Czajkowski;Jeffrey L. Vagle;Martin O. Hofmann	2005		10.1007/11683704_9	synchronization;simulation;decision support system;computer science;artificial intelligence	AI	-22.674391164107355	-7.872292024937821	138971
613b390164755af1508e6178c7e554f15351d483	a knowledge-based and decision support system for the planning of material handling systems	support system;knowledge base	Material Handling Systems (MHS) require considerable capital investment. Their efficiency is therefore of vital importance. This implies that their design and operation has to be planned thoroughly. Even though there is a number of planning tools available which provide support for the different planning phases, these tools rarely operate on common data structures and usually require considerable skill for their handling. Thus, as the complexity of MHSs increase, there is a growing demand for an integrated tool kit to support the planning engineer in selecting and evaluating different MHS design alternatives [Grotrian87]. The overall objective of our project was therefore to develop a decision support system that would provide guidance for the engineer throughout the entire MHS planning cycle. In designing this system, special emphasis was put on achieving the following subtasks:	data structure;decision support system;global variable;knowledge-based systems;list of toolkits;material handling	Ralf Becker;H. J. Steffens	1990		10.1145/98784.98858	knowledge base;decision support system;intelligent decision support system;computer science;knowledge management;artificial intelligence;data mining	Robotics	-31.769868618999165	-7.916916378156387	139060
455d8208dabc1ef76bdd3b57f6b9cd161582c465	hybrid storage architecture and efficient mapreduce processing for unstructured data		Abstract As we are now entering the era of data deluge, how to efficiently manage these massive data is becoming a great challenge, especially for the exponentially growing unstructured data, which is far more than structured and semi-structured data. However, unstructured data is more complex for its variety. That is to say, different types of unstructured data have different file size, type and usage, which need different storage and processing for high efficiency. In this paper, we propose a hybrid storage architecture to store the pervasive unstructured data. This hybrid architecture integrates various kinds of data stores within a unified framework, where each type of unstructured data can find its suitable placement policy and it is transparent to users. In addition, we present several partitioning strategies based on the unified framework, which are beneficial to the MapReduce-based batch processing for these unstructured data. The experiments demonstrate that it is possible to build an efficient and smart system through the hybrid architecture and the partitioning strategies.	mapreduce	Weiming Lu;Yaoguang Wang;Jingyuan Jiang;Jian Liu;Yapeng Shen;Baogang Wei	2017	Parallel Computing	10.1016/j.parco.2017.08.008	batch processing;computer science;theoretical computer science;architecture;data architecture;file size;smart system;unstructured data;distributed computing	HPC	-32.8960341487454	-1.0411086111460037	139130
bf87f24eb62ff4200a63229fc63a7a05d45ce134	the priority r-tree: a practically efficient and worst-case optimal r-tree	distributed data;asymptotic optimality;synthetic data;r trees	We present the priority R-tree, or PR-tree, which is the first R-tree variant that always answers a window query using  O (( N / B ) 1−1/ d  p T / B ) I/Os, where  N  is the number of  d -dimensional (hyper-) rectangles stored in the R-tree,  B  is the disk block size, and  T  is the output size. This is provably asymptotically optimal and significantly better than other R-tree variants, where a query may visit all  N / B  leaves in the tree even when  T  e 0. We also present an extensive experimental study of the practical performance of the PR-tree using both real-life and synthetic data. This study shows that the PR-tree performs similarly to the best-known R-tree variants on real-life and relatively nicely distributed data, but outperforms them significantly on more extreme data.	best, worst and average case;priority r-tree	Lars Arge;Mark de Berg;Herman J. Haverkort;Ke Yi	2008	ACM Trans. Algorithms	10.1145/1328911.1328920	r-tree;mathematical optimization;combinatorics;computer science;theoretical computer science;data mining;mathematics;algorithm;synthetic data	DB	-24.79573030813224	1.2157035560847342	139254
2220407ce6048a793e1ae6300fee6015e42f5a44	engineering experts critics for cooperative systems	cooperative systems	Knowledge collection systems often assume they are cooperating with an unbiased expert. They have few functions for checking and fixing the realism of the expertise transferred to the knowledge base, plan, document or other product of the interaction. The same problem arises when human knowledge engineers interview experts. The knowledge engineer may suffer from the same biases as the domain expert. Such biases remain in the knowledge base and cause difficulties for years to come. To prevent such difficulties, this paper introduces the reader to “critic engineering”, a methodology that is useful when it is necessary to doubt, trap and repair expert judgment during a knowledge collection process. With the use of this method, the human expert and knowledge-based critic form a cooperative system. Neither agent alone can complete the task as well as the two together. The methodology suggested here offers a number of extensions to traditional knowledge engineering techniques. Traditional knowledge engineering often answers the questions delineated in generic task (GT) theory, yet GT theory fails to provide four additional sets of questions that one must answer to engineer a knowledge base, plan, design or diagnosis when the expert is prone to error. This extended methodology is called “critic engineering”.	consensus dynamics	Barry G. Silverman;R. Gregory Wenig	1993	Knowledge Eng. Review	10.1017/S0269888900000321	computer science;knowledge management;management science	DB	-21.1410137636616	-4.945276478289309	139387
2b32cc4e3fd74c12e0414b95983e4568a5431b27	efficient keyword search over data-centric xml documents	bloom filter;keyword search;indexation;xml document;high efficiency	We in this paper investigate keyword search over data-centric XML documents. We first present a novel method to divide an XML document into self-integrated subtrees, which are connected subtrees and can capture different structural information of the XML document. We then propose the meaningful self-integrated trees, which contain all the keywords and describe how the keywords are interrelated, to answer keyword search over XML documents. In addition, we introduce the B+-tree index to accelerate the retrieval of those meaningful self-integrated trees. Moreover, to further enhance the performance of keyword search, we present Bloom Filter to improve the efficiency of generating those meaningful self-integrated trees. Finally, we conducted extensive experiments to evaluate the performance of our method, and the experimental results demonstrate that our method achieves high efficiency and outperforms the existing approaches significantly.	xml	Guoliang Li;Jianhua Feng;Na Ta;Lizhu Zhou	2007		10.1007/978-3-540-72524-4_51	xml validation;simple api for xml;xml;computer science;bloom filter;data mining;xml database;database;keyword density;programming language;world wide web;information retrieval;efficient xml interchange	DB	-31.083485014486985	4.155310466731085	139602
c94d016f2523be2769b21c8a7296c21b5057ac68	fast nearest neighbor search on road networks	busqueda informacion;base donnee;distribution donnee;algoritmo busqueda;diagramme voronoi;distribution network;reseau distribution;road network;espace euclidien;information retrieval;algorithme recherche;structure arborescente;metodo arborescente;search algorithm;interrogation base donnee;database;interrogacion base datos;base dato;espacio euclidiano;mathematical analysis;data distribution;red distribucion;red carretera;network topology;vecino mas cercano;estructura arborescente;recherche information;indexation;tree structure;nearest neighbor;explosives;reseau routier;euclidean space;plus proche voisin;tree structured method;nearest neighbour;methode arborescente;explosivo;nearest neighbor search;information system;diagrama voronoi;topologie circuit;distribucion dato;database query;systeme information;voronoi diagram;explosif;sistema informacion	Nearest neighbor (NN) queries have been extended from Euclidean spaces to road networks. Existing approaches are either based on Dijkstra-like network expansion or NN/distance precomputation. The former may cause an explosive number of node accesses for sparse datasets because all nodes closer than the NN to the query must be visited. The latter, e.g., the Voronoi Network Nearest Neighbor (V N) approach, can handle sparse datasets but is inappropriate for medium and dense datasets due to its high precomputation and storage overhead. In this paper, we propose a new approach that indexes the network topology based on a novel network reduction technique. It simplifies the network by replacing the graph topology with a set of interconnected tree-based structures called SPIE’s. An nd index is developed for each SPIE and our new (k)NN search algorithms on an SPIE follow a predetermined tree path to avoid costly network expansion. By mathematical analysis and experimental results, our new approach is shown to be efficient and robust for various network topologies and data distributions.	k-nearest neighbors algorithm;nearest neighbor search;network topology;overhead (computing);precomputation;search algorithm;sparse matrix;topological graph theory;tree (data structure)	Haibo Hu;Dik Lun Lee;Jianliang Xu	2006		10.1007/11687238_14	explosive material;voronoi diagram;computer science;artificial intelligence;euclidean space;machine learning;data mining;mathematics;tree structure;nearest neighbor search;k-nearest neighbors algorithm;information system;network topology;search algorithm	DB	-25.499960474890333	1.6114139393642208	139847
47b177b31bff03c72c7eac6ad292ad68f5c03467	top-k projection queries for probabilistic business processes	language inclusion;satisfiability;web site design;regular expressions;xml;business process	A Business Process (BP) consists of some business activities undertaken by one or more organizations in pursuit of some business goal. Tools for querying and analyzing BP specifications are extremely valuable for companies. In particular, given a BP specification, identifying the top-k flows that are most likely to occur in practice, out of those satisfying a given query criteria, is crucial for various applications such as personalized advertizement and BP web-site design.  This paper studies, for the first time, top-k query evaluation for queries with projection in this context. We analyze the complexity of the problem for different classes of distribution functions for the flows likelihood, and provide efficient (PTIME) algorithms whenever possible. Furthermore, we show an interesting application of our algorithms to the analysis of BP execution traces (logs), for recovering missing information about the run-time process behavior, that has not been recorded in the logs.	aggregate function;algorithm;business process;maximal set;p (complexity);personalization;tracing (software)	Daniel Deutch;Tova Milo	2009		10.1145/1514894.1514923	xml;computer science;theoretical computer science;data mining;database;business process;programming language;regular expression;algorithm;satisfiability	DB	-23.418579544827526	2.1540181803338925	140000
4e5b3397fa56f6a37992ea169210c2fe419dfa92	a sampling approach for xml query selectivity estimation	anonymization bias;extensible markup language;performance evaluation;complex structure;query optimization;data privacy;tree structure;sampling methods	As the Extensible Markup Language (XML) rapidly establishes itself as the de facto standard for presenting, storing, and exchanging data on the Internet, large volume of XML data and their supporting facilities start to surface. A fast and accurate selectivity estimation mechanism is of practical importance because selectivity estimation plays a fundamental role in XML query optimization. Recently proposed techniques are all based on some forms of structure synopses that could be time-consuming to build and not effective for summarizing complex structure relationships. In this research, we propose an innovative sampling method that can capture the tree structures and intricate relationships among nodes in a simple and effective way. The derived sample tree is stored as a synopsis for selectivity estimation. Extensive experimental results show that, in comparison with the state-of-the-art structure synopses, specifically the TreeSketch and Xseed synopses, our sample tree synopsis applies to a broader range of query types, requires several orders of magnitude less construction time, and generates estimates with considerably better precision for complex datasets.	markup language;mathematical optimization;query optimization;sampling (signal processing);selectivity (electronic);video synopsis;xml	Cheng Luo;Zhewei Jiang;Wen-Chi Hou;Feng Yu;Qiang Zhu	2009		10.1145/1516360.1516400	sampling;query optimization;xml;information privacy;computer science;data mining;generalized complex structure;database;tree structure;information retrieval	DB	-31.65887336903989	3.78619666082903	140179
3356b61200c5d186d1e7600c739afc60b3e176ed	hierarchical music representation for composition and analysis	informatica;tratamiento datos;representation;computer program;hierarchical structure;procesamiento informacion;composition;composicion;hierarchized structure;musica;data processing;structure hierarchisee;tratamiento lenguaje;traitement donnee;abstract data type;analisis automatico;functional programming;musique;automatic analysis;language processing;information processing;traitement langage;analyse automatique;informatique;programmation fonctionnelle;computer science;traitement information;programacion funcional;music;lenguaje formal;estructura jerarquizada;formal language;representacion;langage formel	In this paper, we present intermediate results of continuing research into the utility of generalised hierarchical structures for the representation of musical information. We build on an abstract data type presented in Wiggins et al. (1989), using constituents, which are structurally significant groupings of musical events. We suggest that a division into such groupings can be musically meaningful, and that it can be more flexible than similar approaches. We demonstrate our representation system at work in both analysis and composition, with output from computer programs. We conclude that it is possible and useful to represent music in a way independent of the particular style, tonal system, etc., of the music	abstract data type;computer program	Alan Smaill;Geraint A. Wiggins;Mitch Harris	1993	Computers and the Humanities	10.1007/BF01830712	composition;formal language;speech recognition;data processing;information processing;computer science;artificial intelligence;music;programming language;functional programming;abstract data type;representation;algorithm	AI	-22.23785911640984	0.19347857835702822	140208
10d7b368b058dbea10f3d1cb4e1f1e6779134690	enhancing reliability throughout knowledge discovery process	knowledge management data mining;data mining techniques;knowledge management;knowledge discovery systems;data mining;meta knowledge management knowledge discovery systems data mining techniques data driven approaches knowledge driven approach;meta knowledge management;data driven approaches;knowledge driven approach;knowledge management delta modulation data mining databases educational institutions statistics artificial intelligence machine learning pattern recognition decision support systems;knowledge discovery	Reliability is a key issue in knowledge discovery. However, this topic is not fully explored in data mining community. This paper takes a process perspective towards the reliability of knowledge discovery, and the reliability of extracted knowledge is evaluated by the reliability of whole knowledge discovery process. To describe the relationship between the final reliability and the reliability in each stage of process, a reliability model for generic knowledge process is proposed, and is further extended to the context of cross-industry standard process for data mining (CRISP-DM). Moreover, eight factors contributing to knowledge discovery reliability are presented in the order of six phases in CRISP-DM. Based on these factors, ten suggestions on how to enhance reliability throughout knowledge discovery process are provided	academy;cross industry standard process for data mining;dm-crypt;knowledge management;mathematical model;refinement (computing);system identification;technical standard	Mykola Pechenizkiy;Alexey Tsymbal;Seppo Puuronen	2005	Sixth IEEE International Conference on Data Mining - Workshops (ICDMW'06)	10.1109/DEXA.2005.124	knowledge base;software mining;data management;computer science;knowledge management;data science;mathematical knowledge management;knowledge-based systems;knowledge engineering;open knowledge base connectivity;data mining;knowledge extraction;personal knowledge management;commonsense knowledge;domain knowledge	ML	-32.07073940405157	-6.2172270098331905	140354
22ac5d9314ac56b78a7b2bacb6e189bc8e1276b1	a prototype dss for structuring and diagnosing managerial problems	base donnee;causal relationships discrepancy diagnosis problem structuring decision support systems dss managerial problem recognition artificial intelligence database management structural modeling organizational database;database management systems;sistema informatico;database;base dato;computer system;teoria decision;theorie decision;decision theory;decision support systems;systeme informatique;knowledge engineering database management systems decision support systems knowledge based systems;prototypes decision support systems databases mathematical model artificial intelligence system performance petroleum analysis of variance diagnostic expert systems testing;knowledge based systems;knowledge engineering	A methodology for managerial problem recognition, structuring, and diagnosis is presented. The system integrates diagnostic concepts from artificial intelligence with database management and structural modeling techniques. Given an organizational database, a user specifies control bounds for selected data items and causal relationships between variables in the database. Structural modeling techniques are used to represent knowledge of causal relations. If a control variable's value is out of bounds, structural models are used in an attempt to diagnose (explain) the discrepancy. If the discrepancy is explained, a diagnosis is at hand. If not, the user's model is somewhat deficient and what-if analysis can be used to construct an improved model. This methodology has been implemented in a prototype decision support system (DSS) that is described, along with an example of its use. >		Nasser H. A. Mohammed;James F. Courtney;David B. Paradice	1988	IEEE Trans. Systems, Man, and Cybernetics	10.1109/21.23089	decision theory;computer science;knowledge management;artificial intelligence;machine learning;knowledge engineering;data mining	Robotics	-27.590397085900037	-3.415934251498059	140392
c1380fa4e7e3c64cc94a9c3d45ac086cf5c61e43	problem structuring methods in military command and control	structure methods;multiple perspectives;knowledge based system;problem structuring methods;rule based;knowledge management;operations research;soft operational research;military command and control;command and control;knowledge based system architecture;problem solving	In an authorized military hierarchy organization, the procedure of problem solving must be co-ordinated with the tasks of planning, directing, and controlling. In most combat situations, problem solving knowledge is acquired from an expert (commander) or a single group of experts (staff) in a military organization. Therefore, these multiple actors (commander and the staff), multiple perspectives (multi-expertise and knowledge types), incommensurable and/or conflicting interests (resource allocation and distribution among staff planning), important intangibles (ambiguous quantitative or qualitative apparatus), and key uncertainties (unexpected internal and external situations) are part of unstructured problem. In this article, military strategy and tactics are acquired as case knowledge, rule knowledge, and heuristic knowledge content in terms of representing combat formations and planning mechanism to support problem structuring and the solution of military command and control. By doing so, this article presents a knowledge-based system architecture, including case base, rule base, heuristic base, and learning paradigm, for the military command and control procedure with strategic guidance (commander's strategy and tactics) and tactical planning (staff plans generation) system functions in terms of implementing problem structuring methods in military command and control.		Shu-Hsien Liao	2008	Expert Syst. Appl.	10.1016/j.eswa.2007.07.012	rule-based system;command and control;computer science;knowledge management;artificial intelligence;knowledge-based systems;mission command;management science;operations research	DB	-20.462655140364312	-8.263545049330595	140475
32ff0f9f2fe30ad4d235afa4c2c4802ef9a9d144	an expert system for conceptual schema design: a machine learning approach	representacion conocimientos;base donnee;sistema experto;learning;concepcion sistema;relacion hombre maquina;conceptual analysis;database;base dato;man machine relation;intelligence artificielle;analisis conceptual;systeme conversationnel;aprendizaje;apprentissage;conceptual schema;machine learning;interactive system;system design;sistema conversacional;artificial intelligence;relation homme machine;inteligencia artificial;systeme expert;analyse conceptuelle;knowledge representation;representation connaissances;conception systeme;expert system	Abstract In this paper, we report the design specifications and design principles of EXIS, an expert system for conceptual schema design for an inofrmation system currently under development. We focus on machine learning aspects applicable to schema design. The main idea can be highlighted better if integrated with a complete framework of the design environment. Therefore, we first describe a conceptual database model consisting of a semantic model and an event model. Hereafter, we present our approach to design knowledge acquisition and representation which is based on inducing schema design rules from examples. We also present relevant aspects of the theory of Rough Sets and the learning method used in our system. Throughout the paper we discuss several concepts and techniques for expert system design which proved very useful and can be adapted to any other application. Here we tend to avoid being ambiguous by using first order logic to express our ideas.†	conceptual schema;expert system;machine learning	Ramin Yasdi;Wojciech Ziarko	1988	International Journal of Man-Machine Studies	10.1016/S0020-7373(88)80001-4	computer science;three schema approach;conceptual schema;artificial intelligence;machine learning;database schema;expert system;active learning;algorithm;systems design	Arch	-24.140936130395808	-3.1924781251524585	140693
e26f3e2ea5fdf99b4cd240bf1ef4410bfe27ccfa	visual analytics with unparalleled variety scaling for big earth data		We have devised and implemented a key technology, SpatioTemporal Adaptive-Resolution Encoding (STARE), in an array database management system, i.e. SciDB, to achieve unparalleled variety scaling for Big Earth Data, enabling rapid-response visual analytics. STARE not only serves as a unifying data representation homogenizing diverse varieties of Earth Science Datasets, but also supports spatiotemporal data placement alignment of these datasets to optimize a major class of Earth Science data analyses, i.e. those requiring spatiotemporal coincidence. Using STARE, we tailor a data partitioning and distribution strategy for the data access patterns of our scientific analysis, leading to optimal use of distributed resources. With STARE, rapid-response visual analytics are made possible through a high-level query interface, allowing geoscientists to perform data exploration visually, intuitively and interactively. We envision a system based on these innovations to relieve geoscientists of most laborious data management chores so that they may focus better on scientific issues and investigations. A significant boost in scientific productivity may thus be expected. We demonstrate these advantages with a prototypical system including comparisons to alternatives.	central processing unit;clustered file system;computer data storage;data (computing);data access;database;end-to-end encryption;end-to-end principle;experiment;feature extraction;grand challenges;graphics processing unit;high- and low-level;holism;image scaling;interactivity;loose coupling;memory hierarchy;persistence (computer science);quadtree;round-robin scheduling;scalability;scidb;spatiotemporal pattern;tree traversal;visual analytics;volume rendering	Lina Yu;Michael L. Rilee;Yu Pan;Feiyu Zhu;Kwo-Sen Kuo;Hongfeng Yu	2017	2017 IEEE International Conference on Big Data (Big Data)	10.1109/BigData.2017.8257966	artificial intelligence;array dbms;machine learning;data science;computer science;data modeling;visual analytics;search engine indexing;distributed database;external data representation;data management;data access	DB	-32.841632977903906	0.26378292337406545	140974
f6f4cf733b4a9ba7859bbd4d3a58757dd1bd4e6e	unleashing xquery for data-independent programming		The XQuery language was initially developed as an SQL equivalent for XML data, but its roots in functional programming make it also a perfect choice for processing almost any kind of structured and semi-structured data. Apart from standard XML processing, however, advanced language features make it hard to efficiently implement the complete language for large data volumes. This work proposes a novel compilation strategy that provides both flexibility and efficiency to unleash XQuery’s potential as data programming language. It combines the simplicity and versatility of a storage-independent data abstraction with the scalability advantages of set-oriented processing. Expensive iterative sections in a query are unrolled to a pipeline of relational-style operators, which is open for optimized join processing, index use, and parallelization. The remaining aspects of the language are processed in a standard fashion, yet can be compiled anytime to more efficient native operations of the actual runtime environment. This hybrid compilation mechanism yields an efficient and highly flexible query engine that is able to drive any computation from simple XML transformation to complex data analysis, even on non-XML data. Experiments with our prototype and state-of-the-art competitors in classic XML query processing and business analytics over relational data attest the generality and efficiency of the design.	abstraction (software engineering);anytime algorithm;business analytics;compiler;computation;computer data storage;data access;data model;data store;database;experiment;exploit (computer security);flwor;fastest;functional programming;iterative method;json;locality of reference;mathematical optimization;microsoft outlook for mac;parallel computing;pipeline (computing);programming language;prototype;query language;query optimization;rewriting;runtime system;sql;scalability;seamless3d;semi-structured data;semiconductor industry;simple xml;xml database;xquery	Sebastian Bächle;Caetano Sauer	2014	Datenbank-Spektrum	10.1007/s13222-014-0160-3	xml validation;processing instruction;data manipulation language;data control language;streaming xml;computer science;operating system;xml framework;data mining;xml database;database;xml signature;programming language;world wide web;information retrieval;efficient xml interchange	DB	-32.40648655163511	1.7867303826996628	140994
e2dbe650c76dd56d3e933539272be93b43152d6b	inferential knowledge sharing with goal hierarchies in distributed engineering systems.	engineering system;knowledge sharing			Richard Dapoigny;Patrick Barlatier;Nacima Mellal;Eric Benoit;Laurent Foulloy	2005			computer science;knowledge management;knowledge engineering;data mining	DB	-31.59042889409188	-6.607732805869647	141015
4f6012b54736a7eafc4b5a25d0ca2a5c45bb5062	scalability in formal concept analysis	unified medical language system;term extraction;medical taxonomy;formal concept analysis	Formal Concept Analysis is a symbolic learning technique derived from mathematical algebra and order theory. The technique has been applied to a broad range of knowledge representation and exploration tasks in a variety of domains. Most recorded applications of formal concept analysis deal with a small number of objects and attributes, in which case the complexity of the algorithms used for indexing and retrieving data is not a signiicant issue. However, when Formal Concept Analysis is applied to exploration of a large numbers of objects and attributes, the size of the data makes issues of complexity and scalability crucial. This paper presents the results of experiments carried out with a set of 4,000 medical discharge summaries in which were recognised 300 attributes from the Uniied Medical Language System (UMLS). In this domain, the objects are medical documents (4,000) and the attributes are UMLS terms extracted from the documents (300). When Formal Concept Analysis is used to iteratively analyse and visualize this data, complexity and scalability become critically important. Although the amount of data used in this experiment is small when compared with the size of primary memory in modern computers, the results are still important since the probability distributions which determine the eeciencies are likely to remain stable as the size of the data is increased. Our work presents two outcomes, rstly we present a methodology for exploring knowledge in text documents using Formal Concept Analysis by employing conceptual scales created as the result of direct manipulation of a line diagram. The conceptual scales lead to small derived puriied contexts that are represented using nested line diagrams. Secondly, we present an algorithm for the fast determination of these puriied contexts from a compressed representation of the a large formal context. Our work draws on existing encoding and compression techniques to show how rudimentary data analysis can lead to substantial eeciency improvements to knowledge visualisation.	algorithm;computation;computer data storage;data compression;data structure;direct manipulation interface;discharger;experiment;formal concept analysis;hasse diagram;knowledge representation and reasoning;scalability;semiconductor industry;time complexity	Richard Cole;Peter W. Eklund	1999	Computational Intelligence	10.1111/0824-7935.00079	computer science;formal concept analysis;artificial intelligence;theoretical computer science;machine learning;data mining;unified medical language system;algorithm	AI	-28.04902441048971	-4.186870500068609	141197
721de6c4262cfecee682746532b3d8400d936135	parallel data cube construction for high performance on-line analytical processing	data views;relational olap;molap;decision support;data cube;distributed memory systems;concurrent computing;multi dimensional database techniques;high performance online analytical processing;memory management;query processing;complex queries;precomputed aggregate calculations;distributed computing;distributed memory machine;performance analysis aggregates data analysis algorithm design and analysis concurrent computing distributed computing decision support systems relational databases query processing memory management;multi dimensional;arrays;data analysis;decision support system;in memory parallel data cube construction;data structures;aggregates;decision support systems;performance analysis;distributed databases;distributed memory machines in memory parallel data cube construction high performance online analytical processing complex queries data views relational olap multi dimensional database techniques molap decision support systems multi dimensional arrays precomputed aggregate calculations query processing;distributed memory machines;relational databases;efficient query processing;high performance;online operation;multi dimensional arrays;algorithm design and analysis;data structures decision support systems hypercube networks online operation distributed databases query processing distributed memory systems arrays;hypercube networks;on line analytical processing	Decision support systems use On-Line Analytical Processing (OLAP) to analyze data by posing complex queries that require diierent views of data. Traditionally , a relational approach (ROLAP) has been taken to build such systems. More recently, multi-dimensional database techniques (MOLAP) have been applied to decision-support applications. Data is stored in multi-dimensional arrays which is a natural way to express the multi-dimensionality of the enterprise and is more suited for analysis. Precomputed aggregate calculations in a Data cube can provide eecient query processing for OLAP applications. In this paper we present algorithms and results for in-memory data cube construction on distributed memory machines.	aggregate data;algorithm;array data structure;data cube;decision support system;distributed memory;ibm notes;in-memory database;online analytical processing;online and offline;precomputation;relational database;shallow parsing	Sanjay Goil;Alok N. Choudhary	1997		10.1109/HIPC.1997.634462	algorithm design;parallel computing;decision support system;online analytical processing;relational database;computer science;theoretical computer science;data mining;database;distributed computing;data analysis;programming language;data cube;memory management	DB	-28.816904820296237	2.6035413991542327	141838
9dd4575955bcc0bf8d9d3f092b0268afd9685909	applying text mining on electronic messages for competitive intelligence	distributed system;commerce electronique;correlacion;intercambio informacion;text;systeme reparti;electronic mail;donnee textuelle;comercio electronico;petite moyenne entreprise;red www;text mining;dato textual;reseau web;correo electronico;texte;data mining;intelligence economique;domain knowledge;courrier electronique;small and medium size enterprise;sistema repartido;fouille donnee;echange information;small and medium enterprise;information exchange;small medium sized firm;textual data;competitive intelligence;world wide web;correlation;inteligencia economica;texto;pequenas y medianas empresas;services marketing;busca dato;electronic trade;competitive advantage	This paper presents how text -mining techniques can be used to analyze an enterprise’s external environment searching for competitors, related products and services, marketing strategies and customers’ opinions. A case study, using the UNCTAD Electronic Trading Opportunities (ETO) System, is presented. The ETO system enables “Trade Points” and trade-related bodies to exchange information by e-mail, offering an enormous trade potential and opportunities for small and medium-sized enterprises. As the efficiency of the ETO System is limited, since the volume of circulating messages is very large and the selection and manipulation of this amount of messages surpasses the human analytical limit, we use text -mining tools to support the process. The described strategy uses domain knowledge to extract concepts from the texts. After the concepts are extracted, a mining tool searches for patterns in concept distributions and correlations. Many kinds of analyses can be performed on the resulting patterns to identify strategic information for Competitive Intelligence. Cases and experiments are presented to discuss the proposed strategy and to demonstrate its application in CI processes. The main contribution of the related work is the use of an inexpensive strategy to allow competitive advantages to small and medium enterprises at minimal costs.	electronic trading;email;experiment;r. daneel olivaw;text mining	José Palazzo Moreira de Oliveira;Stanley Loh;Leandro Krug Wives;Rui Gureghian Scarinci;Daniela Leal Musa;Lydia Silva Muñoz;Christian Zambenedetti	2004		10.1007/978-3-540-30077-9_28	text mining;information exchange;competitive intelligence;computer science;artificial intelligence;marketing;data mining;operations research;world wide web;correlation;domain knowledge;competitive advantage	HCI	-27.04413366838309	-4.291914847052719	141992
7f9449e12a799138aff94d974f3b817376ee2c28	a distributed algorithm for formal concepts processing based on search subspaces	distributed algorithm	The processing of dense contexts is a common problem in Formal Concept Analysis. From input contexts, all possible combinations must be evaluated in order to obtain all correlations between objects and attributes. The state-of-the-art shows that this problem can be solved through distributed processing. Partial concepts would be obtained from a distributed environment composed of machine clusters in order to achieve the final set of concepts. Therefore, the goal of this paper is to propose, develop, and evaluate a distributed algorithm with high performance to solve the problem of dense contexts. The speedup achieved through the distributed algorithm shows an improvement of performance, but mainly, a high-balance workload which reduces the processing time considerably. For this reason, the main contribution of this paper is the distributed algorithm, capable of accelerating the processing for dense formal contexts.	block size (cryptography);distributed algorithm;distributed computing;experiment;formal concept analysis;interaction;social network;speedup;turing machine	Nilander R. M. de Moraes;Luis E. Zárate;Henrique C. Freitas	2010			distributed algorithm;computer science;theoretical computer science;machine learning;data mining;database;distributed computing	DB	-23.310977970496342	1.603875403670155	142071
e63bc8acc21b15bacedbdb116fd2a4d7e4466344	automatic generation of a hybrid query execution engine		The ever-increasing need for fast data processing demands new methods for efficient query execution. Just-intime query compilation techniques have been demonstrated to improve performance in a set of analytical tasks significantly. In this work, we investigate the possibility of adding this approach to existing database solutions and the benefits it provides. To that end, we create a set of automated tools to create a runtime code generation engine and integrate such an engine into SQLite which is one of the most popular relational databases in the world and is used in a large variety of contexts. Speedups of up to 1.7x were observed in microbenchmarks with queries involving a large number of operations.	callback (computer programming);code generation (compiler);compiler;experiment;just-in-time compilation;opcode;overhead (computing);partial template specialization;relational database;run time (program lifecycle phase);sqlite;selectivity (electronic);self-modifying code;thread (computing);threaded code	Aleksei Kashuba;Hannes Mühleisen	2018	CoRR		database;code generation;data mining;relational database;computer science;data processing	DB	-31.603421508136275	2.356008896834719	142105
29ab46a4eb99dd915a90ef1a109ea6a13b6195ff	non-structure data storage technology: a discussion	distributed database;fault tolerant;query processing;storage management;software fault tolerance;storage management data structures query processing relational databases software fault tolerance;data model;programming model;data storage;servers;key value non structure nosql;file system;data structures;fault tolerance;non structural;distributed databases;key value;relational databases;non structure;peer to peer computing;peer to peer computing servers distributed databases data models file systems fault tolerance;nosql;file systems;data models;structured data;programming model nonstructure data storage technology structured data complex query data administration nosql database cassandra hbase mongodb data distribution mechanism fault tolerance	The traditional database is designed for the structured data and the complex query. In the environment of the cloud, the scale of data is very large, the data is non-structured, the request of the data is dynamic, these characteristics raise new challenges for the data's storage and administration, in this context, the NoSQL database comes into being. The article compares and analyzes the general structure of the Cassandra, HBase, MongoDB, the data's distribution mechanism in the cluster, the mechanism of the fault tolerance, their support for the programming model and some relevant technologies, in order to provide some references for selecting or building the NoSQL database solution.	apache cassandra;apache hbase;cloud computing;computer data storage;data storage technology;data model;fault tolerance;mongodb;nosql;programming model;requirement;requirements analysis	Shidong Huang;Lizhi Cai;Zhenyu Liu;Yun Hu	2012	2012 IEEE/ACIS 11th International Conference on Computer and Information Science	10.1109/ICIS.2012.76	computer science;data administration;data mining;xml database;database;view;world wide web;database design	DB	-30.212715588183364	1.6219426043327405	142403
e7cb7de994655de0e8ae2b4f8098daa9c71082ba	a logic for information retrieval	sistema experto;information systems;expert systems;information retrieval;intelligence artificielle;recherche documentaire;online systems;recuperacion documental;artificial intelligence;search strategies;document retrieval;inteligencia artificial;systeme expert;article;microcomputers;expert system;deduction	Abstract   This paper examines the potential of recent work in artificial intelligence for the development of more effective information retrieval systems. The primary task in this research has been to examine and define the role of an expert system in the domain of bibliographic retrieval. Once such a goal can be described the available knowledge representations and techniques can be evaluated. This paper examines the role of an expert bibliographic retrieval system, examines an artificial intelligence view of information retrieval, and then describes a prototype expert information retrieval system that has been designed and implemented.		Carolyn R. Watters;Michael A. Shepherd	1987	Inf. Process. Manage.	10.1016/0306-4573(87)90112-9	document retrieval;legal expert system;question answering;relevance;cognitive models of information retrieval;computer science;artificial intelligence;data mining;microcomputer;adversarial information retrieval;data retrieval;expert system;information retrieval;information system;human–computer information retrieval	DB	-25.18268284238422	-2.828467326208883	142800
45a1707e6011f5437b240d5f2a11ba36b96f2aa2	on packing r-trees	bottom up;range query;data distribution;hilbert curve	We propose new R-tree packing techniques for static databases. Given a collection of rectangles , we sort them and we build the R-tree bottom-up. There are several ways to sort the rectangles; the innovation of this work is the use of fractals, and speciically the hilbert curve, to achieve better ordering of the rectangles and eventually better packing. We proposed and implemented several variations and performed experiments on synthetic, as well as real data (TIGER les from the U.S. Bureau of Census). The winning variation (`2D-c') was the one that sorts the rectangles according to the hilbert value of the center. This variation consistently outperforms the packing method of Roussopoulos and Leifker 24], as well as other R-tree variants. The performance gain of the our method seems to increase with the skeweness of the data distribution; speciically, on the (highly skewed) TIGER dataset, it achieves up to 58% improvement in response time over the older packing algorithm and 36% over the best known R-tree variant. We also, introduce an analytical formula to compute the average response time of a range query as a function of the geometric characteristics of the R-tree.	algorithm;bottom-up parsing;database;experiment;fractal;hilbert curve;r-tree;range query (database);response time (technology);set packing;synthetic data;tipu's tiger;top-down and bottom-up design;topologically integrated geographic encoding and referencing	Ibrahim Kamel;Christos Faloutsos	1993		10.1145/170088.170403	range query;top-down and bottom-up design	DB	-24.972145364381856	1.2450871072983347	143051
6b55d2cf175231de8244d662ae1bf26458e07b4e	extending a knowledge base to support explanations	explanation;knowledge based system;knowledge representation explanation knowledge based systems;problem solving knowledge based systems intelligent systems educational institutions character generation natural languages programming profession cognitive science appropriate technology government;heuristic knowledge procedure knowledge knowledge representation knowledge base explanations relations application abstraction;knowledge representation;knowledge based systems;knowledge base	The quality of the explanations given by a knowledge-based system depends heavily on the knowledge in the system. In this paper, we argue that the knowledge needed for explanations includes domain problem solving knowledge, the principles that support it, and also knowledge a b u t relations and their semantics. We also describe how this knowledge can be organized in a knowledge base to support explanations in an intelligent tutoring system.	knowledge base;knowledge-based systems;problem solving	Yuemei Zhang;Martha W. Evens;Joel A. Michael;Allen A. Rovick	1990		10.1109/CBMSYS.1990.109407	natural language processing;knowledge representation and reasoning;knowledge base;organizational learning;empirical evidence;knowledge integration;computer science;knowledge management;artificial intelligence;body of knowledge;mathematical knowledge management;knowledge-based systems;knowledge engineering;open knowledge base connectivity;procedural knowledge;knowledge extraction;personal knowledge management;commonsense knowledge;knowledge value chain;domain knowledge	AI	-30.85459411485573	-6.443656252307914	143365
9d9b0804774567897fc409318db8684112ed58b7	artificial neural network in the control process of object’s states basis for organization of a servicing system of a technical objects	system modeling;artificial neural networks;diagnostic information;servicing process;knowledge base;expert system	This paper presents a method to creation of a servicing expert system including an artificial neural network. The theoretical basis was presented with the model of an operation process of objects in the form of the following models: mathematical (analytical), graphical and descriptional. For the tests, a model was developed of an organization of a servicing technical system of those technical objects which require short shutdown times (aircrafts, radiolocation systems, etc.). The mathematical basis was presented for the execution of the task of servicing of a technical object. The idea of the servicing of the object was described as a transformation of the properties of the operational function of the object from the space of the current servicing to the form of the space of the features of the nominal (model) operation of the object. The results were presented of the radar system.	artificial neural network;expert system;radar;shutdown (computing)	Stanislaw Duer	2011	Neural Computing and Applications	10.1007/s00521-011-0606-6	knowledge base;simulation;systems modeling;computer science;artificial intelligence;expert system;artificial neural network	PL	-22.62712335534493	-5.0878903726578155	143503
e101deccde1688f9bb1ca0ddddf54fca0f8412d9	a multi-agent view of distributed collaborative decision support systems			decision support system	Abdelkader Adla;Bakhta Nachet;Abdelkader Ould-Mahraz	2012		10.3233/978-1-61499-073-4-253	computer science;knowledge management;decision support system;intelligent decision support system	HCI	-30.852329478942764	-9.04656489640333	143589
ae9114d20bb8b2a98c592233f6b3549b47922d17	artificial intelligence tools for handling legal evidence	forensic;artificial intelligent;evidential reasoning;bayesian;link analysis;procedural;generalization			Ephraim Nissan	2007		10.4018/978-1-59140-987-8.ch007	computer science;artificial intelligence;machine learning;data mining;procedural reasoning system	Logic	-27.691579490626133	-8.478345982724724	143605
86faa6436c54de5a1ffb34910b83f11e0a347019	the inverted data warehouse based on targit xbone		We present TARGIT’s Xbone memory-based analytics server and define the concept of an Inverted Data Warehouse (IDW). We demonstrate the high-performance analytics properties of this particular design, as well as its resistance to failures. Additionally, we present a large scale solution in which TARGIT Xbone and IDW are implemented incorporating Google search data. The solution is used for so-called Search Engine Optimization (SEO) and can reveal interesting information about Google’s algorithmic behavior on specific searches. Finally, we demonstrate the combined TARGIT Xbone and IDW to be very cost-effective and thus available to small enterprises that would normally not benefit from Big Data analytics.	big data;commodity computing;data mining;google search;investigative data warehouse;mined;program optimization;relevance;search engine optimization;server (computing)	Morten Middelfart	2014		10.1007/978-3-662-46839-5_2	data mining;search engine optimization;big data;data warehouse;analytics;computer science	ML	-33.25460561469533	0.4553268682306576	144033
38c677fe58b6f04eb6c02569d14c6b56ba223ef0	query-adaptive data space partitioning using variable-size storage clusters	satisfiability;spatial access method;evaluation studies;cost model	"""All spatial access methods decompose the data space into a number of subspaces or cells. The size of a cell is determined by the capacity of a single disk page. Here we present considerations on data space partitions which use large multi-page storage clusters of variable size rather than single pages. Its main motivation is given by real complex geographical objects ranging from 100 Bytes to 10 KBytes in storage size. For such objects it does not make sense to insist on the one-page-per-cell paradigm. The problem of determining """"good"""" multi-page storage clusters is attacked by introducing a cost model. In the ideal case an expected query is satisfied by accessing only one storage duster of appropriate size. To get near to this optimum we adapt the data space partition to query ranges. We call this approach a query driven partitioning strategy. Initial evaluation studies show the feasibility of this approach. 1 I n t r o d u c t i o n The handling of spatial data in geographic information systems (GIS) is an important and complex problem of high significance. There is a need to be able to manage geographical objects like highways, rivers or cities within maps, or streets, parcels and buildings in an urban planning environment. Every geographical object consists of spatial and non-spatial or thematic description parts. The spatial description itself is a complex information in structure and storage size and is often much more space consuming than the thematic information. The structure of spatial description may consist of multiple discontinuous subparts or may have areas with holes. For example (see Figure 1) discontinuous subparts appear at a river line, that is interrupted by a lake or a river with several springs. A lake containing islands is an example of an area with holes. The storage size of such spatial descriptions out of a collection of spatial objects ranges e.g. from simple triangles up to sophisticated polygons with several hundreds of points [2]. Accordingly the storage space needed for a single spatial object may vary from a few bytes to tens of thousands of bytes. The great variety and the overall large storage requirements for spatial objects necessitates a redesign of storage management components in spatial databases to support GIS applications [11]. Special storage managers must be involved to support the varying size of spatial objects. Moreover the execution of spatial"""		Gisbert Dröge;Hans-Jörg Schek	1993		10.1007/3-540-56869-7_19	parallel computing;computer science;theoretical computer science;database	DB	-27.333775758009352	2.1811892714333636	144154
1cdc8ed71b9d1196488d28c2a7b5b03f68439003	the formal aspects of structured modeling	modelizacion;graph theory;structural model;teoria grafo;systeme aide decision;sistema informatico;computer system;reseau;theorie graphe;red;modelisation;decision support system;information systems decision support systems foundations for the design of;recherche operationnelle;networks graphs theory graph based paradigm for model description;systeme informatique;operational research;modeling;investigacion operacional;network;philosophy of modeling a formalism for model description	Structured modeling is an approach to the development of a new generation of computer-based modeling environments. This paper, which is part of a series, presents a formal development of the definitions and theory of structured modeling.		Arthur M. Geoffrion	1989	Operations Research	10.1287/opre.37.1.30	systems modeling;decision support system;computer science;artificial intelligence;graph theory;mathematics;modeling language;algorithm	ML	-23.133087136749808	-2.6405872883514956	144248
12a069429e579493623383c7ecc481a060fe180b	advanced fuzzy inference engines in situation aware computing	context aware application;context aware computing;context awareness;context aware;fuzzy set;procesamiento informacion;03cxx;pervasive computing;teoria conjunto;semantics;conjunto difuso;theorie ensemble;prise de decision;ensemble flou;environmental parameter;context inference;set theory;semantica;semantique;classification;fuzzy set theory;hierarchical representation;information processing;fuzzy inference;situation awareness;03exx;sistema difuso;systeme flou;traitement information;toma decision;clasificacion;fuzzy system;advanced semantics	We focus on the very important family of context-aware applications. Context-aware computing relies on tasks like capturing/sensing environmental parameters (e.g., lightness, location), classifying context, and inferring further knowledge about that context (determine the situation the user is currently in). However, the relevant applications have to deal with the inherent imperfection of context sensing for decision making. We propose an extension of context representation and inference for situation-aware applications. Our model relies on Fuzzy Set Theory to accommodate the imperfection of sensed context. Based on this model, we have developed three fuzzy inference engines, which rely on advanced semantics (specialization, mereological and compatibility relations). We have evaluated the proposed engines through a series of experiments involving real users. Our findings indicate the strong points of the proposed context classification and inference processes.		Christos Anagnostopoulos;Stathes Hadjiefthymiades	2010	Fuzzy Sets and Systems	10.1016/j.fss.2009.09.022	information processing;adaptive neuro fuzzy inference system;computer science;artificial intelligence;machine learning;data mining;mathematics;semantics;context model;fuzzy set;fuzzy control system	NLP	-21.207466589213247	-0.9173116882162212	144318
2944568377e25ae2b3a60e2355f9f35f7612fbe0	impression store: compressive sensing-based storage for big data analytics		For many big data analytics workloads, approximate results suffice. This begs the question, whether and how the underlying system architecture can take advantage of such relaxations, thereby lifting constraints inherent in today’s architectures. This position paper explores one of the possible directions. Impression Store is a distributed storage system with the abstraction of big data vectors. It aggregates updates internally and responds to the retrieval of top-K high-value entries. With proper extension, Impression Store supports various aggregations, top-K queries, outlier and major mode detection. While restricted in scope, such queries represent a substantial and important portion of many production workloads. In return, the system has unparalleled scalability; any node in the system can process any query, both reads and updates. The key technique we leverage is compressive sensing, a technique that substantially reduces the amount of active memory state, IO, and traffic volume needed to achieve such scalability.	aggregate function;approximation algorithm;big data;clustered file system;compressed sensing;computer data storage;lambda lifting;scalability;symmetry-breaking constraints;systems architecture	Jiaxing Zhang;Ying Yan;Liang Jeff Chen;Minjie Wang;Thomas Moscibroda;Zheng Zhang	2014			computer science;operating system;data mining;database;world wide web	DB	-32.123124470856105	-0.07599826627827934	144363
cf3e057316429364a149ef064207f799eba2153d	contributions to time-bounded problem solving using knowledge-based techniques			knowledge-based systems;problem solving	Niladri Chatterjee	1995				Logic	-27.89982938569499	-7.756379025484081	144485
41a12c3f60f198b41a51da3a8c65b6ea4a8fdaca	typed feature formalisms as a common basis for linguistic specification	head driven phrase structure grammar;semantic processing;kunstliche intelligenz;finite automata;logical form	Typed feature formalisms TFF play an increasingly impor tant role in CL and NLP Many of these systems are inspired by Pollard and Sag s work on Head Driven Phrase Structure Grammar HPSG which has shown that a great deal of syntax and semantics can be neatly encoded within TFF However syntax and semantics are not the only ar eas in which TFF can be bene cially employed In this paper I will show that TFF can also be used as a means to model nite automata FA and to perform certain types of logical inferencing In particular I will i describe how FA can be de ned and processed within TFF and ii propose a conservative extension to HPSG which allows for a restrict ed form of semantic processing within TFF so that the construction of syntax and semantics can be intertwined with the simpli cation of the logical form of an utterance The approach which I propose provides a uniform HPSG oriented framework for di erent levels of linguistic pro cessing including allomorphy and morphotactics syntax semantics and logical form simpli cation Acknowledgements This paper has bene ted from numerous people at vari ous workshops where parts of it have been presented in particular at the Sprach wissenschaftliches Kolloquium Univ of T ubingen the st Annual Meeting of the ACL Columbus Ohio and the International Workshop on Machine Translation and the Lexicon of the European Association for Machine Transla tion Heidelberg I would like to thank Elizabeth Hinkelman for reading a draft of this paper I m especially indebted to Petra Ste ens for carefully reading the pre nal version and for making detailed suggestions	automata theory;columbus;head-driven phrase structure grammar;lexicon;machine translation;natural language processing;time to first fix	Hans-Ulrich Krieger	1993		10.1007/3-540-59040-4_23	natural language processing;computer science;linguistics;algorithm	NLP	-19.938651512828336	2.676732890832951	144532
349f63b6bd4b5656cb5331827bfaa1feaa8faa6d	some notes on knowledge assimilation in deductive databases	default reasoning;abduction;adquisicion del conocimiento;base connaissance;intelligence artificielle;ingenieria logiciel;logical programming;acquisition connaissances;raisonnement;software engineering;deductive database;abduccion;programmation logique;belief revision;base dato deductiva;agent intelligent;knowledge acquisition;razonamiento;intelligent agent;genie logiciel;artificial intelligence;base conocimiento;base donnee deductive;agente inteligente;inteligencia artificial;query answering;reasoning;programacion logica;abductive logic programming;deductive databases;knowledge base	This paper is intended to serve as a background for studies in the field of knowledge assimilation in deductive databases. Rather than presenting a formal theory or a technical methodology, it provides a largely informal overview of some of the constituent issues of knowledge assimilation. Various tasks of knowledge assimilation, particularly those related to the integrity-preserving satisfaction of update requests, are discussed. Also the use of abductive logic programming for knowledge assimilation is addressed. Close interrelationships of seemingly disconnected tasks such as query answering, updating, default reasoning, belief revision, and of the underlying inference principles of deduction and abduction, are pointed out. Particular attention is paid to the various kinds of hypotheses used in abductive logic programming for implementing knowledge assimilation.	data assimilation;database	Hendrik Decker	1998		10.1007/BFb0055502	knowledge base;computer science;artificial intelligence;machine learning;data mining;database;belief revision;intelligent agent;reason;algorithm;abductive logic programming	DB	-24.19362212522726	-3.0317857719592074	144594
915a4918400818e65476eda23abe1aa84d4b7d88	a truth maintenance system for supporting constraint-based reasoning		Many types of choice problems that arise in design, be it architectural/engineering design or the design of economic models, can be formulated as constraint satisfaction problems (CSPs). In general, TMSs are a useful computational mechanism for maintaining consistent beliefs or assumptions in problems characterized by a set of constraints. They also enable a problem solver to explore a search space more efficiently by recording the causes of failed partial solutions, and provide a limited explanation capability since reasons for beliefs are recorded explicitly. In this paper we describe a Truth Maintenance System (TMS) whose architecture has been motivated by the structure of a commonly occurring type of CSP. We show that by exploiting structural features of dependency constraints involved in CSPs and adopting a certain delineation of responsibilities between the TMS and a problem solver, considerable simplicity in the TMS architecture and efficiency in its status assignment algorithms is achieved. We also compare how reasoning systems designed for solving constraint satisfaction problems using our specialized TMS differ from those using other truth maintenance models such as McAllester's RUP and de Kleer's ATMS.	case-based reasoning;reason maintenance	Vasant Dhar	1989	Decision Support Systems	10.1016/0167-9236(89)90017-1	artificial intelligence;theoretical computer science;machine learning;data mining;database;algorithm	AI	-19.7163041558477	-4.341956014300885	144655
33ce7dc55c3d41dee23b8e62d454526378692289	planning, evaluation and decision criteria for the development of the first hospital computer system in argentina			common criteria;linc	Roberto Julio Schteingart	1978			multiple-criteria decision analysis;data mining;computer science	HCI	-31.139968640082156	-9.279560475216654	144780
1554005e82b57d2b8bb3c0c266fc9f24590dea29	information filters and their implementation in the syllog system	syllog system;information filter;knowledge base	"""The study of knowledge has always been one of the central issues of the AI research. The general belief has been that """"the more you have it the better you are."""" It was well understood that incorrect knowledge is harmful and should be avoided, but correct knowledge had been mostly considered beneficial. The potential harmfulness of correct knowledge received attention in very few early works but has become a key issue in several recent works (Markovitch, & Scott, 1988a; Markovitch, & Scott, 1988b; Minton, 1988; Tambe, & Newell, 1988) ."""		Shaul Markovitch;Paul D. Scott	1989			knowledge base;simulation;computer science;knowledge management;artificial intelligence;knowledge-based systems;machine learning;data mining	AI	-21.143721099458443	-9.681563087544083	145075
a6f8fe289aa61788336f912128e48ecdc7f0190a	document-oriented data warehouses: models and extended cuboids, extended cuboids in oriented document	data models data warehouses object oriented modeling relational databases warehousing load modeling big data;big data;olap cuboid nosql document oriented system big data warehouse;document specific implementation features document oriented data warehouses extended cuboids big data trend not only sql systems nosql systems data structuration storage possibilities document oriented system olap cuboids relational database models document oriented models document oriented features drill down queries;warehousing;relational databases;storage management big data data mining data warehouses query processing;data warehouses;load modeling;object oriented modeling;data models	Within the Big Data trend, there is an increasing interest in Not-only-SQL systems (NoSQL). These systems are promising candidates for implementing data warehouses particularly due to the data structuration/storage possibilities they offer. In this paper, we investigate data warehouse instantiation using a document-oriented system (a special class of NoSQL systems). On the one hand, we analyze several issues including modeling, querying, loading data and OLAP cuboids. We compare document-oriented models (with and without normalization) to analogous relational database models. On the other hand, we suggest improvements in order to benefit from document-oriented features. We focus particularly on extended versions of OLAP cuboids that exploit nesting and arrays. They are shown to work better on workloads with drill-down queries. Research in this direction is new. As existing work focuses on feasibility issues, document-specific implementation features, modeling and cross-model comparison.	big data;cuboid;data drilling;database model;instance (computer science);model selection;nosql;online analytical processing;relational database;sql	Max Chevalier;Mohammed El Malki;Arlind Kopliku;Olivier Teste;Ronan Tournier	2016	2016 IEEE Tenth International Conference on Research Challenges in Information Science (RCIS)	10.1109/RCIS.2016.7549351	data modeling;big data;relational database;computer science;data science;data mining;database;warehouse	DB	-32.60427031278745	1.541422220322677	145281
4b7de76e9aff445bb5f384080580c9d85ff1a590	approximation techniques for spatial data	range query;management system;geographic information system;spatial data;query processing;spatial join;query optimization;spatial database;data summarization;clustering;incremental data bubbles;relational database system;technical report;computer science	Spatial Database Management Systems (SDBMS), e.g., Geographical Information Systems, that manage spatial objects such as points, lines, and hyper-rectangles, often have very high query processing costs. Accurate selectivity estimation during query optimization therefore is crucially important for finding good query plans, especially when spatial joins are involved. Selectivity estimation has been studied for relational database systems, but to date has only received little attention in SDBMS. In this paper, we introduce novel methods that permit high-quality selectivity estimation for spatial joins and range queries. Our techniques can be constructed in a single scan over the input, handle inserts and deletes to the database incrementally, and hence they can also be used for processing of streaming spatial data. In contrast to previous approaches, our techniques return approximate results that come with provable probabilistic quality guarantees. We present a detailed analysis and experimentally demonstrate the efficacy of the proposed techniques.	approximation algorithm;communication endpoint;experiment;geographic information system;mathematical optimization;provable security;query optimization;range query (data structures);relational database;selectivity (electronic);spatial database;spatial query;vhdl-ams	Abhinandan Das;Johannes Gehrke;Mirek Riedewald	2004		10.1145/1007568.1007646	online aggregation;range query;sargable;query optimization;relational database management system;query expansion;computer science;technical report;query by example;data mining;management system;database;spatial analysis;geographic information system;cluster analysis;view;spatial database;information retrieval;alias;query language;spatial query	DB	-27.192827210274324	2.849211506083501	145341
1129fd59243567101a18897a7258d3401d8eb1ec	graph-based synopses for relational selectivity estimation	relational data;synopses;relational database;approximation;relational;selectivity;synthetic data	"""This paper introduces the Tuple Graph (TUG) synopses, a new class of data summaries that enable accurate selectivity estimates for complex relational queries. The proposed summarization framework adopts a """"semi-structured"""" view of the relational database, modeling a relational data set as a graph of tuples and join queries as graph traversals respectively. The key idea is to approximate the structure of the induced data graph in a concise synopsis, and to estimate the selectivity of a query by performing the corresponding traversal over the summarized graph. We detail the TUG synopsis model that is based on this novel approach, and we describe an efficient and scalable construction algorithm for building accurate TUGs within a specific storage budget. We validate the performance of TUGs with an extensive experimental study on real-life and synthetic data sets. Our results verify the effectiveness of TUGs in generating accurate selectivity estimates for complex join queries, and demonstrate their benefits over existing summarization techniques."""	approximation algorithm;experiment;graph (discrete mathematics);real life;relational database;scalability;selectivity (electronic);semiconductor industry;synthetic data;tree traversal;video synopsis	Joshua Spiegel;Neoklis Polyzotis	2006		10.1145/1142473.1142497	wait-for graph;relational model;relational database;computer science;data mining;database;conjunctive query;graph database;information retrieval	DB	-30.959958892329368	3.8335423839965013	145368
048a731c45b1f95330bf971a4000d0b88f4915e7	a file-based approach for recommender systems in high-performance computing environments	databases;file based;database system;random access memory;memory management;high performance computing;database management systems;storage management;information overload;scalable;data storage;hpc;recommender system;technology and engineering;high performance computer;random access storage;random access memory recommender systems databases educational institutions memory management conferences;storage management database management systems random access storage recommender systems;recommender systems;nosql;conferences;processor cores file based approach recommender systems high performance computing environments recommendation systems information overload huge datasets ram memory computing node scalable data storage approach database systems recommendation process file based data storage approach file based recommendation system;nosql recommender system file based high performance computing hpc scalable	Since recommendation systems tackle the problem of information overload, the processing of huge datasets can not be avoided. When these datasets no longer fit into the RAM memory of a computing node, a scalable data storage approach is required. While database systems are frequently used for this goal, they have their disadvantages and when not properly designed may slow down the recommendation process. In this paper we propose an alternative file-based data storage approach that is particularly well suited for a high-performance computing environment where the usage of databases may not always be an option. By breaking down the recommendation process in separate phases and carefully structuring the input and output of each phase, we have build a file-based recommendation system that scales proportional with the number of computing nodes and processor cores available in each node.	algorithm;apache hadoop;computation;computational problem;computer data storage;database;embarrassingly parallel;image scaling;information overload;input/output;movielens;nosql;parallel computing;random-access memory;recommender system;scalability;supercomputer	Simon Dooms;Toon De Pessemier;Luc Martens	2011	2011 22nd International Workshop on Database and Expert Systems Applications	10.1109/DEXA.2011.3	scalability;computer science;information overload;computer data storage;data mining;database;world wide web;recommender system;memory management	DB	-31.214761365347123	-0.41370254835699305	145413
797d188f9f081199287d0fc6f59e2036ee032b7d	towards a computational theory of cognitive maps	cognitive map;representacion conocimientos;sistema experto;computability theory;etude theorique;carte cognitive;etude experimentale;information visuelle;apprentissage conceptuel;intelligence artificielle;informacion visual;aprendizaje conceptual;visual information;estudio teorico;concept learning;artificial intelligence;inteligencia artificial;systeme expert;theoretical study;knowledge representation;representation connaissances;estudio experimental;mapa cognoscitivo;expert system	A computational theory of cognitive maps is developed which can explain some of the current findings about cognitive maps in the psychological literature and which provides a coherent framework for future development. The theory is tested with several computer implementations which demonstrate how the shape of the environment is computed and how one's conceptual representation of the environment is derived. We begin with the idea that the cognitive mapping process should be studied as two loosely coupled modules: The first module, known as the raw cognitive map, is computed from information made explicit in Marr's 212-D sketch and not from high-level descriptions of what we perceive. The second module, known as the full cognitive map, takes the raw cognitive map as input and produces different “abstract representations” for solving high-level spatial tasks faced by the individual.	cognitive map;computation	Wai-Kiang Yeap	1988	Artif. Intell.	10.1016/0004-3702(88)90064-1	knowledge representation and reasoning;cognitive model;concept learning;fuzzy cognitive map;computability theory;cognitive models of information retrieval;cognitive map;computer science;artificial intelligence;expert system	AI	-24.939763698778847	-9.081532645840207	145499
dfc38ca4358c24b313f90063fad62bfb15f57a29	design of a signature file method that accounts for non-uniform occurrence and query frequencies	signature file;text retrieval;access method;extraction method	In this paper we study a variation of the signature Ale access method for text and attribute retrieval. According to this method, the documents (or records) are stored sequentially in the “text flle”. Abstractions (“signatures”) of the documents (or records) are stored in the “signature Ale”. The latter serves as a Alter on retrieval: It helps discarding a large number of non-qualifying documents. We propose a signature extraction method that takes into account the query and occurrence frequencies, thus achieving better performance. The model we present is general enough, so that results can be applied not only for text retrieval but also for files with formatted data.	digital signature;document retrieval;information retrieval	Christos Faloutsos;Stavros Christodoulakis	1985			flat file database;computer science;journaling file system;data mining;database;open;access method;information retrieval	DB	-28.440546493282064	3.396927758513048	145706
08921e406ba290eb732299f5ef332659dafd1ba8	the trace club expert system and databases			database;expert system	Gavin Williams	1988			club;database;data mining;expert system;computer science	DB	-30.311368574491222	-8.821502936924906	145969
4748a152a5a82fa0a675260c43f815878cc9824b	emulating gestalt mechanisms by combining symbolic and subsymbolic information processing procedures	sound recognition;learning algorithm;gestalt theory;teoria gestalt;algorithme apprentissage;theorie gestalt;reconocimiento sonido;musical sound;pattern matching;reconnaissance son;musicologie;concordance forme;son musical;reseau neuronal;algoritmo aprendizaje;red neuronal;sonido musical;neural network	Pattern matching based on Gestalt principles can be understood as a process of discovering an abstract logical function. Knowledge based processes are needed in order to handle musical information on higher levels and to describe the relations between patterns in special musical contexts, e.g. in harmonic perception. The article sketches a computational approach to a cognitive model of Gestalt perception integrating subsymbolic and symbolic methods of information processing. The model is based on the combination of a neural network (backpropagation net) with methods of inductive learning in artificial intelligence. For the special case of musical transposition, the transfer of information between subsymbolic and symbolic based methods is demonstrated. From this point of view, the recognition of musical transposition, being a special case of Gestalt perception, depends on the inference of logical functions which can not be achieved by standard methods of pattern matching.	artificial intelligence;emulator;gestalt psychology;information processing	Udo Mattusch	1996		10.1007/BFb0034134	computer vision;computer science;artificial intelligence;machine learning;pattern matching;mathematics;programming language;artificial neural network;algorithm;gestalt psychology	AI	-25.05512819382367	-9.171271295516199	146008
a5f8fc6f173045b22962b165b245e554aba724f6	a selective survey of the use of artificial intelligence for database design systems	entity relationship model;representacion conocimientos;base donnee;concepcion sistema;logic design;specification;database;base dato;base connaissance;modelo entidad relacion;intelligence artificielle;modele entite relation;artificial intelligent;conception logique;especificacion;system design;relational model;query;artificial intelligence;base conocimiento;inteligencia artificial;database design;knowledge representation;representation connaissances;concepcion logica;conception systeme;knowledge based systems;requete;knowledge base	One of the significant developments of research in Artificial Intelligence and databases is the adoption of knowledge-based techniques for automating database design. This paper examines the database design process by phases, identifies the key challenges of each, and discusses how a knowledge-based approach could contribute to meeting these challenges. For each design phase, two or three prototype systems that employ interesting knowledge-based techniques are analyzed using a common framework and example. Based on this discussion, suggestions are made for future research.	artificial intelligence;database design	Veda C. Storey	1993	Data Knowl. Eng.	10.1016/0169-023X(93)90045-Q	knowledge base;logic synthesis;relational model;artificial architecture;entity–relationship model;computer science;artificial intelligence;data mining;database;specification;algorithm;database design;systems design	AI	-24.681557672663086	-2.8309732714417417	146033
cacc467bcf1ddd63979344485e5722225558ea61	knowledge and planning in an action-based multi-agent framework: a case study	multiagent system;game theory;teoria juego;intelligence artificielle;theorie jeu;strategic planning;domain modelling;agent intelligent;logique epistemique;intelligent agent;common knowledge;calcul situation;planification strategique;epistemic logic;artificial intelligence;interaction protocol;agente inteligente;inteligencia artificial;sistema multiagente;situation calculus;planificacion estrategica;systeme multiagent	The situation calculus is a logical formalism that has been extensively developed for planning. We apply the formalism in a complex multi-agent domain, modelled on the game of Clue. We find that the situation calculus, with suitable extensions, supplies a unified representation of (1) the interaction protocol, or structure of the game, (2) the dynamics of the knowledge and common knowledge of the agents, and (3) principles of strategic planning.	formal system;game theory;interaction protocol;multi-agent system;semantics (computer science);situation calculus	Bradley Bart;James P. Delgrande;Oliver Schulte	2001		10.1007/3-540-45153-6_12	game theory;simulation;strategic planning;epistemic modal logic;computer science;artificial intelligence;situation calculus;intelligent agent;algorithm;common knowledge	AI	-22.85242716639944	-7.813660456735546	146067
e81342bf755f7f604ad0e0fe4583d93c1a8c3c5f	fuzzy set methods for uncertainty management in intelligence analysis	multiagent system;fuzzy set;procesamiento informacion;incertidumbre;uncertainty;conjunto difuso;uncertainty management;ensemble flou;weapon;intelligence artificielle;imperfect information;intelligence analysis;information processing;arma;informacion imperfecta;artificial intelligence;incertitude;inteligencia artificial;arme;traitement information;sistema multiagente;systeme multiagent;information imparfaite	Abstract#R##N##R##N#Considerable concern has arisen regarding the quality of intelligence analysis. This has been, in large part, motivated by the task of determining whether Iraq had weapons of mass destruction. One problem that made this analysis difficult was the uncertainty in much of the information available to the intelligence analysts. In this work, we introduce some tools that can be of use to intelligence analysts for representing and processing uncertain information. We make considerable use of technologies based on fuzzy sets and related disciplines such as approximate reasoning. © 2006 Wiley Periodicals, Inc. Int J Int Syst 21: 523–544, 2006.	fuzzy set;uncertainty quantification	Ronald R. Yager	2006	Int. J. Intell. Syst.	10.1002/int.20143	intelligence analysis;uncertainty;information processing;computer science;artificial intelligence;perfect information;machine learning;data mining;fuzzy set;operations research	AI	-21.21802017075533	-2.4720043458852756	146118
38d95b44e28e9aa7cc9c650633022f3c4d7aa340	medical decision making by computer			medical decision making	V. X. Gledhill	1972	Australian Computer Journal		decision analysis;r-cast;medical algorithm;computer science;data mining;decision support system;business decision mapping	Theory	-30.805809537109074	-9.134226052209923	146410
5f9f5b378b51f9598302cd5b9e9a99d116b40fbc	cognitive maps and fuzzy implications	cognitive map;fuzzy set;carte cognitive;social sciences;conjunto difuso;transitivite;ensemble flou;analyse;fuzzy sets;modelo;mapa cognitiva;analysis;modele;inconsistency;models;fuzzy implication;analisis	A cognitive map is a collection of nodes linked by some arcs. Up to this point, there is unanimity in the literature about the previous definition. But if we look closer at the meaning of the nodes and links, we can see that there are crucial differences between the various authors. And these differences are not always explicit. In spite of this, it seems that many authors perform on the maps the same kind of analysis (strongly inspired by the book of Axelrod), even if these analyses are not consistent with their conception of a cognitive map. That is why it is important to clearly and formally define the kind of map used. In this paper, we propose a formal definition of a cognitive map relying on the concept of fuzzy implication. Thus in our framework, a node is a logical proposition and a link is an implication. Starting from our definition, we show some properties of this kind of maps and some analysis techniques.	cognitive map	Thierry Marchant	1999	European Journal of Operational Research	10.1016/S0377-2217(98)00133-7	fuzzy cognitive map;computer science;artificial intelligence;analysis;mathematics;fuzzy set	Crypto	-19.510299513661177	-0.7554634255110326	146689
6fa8654cf4595db545d38edd4ca8fd60a9624510	a formal method for deriving command production language from human intents	formal method	Human intents are mental states which drive actions. The expressions of such actions by humans are usually in the form of (knowledge) abstractions and concepts which carry certain properties germane to language structures and representation. With these assumptions, a method for human intent representation using command language is developed.	formal methods	Celestine A. Ntuen	1997			finite-state machine;formal specification;knowledge representation and reasoning;command language;specification language;computer science;formal methods;natural language processing;abstraction;artificial intelligence;expression (mathematics)	NLP	-24.487914063631724	-7.367277651703862	146692
17c7ccc5cd68411fe4d41e15e6e2e438b72ceda1	a case‐based reasoning approach for building a decision model	decision models;case base reasoning;decision analysis;case based reasoning;influence diagram	A methodology based on case-based reasoning is proposed to build a topological-level influence diagram. It is then applied to a project proposal review process. The formulation of decision problems requires much time and effort, and the resulting model, such as an influence diagram, is applicable only to one specific problem. However, some prior knowledge from the experience in modeling influence diagrams can be utilized to resolve other similar decision problems. The basic idea of case-based reasoning is that humans reuse the problem-solving experience to solve new problems. In this paper, we suggest case-based decision class analysis (CB-DCA), a methodology based on case-based reasoning, to build an influence diagram. CB-DCA is composed of a case retrieval procedure and an adaptation procedure. Two measures are suggested for the retrieval procedure, one a fitting ratio and the other a garbage ratio. The adaptation procedure is based on decision-analytic knowledge and decision participants’ domain-specific knowledge. Our proposed methodology has been applied to an environmental review process in which decision-makers need decision models to decide whether a project proposal is accepted or not. Experimental results show that our methodology for decision class analysis provides decision-makers with robust knowledge-based support. Expert Systems, July 2002, Vol. 19, No. 3 123	artificial neural network;cb unix;case-based reasoning;decision problem;decision support system;experiment;expert system;influence diagram;problem solving	Jae Kwang Lee;Jae Kyeong Kim	2002	Expert Systems	10.1111/1468-0394.00198	case-based reasoning;optimal decision;influence diagram;decision analysis;decision engineering;computer science;knowledge management;artificial intelligence;model-based reasoning;decision tree;decision rule;management science;evidential reasoning approach;weighted sum model;business decision mapping	AI	-21.586372527610248	-5.059074376320412	146742
711012e12a75264c0b65c339d797673c468eb72d	query expansion methods for collaborative information retrieval	busqueda informacion;utilisation information;uso informacion;systeme documentaire;information retrieval system;text mining;information use;information retrieval;elargissement requete;interrogation base donnee;interrogacion base datos;data mining;ensanchamiento requerimiento;fouille donnee;recherche information;sistema recuperacion documental;decouverte connaissance;document retrieval system;descubrimiento conocimiento;information system;query expansion;busca dato;database query;systeme information;collaborative information retrieval;sistema informacion;knowledge discovery	Information Retrieval Systeme haben in den letzten Jahren nur geringe Verbesserungen in der Retrieval Performance erzielt. Wir arbeiten an neuen Ansätzen, dem sogenannten Collaborativen Information Retrieval (CIR), die das Potential haben, starke Verbesserungen zu erreichen. CIR ist die Methode, mit der durch Ausnutzen von Informationen aus früheren Anfragen die Retrieval Peformance für die aktuelle Anfrage verbessert wird. Wir haben ein eingeschränktes Szenario, in dem nur alte Anfragen und dazu relevante Antwortdokumente zur Verfügung stehen. Neue Ansätze für Methoden der Query Expansion führen unter diesen Bedingungen zu Verbesserungen der Retrieval Performance . The accuracy of ad-hoc document retrieval systems has reached a stable plateau in the last few years. We are working on so-called collaborative information retrieval (CIR) systems which have the potential to overcome the current limits. We define CIR as a task, where an information retrieval (IR) system uses information gathered from previous search processes from one or several users to improve retrieval performance for the current user searching for information. We focus on a restricted setting in CIR in which only old queries and correct answer documents to these queries are available for improving a new query. For this restricted setting we propose new approaches for query expansion procedures. We show how CIR methods can improve overall IR performance.	committed information rate;document retrieval;hoc (programming language);information retrieval;query expansion	Armin Hust	2004	Informatik - Forschung und Entwicklung	10.1007/s00450-004-0174-4	text mining;query expansion;computer science;artificial intelligence;data mining;information system	Web+IR	-30.556950375141927	-3.105394289290488	146744
bc54ed7413a774be0a5453940fee546740165f02	query-adaptive online partitioning of associated data for efficient retrieval	itemsets;resource description framework;servers;distributed databases;adaptation models;data models;partitioning algorithms	Data partitioning is a crucial component of any distributed storage system that wants to scale. For retrieval efficiency, data frequently requested together in the same query should be placed on the same server as much as possible. Although intuitive, this is not easy to be implemented if constrained by load balancing, computationally, it is an NP hard problem. Existing research has offered approximate solutions optimized for a given workload of queries, in which the order as to when each query is received is not considered. This paper initiates a new study on online partitioning algorithms that are sequentially optimized for a query sequence. In the new problem, the queries arrive in a stream manner, unknown, and given the option to revise the partition after each query, the objective is to minimize the total query processing cost and data migration cost. We formulate this problem formally, investigate several online heuristics, and evaluate them using simulation.	approximation algorithm;clustered file system;competitive analysis (online algorithm);computer data storage;database;geolocation;heuristic (computer science);load balancing (computing);np-hardness;online algorithm;partition problem;sequential access;server (computing);simulation	Shaobo Zhang;Duc A. Tran	2017	2017 IEEE 31st International Conference on Advanced Information Networking and Applications (AINA)	10.1109/AINA.2017.72	data modeling;online aggregation;sargable;query optimization;query expansion;web query classification;computer science;theoretical computer science;rdf;data mining;database;distributed database;server	DB	-25.931400340902094	3.9470089759122793	146859
4c21fc30342e78d9274b3145d8ccdf41d437c59a	geographic hypermedia using search space transformation	heterogeneous data;video signal processing;geographic hypermedia;video browsing;modified r-tree;attribute query;geographic information systems;tree searching;point query;geospatial video clip;gps data;nonspatial data;geospatial video;implementable data model;geographic hypermedia navigation;hypermedia;query formulation;search space transformation;query processing;point query processing algorithm;data model;search space	We present a new approach for linking heterogeneous data of the same objective nature, such as 2D maps, 3D virtual environments and videos with GPS data. We have identified three key challenges (georeferencing, content creation for geospatial videos and bidirectional linking) that should be addressed to link among geographic hypermedia. We propose an easily implementable data model that serves well as a foundation for point query in 2D and attribute query in a video. We also present a point query processing algorithm for video browsing by using the modified R-tree. The proposed method supports geographic hypermedia navigation by providing the bidirectional linking. Experimental results indicate that the proposed approach is effective in retrieving geospatial video clips and nonspatial data.	algorithm;data model;database;geographic information system;global positioning system;hypermedia;map;mobile device;r-tree;video clip;virtual reality;virtual world	Sung-Soo Kim;Jong Hyun Park	2004	Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.	10.1109/ICPR.2004.1333779	sargable;query optimization;query expansion;web query classification;data model;computer science;database;geographic information system;web search query;world wide web;information retrieval;query language	Visualization	-27.319468312372138	-0.28704926364258654	146920
22b5af42c58d40880559e6dab32ddf4c3f2557ca	odysseusrecsys: collaborative filtering based on a data stream management system		The development of algorithms for online Collaborative Filtering (CF) in the past few years enables to add new rating data to existing models. The Recommender System (RecSys) task changes from calculating recommendations from a static and finite dataset to continuously processing rating data. Instead of using stream processing frameworks to implement CF algorithms, we present a prototype that extends the open source Data Stream Management System (DSMS) Odysseus in a generic and domain-independent way. The user can build a custom RecSys that benefits from existing DSMS features by defining a continuous query with a declarative query language.	algorithm;collaborative filtering;management system;open-source software;prototype;query language;recommender system;stream processing	Cornelius A. Ludmann;Marco Grawunder;Hans-Jürgen Appelrath	2015			information retrieval;source data;collaborative filtering;recommender system;stream processing;management system;query language;data stream management system;computer science	DB	-32.51240020936629	2.326815624867381	146947
b8e241b1c02c55c9f1f69d857fe4254749bc5500	local search in the coordination of intelligent agents	intelligent agent;local search	In a world inhabited by numerous agents pursuing distinct goals, conflicts are inevitable. To succeed in the environment, an agent must explicitly reason about the behaviors of other agents as well as itself, and be prepared to find new behaviors that are more coordinated. Because traditional AI has had great success viewing problem solving as a search in a problem space, we have chosen to represent the process of coordination as a distributed search (Durfee eZ al. 1994). In searching through a joint behavior space for coherent coordination patterns, an agent must observe three kinds of constraints: its abilities, its goals, and the activities of other agents in the environment. The nature of the third constraint is dependent on the abilities and goals of the other agents in the environment. Knowledge of other agents’ planned actions is often sufficient for conflict avoidance; however, the ability to reason about alternative activities not only for oneself but for other agents requires deeper modeling of them. Our concept of the behavior as a modeling structure contains not only spatial and temporal information about agents’ actions but also represents their goals and capabilities. With this modeling information an agent can reason from other agents’ goals and capabilities to arrive at likely alternative behaviors for them and itself. We call this local search. Depending on the distribution of knowledge among the agents, local search might occur at any number of agents. Our approach complements the distributed search process of (Durfee & Montgomery 1991)’ which emphasized the efficient propagation of information among agents rather than the local search of an individual agent. We are investigating local search in the producerconsumer-transporter (PCT) domain, by implementing a search for coordination patterns for solving package delivery problems. In a PCT problem, “producers” create objects that must be delivered by “transporters” to %onsumer” agents, who cause the objects to disappear. Representing coordination schemes as a hierarchy of behaviors, we have been able to generate many different agent organizations by decomposing according to agent goals and agent capa-	coherence (physics);distributed web crawling;intelligent agent;local search (optimization);montgomery modular multiplication;problem domain;problem solving;software propagation;transporter classification database;type inhabitation	Daniel E. Damouth;Edmund H. Durfee	1994			agent architecture;computer science;artificial intelligence;local search;intelligent agent	AI	-23.136295754366046	-9.514417649789937	147009
00ba27ddbe10210827821464fdfaac22ad6c6cfb	learning-based sparql query performance modeling and prediction	sparql;feature modeling;prediction;query performance	One of the challenges of managing an RDF database is predicting performance of SPARQL queries before they are executed. Performance characteristics, such as the execution time and memory usage, can help data consumers identify unexpected long-running queries before they start and estimate the system workload for query scheduling. Extensive works address such performance prediction problem in traditional SQL queries but they are not directly applicable to SPARQL queries. In this paper, we adopt machine learning techniques to predict the performance of SPARQL queries. Our work focuses on modeling features of a SPARQL query to a vector representation. Our feature modeling method does not depend on the knowledge of underlying systems and the structure of the underlying data, but only on the nature of SPARQL queries. Then we use these features to train prediction models. We propose a two-step prediction process and consider performances in both cold and warm stages. Evaluations are performed on real world SPRAQL queries, whose execution time ranges from milliseconds to hours. The results demonstrate that the proposed approach can effectively predict SPARQL query performance and outperforms state-of-the-art approaches.	central processing unit;conjunctive query;dimensionality reduction;feature (machine learning);feature model;k-nearest neighbors algorithm;machine learning;performance prediction;run time (program lifecycle phase);sparql;scheduling (computing);triplestore	Wei Emma Zhang;Quan Z. Sheng;Yongrui Qin;Kerry L. Taylor;Lina Yao	2017	World Wide Web	10.1007/s11280-017-0498-1	workload;rdf;sql;data mining;artificial intelligence;web search query;machine learning;performance prediction;computer science;sparql;rdf query language;query optimization	Web+IR	-32.05702818540864	-1.6539684771808632	147171
fb7b837399b09acb2ac6838f900e5aa7267dc10c	towards ontology-based cognitive vision	knowledge based vision;vision system;cognitive vision;knowledge acquisition;ontological engineering;knowledge base	This paper details a visual-concept-ontology-driven knowledge acquisition methodology. We propose to use a visual concept ontology to guide experts in the visual description of the objects of their domain (e.g., pollen grain). The proposed knowledge acquisition process results in a knowledge base enabling semantic image interpretation. An important benefit of our approach is that the knowledge acquisition process guided by the ontology leads to a knowledge base close to low-level vision. A visual concept ontology and a dedicated knowledge acquisition tool have been developed and are presented. We propose a generic methodology that is not linked to any application domain. An example shows how the knowledge acquisition model can be applied to the description of pollen grain images.	application domain;generic-case complexity;high- and low-level;knowledge acquisition;knowledge base;pollen	Nicolas Maillot;Monique Thonnat;Alain Boucher	2004	Machine Vision and Applications	10.1007/s00138-004-0142-9	computer vision;knowledge base;machine vision;idef3;computer science;knowledge management;ontology;body of knowledge;knowledge-based systems;open knowledge base connectivity;data mining;procedural knowledge;knowledge extraction;personal knowledge management;domain knowledge	AI	-33.4476294949318	-5.106350799846038	147312
517a2af00dbd9b45f2c053e6c33ec1dd1b4f787d	on answering why-not questions in reverse skyline queries	query processing;information extraction;question answering information retrieval;data management;swinburne;data point modification techniques reverse skyline queries why not questions dynamic skylines query point why not point;0805 distributed computing;college of science and engineering;question answering information retrieval query processing;access control;0806 information systems;semantics companies database systems complexity theory heuristic algorithms educational institutions	This paper aims at answering the so called why-not questions in reverse skyline queries. A reverse skyline query retrieves all data points whose dynamic skylines contain the query point. We outline the benefit and the semantics of answering why-not questions in reverse skyline queries. In connection with this, we show how to modify the why-not point and the query point to include the why-not point in the reverse skyline of the query point. We then show, how a query point can be positioned safely anywhere within a region (i.e., called safe region) without losing any of the existing reverse skyline points. We also show how to answer why-not questions considering the safe region of the query point. Our approach efficiently combines both query point and data point modification techniques to produce meaningful answers. Experimental results also demonstrate that our approach can produce high quality explanations for why-not questions in reverse skyline queries.	approximation algorithm;data point;display resolution;information retrieval;pareto efficiency;quality of results	Md. Saiful Islam;Rui Zhou;Chengfei Liu	2013	2013 IEEE 29th International Conference on Data Engineering (ICDE)	10.1109/ICDE.2013.6544890	query optimization;query expansion;data management;computer science;access control;data mining;database;web search query;range query;information extraction;information retrieval;query language	DB	-25.365359380555123	3.7903134549722326	147389
0a647b8e1aa82fc3a0c9defedc1f57c9ceb412b3	using group support systems for developing a knowledge-based explanation facility	traitement automatise;description systeme;evaluation performance;system description;sistema experto;comparative analysis;adquisicion del conocimiento;performance evaluation;expert systems;group support system;decision aid;estudio comparativo;evaluacion prestacion;computer supported cooperative work;accounting;finalyzer;ayuda decision;acquisition connaissances;group support systems;multi user;comptabilite;etude comparative;tratamiento automatizado;decision making process;knowledge acquisition;comparative study;traitement manuel;aide decision;financial analysis;delphi method;contabilidad;descripcion sistema;group memory;systeme expert;delphi approach;metodo delphi;manual processing;methode delphi;tratamiento manual;automated processing;knowledge base;expert system;knowledge engineering	"""Group Support Systems (GSS) have been shown to improve the satisfaction and performance of multi-user decision making processes. This study describes how they can be used to acquire knowledge from multiple domain experts for building the explanation facilities of expert systems. In this study, a manual iterative Delphi approach was initially used to acquire knowledge from """"ve experts to build the explanation facility of a """"nancial analysis expert system. Next, GSS tools were utilized to repeat the acquisition of explanatory knowledge in a computer supported cooperative work environment. Comparative analysis suggests that the use of the parallel communication and group memory GSS tools enable process gains, such as getting more complete information, synergy, simulation, and learning, to be realized. Moreover, the fact that the domain experts worked together instead of separately reduced production blocking and cognitive inertia since they could build upon each other's comments. Signi""""cant reductions in the time required by experts and the knowledge engineer/facilitator suggests that GSS represent an e$cient and e!ective approach for supporting the acquisition of explanatory knowledge from multiple experts. ( 2000 Elsevier Science Ltd. All rights"""	blocking (computing);computer-supported cooperative work;expert system;iterative method;knowledge engineer;multi-user;simulation;synergy	Jasbir Singh Dhaliwal;Lai Lai Tung	2000	Int J. Information Management	10.1016/S0268-4012(99)00061-4	delphi method;computer science;engineering;knowledge management;artificial intelligence;operations research;expert system;domain knowledge	AI	-27.03067603345379	-4.546847048717404	147565
37c4702a7e7d35d45e8ee4a487104eda742fcc01	representation and use of explicit justifications for knowledge base refinements	first principle;domain knowledge;knowledge base	We discuss the representation and use of justification structures as an aid to knowledge base refinement. We show how justifications can be used by a system to generate explanations – for its own use – of potential causes of observed failures. We discuss specific information that is usefully included in these justifications to allow the system to isolate potential faulty supporting beliefs for its rules and to effect repairs. This research is part of a larger effort to develop a Learning Apprentice System (LAS) that partially automates initial construction of a knowledge base from first-principle domain knowledge as well as knowledge base refinement during routine use. A simple implementation has been constructed that demonstrates the feasibility of building such a system.	knowledge base;refinement (computing)	Reid G. Smith;Howard A. Winston;Tom M. Mitchell;Bruce G. Buchanan	1985			knowledge base;first principle;computer science;knowledge management;artificial intelligence;body of knowledge;knowledge-based systems;data mining;domain knowledge	AI	-30.36108077841604	-6.281125097637096	147657
59099c2a1e992f01190539cf0731e394d117792a	versioning a full-text information retrieval system	range query;information retrieval system;inverted index;data replication;information management;indexation	In this paper, we present an approach to the incorporation of object versioning into a distributed full-text information retrieval system. We propose an implementation based on “partially versioned” index sets, arguing that its space overhead and query-time performance make it suitable for full-text IR, with its heavy dependence on inverted indexing. We develop algorithms for computing both historical queries and time range queries and show how these algorithms can be applied to a number of problems in distributed information management, such as data replication, caching, transactional consistency, and hybrid media repositories.	ai winter;algorithm;alvey;blender (software);cache (computing);customer support;distributed computing;information management;information retrieval;norman packard;overhead (computing);range query (data structures);range query (database);replication (computing);software versioning;stellar classification	Peter G. Anick;Rex A. Flynn	1992		10.1145/133160.133183	range query;inverted index;computer science;data mining;database;information management;world wide web;information retrieval;replication	DB	-29.800198052868456	1.4917663135890085	147786
574ea145b6fae974ce452019bbeea072ca250d31	a short account of knowledge engineering	knowledge engineering		knowledge engineering	John Fox	1984	Knowledge Eng. Review	10.1017/S0269888900000424	computer science;systems engineering;knowledge management;artificial intelligence;knowledge engineering;domain knowledge	DB	-31.56319701193511	-6.488387093626225	147804
2c3465dab5f8ba79e93b267048c968e0a1f3d419	legal expert systems: the inadequacy of a rule-based approach	expert systems;rule based systems;rule based;legal reasoning;law;case based systems;expert system	The two different categories of legal AI system are described, and legal analysis systems are chosen as objects of study. So-called judgment machines are discussed, but it is decided that research in legal AI systems would be best carried-out in the area of legal expert systems. The process of legal reasoning is briefly examined, and two different methods of legal knowledge representation are discussed (rule-based systems and case-based systems). It is argued that a rule-based approach to legal expert systems is inappropriate given the requirements of lawyers and the nature of legal reasoning about cases. A new approach is described, incorporating both rulebased and case-based knowledge representation. It is claimed that such an approach can form the basis of an effective and useful legal expert system. CR	knowledge representation and reasoning;legal expert system;logic programming;requirement;rule-based system	James Popple	1990	Australian Computer Journal	10.2139/ssrn.1335646	rule-based system;legal expert system;computer science;knowledge management;artificial intelligence;data mining;management science;expert system	AI	-21.710594048470387	-5.998415726276765	147810
616c630baffaaee16dad3899dedd450ac13c26cf	a survey on domain knowledge representation with frames		Domain knowledge acquisition, presentation and maintenance play an important role in software development. Frame-based knowledge bases are used to support the decision-making process. We believe that a use of a knowledge base that supports model transformations is not less important. To clarify the current state of a use of frame systems we have investigated recent research in the field to find out about techniques used for knowledge acquisition, frame elements, implementation technologies, existing limitations in implementation and integration with other knowledge representation formats. The overview showed that knowledge acquisition often is manual, procedural knowledge in frames can be separated, web-enabled knowledge bases are the trend, and the frame systems can be used in hybrid knowledge bases. However, some limitations in performance and integration with other knowledge representation systems exist due to support of different world paradigms. The obtained results show that despite existing limitations, frame-based knowledge systems still are in use and researchers found ways how to adapt them to the modern requirements.	knowledge acquisition;knowledge base;knowledge representation and reasoning;knowledge-based systems;model transformation;requirement;software development	Vladislavs Nazaruks;Janis Osis	2017		10.5220/0006388303460354	data mining;computer science;domain knowledge	AI	-31.756634683593713	-4.099245225938594	147860
7fb34797b4ec1eb1dcebfdab1956515ac8710ac6	intelligent control of complex materials processes	architecture systeme;materials processing;conceptual analysis;base connaissance;intelligence artificielle;materials;analisis conceptual;intelligent control;processing time;production process;control proceso;materiau;material;process control;processus fabrication;temps traitement;artificial intelligence;base conocimiento;arquitectura sistema;inteligencia artificial;analyse conceptuelle;system architecture;tiempo proceso;commande processus;proceso fabricacion;knowledge base	A blackboard based intelligent control system has been developed for a family of complex non-equilibrium materials processes. The system is being tested in the laboratory for control of a particular high risk, high value-added step in the manufacture of carbon-carbon composites. The system uses knowledge based methods in several fundamental ways to fill gaps left by control theory and process models. The system has been demonstrated to substantially reduce processing time	intelligent control	William J. Pardee;Michael A. Shaff;Barbara Hayes-Roth	1990	AI EDAM	10.1017/S0890060400002249	knowledge base;real-time control system;computer science;engineering;artificial intelligence;process control;scheduling;intelligent control	AI	-23.72014084405602	-4.728081768657545	147940
385ff79dc6182c49010f75e92760d8c5e9d9abd7	towards a high quality and web-scalable table search engine	search engine;webtables;search;keyword search;fusiontables;structured data	For over a decade, a large number of studies have explored efficient mechanisms for finding relevant information from structured sources such as relational, semi-structured, and graph databases. While successful in its own right, keyword search over structured data has yet to gain wide spread adoption on the Web, and it is not because of the lack of structured data on the Web. In fact, the Web offers orders of magnitude more structured data than any offline data source: 14 billions tables can be gathered just by considering page content between the table tags. The main reason is that keyword search over structured data on the Web presents a unique set of challenges that are quite different from its non-Web counterparts, as well as different from searching over documents (where the search engines have excelled). In this talk, I will discuss those challenges and our approaches in addressing them at Google's WebTables project. In particular, I will present the table search engine, our initial effort toward building a high quality and scalable structured data search engine, along with our other efforts on managing structured data on the Web.	data model;display resolution;graph database;online and offline;relational database;scalability;search algorithm;semi-structured data;semiconductor industry;web search engine;world wide web	Cong Yu	2012		10.1145/2254736.2254738	search-oriented architecture;database search engine;organic search;metasearch engine;computer science;spamdexing;data mining;search analytics;web search query;world wide web;information retrieval;search engine	DB	-33.33753981612785	3.230328675485072	148045
16d35e11bfa551c23dddba7262a14b112238df9c	eufid: the end user friendly interface to data management systems	data management system	This paper describes a man-machine interface system, called EUFID, that will permit users of data management systems to communicate with those systems in natural language. At the same time, EUFID will act as a security screen to prevent unauthorized users from having access to particular fields in a data base. Our specific objective is to build a system that will be practical, efficient, and widely usable in existing, real-world applications. Our approach is to model the restricted set of linguistic structures and functions required for each application, rather than the manifold linguistic properties of natural language per se. This allows our system to be powerful enough to efficiently process English queries against specific data bases without attempting to understand forms of English that have little or no function in the contexts of those data bases.	management system	Iris Kameny;J. Weiner;Marilyn Crilley;Johannes Burger;R. Gates;David Brill	1978			computer science;data mining;database;programming language;world wide web	DB	-33.16628563302925	-3.5350742828925883	148215
e7dbd58f695207062c092b651c3dce3fd57aa11d	graphical models as languages for computer assisted diagnosis and decision making	graph theory;computer assisted diagnosis;human interaction;belief;teoria grafo;bayes net;life cycle;graphical language;systeme aide decision;proceso markov;diagnostico;sistema ayuda decision;prise decision;theorie graphe;reseau bayes;graphe croyance;decision support system;croyance;machine learning;belief graph;processus markov;markov process;graphical model;point of view;creencia;toma decision;diagnosis;lenguaje grafico;langage graphique;diagnostic	Over the last decade, graphical models for computer assisted diagnosis and decision making have become increasingly popular. Graphical models were originally introduced as ways of decomposing distributions over a large set of variables. However, the main reason for their popularity is that graphs are easy for humans to survey, and most often humans take part in the construction, test, and use of systems for diagnosis and decision making. In other words, at various points in the life cycle of a system, the model is interpreted by a human or communicated between humans. As opposed to machine learning, we shall call this activity human interacted modeling. In this paper we look at graphical models from this point of view. We introduce various kinds of graphical models, and the comprehensibility of their syntax and semantics is in focus.	graphical model	Finn Verner Jensen	2001		10.1007/3-540-44652-4_1	biological life cycle;interpersonal relationship;decision support system;computer science;artificial intelligence;graph theory;belief;machine learning;bayesian network;graphical model;markov process;algorithm	AI	-22.75524882235967	-2.5087197211021337	148287
6ddbbcbe0e291b60c44ef8c353af97a76ce0826b	data structures in lexicography: from trees to graphs		In lexicography, a dictionary entry is typically encoded in XML as a tree: a hierarchical data structure of parent-child relations where every element has at most one parent. This choice of data structure makes some aspects of the lexicographer’s work unnecessarily difficult, from deciding where to place multi-word items to reversing an entire bilingual dictionary. This paper proposes that these and other notorious areas of difficulty can be made easier by remodelling dictionaries as graphs rather than trees. However, unlike other authors who have proposed a radical departure from tree structures and whose proposals have remained largely unimplemented, this paper proposes a conservative compromise in which existing tree structures become augmented with specific types of inter-entry relations designed to solve specific problems.	bilingual dictionary;data structure;dictionary writing system;hierarchical database model;lexicography;programming paradigm;reversing: secrets of reverse engineering;xml	Michal Boleslav Mechura	2016			xml;machine-readable dictionary;natural language processing;tree structure;hierarchical database model;data structure;bilingual dictionary;artificial intelligence;reversing;compromise;computer science	NLP	-32.87137722314676	-3.4221787946706845	148386
845534c7e29d76620dd1ab1123f9045870682896	hash-search: an efficient slca-based keyword search algorithm on xml documents	relational data;information retrieval;efficient algorithm;keyword search;lowest common ancestor;xml document	XML is a de-facto standard for exchanging and presenting information and keyword search over XML documents has become an interesting topic. However semi-structured XML data give rise to many challenges of conventional information retrieval technologies. In order to return highly-related data nodes and improve the quality of keyword search result, SLCA( Smallest Lowest Common Ancestor   )-based keyword search on XML data is recently attracting more and more attention in the database community. In this paper, we design efficient index and propose hash-based method to answer SLCA-based keyword search queries. Our approach outperforms  Incremental Multiway-SLCA approach  , which is the most efficient algorithms in the literature. We demonstrate the effectiveness of our algorithms analytically and experimentally.	hash table;search algorithm;xml	Weiyan Wang;Xiaoling Wang;Aoying Zhou	2009		10.1007/978-3-642-00887-0_44	xml validation;simple api for xml;xml;relational database;computer science;data mining;xml database;database;keyword density;xml signature;information retrieval;efficient xml interchange;lowest common ancestor	DB	-31.627243633421905	4.153443734083272	148480
98456c54cbfc6b5a187e9931ba5cc80783689cd2	probabilistic plan recognition for intelligent information agents: towards proactive software assis	plan recognition;information agent	In this paper, we present a software assistant agent that can proactively manage information on behalf of cognitively overloaded users. We develop an agent architecture, known here as ANTicipatory Information and Planning Agent (ANTIPA), to provide the user with relevant information in a timely manner. In order both to recognize user plans unobtrusively and to reason about time constraints, ANTIPA integrates probabilistic plan recognition with constraint-based information gathering. This paper focuses on our probabilistic plan prediction algorithm inspired by a decision theory that human users make decisions based on long-term outcomes. A proof of concept user study shows a promising result.	agent architecture;algorithm;authorization;decision theory;information needs;partially observable markov decision process;problem domain;real life;sensor;state space;usability testing;wizard (software)	Jean Oh;Felipe Meneguzzi;Katia P. Sycara	2011			computer vision;knowledge management;data mining	AI	-19.582103252906776	-7.8567405891631985	148524
98690105cc4eb823b3834dd98324fefaec9c2e56	rule base revision system theres using progol.	rule based system;rule based;rule extraction;object oriented analysis;knowledge base		belief revision;progol;rule-based system	N. Moriuchi	2000			rule-based system;object-oriented analysis and design;computer science;artificial intelligence	Robotics	-28.2646914409981	-7.199928416747174	148819
757f411ab1f1674c684c0c453bc5e1486d3539e6	can we trace back cognitive processes in root cause analysis	software;metodo analisis;operator;metodologia;logiciel;operador;accident;hombre;methodologie;erreur humaine;cognitive process;methode analyse;error humano;causalite;analysis method;cognition;human;operateur;root cause analysis;cognicion;logicial;accidente;methodology;human error;homme;causality;causalidad	We propose a methodological approach and an analysis tool allowing to graphically trace back the events preceding an accident throughout the cognitive process of the operators involved. The approach is illustrated by a case study concerning the analysis of errors in a series of fill flight simulator sessions.		Mauro Pedrali;Rémi Bastide	1997			cognition;computer science;artificial intelligence;algorithm	PL	-24.869038455329633	-6.798175459894585	148847
1d5888c0259ce2da4035d3053ea42c1f766004c1	an asynchronous group decision support system study for intelligent multicriteria decision making	multicriteria decision making;operations research;decision maker;decision support systems intelligent systems decision making identity based encryption workstations time factors computer networks computational efficiency computational intelligence;research method;group decision support;lotus notes group decision support system intelligent multicriteria decision making group decision making group decision support systems;group decision support systems;decision making process;group decision support system;system development;group decision making;process model;operations research group decision support systems	In modern business world group decision-making is becoming an extremely important activity. Recent research on computerized group decision support has focused extensively on supporting people co-located in place and time. There is a need, however, for more research focused on asynchronous group decision support systems (GDSS). Multicriteria decision-making (MCDM) theory has produced a wide range of techniques suitable for use in a variety of decision situations. With the support of an asynchronous GDSS which intelligently guides decision makers using an appropriate MCDM technique for a given task, the opportunity exists to explore the improvement of decision-making processes. In this paper we propose an intelligent MCDM process model assisted by an asynchronous GDSS. The paper describes a research project, which focuses on the development of an asynchronous GDSS to support MCDM processes in Lotus Notes environment. The system also contains an intelligent component, which can guide users to select a suitable MCDM model for different problems and tasks. A system development research method is adopted in our	decision support system;ibm notes;process modeling	Patrick P. Cao;Frada Burstein	1999		10.1109/HICSS.1999.772713	decision-making;r-cast;decision support system;intelligent decision support system;decision analysis;decision engineering;knowledge management;decision tree;management science;evidential reasoning approach;business decision mapping	Robotics	-31.511707228776157	-9.420032222018149	148896
68d0454220da2376d7854cf80f128e86c8412eed	compressed database structure to manage large scale data in a distributed environment	large scale;distributed environment	Loss-less data compression is attractive in database systems as it may facilitate query performance improvement and storage reduction. Although there are many compression techniques which handle the whole database in main memory, problems arise when the amount of data increases gradually over time, and also when the data has high cardinality. Management of a rapidly evolving large volume of data in a scalable way is very challenging. This paper describes a disk based single vector large data cardinality approach, incorporating data compression in a distributed environment. The approach provides substantial storage performance improvement compared to other high performance database systems. The compressed database structure presented provides direct addressability in a distributed environment, thereby reducing retrieval latency when handling large volumes of data.	byte;computer data storage;data compression;intel tera-scale;oracle database;scalability;terascale (microarchitecture);terabyte	B. M. Monjurul Alom;Frans A. Henskens;Michael Hannaford	2008			computer science;data science;data mining;database;distributed computing environment	DB	-30.18081191058309	1.3109261182441831	149048
19a3ffdb39a4ce539231de431db6844c07fc48f1	first experiments in cultural alignment repair		Alignments between ontologies may be established through agents holding such ontologies attempting at communicating and taking appropriate action when communication fails. This approach has the advantage of not assuming that everything should be set correctly before trying to communicate and of being able to overcome failures. We test here the adaptation of this approach to alignment repair, i.e., the improvement of incorrect alignments. For that purpose, we perform a series of experiments in which agents react to mistakes in alignments. The agents only know about their ontologies and alignments with others and they act in a fully decentralised way. We show that such a society of agents is able to converge towards successful communication through improving the objective correctness of alignments. The obtained results are on par with a baseline of a priori alignment repair algorithms.	algorithm;baseline (configuration management);converge;correctness (computer science);experiment;ontology (information science);sequence alignment	Jérôme Euzenat	2014			machine learning;ontology (information science);artificial intelligence;correctness;computer science	AI	-21.51544126592399	-9.36978456287414	149252
11b2f14e85c88f37d310ac15ecf941a457006052	becoming increasingly reactive	explanation-based learning mechanism;planning subsystem;theo-agent architecture;new stimulus-response rule;simple mobile robot;robot agent;rapid reaction;stimulus-response subsystem;new situation;robot control architecture	We describe a robot control architecture which combines a stimulus-response subsystem for rapid reaction, with a search-based planner for handling unanticipated situations. The robot agent continually chooses which action it is to perform, using the stimulusresponse subsystem when possible, and falling back on the planning subsystem when necessary. Whenever it is forced to plan, it applies an explanation-based learning mechanism to formulate a new stimulus-response rule to cover this new situation and others similar to it. With experience, the agent becomes increasingly reactive as its learning component acquires new stimulus-response rules that eliminate the need for planning in similar subsequent situations. This Theo-Agent architecture is described, and results are presented demonstrating its ability to reduce routine reaction time for a simple mobile robot from minutes to under a second.	agent architecture;explanation-based learning;mobile robot;robot control	Tom M. Mitchell	1990			simulation;artificial intelligence	AI	-19.451998469625007	-7.9235720924119	149523
2c77d6bd7ea868b31398be0b05ac92bd63d417f3	a multi-agent-based voltage control in power systems using distributed reinforcement learning	simulation ordinateur;modelizacion;voltage control;multidisciplinaire;multiagent system;dispositivo potencia;teleenseignement;commande tension;reseau electrique;agent based;electrical network;reinforcement learning;red electrica;dispositif puissance;langage java;intelligence artificielle;control tension;potencia;java agent development jade;modelisation;tension electrique;multi agent systems;apprentissage renforce;distributed reinforcement learning;power system;voltage;power device;artificial intelligence;puissance;multidisciplinary;coordinacion;lenguaje java;teleensenanza;multidisciplinar;simulacion computadora;inteligencia artificial;secondary voltage control;remote teaching;sistema multiagente;voltaje;aprendizaje reforzado;modeling;computer simulation;power;systeme multiagent;coordination;java language	In this paper we show the application of multi-agent modeling and simulation with distributed reinforcement learning to one of the major problems in power system operations, i.e. voltage control. In this research some agents in the power network work together to provide a desirable voltage profile, using a combination of multi-agent system (MAS) technology and some of the reinforcement learning approaches. In this schema, individual agents who are assigned to voltage controller devices in the power system learn from their experiences to control the system voltage, and also cooperate and communicate with each other to satisfy the whole team goals. A detailed evaluation of methods for controlling voltage in power systems, including multi-agent coordination and distributed reinforcement learning (DRL), demonstrates that this framework yields effective plans, good agent coordination, and successful implementation. In the proposed approach, agent development and communication simulation have been carried out in the Java Agent Development (JADE) framework.	agent-based model;ibm power systems;reinforcement learning	M. Reza Tousi;S. Hossein Hosseinian;Mohammad Bagher Menhaj	2011	Simulation	10.1177/0037549710367904	computer simulation;embedded system;error-driven learning;voltage;simulation;computer science;engineering;artificial intelligence;reinforcement learning	Robotics	-22.788026310473995	-6.7310994754470865	149705
b1ff7bda05720f68641fc51ced5441a13ee271cc	conceptual structures of multicontexts	formal concept analysis	Formal Concept Analysis is based on a formalization of context. Since there are situations where the consideration of one context is not suucient, it is desirable to introduce a formalization of a network of contexts. In this paper, such formalization is given by the notion of mul-ticontext. The aim of the paper is to ooer a rst study of multicontexts and their conceptual structures; in particular, descriptions of conceptual coherences within the formalized network of contexts are presented. The theoretical considerations are illustrated by examples.	formal concept analysis	Rudolf Wille	1996		10.1007/3-540-61534-2_2	computer science;formal concept analysis;conceptual system	SE	-20.590250793905433	4.1314422081701645	149730
a9e4a6baab03b3d607aa6176eaeefdb220365d8a	adaptable similarity search in 3-d spatial database systems	spatial database;similarity search		similarity search;spatial database	Thomas Seidl	1998			spatial database;spatiotemporal database	DB	-27.935662751118212	0.3148009472134141	149791
f2b21627641fbc852a6e767fcee7ed5642aba97d	array dbms and satellite imagery: towards big raster data in the cloud		Satellite imagery have always been “big” data. Array DBMS is one of the tools to streamline raster data processing. However, raster data are usually stored in files, not in databases. Respective command line tools have long been developed to process these files. Most of the tools are feature-rich and free but optimized for a single machine. The approach of partially delegating in situ raster data processing to such tools has been recently proposed. The approach includes a new formal N-d array data model to abstract from the files and the tools as well as new formal distributed algorithms based on the model. ChronosServer is a distributed array DBMS under development into which the approach is being integrated. This paper extends the approach with a new algorithm for the reshaping (tiling) of arbitrary N-d arrays onto a set of overlapping N-d arrays with a fixed shape. Cutting arrays with an overlap enables to perform a broad range of large imagery processing operations in a distributed shared-nothing fashion. Currently ChronosServer provides a rich collection of raster operations at scale and outperforms SciDB up to 80(times ) on Landsat data. SciDB is the only freely available distributed array DBMS to date. Experiments were carried out on 8- and 16-node clusters in Microsoft Azure Cloud.		Ramon Antonio Rodriges Zalipynis;Evgeniy Pozdeev;Anton Bryukhov	2017		10.1007/978-3-319-73013-4_25	distributed algorithm;array dbms;big data;database;satellite imagery;data model;cloud computing;raster data;raster graphics;computer science	DB	-32.176243413549415	-2.0886808998432995	150421
275e03f5a41165210d921c4214b57e7f54f6188d	error repair and knowledge acquisition via case-based reasoning	knowledge refinement;case base reasoning;pattern search;error repair;pattern detection;knowledge acquisition;network structure;case based reasoning;problem solving;fault diagnosis;lsi circuit layout;rule based reasoning;knowledge base	To cope with the knowledge acquisition bottleneck, the authors propose a new architecture combining rule-based reasoning (RBR) , case-based reasoning (CBR) and knowledge acquisition technology in a system which solves pattern search problems. The RBR part searches for specified patterns in a large space represented by a network structure such as an LSI circuit diagram, which contains a great number of patterns and variations. It then carries out specified actions, such as fault diagnosis, on the patterns that are found. The outputs of the RBR part are transferred to the CBR part. ‘The user of the system detects and repairs a few pattern detection errors caused by the RBR part, The CBR part detects and repairs all remaining errors which can be estimated from the user detected ones. The repaired results are sent back to the RBR part to recover the RBR output. The repaired results are also stored automatically in the case base. Similar cases are grouped in a same case family. The knowledge acquisition part relates each case family to an incomplete rule in the RBR knowledge base and proposes modifying the rule. Eventually, the system can obtain refined rules with the cooperation of domain experts. Thus, the problem solving process and knowledge acquisition process are performed cyclically. The architecture was successfully applied to a pair condition extraction problem for an analog LSI circuit layout system. @ 1997 Elsevier Science B.V.	case-based reasoning;circuit diagram;embedded system;integrated circuit;interactivity;knowledge acquisition;knowledge base;logic programming;pattern recognition;pattern search (optimization);problem solving;software deployment	Takeshi Kohno;Susumu Hamada;Dai Araki;Shoichi Kojima;Toshikazu D Tanaka	1997	Artif. Intell.	10.1016/S0004-3702(96)00059-8	pattern search;case-based reasoning;knowledge base;computer science;artificial intelligence;data mining;algorithm	AI	-29.20604465214626	-4.743002712674723	150522
18eaede5c09d6d8ac7de68308428ecd6848f85c8	web semantics in the clouds	distributed system;search engine;query processing;search engines;resource allocation;computational intelligence;web semantique;semantic search cloud computing semantic web distributed systems scalability;intelligence artificielle;query processing cloud computing dynamic resource allocation semantic web meta data search engine;scaling up;dynamic resource allocation;html;computer architecture;nube;large scale;complex data;programming profession;web semantica;clouds;pipelines;semantic web;semantic web meta data query processing resource allocation search engines;software framework;artificial intelligence;meta data;semantic search;scalability;inteligencia artificial;nuage;distributed systems;file systems cloud computing html hardware computer architecture programming profession pipelines open source software java computational intelligence;file systems;open source software;cloud computing;hardware;java	Cloud computing refers to the use of large-scale computer clusters often built from low-cost hardware and network equipment, where resources are allocated dynamically among users of the cluster. While the paradigm is not entirely novel, recent developments in software frameworks for cloud computing are making it increasingly easy for programmers to parallelize and thereby scale-up complex data-processing tasks. This article investigates how this trend is impacting the semantic Web field and shows how cloud computing can be used to analyze, query, and reason with the massive amounts of metadata handled by semantic search engines.	cloud computing;computer cluster;memory management;programmer;programming paradigm;semantic web;semantic search;software framework;web search engine	Peter Mika;Giovanni Tummarello	2008	IEEE Intelligent Systems	10.1109/MIS.2008.94	semantic computing;cloud computing;semantic search;semantic grid;computer science;artificial intelligence;theoretical computer science;computational intelligence;database;world wide web;search engine	OS	-31.296927137699445	-1.2536856352253467	150584
26559b948d10230db9bf267aa83683101103ac7d	a model for integrating dialogue and the execution of joint plans	planning;dialogue.;agent interaction	Coming up with a plan for a team that operates in a non-deterministic environment is a complex process, and the problem is further complicated by the need for team members to communicate while the plan is being executed. Such communication is required, for example, to make sure that information critical to the plan is passed in time for it to be useful. In this paper we present a model for constructing joint plans for a team of agents that takes into account their communication needs. The model builds on recent developments in symbolic non-deterministic planning, ideas that have not previously been applied to this problem.		Yuqing Tang;Timothy J. Norman;Simon Parsons	2009		10.1145/1558109.1558134	planning;simulation;model building;knowledge management;artificial intelligence	AI	-19.421558422779604	-8.333989108261523	150622
bce06eadaee0b15cb5736155169eafac63d46662	mining@home: toward a public-resource computing framework for distributed data mining		Several classes of scientific and commercial applications require the execution of a large number of independent tasks. One highly successful and low-cost mechanism for acquiring the necessary computing power for these applications is the ‘public-resource computing’, or ‘desktop Grid’ paradigm, which exploits the computational power of private computers. So far, this paradigm has not been applied to data mining applications for two main reasons. First, it is not straightforward to decompose a data mining algorithm into truly independent sub-tasks. Second, the large volume of the involved data makes it difficult to handle the communication costs of a parallel paradigm. This paper introduces a general framework for distributed data mining applications called Mining@home. In particular, we focus on one of the main data mining problems: the extraction of closed frequent itemsets from transactional databases. We show that it is possible to decompose this problem into independent tasks, which however need to share a large volume of the data. We thus introduce a data-intensive computing network, which adopts a P2P topology based on super peers with caching capabilities, aiming to support the dissemination of large amounts of information. Finally, we evaluate the execution of a pattern extraction task on such network. Copyright © 2009 John Wiley & Sons, Ltd.	algorithm;cluster analysis;computation;concurrency control;cube;data mining;data-intensive computing;database;desktop computer;feature extraction;john d. wiley;load balancing (computing);pattern recognition;peer-to-peer;programming paradigm;simulation;time complexity;universal instantiation;volunteer computing;web page	Claudio Lucchese;Carlo Mastroianni;Salvatore Orlando;Domenico Talia	2010	Concurrency and Computation: Practice and Experience	10.1002/cpe.1545		DB	-31.72642928504223	-1.323545540564382	150688
82a344d19d6acfbc04f3e95a1f7e24b192c79457	data validation during diagnosis: a step beyond traditional sensor validation		A well known problem in diagnosis is the difficulty of providing correct diagnostic conclusions in light incorrect or missing data. Traditional approaches to solving this problem, as typified in the domains of various complex mechanical systems, validate data by using various kinds of redundancy in sensor hardware. While such techniques are useful, we propose that another level of redundancy exists beyond the hardware level, the redundancy provided by expectations derived during diagnosis. That is, in the process of exploring the space of possible malfunctions, initial data and intermediate conclusions set up expectations of the characteristics of the final answer. These expectations then provide a basis for judging the validity of the derived answer.’ We will show how such expectationbased data validation is a natural part of diagnosis as performed by hierarchical classification expert systems.	belief revision;compiler;data validation;expert system;james r. wait;missing data;problem solving;redundancy (engineering);rejection sampling;sensor;shallow parsing	B. Chandrasekaran;William F. Punch	1987				AI	-21.07014444571387	-4.415572873001269	150842
6b8f355da39a216e73c1c23eb236bfb16fa7e1a5	optimizing random retrievals from clv format optical disks	optic disk;random access	One technique often employed to improve retrieval performance from storage devices is to red;cee seek costs by to clusterin quently accessed data toget er ‘ii R locations on the storage device that are ph sically close. For magnetic disks etermlning the best position B on the disk to place frequently accessed data is strai optical disks with htforwarcl, for t eir fi many different recording formats the solution is much more difficult. We develop a detailed model for the E lacement of data on Constant inear Velocity (CLV) format optical disks that includes distribution of stora e ca acity across the disks surface whit is variable for CLV “t R format optical disks), the seek erformance of the disk drive, de avs r dye ,to yotational latency, and t& &&lbs;;on of ac$esses ovet We derive closed form expressio;s which determine the position of frequently accessed data that will minimize the expected cost of random accesses to the data set.	algorithm;approximation;data access;data item;disk storage;floppy disk;hard disk drive performance characteristics;loss function;optimizing compiler;recording format;recursion;velocity	Daniel Alexander Ford;Stavros Christodoulakis	1991			computer science;random access	DB	-27.328292126446044	3.379969553470194	150949
f55d243e47aefe68b82a000ff50c204bff3da3d3	nonstop sql/mx primitives for knowledge discovery	data mining;structural change;knowledge discovery process;regression analysis;high performance;knowledge discovery	In this paper, we describe the primitives that we have implemented in NonStop SQL/MX, a commercial DBMS, to support the knowledge discovery process. These primitives, combined with the high-performance SQL&IX engine, represent a powerful kernel for performing basic knowledge discovery tasks in a scalable and efftcient manner.	data mining;database;kernel (operating system);nonstop sql;raspberry pi 3 model b (latest version);scalability	John Clear;Debbie Dunn;Brad Harvey;Michael L. Heytens;Peter Lohman;Abhay Mehta;Mark Melton;Lars Rohrberg;Ashok Savasere;Robert M. Wehrmeister;Melody Xu	1999		10.1145/312129.312309	software mining;computer science;data science;structural change;data mining;database;knowledge extraction;regression analysis	ML	-32.667638395843476	1.1008954610155488	150983
0f947f6391f7e31f72147fb4146b6a4254a34f9d	representational effects on the solving of an unstructured decision problem	methode empirique;performance analysis decision support systems graphics problem solving humans tree graphs potential well availability nails;decision aid;systeme aide decision;metodo empirico;empirical method;diagrams;ayuda decision;diagrams decision theory;teoria decision;resolucion problema;decision problem;decision support system;theorie decision;decision theory;aide decision;decision support systems problem formulation dss unstructured decision problem graphic problem representations problem structuring decision analysis;problem solving;resolution probleme	The use of graphic problem representations in problem structuring or formulation is considered, as well as the potential effects of incomplete representations on problem solving. Empirical results suggest that humans cannot reliably identify key problem elements and that the availability of incomplete graphic depictions can constrain the scope of problem-solving activity, harming overall performance. The nature of these process effects is investigated, with implications being drawn for decision-aiding practice. The analysis of how the adverse performance effects occur is shown to have significant implications for decision analysis, decision-support systems and other decision-aiding technologies. >	decision problem	Gerald F. Smith	1989	IEEE Trans. Systems, Man, and Cybernetics	10.1109/21.44024	decision support system;decision theory;computer science;artificial intelligence;diagram;decision problem;empirical research;operations research;algorithm;statistics	Embedded	-24.22194276252261	-6.432896865173549	151770
65292b7aa594304bb80008086dcf3b4b46d448e0	m-cope: a multiple continuous query processing engine	query processing;data stream;continuous query;query optimization;dsms;network traffic;process engineering;data stream management system;multiple query optimization	A data stream management system (DSMS) should support an efficient evaluation scheme for long-running continuous queries over infinite data streams. This demonstration presents a scalable query processing engine, M-COPE (Multiple Continuous Query Processing Engine) developed to evaluate multiple continuous queries efficiently. A multiple query optimization scheme implemented in the system generates a single network of operations as an execution plan for registered queries in order to maximize the reuse of the intermediate results of common sub-expressions in the queries adaptively. In this paper, we describe the overall architecture of M-COPE along with its special features. Network traffic flow streams are used to demonstrate the main features of M-COPE.	mathematical optimization;network packet;query optimization;query plan;scalability	Hong Kyu Park;Se Jung Shin;Sang Hyuck Na;Won Suk Lee	2009		10.1145/1645953.1646303	online aggregation;sargable;query optimization;query expansion;web query classification;computer science;query by example;theoretical computer science;data mining;database;rdf query language;web search query;spatial query	DB	-32.010643236890814	2.5233175240506287	151829
7d28b4a032c02f8482d46d30b90724eae84a68a7	computer-aided identification of mechanical system's technical state with the aid of case-based reasoning	case based reasoning cbr;case base reasoning;identification of technical states for mechanical systems;mechanical systems	The paper considers application of the case-based reasoning paradigm for solving of the problem related to computer-aided identification of technical states for mechanical systems employed in petrochemistry. The results of investigation are case models corresponding to the concept of ''incident'' (known from the practice of reaching reliability) and reflecting the properties and states of mechanical systems; an algorithm of case retrieval; the architecture and software for computer-aided identification of technical states for mechanical systems.	case-based reasoning	Olga A. Nikolaychuk;Alexandr Yu. Yurin	2008	Expert Syst. Appl.	10.1016/j.eswa.2006.10.001	computer science;artificial intelligence;mechanical system	AI	-28.860647565336528	-6.2359936664350295	151864
07fa6e6e5975c83446b680240668bebd8a9cd838	constructing translations between individual vocabularies in multi-agent systems	semantica operacional;representacion conocimientos;multiagent system;systeme intelligent;formal specification;multi agent system;sistema inteligente;operational semantics;specification formelle;especificacion formal;semantique operationnelle;intelligent system;knowledge representation;sistema multiagente;representation connaissances;possible worlds;systeme multiagent	In multi-agent systems, diierent agents usually employ different languages to express their informational and motivational attitudes. During communication processes, the agents should therefore employ some translation mechanism in order to understand the information provided by the other agents. In this paper, we develop a logical framework based on a possible world semantics to model the informational attitudes of agents. The framework covers operations to incorporate newly acquired information, even in case this information is not expressed in the agents' own vocabulary. Finally, we deene an abstract programming language for bilateral communication processes in which agents use these operations to build translations between their individual vocabularies.	bilateral filter;logical framework;multi-agent system;possible world;programming language;vocabulary	Rogier M. van Eijk;Frank S. de Boer;Wiebe van der Hoek;John-Jules Ch. Meyer	1998		10.1007/BFb0057449	knowledge representation and reasoning;computer science;artificial intelligence;multi-agent system;formal specification;database;possible world;programming language;operational semantics;algorithm	AI	-24.372842761551233	-8.074150945836772	151886
20a2d69be91949fa1f7f969c7d259563b25159a1	discovering prerequisite relationships among knowledge components		Knowing the prerequisite structure among the knowledge components in a domain is crucial for instruction and assessment. Treating Knowledge Components as latent variables, we investigate how data on the items that test these KCs can be used to discover the prerequisite structure among the KCs. By modeling the pre-requisite relations as a causal graph, we can then search for the causal structure among the latents via an extension of an algorithm introduced by Spirtes, Glymour, and Scheines in 2000. We validate the algorithm using simulated data.	algorithm;causal filter;causal graph;latent variable	Richard Scheines;Elizabeth Silver;Ilya M. Goldin	2014			machine learning;computer science;artificial intelligence;causal structure;latent variable;graph	AI	-20.716906107736197	0.7750476421239694	151967
3e3aa48e6601493d6cac03e3440470f52e909b95	dynamic load balancing for distributed search	dynamic load balancing;resource allocation;storage management;distributed processing;dynamic partitioning dynamic load balancing distributed search system resource utilization data search content based retrieval digital images digital sound active storage devices intermediate processors host computers resource availability network bandwidth compute node query predicate;complex data;distributed search;digital image;storage management distributed processing resource allocation content based retrieval;load management distributed computing computer networks costs bandwidth information retrieval image retrieval content based retrieval digital images explosives;content based retrieval	This paper examines how computation can be mapped across the nodes of a distributed search system to effectively utilize available resources. We specifically address computationally intensive search of complex data, such as content-based retrieval of digital images or sounds, where sophisticated algorithms must be evaluated on the objects of interest. Since these problems require significant computation, we distribute the search over a collection of compute nodes, such as active storage devices, intermediate processors and host computers. A key challenge with mapping the desired computation to the available resources is that the most efficient distribution depends on several factors: relative power and number of compute nodes; network bandwidth between the compute nodes; the cost of evaluating query predicates; and the selectivity of the given query. This wide range of variables renders manual partitioning of the computation infeasible, particularly since some of the parameters (e.g., available network bandwidth) can change during the course of a search. This paper proposes several techniques for dynamic partitioning of computation, and demonstrates that they can significantly improve efficiency for distributed search applications.	algorithm;application domain;bandwidth (signal processing);brute-force search;central processing unit;computation;computer;content-based image retrieval;digital image;distributed computing;distributed web crawling;experiment;load balancing (computing);query language;rendering (computer graphics);selectivity (electronic)	Larry Huston;Alex Nizhner;Padmanabhan Pillai;Rahul Sukthankar;Peter Steenkiste;Jiaying Zhang	2005	HPDC-14. Proceedings. 14th IEEE International Symposium on High Performance Distributed Computing, 2005.	10.1109/HPDC.2005.1520953	distributed algorithm;parallel computing;query expansion;resource allocation;computer science;theoretical computer science;database;distributed computing;management;digital image;complex data type	HPC	-29.398348885957343	1.0663704845671798	152330
5038dd1ea2a15c47509c7729f37456780826222d	a visualization environment for planning	automated planning;graphical interface;machine learning;graphical interfaces;problem solving;knowledge engineering	This article presents ViTAPlan-2, a visual tool for adaptive planning that is build on top of HAPRC, a rule-configurable planning system, which automatically adapts to each problem, in order to achieve best performance. Apart from HAPRC, ViTAPlan can be interfaced with any other planning system that supports the PDDL language. More than just being a user friendly environment for executing the underlying planner, the tool serves as a unified planning environment for encoding a new problem problem, solving it, visualizing the solution and monitoring its execution on a simulation of the problem's word. The tool consists of various subsystems , each one accompanied by a graphical interface, that collaborate with each other and assist the user, whether he is a knowledge engineer, a domain expert, an academic or even an end user in industry, to carry out complex planning tasks.	graphical user interface;knowledge engineer;planning domain definition language;simulation;subject-matter expert;usability	Dimitris Vrakas;Ioannis P. Vlahavas	2005	International Journal on Artificial Intelligence Tools	10.1142/S0218213005002491	simulation;human–computer interaction;computer science;knowledge management;artificial intelligence;machine learning;knowledge engineering;graphical user interface	Robotics	-30.3672408664118	-5.470833646194483	152426
dc28b1de02f4e1ff19b61405fc7625fccd90c9a5	a*-tree: a structure for storage and modeling of uncertain multidimensional arrays	large-scale array data;bayesian network;underlying joint distribution;multidimensional data;novel data structure;uncertain multidimensional array;alternative graphical model;unified model;multidimensional array tree;graphical model;multidimensional array database system	Multidimensional array database systems are suited for scientific and engineering applications. Data in these applications is often uncertain and imprecise due to errors in the instruments and observations, etc. There are often correlations exhibited in the distribution of values among the cells of an array. Typically, the correlation is stronger for cells that are close to each other and weaker for cells that are far away. We devise a novel data structure, called the A*-tree (multidimensional Array tree), demonstrating that by taking advantage of the predictable and structured correlations of multidimensional data, we can have a more efficient way of modeling and answering queries on largescale array data. An A*-tree is a unified model for storage and inference. The graphical model that is assumed in an A*-tree is essentially a Bayesian Network. We analyze and experimentally verify the accuracy of an A*-tree encoding of the underlying joint distribution. We also study the efficiency of query processing over A*-trees, comparing it to an alternative graphical model.	array data structure;bayesian network;database;experiment;graphical model;unified model	Tingjian Ge;Stanley B. Zdonik	2010	PVLDB	10.14778/1920841.1920963	computer science;theoretical computer science;data mining;database;statistics	DB	-24.231600639690903	3.408676279981603	152527
67c27a5b6ed3e9fad6120d8b706f70ed8fa52265	towards semantic interoperability in a clinical trials management system	clinical data;pistage;management system;red internacional;aplicacion medical;analisis datos;securite;interoperabilite;interoperabilidad;web semantique;rastreo;semantic technologies;semantics;base connaissance;clinical trial;semantica;semantique;semantic interoperability;data analysis;ensayo clinico;analyse syntaxique;metamodel;metamodele;analisis sintaxico;metamodelo;web semantica;syntactic analysis;safety;semantic web;base conocimiento;completitud;analyse donnee;reseau international;medical application;essai clinique;interoperability;completeness;immune tolerance;completude;seguridad;international network;tracking;application medicale;knowledge base	Clinical trials are studies in human patients to evaluate the safety and effectiveness of new therapies. Managing a clinical trial from its inception to completion typically involves multiple disparate applications facilitating activities such as trial design specification, clinical sites management, participants tracking, and trial data analysis. There remains however a strong impetus to integrate these diverse subsystems – each supporting different but related functions of clinical trial management – at syntactic and semantic levels so as to improve clarity, consistency and correctness in specifying clinical trials, and in acquiring and analyzing clinical data. The situation becomes especially critical with the need to manage multiple clinical trials at various sites, and to facilitate meta-analyses on trials. This paper introduces a knowledge-based framework that we are building to support a suite of clinical trial management subsystems. Our initiative uses semantic technologies to provide a consistent basis for the subsystems to interoperate. We are adapting this approach to the Immune Tolerance Network (ITN), an international research consortium developing new therapeutics in immune-mediated disorders.	correctness (computer science);management system;semantic interoperability	Ravi D. Shankar;Susana B. Martins;Martin J. O'Connor;David B. Parrish;Amarendra K Das	2006		10.1007/11926078_65	metamodeling;semantic interoperability;interoperability;knowledge base;completeness;computer science;artificial intelligence;parsing;semantic web;clinical trial;data mining;management system;database;semantics;tracking;semantic technology;data analysis;world wide web;statistics	AI	-25.75980071538348	-4.569649228151663	152677
4627a794898f43714976ccc13e691deb14c31b6e	visualization of community knowledge interaction using associative representation	representacion conocimientos;systeme intelligent;procesamiento informacion;sistema inteligente;verification connaissances;systeme conversationnel;heterogeneous information;interactive system;information processing;intelligent system;background knowledge;sistema conversacional;information system;knowledge representation;traitement information;representation connaissances;systeme information;knowledge verification;sistema informacion	This paper describes a method for supporting knowledge evolution and facilitating awareness in a community at the same time. We propose two ideas. One is associative representation for facilitating externalization of both personal and community information. The associative representation links heterogeneous information without defining the semantics strictly. We leave the interpretation of the semantics to human background knowledge. The other is visualization of information interaction in a community using talking-alter-egos metaphor. Takingalteregos metaphor mimics a salon in which alter-ego representing each community member interact with each others, thereby the community member can see how their own or others' knowledge interact.#R##N##R##N#We have developed a called CoMeMo-Community that pursue collaborative story generation based on the talking-alter-egos metaphor. We investigated how far people can exchange ideas with associative representation and how people react the talking-alter-egos metaphor.		Takashi Hirata;Harumi Maeda;Toyoaki Nishida	1998		10.1007/3-540-49292-5_11	information processing;computer science;artificial intelligence;information system;algorithm	HCI	-24.46046093259727	-8.642119113641723	152998
606b53c4b8718d2fca75412805c5826d4d626c47	open architecture for natural language distributed systems	distributed system;representacion conocimientos;linguistica matematica;architecture systeme;systeme reparti;information retrieval system;lexical functional grammar;systeme base connaissances;open architecture;sistema repartido;computational linguistic;natural language;word recognition;linguistique mathematique;natural language interface;arquitectura sistema;common sense;computational linguistics;information system;knowledge representation;system architecture;representation connaissances;systeme information;knowledge based systems;domain specificity;object model;sistema informacion;knowledge base	Current information retrieval systems focus on or are limited to the use of key words recognition rather than full Natural Language interface. This paper presents an open architecture for Natural Language interface to specific domain. We describe the requirements and the design objectives of a Natural Language interface to an object model that can retrieve answers from a knowledge base. The novelty of the proposed work is that it is based on Computational Linguistic theory, namely, the Lexical-Functional Grammar theory. The linguistic presentation structures of this theory have been formulated into linguistic rules to enhance thc understanding of user queries. In addition, common-sense domain specific knowledge has been investigated and proved to be a suitable approach for eliminating some Natural Language ambiguities found within specific domain. The adoption of the developed prototype into an open architecture is also investigated. This should provide a framework for building similar applications within the chosen domain. The framework has been tested using several newspaper stories to prove the accuracy of our approach.		A. Yamani;Ala Al-Zobaidie;Mohamed T. Ibrahim	1998		10.1007/BFb0054520	natural language processing;language identification;knowledge base;natural language programming;question answering;object model;natural language user interface;open architecture;word recognition;computer science;artificial intelligence;computational linguistics;natural language;lexical functional grammar;information system;algorithm	ML	-24.834205668654143	-2.503920540717797	153395
4eea0d2f9e5337642a5e1799a06ef124223b60f2	time-rollback using logs in historical databases		Abstract   A temporal database supports both real-world (i.e. valid) time and transaction time, and provides the rollback capability. A historical database, on the other hand, supports only the valid time. The paper shows that the rollback capability can be provided for the historical databases by making use of logs, and this is proposed as an efficient alternative to implement temporal databases. The proposed method is shown to be more efficient when the number of rollback queries is not high. Algorithms are presented to apply corrections to the historical databases and for the rollback operation.		N. L. Sarda	1993	Information & Software Technology	10.1016/0950-5849(93)90054-7	rollback;computer science;data mining;database;world wide web	HCI	-28.0338184480223	3.2522403649458123	153554
1e4d74547b3679884486f056082e1f96817dbd28	design and implementation of a replay framework based on a partial order planner	design and implementation;explanation based learning;artificial intelligence;design;planning;knowledge base;partial order;mathematics computers information science management law miscellaneous	In this paper we describe the design and implementation of the derivation replay framework, dersnlp+ebl (Derivational snlp+ebl), which is based within a partial order planner. dersnlp+ebl replays previous plan derivations by first repeating its earlier decisions in the context of the new problem situation, then extending the replayed path to obtain a complete solution for the new problem. When the replayed path cannot be extended into a new solution, explanation-based learning (ebl) techniques are employed to identify the features of the new problem which prevent this extension. These features are then added as censors on the retrieval of the stored case. To keep retrieval costs low, dersnlp+ebl normally stores plan derivations for individual goals, and replays one or more of these derivations in solving multi-goal problems. Cases covering multiple goals are stored only when subplans for individual goals cannot be successfully merged. The aim in constructing the case library is to predict these goal interactions and to store a multi-goal case for each set of negatively interacting goals. We provide empirical results demonstrating the effectiveness of dersnlp+ebl in improving planning performance on randomly-generated problems drawn from a complex domain.	censoring (statistics);explanation-based learning;interaction;query plan;randomness;search tree;tree (data structure)	Laurie H. Ihrig;Subbarao Kambhampati	1996			planning;design;knowledge base;simulation;computer science;artificial intelligence;machine learning;algorithm	AI	-20.552254590781786	-4.92647661779372	153728
988e6d60b4b98430c52aa6eb18b761db429d6ad5	fuzzy logic and intelligent agents: towards the next step of capital budgeting decision support	decision support;investment decision;soft computing;real time;fuzzy logic;business environment;support system;access to information;intelligent agents;real option;intelligent agent;capital budgeting	The economic life of large investments is long and thus necessitates constant dynamic managerial actions. To be able to act in an optimal way in the dynamic management of large investments managers need the support of advanced analytical tools. They need to have constant access to information about the real time situation of the investment, as well as, access to up-to-date information about changes in the business environment. What is more challenging, they need to integrate qualitative information into quantitative analysis process, and to integrate foresight information into the capital budgeting process. In this paper we will look at how emerging soft computing technologies, specifically fuzzy logic and intelligent agents, will help to provide a better support in such a context and then to frame a support system that will make an integrated application of the aforementioned technologies. We will first develop a holistic framework for an agentfacilitated capital budgeting system using a fuzzy real option approach. We will then discuss how intelligent agents can be applied to collect decision information, both qualitative and quantitative, and to facilitate the integration of foresight information into capital budgeting process. Integration of qualitative information into quantitative analysis process will be discussed. Methods for integrating qualitative and quantitative information into fuzzy numbers, as well as, methods for using the fuzzy numbers in capital budgeting will be presented. A specification of how the agents can be constructed is elaborated.	agent-based model;decision support system;design rule for camera file system;freedom of information laws by country;fuzzy logic;fuzzy mathematics;fuzzy number;holism;intelligent agent;numerical analysis;soft computing;software agent	Mikael Collan;Shuhua Liu	2003	Industrial Management and Data Systems	10.1108/02635570310479981	fuzzy logic;r-cast;decision support system;intelligent decision support system;computer science;knowledge management;artificial intelligence;marketing;operations management;capital budgeting;intelligent agent;commerce	AI	-32.50168040729422	-8.348636346390956	153737
4f7a572a013a2d07a9b53cbe955794c0fb73fa06	the integrated decision model in emergency dispatch management and it's implications for design	cognitive engineering;emergency dispatch management;naturalistic decision making;integrated decision model.;decision models;hci	This paper reports on a review of a number of well-known naturalistic decision making. NDM, models (Zsambok & Klein, 1997). Key features of these models were identified and were found to represent different views of the same naturalistic decision making process. The key features of these different views were then integrated into a single model called the Integrated Decision Model. To validate the model, a study of emergency dispatch co-ordinators was conducted to determine the 'goodness of fit' of the model in a naturalistic domain. A Critical Decision Method-based study (Klein et al., 1989) was conducted and found enough evidence in the emergency dispatch management process to support the proposal of the Integrated Decision Model.	adobe streamline;decision theory;dynamic dispatch	B. L. William Wong	2000	Australasian J. of Inf. Systems		decision model;r-cast;simulation;decision analysis;decision engineering;computer science;engineering;knowledge management;artificial intelligence;decision tree;decision rule;naturalistic decision-making;management science;evidential reasoning approach;cognitive ergonomics;business decision mapping	AI	-31.251864567959277	-8.774875287502747	153854
f7554c7d6650578809b286d4db75987dbde7bb4b	towards morally sensitive action selection for autonomous social robots	robot sensing systems collision avoidance ethics mobile robots cleaning	Autonomous social robots embedded in human societies have to be sensitive to human social interactions and thus to moral norms and principles guiding these interactions. Actions that violate norms can lead to the violator being blamed. Robots thus need to be able to anticipate possible norm violations and attempt to prevent them while they execute actions. If norm violations cannot be prevented (e.g., in a moral dilemma situation in which every action leads to a norm violation), then the robot needs to be able to justify the action to address any potential blame. In this paper, we present a first attempt at an action execution system for social robots that can (a) detect (some) norm violations, (b) consult an ethical reasoner for guidance on what to do in moral dilemma situations, and (c) it can keep track of execution traces and any resulting states that might have violated norms in order to produce justifications.	action selection;autonomous robot;embedded system;emoticon;interaction;prisoner's dilemma;semantic reasoner;social robot;tracing (software)	Matthias Scheutz;Bertram F. Malle;Gordon Briggs	2015	2015 24th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)	10.1109/ROMAN.2015.7333661	computer science;artificial intelligence	Robotics	-20.038934929470077	-9.319142528439556	153891
5b5909cd3757e2b9ab6f8784f4c7d936865076ea	strategic directions in artificial intelligence	multiagent system;programme recherche;sistema experto;systeme intelligent;critical study;sistema inteligente;base connaissance;search strategy;intelligence artificielle;etude critique;artificial intelligent;estudio critico;agent intelligent;strategie recherche;intelligent system;intelligent agent;programa investigacion;artificial intelligence;base conocimiento;agente inteligente;inteligencia artificial;systeme expert;sistema multiagente;research program;systeme multiagent;estrategia investigacion;knowledge base;expert system	—constructing intelligent machines, whether or not these operate in the same way as people do; —formalizing knowledge and mechanizing reasoning, both commonsense and refined expertise, in all areas of human endeavor; —using computational models to understand the psychology and behavior of people, animals, and artificial agents; and —making working with computers as easy and as helpful as working with skilled, cooperative, and possibly expert people.	artificial intelligence;computation;computational model;computer;intelligent agent	Jon Doyle;Thomas L. Dean	1996	ACM Comput. Surv.	10.1145/242223.242268	knowledge base;intelligence cycle;simulation;music and artificial intelligence;marketing and artificial intelligence;computer science;artificial intelligence;operations research;expert system;intelligent agent	AI	-24.562966668646762	-8.797715237770557	153908
5f0f6bfd3c0a4e3fdb2d469def939bb5de9dc402	dialogue strategy for horizontal communication in mas organization	organization;argumentation;dialogue strategy;multi agent systems;communication	A multi agent organization model defines the structure, roles, interaction ways and coordination styles of a multi agent system. Multi agent organizations may constrain the communication between included members. Organizations’ communication is the process of sending or receiving all the messages through a group of agents in order to achieve common goals. In a dialogue, agents follow some rules that define the permissive speech acts called dialogue protocol. Aiming for common goals, the dialogue strategy is the policy of agents to choose a particular speech act among the allowed ones by the protocol. In this paper a formal model for dialogue strategy for a group of agents in an organization is proposed in order to choose the most preferable speech acts. The argumentation theory is applied to the proposed method to define the values of plausible speech acts and to rank them. The algorithm finds the best option to utter and also it decreases the volume of exchangingmessages. The proposed dialogue strategy is illustrated via a deliberation dialogue example in a group of agents.	algorithm;experiment;formal language;multi-agent system;software agent	Akram Beigi;Nasser Mozayani	2016	Computational & Mathematical Organization Theory	10.1007/s10588-015-9201-1	economics;computer science;organization;knowledge management;artificial intelligence;multi-agent system;communication	AI	-21.29509035385198	-9.225172635778867	153920
2bbcdc4c14d178935efdba30a3277a61ee412997	chunked extendible dense arrays for scientific data storage	history;arrays indexes vectors resource management history memory;extendible arrays;storage management;resource management;hdf5 chunked extendible dense arrays scientific data storage extremely large databases community large scale scientific applications multidimensional arrays scientific databases access mapping function k dimensional index linear index hierarchical data format version 5;computed access extendible arrays multidemensional dense arrays;arrays;indexes;very large databases storage management;vectors;multidemensional;very large databases;dense arrays;computed access;memory	Several meetings of the Extremely Large Databases Community for large scale scientific applications, advocate the use of multidimensional arrays as the appropriate model for representing scientific databases. Scientific databases gradually grow to massive sizes of the order of terabytes and petabytes. As such, the storage of such databases require efficient dynamic storage schemes where the array is allowed to arbitrary extend the bounds of the dimensions. Conventional multidimensional array representations cannot extend or shrink their bounds without relocating elements of the dataset. In general, extendibility of the bounds of the dimensions, is limited to only one dimension. This paper presents a technique for storing dense multidimensional arrays by chunks such that the array can be extended along any dimension without compromising the access time for an element. This is done with a computed access mapping function, that maps the kdimensional index onto a linear index of the storage locations. This concept forms the basis for the implementation of an array file of any number of dimensions, where the bounds of the array can be extended arbitrarily. Such a feature currently exists in the Hierarchical Data Format version 5 (HDF5). However, extending the bound of a dimension in the HDF5 array file can be unusually expensive in time. Such extensions in our storage scheme for dense array files can still be performed while still accessing elements of the array at orders of magnitude faster than in HDF5 or conventional arrays-files.	access time;array data structure;chunked transfer encoding;computer science;database;extensibility;global arrays (ga);hierarchical data format;index (publishing);map;memory management;out of memory;petabyte;shallow parsing;terabyte;water industry telemetry standard	Gideon Nimako;Ekow J. Otoo;Daniel Ohene-Kwofie	2012	2012 41st International Conference on Parallel Processing Workshops	10.1109/ICPPW.2012.9	database index;array data structure;parallel computing;computer science;resource management;theoretical computer science;data mining;database;memory;sparse array	DB	-30.910194630658882	0.776656266859136	153941
93033e7b1806a1f05ec5978fe32bf350ecf022e3	processing and optimization of complex queries in schema-based p2p-networks	sensibilidad contexto;distributed system;topology;optimisation;gestion memoire;base donnee repartie;base donnee;distribution donnee;systeme reparti;context aware;distributed database;analisis estadistico;red www;optimizacion;query processing;routing;par a par;elargissement requete;traitement requete;storage management;topologie;interrogation base donnee;data management;reseau web;database;routage;base repartida dato;interrogacion base datos;base dato;p2p;donnee globale;probabilistic approach;data distribution;topologia;distributed query processing;gestion memoria;distributed database system;dato global;sistema repartido;global data;statistical analysis;poste a poste;ensanchamiento requerimiento;enfoque probabilista;approche probabiliste;analyse statistique;world wide web;optimization;tratamiento pregunta;p2p networks;sensibilite contexte;query expansion;peer to peer;distribucion dato;database query;enrutamiento	Peer-to-Peer infrastructures are emerging as one of the important data management infrastructures in the World Wide Web. So far, however, most work has focused on simple P2P networks which tackle efficient query distribution to a large set of peers but assume that each query can be answered completely at each peer. For queries which need data from more than one peer to be executed this is clearly insufficient. Unfortunately, though quite a few database techniques can be re-used in the P2P context, a P2P data management infrastructure poses additional challenges caused by the dynamic nature of these networks. In P2P networks, we can neither assume global knowledge about data distribution, nor are static topologies and static query plans suitable for these networks. Unlike in traditional distributed database systems, we have no complete schema and distribution instance available but rather work with distributed schema information which can only direct query processing tasks from one node to one or more neighboring nodes. In this paper we first describe briefly the super-peer based topology and the “schema-aware distributed routing indices extended with statistics and describe how this statistics are extracted and updated. Then we show how these indices facilitate the distribution and dynamic expansion of query plans. Finally we propose transformation rules to optimize query plans and discuss different optimization strategies. Our techniques enable distributed query processing in a schema-based P2P network.	data-intensive computing;distributed computing;distributed database;experiment;federated database system;mathematical optimization;peer-to-peer;program optimization;routing;simulation;world wide web	Hadhami Dhraief;Alfons Kemper;Wolfgang Nejdl;Christian Dirk Wiesner	2004		10.1007/978-3-540-31838-5_3	sargable;query optimization;routing;query expansion;web query classification;data management;computer science;peer-to-peer;data mining;database;view;world wide web;distributed database;query language	DB	-26.02814491915897	3.314456009459606	154258
164bf76c783c9fd2e740c0aabd1f3c7fa77cf3d9	querying large knowledge graphs over triple pattern fragments: an empirical study		Triple Pattern Fragments (TPFs) are a novel interface for accessing data in knowledge graphs on the web. So far, work on performance evaluation and optimization has focused mainly on SPARQL query execution over TPF servers. However, in order to devise querying techniques that efficiently access large knowledge graphs via TPFs, we need to identify and understand the variables that influence the performance of TPF servers on a fine-grained level. In this work, we assess the performance of TPFs by measuring the response time for different requests and analyze how the requests’ properties, as well as the TPF server configuration, may impact the performance. For this purpose, we developed the Triple Pattern Fragment Profiler to determine the performance of TPF server. The resource is openly available at https://doi.org/10.5281/zenodo.1211621. To this end, we conduct an empirical study over four large knowledge graphs in different server environments and configurations. As part of our analysis, we provide an extensive evaluation of the results and focus on the impact of the variables: triple pattern type, answer cardinality, page size, backend and the environment type on the response time. The results suggest that all variables impact on the measured response time and allow for deriving suggestions for TPF server configurations and query optimization.		Lars Heling;Maribel Acosta;Maria Maleshkova;York Sure-Vetter	2018		10.1007/978-3-030-00668-6_6	empirical research;data mining;database;cardinality;sparql;computer science;query optimization;page;response time;monad (category theory);server	NLP	-31.839852508706723	2.94439524438021	154328
320d962d7548c53334ddacb4173d1fe75a09b2c6	loose coupling of failure explanarion and repair: using learning goals to sequence learning models	raisonnement base sur cas;razonamiento fundado sobre caso;learning;intelligence artificielle;system performance;sequence learning;aprendizaje;learning system;apprentissage;learning methods;indexation;artificial intelligence;inteligencia artificial;case based reasoning	Because learning methods (i.e., knowledge repairs) can negatively interact, the arbitrary ordering of knowledge repairs can lead to worse system performance than no learning at all. Therefore, the problem of choosing appropriate learning methods given a performance failure is a significant problem for learning systems. Traditional case-based reasoners index learning or repair methods by specific failure characteristics so that once a failure is detected, a learning method can be brought to bear. Such tight coupling can be contrasted to a loose coupling in which the interaction between failure explanation and learning is mediated by the presence of learning goals generated by the learner. In an empirical study, the Meta-AQUA implementation performed significantly better under the guidance of learning goals (loose coupling) than under a condition in which learning goals were ablated (tight coupling). The conclusion is that unless repair interactions are known not to exist, a loose coupling is necessary for effective learning.	loose coupling	Michael T. Cox	1997		10.1007/3-540-63233-6_512	multi-task learning;case-based reasoning;error-driven learning;simulation;sequence learning;computer science;artificial intelligence;machine learning;algorithm	NLP	-20.792749117637477	-5.556269716852678	154452
27e767e3765a2d08e6f83b758d54a065a4296f4a	on-line computer auditing	information systems;business data processing	An auditing technique that audits transactions as they are being processed will be introduced. Concurrent and Intermittent Simulation (CIS) is an auditing technique that is very similar to parallel simulation in terms of the amount of work and type of code that must be completed by the auditor. However, in terms of capabilities, CIS is much more advanced than parallel simulation.  CIS is an auditing technique that simulates the instruction execution of the application at the time the application is processing a transaction. All data and input to the application is accessible by and shared with the simulation. This means that the simulation knows about each transaction that is entered to the application, all accesses to the data base by the application can be monitored and all working-storage values of the application can be accessed by the simulation. Before any updates are made to the data base or before any output is returned to the users, the simulation can verify the results by executing the appropriate instructions of the simulation by having access to the transaction, working storage values and the data base. If an inconsistency is found, all pertinent information about the system status can be put into the exception log. The simulation then has the choice to allow the results computed by the application to be used, to use the results computed by the simulation or not to use any of the results, as if there was no transaction.	database;relevance;simulation	Harvey S. Koch	1979		10.1145/800177.810066	real-time computing;computer science;data mining;database	DB	-24.355377032158728	-0.021199323183686256	154485
4bdda26a387760c7187f2311386108f7dc12b3de	does prior knowledge facilitate the development of knowledge-based systems?	empirical study;knowledge based system;first year;prior knowledge;high performance;domain specificity;knowledge base	One factor that affects the rate of knowledge base construction is the availability and reuse of prior knowledge in ontologies and domain-specific knowledge bases. This paper reports an empirical study of reuse performed in the first year of the High Performance Knowledge Bases (HPKB) initiative. The study shows that some kinds of prior knowledge help more than others, and that several factors affect how much use is made of the knowledge.	cyc;design of experiments;domain-specific language;experiment;high performance knowledge bases;knowledge base;knowledge engineer;knowledge-based systems;ontology (information science);word lists by frequency	Paul R. Cohen;Vinay K. Chaudhri;Adam Pease;Robert Schrag	1999			knowledge base;computer science;knowledge management;artificial intelligence;data science;body of knowledge;mathematical knowledge management;knowledge-based systems;data mining;procedural knowledge;knowledge extraction;personal knowledge management;commonsense knowledge;empirical research;knowledge value chain;domain knowledge	AI	-32.68793606461827	-5.828608560798413	154489
7287bd8157bb0f18b4fe4528ba109c7fcbb075c0	causal knowledge network integration for life cycle assessment	degree of causal representation;knowledge integration;life cycle assessment;causal knowledge	Sustainability requires emphasizing the importance of environmental causes and effects among design knowledge from heterogeneous stakeholders to make a sustainable decision. Recently, such causes and effects have been well developed in ontological representation, which has been challenged to generate and integrate multiple domain knowledge due to its domain specific characteristics. Moreover, it is too challengeable to represent heterogeneous, domain-specific design knowledge in a standardized way. Causal knowledge can meet the necessity of knowledge integration in domains. Therefore, this paper aims to develop a causal knowledge integration system with the authors’ previous mathematical causal knowledge representation.	causal filter;causality;domain-specific language;knowledge integration;knowledge management;knowledge representation and reasoning	Yun Seon Kim;Keunho Choi;Kyoung-Yun Kim	2011			knowledge integration;computer science;knowledge management;body of knowledge;knowledge-based systems;machine learning;data mining;management science;procedural knowledge;personal knowledge management;knowledge value chain;domain knowledge	AI	-32.75435833195905	-6.992873932775094	154525
6293ae512312247dc7ed2fb3e991bd37d3a3363f	mtls: a tool for extending and refining knowledge bases	goal driven knowledge discovery method;learning tasks;automobiles;human expert;maintenance;interactive multistrategy learning system;multistrategy learning;mtls;testing;trees mathematics;learning systems marine vehicles safety maintenance computer science knowledge acquisition humans knowledge representation automobiles testing;autonomic system;learning systems;learning by example;learning methods;marine vehicles;revised rules mtls knowledge base refinement interactive multistrategy learning system learning from examples knowledge discovery plausible justification trees learning tasks goal driven knowledge discovery method human expert knowledge base;knowledge acquisition;learning from examples;safety;knowledge base refinement;humans;computer science;interactive systems learning by example knowledge acquisition knowledge based systems trees mathematics;knowledge representation;revised rules;interactive systems;plausible justification trees;knowledge based systems;knowledge base;knowledge discovery;knowledge engineering	This paper presents an interactive multistrategy learning system (MTLS) that extends and refines knowledge bases by learning from input examples, discovering new knowledge, and cooperating with a user The use of the multistrategy learning approach based on plausible justification trees allows MTLS to perform learning tasks that are beyond the capability of a single strategy learning method. A goal-driven knowledge discovery method has been developed and integrated into MTLS to produce additional knowledge needed by the system. MTLS also allows a human expert to guide it to refine and extend the knowledge base. This cooperation between a human expert and the learner enables the system to perform tasks that are intrinsically difJicult for an autonomous system. The resulting knowledge base may include new rules discovered from data, as well as revised rules, and new facts learned by analogy. MTLS has been developed as a tool to be used by a domain expert to build a knowledge base to reduce the need of assistance from a knowledge engineer:	autonomous system (internet);knowledge base;knowledge engineer;multiplexed transport layer security;subject-matter expert	Ockkeun Lee;Gheorghe Tecuci	1997		10.1109/TAI.1997.632299	knowledge base;computer science;knowledge management;artificial intelligence;knowledge-based systems;machine learning;knowledge engineering;software testing	ML	-30.775204113271197	-6.4463782276522235	154766
4d48eed4d8b35e1b1d5374240e7f775dc6a3e1f7	heuristics for join processing using nonclustered indexes	buffer size;modelizacion;memoria tampon;base relacional dato;page accesses;optimisation;graphe biparti;optimizacion;heuristic method;juntura;interrogation base donnee;interrogacion base datos;performance comparison;nonclustered indexes;metodo heuristico;access paths;relational database;bipartite graph model;join;modelisation;jointure;query optimisation;tuples;indexation;relations;join processing;buffer size relational databases query optimisation join processing nonclustered indexes access paths bipartite graph model relations tuples page accesses database environment;base donnee relationnelle;grafico bipartido;optimization;relational databases;methode heuristique;relational databases cost function buffer storage computational efficiency indexes bipartite graph query processing computer science indexing;memoire tampon;bipartite graph;modeling;database query;database environment;buffer memory	Finding efficient procedures for implementing relational database operations, such as the join, is an important database problem. In this paper, we examine join processing when the access paths available are nonclustered indexes on the joining attribute(s) for both relations involved in the join. We use a bipartite graph model to represent the pages from the two relations which contain tuples that are to be joined. We are interested in minimizing the number of page accesses needed to compute a join in our database environment. We explore this problem from two perspectives. The first is to reduce the maximum buffer size so that no page is accessed more than once and the second is to reduce the number of page accesses for a fixed buffer size. We have developed heuristics for these problems and include performance comparisons of these heuristics and another method which recently appeared in the literature. The results show that one particular heuristic performs very well for addressing the problem from	heuristic (computer science);join (sql);relational database	Edward Omiecinski	1989	IEEE Trans. Software Eng.	10.1109/32.21722	hash join;relational database;computer science;theoretical computer science;data mining;database;sort-merge join	DB	-27.09235291268693	4.156376082056028	154786
d92392ac575be8c86d31b5873a77e4935cf39165	projectilesort - rule based parallel sorting algorithm - architecture for reconfigurable multi-partition object arrays		Data can be store as structured, semi-structured or unstructured formats in various distributed environments. Extraction of data from multiple data sources or data warehouse and convert to a proper order is quite time consuming task even using the latest hardware and software technologies. Sorting is one of the key concepts that helps to improve the efficiency of various computational process. Unlike the early single processor or single server operations using monolithic applications, multi-core distributed environments required more advanced computational theories and algorithms. Existing sorting theories are basically derived from linear algorithms and enhanced to support for distributed processing. This research discuss the characteristics of existing parallel sorting algorithms, techniques, limitation. Aim and objective is to introduce a new pure parallel sorting algorithm and sorting architecture that pointing to execute under latest distributed environments. The proposed pattern and the sorting architecture can be used as rule based parallel sorting technique and algorithm to support for any type of distributed environment to sort infinite dataset.	computation;distributed computing;multi-core processor;semi-structured data;semiconductor industry;server (computing);sorting algorithm;theory	Nandika Liyanage	2017	2017 18th International Conference on Parallel and Distributed Computing, Applications and Technologies (PDCAT)	10.1109/PDCAT.2017.00031	rule-based system;distributed computing;sort;software;distributed computing environment;computer science;distributed database;data warehouse;sorting algorithm;sorting	HPC	-33.03589456492407	-1.427592509137159	155177
e7f4fb1e002ff5299238051eeb19b5565dd37770	multi-agent modelling of decision support systems based on visual data mining			data mining;decision support system	Hamdi Ellouzi;Hela Ltifi;Mounir Ben Ayed	2017	Multiagent and Grid Systems	10.3233/MGS-170260	intelligent decision support system;machine learning;pattern recognition;data mining	ML	-30.37611367514178	-8.610711991156075	155446
8fbfbe79be17a7eb7c07c033d0e045ecb55cd171	resolving resource incompatibilities in intelligent agents		An intelligent agent may in general pursue multiple procedural goals simultaneously, which may lead to arise some conflicts (incompatibilities) among them. In this paper, we focus on the incompatibilities that emerge due to resources limitations. Thus, the contribution of this article is twofold. On one hand, we give an algorithm for identifying resource incompatibilities from a set of pursued goals and, on the other hand, we propose two ways for selecting those goals that will continue to be pursued: (i)the first is based on abstract argumentation theory, and (ii) the second based on two algorithms developed by us. We illustrate our proposal using examples throughout the article.	algorithm;intelligent agent;resultant;software incompatibility	M. Mariela Morveli-Espinoza;Ayslan Trevizan Possebom;Cesar Augusto Tacla	2017	2017 Brazilian Conference on Intelligent Systems (BRACIS)	10.1109/BRACIS.2017.28	management science;intelligent agent;argumentation theory;intelligent decision support system;computer science	Robotics	-21.328237896523525	-8.409329085175578	155484
4f4332914ee7600a199bb4ad7498c7bfb52f99d4	evaluating generalization in multiagent systems using agent-interaction graphs		Learning from interactions between agents is a key component for inference in multiagent systems. Depending on the downstream task, there could be multiple criteria for evaluating the generalization performance of learning. In this work, we propose a novel framework for evaluating generalization in multiagent systems based on agent-interaction graphs. An agent-interaction graph models agents as nodes and interactions as hyper-edges between participating agents. Using this abstract data structure, we define three notions of generalization for principled evaluation of learning in multiagent systems.	abstract data type;agent-based model;data structure;downstream (software development);interaction;multi-agent system	Aditya Grover;Maruan Al-Shedivat;Jayesh K. Gupta;Yuri Burda;Harrison A Edwards	2018			computer science;artificial intelligence;machine learning;data structure;multi-agent system;inference;graph	AI	-26.288220720450287	-9.407250202692397	155692
91a190298c9d8716e8779e4620cb15af52c7d46a	xq2p: efficient xquery p2p time series processing	p2p;large data sets;time series;financial analysis	In this demonstration, we propose a model for the management of XML time series (TS), using the new X Query 1.1 window operator. We argue that centralized computation is slow, and demonstrate XQ2P, our prototype of efficient XQuery P2P TS computation in the conte xt of financial analysis of large data sets (>1M values).	centralized computing;computation;prototype;time series;window operator;xml;xquery	Bogdan Butnaru;Benjamin Nguyen;Georges Gardarin;Laurent Yeh	2009	CoRR		financial analysis;computer science;time series;peer-to-peer;data mining;database;information retrieval	DB	-32.32846943801235	0.5157457242971872	155990
e32eb1896384c3f27e8ef794ff6baf168603a951	hierarchical model-based diagnosis for high autonomy systems	model based diagnosis;component model;hierarchical model	Deep reasoning diagnostic procedures are model-based, inferring single or multiple faults from the knowledge of faulty behavior of component models and their causal structure. The overall goal of this paper is to develop a hierarchical diagnostic system that exploit knowledge of structure and behavior. To do this, we use a hierarchical architecture including local and global diagnosers. Such a diagnostic system for high autonomy systems has been implemented and tested on several examples in the domain of robot-managed fluid-handling laboratory.	hierarchical database model	Sung-Do Chi;Bernard P. Zeigler	1994	Journal of Intelligent and Robotic Systems	10.1007/BF01276498	simulation;computer science;engineering;artificial intelligence;machine learning;component object model;hierarchical database model	Robotics	-21.20096504206766	-4.012779958940604	156016
f94c3c9b9867299e948e3aa744077de8418da4df	declarative knowledge representation in planning and scheduling	knowledge representation		automated planning and scheduling;declarative programming;knowledge representation and reasoning;scheduling (computing)	Jacek Gibert	1992			automated planning and scheduling;knowledge representation and reasoning;descriptive knowledge;natural language processing;scheduling (computing);machine learning;computer science;procedural knowledge;artificial intelligence	AI	-27.58815810286771	-7.7140156684584635	156376
226009bec4ed9ba098b4ab4bf68534607eb01018	svarne - an expert system based on tacit knowledge			expert system	Charlotte Magnusson;Jan Eric Larsson;Kirsten Rassmus-Gröhn	1997				AI	-30.477192330712334	-8.991743631699137	156560
42fb1b1eeae278d39fc6ec0abdf7044390b2094d	on the problems of representation and propagation of uncertainty in expert systems	regle inference;inference rule;modelisation;resolucion de problema;systeme expert;knowledge representation;representation connaissances;modeling;modelaje;problem solving;resolution probleme;information incertaine;expert system	This paper discusses the adaptation of basic knowledge representation constructs to the treatment of imprecise or uncertain information, the modelling of which uses distribution and scalar value respectively. Other representation issues such as the use of variables and the organization of rules in networks are briefly addressed. Then the problems of matching and propagation (in deductive inference and combination mechanisms) are considered in the specific setting of imprecision and uncertainty. Lastly, some questions concerning control strategies involved in such a problem are briefly considered.	expert system;knowledge representation and reasoning;optimal control;propagation of uncertainty;software propagation	Roger Martin-Clouaire;Henri Prade	1985	International Journal of Man-Machine Studies	10.1016/S0020-7373(85)80002-X	legal expert system;systems modeling;computer science;artificial intelligence;machine learning;expert system;algorithm;rule of inference	AI	-21.558252233532063	-2.233815801077075	156669
92dd4f0b0877a66946b7f322347573aa4c498a90	a cognition-inspired knowledge representation approach for knowledge-based interpretation systems	petroleum geology;reasoning;knowledge representation;ontology	We propose a hybrid approach for knowledge representation that combines classic representations (such as rules and ontologies) with cognitively plausible representations, such as prototypes and exemplars. The resulting framework can be used for developing knowledge-based systems that combine knowledge-driven and data-driven techniques. We also present how this approach can be used for developing an application for interpretation of depositional processes in Petroleum Geology.	cognition;database;knowledge representation and reasoning;knowledge-based systems;ontology (information science);sandro corsaro;top-down and bottom-up design	Joel Luis Carbonera;Mara Abel	2015		10.5220/0005467106440649	natural language processing;knowledge representation and reasoning;computer science;knowledge management;artificial intelligence;body of knowledge;knowledge-based systems;ontology;open knowledge base connectivity;petroleum geology;reason	AI	-28.412015886606163	-7.691223737464633	156694
ed6816acc0d8b90fae607c0359009f316f8f22d4	a novel bad data processing algorithm for analog data in substation automation systems	tratamiento datos;analyse erreur;systeme temps reel;power system simulation;analisis numerico;fiabilidad;reliability;conservative reasoning;matematicas aplicadas;mathematiques appliquees;analisis datos;03d55;rule based;calcul erreur;data processing;traitement donnee;65gxx;raisonnement;analyse numerique;algorithme;algorithm;analog data;error analysis;data analysis;numerical analysis;fiabilite;razonamiento;calculo error;analyse donnee;real time system;sistema tiempo real;electric power;reasoning;applied mathematics;modular approach;algoritmo;bad data processing	0096-3003/$ see front matter 2008 Elsevier Inc doi:10.1016/j.amc.2008.05.137 * Corresponding author. E-mail address: takeitez@ulsan.ac.kr (S.H. Hyun) In this paper, a Bad Data Processing (BDP) algorithm for analog data in a substation is suggested. Considering that every substation has a standardized hierarchy and the electric power flows in a fixed direction, rules for detection and correction of bad data are derived through logical analysis of error–symptoms relations. Conservative reasoning combined with modular approach is utilized, in this paper, to enhance the efficiency and reliability of data correction. The rule base used in this paper is constructed through logical analysis of possible errors and their symptoms that may occur. The suggested algorithm is proved to be useful in both convenience and accuracy by excessive application to a substation emulator realized in real-time digital power system simulator. 2008 Elsevier Inc. All rights reserved.	algorithm;bandwidth-delay product;emulator;iterative method;real-time transcription;rule-based system;simulation;traction substation	Seung Ho Hyun;Bogun Jin;Seung Jae Lee	2008	Applied Mathematics and Computation	10.1016/j.amc.2008.05.137	electric power;data processing;analog signal;numerical analysis;computer science;artificial intelligence;power system simulation;reliability;mathematics;data analysis;reason;algorithm;modular approach to software construction operation and test	Logic	-22.598205981775976	-4.250409037835768	156844
22ec26f71eeaac0c50667c0fd52588139fb4e862	batched searching in database organizations	database organization	Savings in the number of page accesses due to batching on sequential, tree-structured, and random files are well known and have been reported in the literature. This paper asserts that substantial savings can also be obtained in database organizations by batching the requests for records (in queries), and also by batching intermediate processing requests while traversing the database. A simple database having two interrelated files is used to demonstrate such savings. For the simple database, three variations on batching are reported and compared with the case of unbatched requests. New mathematical expressions have been developed for the batched cases as well as for the unbatched case, and the savings are demonstrated with some example problems. As an extension, larger databases will enjoy even greater savings due to batching. The paper also discusses several strategies for applying the batching approach to current databases, and the advantages of emerging very large main memories for the batching approach.	auxiliary memory;database;tree (data structure)	Prashant Palvia	1988	Inf. Sci.	10.1016/0020-0255(88)90006-0	computer science;data mining;database	DB	-28.75930726464604	3.978708146058768	157233
ec36910a858b9d29e736462422e5a0b0bca3f556	toward a pragmatic taxonomy of knowledge maps: classification principles, sample typologies, and application examples	knowledge management;data visualisation;knowledge representation data visualisation knowledge management;knowledge map classification;pragmatic taxonomy;knowledge representation;knowledge management pragmatic taxonomy knowledge map classification;taxonomy knowledge management context problem solving heart	This paper discusses pragmatic ways of classifying knowledge maps to give an overview of their application formats and contexts. We show where and how the term knowledge map has been previously used and what criteria must be met in a sound and useful knowledge map classification. Various possible classification principles are presented. A table matches map formats to purposes and contents to serve as a selection framework in knowledge management processes. Examples of some of the main types of knowledge maps are presented to illustrate the varieties of knowledge mapping present in the classifications. The article concludes by discussing its limitations and future research questions	autonomy;cognitive map;data validation;display resolution;knowledge management;open research;requirement;taxonomy (general)	Martin J. Eppler	2006	Tenth International Conference on Information Visualisation (IV'06)	10.1109/IV.2006.111	computer science;knowledge management;data science;body of knowledge;mathematical knowledge management;knowledge-based systems;knowledge engineering;data mining;knowledge extraction;personal knowledge management;domain knowledge	ML	-32.61900648251631	-6.300645484920868	157367
44447e0a2b13de4588f4f5892ca1872330e6d119	multidimensional data structures and techniques for efficient decision making	range minimum query;multidimensional data structures;range aggregation query;multidimensional data structure;risk management;computational geometry;range selection query;decision making process;weighted median;data structure	In this paper we present several novel efficient techniques and multidimensional data structures which can improve the decision making process in many domains. We consider online range aggregation, range selection and range weighted median queries; for most of them, the presented data structures and techniques can provide answers in polylogarithmic time. The presented results have applications in many business and economic scenarios, some of which are described in detail in the paper. Key-Words: Decision making, Multidimensional data structures, Risk management, Range aggregation query, Range selection query, Range minimum query, Weighted median.	aggregate data;data compression;data cube;data structure;database;mathematical optimization;olap cube;online analytical processing;polylogarithmic function;range minimum query;range query (database);risk management;scheduling (computing);time complexity	Madalina Ecaterina Andreica;Mugurel Ionut Andreica;Nicolae Cataniciu	2009	CoRR		decision-making;data structure;risk management;computational geometry;computer science;machine learning;data mining;database;mathematics;geometry;weighted median	Theory	-27.953818274331134	1.3963690594523732	157390
7a6c1e4a872ecc1f690e678b83d19dffd61b4895	a systematic approach for design and planning of mechanical assemblies	ensamblaje mecanico;assemblage mecanique;conception;intelligence artificielle;planificacion;diseno;artificial intelligence;design;planning;inteligencia artificial;planification;reseau neuronal;red neuronal;mechanical joint;neural network			C. L. Philip Chen;C. A. Wichman	1993	AI EDAM	10.1017/S0890060400000044	mechanical joint;planning;design;computer science;engineering;artificial intelligence;management;operations research;artificial neural network	AI	-24.234635918700956	-4.83611895888556	157457
fdf23f3e81dd3ecef1318b89f02f843cec281655	an approach to the development of commonsense knowledge modeling systems for disaster management	disaster management;expert systems;formal model;information gathering;fuzzy logic;principal component analysis;ayurvedic medicine;article;article abstract;knowledge modeling;principal component;expert system	Knowledge is the fundamental resource that allows us to function intelligently. Similarly, organizations typically use different types of knowledge to enhance their performance. Commonsense knowledge that is not well formalized modeling is the key to disaster management in the process of information gathering into a formalized way. Modeling commonsense knowledge is crucial for classifying and presenting of unstructured knowledge. This paper suggests an approach to achieving this objective, by proposing a three-phase knowledge modeling approach. At the initial stage commonsense knowledge is converted into a questionnaire. Removing dependencies among the questions are modeled using principal component analysis. Classification of the knowledge is processed through fuzzy logic module, which is constructed on the basis of principal components. Further explanations for classified knowledge are derived by expert system technology. We have implemented the system using FLEX expert system shell, SPSS, XML, and VB. This paper describes one such approach using classification of human constituents in Ayurvedic medicine. Evaluation of the system has shown 77% accuracy.	architecture domain;artificial intelligence;commonsense knowledge (artificial intelligence);expert system;fuzzy logic;inference engine;knowledge management;knowledge modeling;principal component analysis;spss;visual basic;web application;xml	D. S. Kalana Mendis;Asoka S. Karunananda;Udaya Samaratunga;Uditha Ratnayake	2007	Artificial Intelligence Review	10.1007/s10462-009-9097-6	legal expert system;knowledge base;computer science;knowledge management;artificial intelligence;body of knowledge;knowledge-based systems;machine learning;knowledge engineering;data mining;procedural knowledge;knowledge extraction;commonsense knowledge;expert system;domain knowledge;principal component analysis	AI	-31.584047992193288	-6.389431321424797	157665
9826edf27e74c45d489ea656da6eb9145b336e37	principles and practice in verifying rule-based systems	rule based system		rule-based system;verification and validation	Alun D. Preece;Rajjan Shinghal;Aïda Batarekh	1992	Knowledge Eng. Review	10.1017/S026988890000624X	computer science;artificial intelligence	SE	-28.015616013474155	-7.407078745617817	157705
eaba9ce04fa06d54068c238c0ffb38294b2e7729	an intelligent system for prioritisation of organ transplant patient waiting lists using fuzzy logic	clavette;business and management;forecasting;tiempo espera;logica booleana;characteristic;reliability;key seat;systeme intelligent;project management;hospital;information systems;articulo sintesis;aplicacion medical;maintenance;systeme aide decision;article synthese;prioritisation;flexibilidad;medicina;soft or;sistema inteligente;information technology;decision borrosa;logique floue;packing;logica difusa;decision floue;sistema ayuda decision;prise decision;caracteristica;operations research;location;organ transplants;investment;journal;journal of the operational research society;medecine;temps attente;inventory;waiting list;fuzzy logic;purchasing;hopital;decision support system;history of or;modelo logico;logistics;transplantation organe;marketing;scheduling;waiting time;chaveta;chirurgie;intelligent system;surgery;caracteristique;production;communications technology;logique booleenne;number;medicine;cirugia;flexibilite;medical application;sistema difuso;organ transplantation;computer science;systeme flou;logic model;operational research;nombre;toma decision;review;waiting lists;boolean logic;fuzzy system;applications of operational research;numero;or society;flexibility;fuzzy decision;jors;management science;infrastructure;modele logique;application medicale	The objective of this paper is to investigate the effectiveness of using fuzzy logic in a complex decision-making capacity, and in particular, for the prioritisation of kidney transplant recipients. Fuzzy logic is an extension to Boolean logic allowing an element to have degrees of true and false as opposed to being either 100% true or 100% false. Thus, it can account for the ‘shades of grey’ found in many real-world situations. In this paper, two fuzzy logic models are developed demonstrating its effectiveness as a model for vastly improving the current prioritisation system used in the UK and abroad. The first model converts an element of the current kidney transplant prioritisation system used in the UK into fuzzy logic. The result is an improvement to the current system and a demonstration of fuzzy logic as an effective decision-making approach. The second model offers an alternative prioritisation system to overcome the limitations of the current system both in the UK and abroad, as brought up by research reviewed in this paper. The current UK transplant prioritisation system, adapted in the first model, uses objective criteria (age of recipient, waiting time, etc) as inputs into the decision-making process. This alternative model takes advantage of the facility for infinitely varying inputs into fuzzy logic and a system is developed that can handle subjective (humanistic) criteria (pain level, quality of life, etc) that are key to arriving at such important decisions. Furthermore, the model is highly flexible allowing any number of criteria to be used and the individual characteristics of each criterion to be altered. The result is a model that utilises the scope of fuzzy logic’s flexibility, usability and effectiveness in the field of decision-making and a transplant prioritisation method vastly superior to the original system, which is constrained by its use of only objective criteria. The ‘humanistic’ model demonstrates the ability of fuzzy logic to consider subjective and complex criteria. However, the criteria used are not intended to be exhaustive. It is simply a template to which medical professionals can apply limitless additional criteria. The model is produced as an alternative to any current national system. However, the model can also be used by individual hospitals to decide initially whether a patient should be placed on the transplant or surgery waiting list. The model can be further adapted and used for the transplant of other organs or similar decisions in medicine. Concurrently with the research and work carried out to develop the two models the investigation focused on the constraints of the current systems used in the UK and the US and the seemingly impossible dilemmas experienced by those having to make the prioritisation decisions. By removing the parameters of objective-only inputs the ‘humanistic’ model eradicates the previous limitations on decision-making. Journal of the Operational Research Society (2004) 55, 103–115. doi:10.1057/palgrave.jors.2601552	artificial intelligence;boolean algebra;fuzzy logic;logical connective;usability	T. Perris;A. W. Labib	2004	JORS	10.1057/palgrave.jors.2601552	fuzzy logic;project management;logistics;inventory;decision support system;economics;forecasting;investment;computer science;artificial intelligence;marketing;operations management;reliability;mathematics;characteristic;location;management;operations research;information technology;scheduling;organ transplantation	AI	-25.445501190365636	-5.576409480307885	157768
3be7a45e3a1b2745c85d0370185adc5bff73537f	moving geo databases to smart phones - an approach for offline location-based applications		Mobile location-based services usually access large geo databases via a wireless network. Sometimes it is useful to store a certain amount of geo objects on the mobile device. In this paper we present an approach to store geo data in an embedded SQL database on smart phones. Besides the storage of geometries in SQL tables we have to provide an efficient geometric indexing mechanism. We used the Extended Split Index that successfully was used for large server geo databases. Due to the special characteristics of embedded smart phone databases we have to introduce an extension, the In-Memory Index. Our paper concludes with performance considerations.	database;embedded sql;embedded system;location-based service;mobile app;mobile device;naivety;online and offline;run time (program lifecycle phase);server (computing);smartphone	Jörg Roth	2011			world wide web;internet privacy;computer science	DB	-26.338972403128093	-0.0606985605161544	157812
e6c996a48dda978c598a7639741eebf6068cfb28	a big data platform integrating compressed linear algebra with columnar databases	linear algebra;data compression;arrays;big data;distributed databases;encoding	Key foundational components of Big Data frameworks include efficient large-scale storage and high-performance linear algebra. This paper discusses efficient implementations that utilize compression techniques inspired by columnar relational databases for improving space and time profiles for vector and matrix operations. In addition, linear algebra operations are integrated with columnar relational algebra operations both in dense and compressed forms. For several of the operations substantial speedups are obtained by operating directly on the compressed relations, vectors and matrices. Advantages of mixing and matching relational and linear algebra operations are also pointed out. Both serial and parallel implementations are provided in the ScalaTion Big Data Analytics Framework.	apache hbase;apache hadoop;big data;code;column-oriented dbms;hoc (programming language);in-memory database;lapack;linear algebra;mixing (mathematics);open-source software;relational algebra;relational database;run-length encoding;serialization;sparse matrix;speedup	Vishnu Gowda Harish;Vinay Kumar Bingi;John A. Miller	2016	2016 IEEE International Conference on Big Data (Big Data)	10.1109/BigData.2016.7840858	computer science;theoretical computer science;data mining;database;numerical linear algebra	DB	-31.584188264670086	1.2635015715791913	157963
79e5b127901623acba5608e98c26e7562265e12c	generate, test and debug: combining associational rules and causal models	left hand side;right hand side;testing and debugging;association rule;control flow;is success;causal models;domain specificity;route planning;problem solving	We present a problem solving paradigm called generate, test and debug (GTD) that combines associational rules and causal models, producing a system with both the efficiency of rules and the breadth of problem solving power of causal models. The generator uses associational rules to generate plausible hypotheses; the tester uses causal models to test the hypotheses and produce a detailed characterization of the discrepancy in case of failure. The debugger uses the ability to reason about the causal models, along with a body of domain-independent debugging knowledge, to determine how to repair the buggy hypotheses. The GTD paradigm has been implemented and tested in three different domains; we report in detail on its application to our principal domain of geologic interpretation. We also explore in some depth the character of the problems for which GTD is well suited and consider the character of the knowledge required for successful use of the paradigm.	causal filter;debug;debugger;discrepancy function;problem solving;programming paradigm;uniform theory of diffraction;usability testing	Reid G. Simmons;Randall Davis	1987			association rule learning;computer science;artificial intelligence;machine learning;mathematics;programming language;control flow;algorithm;statistics;causal model	AI	-23.545076902780508	0.15338382436582423	158108
d91cf6f59add8402a0a0177b2446fd4c89fa5ca9	prototypical knowledge for expert systems: a retrospective analysis	representacion conocimientos;sistema experto;architecture systeme;base connaissance;intelligence artificielle;raisonnement;razonamiento;artificial intelligence;base conocimiento;arquitectura sistema;inteligencia artificial;systeme expert;reasoning;knowledge representation;system architecture;representation connaissances;knowledge base;expert system	"""The article, """"Prototypical knowledge for expert systems"""" [1 ], published in 1983, was a summarization of my dissertation research at Stanford University in the late 1970s. This was the era of the first rule-based expert systems--MYCIN, Meta-DENDRAL, PUFF, and others from Stanford, and the early frame systems from MIT, such as PIP and NUDGE. I remember being intrigued by the simplicity of the rule-based architectures and the ease with which one could express logical heuristics that did indeed seem to be part of what an expert knew about his area of expertise. I also remember the frustration that was felt by experts and application developers alike over a lack of control in the dialog between application end users and the expert consultation system. Backward-chaining of rules was certainly efficient, but unpredictable, and resulted in dialogs that were not consistent with the way a human expert would have acquired information to solve the problem. I realized that there was not only domain-specific knowledge that was necessary to infer new information for applications, but also control knowledge that was specific to each application, and that would direct the acquisition and application of domain knowledge. Many of the frame systems represented control knowledge in the frame structure itself, directing the flow of the consultation for each context. The knowledge representation that I devised took the best ideas from both sets of research and created what we would call today one of the first """"hybrid"""" architectures."""	backward chaining;dendral;domain-specific language;expert system;heuristic (computer science);knowledge representation and reasoning;logic programming;mycin;nudge (instant messaging);dialog;pip	Janice S. Aikins	1993	Artif. Intell.	10.1016/0004-3702(93)90187-G	knowledge representation and reasoning;knowledge base;computer science;artificial intelligence;operations research;expert system;reason	AI	-25.936790371370122	-5.874279292190146	158639
16e4250424c8c58b4edb4704b468e2ee54376993	column-oriented datalog materialization for large knowledge graphs (extended technical report)		The evaluation of Datalog rules over large Knowledge Graphs (KGs) is essential for many applications. In this paper, we present a new method of materializing Datalog inferences, which combines a column-based memory layout with novel optimization methods that avoid redundant inferences at runtime. The pro-active caching of certain subqueries further increases efficiency. Our empirical evaluation shows that this approach can often match or even surpass the performance of state-of-the-art systems, especially under restricted resources.	cache (computing);datalog;mathematical optimization;run time (program lifecycle phase);sql	Jacopo Urbani;Ceriel J. H. Jacobs;Markus Krötzsch	2016			computer science;database;programming language;algorithm	AI	-31.006720326279495	3.5281832718921975	158779
3f8d104ebd8b63b761db1dc49747fbbb40681b70	archiving scientific data	computacion informatica;keys for xml;edit distance;scientific data;index structure;hierarchical data;ciencias basicas y experimentales;external memory;grupo a;data structure	Archiving is important for scientific data, where it is necessary to record all past versions of a database in order to verify findings based upon a specific version. Much scientific data is held in a hierachical format and has a key structure that provides a canonical identification for each element of the hierarchy. In this article, we exploit these properties to develop an archiving technique that is both efficient in its use of space and preserves the continuity of elements through versions of the database, something that is not provided by traditional minimum-edit-distance diff approaches. The approach also uses timestamps. All versions of the data are merged into one hierarchy where an element appearing in multiple versions is stored only once along with a timestamp. By identifying the semantic continuity of elements and merging them into one data structure, our technique is capable of providing meaningful change descriptions, the archive allows us to easily answer certain temporal queries such as retrieval of any specific version from the archive and finding the history of an element. This is in contrast with approaches that store a sequence of deltas where such operations may require undoing a large number of changes or significant reasoning with the deltas. A suite of experiments also demonstrates that our archive does not incur any significant space overhead when contrasted with diff approaches. Another useful property of our approach is that we use XML format to represent hierarchical data and the resulting archive is also in XML. Hence, XML tools can be directly applied on our archive. In particular, we apply an XML compressor on our archive, and our experiments show that our compressed archive outperforms compressed diff-based repositories in space efficiency. We also show how we can extend our archiving tool to an external memory archiver for higher scalability and describe various index structures that can further improve the efficiency of some temporal queries on our archive.	addendum;application checkpointing;archive;business continuity;data structure;delta encoding;diff utility;digital library;edit distance;experiment;hierarchical database model;online mendelian inheritance in man;overhead (computing);regular language description for xml;requirement;swiss-model;scalability;scott continuity;switzerland;synthetic data;trusted timestamping;xml namespace	Peter Buneman;Sanjeev Khanna;Keishi Tajima;Wang Chiew Tan	2004	ACM Trans. Database Syst.	10.1145/974750.974752	edit distance;data structure;computer science;data mining;database;programming language;world wide web;hierarchical database model;data	DB	-29.485818556765544	3.7232284716203625	158872
f79c44fb0837eef7e63dfa0fc87deb19fadfc2d7	data mining for the e-business: developments and directions	tratamiento datos;extraction information;gestion informacion;entreprise;procesamiento informacion;extraction connaissance;e business;analisis datos;information extraction;fournisseur;information technology;extraction forme;empresa;comercializacion;knowledge extraction;data processing;traitement donnee;supplier;technologie information;data mining;commercialisation;data analysis;inteligencia;extraccion forma;fouille donnee;marketing;information management;firm;information processing;analyse donnee;intelligence;gestion information;traitement information;tecnologia informacion;pattern extraction;proveedor;extraction informacion	This paper describes data mining and e-business and then shows how data mining may be applied to e-business to gather consumer/supplier intelligence so that targeted marketing and merchandising may be carried out.© (2000) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.	data mining;electronic business	Alfred Grasso;Harry Sleeper;Bhavani M. Thuraisingham;Yike Guo	2000		10.1117/12.381755	engineering;marketing;data mining;operations research	ML	-26.959206871733457	-4.377170732587294	159077
e8f7adacc31866f9ee2e4ff0b33e55ee8566d9b8	dynamically scripted artificial intelligence	extensibility;artificial intelligent;artificial intelligence	In this paper, we describe techniques used in developing an extensible Artificial Intelligence system.	artificial intelligence system	Nathan Dawson;Hunter Hale	2005		10.1145/1167350.1167377	natural language processing;artificial architecture;computer science;artificial intelligence;machine learning;artificial intelligence system;procedural reasoning system;artificial intelligence, situated approach	AI	-27.393058164153427	-8.482480084270257	159342
0276030ee24b79d037c56f64aa92b7dd0a16932c	the generation and evolution of adaptation rules in requirements driven self-adaptive systems	reinforcement learning;case based reasoning;adaptation plan;requirement driven self adaptation	One of the challenges in self-adaptive software systems is to make adaptation plans in response to possible changes. A good plan mechanism shall have the capability of: 1) selecting the most appropriate adaptation actions in response to changes both in the environment and requirements, 2) making adaptation decisions efficiently to react timely to arising situations at run-time. In existing approaches for plan process, rule-based adaptation provides an efficient offline planning method. However, it can react neither to changeable requirements nor to unexpected environment changes. On the contrary, goal-based and utility-based approaches provide online planning mechanisms, which can well handle a highly uncertain environment with dynamically changing requirements and environment. However, online adaptation decision making is often computationally expensive and may encounter less-efficiency problems. The aim of our research is to improve the planning processin requirements driven self-adaptive systems, i.e., enabling the self-adaptive system to efficiently make adaptation plans to cope with the dynamic environment and changeable requirements. To achieve such advantages, we propose a solution to enhance the traditional rule-based adaptation with a rule generation and a rule evolution process, so that the proposed approach can maintain the advantages of efficient planning process while being enhanced with the capability of dealing with runtime uncertainty.	adaptive system;analysis of algorithms;logic programming;online and offline;requirement;run time (program lifecycle phase);software system	Tianqi Zhao	2016	2016 IEEE 24th International Requirements Engineering Conference (RE)	10.1109/RE.2016.18	case-based reasoning;computer science;systems engineering;engineering;knowledge management;operations management;adaptive system;reinforcement learning	SE	-19.56154735682059	-6.8532347474696005	159427
a8be22280669883d3075dd54ca7aca48d21ba775	integrating shallow and deep knowledge in the design of an on-line process monitoring system	operateur humain;representacion conocimientos;operador humano;concepcion sistema;real time;base connaissance;intelligence artificielle;process monitoring;monitoring;system design;temps reel;human operator;tiempo real;artificial intelligence;base conocimiento;inteligencia artificial;monitorage;knowledge representation;monitoreo;representation connaissances;conception systeme;knowledge base	Monitoring and malfunction diagnosis of complex industrial plants involves, in addition to shallow empirical knowledge, knowledge about plant operation, also deep knowledge about structure and function. This paper presents the results obtained in the design and experimentation of PROP and PROP-2 systems, devoted on-line monitoring, and diagnosis of pollution phenomena in the cycle water of a thermal power plant. In particular, it focuses on PROP-2 architecture, which encompasses a four-level hierarchical knowledge-base including both empirical knowledge and a deep model of the plant. Shallow knowledge is represented by production rules and event-graphs (a formalism for expressing procedural knowledge), while deep knowledge is expressed using a representation language based on the concept of component. One major contribution of the proposed approach has been to show in a running experimental system that a suitable blend of shallow and deep knowledge can offer substantial advantages over a single paradigm.	experiment;experimental system;online and offline;production (computer science);programming paradigm;semantics (computer science)	Massimo Gallanti;Luca Gilardoni;Giovanni Guida;Alberto Stefanini;Lorenzo Tomada	1987	International Journal of Man-Machine Studies	10.1016/S0020-7373(87)80022-6	knowledge base;computer science;artificial intelligence;knowledge-based systems;operations research;systems design	AI	-23.43965533349125	-4.39269477465543	159575
42dc69492eeca1299f8147504d9d4868a055f997	towards reasoning about the past in neural-symbolic systems	artificial intelligent;reactive system;logic programs;knowledge representation	Reasoning about the past is of fundamental importance in several applications in computer science and artificial intelligence, including reactive systems and planning. In this paper we propose efficient temporal knowledge representation algorithms to reason about and implement past time logical operators in neural-symbolic systems. We do so by extending models of the Connectionist Inductive Learning and Logic Programming System with past operators. This contributes towards integrated learning and reasoning systems considering temporal aspects. We validate the effectiveness of our approach by means of case studies.	algorithm;artificial intelligence;automated planning and scheduling;computer science;connectionism;knowledge representation and reasoning;logic programming;logical connective;stepping level;temporal logic	Rafael V. Borges;Luís C. Lamb;Artur S. d'Avila Garcez	2007			knowledge representation and reasoning;opportunistic reasoning;reactive system;computer science;artificial intelligence;model-based reasoning;machine learning;reasoning system;automated reasoning;algorithm	AI	-21.079735602681428	1.2033078870914133	159606
fcf6a1ac430180f3529d11e62c303cfd637f07d5	using client-side access partitioning for data clustering in big data applications	pattern clustering;information technology;data security;storage management;load balance;client-side access partitioning;data requests;client-side partitioning;data storage;big data management systems;data clustering;very large databases;big data;performance	Big data has been playing a critical role in modern information technology. In some big data management systems, to simplify client design, data requests from clients are even distributed to all nodes and then are routed to the final correct data storage inside the data cluster. After performing analysis on this working mechanism, we point out some design problems and advocate the client-side access partitioning, i.e., clients know precisely which node of the cluster should be accessed for sought information. This approach could provide a fast access. We also implement a first-stage application based on client-side access partitioning for evaluation purpose and the result demonstrates our approach is effective.	big data;client-side;cluster analysis;computer data storage;data system;information retrieval;nosql;routing	Dapeng Liu;Shaochun Xu;Zengdi Cui	2014	15th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)	10.1109/SNPD.2014.6888697	data stream clustering;data science;data mining;database;cluster analysis;clustering high-dimensional data	DB	-31.815560612915185	-0.8524447531226377	159732
45c2a3f966073ac31216069436009c5648c4b317	the effect of data granularity on load data compression		A vast volume of data is generated through smart metering. Suitable compression mechanisms for this kind of data are highly desirable to better utilize low-bandwidth links and to save costs and energy. To date, the important factor of data resolution has been neglected in the compression of smart meter data. In this paper, we review and evaluate compression methods for smart metering in the context of different resolutions. We show that state-of-the-art compression methods are well suited for high resolution, but not for low resolution data. Furthermore, we elaborate on the compression performance differences between appliance-level and household-level load data. We conclude that the latter are practically incompressible at most resolutions.	data compression	Andreas Unterweger;Dominik Engel;Martin Ringwelski	2015		10.1007/978-3-319-25876-8_7	embedded system;computer hardware;computer science;theoretical computer science	Arch	-31.735502954087686	-3.072123887882584	160096
23247311838c7ce6bf1af1a776d6839e0c6796b6	an experimental comparison of knowledge representation schemes	knowledge representation	Many techniques for representing knowledge have been proposed, but there have been few reports that compare their application This article presents an experimental comparison of four knowledge representation schemes: a simple production system, a structured production system, a frame system, and a logic system. We built four pilot expert systems to solve the same problem: risk management of a large construction project Observations are made about how the structure of the domain knowledge affects the implementation of expert systems and their run time efficiency WIT: THINK THAT IT IS NECESSARY to clarify the advantages and disadvantages of knowledge representation techniques from the expert system designer’s point of view. Barr and Feigenbaum (1981) point out that “many researchers feel that the representation of knowledge is the key issue at this point in the development of AI.” Stefik et al (1982) point out some domain characteristics that affect expert system design: large search spaces, a need for tentative reasoning, time varying and noisy data. These four characteristics are elaborated into eleven case studies, and some guidelines for expert systems construcThe authors would like to express their appreciation to Dr Edward A. Feigenbaum and H Penny Nii of Stanford University for discussing the early results of this research during their visit to the authors’ laboratory The authors also would like to acknowledge the support of Dr Jun Kawasaki, the general manager of Systems Development Laboratory, Hitachi, Ltd tion are provided. This information helps an expert system designer clarify the domain’s characteristics and develop a conceptual system design. However, little information is provided for selecting adequate techniques after the system’s function (input/output) is determined. Ennis (1982) reports her experiences with building an expert system using several tools such as EXPERT, UNITS, EMYCIN, and OPS-5. The system was designed to interpret the X-ray powder diffraction spectra of rocks to determine their constituent minerals. This article focuses on expert system building tools; however, there may be many cases where no such tools are available. This article’s purpose is to present an experimental comparison of four pilot expert systems whose knowledge representations are a simple production system, a structured production system, a frame system, and a logic system. The domain of the four systems is the risk management of large construction projects. Each system’s function (inputs/outputs) is exactly the same: to assist a project manager effectively in controlling risks as they arise during project execution. We will compare the difficulties of implementing the knowledge base and the inference engine, as well as run time efficiency. The systems were implemented on a VAX II/780 using Franz-LISP. THE AI MAGAZINE Summer 1984 29	conceptual system;expert system;feigenbaum constants;frame language;franz lisp;inference engine;input/output;knowledge base;knowledge representation and reasoning;production system (computer science);risk management;run time (program lifecycle phase);signal-to-noise ratio;systems design;vax	Kiyoshi Niwa;Koji Sasaki;Hirokazu Ihara	1984	AI Magazine		knowledge representation and reasoning;knowledge base;computer science;knowledge management;knowledge-based systems;data mining;management science	AI	-30.479060829074058	-5.923115646767006	160125
486a723240b882cfcb1cd9530d9064f5f4b5a9a4	proposal of spatio-temporal indexing methods for moving objects	moving object;indexing method	This paper deals with spatio-temporal indexing methods for moving objects. Spatio-temporal indexing method gives a basis for data management structure for efficient search for large scale spatio-temporal data. In this paper, two types of indexing methods are proposed for managing spatio-temporal data for moving objects. Both methods are evaluated by computer simulation, and the merits and demerits of both methods are discussed.		Shogo Nishida;Hiroshi Nozawa;Naoki Saiwaki	1998			search engine indexing;data structure;computer science;data mining;database;geographic information system;programming language	DB	-27.121889909141068	0.7470644580972906	160373
63642f54324f96269c2470a51666ac71ec4e264e	gestion de l'évolution dans les bases de connaissances : une approche par les règles. (evolution management in knowledge bases : an approach using rules)		The work presented in this thesis concerns schema and object evolution in knowledge based systems. It is realized in the framework of the knowledge representation system Shood. We try to solve extensibility and reuse problems for evolution mechanisms. We propose an evolution system which allows the definition and the management of the dynamics. This management is realized by a set of mechanisms, such as instance classification, active rules, and a set of modification operations called the evolution support. We propose also a mechanism based on evolution rules and strategies to allow the declarative expression of constraints for structures and data evolution. The designer can define strategies to organize these constraints. According to his needs, he can define multiple strategies, but only one will be active at a given moment. The mechanism implementing evolution rules and strategies is developped mainly by mapping between an evolution rule and several active rules, and between a strategy and a rule base.		Fethi Bounaas	1995				DB	-24.018830756697447	-1.389259250839935	160471
2757d4fb6a20507a3cb04f0429edeaf665df7f7a	apache vxquery: a scalable xquery implementation		The wide use of XML for document management and data exchange has created the need to query large repositories of XML data. To efficiently query such large data collections and take advantage of parallelism, we have implemented Apache VXQuery, an open-source scalable XQuery processor. The system builds upon two other open-source frameworks – Hyracks, a parallel execution engine, and Algebricks, a language agnostic compiler toolbox. Apache VXQuery extends these two frameworks and provides an implementation of the XQuery specifics (data model, data-model dependent functions and optimizations, and a parser). We describe the architecture of Apache VXQuery, its integration with Hyracks and Algebricks, and the XQuery optimization rules applied to the query plan to improve path expression efficiency and to enable query parallelism. An experimental evaluation using a real 500GB dataset with various selection, aggregation and join XML queries shows that Apache VXQuery performs well both in terms of scaleup and speed-up. Our experiments show that it is about 3x faster than Saxon (an open-source and commercial XQuery processor) on a 4-core, single node implementation, and around 2.5x faster than Apache MRQL (a MapReduce-based parallel query processor) on an eight (4-core) node cluster.	apache stanbol;compiler;data model;experiment;language-independent specification;mapreduce;mathematical optimization;open-source software;parallel computing;path expression;query plan;scalability;xml;xquery	E. Preston Carman;Till Westmann;Vinayak R. Borkar;Michael J. Carey;Vassilis J. Tsotras	2015	CoRR		computer science;data mining;database;programming language;world wide web	DB	-32.27596413158013	2.0129080024220576	160507
d4efbaaec4bf62dca70f9d5c344c7e24420b8672	overcoming the knowledge acquisition bottleneck in map generalization: the role of interactive systems and computational intelligence	limiting factor;computational intelligence;rule based;explicit knowledge;interactive system;knowledge acquisition;knowledge base;expert system	Past research in cartographic generalization has shown that algorithmic methods are well suited to handle narrow tasks, but appear to have limited potential so solve the entire generalization process comprehensively. Attempts to use systems based on explicit knowledge representation (e.g., rule-based or expert systems) also had relatively little success. The major limiting factor to explicit knowledge systems in generalization is the scarcity of formalized knowledge available. That is, knowledge acquisition (KA) forms the major bottleneck to progress of knowledge-based techniques.	cartographic generalization;computational intelligence;knowledge acquisition	Robert Weibel;Stefan F. Keller;Tumasch Reichenbacher	1995		10.1007/3-540-60392-1_10	natural language processing;legal expert system;knowledge base;limiting factor;computer science;knowledge management;artificial intelligence;explicit knowledge;knowledge-based systems;machine learning;computational intelligence;open knowledge base connectivity;procedural knowledge;commonsense knowledge;expert system;domain knowledge	AI	-27.934322293220895	-8.978171038020758	160615
f2e71d84f9de15dbfd71836f3609bb97defceb2c	an implementation of agent-based ontology alignment		Various knowledge-based information systems contain distinct knowledge representations reflecting different domains of interest and different viewpoints across domains of discourse. For efficient use of knowledge-based systems it is necessary to know semantic relations or alignment between different knowledge representations. One of the promising approaches is the use of intelligent software agents where agents communicate in order to align respective knowledge representations. The paper presents an approach for ontology alignment based on implementation of meaning negotiation between intelligent agents. In the approach, negotiation leads in iterative way. On each step agents compare ontological contexts and use propositional substitutions in order to reduce semantic distance between the contexts. The focus of the paper is the implementation of agents’ negotiation strategy.	align (company);information system;intelligent agent;iteration;knowledge-based systems;ontology alignment;propositional calculus;software agent	Maxim Davidovsky;Vadim Ermolayev;Vyacheslav Tolok	2012			semantic similarity;knowledge management;ontology;process ontology;intelligent agent;ontology-based data integration;ontology alignment;suggested upper merged ontology;software agent;computer science	AI	-22.069710747565058	-9.354833190269046	161135
0d942eb0e7329e6be40cc7f1835ef24acb744274	enhancing database management to knowledge base management: the role of information retrieval technology	base relacional dato;systeme intelligent;research needs;information systems;information non structuree;systeme documentaire;nonstructured information;almacenamiento informacion;concepcion sistema;analog;database management systems;information retrieval;implementation;sistema inteligente;interrogation base donnee;interrogacion base datos;base connaissance;pregunta documental;intelligence artificielle;relational database;database management;analogo;documentation data processing;question documentaire;ejecucion;modelo;information storage;analogue;recherche documentaire;object oriented;system design;traitement document;sistema recuperacion documental;document retrieval system;recuperacion documental;intelligent system;base donnee relationnelle;query;oriente objet;artificial intelligence;base conocimiento;stockage information;modele;document retrieval;document processing;inteligencia artificial;systeme gestion base donnee;information system;sistema gestion base datos;orientado objeto;database management system;models;database query;conception systeme;systeme information;tratamiento documento;sistema informacion;knowledge base;informacion documental;informatique documentaire	Abstract   In recent years, there has been an enhancement from database management to knowledge base management. In this article, we investigate an important, yet not widely addressed issue: the role of information retrieval in this enhancement. Central to our investigation is to develop techniques of building information systems which are able to extract and store information from texts so that the stored information can be combined to satisfy unanticipated queries. Success of such techniques will have important practical impact on the automatic construction of knowledge bases from documents, thus reducing the knowledge acquisition bottleneck. We argue that this is not a task which can simply be reduced to database management through logic-based approaches (such as deductive databases), and explore an approach to achieve this by extending classical information retrieval techniques. An experiment has been carried out to implement a conceptual model for storage and retrieval of integrating short scientific documents (written in restricted English) using unstructured relational databases. This article does not address the issues dealing with general information retrieval; instead, by restricting our focus on the problem investigated, we have gained some insight on the relationship between information retrieval and database management, as well as the relationship between information retrieval and knowledge base management.	information retrieval;knowledge base	Zhengxin Chen	1994	Inf. Process. Manage.	10.1016/0306-4573(94)90054-X	document retrieval;knowledge base;question answering;relevance;cognitive models of information retrieval;data management;computer science;artificial intelligence;personal information management;data mining;database;structure of management information;personal knowledge management;data retrieval;information retrieval;information system;human–computer information retrieval	DB	-25.251689453217345	-2.5667438995563074	161373
1aff8df1a15932a199a482909d2ec41f832cfe6b	a theory of case-based decisions	expected utility theory;decision theory;decision maker;inductive inference;rule based system	Give us 5 minutes and we will show you the best book to read today. This is it, the a theory of case based decisions that will be your best choice for better reading book. Your five times will not spend wasted by reading this website. You can take the book as a source to make better concept. Referring the books that can be situated with your needs is sometime difficult. But here, this is so easy. You can find the best thing of book that you can read.	book;situated;theory	Itzhak Gilboa;David Schmeidler	2001			prospect theory;causal decision theory;optimal decision;decision theory;two-moment decision model;expected utility hypothesis;decision analysis;decision field theory;decision engineering;decision tree;decision rule;mathematics;subjective expected utility;management science;evidential reasoning approach;evidential decision theory;social psychology;welfare economics;von neumann–morgenstern utility theorem;weighted sum model;business decision mapping	NLP	-29.665230464253682	-9.535062814377653	161443
cfa035a41201f8c85d1049ec56004e6ed2d19dda	vsfs: a searchable distributed file system	unix data models distributed databases indexing network operating systems query languages;data filtering functionality searchable distributed file system versatile searchable file system posix compatible namespace namespace based file query language nfql analytics applications vsfs high performance file search service vsfs versatile file indexing mechanism real world analytics applications;information filtering;file system managements;indexing methods;servers;big data;indexing;servers indexing real time systems big data file systems;distributed systems;file systems;real time systems	In this paper, we propose a Versatile Searchable File System, VSFS, which builds POSIX-compatible namespace using a novel Namespace-based File Query Language (NFQL). This enables analytics applications to utilize VSFS high-performance file-search service without changing their data model. VSFS versatile file-indexing mechanism is designed to offer great flexibility for applications to control indices to satisfy analytics needs. The evaluations driven by two real-world analytics applications demonstrate VSFS' high scalability and powerful data-filtering functionality.	clustered file system;dce distributed file system;data model;posix;query language;scalability;web search engine	Lei Xu;Ziling Huang;Hong Jiang;Lei Tian;David R. Swanson	2014	2014 9th Parallel Data Storage Workshop	10.1109/PDSW.2014.10	self-certifying file system;computer file;computer science;versioning file system;operating system;unix file types;ssh file transfer protocol;database;open;distributed file system;file system fragmentation;global namespace;world wide web;virtual file system	HPC	-31.540055327706305	1.6185797805481186	161599
204982f05db9b6781493a4aaf1e39ad9942ebb41	on a semantics for neural networks based on fuzzy quantifiers	linguistic model;quantization;cuantificacion;metodologia;learning;sistema informatico;logique floue;computer system;logique propositionnelle;intelligence artificielle;quantification;logica borrosa;raisonnement;methodologie;aprendizaje;fuzzy logic;apprentissage;modele linguistique;propositional logic;modelo linguistico;razonamiento;artificial intelligence;systeme informatique;inteligencia artificial;logica proposicional;reasoning;reseau neuronal;methodology;red neuronal;neural network	Abstract#R##N##R##N#We describe the aggregation process of the typical artificial neuron. We introduce the concept of a fuzzy linguistic quantifier and describe the process for determining the truth of propositions containing linguistic quantifiers. We show how this truth value can be viewed as the firing level of an artificial neuron. We show the relationship between fuzzy sets and neural inputs. A new class of neurons called owa-neurons is described. A learning algorithm for this class of neurons is presented. We provide a methodology for processing information in non-numeric neural networks. © 1992 John Wiley & Sons, Inc.	artificial neural network;fuzzy logic	Ronald R. Yager	1992	Int. J. Intell. Syst.	10.1002/int.4550070805	fuzzy logic;quantization;computer science;artificial intelligence;neuro-fuzzy;methodology;propositional calculus;reason;artificial neural network;algorithm;cognitive science	ML	-21.045370180289353	-1.8673301078840683	161899
77bd079cffb6f85de35c20d166f44d76c0b1f986	the use of psycholinguistics rules in case of creating an intelligent chatterbot	comparative analysis;programming language;artificial intelligent;data base;intelligent systems;intelligent system;artificial intelligence;chatbot programming;chatbot	This paper presents the use of psycholinguistics [1] rules in the case of creating an intelligent Chatterbot. Synonyms [2], hyponyms [2] and hypernyms [2] will be defined and implemented as the rules while the database of Chatterbot being created. The algorithm thanks to which it is possible to get better accuracy in generated and searched phrases by Chatterbot is presented in the paper. Moreover, authors have also made comparative analysis of existing platforms and the Chatterbot programming languages with the proposed system.		Slawomir Wiak;Przemyslaw Kosiorowski	2010		10.1007/978-3-642-13232-2_85	natural language processing;qualitative comparative analysis;intelligent decision support system;computer science;artificial intelligence;machine learning	Robotics	-27.361022756462955	-8.645154885589472	162101
b52084fcfcc9daf889b0f47a904898bd8469d266	efficient spatial query processing for knn queries using well organised net-grid partition indexing approach		In recent years, most of the applications use mobile devices with geographical positioning systems support for providing location-based services. However, the queries sent through the mobile devices to obtain such services consume more time for processing due to the size of the spatial data. In order to solve this problem, an efficient indexing method for providing effective query processing services in mobile computing environments is proposed. This indexing method increases the efficiency of the query retrieval in mobile network environments. Since, all the existing mobile-based network applications utilise the node to node access of spatial objects for processing the query, the mobile query retrieval part in spatial databases is becoming the greatest disadvantage by consuming more time to process the query. The experimental results carried out using the proposed net-grid-based partition index approach show that the proposed model provides fast retrieval with high accuracy in processing of spatial queries.	database;k-nearest neighbors algorithm;spatial query	K. Geetha;Arputharaj Kannan	2018	IJDMMM	10.1504/IJDMMM.2018.10015876	data mining;grid;location-based service;computer science;spatial analysis;search engine indexing;mobile device;mobile computing;cellular network;spatial query	DB	-26.766853770269	0.6251436331920353	162218
af8aa9dc643828df4d648d28f453d19c957a83a3	an efficient quad-tree based index structure for cloud data management	local quad-tree index;local index;cloud data management;proposed index structure;global index;efficient index structure;cloud computing;efficient quad-tree;reliable index structure;multi-dimensional index structure;cloud computing system;query processing	local quad-tree index;local index;cloud data management;proposed index structure;global index;efficient index structure;cloud computing;efficient quad-tree;reliable index structure;multi-dimensional index structure;cloud computing system;query processing	quadtree	Linlin Ding;Baiyou Qiao;Guoren Wang;Chen Chen	2011		10.1007/978-3-642-23535-1_22	computer science;data mining;database;utility computing;world wide web	DB	-29.46580098629046	0.8488697966357368	162230
a37f37a4ed913dd074ae29adb64e579d5582959f	reflection and meta-level artificial intelligence architectures	artificial intelligent		artificial intelligence;holography	Ramón López de Mántaras	1996	Future Generation Comp. Syst.	10.1016/0167-739X(96)00002-7	artificial architecture;computer science;artificial intelligence system;artificial intelligence, situated approach	AI	-27.435013227989568	-8.663134569829243	162440
d2c9754f5d036647e4d2b1c65825a9120e5c4624	data warehouse performance: selected techniques and data structures		Data stored in a data warehouse (DW) are retrieved and analyzed by complex analytical applications, often expressed by means of star queries. Such queries often scan huge volumes of data and are computationally complex. For this reason, an acceptable (or good) DW performance is one of the important features that must be guaranteed for DW users. Good DW performance can be achieved in multiple components of a DW architecture, starting from hardware (e.g., parallel processing on multiple nodes, fast disks, huge main memory, fast multi-core processor), through physical storage schemes (e.g., row storage, column storage, multidimensional store, data and index compression algorithms), state of the art techniques of query optimization (e.g., cost models and size estimation techniques, parallel query optimization and execution, join algorithms), and additional data structures improving data searching efficiency (e.g., indexes, materialized views, clusters, partitions). In this chapter we aim at presenting only a narrow aspect of the aforementioned technologies. We discuss three types of data structures, namely indexes (bitmap, join, and bitmap join), materialized views, and partitioned tables. We show how they are being applied in the process of executing star queries in three commercial database/data warehouse management systems, i.e., Oracle, DB2, and SQL Server.		Robert Wrembel	2011		10.1007/978-3-642-27358-2_2	data warehouse;data mining;database	HPC	-29.155097241409884	2.4412628940403125	162445
924013ff90c207d13259f9a6e0685272e2ed5538	swifttuna: responsive and incremental visual exploration of large-scale multidimensional data		For interactive exploration of large-scale data, a preprocessing scheme (e.g., data cubes) has often been used to summarize the data and provide low-latency responses. However, such a scheme suffers from a prohibitively large amount of memory footprint as more dimensions are involved in querying, and a strong prerequisite that specific data structures have to be built from the data before querying. In this paper, we present SwiftTuna, a holistic system that streamlines the visual information seeking process on large-scale multidimensional data. SwiftTuna exploits an in-memory computing engine, Apache Spark, to achieve both scalability and performance without building precomputed data structures. We also present a novel interactive visualization technique, tailed charts, to facilitate large-scale multidimensional data exploration. To support responsive querying on large-scale data, SwiftTuna leverages an incremental processing approach, providing immediate low-fidelity responses (i.e., prompt responses) as well as delayed high-fidelity responses (i.e., incremental responses). Our performance evaluation demonstrates that SwiftTuna allows data exploration of a real-world dataset with four billion records while preserving the latency between incremental responses within a few seconds.	apache spark;block size (cryptography);chart;data cube;data structure;dot plot (bioinformatics);gradient;holism;in-memory database;in-memory processing;information seeking;interaction technique;interactive visualization;memory footprint;optimization problem;performance evaluation;precomputation;preprocessor;responsiveness;sampling (signal processing);scalability;search engine optimization;stratified sampling;throughput;usability testing	Jaemin Jo;Wonjae Kim;Seunghoon Yoo;Bo Hyoung Kim;Jinwook Seo	2017	2017 IEEE Pacific Visualization Symposium (PacificVis)	10.1109/PACIFICVIS.2017.8031587	data visualization;memory footprint;interactive visual analysis;visual analytics;scalability;data mining;information visualization;data cube;data structure;computer science	DB	-32.5821920076694	-0.3423627958031906	162473
8a58058834589d3f19d4dfc3241d3e61695784be	motives for intelligent agents: computational scripts for emotion concepts	intelligent agent			Christine L. Lisetti	1997			natural language processing;agent architecture;computer science;knowledge management;artificial intelligence;intelligent agent	AI	-26.46645278261193	-9.266697831746272	162489
f7d00cf121bb3c4f0e58f00c88086ea937447aaf	skyline queries: an introduction	partitioning algorithms encoding query processing informatics electronic mail extraterrestrial measurements complexity theory;time series decision support systems multidimensional systems query processing;spatial skyline queries multicriteria decision support applications multidimensional spaces metric spaces dynamic spaces time series data skyline query processing application specific problems k dominant skylines top k dominating queries	During the two past decades, skyline queries were used in several multi-criteria decision support applications. Given a dominance relationship in a dataset, a skyline query returns the objects that cannot be dominated by any other objects. Skyline queries were studied extensively in multidimensional spaces, in subspaces, in metric spaces, in dynamic spaces, in streaming environments, and in time-series data. Several algorithms were proposed for skyline query processing, such as window-based, progressive, distributed, geometric-based, index-based, divide- and-conquer, and dynamic programming algorithms. Moreover, several variations were proposed to solve application-specific problems like k-dominant skylines, top-k dominating queries, spatial skyline queries, and others. As the number of objects that are returned in a skyline query may become large, there is also an extensive study for the cardinality of skyline queries. This extensive research depicts the importance of skyline queries and their variations in modern applications.	algorithm;database;decision support system;dynamic programming;information retrieval;media queries;pareto efficiency;skyline operator;time series;uncertain data	Eleftherios Tiakas;Apostolos N. Papadopoulos;Yannis Manolopoulos	2015	2015 6th International Conference on Information, Intelligence, Systems and Applications (IISA)	10.1109/IISA.2015.7388053	theoretical computer science;data mining;database;mathematics	DB	-27.5288639207712	1.3729021215180661	162552
3a3f13d5d435a9d44f7b49eaac806c398c41c421	a new integration mechanism for knowledge map of complex product development	product development groupware knowledge management;whkm integration mechanism complex product development knowledge sharing multidisciplinary teams product structures development knowledge concept knowledge map ckm process knowledge map pkm expert knowledge map ekm local detailed knowledge map ldkm whole framework knowledge map;missiles;product development propulsion knowledge engineering merging missiles educational institutions joining processes;merging;joining processes;integration method complex product development knowledge integration map the local detailed map the whole framework map;propulsion;knowledge engineering;product development	Currently complex product development involves effective communication and knowledge sharing across multidisciplinary teams and departments, whose processes and product structures are complicated. This paper presents a generalized framework to integrate knowledge maps to reuse development knowledge adequately. Firstly, the knowledge map of complex product development is classified as the concept knowledge map (CKM), the process knowledge map (PKM) and the expert knowledge map (EKM). Based on the classification, the proposed approach integrates the local detailed knowledge map (LDKM) collaboratively designed by different departments into the whole framework knowledge map (WHKM) designed by leading department using the map node mapping method. By establishing the relations among the concept map, the process map and the expert map using knowledge instance mapping, those three kinds of knowledge maps are integrated to a global detailed knowledge integration map which describes the development knowledge from various aspects.	automatic taxonomy construction;cognitive map;concept map;knowledge integration;knowledge management;new product development	Jihong Liu;Ling Ge	2013	2013 10th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)	10.1109/FSKD.2013.6816252	knowledge base;propulsion;knowledge integration;knowledge management;artificial intelligence;knowledge-based systems;knowledge engineering;open knowledge base connectivity;knowledge extraction;domain knowledge;new product development	Robotics	-31.68943512272978	-5.693948967921867	162641
f85e4d2edf56f8b39b645bf5259a82b1b8a08127	a quantitative model of capabilities in multi-agent systems	scheduling algorithm;multi agent system;cognitive ability;mathematical model	Reasoning about capabilities in multi-agent systems is crucial for many applications. There are two aspects of reasoning about the capabilities of an agent to achieve its goals. One is the symbolic, logical reasoning, which is based on whether the agent can determine a plan that can be used to achieve a goal (i.e. “know-how”). Another is quantitative reasoning about levels of skill and whether a set of tasks can be achieved within a specified time and with quality constraints. Capabilities in this sense are determined by the limits of internal processing capacity. Both artificial agents and human agents can be subject to processing capacity limits, whether due to CPU speed, or cognitive limits on attention, memory, etc. This could depend not only on the unique skill levels of each individual and their cognitive abilities, but also on their ability to handle those task demands in parallel, given intrinsic limits on internal cognitive resource capacities. In past work, researchers have focused primarily on the logical aspect of capability reasoning; less work has been done on modeling the quantitative aspects of reasoning about capabilities in agents. In this paper, we introduce a general mathematical model to define the capabilities of agents to achieve a set of tasks. Our definitions of capabilities are based on whether a feasible schedule exists to complete the tasks within the constraints, either in static environments or dynamic environments, for which we present two corresponding preliminary scheduling algorithms. We illustrate this model with two experiments to evaluate the algorithms. We conclude by discussing the potential applications of this model, and future work.	algorithm;central processing unit;cognition;experiment;intelligent agent;mathematical model;multi-agent system;reasoning system;scheduling (computing)	Linli He;Thomas R. Ioerger	2003			logical reasoning;scheduling (computing);multi-agent system;machine learning;qualitative reasoning;instructions per second;computer science;artificial intelligence;cognition	AI	-19.617704441964815	-9.343050037428114	162778
1bcb2e237549af16b3470943d4bf86d4300570af	trie-based similarity search and join	edit distance;string similarity;deletion neighborhood	Driven by the increasing demands from applications such as data cleansing, integration, and bioinformatics, approximate string matching queries have gain much attention recently. In this paper, we present the design and implementation of a trie-based system which supports both string similarity search and join based on our recent work [23].	approximate string matching;bioinformatics;similarity search;string metric;string searching algorithm;trie	Jianbin Qin;Xiaoling Zhou;Wei Wang;Chuan Xiao	2013		10.1145/2457317.2457389	edit distance;approximate string matching;commentz-walter algorithm;computer science;theoretical computer science;data mining;database;string metric;string searching algorithm	DB	-28.47764955721833	0.2862162941525344	162873
7fa28ba8f601ba6004269d32530823076ea291d1	a general knowledge mediation infrastructure for multi-agent systems	access point;multi agent system;semantic integration;general knowledge;domain knowledge;multi agent systems;knowledge acquisition;intelligent system;finite state automaton;use case;domain specificity	Intelligent decision making needs to be equipped with broader knowledge in order to enhance the decision quality. Knowledge for decision making can be categorized as domain specific and general. Applying domain knowledge in intelligent systems is not new, but applying general knowledge to support business decision making is a possible way to obtain an edge over competitors. For this reason, the paper focuses primarily on designing a general knowledge mediation infrastructure (GKMI) which supports the use of general knowledge from multiple heterogeneous sources, and provides an unified access point for typical multi-agent systems (MAS) to access that knowledge. The finite state automaton (FSA) is used to model and analyze the commonsense inference ability of GKMI. By carrying out two use cases of GKMI for MAS development and operation the effectiveness of this infrastructure is examined.		Kun Chen;Huaiqing Wang;Hokyin Lai	2011	Expert Syst. Appl.	10.1016/j.eswa.2010.06.091	use case;knowledge base;semantic integration;intelligent decision support system;computer science;knowledge management;artificial intelligence;body of knowledge;knowledge-based systems;machine learning;multi-agent system;open knowledge base connectivity;data mining;procedural knowledge;knowledge extraction;general knowledge;commonsense knowledge;knowledge value chain;domain knowledge	AI	-32.370077112132456	-6.774629943239721	162940
220cc6c0b2c93b5e8c2ecc6c29173f464b78c9c3	generic tasks as building blocks for knowledge-based systems: the diagnosis and routine design examples	knowledge based system;building block		knowledge-based systems	B. Chandrasekaran	1988	Knowledge Eng. Review	10.1017/S0269888900004458	computer science;knowledge management;artificial intelligence;theoretical computer science;knowledge-based systems	AI	-30.00953569618002	-6.504518608192725	163013
205f5dec0024885037dec69e06a9e0b3f10a0f33	communicating vague spatial concepts in human-gis interactions: a collaborative dialogue approach	lenguaje natural;intercambio informacion;spatial context;fiabilidad;reliability;systeme information geographique;geographic information system;man machine dialogue;teoria sistema;interaction;langage naturel;hombre;contextual information;systems theory;shared knowledge;echange information;theorie systeme;natural language;fiabilite;information exchange;contexto;human;contexte;dialogo hombre maquina;interaccion;information system;context;systeme information;sistema informacion geografica;homme;dialogue homme machine;sistema informacion	Natural language requests involving vague spatial concepts are not easily communicated to a GIS because the meaning of spatial concepts depends largely on the contexts (such as task, spatial contexts, and user’s personal background) that may or may not be available or specified in the system. To address such problems, we developed a collaborative dialogue approach that enables the system and the user to construct shared knowledge about relevant contexts. The system is able to anticipate what contextual knowledge must be shared, and to form a plan to exchange contextual information based on the system’s belief on who knows what. To account those user contexts that are not easily communicated by language, direct feedback approach is used to refine the system’s belief so that the intended meaning is properly grounded. The approach is implemented as a dialogue agent, GeoDialogue, and is illustrated through an example dialogue involving the communication of the vague spatial	geographic information system;interaction;natural language;vagueness	Guoray Cai;Hongmei Wang;Alan M. MacEachren	2003		10.1007/978-3-540-39923-0_19	interaction;information exchange;computer science;artificial intelligence;spatial contextual awareness;reliability;mathematics;geographic information system;natural language;systems theory;information system;statistics	AI	-24.449563867295232	-8.247188341336967	163140
079433997c7270ea95e23042ffd8105d75d22cdd	a query processing strategy for the decomposed storage model	query processing;indexes	Handling parallelism in database systems involves the specification of a storage model, a placement strategy, and a query processing strategy. An important goal is to determine the appropriate combination of these three strategies in order to obtain the best performance advantages. In this paper we present a novel and promising query processing strategy for a decomposed storage model. We discuss some of the qualitative advantages of the scheme. We also compare the performance of the proposed “pivot” strategy with conventional query processing for the n-ary storage model. The comparison is performed using the Wisconsin Benchmarks.	benchmark (computing);database;parallel computing;storage model	Setrag Khoshafian;George P. Copeland;Thomas Jagodis;Haran Boral;Patrick Valduriez	1987	1987 IEEE Third International Conference on Data Engineering	10.1109/ICDE.1987.7272433	database index;online aggregation;sargable;query optimization;query expansion;computer science;theoretical computer science;data mining;database	DB	-29.048141741406045	2.7314908735709853	163234
a9b36987a47123e9243fe7e4aad80ae687130a21	on personal and role mental attitudes: a preliminary dependence-based analysis	representacion mental;layered architecture;multiagent system;systeme intelligent;relacion hombre maquina;sistema inteligente;representation mentale;man machine relation;ingenieria logiciel;software engineering;mental representation;agent intelligent;cognition;intelligent system;intelligent agent;genie logiciel;cognicion;relation homme machine;agente inteligente;sistema multiagente;systeme multiagent	In this paper, we present some preliminary results concerning the extension of dependence theory 2] and social reasoning 9] to cope with the notion of organizational roles. In order to accomplish this task, we rst present a rather informal deenition of organization and roles, showing how this additional dimension can be represented within a 3-layered architecture of mental states. We then restrict our analysis to the domain level, by extending our original notions of unilateral and bilateral dependence, as well as that of goal situation, to show that just by representing explicitly the input source of some mental attitudes one can easily explain some interesting social phenomena, like agents' adequacy to roles and vice-versa. For methodological reasons, we divide this analysis along two main axes, i.e., the inter-agent and the intra-agent dimensions. The task of proposing a complete, formal and rather universal representation of organizations and their associated roles is a very diicult one. Indeed, several diierent dimensions are used by researchers in social sciences 3, 5], distributed AI and multi-agent systems 4, 7], and distributed computing 1, 6] to characterize what an organization is and what its main components are. In a certain sense, quite all of these proposed descriptions comprise both a factual and a procedural dimension. By factual we mean a mere observable behavior, despite its internal functioning: an organization has its high level goals, its observable inputs and outputs.	bilateral filter;distributed artificial intelligence;distributed computing;high-level programming language;mental state;multi-agent system;observable;procedural programming	Jaime Simão Sichman;Rosaria Conte	1998		10.1007/10692710_1	simulation;cognition;computer science;artificial intelligence;multitier architecture;mental representation;intelligent agent;algorithm	AI	-24.24139996216731	-8.415166340382493	163260
c6a229d36973299038adccc4b3992f72a6d4fcf7	knowledge discovery in data using formal concept analysis and random projections	attribute implications;redukcja wymiarowości;odkrywanie wiedzy;random projections;concept lattices;dimensionality reduction;projekcja losowa;formal concept analysis;knowledge discovery	In this paper our objective is to propose a random projections based formal concept analysis for knowledge discovery in data. We demonstrate the implementation of the proposed method on two real world healthcare datasets. Formal Concept Analysis (FCA) is a mathematical framework that offers a conceptual knowledge representation through hierarchical conceptual structures called concept lattices. However, during the design of a concept lattice, complexity plays a major role.	complexity;data mining;formal concept analysis;knowledge representation and reasoning;rp (complexity);random projection;singular value decomposition;terabyte	Cherukuri Aswani Kumar	2011	Applied Mathematics and Computer Science	10.2478/v10006-011-0059-1	computer science;formal concept analysis;knowledge management;machine learning;data mining;mathematics;knowledge extraction;lattice miner;dimensionality reduction	AI	-33.542316392791655	-5.261702381248815	163729
3e6c4790eee0a3dfcdb6ce444147aa1fd42f7063	indexing of moving objects for location-based services	database indexing;moving object;multidimensional indexing;wireless devices;location based service;tree data structures;r tree;internet;indexation;internet services;moving objects;update performance moving object indexing location based services internet wireless devices dynamic database r tree based technique bounding regions tree structure performance experiments search performance;indexing web and internet services databases computer science vehicle dynamics tree data structures home appliances computer industry mobile computing context awareness;mobile computing;access method;tree data structures database indexing internet mobile computing	Visionaries predict that the Internet will soon extend to billions of wireless devices, or objects, a substantial fra ction of which will offer their changing positions to locatio nbased services. This paper assumes an Internet-service sce nario where objects that have not reported their position within a specified duration of time are expected to no longer be interested in, or of interest to, the service. Due to the possibility of many “expiring” objects, a highly dynamic database results. The paper presents an R-tree based technique for the indexing of the current positions of such objects. Different types of bounding regions are studied, and new algorithms are provided for maintaining the tree structure. Performance experiments indicate that, when compared to the approach where the objects are not assumed to expire, the new indexing technique can improve search performance by a factor of two or more without sacrificing update performance.	algorithm;experiment;internet;lazy evaluation;linear function;online and offline;r-tree;relevance;tree structure	Simonas Saltenis;Christian S. Jensen	2002		10.1109/ICDE.2002.994759	r-tree;database index;the internet;computer science;location-based service;data mining;database;tree;programming language;mobile computing;access method;world wide web	DB	-26.583917720304616	0.26429072443386376	163825
fb0f1d9b040a038129adc8d34c8aefe8c353ac35	design and application of a new knowledge engineering tool for solving real-world problems	programming environment;multistrategy reasoning;multistrategy learning;automatic generation;open architecture;development tool;machine learning;production rules automatic generation;programming tool;production rule;knowledge engineering	The results of a broader cognitive research on an intelligent knowledge engineering program environment are described. This intelligent programming tool features an open architecture, modularity and an idea to use multistrategy learning, multistrategy knowledge representation and integration of various schemes of knowledge processing in a single inferential process. Some selected applications of the developed tool, carefully examined at various levels, are briefly dealt with.	knowledge engineering	Zdzislaw S. Hippe	1996	Knowl.-Based Syst.	10.1016/S0950-7051(96)00002-0	open architecture;computer science;knowledge management;artificial intelligence;machine learning;knowledge engineering	SE	-30.100513393013124	-6.1307730325317245	163865
23184b21652a2e4c06886c11457e06ce9c2df922	mobile data management (part 3) - dynamark: benchmarking dynamic spatial indexing for location-based services	spatial index;location based service		location-based service	Jussi Myllymaki;James H. Kaufman	2003	IEEE Distributed Systems Online		mobile broadband;computer science;benchmarking;world wide web;search engine indexing;data mining;database;spatial database;location-based service	Visualization	-27.677651991526034	0.34204608126936403	164034
679c4d05d6d882653b3477432670bd4efa2e651e	multi-agent logic with distances, uncertainty and interaction based on linear temporal frames				Vladimir V. Rybakov	2012		10.3233/978-1-61499-105-2-169	discrete mathematics;machine learning;artificial intelligence;mathematics	Vision	-26.089516212274933	-7.654880305049017	164334
3ff5cf0738ba9187666f652525927aeb20f58b18	mquery: fast graph query via semantic indexing for mobile context	mquery;graph theory;sgi mobile social network graph query mquery;shortest path;query interfaces;semantic indexing;context aware;query processing;connection subgraph query mquery fast graph query semantic indexing mobile context data ubiquitous platform context aware intelligent computing resource description framework rdf compressed index method query interfaces shortest path query neighbor query instance query;neighbor query;compressed index method;resource description framework;indexing method;social network;indexing;graph query;indexation;connection subgraph query;mobile context data;shortest path query;query processing graph theory indexing mobile computing;instance query;mobile computing;context aware intelligent computing;ubiquitous platform;sgi;rdf;mobile social network;fast graph query	Mobile is becoming a ubiquitous platform for context-aware intelligent computing. One fundamental but usually ignored issue is how to efficiently manage (e.g., index and query) the mobile context data. To this end, we present a unified framework and have developed a toolkit, referred to as MQuery. More specifically, the mobile context data is represented in the standard RDF (Resource Description Framework) format. We propose a compressed-index method which takes less than 50% of the memory cost (of the traditional method) to index the context data. Four query interfaces have been developed for efficiently querying the context data including: instance query, neighbor query, shortest path query, and connection subgraph query. Experimental results on two real datasets demonstrate the efficiency of MQuery.	lossy compression;resource description framework;shortest path problem;unified framework	Yuan Zhang;Ning Zhang;Jie Tang;Jinghai Rao;Wenbin Tang	2010	2010 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology	10.1109/WI-IAT.2010.137	online aggregation;sargable;query optimization;query expansion;web query classification;ranking;boolean conjunctive query;computer science;sparql;query by example;graph theory;theoretical computer science;rdf;database;rdf query language;web search query;mobile computing;view;world wide web;information retrieval;query language;object query language	DB	-31.54643237463001	3.266541815797078	164410
a15dae0996e0d2dd3602457e8dc0d6e8772c0135	prolexs: creating law and order in a heterogeneous domain	sistema experto;application juridique;base connaissance;lenguaje especializado;intelligence artificielle;raisonnement;inferencia;razonamiento;artificial intelligence;base conocimiento;inteligencia artificial;systeme expert;reasoning;langage specialise;special purpose language;inference;knowledge base;expert system	This article defines a heterogeneous domain as a domain in which typical problems can only be solved by combining several distinctive knowledge sources. The legal domain, in this view, must be considered heterogeneous since classical rule-based knowledge sources like legislation, cooperate with expertise and case-law, both possibly represented quite differently, to produce useful results. The article continues with the description of the architecture of PROLEXS, an expert system built to operate in such domains. An example dialogue is added to sketch the problems of building knowledge-based systems in heterogeneous domains and the way PROLEXS approaches those problems. Finally the current PROLEXS research involving neural networks and case-based reasoning is introduced.	artificial neural network;case-based reasoning;expert system;knowledge-based systems;logic programming	R. F. Walker;Anja Oskamp;J. A. Schrickx;G. J. van Opdorp;P. H. van den Berg	1991	International Journal of Man-Machine Studies	10.1016/S0020-7373(07)80007-1	natural language processing;knowledge base;computer science;artificial intelligence;expert system;reason;algorithm	AI	-24.0812850267404	-1.9725498364785092	164531
35c0f5d5e44ac71a93abaedf0c41491691f5f85c	using expert systems for simulation modeling of patient scheduling	modelizacion;sistema experto;expert systems;control logic;health care delivery;patient appointments;veterinary practice simulator;modelisation;regle decision;sante;scheduling;modele simulation;ordonamiento;health;regla decision;modelo simulacion;systeme expert;salud;modeling;simulation model;logique control;ordonnancement;decision rule;expert system	Modeling the scheduling of patient appointments is an important issue in simulating a health care delivery facility. A simulation model must include the control logic of appointment scheduling software and the explicit and implicit decision rules used by the human scheduler in selecting an appointment time. Expert systems provide one way of modeling such control logic and decision rules. We describe a structure for an expert system that models patient appointment scheduling and the integration of such an expert system within a simulation model. An example expert system for a small animal veterinary clinic is presented.	expert system;scheduling (computing);simulation	Charles R. Standridge;Duane Steward	2000	Simulation	10.1177/003754970007500303	simulation;computer science;knowledge management;artificial intelligence;decision rule;health;scheduling;expert system	AI	-25.329026309143043	-5.594384540131038	164574
343f68e673de4b1ee328a4da299f47ca1c0bf7c5	information retrieval by constrained spreading activation in semantic networks	sistema experto;grantsmanship;expert systems;information retrieval;recuperacion de informacion;semantic network;chercheur;base connaissance;conception;intelligence artificielle;research worker;recherche information;diseno;spreading activation;artificial intelligence;algorithms;base conocimiento;design;evaluation;search strategies;research proposals;inteligencia artificial;systeme expert;investigator;relevance information retrieval;grant;grants;knowledge base;expert system	Abstract   GRANT is an expert system for finding sources of funding given research proposals. Its search method-constrained spreading activation—makes inferences about the goals of the user and thus finds information that the user did not explicitly request but that is likely to be useful. The architecture of GRANT and the implementation of constrained spreading activation are described, and grant's performance is evaluated.	information retrieval;semantic network;spreading activation	Paul R. Cohen;Rick Kjeldsen	1987	Inf. Process. Manage.	10.1016/0306-4573(87)90017-3	design;computer science;artificial intelligence;evaluation;data mining;spreading activation;semantic network;operations research;world wide web;expert system	Web+IR	-25.174717151560472	-2.750228434613952	164825
d42c5e67c72d6af93c1b2bff7355fd37cda1bd01	an interval-based approach for reasoning about land use change trajectories		In this paper, we present a formal spatiotemporal interval logic mechanism extended from Allen's interval temporal logic to the spatial context. The method contributes with a tool to help scientists and policy makers to reason about land use change. We use an interval-based approach and the concept of events to reason about land use changes trajectories resulting in a formalism that allows users to express queries about the land use. To show the use of our formalism we use it to identify and quantifying land use transitions in Mato Grosso state in Brazil from annual classified maps of land use.	interval temporal logic;map;semantics (computer science)	Adeline M. Maciel;Lúbia Vinhas;Gilberto Câmara;Michelle Cristina Araújo Picoli;Rodrigo Anzolin Begotti	2018	IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2018.8518765	management science;interval temporal logic;remote sensing;spatial contextual awareness;land use;land use, land-use change and forestry;computer science;formalism (philosophy);cognition	Embedded	-21.317603698164124	0.7202024795245112	165035
76afd7eda135cd3e82b9fa7e59f58aa66d042fac	cosimo: a cognitive simulation model of human decision making and behavior in accident management of complex plants	modelizacion;modularity behavioural sciences computing cognitive simulation model human decision making accident management cosimo man machine system blackboard architecture;operateur humain;modele cosimo;blackboard architecture;operador humano;central electrica;behavioural sciences computing;relacion hombre maquina;simulation;accident;man machine relation;simulacion;prise decision;humans decision making accidents man machine systems computational modeling control systems safety decision support systems control system synthesis pattern recognition;man machine system;modelisation;power plant;control proceso;centrale electrique;accidents;complex system;surete;cognition;process control;human operator;pattern recognition;cognicion;nuclear power plant;centrale nucleaire;relation homme machine;accidente;toma decision;theoretical foundation;seguridad;modeling;information seeking;simulation model;cognitive function;physiological models;man machine systems;safeguard;central nuclear;commande processus;physiological models accidents behavioural sciences computing blackboard architecture digital simulation man machine systems;human decision making;digital simulation	A cognitive simulation model (COSIMO) which simulates the behavior of an operator controlling a complex system during the management of accidents is described. Particular attention is paid to the theoretical foundations of the model, to its computational implementation, and to a number of simulations of man-machine system interactions. The approach followed in COSIMO is to build a structure for the various kinds of cognitive functions that are performed by an operator in complex environments, e.g., information seeking, pattern recognition and diagnosis, monitoring, planning, and acting on the system. For the implementation of COSIMO a blackboard architecture has been chosen. This architecture offers enough flexibility and modularity for modeling the overall state of the human and of the environment. >	artificial intelligence;simulation	Pietro Carlo Cacciabue;Françoise Decortis;Bartolome Drozdowicz;Michel Masson;Jean-Pierre Nordvik	1992	IEEE Trans. Systems, Man, and Cybernetics	10.1109/21.179844	simulation;cognition;computer science;artificial intelligence;machine learning;process control	AI	-24.44457108614493	-6.384308716443931	165139
6907f6b41eb77373a431ea83b8614620d71b1ce7	efficient and tunable similar set retrieval	design principle;linkages among documents;query processing;information retrieval;model complexity;constraint optimization problem;metasearch;distributed collection;collaborative filtering;indexation;data structure;object relational	Set value attributes are a concise and natural way to model complex data sets. Modern Object Relational systems support set value attributes and allow various query capabilities on them. In this paper we initiate a formal study of indexing techniques for set value attributes based on similarity, for suitably defined notions of similarity between sets. Such techniques are necessary in modern applications such as recommendations through collaborative filtering and automated advertising. Our techniques are probabilistic and approximate in nature. As a design principle we create structures that make use of well known and widely used data structuring techniques, as a means to ease integration with existing infrastructure. We show how the problem of indexing a collection of sets based on similarity can be reduced to the problem of indexing suitably encoded (in a way that preserves similarity) binary vectors in Hamming space thus, reducing the problem to one of similarity query processing in Hamming space. Then, we introduce and analyze two data structure primitives that we use in cooperation to perform similarity query processing in a Hamming space. We show how the resulting indexing technique can be optimized for properties of interest by formulating constraint optimization problems based on the space one is willing to devote for indexing. Finally we present experimental results from a prototype implementation of our techniques using real life datasets exploring the accuracy and efficiency of our overall approach as well as the quality of our solutions to problems related to the optimization of the indexing scheme.	approximation algorithm;collaborative filtering;constrained optimization;data structure;database;hamming space;mathematical optimization;prototype;real life;window function	Aristides Gionis;Dimitrios Gunopulos;Nick Koudas	2001		10.1145/375663.375689	data structure;computer science;theoretical computer science;collaborative filtering;data mining;database;programming language;information retrieval	DB	-31.652801002784795	3.360628359655015	165330
1e390cf4ecfeae3c8f22ac73411792e06fb3eb66	old dogs are great at new tricks: column stores for ir prototyping	bm25;relational databases	"""We make the suggestion that instead of implementing custom index structures and query evaluation algorithms, IR researchers should simply store document representations in a column-oriented relational database and implement ranking models using SQL. For rapid prototyping, this is particularly advantageous since researchers can explore new scoring functions and features by simply issuing SQL queries, without needing to write imperative code. We demonstrate the feasibility of this approach by an implementation of conjunctive BM25 using two modern column stores. Experiments on a web collection show that a retrieval engine built in this manner achieves effectiveness and efficiency on par with custom-built retrieval engines, but provides many additional advantages, including cleaner query semantics, a simpler architecture, built-in support for error analysis, and the ability to exploit advances in database technology """"for free""""."""	algorithm;error analysis (mathematics);imperative programming;rapid prototyping;relational database;sql;scoring functions for docking	Hannes Mühleisen;Thaer Samar;Jimmy J. Lin;Arjen P. de Vries	2014		10.1145/2600428.2609460	relational database;computer science;query by example;machine learning;data mining;database;okapi bm25;world wide web;information retrieval	Web+IR	-33.24013506746981	4.178569815693894	165680
2596ed6881b94bc6bd193d929226b377d8b7aa05	search system for behavior time segments from accumulated sensor data in room environment	query language;sensing room environment search system sequential sensor data home environment time segment generation database table query language;sensor fusion query languages relational databases;sensor systems intelligent sensors tv spatial databases database languages tactile sensors temperature sensors sampling methods data analysis database systems;generation time;query languages;search system;time segment generation;home environment;database table;sensing room environment;sequential sensor data;relational databases;sensor fusion	This paper proposes a search system for a database that stores heterogeneous sensor data in home environment. In the system, sequential sensor data are treated as time segments. The time segments are generated with Flexible representation among programs for time segment generation, database table and tokens in query. Various search and aggregation of time segments about daily behavior (ex. average of sleeping time for every week and frequency of watching TV when sitting) are realized with the simple query language for generated time segments. We demonstrated the search system with a large amount of sensor data accumulated in our Sensing Room, which is a room including many heterogeneous sensors.	aggregate data;embedded system;mathematical optimization;parallel computing;query language;query optimization;resource description framework;sensor;table (database)	Hiroshi Noguchi;Hiroyuki Kojo;Taketoshi Mori;Tomomasa Sato	2007	Future Generation Communication and Networking (FGCN 2007)	10.1109/FGCN.2007.198	query optimization;computer science;data mining;database;view;information retrieval	DB	-26.92050084316806	-0.9065520662351695	166073
8c9537bd80aa8e81d1e29b12f558473d5f81d1c5	ontologica: exploiting ontologies and natural language for representing and querying railway management logics	artificial intelligence;knowledge representation;ontology;natural language processing		natural language;ontology (information science)	Daniela Briola;Riccardo Caccia;Michele Bozzano;Angela Locoro	2012		10.3233/978-1-61499-105-2-376	natural language processing;question answering;computer science;knowledge management;data mining;commonsense knowledge	AI	-28.034409616487483	-7.173773285019764	166173
f31a02de4e031585022848a67c468752d60268bf	plan revision in person-machine dialogue	overall purpose;cooperative process;dialogue deviation;full dialogue;dialogue session;dialogue management;plan revision;novice user;speech act;person-machine dialogue;expert problem solver;different type	Dialogue is a cooperative process in which each speech act of the participants contributes to the overall purpose of the dialogne. Participating in a full dialogue implies understanding at each point of the dialogue session the role of each speech act with respect to the rest of the dialogne. We concentrate in this paper on speech acts that diverge from the straightforward unfolding of the dialogue. Such speech acts represent dialogue deviations. We analyze the representation of different types and degrees of deviations and present a plan revision mechanism for dialogue management that permits their treatment in the context of advice giving dialogues between a novice user and an expert problem solver.	belief revision;color gradient;dialog system;directive (programming);exptime;entity framework;prolog;prototype;solver;unfolding (dsp implementation);workstation	Cléo Jullien;Jean-Charles Marty	1989			computer science;knowledge management;artificial intelligence	NLP	-21.403319536496895	-9.06189863192847	166205
d69a4db0e331997ff42b5c71caee2f614734a738	efficient querying relaxed dominant relationship between product items based on rank aggregation	search engine;rank aggregation;data structure;structured data	Current search engines cannot effectively rank those relational data, which exists on dynamic websites supported by online databases. In this study, to rank such structured data, we propose a new model, Relaxed Dominant Relationship (RDR), which extends the state-of-the-art work by incorporating rank aggregation methods. We propose efficient strategies on building compressed data structure to encode the core part of RDR between items. Efficient querying approaches are devised to facilitate the ranking process and to answer the RDR query. Extensive experiments are conducted and the results illustrate the effectiveness and efficiency of our methods.	compressed data structure;data compression;database;encode;experiment;restrictive design rules;web search engine	Zhenglu Yang;Lin Li;Masaru Kitsuregawa	2008			data structure;data model;computer science;machine learning;data mining;database;search engine	AI	-31.2000459166191	4.068205929905088	166255
7856acda1e3eb6583b001ec9dcaa1d32c998d2cc	utr-tree: an index structure for the full uncertain trajectories of network-constrained moving objects	databases;moving object;dynamic index maintenance technique;electronic mail;moving objects databases;trajectory moving objects database uncertainty index;uncertainty;moving object database;uncertainty databases conference management indexing transportation indexes sampling methods global positioning system electronic mail software algorithms;moving objects database;index structure;conference management;uncertain trajectories;uncertainty management;index;location update;uncertainty handling;junctions;indexing method;visual databases uncertainty handling;indexes;trajectory;indexing;global positioning system;network constrained moving objects;dynamic index maintenance technique utr tree index structure uncertain trajectories network constrained moving objects uncertainty management moving objects databases;indexation;transportation;mobile communication;software algorithms;sampling methods;utr tree;life span;data models;visual databases	The uncertainty management problem for moving objects databases has been well studied recently, with many models and algorithms proposed. However, very limited work has dealt with the index of uncertain trajectories for a running moving objects database. In this paper, we propose an index framework, the UTR- Tree, for indexing the full uncertain trajectories of network constrained moving objects. Through a dynamic index maintenance technique which is associated with location updates, the UTR-Tree can deal with the full uncertain trajectories, which include not only the historical locations of moving objects, but also their current and near future location information with uncertainty considered, so that the queries on the whole life span of the moving objects can be efficiently supported. The experimental results show that the UTR-Tree outperforms previously proposed network- based moving object index methods in dealing with full uncertain trajectories.	abstract syntax tree;algorithm;database index;r-tree;spatiotemporal database;uncertainty quantification	Zhiming Ding	2008	The Ninth International Conference on Mobile Data Management (mdm 2008)	10.1109/MDM.2008.8	database index;computer science;data mining;database;information retrieval	DB	-26.94841504220723	0.7542729216623709	166262
f4d162465018cfff321f70691e94da6494877a8d	an overview of knowledge-acquisition and transfer	regle inference;representacion conocimientos;metodologia;base connaissance;acquisition connaissance;classification;methodologie;actitud cognitiva;transfert connaissance;attitude cognitive;inference rule;knowledge acquisition;cognitive attitude;knowledge transfer;base conocimiento;methodology;knowledge representation;representation connaissances;clasificacion;regla inferencia;knowledge base	A distributed anticipatory system formulation of knowledge acquisition and transfer processes is presented which provides scientific foundations for knowledge engineering. The formulation gives an operational model of the notion of expertise and the role it plays in our society. It suggests that the basic cognitive system that should be considered is a social organization, rather than an individual. Computational models of inductive inference already developed can be applied directly to the social model. One practical consequence of the model is a hierarchy of knowledge transfer methodologies which defines the areas of application of the knowledge-engineering techniques already in use. This analysis clarifies some of the problems of expertise transfer noted in the literature, in particular, what forms of knowledge are accessible through what methodologies. The model is being used as a framework within which to extend and develop a family of knowledge-support systems to expedite the development of expert-system applications.		Brian R. Gaines	1987	International Journal of Man-Machine Studies	10.1016/S0020-7373(87)80081-0	knowledge base;biological classification;computer science;knowledge management;artificial intelligence;methodology;rule of inference	Arch	-23.883129203154624	-3.4167197251669577	166549
18408dec51cd9d25e27b90df8dbb6ada78078362	a declarative framework for matching iterative and aggregative patterns against event streams	event processing framework;corresponding execution model;real-time processing;declarative framework;effective pattern;language execution model;expressive language;event processing;important class;complex pattern;aggregative pattern;event stream	Complex Event Processing as well as pattern matching against streams have become important in many areas including financial services, mobile devices, sensor-based applications, click stream analysis, real-time processing in Web 2.0 and 3.0 applications and so forth. However, there is a number of issues to be considered in order to enable effective pattern matching in modern applications. A language for describing patterns needs to feature a well-defined semantics, it needs be rich enough to express important classes of complex patterns such as iterative and aggregative patterns, and the language execution model needs to be efficient since event processing is a real-time processing. In this paper, we present an event processing framework which includes an expressive language featuring a precise semantics and a corresponding execution model, expressive enough to represent iterative and aggregative patterns. Our approach is based on a logic, hence we analyse deductive capabilities of such an event processing framework. Finally, we provide an open source implementation and present experimental results of our	clickstream;complex event processing;declarative programming;iteration;iterative method;mobile device;open-source software;pattern matching;performance evaluation;prototype;real-time clock;real-time web;semantics (computer science);web 2.0	Darko Anicic;Sebastian Rudolph;Paul Fodor;Nenad Stojanovic	2011		10.1007/978-3-642-22546-8_12	real-time computing;computer science;complex event processing;data mining;database;programming language	DB	-32.67848399304599	2.7254348768878316	166714
ec5bb2bd0282e91a57b4af17ffa2e955c380c2f1	on uncertainty handling in plausible reasoning with conceptual graphs	dempster shafer theory of evidence;bayesian inference;information retrieval model;conceptual graph;production rule	Plausible reasoning with conceptual graphs (CG) presents special problems. This paper treats plausible reasoning as a problem of matching a query CG with a target CG and computing the likelihood of relevance of the target CG from the degree of match. While the emphasis is on information retrieval modeled as plausible reasoning, the approach is applicable to schema selection, casebased reasoning and inferencing with production rules which have a CG as their premise. Two levels of uncertainty handling are suggested. Micro-level analysis combines evidence from individual concept and relation node matches between the query CG and a target CG. Macro-level analysis combines evidence from multiple, possibly overlapping, fragments of the query CG that occur in different parts of the target CG. The use of the Dempster-Shafer theory of evidence and Bayesian inference for combining and propagating evidence is examined.	conceptual graph	Sung H. Myaeng;Christopher S. G. Khoo	1992		10.1007/3-540-57454-9_11	conceptual graph;dempster–shafer theory;machine learning;pattern recognition;data mining;mathematics;bayesian inference;statistics	AI	-19.483310271571796	0.19934010441959615	166736
b588d2f67c6d4adeacb7ff19932c5b404a993827	towards a comprehensive xml benchamrk		XML benchmarks are tools used for measuring and evaluating the performance of new XML developments such as XML/RDBMS/OO mapping techniques and XML storages. With XML benchmarks, the evaluation process is done by executing predefined query-set over the benchmark's dataset members; where the performance of the new development is compared against the performance of some existing techniques. Yet, none of the existing XML benchmarks seems directly investigated the effect of sought data location on the query performance. This research is an attempt to investigate the rationale of adding the Data Dimension (DD) to the 3D~XBench features. To achieve this, a new set of queries was added to the query-set of the 3D~XBench to test the effect of changing the location of the sought records. The preliminarily experimental results have shown that the query execution time is also driven by the location of the sought data in the underlying XML database; and therefore, the Data Dimension can be added to the existing features of the 3D	benchmark (computing);central processing unit;design rationale;directory traversal attack;information systems;input/output;run time (program lifecycle phase);schmidt decomposition;sorting;tree traversal;value (ethics);xml database;xml namespace	Fatima Al-Sedairi;Mohammed Al-Badawi;Abdallah Al-Hamdani	2015		10.5220/0005474101120117	binary xml;xml base;document structure description;xml schema editor;cxml;efficient xml interchange	DB	-33.21506162983395	1.6261522212287436	166959
9439383c56ddc5873d77b208e93658b4504896f2	an improved spatio-temporal index structure for predictive queries	database indexing;moving object;vctpr tree;query processing;query performance spatio temporal index structure predictive query spatial index structures velocity difference moving objects query execution vctpr tree;sorting;index structure;construction industry;trees mathematics database indexing query processing;spatial index;trees mathematics;satisfiability;data mining;databases laboratories communication system traffic control fuzzy systems intelligent structures computer vision educational technology computer science education spatial indexes wireless communication;indexing;spatial index structures;spatial databases;query execution;predictive query;moving objects;query performance;velocity difference;spatio temporal index structure	The TPR-tree is a popular index structure utilized to query the present or future positions of moving objects. Compared with the traditional spatial index structures, the TPR-tree is more appropriate to satisfy the requirements of storage and queries of moving objects. However, when a TPR-tree is constructed, the velocity difference of moving objects in the same direction hasn’t been considered. This will cause numerous invalid regions with lower efficiency for query execution. To solve this problem, we present an enhanced spatio-temporal index structure: VCTPR-tree, and it can avoid invalid regions by introducing velocity to change the time stamps thus improve the query performance.	abstract syntax tree;algorithm;database index;experiment;list of algorithms;requirement;spatial database;velocity (software development)	Meng Gao;Yingyuan Xiao	2009	2009 Sixth International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2009.231	database index;sargable;search engine indexing;computer science;sorting;data mining;database;spatial database;information retrieval;satisfiability	DB	-27.046599726901377	1.0861946282265162	167000
eaf2bccd82bf4cffcf1ef85487d6722f6a04716c	materialized view selection and maintenance using multi-query optimization	linkages among documents;t technology general;information retrieval;time window;query optimization;metasearch;distributed collection;materialized views;data warehouse	Materialized views have been found to be very effective at speeding up queries, and are increasingly being supported by commercial databases and data warehouse systems. However, whereas the amount of data entering a warehouse and the number of materialized views are rapidly increasing, the time window available for maintaining materialized views is shrinking. These trends necessitate efficient techniques for the maintenance of materialized views. In this paper, we show how to find an efficient plan for the maintenance of a set of materialized views, by exploiting common subexpressions between different view maintenance expressions. In particular, we show how to efficiently select (a) expressions and indices that can be effectively shared, by transient materialization; (b) additional expressions and indices for permanent materialization; and (c) the best maintenance plan — incremental or recomputation — for each view. These three decisions are highly interdependent, and the choice of one affects the choice of the others. We develop a framework that cleanly integrates the various choices in a systematic and efficient manner. Our evaluations show that many-fold improvement in view maintenance time can be achieved using our techniques. Our algorithms can also be used to efficiently select materialized views to speed up workloads containing queries and updates.	algorithm;database;emoticon;greedy algorithm;interdependence;materialized view;mathematical optimization;microsoft windows;query optimization;scalability;speedup	Hoshi Mistry;Prasan Roy;S. Sudarshan;Krithi Ramamritham	2001		10.1145/375663.375703	materialized view;query optimization;computer science;data warehouse;data mining;database;information retrieval	DB	-26.468226548057554	4.0265082283465325	167045
fbd2f25b0b8981c32021c56c1bf0a76a4004b342	machine learning and inductive logic programming for multi-agent systems	multiagent system;multi agent system;information retrieval;base connaissance;inductive logic programming;logical programming;apprentissage machine;artificial intelligent;machine learning;programmation logique;agent intelligent;intelligent agent;agent systems;base conocimiento;agente inteligente;learning artificial intelligence;sistema multiagente;programacion logica;systeme multiagent;knowledge base;apprentissage intelligence artificielle	Learning is a crucial ability of intelligent agents. Rather than presenting a complete literature review, we focus in this paper on important issues surrounding the application of machine learning (ML) techniques to agents and multi-agent systems (MAS). In this discussion we move from disembodied ML over single-agent learning to full multi-agent learning. In the second part of the paper we focus on the application of Inductive Logic Programming, a knowledge-based ML technique, to MAS, and present an implemented framework in which multi-agent learning experiments can be carried out.	inductive logic programming;machine learning;multi-agent system	Dimitar Kazakov;Daniel Kudenko	2001		10.1007/3-540-47745-4_11	robot learning;multi-task learning;instance-based learning;knowledge base;error-driven learning;algorithmic learning theory;inductive bias;statistical relational learning;computer science;artificial intelligence;machine learning;multi-agent system;inductive transfer;inductive programming;computational learning theory;active learning;intelligent agent;algorithm	AI	-23.2575888813322	-6.591671773431848	167225
47ae22db4b2b38d338cbdec0f597ae3de64ebafe	constraints and modalities in terminological knowledge representation systems			knowledge representation and reasoning	Armin Laux	1995				NLP	-27.838903748350283	-7.494314523413958	167270
1d951e515a84065b4606d9b3945ad289bbbdc83d	improving semantic compression specification in large relational database	relational properties;frequent pattern;database compression;standard compression techniques;semantic compression specification;relational databases data compression;attribute semantics;large relational database;semantic compression;massive data tables;table data compression;lossless semantic techniques;augmented vector quantisation coder;large scale relational databases;semantic compression specification lossless semantic techniques massive data tables augmented vector quantisation coder attribute semantics frequent pattern table data compression semantic compression relational properties standard compression techniques database compression large scale relational databases large relational database	The large-scale relational databases normally have a large size and a high degree of sparsity. This has made database compression very important to improve the performance and save storage space. Using standard compression techniques (syntactic) such as Gzip or Zip does not take advantage of the relational properties, as these techniques do not look at the nature of the data. Since semantic compression accounts for and exploits both the meanings and dynamic ranges of error for individual attributes (lossy compression); and existing data dependencies and correlations between attributes in the table (lossless compression), it is very effective for table-data compression. Inspired by semantic compression, this study proposes a novel independent lossless compression system through utilising data-mining model to find the frequent pattern with maximum gain (representative row) in order to draw attribute semantics, besides a modified version of an augmented vector quantisation coder to increase total throughput of the database compression. This algorithm enables more granular and suitable for every kind of massive data tables after synthetically considering compression ratio, space, and speed. The experimentation with several very large real-life datasets indicates the superiority of the system with respect to previously known lossless semantic techniques.	algorithm;data compression;data dependency;data mining;lossless compression;lossy compression;netbsd gzip / freebsd gzip;real life;relational database;semantic compression;sparse matrix;throughput;vector quantization	Saad Mohamed Darwish	2016	IET Software	10.1049/iet-sen.2015.0054	data compression;lossy compression;relational database;computer science;data mining;semantic compression;database;lossless compression;information retrieval	DB	-30.702081793953415	2.9504449735640357	167325
54fb9b09c2401b1ea7fd70aa26ab85e820592ee9	efficient processing of narrow range queries in multi-dimensional data structures	database indexing;text document multidimensional indexing narrow range query processing multidimensional data structures data mining;range query;text document multidimensional indexing;multidimensional data structures;query processing;tree data structures database indexing query processing;vector space;index structure;tree data structures;data mining;multi dimensional;data structures indexing multimedia databases extraterrestrial measurements information retrieval data mining relational databases content based retrieval biomedical imaging geography;indexation;multimedia data;narrow range query processing;vector data;data structure	Multi-dimensional data structures are applied in many real index applications, i.e. data mining, indexing multimedia data, indexing of text documents and so on. Many index structures and algorithms have been proposed. There are two major approaches to multi-dimensional indexing: data structures to indexing metric and vector spaces. R-trees, R*-trees and (B)UB-trees are representatives of the vector data structures. These data structures provide efficient processing of many types of queries, i.e. point queries, range queries and so on. As far as the vector data structures are concerned, the range query retrieves all points in defined hyper box in an n-dimensional space. The narrow range query is an important type of the range query. Its processing is inefficient in vector data structures. Moreover, the efficiency decreases as the dimension of the indexed space increases. We depict an application of the signature for more efficient processing of narrow range queries. The approach puts the signature into the multi-dimensional data structures like R-tree or UB-tree but original functionalities are preserved, i.e. the range query algorithm for general range query. The novel data structure is called the signature data structure, e.g., signature R-tree or signature UB-tree	algorithm;binary tree;curse of dimensionality;data mining;data structure;electronic signature;experiment;r-tree;range query (data structures);range query (database);relevance;tree (data structure);type signature;ub-tree	Michal Krátký;Václav Snásel;Jaroslav Pokorný;Pavel Zezula	2006	2006 10th International Database Engineering and Applications Symposium (IDEAS'06)	10.1109/IDEAS.2006.21	database index;range query;data structure;vector space;computer science;data mining;database;tree;programming language;information retrieval;spatial query	DB	-27.143577544327883	1.222798160559367	167636
80c490b3fa89c4144a0781d084dec2a0545eabd6	constraint propagation with interval labels	constraint propagation;execution time;plenitud;coaccion;contrainte;base connaissance;intelligence artificielle;constraint;propagacion;inferencia;artificial intelligence;temps execution;base conocimiento;etiqueta;etiquette;inteligencia artificial;completeness;tiempo ejecucion;label;completude;inference;propagation;knowledge base	Constramt propagatton ts often used tn AI systems to perform mference about quanttttes Thts paper studtes one parttcular kmd of constramt propagatzon, where quanttttes are labelled wtth stgns or wtth mtervals, and these labels are propagated through recorded constraints We review the uses of such mference schemes m AI systems of vartous kmds, and evaluate thetr strengths and weaknesses In parttcular, we determme the completeness and runnmg time of constratnt propagatton for vartous kmds of labels and constraints. 1. Quantity Knowledge Bases Much of the knowledge in many AI reasoning systems can be expressed as mathematical relations on real-valued quantities. For example, temporal reasoning involves relations on times and durations; spatial reasoning involves relations on lengths and angles; physical reasoning involves relations on masses, temperatures, energies, etc. Frequently, the maintenance of these relationships and the performing of inferences on them can be separated into a distinct module called a quantity knowledge base. A quantity knowledge base interacts with the rest of the system's inference mechanism in two ways. The system provides the quantity knowledge base with mathematical relations, either percewed or derived from nonmathematical inferences; and the quantity knowledge base uses these to infer new relations which it can report to the rest of the system. The problems of combinatorial explosion and search, which plague inference engines generally, take particularly fierce form in quantitative domains. Starting with any quantitative constraint, it is possible to deduce innumerable others. For instance, starting with the constraint 0 < a < b, we can mfer further constraints such as a<X/-~<1⁄2(a+b)<b; ea<eb; fo e-X2 dx < Jo b e -x2 dx . . . . Moreover, it is known that extremely complex and convoluted paths of inference are sometimes needed to draw desired conclusions from Arttfictal Intelhgence 32 (1987) 281-331 0004-3702/87/$3 50 © 1987, Elsevier Science Pubhshers B V (North-Holland)	capacitor plague;inference engine;knowledge base;local consistency;reasoning system;software propagation;spatial–temporal reasoning;temporal logic	Ernest Davis	1987	Artif. Intell.	10.1016/0004-3702(87)90091-9	knowledge base;etiquette;completeness;computer science;artificial intelligence;machine learning;constraint;label;algorithm;local consistency	AI	-20.34255481276566	-1.6938597212282815	167723
4607d214dd7e8a735ce014c60b0113ae9fdfd12c	modificated repertory grid procedure for attribute problem domains			problem domain;repertory grid	Iulian Secrieru	1994	The Computer Science Journal of Moldova		machine learning;discrete mathematics;mathematics;data mining;repertory grid;artificial intelligence	Theory	-28.278889338380132	-8.19538656088286	167744
f5052ac09569a9cb431e3e9177e8416812f94b49	the ucomp protege plugin for crowdsourcing ontology validation	ontology engineering;human computation;protege plugin;crowdsourcing	The validation of ontologies using domain experts is expensive. Crowdsourcing has been shown a viable alternative for many knowledge acquisition tasks. We present a Protégé plugin and a workflow for outsourcing a number of ontology validation tasks to Games with a Purpose and paid micro-task crowdsourcing.	crowdsourcing;human-based computation game;knowledge acquisition;ontology (information science);outsourcing;plug-in (computing);protégé;subject-matter expert	Florian Hanika;Gerhard Wohlgenannt;Marta Sabou	2014			crowdsourcing software development;computer science;knowledge management;data science;data mining;crowdsourcing	HCI	-32.97284029983616	-5.632479745289974	167805
0afa7671041479984d4451b895dc41bbd4db793a	a hybrid system for decision-making about assets in english divorce cases	rule based;hybrid system;problem solving;knowledge base	Abst rac t . A hybrid knowledge-based (combining rule-based and casebased material) system for settling the disposition of the matrimonial home after divorce, according to relevant English law, is described. Development of such a hybrid system is significant because on their own neither rules nor past cases are sufficient for problem-solving even in such a supposedly simple legal area. A multi-layered representation is used for the case material. In responding to a new case, the system can produce initial advice using its rule-base, which may be fuU-fiedged or partial advice, or indicate similar past cases and their decisions. The system is also designed to produce a similarity assessment to explore the best precedents to reach a final solution, taking into account the most relevant knowledge from either representation.	hybrid system;knowledge-based systems;logic programming;problem solving	Kamalendu Pal;John A. Campbell	1995		10.1007/3-540-60654-8_30	engineering;artificial intelligence;operations management;algorithm	AI	-21.24760215303037	-6.761494403280736	168100
6ecd17783e547a844699a4c566da5f1fe444f72a	buckets: smart objects for digital libraries	data base management systems;integrated library systems;database management systems;information retrieval;digital library;digital libraries;computer programs;protocol computers;world wide web;software development tools;smart object	"""BUCKETS: SMART OBJECTS FOR DIGITAL LIBRARIES Michael L. Nelson Old Dominion University, 2000 Director: Dr. Kurt Maly Current discussion of digital libraries (DLs) is often dominated by the merits of the respective storage, search and retrieval functionality of archives, repositories, search engines, search interfaces and database systems. While these technologies are necessary for information management, the information content is more important than the systems used for its storage and retrieval. Digital information should have the same long-term survivability prospects as traditional hardcopy information and should be protected to the extent possible from evolving search engine technologies and vendor vagaries in database management systems. Information content and information retrieval systems should progress on independent paths and make limited assumptions about the status or capabilities of the other. Digital information can achieve independence from archives and DL systems through the use of buckets. Buckets are an aggregative, intelligent construct for publishing in DLs. Buckets allow the decoupling of information content from information storage and retrieval. Buckets exist within the Smart Objects and Dumb Archives model for DLs in that many of the functionalities and responsibilities traditionally associated with archives are """"pushed down"""" (making the archives """"dumber"""") into the buckets (making them """"smarter""""). Some of the responsibilities imbued to buckets are the enforcement of their terms and conditions, and maintenance and display of their contents. These additional responsibilities come at the cost of storage overhead and increased complexity for the archived objects. However, tools have been developed to manage the complexity,"""	archive;computer data storage;coupling (computer programming);database;digital library;information management;information retrieval;library (computing);overhead (computing);self-information;smart objects;web search engine	Michael L. Nelson;Kurt Maly	2001	Commun. ACM	10.1145/374308.374342	digital library;digital asset management;computer science;database;world wide web;internet of things;information retrieval	DB	-32.05998801921421	-2.5059388965192952	168185
7ca7edbab977f5456d75154e931aa70c0b72b1be	intelligent case based machine translation system	apprentissage automatique;memoire;traitement automatique des langues naturelles;cas;regle;case base reasoning;learning model;linguistique appliquee;intelligence artificielle;traduction automatique;machine learning;interaction homme machine;artificial intelligence;strategie hybride;computational linguistics;rule;linguistique informatique;natural language processing;human machine interaction;machine translation;memory;rule based reasoning;applied linguistics	Interactive Hybrid Strategies Machine Translation (IHSMT) system has just been designed to solve the translation problems. It forms a nice interdependent cooperation relation between human and machine by interaction. The system achieves hybrid strategy translation by synthesizing the rule-based reasoning and case-based reasoning, and therefore overcomes the demerits of single strategy. This paper has done some work on learning mechanism of this system and proposes a learning model of human-machine tracking and memorizing (HMTM). This model can store the information of human-machine interaction into memory base as case of machine learning, and then gradually accumulate knowledge to improve the intelligence of MT system.	machine translation	Jian De Wang;Zhao-Xiong Chen;He Yan Huang	2001		10.1007/3-540-44686-9_21	natural language processing;transfer-based machine translation;example-based machine translation;computer science;artificial intelligence;computational linguistics;cas registry number;applied linguistics;linguistics;machine translation;memory;algorithm	NLP	-26.801560372443163	-8.228913856782716	168199
8e59f0a93e056ad595a8cfc6830eaf2f119c8530	continuous visible k nearest neighbor query on moving objects	safe region;spatio temporal databases;continuous visible k nearest neighbor queries;invisible time period	A visible k nearest neighbor (VkNN) query retrieves k objects that are visible and nearest to the query object, where “visible” means that there is no obstacle between an object and the query object. Existing studies on the VkNN query have focused on static data objects. In this paper we investigate how to process the query on moving objects continuously. We propose an effective filtering-andrefinement framework for evaluating this type of queries. We exploit spatial proximity and visibility properties between the query object and data objects to prune search space under this framework. A detailed cost analysis and a comprehensive experimental study are conducted on the proposed framework. The results validate the effectiveness of the pruning techniques and verify the efficiency of the proposed framework. The proposed framework outperforms a straightforward solution by an order of magnitude in terms of both communication and computation costs.	computation;experiment;k-nearest neighbors algorithm;nearest neighbor search	Yanqiu Wang;Rui Zhang;Chuanfei Xu;Jianzhong Qi;Yu Gu;Ge Yu	2014	Inf. Syst.	10.1016/j.is.2014.02.003	sargable;query optimization;computer science;pattern recognition;data mining;database;spatial query	DB	-25.321937972317563	0.18632978075145062	168245
c1e7cc1ea89982d386785514c4c31393828f1232	the role of essential explanation in abduction	abduction;diagnosis;knowledge based systems	The abduction task is to infer the best explanation for a given set of data. One common subtask of abduction is to synthesize the best composite explanatory hypothesis from elementary hypotheses retrieved from memory. The synthesis of best composite explanations, however, is computationally costly. One general approach to controlling the computational cost of synthesizing explanations is to decompose the synthesis search space into smaller spaces that can be searched more efficiently and effectively. The essential hypotheses, that is, the hypotheses that are the only available explanations for specific subsets of the data set, provide one such decomposition. In this method, first the essential hypotheses are included in the composite explanation, and, then, non-essential hypotheses are included to account for the remaining unexplained data elements. In addition to providing a more efficient method for synthesizing composite explanations, this decomposition leads to the formation of more parsimonious explanations. In this paper, we report on a set of experiments in the domain of medical data interpretation that demonstrates that the essential/non-essential decomposition of the abduction search space results in more efficient synthesis of more parsimonious composite explanations.	abductive reasoning	Olivier Fischer;Ashok K. Goel;John R. Svirbely;Jack W. Smith	1991	Artificial Intelligence in Medicine	10.1016/0933-3657(91)90010-9	computer science;artificial intelligence;knowledge-based systems;machine learning;medical diagnosis;algorithm	AI	-20.215598906301015	0.9858574478183152	168262
46c98f7e4242c06338c9e12aabefc2c2db600abe	interaction with a mixed-initiative system for exploratory data analysis	statistical software;mixed initiative;artificial intelligent;statistical analysis;data exploration;experimental evaluation;ai planning;exploratory data analysis	Abstract   Exploratory data analysis (EDA) plays an increasingly important role in statistical analysis. EDA is difficult, however, even with the help of modern statistical software. We have developed an assistant for data exploration, based on Al planning techniques, that addresses some of the strategic shortcomings of conventional software. This paper describes the design and behavior of the system and discusses an experimental evaluation that demonstrates the effectiveness of our approach.		Robert St. Amant;Paul R. Cohen	1998	Knowl.-Based Syst.	10.1016/S0950-7051(97)00038-5	automated planning and scheduling;simulation;computer science;artificial intelligence;data science;data mining;exploratory data analysis;statistics	DB	-32.93081975496022	-9.81669901580317	168354
3fd34048e06f9298981b9ac2e1df5a1cbcb549c5	interaction between requesters and a large mechanized retrieval system		In a large mechanized retrieval system, a certain proportion of the search failures are likely to be directly attributable to inadequate user-system interaction. Request statements may be only distant approximations of actual information requirements. This paper discusses some of the problems of the usersystem interface and suggests methods whereby these problems may be alleviated.		F. W. Lancaster	1968	Information Storage and Retrieval	10.1016/0020-0271(68)90024-7	information retrieval;computer science;data mining	Web+IR	-22.811785087372726	2.0665771383231903	168446
e6b1c01d4d1443a492913f044d270513a71b6c41	demonstration of smoke: a deep breath of data-intensive lineage applications		Data lineage is a fundamental type of information that describes the relationships between input and output data items in a workflow. As such, an immense amount of data-intensive applications with logic over the input-output relationships can be expressed declaratively in lineage terms. Unfortunately, many applications resort to hand-tuned implementations because either lineage systems are not fast enough to meet their requirements or due to no knowledge of the lineage capabilities. Recently, we introduced a set of implementation design principles and associated techniques to optimize lineage-enabled database engines and realized them in our prototype database engine, namely, Smoke. In this demonstration, we showcase lineage as the building block across a variety of data-intensive applications, including tooltips and details on demand; crossfilter; and data profiling. In addition, we show how Smoke outperforms alternative lineage systems to meet or improve on existing hand-tuned implementations of these applications.	data-intensive computing;database engine;input/output;lineage (evolution);prototype;requirement;tooltip	Fotis Psallidas;Eugene Wu	2018		10.1145/3183713.3193537	database;implementation;data mining;computer science;database engine;data profiling;design elements and principles;data lineage;input/output;workflow	DB	-33.224088782808806	1.5390420748694114	168544
6eaa86849aba00f2058c9627d32618d736c2ba9f	accord: a metamodel for the ii generation expert systems	expert system		expert system;metamodeling	Boris Petkoff	1988			natural language processing;computer science;artificial intelligence;metamodeling;data mining;expert system;legal expert system	AI	-28.426563679056624	-7.287212939362827	168584
32212182865a19a2c2bb088117611f3a2ebf3bbe	a dynamic and scalable agent-based approach for knowledge discovery : web site exploration.	agent based;knowledge discovery			Aurelio López-López;Alberto Méndez Torreblanca	2003			software mining;computer science;knowledge management;artificial intelligence;data science;data mining;knowledge extraction	ML	-33.69253361292087	-5.632650746545557	168616
09b80aaf2b92705aa1663bbddff772d884368e8d	"""special issue on """"agro-environmental decision support systems"""""""			decision support system	François Pinet;Sandro Bimonte;André Miralles;Florence Le Ber	2015	Ecological Informatics	10.1016/j.ecoinf.2015.11.003	r-cast;decision support system;intelligent decision support system;decision analysis;decision engineering;business decision mapping	ECom	-30.865155170677916	-9.248699220499448	168716
02a5b485806146f01896a90129506b154404cd6b	structured system of concepts for storing, retrieving, and manipulating chemical information	concept;ontologie;base donnee;etude theorique;manipulacion dato;normalisation;database;base dato;systeme structure;chimie;documentacion;chemistry;normalizacion;estudio teorico;quimica;data handling;theoretical study;maniement donnee;standardization;documentation;concepto		cheminformatics	Paul E. van der Vet;Nicolaas J. I. Mars	1993	Journal of Chemical Information and Computer Sciences	10.1021/ci00014a007	documentation;computer science;artificial intelligence;concept;standardization;algorithm	Theory	-25.901752099500268	-1.9995285179066558	168794
2e15e198247ca7bdac3a7da91e7d326cd11e0ffc	software quality in use characteristic mining from customer reviews	software reviews;opinion mining;electronic commerce;software engineer;information characteristic extraction;information decision making;software reviews customer satisfaction data mining decision making electronic commerce information retrieval internet knowledge based systems ontologies artificial intelligence pattern classification product quality software quality;information retrieval;e commerce;iso standards;rule based;data mining;user effectiveness;software engineering;customer review mining;ontologies artificial intelligence;customer satisfaction;software product quality;product review representation;software product acquisition;ontologies data mining software quality productivity iso standards safety;internet;online download market;safety;pattern classification;software quality ontology;ontologies;iso 9126;productivity;user safety;point of view;product quality;ontology construction;quality model;rule based classification;user satisfaction;ontology;rule based classification opinion mining ontology software quality;software quality;knowledge based systems;rule based classification customer review mining information decision making software product acquisition e commerce web site online download market information characteristic extraction product review representation software product quality iso 9126 software engineer user effectiveness user satisfaction user safety software quality ontology;web site	Reviews from customers who have experience with the software product are an important information decision making for software product acquisition. They usually appear on ecommerce websites or any online download market. If some products have a large number of reviews, customer may not have time to read all of them. Therefore, we need to extract software information characteristic from reviews in order to provide product review representation. Customer can further use it to compare one software product attributes and other products' attributes. Software product quality from user point of view may be used to characterize each software product. ISO 9126 is widely used among software engineer to assess software quality in use. It covers software quality model and contains the quality model characteristic from user perspective: effectiveness, productivity, safety and satisfaction. We propose a methodology for software product reviews mining based on software quality ontology constructed from ISO 9126 and a rule-based classification to finally produce software quality in use scores for software product Representation. The quality in use score for each software characteristic can be used to preliminary determine the quality of the software.	download;e-commerce;iso/iec 9126;logic programming;software engineer;software quality	Warit Leopairote;Athasit Surarerks;Nakornthip Prompoon	2012	2012 Second International Conference on Digital Information and Communication Technology and it's Applications (DICTAP)	10.1109/DICTAP.2012.6215397	e-commerce;iso/iec 9126;software review;personal software process;medical software;verification and validation;productivity;the internet;software quality management;software sizing;computer science;knowledge management;ontology;package development process;software development;software design description;software construction;data mining;database;software technical review;software walkthrough;customer satisfaction;software deployment;software quality control;software quality;software metric;software quality analyst;software peer review	SE	-28.50073501221065	-3.943770873172913	168883
d439d85601e7779c8456ba324065d59e862b0cfc	constructing bodies and their qualities from observations	complex quality;common experiential ground;semantically annotated data;incompatible information ontology;incompatible view;information semantics;formal primitive;constructing bodies;inter-subjective experience;jeremy bentham;information ontology;symbol grounding	The principle challenge for information semantics lies in the degrees of freedom to interpret symbols in terms of thoughts and experiences which leads to incompatible views on the world. Consequently, incompatible information ontologies and interpretations of the described data will remain. Even though there is usually a common experiential ground, it stays often unknown to users of semantically annotated data. This symbol grounding problem is a bottleneck of information semantics, which remains largely unsolved in ontological practice. In this paper, we suggest – in the spirit of Jeremy Bentham – to introduce formal primitives which are directly grounded in inter-subjective experience, and which serve to expose and construct complex qualities in information ontologies.	boolean satisfiability problem;entity;experience;first-order logic;first-order predicate;information retrieval;intersubjectivity;neuron;ontology (information science);ostensive definition;semantic integration;semantic interpretation;semantics (computer science);turing completeness;universality probability;zermelo–fraenkel set theory	Simon Scheider;Florian Probst;Krzysztof Janowicz	2010		10.3233/978-1-60750-535-8-131	philosophy;epistemology;computer science;artificial intelligence;mathematics	AI	-19.56215522353406	3.376351788665658	168971
b07df8aa6e7cfbdbdc77a8377b4d8f63f1f3b365	semi-indexing semi-structured data in tiny space	semi index;succinct data structures;succinct data structure;general techniques;semi structured data;indexation	"""Semi-structured textual formats are gaining increasing popularity for the storage of document collections and rich logs. Their flexibility comes at the cost of having to load and parse a document entirely even if just a small part of it needs to be accessed. For instance, in data analytics massive collections are usually scanned sequentially, selecting a small number of attributes from each document. We propose a technique to attach to a raw, unparsed document (even in compressed form) a """"semi-index"""": a succinct data structure that supports operations on the document tree at speed comparable with an in-memory deserialized object, thus bridging textual formats with binary formats. After describing the general technique, we focus on the JSON format: our experiments show that avoiding the full loading and parsing step can give speedups of up to 12 times for on-disk documents using a small space overhead."""	binary file;bridging (networking);experiment;in-memory database;json;linear algebra;overhead (computing);parsing;partial template specialization;semi-structured data;semi-supervised learning;semiconductor industry;succinct data structure;xpath	Giuseppe Ottaviano;Roberto Grossi	2011		10.1145/2063576.2063790	succinct data structure;computer science;data mining;database;world wide web;information retrieval	DB	-32.18966208944987	1.6793885061765523	169087
24333fef73b1c977a660b97e4ff53f58a8ca6fdc	benchmarking using basic dbms operations	decision support;system performance;data model;indexation;generalization capability	The TPC-H benchmark proved to be successful in th e decision support area. Many commercial database vendors and their related hardware vendors used these benchmarks to show the superiori ty and competitive edge of their products. However, over time, the TPC-H becam e less representative of industry trends as vendors keep tuning their databa se to this benchmark-specific workload. In this paper, we present XMarq, a simpl e benchmark framework that can be used to compare various software/hardwa re combinations. Our benchmark model is currently composed of 25 queries that measure the performance of basic operations such as scans, aggr egations, joins and index access. This benchmark model is based on the TPC-H d ata model due to its maturity and well-understood data generation capabi lity. We also propose metrics to evaluate single-system performance and c ompare two systems. Finally we illustrate the effectiveness of this mod el by showing experimental results comparing two systems under different condi tions.	benchmark (computing);capability maturity model;decision support system;ibm tivoli storage productivity center	Alain Crolotte;Ahmad Ghazal	2010		10.1007/978-3-642-18206-8_15	sdet;simulation;computer science;data mining;database	DB	-33.34111271986853	-0.45216956104465006	169100
0e96a526d43f88f2f41c3d1a0d19c0c691c853fa	the fieldtree: a data structure for geographic information systems	range query;geographic information system;spatial database;semantic information;access method;data structure;spatial access method;database query	Efficient access methods, such as indices, are indispensable for the quick answer to database queries. In spatial databases the selection of an appropriate access method is particularly critical since different types of queries pose distinct requirements and no known data structure outperforms all others for all types of queries. Thus, spatial access methods must be designed for excelling in a particular kind of inquiry while performing reasonably in the other ones. This article describes the Fieldtree, a data structure that provides one of such access methods. The Fieldtree has been designed for GIS and similar applications, where range queries are predominant and spatial nesting and overlaping of objects are common. Besides their hierarchical organization of space, Fieldtrees are characterized by three other features:(i) they subdivide space regularly, (ii) spatial objects are never fragmented, and (iii) semantic information can be used to assign the location of a certain object in the tree. Besides describing the Fieldtree this work presents analytical results on several implementations of those variants, and compares them to published results on the Rtree and the R +tree.	data structure;geographic information system	Andrew U. Frank;Renato Barrera	1989		10.1007/3-540-52208-5_20	local information systems;range query;database theory;object-based spatial database;query expansion;data structure;database tuning;computer science;geospatial analysis;data mining;database;geographic information system;programming language;web search query;view;access method;spatial database;information retrieval;database design;query language;spatial query	Theory	-27.35015885749186	2.29706049771026	169272
b0a6cb6f338d3aaf1e2e0a63d7b84ba6db018e81	editorial: artificial intelligence in medicine and medical decision making europe	medical decision making;artificial intelligent		artificial intelligence;medical decision making	Werner Horn	2000	Artificial Intelligence in Medicine	10.1016/S0933-3657(00)00049-X	clinical decision support system;r-cast;intelligent decision support system;computer science;artificial intelligence;management science;operations research	AI	-30.895179379345645	-9.174537785686834	169578
6ec85f475bdccafb0d2b3bb8423d7522f02c4060	the commonsense algorithm as a basis for computer models of human memory, inference, belief and contextual language comprehension	language comprehension;human memory;human cognition;computer model;information processing;data structure	The notion of a commonsense algorithm is presented as a basic data structure for modeling human cognition. This data structure unifies many current ideas about human memory and information processing. The structure is defined by specifying a set of proposed cognitive primitive links which, when used to build up large structures of actions, states, statechanges and tendencies, provide an adequate formalism for expressing human plans and activities, as well as general mechanisms and computer algorithms. The commonsense algorithm is a type of framework (as Minsky has defined the term) for representing algorithmic processes, hopefully the way humans do.	algorithm;alice and bob;automatic programming;causality;cognition;computer;data structure;electronic circuit;grinberg's theorem;hoc (programming language);information processing;list comprehension;prototype;rs-232;scan line;semantics (computer science);simulation;tom;user story	Chuck Rieger	1975		10.3115/980190.980239	natural language processing;commonsense reasoning;computer science;artificial intelligence;commonsense knowledge;communication	AI	-20.25157917313938	2.9468338777544525	169927
7e264e56d6075ea7da19ad4f74ee95139e1a08a8	reflections on a medical ontology	modelizacion;interfase usuario;representacion conocimientos;ontologie;aplicacion medical;user interface;ontology development;modelisation;philosophy;representation connaissance;design rationale;ontologia;interface utilisateur;medical application;medical ontology;knowledge representation;filosofia;modeling;philosophie;ontology;application medicale;work practice;knowledge engineering	In this paper we confront the divide between the ontologies developed from the requirement of comprehensive and general domain coverage and those devised to meet application-specific requirements. While the generalists typically attach philosophical sophistication to their approach, in supposed contrast to the narrow remit chosen by the application-bound knowledge engineers, we would like to indicate that the latter practice can often reflect a multi-faceted rationale, nuanced by the requirements of the domain. We demonstrate how the necessity of placing ontology-based systems with the work-practices of domain experts introduces unique demands on design rationales and enforces, often implicitly, a philosophical assessment of the necessary concepts and relations that balance the generality and specificity. Such demands are not addressed by generic approaches to modelling the reality of a domain. Indeed, we articulate the philosophical and practical considerations that we have taken into account when developing an application-specific ontology. We would certainly hope that our experiences can be of help to the development of ontologies in similar applications.	amiga reflections	Bo Hu;Srinandan Dasmahapatra;David Dupplaw;Paul H. Lewis;Nigel Shadbolt	2007	International Journal of Man-Machine Studies	10.1016/j.ijhcs.2007.02.005	systems modeling;human–computer interaction;computer science;knowledge management;artificial intelligence;knowledge engineering;ontology;data mining;user interface;design rationale	Arch	-24.695285128420952	-3.6600050071468098	170007
9c90cde5c28ce2183a6343da548036c1d3a01d32	transaction system support for scientific data management.	scientific data management	Problem: Scientific instruments and simulations create huge amount of data, which needs to be processed and managed to extract the information contained. The volume of data almost doubles every year, and the precision of the instruments improve year over year as well. This calls for more sophisticated data management tools which allow the scientists to analyze the data to find specific information they need, or to find trends and anomalies in the data. While relational databases have been very successful in the commercial world, the scientists still store the data in the flat-file formats, and process them in ad-hoc manner. The scientific data require different kinds of access patterns, such as, spatial or temporal access patterns, that are not properly supported in the transaction oriented systems. Even if they do support these new access methods, the performance is not acceptable for the scientists. In the commercial world, column-based databases have been proposed to replace the row-based relational systems for large scale analytical databases. The column-based databases represent the data in a very compact way, using compression and similarity in the data columns. They also scale well with the number of columns in the table, which many of the scientific databases typically have. The column-based databases show great potential to solve many of the scientific database problems, but, they have not been studied by the database community. One of the important problem column-databases face in scientific data management is the lack of an efficient compression scheme to reduce the size of arbitrary precision column values. With the compression scheme, we will be able to achieve performance improvement in the same magnitude as the commercial database systems. Project: In this project, the student will study the implementation of an open source column-based database system and study different compression schemes applicable to arbitrary precision data. The student would integrate the compression scheme with the code, and study the performance of the system for different scientific data management problems. Plan: 1. Study and identify compression schemes suitable for scientific database management systems. 2. Study the code for existing column-based database management system. 3. Integrate the code for the compression scheme with the database code. 4. Evaluate the performance of the system against existing approaches and other schemes. 5. Suggest alternative data organization for scientific databases in column-oriented databases.	arbitrary-precision arithmetic;column (database);flat file database;hoc (programming language);open-source software;relational database;simulation	Michael A. Olson	1993			data management;data management plan;computer science;enterprise data management	DB	-30.849036701012597	1.1913153137889732	170380
28dacd804f467673a00082aef7f71770e1e9d559	adaptation guided retrieval based on formal concept analysis	poetry;raisonnement base sur cas;text;razonamiento fundado sobre caso;on line;en linea;metodo formal;query formulation;conceptual analysis;methode formelle;formulacion pregunta;intelligence artificielle;texte;analisis conceptual;formulation question;formal method;poesie;artificial intelligence;en ligne;profitability;inteligencia artificial;analyse conceptuelle;case based reasoning;texto;formal concept analysis;poesia	In previous papers [5, 4] we have proved the usefulness of Formal Concept Analysis (FCA) as an inductive technique that elicits knowledge embedded in a case library. The dependency knowledge implicitly contained in the case base is captured during the FCA process in the form of dependence rules among the attributes describing the cases. A substitution-based adaptation process is proposed that profits from these dependence rules since substituting an attribute may require to substitute dependant attributes. Dependence rules will guide an interactive query formulation process which favors retrieving cases where successful adaptations can be accomplished. In this paper we exemplify the use of FCA to help query formulation in an application to generate Spanish poetry versions of texts provided by the user.	formal concept analysis	Belén Díaz-Agudo;Pablo Gervás;Pedro A. González-Calero	2003		10.1007/3-540-45006-8_13	poetry;formal methods;computer science;formal concept analysis;artificial intelligence;algorithm	Logic	-24.01283834439423	-2.7650933375266846	170480
37cb4f144a6c401a399ae775e389680542a8c61a	stale view cleaning: getting fresh answers from stale materialized views		Materialized views (MVs), stored pre-computed results, are widely used to facilitate fast queries on large datasets. When new records arrive at a high rate, it is infeasible to continuously update (maintain) MVs and a common solution is to defer maintenance by batching updates together. Between batches the MVs become increasingly stale with incorrect, missing, and superfluous rows leading to increasingly inaccurate query results. We propose Stale View Cleaning (SVC) which addresses this problem from a data cleaning perspective. In SVC, we efficiently clean a sample of rows from a stale MV, and use the clean sample to estimate aggregate query results. While approximate, the estimated query results reflect the most recent data. As sampling can be sensitive to long-tailed distributions, we further explore an outlier indexing technique to give increased accuracy when the data distributions are skewed. SVC complements existing deferred maintenance approaches by giving accurate and bounded query answers between maintenance. We evaluate our method on a generated dataset from the TPC-D benchmark and a real video distribution application. Experiments confirm our theoretical results: (1) cleaning an MV sample is more efficient than full view maintenance, (2) the estimated results are more accurate than using the stale MV, and (3) SVC is applicable for a wide variety of MVs.	aggregate data;aggregate function;approximation algorithm;benchmark (computing);data general eclipse mv/8000;ibm tivoli storage productivity center;materialized view;plasma cleaning;precomputation;sampling (signal processing)	Sanjay Krishnan;Jiannan Wang;Michael J. Franklin;Kenneth Y. Goldberg;Tim Kraska	2015	PVLDB	10.14778/2824032.2824037	real-time computing;computer science;data mining;database	DB	-31.696451225787616	-0.08999568690772329	170493
2490b30311e62f00383eaca17b9b619a29bbb7ef	manhattan based hybrid semantic similarity algorithm for geospatial ontologies	conceptual contexts;spatial relations;euclidean distance;manhattan distance;hybrid model;dimensions;geospatial information retrieval	The interest on the geo-spatial information system is increasing swiftly, which leads to the development of the competent information retrieval system. Among the several semantic similarity models, the existing models such as Geometric Model characterizes the geo-spatial concept using their dimensions (i.e. properties) and the Network Model, using their spatial relations which has yielded less precision. For retrieving the geo-spatial information efficiently, the dimensions and the spatial relations between the geo-spatial concepts must be considered. Hence this paper proposes the Hybrid Model which is the concoction of the Geometric Model's dimensions and the Network Model's relations using the Manhattan distance method for computing semantic distance between geo-spatial query concept and the related geo-spatial concept in the data sources. The results and analysis illustrates that the Hybrid Model using Manhattan distance method could yield better precision, recall and the relevant information retrieval. Further the Manhattan Based Similarity Measure (MBSM) algorithm is proposed which uses the Manhattan Distance Method for computing the semantic similarity among the geo-spatial concepts which yields 10% increase in precision compared to the existing semantic similarity models.	algorithm;ontology (information science);semantic similarity	K. Saruladha;E. Thirumagal;J. Arthi;G. Aghila	2013		10.1007/978-3-319-03599-4_1	semantic similarity;computer science;theoretical computer science;machine learning;data mining;euclidean distance	AI	-30.14767828571068	-2.8559900107049887	170644
dbc93a50ac3dc3f2bce0de403d2ba11e29697f5b	efficient location-aware web search	information retrieval and web search;parallelism;indexing;data structures;buffering	Mobile search is quickly becoming the most common mode of search on the internet. This shift is driving changes in user behaviour, and search engine behaviour. Just over half of all search queries from mobile devices have local intent, making location-aware search an increasingly important problem. In this work, we compare the efficiency and effectiveness of two general types of geographical search queries, range queries and k nearest neighbor queries, for common web search tasks. We test state-of-the-art spatial-textual indexing and search algorithms for both query types on two large datasets. Finally, we present a rank-safe dynamic pruning algorithm that is simple to implement and use with current inverted indexing techniques. Our algorithm is more efficient than the tightly coupled best-in-breed hybrid indexing algorithms that are commonly used for top-k spatial textual queries, and more likely to find relevant documents than techniques derived from range queries.	location awareness;mobile device;range query (data structures);search algorithm;surface web;web search engine;web search query	Joel Mackenzie;Farhana Murtaza Choudhury;J. Shane Culpepper	2015		10.1145/2838931.2838933	beam search;search engine indexing;web query classification;mobile search;metasearch engine;data structure;semantic search;beam stack search;computer science;data buffer;phrase search;data mining;database;incremental heuristic search;best-first search;search analytics;web search query;queries per second;world wide web;information retrieval;search engine;search algorithm	Web+IR	-33.33812062235738	2.8018249978670835	170969
905af7bde43b94dc21b208f06e27ebf85549ad83	a clustering algorithm for hierarchical structures	storage allocation;page faults;clustering techniques;hierarchical structures;hierarchical model;access models;database design	The problem of determining how to store a hierarchic structure in order to minimize the expected access time to it is examined. A paging environment is assumed. The solution space considered is the set of partitions of the hierarchic structure, each partition being stored in heirarchical order. A very fast algorithm which determines the optimal partition of the tree is described. The algorithm has been used to determine the best partition of an IMS type tree into data set groups as well as to evaluate the cost of different alternatives. Actual measurements against the restructured databases have shown the validity of the model used by this method. The measurements have also shown that selecting the wrong choice of clustering instead of the optimal one may substantially increase the expected access time.	access time;algorithm;cluster analysis;database;experiment;feasible region;information management system (ims);lagrangian relaxation;loss function;paging	Mario Schkolnick	1977	ACM Trans. Database Syst.	10.1145/320521.320531	data mining;hierarchical clustering of networks;nearest-neighbor chain algorithm;correlation clustering;cure data clustering algorithm;canopy clustering algorithm;single-linkage clustering;hierarchical clustering;computer science;pattern recognition;brown clustering;artificial intelligence	DB	-26.81053738855824	4.006264590507501	171014
0bebaca93a7c4674571416b791c50b427515e7d2	interleaving planning and robot execution for asynchronous user requests	continuous planning;robotic agents;execution monitoring;replanning	ROGUE is an architecture built on a real robot which provides algorithms for the integration of highlevel planning, low-level robotic execution, and learning. R OGUE addresses successfully several of the challenges of a dynamic office gopher environment. This article presents the techniques for the integration of planning and execution. ROGUEuses and extends a classical planning algorithm to create plans for multiple interacting goals introduced by asynchronous user requests. R OGUE translates the planner’s actions to robot execution actions and monitors real world execution. ROGUE is currently implemented using the PRODIGY4.0 planner and the Xavier robot. This article describes how plans are created for multiple asynchronous goals, and how task priority and compatibility information are used to achieve appropriate efficient execution. We describe how R OGUE communicates with the planner and the robot to interleave planning with execution so that the planner can replan for failed actions, identify the actual outcome of an action with multiple possible outcomes, and take opportunities from changes in the environment. ROGUErepresents a successful integration of a classical artificial intelligence planner with a real mobile robot.	advanced telecommunications computing architecture;autonomous agent;autonomous robot;book;contingency plan;experience;forward error correction;interaction;interrupt;machine learning;multi-user;noon state;norm (social);prodigy;rogue;sensor;testbed	Karen Zita Haigh;Manuela M. Veloso	1998	Auton. Robots	10.1023/A:1008817110013	embedded system;real-time computing;simulation;computer science	AI	-19.40404609892032	-8.57817603418414	171029
d301ebc2107796979914ca54516f7029046347d5	on modeling of linguistic information using random sets	linguistique;langage naturel;semantics;raisonnement;semantique;modelisation;natural language;systeme expert;reasoning;knowledge representation;representation connaissances;modeling;random set;expert system;linguistics	This paper discusses the formal connection between possibility distributions (Zadeh [21]) and the theory of random sets via Choquet’s theorem. Based upon these relationships, it is suggested that plausible inferences and modeling of common sense can be derived from the statistics of random sets. The analysis of subjectivity in meaning representation of natural languages can be carried out by taking account of covariates of individuals as in the statistical analysis of survival data.	natural language	Hung T. Nguyen	1984	Inf. Sci.	10.1016/0020-0255(84)90052-5	natural language processing;systems modeling;computer science;artificial intelligence;mathematics;semantics;natural language;expert system;reason;algorithm	Theory	-20.505018780423665	-1.2750221993519553	171129
8833909325a2e164f4fa6dcbbeeef8516f3a2433	intermediate depth representations	basic level;qualitative disease history;qualitative superposition;category theory;qualitative reasoning;knowledge representation;multilevel model;deep knowledge	The limitations of shallow representations have in part driven AI researchers to focus on deeper representations of knowledge. While deep representations solve some problems, they come at a computational cost. This paper focuses on the computational and representational advantages that may exist in using representations whose depth is intermediate between shallow and deep. The Roschian notion of basic level categories is used to help develop the notion of the cognitively most economic representation. For medical diagnostic systems that reason about time varying aspects of disease, it is proposed that qualitative disease histories are a good intermediate representation, lying between shallow disease patterns and deeper qualitative models. Since no single representation will provide complete coverage of a problem domain, this paper further considers how one could construct, in a principled way, a reasoning system that uses multiple representations. Measures of intra- and inter-representational adequacy are proposed to define the optimal level of such a knowledge base for a given problem. These measures define the trade-offs that occur when using a particular representational level, and the conditions under which a reasoner can decide to switch representations. As an example, the formal relationships between histories and the qualitative models that produce them are shown to define conditions that can be used by a reasoning system to switch from histories to deeper models.		Enrico W. Coiera	1992	Artificial Intelligence in Medicine	10.1016/0933-3657(92)90011-D	knowledge representation and reasoning;qualitative reasoning;computer science;artificial intelligence;multilevel model;machine learning;algorithm;category theory	AI	-19.286197689073838	2.384516895075755	171183
569ce35fff1832c31c8ce3edaf0a446f30343350	fuzzy logics and artificial intelligence	artificial intelligent;fuzzy logic;approximate reasoning;possibilistic logic;artificial intelligence	Abstract   A short overview of artificial intelligence and its relationship with fuzzy logic is provided. We emphasize the role fuzzy logics can play in extending some of the models of Artificial Intelligence.	artificial intelligence;fuzzy logic	Ronald R. Yager	1997	Fuzzy Sets and Systems	10.1016/S0165-0114(97)00086-9	fuzzy logic;t-norm fuzzy logics;fuzzy electronics;adaptive neuro fuzzy inference system;fuzzy classification;computer science;artificial intelligence;neuro-fuzzy;machine learning;computational intelligence;mathematics;symbolic artificial intelligence;automated reasoning;artificial intelligence, situated approach;fuzzy set operations;algorithm;intelligent control	AI	-27.355423568643204	-8.573774663127129	171309
48c3958a6775711714d201e284524bbef04b5486	on the storage and retrieval of continuous media data	dimension alignment;spatial data;sql;continuous media;temporal data;object identity;audio video;time cursor;spatio temporal data;object relational databases;file system;pattern matching;parametric data;active databases	Continuous media applications, which require a guaranteed transfer rate of the data, are becoming an integral part of daily computational life. However, conventional file systems do not provide rate guarantees, and are therefore not suitable for the storage and retrieval of continuous media data (e.g., audio, video). To meet the demands of these new applications, continuous media file systems, which provide rate guarantees by managing critical storage resources such as memory and disks, must be designed. In this paper, we highlight the issues in the storage and retrieval of continuous media data. We first present a simple scheme for concurrently retrieving multiple continuous media streams from disks. We then introduce a a clever allocation technique for storing continuous media data that eliminates disk latency and thus, drastically reduces RAM requirements. We present, for video data, schemes for implementing the operations fast-forward, rewind and pause. Finally, we conclude by outlining directions for future research in the storage and retrieval of continuous media data.	fast forward;floor and ceiling functions;hard disk drive;media controls;random-access memory;requirement	Banu Özden;Rajeev Rastogi;Abraham Silberschatz	1994		10.1145/191246.191303	sql;real-time computing;computer science;theoretical computer science;pattern matching;data mining;database;spatial analysis;temporal database;world wide web;parametric statistics;information retrieval	OS	-27.08517619780249	1.4728098358685306	171534
624452a6c984f8e611ad4e49180aae85209d5aed	a knowledge-based approach for index selection in relational databases	index selection;relational database;statistical properties;database administrator;indexation;relational database management system;knowledge base	DINNER is a knowledge-based tool that assists the administrator of a relational database in the selection of index configurations. Given a set of tables, their statistical properties, and a set of queries on these tables, DINNER recommends an index configuration that includes for each table a primary index and a set of secondary indexes. Although it was proved that the problem is NP-hard, DINNER is capable of handling a practically-useful number of queries (10 queries, half of which are join queries).#R##N##R##N#The database can perform a query with several possible access paths, according to the available indexes. DINNER builds for each query a graph that represents the set of possible solutions (indexes) to be used by the query. For each solution it finds the access path that the database is most likely to choose to perform the query, and estimates the time it takes to perform the access path.#R##N##R##N#In order to find the possible indexes, the access paths that use these indexes, and the time cost of the access paths, DINNER uses knowledge that was elicited from several sources (database administrator (DBA), literature), and represented using several AI representation methods (frames, rules, and logic).#R##N##R##N#After finding the possible indexes, DINNER uses several heuristics to eliminate bad solutions. Then, solutions for all queries are generated. These solutions are searched to find the best one which is the recommendation given by DINNER.#R##N##R##N#The validity of DINNER's recommendations were demonstrated in three ways: examination of a detailed example, a demonstration of the changes in the recommendations as the input is changed, and the evaluation of an expert DBA.	knowledge-based systems;relational database	Yishai A. Feldman;Jacob Reouven	2003	Expert Syst. Appl.	10.1016/S0957-4174(03)00003-4	knowledge base;relational database management system;relational database;computer science;artificial intelligence;machine learning;data mining;database;conjunctive query;view;database administrator;information retrieval	DB	-30.328176220325805	-1.3545897287020285	171743
439df07cf369fef1c5638439239257cc00154d0a	evolution and evaluation in knowledge fusion system	modelizacion;complexite calcul;ingenierie connaissances;matrix theory;intelligence artificielle;autoevaluacion;autoevaluation;modelisation;local knowledge;complejidad computacion;computational complexity;cognition;cognicion;artificial intelligence;inteligencia artificial;modeling;self evaluation;knowledge engineering	The paper presents a method to control evolution of pattern in a knowledge fusion system. A self-adapt evaluation mechanism to assign proper value dynamically to weight parameters is also described. Some rules are defined with aid of the matrix theory to promise the controllablity and describability to the evolution process. A new knowledge object, called LKS (local knowledge state), that can redirect path in knowledge fusion system and evolve to other knowledge object(s) is formed in that model. Experimental results of a case study show that it can improve the efficiency and reduce computational complexity of a knowledge fusion system.	algorithm;computational complexity theory;evolution;knowledge space;redirection (computing);the matrix	Jin Gou;Jiangang Yang;Qian Chen	2005		10.1007/11499305_20	systems modeling;cognition;self-assessment;computer science;artificial intelligence;knowledge-based systems;knowledge engineering;computational complexity theory;algorithm;matrix	AI	-21.673038155038892	-2.30996215831675	172383
ad5e2ada52911b4edf4e81cd32ac077854696e92	one loop does not fit all	column stores;just in time code generation	Just-In-Time (JIT) compilation increasingly becomes a key technology for modern database systems. It allows the creation of code on-the-fly to perfectly match an active query. In the past, it has been argued that a query should be compiled to a single loop that performs all query actions, for example, all selects over all relevant columns. On the other hand, vectorization -- a common feature in modern data systems -- allows for better results by evaluating the query predicates sequentially in different tight for-loops.  In this paper, we study JIT compilation for modern in-memory column-stores in detail and we show that, contrary to the common belief that vectorization outweighs the benefits of having one loop, there are cases in which creating a single loop is actually the optimal solution. In fact, deciding between multiple or a single loop is not a static decision; instead, it depends on (per column) query selectivity. We perform our experiments on a modern column-store prototype that supports vectorization and we show that, depending on selectivity, a different code layout is optimal. When a select operator is implemented with a no-branch design, for low selectivity creating multiple loops performs better than a single loop. A single tight loop performs better otherwise.	automatic vectorization;column (database);column-oriented dbms;compiler;data system;experiment;in-memory database;infinite loop;just-in-time compilation;prototype;selectivity (electronic)	Styliani Pantela;Stratos Idreos	2015		10.1145/2723372.2764944	loop fusion;real-time computing;loop fission;loop dependence analysis;computer science;theoretical computer science;do while loop;database;programming language;inner loop	DB	-30.82502634041631	2.238384260447495	172558
fcb2e899d2ab16655027a3a45cfcea9de18de834	uncovering the conceptual models in ripple down rules	knowledge based system;adquisicion del conocimiento;semantica formal;apprentissage conceptuel;conceptual model;formal semantics;acquisition connaissance;ripple down rules;semantique formelle;aprendizaje conceptual;knowledge acquisition;concept learning;formal concept analysis	The need for analysis and modeling of knowledge has been espoused by many researchers as a prerequisite to building knowledge based systems (KBS). This approach has done little to alleviate the knowledge acquisition (KA) bottleneck or the maintenance problems associated with large KBS. For actual KA and maintenance we prefer to use a technique, known as ripple down rules (RDR) that is simple, yet reliable, and later see what models can be produced from the knowledge for the purpose of reuse. Tools based on Formal Concept Analysis have been added to RDR to uncover and explore the underlying conceptual structures. 1 Models and their Role in Knowledge Acquisition Since Newell’s [21] paper on “The Knowledge Level” there has been increasing awareness and acceptance of the need to model knowledge at a level above its symbolic representation. This notion was further explored by Clancey [3] who used task and problem solving methods analysis to divide problems into “heuristic classification” and “heuristic construction”. Following Van de Velde [33] approaches which have been built on Newell's knowledge level model include:Generic Task Framework [2], KADS and CommonKADS [30], Role-Limiting Methods [19], Components of Expertise and the Componential Methodology [32], Protege and Protege II [24], KIF and Ontolingua [22]. All of these approaches impose a particular structure on the knowledge to enable the current problem to be mapped into the appropriate class of problem situation. The structure chosen will depend on the features of the task and the domain of expertise. While each of the approaches mentioned above are different, Van de Velde [33, p.1218] considers three concepts to be generally included as part of a knowledge level model. These are the domain model, the task model and the problem solving method. While the knowledge level approach is superior to the previous transfer of expertise approach to KA, matching the problem solving method or methods to the problem and adapting them to suit the domain is no mean feat [40]. The use of methodologies such as KADS requires extensive involvement of a knowledge engineer and the modeling process is complex, often doing little to alleviate the KA bottleneck or the problems associated with maintaining large KBS [20]. In the above approaches the purpose of modeling is to allow the capture of expertise in a structured and systematic way. Clancey defines a model as “merely an abstraction, a description and generator of behaviour patterns over time, not a mechanism equivalent to human capacity.” [4, p. 89]. Since models are at best imperfect representations that vary between users and the same user over time [11] we prefer to use simple, yet reliable techniques for KA. However, we are interested in modeling as an end in itself due to their “explanatory value as psychological descriptions” [4, p.89] and their usefulness in instruction [30].ion, a description and generator of behaviour patterns over time, not a mechanism equivalent to human capacity.” [4, p. 89]. Since models are at best imperfect representations that vary between users and the same user over time [11] we prefer to use simple, yet reliable techniques for KA. However, we are interested in modeling as an end in itself due to their “explanatory value as psychological descriptions” [4, p.89] and their usefulness in instruction [30]. This paper considers the combination of two approaches, Formal Concept Analysis (FCA) [36] and Ripple Down Rules (RDR) [5], that reduce KA to tasks that can easily be performed by an expert with minimal a priori modeling or involvement of a knowledge engineer. In FCA the expert defines a context, which is a Object-Attribute crosstable such as the one shown in Figure 1. From the context, formal concepts are derived, which are ordered to provide a complete lattice of sub and super concepts. These concepts can be represented as a line diagram, also known as a Hasse diagram, and can be used to derive implications for use in a knowledge base [37]. This paper investigates starting from the opposite direction by using existing rules in an RDR KBS, acquired manually from an expert, to define contexts and then generate the concepts using FCA to make explicit the models implicit in the rules. KA using RDR requires the expert to assign a conclusion to a case and then to select the feature/s in the case that were used to make that decision. Each time a new case is seen the expert checks the conclusion assigned by the system and if the expert does not agree a new conclusion is assigned and the features which differentiate the current misclassified case from the case associated with the rule that inappropriately fired are selected by the user. FCA requires some consideration of the whole domain as does Repertory Grids [11] and does not consider incremental maintenance. RDR on the other hand develops the whole system on a case by case basis and automatically structures the KBS in such a way to ensure changes are incremental. The classification of cases and identification of features is very simple and probably less demanding for experts than the development of crosstables. Has wings flys suckles young warmblooded coldblooded breeds in water breeds on land has scales	domain model;formal concept analysis;hasse diagram;heuristic (computer science);knowledge acquisition and documentation structuring;knowledge interchange format;knowledge base;knowledge engineer;knowledge level;knowledge-based systems;mental representation;problem solving;protégé;repertory grid;restrictive design rules;ripple effect	Debbie Richards;Paul Compton	1997		10.1007/BFb0027871	concept learning;computer science;formal concept analysis;knowledge management;artificial intelligence;conceptual model;machine learning;formal semantics;data mining	AI	-28.268526694080784	-4.738609741661081	172643
22e2f38c5c2e58039602b03d678f9b2831ca47d1	a framework for specifying group decision-making mechanisms	agent infrastructures;context free;role based access control;group decision making;agent coordination;group decision	Collaborating agents need to make group decisions to establish their initial commitment to a proposed group activity [8] and, once committed, to coordinate the updating of their intentions related to that activity [6]. (We call the first sort context-free group decisions and the latter context-bound.) In addition, collaborating agents must be able to reason effectively about their participation in group decision-making processes.	context-free language	Luke Hunsberger	2005		10.1145/1082473.1082688	r-cast;group decision-making;computer science;knowledge management;role-based access control	HCI	-21.338756161079182	-9.215078693423063	172831
cbee9f9e560ba808d95d81f31dc8e410a2695c09	an intra-algorithm comparison study of complete search fsm implementations in centralized graph transaction databases		Frequent subgraph mining (FSM) algorithms are widely used in various areas of data analysis. Several experimental studies about FSM algorithms were reported in literature; however, these experiments lack some clarifications about the most efficient implementation of a specific algorithm for a context of use (e.g., medium size datasets). In this paper, we present an experimental study with available implementations of two well known complete search FSM algorithms namely gSpan and Gaston. Our main purpose of this experimental study is to find a suitable Frequent Subgraph Mining implementation for indexing centralized graphs databases for aggregated search(CAIR home page: www.irit.fr/CAIR). In this paper, we provide details of the experimental results according to the input variation cases. We propose (for end users) a summary, about the most efficient FSM implementations for each algorithm (i.e., gSpan and Gaston), based on real datasets from the literature.	algorithm;centralized computing	Emanuele Perra;Mohand-Said Hacid;Rafiqul Haque;Abderrazek Jemai	2018		10.1007/978-3-030-01851-1_8	data mining;end user;implementation;search engine indexing;database;computer science;algorithm;home page;graph;database transaction	DB	-32.46646681469418	3.835990403161927	172901
d3dbe94b73f0a69269f739da98eccfa0cb144bd0	reducing a biomarkers list via mathematical programming: application to gene signatures to detect time-dependent hypoxia in cancer	emergency response;emergency response planning;satisfiability;center of gravity determination;complex problem solving;emergency response planning complex problem solving knowledge based agents diverse application domains agent learning complex application domains intelligence analysis center of gravity determination;software agents;learning systems;hierarchical representation;agent learning;natural language;intelligence analysis;center of gravity;subject matter expert;software agents learning systems problem solving;complex application domains;problem solving;knowledge based agents;knowledge base;diverse application domains;problem solving nuclear weapons machine learning ontologies application software intelligent agent gravity computer science drives strategic planning	In biology and medical sciences, highly parallel biological assays spurred a revolution leading to the emergence of the '-omics' era. Dimensionality reduction techniques are necessary to be able to analyze, interpret, validate and take advantage of the tremendous wealth of highly dimensional data they provide. This paper is based on a DNA microarray study providing gene signatures for hypoxia. These gene signatures were tested on a large breast cancer data set for assessing their prognostic power by means of Kaplan-Meier survival, univariate, and multivariate analyses. We explore the use of several mathematical programming-based techniques that aim to reduce the gene signature sizes as much as possible while maintaining the key characteristics of the original signature, more precisely: the signature prognostic and diagnostic significance. The proposed signature reduction techniques have very interesting potential uses. Indeed, by downsizing the relevant data to a manageable size, one can then patent the core set of biomarkers and also create a dedicated assay (e.g.: on a customized array) for routine applications (e.g.: in the clinical set up) leading to individualized medicine capabilities. Our experiments show that the reduced hypoxia signatures reproduced qualitatively and quantitatively in a similar way that of the original ones.	antivirus software;array data structure;dna microarray;dimensionality reduction;emergence;experiment;kaplan–meier estimator;mathematical optimization;omics	Glenn Fung;Renaud Seigneuric;Sriram Krishnan;R. Bharat Rao;Brad G. Wouters;Philippe Lambin	2007	Sixth International Conference on Machine Learning and Applications (ICMLA 2007)	10.1109/ICMLA.2007.61	knowledge base;intelligence analysis;computer science;knowledge management;artificial intelligence;software agent;machine learning;management science;subject-matter expert;natural language;center of gravity;satisfiability	DB	-32.13888006670993	-5.5038515080906185	172989
95bd67cbf5c86eabd152653bceaa05cb0de4dd0c	an open interactive timetabling tool	analysis and design;hierarchical data;support system;object oriented approach	This article deals with the analysis and design of an interactive decision support system for timetable management. This tool will be able to take hierarchical data organization into account and to maintain coherence of the constraints on this data. Our research which has led to the creation of the EDT tool has two aims. The first aim is to provide an open, generic tool which can be developed in many different ways. In order to achieve this aim, we have followed an object-oriented approach and we have defined object classes dedicated for the modeling of the timetabling problem. The second aim is to analyze the needs in timetable manipulation and to provide a generic organization so that the tool can be used in many situations. To achieve this aim, both user based and automated technics are used. keywords: generic timetables model, interactive timetabling, multi points of view, decision support tool	algorithm;chicago engineering design team;constraint programming;decision support system;event dispatching thread;hierarchical database model;interactivity;schedule;scheduling (computing);user profile	Sylvain Piechowiak;Jingxua Ma;René Mandiau	2004		10.1007/11593577_3	simulation;computer science;systems engineering;data mining	HCI	-30.332325260754992	-5.25735854899716	173022
1f4933f7a7a28e69e229243fafa0f5c28e06a94a	ontology-driven induction of decision trees at multiple levels of abstraction	decision tree learning;ontologie;learning algorithm;decision tree;apprentissage inductif;scientific discovery;abstraction;besoin utilisateur;necesidad usuario;algorithme apprentissage;arbol decision;abstraccion;feasibility;aprendizaje por induccion;user need;multiple decision;levels of abstraction;classification rules;ontology based learning;inductive learning;decision multiple;point of view;algoritmo aprendizaje;ontology;arbre decision;practicabilidad;faisabilite	Most learning algorithms for data-driven induction of pattern classifiers (e.g., the decision tree algorithm), typically represent input patterns at a single level of abstraction – usually in the form of an ordered tuple of attribute values. However, in many applications of inductive learning – e.g., scientific discovery, users often need to explore a data set at multiple levels of abstraction, and from different points of view. Each point of view corresponds to a set of ontological (and representational) commitments regarding the domain of interest. The choice of an ontology induces a set of representatios of the data and a set of transformations of the hypothesis space. This paper formalizes the problem of inductive learning using ontologies and data; describes an ontology-driven decision tree learning algorithm to learn classification rules at multiple levels of abstraction; and presents preliminary results to demonstrate the feasibility of the proposed approach.	algorithm;decision tree learning;inductive reasoning;list of algorithms;machine learning;ontology (information science);principle of abstraction	Jun Zhang;Adrian Silvescu;Vasant Honavar	2002		10.1007/3-540-45622-8_25	epistemology;decision tree learning;computer science;artificial intelligence;machine learning;decision tree;ontology;database;abstraction;algorithm	AI	-23.51320541586589	-2.458747779881637	173026
827677fe68f681f338603f2f77afd35702de8f40	maximizing the output rate of multi-way join queries over streaming information sources	information source;input stream;existing symmetric;remote site;expensive runtime plan reorganization;query evaluation;execution plan reorganization;support query;focus shift;binary operator;prototype implementation;output rate	Recently there has been a growing focus in the research community on join query evaluation for scenarios in which input characteristics may not be entirely known and inputs enter the system at highly variable and unpredictable rates. The proposed solutions to date rely upon some combination of streaming binary operators and “on-the-fly” query plan reorganization to deal with this unpredictability. In this paper, we consider a different approach, and propose a multi-input streaming join algorithm we call MJoin. We show through experiments with a prototype implementation that in many instances the MJoin produces outputs sooner than any tree of binary operators, and that it adapts well to changing input parameters without query plan modification. This suggests that the MJoin operator may be a useful addition to systems that evaluate queries containing joins over streaming inputs.	algorithm;experiment;join (sql);prototype;query plan;streaming media	Stratis Viglas;Jeffrey F. Naughton;Josef Burger	2003			real-time computing;theoretical computer science;data mining;database	DB	-29.30341736009349	-1.6004534975765143	173146
e9cf6877e1a4bbe51a23d1f6b56021bb13af36fb	systems framework for regional-scale integrated modelling and assessment	gestion integrada;computadora;modelizacion;gestion integree;systeme regional;metodologia;ordinateur;implementation;integrated modelling;resource management;hombre;integrated management;recherche developpement;prise decision;regional scale;decision maker;computer;integration;methodologie;participatory research;natural resources;modelisation;gestion recursos;natural resource management;research and development;investigacion desarrollo;integracion;integrated modelling and assessment;recurso natural;human;ressource naturelle;gestion ressources;regional development;methodology;sistema regional;implementacion;toma decision;systems framework;modeling;regional system;homme	Computer-based methods of integrated modelling and assessment provide an important means for reviewing policy choices in natural resource management (NRM). Research in support of NRM needs to address a wide range of issues involved, from point-scale biophysical, to business-scale human, to regional-scale planning issues. Research covering the full scope of such issues is by default multi-disciplinary and integrative and therefore analytically, methodologically and operationally challenging. The recently initiated Ord–Bonaparte Program is an example of a research and development program attempting to achieve both levels of integration in an applied NRM context. One key requirement for the success of the program lies in developing a systems framework that: (i) enables the integration of the various disciplinary research activities; and (ii) facilitates the implementation of research outputs by making integrated science relevant to decision-makers and translating new knowledge into outcomes for sustainable regional development. This paper proposes an approaches for such a systems framework. © 2003 IMACS. Published by Elsevier B.V. All rights reserved.	ecology;object-relational database;programmer;real life	Romy Greiner	2004	Mathematics and Computers in Simulation	10.1016/S0378-4754(03)00119-8	decision-making;systems modeling;resource management;methodology;natural resource;natural resource management;implementation;operations research	AI	-23.585034318968333	-3.526269007250621	173574
07721c99ab757e12d7e7df4303747b98280b46d5	acquiring case adaptation knowledge: a hybrid approach	learning;case base reasoning;rule based;hybrid approach;hybrid method;adaptive learning;artificial intelligence;mathematics computers information science management law miscellaneous	The ability of case-based reasoning (CBR) systems to apply cases to novel situations depends on their case adaptation knowledge. However, endowing CBR systems with adequate adaptation knowledge has proven to be a very di cult task. This paper describes a hybrid method for performing case adaptation, using a combination of rule-based and case-based reasoning. It shows how this approach provides a framework for acquiring exible adaptation knowledge from experiences with autonomous adaptation and suggests its potential as a basis for acquisition of adaptation knowledge from interactive user guidance. It also presents initial experimental results examining the bene ts of the approach and comparing the relative contributions of case learning and adaptation learning to reasoning performance.	autonomous robot;case-based reasoning;experience;logic programming	David B. Leake;Andrew Kinley;David C. Wilson	1996			rule-based system;computer science;knowledge management;artificial intelligence;model-based reasoning;machine learning;adaptive learning	AI	-21.168271966001356	-6.092631062200282	173909
2ab71d7abc1ff024abe0173fb3f234f2a3892ba4	planning with time limits in bdi agent programming languages	programming language;belief desire intention;operational semantics;specification language;time limited planning;anytime algorithm;dynamic environment;computer software not elsewhere classified;bdi agents	This paper provides a theoretical basis for performing time limited planning within Belief-Desire-Intention (BDI) agents. The BDI agent architecture is recognised as one of the most popular architectures for developing agents for complex and dynamic environments, in addition to which they have a strong theoretical foundation. Recent work has extended a BDI agent specification language to include HTNstyle planning as a built-in feature. However, the extended semantics assume that agents have an unlimited amount of time available to perform planning, which is often not the case in many dynamic real world environments. We extend previous research by using ideas from anytime algorithms, and allow programmer control over the amount of time the agent spends on planning. We show that the resulting integrated agent specification language has advantages over regular BDI agent reasoning.	agent architecture;anytime algorithm;automated planning and scheduling;belief–desire–intention software model;failure;interrupt;operational semantics;parsing;programmer;simulation;specification language;the australian	Lavindra de Silva;Anthony Dekker;James Harland	2007			simulation;specification language;computer science;knowledge management;artificial intelligence;programming language;operational semantics	AI	-19.458003533724106	-7.816194215584326	173927
475c9fe7cf11a7105624bb0d3bcb2158288f0ddf	ycsb and tpc-h: big data and decision support benchmarks	benchmarking;ycsb;tpc h;decision support systems big data;tpc h benchmarking decision support systems big data ycsb;decision support systems tpc h ycsb big data yahoo cloud serving benchmark;benchmark testing big data decision support systems databases business throughput computers;big data;decision support systems	In computing, a benchmark is the result of running a set of computer programs or a computer program, in order to assess relative performance by running a series of standard tests. By doing this, researchers highlight the characteristics of certain systems and are able to rank the system against the rest. On the other hand, BigData is a hot topic. It not only deals with large amounts of data sets and the procedures and tools used to analyze and manipulate them, but also to a computational turn in research and thought. At the same time, Decision Support applications are related to Big Data as they need to deal with large datasets. In this paper we describe two of the most popular benchmarks, one representing Decision Support Systems (TPC-H), and the other represents the Big Data class (YCSB - Yahoo Cloud Serving Benchmark).	benchmark (computing);big data;computation;computer program;decision support system;ibm tivoli storage productivity center;ycsb	Melyssa Barata;Jorge Bernardino;Pedro Nuno San-Bento Furtado	2014	2014 IEEE International Congress on Big Data	10.1109/BigData.Congress.2014.128	computer science;data science;data mining;database	DB	-33.31632672257423	-0.482046600709674	174454
9a3b3d19cd0c690a8f9fb3800c986691f4802d8a	decision support for infectious diseases - a working prototype	developpement logiciel;aide diagnostic;base relacional dato;clinical data;vocabulaire;search engine;decision support;informatica biomedical;controlled vocabulary;buscador;biomedical data processing;infeccion;aplicacion medical;infection nosocomiale;genie biomedical;systeme aide decision;sistema informatico;vocabulary;informatique biomedicale;hombre;computer system;vocabulario;sistema ayuda decision;relational database;nosocomial infection;prototipo;support system;decision support system;dictionnaire;engines;infeccion nosocomial;biomedical engineering;desarrollo logicial;hospital acquired infections;dictionaries;software development;human;base donnee relationnelle;ingenieria biomedica;medical application;systeme informatique;information system;moteur recherche;infectious disease;infection;data structure;prototype;diccionario;systeme information;diagnostic aid;homme;nosocomial infections;rule based reasoning;application medicale;sistema informacion;ayuda diagnostica	This paper presents a decision support system for nosocomial infections and its integration in the large HIS of the University Hospital of Giessen. The system comprises five different engines and a data dictionary. It is designed to detect hospital acquired infections even in a situation where only a restricted amount of clinical data is available (the data is split up in different information systems). Furthermore the model prevents time consuming manual data entry. The five engines split the main task into: (1) a preselection, which sorts out patients who definitely do not have a nosocomial infection; (2) a rule based reasoning process which detects patients likely to have such an infection; (3) an alarm process which is responsible for the presentation of the alert; (4) an explanation process to follow up the reasoning; and (5) statistic tools to answer specific hygienic questions. A data dictionary supplies the controlled vocabulary, which is required to understand data structures used in the different clinical subsystems and may those with each other.	clinical data;communicable diseases;controlled vocabulary;data dictionary;data structure;decision support system;dictionary [publication type];how true feel alert right now;infection;infections, hospital;information systems;information system;manufactured supplies;outpatients;patients;prototype;statistic (data)	J. Joch;Joachim Dudeck	2001	International journal of medical informatics	10.1016/S1386-5056(01)00209-X	controlled vocabulary;simulation;medicine;decision support system;relational database;computer science;artificial intelligence;software development;prototype;operations research;information system;search engine	Visualization	-25.516267763938647	-5.385546075050685	174530
ef859d39273dae80a33d22aa3ea205c2d6b1e7ad	decision support systems: a rule-based approach	expert systems;knowledge bases;rule based;decision support system;decision support systems;capital budgeting	There are number of ways to classify existing Decision Support Systems (DSSs) . One such classification [RanFed83] distinguishes between Model-oriented DSS, Data-oriented DSS, Decision-oriented DSS, and General Decision Support Systems (GDSS). Model-oriented Decision Support Systems contain models like regression and optimization routines that are developed to aid in decision making in a specific problem domain. They may not be easily transported over problem areas. Data-oriented Decision Support Systems are like database management systems. They focus on providing information and facilities for data storage, retrieval, and update. An example of Data-oriented DSS is an inventory management system. Decisior+oriented Decision Support Systems focus on supporting a specific decision process. An example of a Decision-oriented DSS is MYCIN, which is an expert system for medical diagnosis. MYCIN has led to the development of EMYCIN, a general expert system tool. Finally, General Decision Support Systems are Decision Support Systems that support multiple decision areas or domains.	computer data storage;database;decision support system;expert system;inventory;logic programming;mathematical optimization;mycin;problem domain	Man-kuen S. Chen;Chui-fat C. Chau;Waldo C. Kabat	1985		10.1145/320435.320586	business rule management system;r-cast;decision support system;intelligent decision support system;decision analysis;decision engineering;knowledge management;decision tree;data mining;decision rule;management science;evidential reasoning approach;business decision mapping	DB	-33.34384332028298	-8.039364522000628	174789
b03acca9f4101d4cc44e209bc56ab3c38ca2e3a3	a strategy for near-term success using knowledge-based systems	knowledge based system		knowledge-based systems	S. C. Laufmann	1987	Knowledge Eng. Review	10.1017/S026988890000093X	computer science;knowledge management;artificial intelligence;knowledge-based systems;knowledge engineering;knowledge value chain	AI	-31.362962871229367	-6.729877447605355	174796
03342a29aebbbea6700925d46a43d68a36cad64e	integration of neural networks and expert systems for process fault diagnosis	artificial intelligent;large scale;structure and function;object oriented;fault diagnosis;neural network;knowledge base;expert system	The main thrust of this research is the development of an artificial intelligence (AI) system to be used as an operators' aid in the diagnosis of faults in large-scale chemical process plants. The operator advisory system involves the integration of two fundamentally different AI techniques: expert systems and neural networks. A diagnostic strategy based on the hierarchical use of neural networks is used as a first level filter to diagnose faults commonly encountered in chemical process plants. Once the faults are localized within the process by the neural networks, the deep knowledge expert system analyzes the results, and either confirms the diagnosis or offers alternative solutions. The model-based expert system contains information of the plant's structure and function within its object-oriented knowledge base. The diagnostic strategy can handle novel or previously unencountered faults, noisy process sensor measurements, and multiple faults. The operator advisory system is demonstrated using a multi-column distillation plant as a case study.	expert system;neural networks;thrust	Warren R. Becraft;Peter L. Lee;Robert B. Newell	1991			legal expert system;knowledge base;computer science;artificial intelligence;machine learning;data mining;object-oriented programming;expert system	AI	-28.818466929838255	-6.081795136371395	174903
752dfba2a063cd533cce45a262d6f4d92d8aef58	automated reasoning for city infrastructure maintenance decision support		We present an interactive decision support system for assisting city infrastructure inter-asset management. It combines real-time site specific data retrieval, a knowledge base co-created with domain experts and an inference engine capable of predicting potential consequences and risks resulting from the available data and knowledge. The system can give explanations of each consequence, cope with incomplete and uncertain data by making assumptions about what might be the worst case scenario, and making suggestions for further investigation. This demo presents multiple real-world scenarios, and demonstrates how modifying assumptions (parameter values) can lead to different consequences.		Lijun Wei;Derek R. Magee;Vania Dimitrova;Barry Clarke;Heshan Du;Quratul-ain Mahesar;Kareem Al Ammari;Anthony G. Cohn	2018		10.24963/ijcai.2018/868	computer science;worst-case scenario;artificial intelligence;decision support system;machine learning;data mining;knowledge base;inference engine;automated reasoning;data retrieval;uncertain data	ML	-31.722472480987424	-7.569527641711233	174959
60c6b4b934c8dec24354e492c5742e69366ac96a	representing a robotic domain using temporal description logics	temporal logic;plan instance;temporal language;certain moment;world state;action management;domain action;plan description;temporal description logic;robotic domain;description logics;description logic;robotics	A temporal logic for representing and reasoning on a robotic domain is presented. Actions are represented by describing what is true while the action itself is occurring, and plans are constructed by temporally relating actions and world states. The temporal language is a member of the family of Description Logics, which are characterized by high expressivity combined with good computational properties. The logic is used to organize the domain actions and plans in a taxonomy. The classi cation and recognition tasks, together with the subsumption task form the basis for action management. An action/plan description can be automatically classi ed into a taxonomy; an action/plan instance can be recognized to take place at a certain moment from the observation of what is happening in the world during a time interval.	computation;description logic;robot;subsumption architecture;taxonomy (general);temporal logic	Alessandro Artale;Enrico Franconi	1999	AI EDAM		description logic;temporal logic;biological classification;computer science;artificial intelligence;robotics;algorithm;temporal logic of actions	AI	-20.425749964769444	2.444193111970214	175049
0a21cf94b0340e4bafea54a320d100ba4e267ce8	trusting digital technologies correctly		Trust is a facilitator of interactions among the members of a system, whether these be human agents, artificial agents or a combination of both (a hybrid system). Elsewhere, I have argued that the occurrences of trust are related to, and affect, preexisting relations, like purchasing, negotiation, communication, and delegation (Taddeo 2010a, b). Trust is not to be considered a relation itself but a property of relations, something that changes the way relations occur. Consider, for example, a case of communication. Alice talks to Bob and she informs him that the grocery store down the road is closed for the day As Bob trusts Alice, he believes her and decides not to walk to the shop to double check, instead he starts searching for an alternative place to shop for his groceries. Between Alice and Bob there is a firstorder relation, the communication, which ranges over the two agents, and there is the second-order property of trust that ranges over the first-order-relation and affects the way it occurs. As a property of relations, trust changes the way relations occur by minimising the effort and commitment for the achievement of a given goal of the agents who decides to trust (the trustor). It does so in two ways. First, the trustor can avoid performing the action necessary to achieve her/his goal her/himself, because s/he can count on the trustee to do it—Bob does not walk to the shop to check whether it is actually closed. Second, the trustor can decide not to supervise the trustee’s performance. Bob does not ask Alice how she knows about the opening times of the shop. Delegation without supervision characterises the presence of trust (Taddeo 2010a, b). I define trust as follows:	alice and bob;emoticon;first-order predicate;hybrid system;institute for operations research and the management sciences;intelligent agent;interaction;purchasing;trust (emotion)	Mariarosaria Taddeo	2017	Minds and Machines	10.1007/s11023-017-9450-5	artificial intelligence;computer science	AI	-20.82068312635348	-9.27758736222914	175064
4c2c8c6d5c328baac95c596ac54d242428017f5f	progressive result generation for multi-criteria decision support queries	databases;portals;web based applications;decision support;pareto optimisation;electronic commerce;pareto optimal queries;query processing;job sequencing problem progressive result generation multicriteria decision support queries web searches b2b portals online commerce pareto optimal queries skylines progressive query evaluation framework progxe query processing optimization;application software;web and internet services;user preferences;online commerce;web searches;upper bound;internet;progxe;progressive query evaluation framework;levels of abstraction;query evaluation;multicriteria decision support queries;business;decision support systems;progressive result generation;b2b portals;web search;optimization;computer science;near real time;query processing decision support systems electronic commerce internet pareto optimisation;job sequencing problem;skylines;delay query processing application software business web and internet services costs databases computer science web search portals;pareto optimality;real time systems	Multi-criteria decision support (MCDS) is crucial in many business and web applications such as web searches, B2B portals and on-line commerce. Such MCDS applications need to report results early; as soon as they are being generated so that they can react and formulate competitive decisions in near real-time. The ease in expressing user preferences in web-based applications has made Pareto-optimal (skyline) queries a popular class of MCDS queries. However, state-of-the-art techniques either focus on handling skylines on single input sets (i.e., no joins) or do not tackle the challenge of producing progressive early output results. In this work, we propose a progressive query evaluation framework ProgXe that transforms the execution of queries involving skyline over joins to be non-blocking, i.e., to be progressively generating results early and often. In ProgXe the query processing (join, mapping and skyline) is conducted at multiple levels of abstraction, thereby exploiting the knowledge gained from both input as well as mapped output spaces. This knowledge enables us to identify and reason about abstract-level relationships to guarantee correctness of early output. It also provides optimization opportunities previously missed by current techniques. To further optimize ProgXe, we incorporate an ordering technique that optimizes the rate at which results are reported by translating the optimization of tuple-level processing into a job-sequencing problem. Our experimental study over a wide variety of data sets demonstrates the superiority of our approach over state-of-the-art techniques.	benchmark (computing);blocking (computing);correctness (computer science);database;decision support system;experiment;job shop scheduling;mathematical optimization;non-blocking algorithm;online and offline;pareto efficiency;portals;principle of abstraction;real-time computing;real-time transcription;real-time web;synthetic data;user (computing);web application	Venkatesh Raghavan;Elke A. Rundensteiner	2010	2010 IEEE 26th International Conference on Data Engineering (ICDE 2010)	10.1109/ICDE.2010.5447928	application software;web application;the internet;decision support system;computer science;data mining;database;upper and lower bounds;world wide web	DB	-25.135657436962582	3.218479473652668	175067
5092df33492dc52b05f065c24d10c2bff882749c	a comparison of bdi based real-time reasoning and htn based planning	raisonnement base sur cas;razonamiento fundado sobre caso;belief desire intention model;belief desire intention;real time;belief desire intention bdi;hierarchical task network;practical reasoning;intelligence artificielle;similitude;resolucion problema;planificacion;agents;modelo bdi;temps reel;similarity;tiempo real;artificial intelligence;modele bdi;planning;inteligencia artificial;similitud;planification;case based reasoning;hierarchical task network htn;problem solving;resolution probleme	The Belief-Desire-Intention (BDI) model of agency is an architecture based on Bratman’s theory of practical reasoning. A convenient and accepted representation of the BDI architecture is in terms of goals and plans. In this representation, an agent achieves a goal by selecting and executing plans from a predefined plan library created by the programmer. The BDI architecture is therefore a real-time reasoning and execution system that works in complex environments with predefined plans. Hierarchical Task Network (HTN) decomposition on the other hand is a planning technique which has its roots in classical planning systems such as STRIPS. Although HTN systems belong to the planning literature, they do not create new plans from first principles as STRIPS style planners do, but instead create complete solutions by using partial solutions provided by the programmer. Despite being used for different purposes, HTN and BDI systems appear to have a lot of similarities in the problem solving approaches they adopt. This paper presents these similarities. A systematic method for mapping between the two systems is developed, and experimental results for different kinds of environments are presented.	belief–desire–intention software model;hierarchical task network;problem solving;programmer;real-time computing;real-time transcription;strips	Lavindra de Silva;Lin Padgham	2004		10.1007/978-3-540-30549-1_118	planning;practical reason;case-based reasoning;similarity;computer science;artificial intelligence;similitude;software agent;hierarchical task network	AI	-25.1925685284979	-5.92928346768743	175210
67f7a81377b9a50821ba7ffb9ebd7bfa4b99b509	technical perspective: : supporting linear algebra operations in sql		"""Linear algebra operations are at the core of Machine Learning. Multiple specialized systems have emerged for the scalable, distributed execution of matrix and vector operations. The relationship of such computations to data management and databases however brings frictions. It is well known that a great deal of human time and machine time is being spent nowadays on fetching data out of the database and performing a computation on a specialized system. One answer to the issue is that we truly need a new kind of non-SQL database that is tuned to these computations. The creators of SimSQL opted for the decidedly incremental approach. Can we make a very small set of changes to the relational model and RDBMS software to render them suitable for executing linear algebra in the database? We have come across the """"brand new system"""" versus """"incremental to relational"""" question many times in the database field. E.g., do we need brand new query languages and query processors for data cubes? Or do we need to have our query processors pay attention to specific cases that are especially common in data analytics queries over stars and snowflakes? Do semistructured query languages need to depart from SQL or it is enough to be incremental to SQL? Same for query processors. Repeat the questions to graph data and RDF data. In many cases, new custom systems emerged only to figure out later that we could/should have tackled the problem incrementally. That’s the trap that the authors of this paper avoid. This is not to say that radical changes and extensions should be forbidden. Rather it says that we should closely scrutinize the necessity of the changes, do them when needed and keep them minimal. The authors identify the right opportunities. Here is a non-exhaustive list:"""	central processing unit;computation;data cube;linear algebra;list of code lyoko episodes;machine learning;query language;relational database management system;relational model;resource description framework;sql;scalability	Yannis Papakonstantinou	2018	SIGMOD Record	10.1145/3277006.3277012	database;linear algebra;sql;computer science	DB	-33.42378780430306	-0.29982565370400954	175262
96b5794f6625479e2995433fe466284a47b2fbd6	neutrosophy, method of uncertainties process analysis		This paper presents the importance of Neutrosophy theory in order to find a method that could solve the uncertainties arising on process analysis. The aim of this pilot study is to find a procedure to diminish the uncertainties induced by manufacturing, maintenance, logistics, design, human resources. The study is intended to identify a method to answer uncertainties solving in order to support manufacturing managers, NLP specialists, artificial intelligence researchers and businessman in general.	algorithm;artificial intelligence;logistics;natural language processing	Florentin Smarandache;Mirela Teodorescu	2017				AI	-31.85935158281218	-8.181993752810337	175552
f3c1ced02689dc20ec494a6ff100c640ec6d78f9	two-stage stochastic view selection for data-analysis queries		We consider the problem of selecting an optimal set of views to answer a given collection of queries at the present time (stage 1) as well as several collections of queries in the future (stage 2), with a given probability of occurrence associated with each collection, so as to minimize the expected value of the corresponding query response time, while keeping the total size of the views within a given limit. We formulate this problem as a two-stage stochastic programming problem. We show that this model is equivalent to an integer programming (IP) model that can be solved via various commercial IP solvers. We also study the relationship between the queries and the views in this context and use this relationship to reduce the size of the corresponding IP model, hence increase the scalability of our proposed approach.		Rong Huang;Rada Chirkova;Yahya Fathi	2012		10.1007/978-3-642-32741-4_11	machine learning;pattern recognition;data mining	NLP	-25.815296135325653	3.7900414979880837	175707
a4b2d494280a8292cb7097073cc43dbb7b16b17c	towards an expert system for the manufacturing system planning of products with graded properties	expert system	Ontologies open new ways for representing, sharing and reusing knowledge. This paper is based on the investigations within the Collaborative Research Centre (CRC) Transregio 30. In the CRC, thermomechanically coupled processes are developed and analyzed. They provide the possibility to produce functional graded components. Functional gradation is the targeted and reproducible adaptation of a material microstructure with the intention of establishing the macroscopic properties of the component. The objective is the steady progress of the microstructure ́s variation through at least one spatial dimension. To support the manufacturing system’s planning of products with graded properties, we develop an expert system. An expert system is a software which emulates the reasoning of an expert. One of the components of our intended expert system is an ontology. It assists researchers with the use and reuse of the acquired knowledge of the CRC, as well as with communication between the different projects within the CRC. In this paper we explain the architecture of the intended expert system as well as its elements, placing special emphasis on the ontology. Then we explain the ontology ́s development based on knowledge extraction and representation.	computer-aided design;cyclic redundancy check;emulator;expert system;functional programming;german research centre for artificial intelligence;jess;matlab;ontology (information science);pixel;production system (computer science);programming tool;prototype;protégé;user interface;voxel	Mariana Reyes-Perez;Jan Broekelmann;Jürgen Gausemeier	2009			data mining;manufacturing execution system;computer science;systems engineering;business system planning;computer-integrated manufacturing;expert system	AI	-30.161084712051988	-6.840268676791611	175773
da53cf6b633df4f65d44b2e2ed43290c6254c79e	spatial objects indexing and retrieval for mobile powerpoint	prototypes;tree data structures image retrieval indexing mobile computing;spatial indexes;indexing;mobile communication;mobile handsets;mobile powerpoint pages editing spatial object indexing spatial object retrieval mr tree energy efficient retrieval;mobile communication spatial indexes mobile handsets indexing containers prototypes;containers	Different from the traditional office software, due to the limited storage and computational capability of mobile devices, how to support the page editing in a fast and energy efficient manner is fundamental. To address this challenge, we propose an MR*-tree based indexing mechanism for supporting the fast and energy-efficient retrieval of spatial objects in mobile Power Point pages. Based on which, techniques are developed to support the operations when editing mobile Power Point pages. Prototype has been implemented for supporting the operations of spatial objects in editing mobile Power Point pages.	mobile device;prototype	Jianming Xiang;Zhangbing Zhou;Cong Liu;Qun Wang	2014	2014 10th International Conference on Semantics, Knowledge and Grids	10.1109/SKG.2014.11	search engine indexing;mobile search;mobile telephony;mobile database;computer science;database;prototype;multimedia;world wide web;information retrieval	DB	-26.890034798401956	-0.20194355884962958	176016
61638da901270a8fafab67cd600fa9354f60d98e	mapreduce-based similarity join for metric spaces	distance function;study design;metric space;data processing;data type;similarity join;mapreduce;experimental evaluation;hadoop;open source	Cloud enabled systems have become a crucial component to efficiently process and analyze massive amounts of data. One of the key data processing and analysis operations is the Similarity Join, which retrieves all data pairs whose distances are smaller than a predefined threshold ε. Even though multiple algorithms and implementation techniques have been proposed for Similarity Joins, very little work has addressed the study of Similarity Joins for cloud systems. This paper focuses on the study, design and implementation techniques of cloud-based Similarity Joins. We present MRSimJoin, a MapReduce based algorithm to efficiently solve the Similarity Join problem. This algorithm efficiently partitions and distributes the data until the subsets are small enough to be processed in a single node. MRSimJoin is general enough to be used with data that lies in any metric space, thus it can be used with multiple data types and distance functions. We present guidelines to implement the algorithm in Hadoop, an open-source cloud system. The experimental evaluation of MRSimJoin shows that it has very good execution time and scalability properties.	algorithm;apache hadoop;cloud computing;dhrystone;disk partitioning;iteration;mapreduce;open-source software;performance evaluation;run time (program lifecycle phase);scalability	Yasin N. Silva;Jason M. Reed;Lisa M. Tsosie	2012		10.1145/2347673.2347676	computer science;theoretical computer science;data mining;database	DB	-31.86092207968281	-0.3324264422179861	176143
bc075714de0641a625087830c816b238d2bd037d	a compression technique to materialize transitive closure	base relacional dato;cover coefficient;sistema experto;data compression;intelligence artificielle;relational database;resolucion problema;fermeture transitive;indexing;retrieval effectiveness;directed graph;indexation;clustering indexing relationships;indizacion;base donnee relationnelle;transitive closure;artificial intelligence;document retrieval;compresion dato;inteligencia artificial;cluster validity;systeme gestion base donnee;systeme expert;sistema gestion base datos;database management system;decoupling coefficient;compression donnee;problem solving;resolution probleme;cierre transitivo;expert system	An important feature of database support for expert systems is the ability of the database to answer queries regarding the existence of a path from one node to another in the directed graph underlying some database relation. Given just the database relation, answering such a query is time-consuming, but given the transitive closure of the database relation a table look-up suffices. We present an indexing scheme that permits the storage of the pre-computed transitive closure of a database relation in a compressed form. The existence of a specified tuple in the closure can be determined from this compressed store by a single look-up followed by an index comparision. We show how to add nodes and arcs to the compressed closure incrementally. We also suggest how this compression technique can be used to reduce the effort required to compute the transitive closure.	approximation algorithm;b-tree;computation;computer data storage;data compression;database index;directed acyclic graph;directed graph;dynamic problem (algorithms);expert system;lookup table;on the fly;precomputation;reachability;relation (database);shortest path problem;topological sorting;transitive closure	H. V. Jagadish	1990	ACM Trans. Database Syst.	10.1145/99935.99944	data compression;transitive reduction;document retrieval;search engine indexing;directed graph;relational database;computer science;data mining;database;transitive closure;expert system;algorithm	DB	-25.574295080440034	1.810887693075147	176307
a330676ff7858db2b493c58cbf6f898cf124ea2f	"""corrigendum to """"maximum-margin classification of sequential data with infinitely-long temporal dependencies"""" [expert systems with applications 40 (11) (2013) 4519-4527]"""			expert system	Sotirios P. Chatzis	2013	Expert Syst. Appl.	10.1016/j.eswa.2013.06.042	calculus;algorithm	ML	-28.02042179108594	-7.643805640872231	176448
0b19eef429b8316c37fd78fc8171906cf9e59ad4	motion economy and planning	ai planning	We have formulated the motion economy problem as an AI planning problem. We have developed and formalized a model for representing the actions, environment and products of the domain. We have interpreted and represented the expertise of motion economy specialists as search control rules by using temporal formulas and used them to generate quality plans.	automated planning and scheduling;motion planning;temporal logic	Canan Özgen;Aysenur Birtürk	2006			simulation;automated planning and scheduling;economy;computer science	AI	-20.547902317690617	-8.369907646844212	176545
f8ab5f1d6f1da05eac9baf514637982c84494817	model-based fault diagnosis: knowledge acquisition and system design	model based approach;system design;knowledge acquisition;fault diagnosis;knowledge base	Within this paper a model-based approach for knowledge-based diagnosis-systems is discussed. The model is derived from the fault recognition and fault searching techniques used by maintenance experts in general. The presented model has implications on the knowledge acquisition method in a way that only such data that are usually known to the maintenance expert have to be gathered and interlinked. This means that knowledge acquisition has only to deal with a knowledge source that can be prestructured. The model for knowledge-based diagnosis-systems and the knowledge acquisition method has been successfully used to develop diagnosis systems for a SMD-insertion machine and an extrusion machine. Time needed for knowledge acquistion has been reduced.	knowledge acquisition;smt placement equipment;service mapping description;systems design	Thomas H. Schmidt	1989		10.1145/66617.66620	knowledge base;idef3;computer science;knowledge management;artificial intelligence;knowledge-based systems;data mining;knowledge extraction;systems design	AI	-29.520299505002516	-6.39250959905889	176555
02dec36def1e4d283a256ae5cd37c8cfe50fe8a9	a knowledge based framework for the design of soft-computing systems	soft-computing systems;knowledge base;soft computing	This paper presents a systematic framework for the design of intelligent decision support systems based upon soft computing paradigms like neural networks, genetic algorithms, simulated annealing and fuzzy logic. The approach applies knowledge based systems techniques to support development and application of models in these computing paradigms. The long-term goal of this research is to automate the design of soft-computing systems from a domain expert’s description of the problem situation and a set of input data.	soft computing	Satheesh Ramachandran;Madhav Erraguntla;Perakath C. Benjamin	2002			knowledge base;knowledge integration;systems engineering;knowledge management;knowledge-based systems;open knowledge base connectivity;data mining;knowledge extraction;domain knowledge	EDA	-30.54006459378223	-7.173703830978742	176655
13c174e9f2e988cad9c0307e6c1b796f61de5d34	a learning of adaptation knowledge for case-based reasoning system	case base reasoning		case-based reasoning;reasoning system	Kyung-Dal Cho;Jae-Pil Lee;Ki-Tae Kim;Su-Cheol Hwang	1999			model-based reasoning;deductive reasoning;opportunistic reasoning;artificial intelligence;procedural reasoning system;machine learning;computer science;reasoning system;case-based reasoning;verbal reasoning;knowledge management;legal expert system	AI	-27.770253249285876	-7.812372637349278	176748
4730091309d1c99ef165c9c1cf96e3b7eb7f9045	negotiation support for compiling knowledge	knowledge based system;development environment;rapid prototyping;critiquing systems;knowledge acquisition;negotiation support;information exchange;authoring tool	Critiquing systems, a special kind of knowledge-based systems, can be seen as a personal assistant helping to reflect on a particular design and to improve it. They have successfully demonstrated their capability to aid users during design tasks. Critiquing systems make knowledge and expertise of different domain experts available to end users of authoring tools. Unfortunately it is a quite demanding and time consuming task to get the expertise of different distributed domain experts into such a system and to maintain it. That might explain why many critiquing systems only exist as prototypical implementations. We propose a collaborative development environment for supporting domain experts in constructing design-oriented critiquing systems. Our approach intends to support rapid prototyping, establishment and maintenance of critiquing systems. Especially domain experts not familiar with programming shall be enabled to participate in this process. We expect to improve and to ensure the knowledge base's quality by supporting information exchange and negotiation processes between domain experts.	collaborative development environment;compiler;expect;information exchange;knowledge base;knowledge-based systems;rapid prototyping;subject-matter expert	Marita Dücker;Bernd Gutkauf;Stefanie Thies	1999		10.1145/320297.320300	information exchange;computer science;knowledge management;artificial intelligence;knowledge-based systems;data mining;development environment	AI	-29.987548153539624	-5.491503850818518	176765
48c1329713309283d87fc7b67d17132416d17c2b	autoadmin: self-tuning database systemstechnology.	database system;physical design;relational database;similarity function;physical database design	The AutoAdmin research project was launched in the Fall of 1996 in Microsoft Research with the goal of making database systems significantly more self-tuning. Initially, we focused on automating the physical design for relational databases. Our research effort led to successful incorporation of our tuning technology in Microsoft SQL Server and was subsequently also followed by similar functionality in other relational DBMS products. In 1998, we developed the concept of self-tuning histograms, which remains an active research topic. We also attempted to deepen our understanding of monitoring infrastructure in the relational DBMS context as this is one of the core foundations of the “monitor-diagnose-tune” paradigm needed for making relational DBMS selftuning. This article gives an overview of our progress in the above three areas – physical database design, self-tuning histograms and monitoring infrastructure.	database design;microsoft research;microsoft sql server;physical design (electronics);programming paradigm;relational database management system;self-replication;self-tuning;total cost of ownership	Sanjay Agrawal;Nicolas Bruno;Surajit Chaudhuri;Vivek R. Narasayya	2006	IEEE Data Eng. Bull.		physical design;database theory;relational model;relational database;computer science;database model;database;database catalog;view;database schema;physical data model;database design;spatiotemporal database;component-oriented database	DB	-33.60115028204874	3.1558662519567804	177154
0b6febe884c29a1197b8da441ba940dcecb0702e	noa - a normative agent architecture		NoA is an agent architecture that supports the development of agents motivated by norms: obligations, permissions and prohibitions. Obligations motivate a normative agent to act: a motive to achieve a state of affairs or to perform some action. Prohibitions restrict an agent's behaviour, whereas permissions allow an agent to pursue certain activities. To test the architecture, NoA agents arc applied to automated business transaction scenarios where the correct execution of contracts is paramount to create a situation of trust.	agent architecture	Martin J. Kollingbaum;Timothy J. Norman	2003				AI	-20.168057445775027	-9.584436596898938	177291
f74417b926c4982753cd2c441ddcf0c99ebb8237	the ibn darraj project: spatiotemporal reasoning engine based on evidence combination			semantic reasoner;spatial–temporal reasoning	Mohammad Hossein Haqiqat khah;Babak Nadjar Araabi	2016			machine learning;semantic reasoner;computer science;artificial intelligence	AI	-27.148268589419605	-7.750266057435581	177755
d42c0a35d1c549921c1f3be6162c28e5023b57e4	spatial queries in the presence of obstacles	camino mas corto;busqueda informacion;arbre r;obstaculo;shortest path;geographic information science;base donnee;euclidean theory;metric space;espace metrique;query processing;selected works;espacio metrico;information retrieval;efficient algorithm;interrogation base donnee;database;interrogacion base datos;base dato;plus court chemin;euclidean distance;r tree;spatial database;arbol r;vecino mas cercano;recherche information;traitement question;base donnee spatiale;indexation;nearest neighbor;theorie euclidienne;plus proche voisin;bepress;nearest neighbour;information system;database query;obstacle;teoria euclidiana	Despite the existence of obstacles in many database applications, traditional spatial query processing utilizes the Euclidean distance metric assuming that points in space are directly reachable. In this paper, we study spatial queries in the presence of obstacles, where the obstructed distance between two points is defined as the length of the shortest path that connects them without crossing any obstacles. We propose efficient algorithms for the most important query types, namely, range search, nearest neighbors, e-distance joins and closest pairs, considering that both data objects and obstacles are indexed by R-trees. The effectiveness of the proposed solutions is verified through extensive experiments.	algorithm;closest pair of points problem;computation;computational geometry;computer data storage;database;entity;euclidean distance;experiment;r-tree;range searching;real life;shortest path problem;spatial query;visibility graph	Jun Zhang;Dimitris Papadias;Kyriakos Mouratidis;Manli Zhu	2004		10.1007/978-3-540-24741-8_22	r-tree;metric space;computer science;theoretical computer science;euclidean shortest path;data mining;euclidean distance;database;mathematics;geographic information system;shortest path problem;k-nearest neighbors algorithm;spatial database;information system;algorithm;spatial query	DB	-26.248699382141528	1.2083838632109936	177889
add07f1bf9edd2864432e7e270e7a574e87eba45	metric-based similarity search in unstructured peer-to-peer systems	own data autonomously;metric-based similarity search;query routing algorithm;query routing;metric space;metric routing;generic metric distance function;efficient search mechanism;efficient search;unstructured peer-to-peer system;similarity search	Peer-to-peer systems constitute a promising solution for deploying novel applications, such as distributed image retrieval. Efficient search over widely distributed multimedia content requires techniques for distributed retrieval based on generic metric distance functions. In this paper, we propose a framework for distributed metric-based similarity search, where each participating peer stores its own data autonomously. In order to establish a scalable and efficient search mechanism, we adopt a super-peer architecture, where super-peers are responsible for query routing. We propose the construction of metric routing indices suitable for distributed similarity search in metric spaces. Furthermore, we present a query routing algorithm that exploits pruning techniques to selectively direct queries to super-peers and peers with relevant data. We study the performance of the proposed framework using both synthetic and real data demonstrate its scalability over a wide range of experimental setups.	algorithm;dhrystone;image retrieval;peer-to-peer;routing;scalability;similarity search	Akrivi Vlachou;Christos Doulkeridis;Yannis Kotidis	2012	Trans. Large-Scale Data- and Knowledge-Centered Systems	10.1007/978-3-642-28148-8_2	computer science;theoretical computer science;data mining;database	DB	-29.54627752617212	-0.45366315957680725	178005
cc07f5e6940eb3ee9625c288d08846c084d02fdf	continuous access to cloud event services with event pipe queries	data movement overhead;query processing result;continuous query;continuous access;operational event pipe;long-standing sql query;single continuous query;query engine;cloud service;postgresql query engine;enterprise application;event service;event pipe query	When cloud services become popular, how to consume a cloud service efficiently by an enterprise application, as the client of the cloud service either on a device or on the application tier of the enterprise software stack, is an important issue. Focusing on the consumption of the real-time events service, in this work we extend the Data Access Object (DAO) pattern of enterprise applications for on-demand access and analysis of real-time events.#R##N##R##N#We introduce the notion of Operational Event Pipe for caching the most recent events delivered by an event service, and the on-demand data analysis pattern based on this notion. We implemented the operational event pipe as a special kind of continuous query referred to as Event Pipe Query (EPQ). An EPQ is a long-standing SQL query with User Defined Functions (UDFs) that provides a pipe for the stream data to be buffered and to flow continuously in the boundary of a sliding window; when not requested, the EPQ just maintains and updates the buffer but returns noting, once requested, it returns the query processing results on the selected part of the sliding window buffer, under the request-and-rewind mechanism. Integrating event buffering and analysis in a single continuous query leverages the SQL expressive power and the query engine's data processing capability, and reduces the data movement overhead.#R##N##R##N#By extending the PostgreSQL query engine, we implement this operation pattern as the Continuous Data Access Object (CDAO) - an extension to the J2EE DAO. While DAO provides static data access interfaces, CDAO adds dynamic event processing interfaces with one or more EPQs.		Qiming Chen;Meichun Hsu	2011		10.1007/978-3-642-25106-1_8	real-time computing;computer science;data mining;database	DB	-31.5585199720445	2.427686342431309	178050
35edd926ea34cb70cc1dfee35819dad67b1cad19	a study of partitioning and parallel udf execution with the sap hana database	rank join queries;top k queries;exploratory queries	Large-scale data analysis relies on custom code both for preparing the data for analysis as well as for the core analysis algorithms. The map-reduce framework offers a simple model to parallelize custom code, but it does not integrate well with relational databases. Likewise, the literature on optimizing queries in relational databases has largely ignored user-defined functions (UDFs). In this paper, we discuss annotations for user-defined functions that facilitate optimizations that both consider relational operators and UDFs. In this paper we focus on optimizations that enable the parallel execution of relational operators and UDFs for a number of typical patterns. A study on real-world data investigates the opportunities for parallelization of complex data flows containing both relational operators and UDFs.	algorithm;experiment;linear algebra;mapreduce;mathematical optimization;parallel computing;query optimization;relational database;relational operator;sap hana;speedup;universal disk format;user-defined function	Philipp Große;Norman May;Wolfgang Lehner	2014		10.1145/2618243.2618274	computer science;theoretical computer science;data mining;database;conjunctive query	DB	-31.93955963469736	1.484826452999249	178258
180361ead4988a481824f3e83767837dfee634d4	solving vt in vital: a study in model construction and knowledge reuse	modelizacion;representacion conocimientos;knowledge based system;knowledge reuse;top down;performance;conception;vt problem;gramatica generativa;vital;sisyphus;resolucion problema;modelisation;machine learning;elevateur;levels of abstraction;elevator;operational conceptual modelling language;generative grammar;diseno;grammaire generative;design;elevador;rendimiento;knowledge representation;representation connaissances;modeling;domain ontology;problem solving;resolution probleme	In this paper we discuss a solution to the Sisyphus II elevator design problem developed using the VITAL approach to structured knowledge-based system development . In particular we illustrate in detail the process by which an initial model of Propose & Revise problem solving was constructed using a generative grammar of model fragments and then refined and operationalized in the VITAL operational conceptual modelling language (OCML) . In the paper we also discuss in detail the properties of a particular Propose & Revise architecture , called ‘‘Complete-Model-then-Revise’’ , and we show that it compares favourably in terms of competence with alternative Propose & Revise models . Moreover , using as an example the VT domain ontology provided as part of the Sisyphus II task , we critically examine the issues af fecting the development of reusable ontologies . Finally , we discuss the performance of our problem solver and we show how we can use machine learning techniques to uncover additional strategic knowledge not present in the VT domain . ÷ 1996 Academic Press Limited	generative grammar;knowledge-based systems;machine learning;modeling language;ocml;ontology (information science);problem solving;solver;vitalism	Enrico Motta;Arthur Stutt;Zdenek Zdráhal;Kieron O'Hara;Nigel Shadbolt	1996	Int. J. Hum.-Comput. Stud.	10.1006/ijhc.1996.0018	knowledge representation and reasoning;design;generative grammar;performance;elevator;computer science;artificial intelligence;knowledge-based systems;algorithm	AI	-24.314996483109837	-3.176973291223413	178279
153c2655d90f7882493f643f25cdabb68f89bb6b	applying cbr and object database techniques in chemical process design	raisonnement base sur cas;razonamiento fundado sobre caso;base donnee;design process;case base reasoning;securite;preparation voie chimique;reutilizacion;connaissance;preparacion via quimica;database;object database;base dato;conocimiento;reuse;process design;preparacion serie fabricacion;resolucion problema;knowledge;chimie;safety;chemistry;base donnee orientee objet;quimica;object oriented databases;process planning;case based reasoning;process engineering;seguridad;preparation gamme fabrication;chemical processing;reutilisation;problem solving;resolution probleme;processus conception	The aim of this paper is to introduce a new method for finding and reusing process equipment design and inherently safer process configurations by case-based reasoning (CBR) and object database techniques. CBR is based on finding most alike existing solutions and applying the knowledge of their properties for solving new problems in the early phases of design. This supports design engineer's knowledge by allowing a systematic reuse of existing experience in order to improve the quality and safety of new designs. The possibilities of CBR and object database techniques in chemical process engineering field have been illustrated by two prototype applications.	case-based reasoning;prototype	Timo Seuranen;Elina Pajula;Markku Hurme	2001		10.1007/3-540-44593-5_52	process design;case-based reasoning;design process;computer science;artificial intelligence;reuse;knowledge	DB	-24.339944018225346	-4.207098059345535	178484
7fb61de2595e7722ad5f3800d4cc0081da77a27e	a probabilistic index structure for querying future positions of moving objects		We are witnessing a tremendous increase in internet connected, geo-positioned mobile devices, e.g., smartphones and personal navigation devices. Therefore, location related services are becoming more and more important. This results in a very high load on both communication networks and server-side infrastructure. To avoid an overload we point out the beneficial effects of exploiting future routes for the early generation of the expected results of spatio-temporal queries. Probability density functions are employed to model the uncertain movement of objects. This kind of probable results is important for operative analytics in many applications like smart fleet management or intelligent logistics. An index structure is presented which allows for a fast maintenance of query results under continuous changes of mobile objects. We present a cost model to derive initialization parameters of the index and show that extensive parallelization is possible. A set of experiments based on realistic data shows the efficiency of our approach.		Philip Schmiegelt;Andreas Behrend;Bernhard Seeger;Wolfgang Koch	2013		10.1007/978-3-642-40683-6_9	simulation;computer science;theoretical computer science;data mining;database	DB	-26.55217016453097	0.3205895313123195	178717
9a2704fe9c339efe10ade4e5c67861bb218a2807	bulk loading large collections of hyperlinked resources	web graph;bulk loading;larges graphs;relational database;indexation	The problem of loading large collections of hyperlinked resources into a relational database is complicated with inter-node references when these references cannot be indexed. We show that this scenario can arise in many real life hyperlinked resources and propose several solutions to address the problem. We run some experiments over a graph of the Web with 178 million nodes and around 1 billion edges and report our results.	experiment;graph (discrete mathematics);hyperlink;real life;relational database;world wide web	Davood Rafiei	2005		10.1145/1083356.1083413	relational database;computer science;data mining;database;world wide web	DB	-30.769712598796236	0.41469114406837376	178780
7f5380d3c1e85d2dbe02b803e3522c7e41c42898	implementing flexible operators for regular path queries	computer science and information systems	Given the heterogeneity of complex graph data on the web, such as RDF linked data, a user wishing to query such data may lack full knowledge of its structure and irregularities. Hence, providing users with flexible querying capabilities can be beneficial. The query language we adopt comprises conjunctions of regular path queries, thus including extensions proposed for SPARQL 1.1 to allow for querying paths using regular expressions. To this language we add two operators: APPROX, supporting standard notions of approximation based on edit distance, and RELAX, which performs query relaxation based on RDFS inference rules. We describe our techniques for implementing the extended language and present a performance study undertaken on two real-world data sets. Our baseline implementation performs competitively with other automaton-based approaches, and we demonstrate empirically how various optimisations can decrease execution times of queries containing APPROX and RELAX, sometimes by orders of magnitude.	approximation;automaton;baseline (configuration management);edit distance;linear programming relaxation;linked data;query language;rdf schema;regular expression;resource description framework;sparql	Petra Selmer;Alexandra Poulovassilis;Peter T. Wood	2015			computer science;theoretical computer science;data mining;database;rdf query language	DB	-32.598570043739784	3.842377971777637	178791
21072183acb35d639199001365cd12541dd3d307	a model of informational control in active network structures in case of an incomplete awareness of the principal	influence level;network structure;network structure element;beneficial network structure;active network structure;informational control;incomplete awareness	A model of informational control in network structures is analyzed. The result of informational control depends on the principalu0027s awareness and the influence levels of network structure elements (agents). Finally, the issue of beneficial network structures (from the principalu0027s viewpoint) is studied.		D. N. Fedyanin;Alexander G. Chkhartishvili	2013	Automation and Remote Control	10.1134/S0005117913120175	engineering;artificial intelligence;data mining;management science	Robotics	-23.13183956967858	-8.397685321818862	178807
81511fc6ff32ef4f953cb7cfa83c4b7a4de2c40e	map/reduce on emf models	big data;map reduce;meta modeling;emf;cloud computing	Map/Reduce is the programming model in cloud computing. It enables the processing of data sets of unprecedented size, but it also delegates the handling of complex data structures completely to its users. In this paper, we apply Map/Reduce to EMF-based models to cope with complex data structures in the familiar an easy-to-use and type-safe EMF fashion, combining the advantages of both technologies. We use our framework EMF-Fragments to store very large EMF models in distributed key-value stores (Hadoop's Hbase). This allows us to build Map/Reduce programs that use EMF's generated APIs to process those very large EMF-models. We present our framework and two example Map/Reduce jobs for querying software models and for analyzing sensor data represented as EMF-models.	apache hbase;apache hadoop;attribute–value pair;cloud computing;data structure;eclipse modeling framework;mapreduce;programming model;type safety	Markus Scheidgen;Anatolij Zubow	2012		10.1145/2446224.2446231	embedded system;real-time computing;big data;cloud computing;computer science;electromotive force;operating system;data mining	ML	-32.48156787530932	1.469584877751841	179049
329d579775d24a73ce0edc02723d13b0e01dfef6	driver input selection for main-memory multi-way joins	concept drift;regression tree;data streams	Stream query processing has a particularly broad range of applications from sensor data processing and internet traffic analysis to runtime monitoring of stock market and server logs, and scientific simulations. This work focuses on multi-way join queries over streamed data, which are processed with the help of a n-ary join. More specifically, we propose a novel main-memory variant of the influential MJoin operator proposed in [35], which processes input data in batches with a view to improving the CPU efficiency, and explicitly controls the order of execution within each batch without being restricted by the time of input arrival, as current state-of-the-art solutions do. To this end, we also propose policies for selecting the execution order, and we show that our approach can yield important performance benefits.	central processing unit;computer data storage;database;feature selection;server (computing);simulation;streaming media;traffic analysis	Emmanouil Valsomatzis;Anastasios Gounaris	2013		10.1145/2480362.2480521	real-time computing;computer science;concept drift;operating system;machine learning;decision tree;data mining;database;data stream mining;world wide web	DB	-28.662750868082636	-1.6662666689363137	179151
31f7367bffaae16ac9b8e71bfd9391830b50c30a	partial replica selection based on relevance for information retrieval	word boundary identification;information retrieval;chinese text segmentation;logistic regression;multi word terms	Partial collection replication improves performance and scalability of a large-scale distributed information retrieval system by distributing excessive workloads, reducing network latency, and restricting some searches to a small percentage of data. In this paper, we rst examine queries from real system logs and show that there is su cient query locality in real systems to justify partial collection replication. We then present a method for constructing a hierarchy of partial replicas from a collection where each replica is a subset of all larger replicas, and extend the inference network model to rank and select partial replicas. We compare our new selection algorithm to previous work on collection selection over a range of tuning parameters. For a given query, our replica selection algorithm correctly determines the most relevant of the replicas or original collection, and thus maintains the highest retrieval e ectiveness while searching the least data as compared with the other ranking functions. Simulation results show that with load balancing, partial replication consistently improves performance over collection partitioning on multiple disks of a shared-memory multiprocessor and it requires only modest query locality.	bayesian network;information retrieval;load balancing (computing);locality of reference;multiprocessing;network model;relevance;scalability;selection algorithm;shared memory;simulation	Zhihong Lu;Kathryn S. McKinley	1999		10.1145/312624.312662	computer science;machine learning;data mining;database;logistic regression;world wide web;information retrieval	Web+IR	-28.86414663418056	3.040934356271617	179266
883b3ba65d3a9f8f95eb3a634db121d687f6820a	automatic generation of simulation program by xml-prolog	automatic generation	"""We have developed an intelligent simulation development environment and intelligent search engine using """"XML-Prolog"""" for as a framework for efficient and stable simulation development. This development environment has a cycle that consists of three elements (modeling, programming, and result generation) which are managed as XML documents. To illustrate the framework, a Monte Carlo simulation and sampling of resultant distributions are performed by logically merging knowledge concerning Monte Carlo methods and sampling techniques retrieved by intellectual means. Knowledge of how programs communicate is added the already merged knowledge and the XML documents are generated. The programming process comprises of simulation conditions and initial values programs which are generated by XML documents. In this paper, we describe automatically generating programs for Monte Carlo simulation with LPWS (Legendre polynomial weighted sampling) using XML-Prolog."""	legendre polynomials;monte carlo method;polynomial;prolog;resultant;sampling (signal processing);simulation;web search engine;xml	Ryo Fukuhara;Tomoharu Matsunaka;Kazutaka Kitamori	2005			simulation;computer science;theoretical computer science;data mining	AI	-30.07573950785698	-4.581520801968273	179407
bbcfa6b02f39ba366c5f28ca4133314c7d56848f	coordination with collective and individual decisions	decision sequentielle;politica optima;decision models;sequential decision;disaster;multiagent system;decision secuencial;sismo;diminution cout;seisme;cost reduction;partial observability;prise de decision;intelligence artificielle;optimal policy;earthquakes;teoria decision;large scale;observabilidad parcial;multiple decision;theorie decision;sinistre;decision theory;decision theoretic;collective learning;terrorisme;observabilite partielle;artificial intelligence;decision multiple;coordinacion;partial observation;inteligencia artificial;reduccion costes;sistema multiagente;toma decision;politique optimale;terrorismo;apprentissage collectif;cost lowering;aprendizaje colectivo;systeme multiagent;terrorism;coordination;siniestro	The response to a large-scale disaster, e.g. an earthquake or a terrorist incident, urges for low-cost policies that coordinate sequential decisions of multiple agents. Decisions range from collective (common good) to individual (self-interested) perspectives, intuitively shaping a two-layer decision model. However, current decision theoretic models are either purely collective or purely individual and seek optimal policies. We present a two-layer, collective versus individual (CvI) decision model and explore the tradeoff between cost reduction and loss of optimality while learning coordination skills. Experiments, in a partially observable domain, test our approach for learning a collective policy and results show near-optimal policies that exhibit coordinated behavior.	experiment;noise shaping;partially observable system;theory	Paulo Trigo;Anders Jonsson;Helder Coelho	2006		10.1007/11874850_8	collaborative learning;decision model;disaster;decision theory;artificial intelligence;terrorism;operations research;statistics	ML	-23.442884372213154	-9.12273494257565	179536
3bcf5a4221ccea0f7c8030f85f9f8a2f1f45996c	intention reconcilation in the context of teamwork: an initial empirical investigation	representacion conocimientos;multiagent system;systeme intelligent;systeme cooperatif;sistema inteligente;prise decision;satisfiability;cooperative systems;intelligent system;information system;knowledge representation;sistema multiagente;toma decision;representation connaissances;systeme information;systeme multiagent;environmental factor;sistema informacion	With growing opportunities for individually motivated agents to work collaboratively to satisfy shared goals, it becomes increasingly important to design agents that can make intelligent decisions in the context of commitments to group activities. In particular, agents need to be able to reconcile their intentions to do team-related actions with other, conflicting intentions. We present the SPIRE experimental system that allows the process of intention reconciliation in team contexts to be simulated and studied. SPIRE enables us to examine the influence of team norms and environmental factors on team members faced with conflicting intentions, as well as the effectiveness of different intention-reconciliation strategies. We discuss results from pilot experiments that confirm the reasonableness of our model of the problem and illustrate some of the issues involved, and we lay the groundwork for future experiments that will allow us to derive principles for designers of collaboration-capable agents.	experiment;experimental system;multi-agent system;problem solving;simulation;software design	David G. Sullivan;Alyssa Glass;Barbara J. Grosz;Sarit Kraus	1999		10.1007/3-540-48414-0_10	knowledge representation and reasoning;simulation;computer science;artificial intelligence;computer security;information system;satisfiability	AI	-23.26747756082946	-8.376272867259333	179985
a141d3b23f26066c7b6511d4e8d4ed615884578f	coordinating self interested autonomous planning agents		Coordination in multi-agent planning systems aims at ensuring that the plans of the participating agents do not conflict and the individual as well as the common goals of the agents can be achieved. In the multi-agent planning literature, one can distinguish three main approaches to coordination. In the first approach coordination between the agents is established after the completion of the individual planning processes. It is assumed that agents independently work on their own part of the planning problem and achieve a solution for it. Then, in an after-planning coordination phase, possible conflicts between independently generated individual plans are resolved and positive interactions between them are exploited by exchanging and revising parts of the individual plans. The second approach treats coordination and planning as intertwined processes where the agents continuously exchange planning information to arrive at a joint solution. In the third approach coordination takes place before the agents make their plans. Well-known examples of the pre-planning coordination approach use implicit coordination techniques such as social laws and conventions, and (negotiation) protocols such as the Contract Net protocol. We focus on coordination between agents that are self-interested, do not want to be interfered with during their individual planning processes and do not want to revise their plans when a joint plan has to be composed. Examples are (i) planning for multi-modal transportation tasks by several independent and competitive transportation companies, (ii) manufacturing tasks, and (iii) patient-centered health-care systems. It can easily be seen that the requirements of autonomous planning and revision-free combination remove the first two coordination approaches from consideration: post-planning coordination will, in general, require agents to revise their individual plans, while coordination during planning violates the requirement of autonomous planning. Therefore, we need to coordinate the agents before the planning phase. While existing pre-planning coordination research focuses on implicit coordination approaches — constraints are imposed independently of the particular goals or tasks the agents have to solve — we are looking for an explicit coordination approach: based on the specific set of tasks to be achieved, the assignment policies, and the (inter-agent) dependencies, we specify which constraints have to be imposed on the tasks to achieve revision-free coordination between the agents.	augmented assignment;automated planning and scheduling;autonomous robot;contract net protocol;interaction;modal logic;multi-agent system;requirement	Adriaan ter Mors;Cees Witteveen	2005			machine learning;artificial intelligence;human–computer interaction;computer science	AI	-19.589856360813698	-9.28191762885881	180229
a520b88d7edd073657097bcfc6f0569309c051eb	interval methods in knowledge representation			interval arithmetic;knowledge representation and reasoning	Vladik Kreinovich	2017	International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems	10.1142/S0218488517970030		Robotics	-27.815821703764158	-7.622932870303809	180464
80f6870a4065b6a3acb1af162924ceb92f613da3	cross-searching subject gateways: the query routing and forward knowledge approach	internet;information services;indexing;information retrieval;world wide web;comparative analysis		routing	John Kirriemuir;Dan Brickley;Susan Welsh;Jon Knight;Martin Hamilton	1998	D-Lib Magazine			DB	-29.380131049988176	-0.5134550712859942	180498
c6d959d0fec5d730722644b6b632e8ce5dee0894	a tool for performance evaluation of database systems for small computer systems	database system;software tool;performance evaluation;query optimization;statistical computing;software tools;database management system;performance evaluation tools;database query;database performance	This paper proposes a tool that provides a comparative evaluation of the performance of database systems. The tool has four components, the input component, the database-queries components, the statistical computation component and the report component. The input includes small, medium and large files as well as parameters relating to the database management systems being evaluated, the computers used for evaluation and ,the computer access normalization factor. The database-queries component incIudes several prepared sets of queries that test access paths, range retrievals, joins, query optimization and updates. The database management systems under evaluation executes the queries on small, medium and large files. The statistical computation component accrues the performance timings and executes statistical functions on these data. Finally, the report component provides the resulting evaluation in a tabular form. The tool has a user friendly interface. It provides reliable and accurate results even when the database management systems are running on different computers. INTRODUCTION One of the attractions of relational databases is that the user does not need to specify the access path to the data to be retrieved (Finkelstein, Schkolnick, & Tiberio 1988). Database management systems provide variety of indices that determine the paths (Mackert & Lohman 1986). Query optimization modules are also provided. Unfortunately, the database management system manuals contain little or no information on performance. Furthermore, indexing does not always perform according to specifications. The literature provides a considerable number of theoretical analyses of performance issues (Bitton and Turbyiill, 1988; Ip et al., 1983; Mackert and Lohman,1989, Motzkin, 1991 and many others). However, very little is available about the performance of commercial database systems. Some information is available on the performance of SQL (Ortali, 1988), and INGRES (Rowe and Stonebraker, 1986; Youssefi and Wong, 1986). Information may be available in the form of internal memos or reports in some corporations. Unfortunately the user has no easy way of knowing what performance to expect. There “Permission to copy without fee alI or part of this material is granted provided that the copies are not made or distributed for direct commerical advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copying is by Penmssion of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission.” @ 19% ACM O-89791-658-1 95 0002 3.50 are no convenient tools that perform evaluation and comparison of database management system. The tool proposed here provides performance evaluation of database systems. It determines the conditions under which indexes perform the best, as well as those under which they do not perform well. The input includes small, medium and large files, information relating to the database management systems being evaluated, the computers used for evaluation and the computer acc:ess normalization factor. The tool includes several sets of database queries that test access paths, range retrievals, joins, query optimization and updates. The statistical computation component of the tool accrues the performance timings and executes statistical functions on these data. Finally, the report component provides the resulting evaluation in a tabular form. The performance evaluation tool PET has a user friendly interface. It is very easy to use. The user needs only to load the database system into the tool using the Create and Open Database buttons, create indexes if so desired and click the analysis button. The pre-stored queries are then executed followed by statistical analysis. Reliable and accurate results in report form are then displayed. The proposed tool is based on an early performance model developed by Motzkin & Boume (Motzkin & Boume 1993). THE PERFORMANCE EVALUATION TOOL In this section we describe the Performance Evaluation Tool PET that we designed and implemented. The PET allows the user to create the database, load the data from a file, create the indices, drop the indices, run groups of queries , set the configuration of the system, analyze the timings and generate on screen and hard copy results. The PET provides on-line help for each of its functions. The graphical user interface is provided using VISUAL BASIC. The current version of the PET runs under Microsoft Windows. The overall structure When a user initiates PET, he/she is presented with the Main Menu of the system. The main menu is as shown in Figure 1. Each function of the menu can be invoked by clicking the appropriate command button. As will be seen in the following sections each of the functions has its own set of windows, when the user is done with the tool, he/she returns to the main menu	computation;computer;fourier–motzkin elimination;graphical user interface;ingres;lazy evaluation;mathematical optimization;microsoft windows;online and offline;performance evaluation;polyethylene terephthalate;query optimization;relational database;sql;table (information);usability;visual basic	Dalia Motzkin;R. Ellendula;M. Kamali;S. Tiwari	1995		10.1145/315891.316040	cost database;query optimization;database theory;database server;database tuning;computer science;data administration;database model;data mining;database;denormalization;database schema;physical data model;computational statistics;information retrieval;database testing;database design;component-oriented database	DB	-29.361596513994627	3.5682345180862027	180617
b2102e4b7c890e0bcfde6f1945fb9f015f47ffb5	a framework for problem-solving knowledge mining from users' actions	problem solving	In this paper, we present an approach to capture problem-solving knowledge using a promising technique of data and knowledge discovery based on a combination of sequential pattern mining and association rules discovery.	data mining	Roger Nkambou;Engelbert Mephu Nguifo;Olivier Couturier;Philippe Fournier-Viger	2007			software mining;computer science;knowledge management;data science;data mining;knowledge extraction;data stream mining;k-optimal pattern discovery	NLP	-33.538761369299046	-5.662501416291465	180857
9ae46f1de2b61bed29f9473056fe326f8d584650	algorithms for fundamental spatial aggregate operations over regions	spatial aggregates;vector algorithms;aggregate operations	Aggregate operators are a useful class of operators in relational databases. In this paper, we examine spatial aggregate operators over regions. Spatial aggregates are defined to operate over a set of regions, and return a single region as a result. We systematically identify individual spatial aggregate operations by extending existing spatial operations into aggregate form. Semantic meaning for each operator is defined over a specified data model. Once defined, algorithms for computing spatial aggregates over regions are provided. We show that all proposed aggregates can be computed using a single algorithm. Furthermore, we provide serial and parallel algorithm constructions that can take advantage of vector co-processors, such as graphical processing units (GPUs), and that can be integrated into map/reduce queries to take advantage of big data-style clusters. Example queries and their results are provided.	aggregate data;aggregate function;big data;central processing unit;data model;graphical user interface;graphics processing unit;mapreduce;parallel algorithm;prototype;relational database;spatial analysis	Mark McKenney;Brian Olsen	2013		10.1145/2534921.2534930	operations management;theoretical computer science;data mining;mathematics	DB	-29.753649231677322	2.874838939469783	181451
e1c6a81849aaa0eaaaf99da003a46d6c5192e9e1	logos: a constraint-directed reasoning shell for operations management	expert systems;job shop scheduling single machine scheduling dynamic scheduling production facilities problem solving electric breakdown expert systems aircraft chaos instruments;dynamic model;inference mechanisms;temporal constraints;software tools expert systems inference mechanisms scheduling;scheduling;constraint system;software tools;operations management;logos constraint directed reasoning shell operations management logos reasoning shell expert system reactive scheduling constraint system temporal constraints problem solving approach constraint based reasoning dynamic modeling;problem solving;expert system	A description is given of the Logos reasoning shell, an expert system that performs reactive scheduling, i.e. reschedules operations in the context of new information. Logos' constraint system supports the use of temporal constraints as well as the need to add and retract constraints incrementally. Logos' problem-solving approach is discussed and constraint-based reasoning and dynamic modeling, both of which are used by Logos, are examined. Some current applications are described.<<ETX>>	case-based reasoning;expert system;problem solving;scheduling (computing)	Alex Chao-Chiang Meng;Michael Sullivan	1991	IEEE Expert	10.1109/64.73814	legal expert system;simulation;computer science;artificial intelligence;machine learning;reasoning system;scheduling;expert system	AI	-19.80015324809956	-6.111283126982785	181462
5d7c0cb351c8a90345890de4db5dc73188fb287a	triplecloud: an infrastructure for exploratory querying over web-scale rdf data	distributed data;google;distributed database;query processing;search engines;key value stores rdf cloud computing sparql;google app engine triplecloud exploratory query web scale rdf data cloud based key value store robust query engine complex query;resource description framework;servers;internet;engines;resource description framework engines distributed databases google scalability servers;web based management;distributed databases;semantic web;cost effectiveness;sparql;scalability;entry barrier;key value stores;similarity search;search engines internet query processing;rdf;cloud computing;massive data sets	As the availability of large scale RDF data sets has grown, there has been a corresponding growth in researchers' and practitioners' interest in analyzing and investigating these data sets. However, given their size and messiness, there is significant overhead in setting up the infrastructure to store and query them. In this paper, we present Triple Cloud, a system that aims to lower the entry cost to exploring Web-scale RDF data sets. The system takes advantage of existing cloud based key-value stores (e.g.BigTable, HBase) to both enable scalability as well as hide the complexities of infrastructure deployment and maintenance. It layers over these key-value stores a robust query engine able to return approximate answers. We test the scalability of the approach scaling to over 3 billion triples for complex queries. In addition to an implementation over HBase, Triple Cloud runs over the Google App Engine, allowing us to perform a cost evaluation of the approach.	apache hbase;approximation algorithm;attribute–value pair;exploratory testing;google app engine;google bigtable;image scaling;overhead (computing);resource description framework;scalability;software deployment	Christophe Guéret;Spyros Kotoulas;Paul T. Groth	2011	2011 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology	10.1109/WI-IAT.2011.166	computer science;rdf;data mining;database;world wide web;distributed database;information retrieval	DB	-31.647740085917654	0.31792527355105443	181528
beda07aed87f2d4fc0f2569a46362dba3e70ab8e	instance-level worst-case query bounds on r-trees	instance level worst case bound;r tree;theory;performance analysis	Even with its significant impacts on the database area, the R-tree is often criticized by its lack of good worst-case guarantees. For example, in range search (where we want to report all the data points in a query rectangle), it is known that on adversely designed datasets and queries, an R-tree can be as slow as a sequential scan that simply reads all the data points. Nevertheless, R-trees work so well on real data that they have been widely implemented in commercial systems. This stark contrast has caused long-term controversy between practitioners and theoreticians as to whether this structure deserves its fame. This paper provides theoretical evidence that, somewhat surprisingly, R-trees are efficient in the worst case for range search on many real datasets. Given any integer $$K$$ K , we explain how to obtain an upper bound on the cost of answering all (i.e., infinitely many) range queries retrieving at most $$K$$ K objects. On practical data, the upper bound is only a fraction of the overhead of sequential scan (unless, apparently, $$K$$ K is at the same order as the dataset size). Our upper bounds are tight up to a constant factor, namely they cannot be lowered by more than $$O(1)$$ O ( 1 ) times while still capturing the most expensive queries. Our upper bounds can be calculated in constant time by remembering only three integers. These integers, in turn, are generated from only the leaf MBRs of an R-tree, but not the leaf nodes themselves. In practice, the internal nodes are often buffered in memory, so that the integers aforementioned can be efficiently maintained along with the data updates and made available to a query optimizer at any time. Furthermore, our analytical framework introduces instance-level query bound as a new technique for evaluating the efficiency of heuristic structures in a theory-flavored manner (previously, experimentation was the dominant assessment method).	best, worst and average case;data point;full table scan;heuristic;mathematical optimization;overhead (computing);query optimization;r-tree;range query (data structures);range query (database);range searching;response time (technology);selectivity (electronic);time complexity;tree (data structure);universal quantification	Yufei Tao;Yi Yang;Xiaocheng Hu;Cheng Sheng;Shuigeng Zhou	2013	The VLDB Journal	10.1007/s00778-013-0339-5	r-tree;computer science;theoretical computer science;data mining;database;range query;theory;algorithm	DB	-24.77484051125682	1.2707101228006463	181689
bdac7d3e2ccf9acf869b2d0718acdee178b4e178	lineage encoding: an efficient wireless xml streaming supporting twig pattern queries	query processing;wireless broadcast;wireless communication;indexes;twig pattern matching;mobile communication;xml;xml wireless communication cities and towns query processing mobile communication indexes encoding;cities and towns;wireless xml broadcasting methods lineage encoding wireless xml streaming twig pattern queries xml dissemination scheme mobile computing xml data streaming wireless environment structure indexing xml elements text content lineage code twig pattern query processing mobile clients;mobile computing;encoding;xml streaming twig pattern matching wireless broadcast;xml mobile computing query processing;xml streaming	In this paper, we propose an energy and latency efficient XML dissemination scheme for the mobile computing. We define a novel unit structure called G-node for streaming XML data in the wireless environment. It exploits the benefits of the structure indexing and attribute summarization that can integrate relevant XML elements into a group. It provides a way for selective access of their attribute values and text content. We also propose a lightweight and effective encoding scheme, called Lineage Encoding, to support evaluation of predicates and twig pattern queries over the stream. The Lineage Encoding scheme represents the parent-child relationships among XML elements as a sequence of bit-strings, called Lineage Code(V, H), and provides basic operators and functions for effective twig pattern query processing at mobile clients. Extensive experiments using real and synthetic data sets demonstrate our scheme outperforms conventional wireless XML broadcasting methods for simple path queries as well as complex twig pattern queries with predicate conditions.	access time;correctness (computer science);database;depth-first search;experiment;line code;lineage (evolution);mobile computing;network packet;path (graph theory);pattern matching;predicate (mathematical logic);streaming xml;synthetic data;tree traversal;twig	Jun Pyo Park;Chang-Sup Park;Yon Dohn Chung	2013	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2011.202	database index;xml encryption;simple api for xml;xml;mobile telephony;streaming xml;computer science;fast infoset;theoretical computer science;xml framework;soap;xml database;database;xml signature;mobile computing;world wide web;efficient xml interchange;wireless;encoding	DB	-27.941985709842843	0.2702576882944383	181709
c659ad192d833f2967f8a9e5dce7bda0a0ffd352	dyntarm: an in-memory data structure for targeted strong and rare association rule mining over time-varying domains	association mining;query processing;storage management;temporal pattern in memory data structure targeted strong rule mining rare association rule mining time varying domain government agencies time stream data temporal data association rule mining algorithm dynamic targeted querying data processing latency dyntarm algorithm targeted association rule discovery rare association rule discovery data streams volatility index;trend analysis;itemset tree;data mining;data structures;trend analysis rare rule mining temporal mining itemset tree association mining stream mining;storage management data handling data mining data structures query processing;itemsets association rules heuristic algorithms market research algorithm design and analysis;stream mining;rare rule mining;temporal mining;data handling	Recently, with companies and government agencies saving large repositories of time stream/temporal data, there is a large push for adapting association rule mining algorithms for dynamic, targeted querying. In addition, issues with data processing latency and results depreciating in value with the passage of time, create a need for swifter and more efficient processing. The aim of targeted association mining is to find potentially interesting implications in large repositories of data. Using targeted association mining techniques, specific implications that contain items of user interest can be found faster and before the implications have depreciated in value beyond usefulness. In this paper, the DynTARM algorithm is proposed for the discovery of targeted and rare association rules. DynTARM has the flexibility to discover strong and rare association rules from data streams within the user's sphere of interest. By introducing a measure, called the Volatility Index, to assess the fluctuation in the confidence of rules, rules conforming to different temporal patterns are discovered.	algorithm;association rule learning;conformity;data structure;quantum fluctuation;volatility	Jennifer Lavergne;Ryan G. Benton;Vijay V. Raghavan;Alaaeldin M. Hafez	2013	2013 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)	10.1109/WI-IAT.2013.43	concept mining;association rule learning;computer science;data science;data mining;database;fsa-red algorithm;data stream mining;k-optimal pattern discovery	ML	-28.352879943337456	-1.9080428443478858	181731
a3dcd3ed6c9ef7519d9bf59fdaeccf3834ca0d77	cognitive systems for intelligent business information management in cognitive economy	cognitive economy;intelligent data analysis;business information management	This paper will present new theoretical and applied solutions for intelligent data analysis and information management in the fields of cognitive economics. Intelligent data analysis and information management are performed by information systems called cognitive systems, dedicated for semantic interpretation of acquired business information. To interpret the meaning of the analysed data, complex linguistic algorithms must be used, based on which it is possible to find the core information elements for business processes forecasting and economical knowledge management. The presentation of selected methods of semantic data analysis in cognitive economy, which allow to perform both local and global information management forms the main subject of this paper. Here, semantic analysis methods are dedicated to cognitive economics problems, namely the interpretation, analysis and assessment of the meaning of selected sets of economic/financial ratios. The meaning of the interpreted data sets is assessed by analysing the layers of meaning contained in data analysed sets. Obtained semantic information may be used in future business processes evaluation and forecasting. © 2014 Elsevier Ltd. All rights reserved.	algorithm;artificial intelligence;business process;cognition;information management;information system;knowledge management;semantic analysis (compilers);semantic interpretation	Lidia Ogiela;Marek R. Ogiela	2014	Int J. Information Management	10.1016/j.ijinfomgt.2014.08.001	information;cognitive models of information retrieval;computer science;knowledge management;marketing;personal information management;data mining;management science;information management;management;world wide web;information system	AI	-32.97495297150777	-8.372661598948094	181960
144f1edc46f0a10d114d61a8597e9330e6a3a175	multi query optimization in glade	sql on hadoop;abdur;database;query optimization;computer science multi query optimization in glade university of california;glade;big data;computer science;electrical engineering;distributed systems;merced florin rusu rafay	SQL-on-Hadoop systems, query optimization, data distribution over multiple nodes and parallelization techniques are few of the areas under extreme research these days. Big names like Amazon, Google, Microsoft and many more are working on implementing systems for faster access of data from multiple nodes, reducing data mobility and increasing the parallelization. Customer’s queries are retrieved and reviewed by the database systems in an efficient way in the least amount of time by the introduction of various parallelization techniques by running the same query in parallel over different nodes carrying the data. Apart from multi-threading parallelization, there is another way of parallelization that can be performed in order to further reduce retrieval time, hence improving the efficiency of the system; parallelization on user queries on top of a DBMS/RDBMS. In this paper, we will study one such technique of how multiple queries can run simultaneously on a system in order to increase the system efficiency by reducing the data retrieval from the storage. Maximum sharing of workload has been performed by generating optimal and ubiquitous join plans for a set of queries and then fed them to GLADE (Generalized Linear Aggregate Distributed Engine), a scalable distributed system for large scale data analytics. Our main work is centered on generating GLADE join plans for a Multi-Query, satisfying maximum number of queries in order to maximize data sharing and minimize data retrieval for each individual query.	glade interface designer;microform;query optimization	Abdur Rafay	2016	CoRR		query optimization;big data;computer science;theoretical computer science;data mining;database;programming language	DB	-30.608425430426696	0.5644231936774836	182773
bb15348ae39c00467e6c0f2b971b3ff6d08d1a6c	scalable classification over sql databases	databases;file servers;scalability characteristics;decision trees scalable classification sql databases data intensive operations middleware backend sql database scalability characteristics enhanced client microsoft sql server 7 data size;decision tree;probability;backend sql database;scalable classification;sql;information retrieval;client server systems;decision trees relational databases sql client server systems;enhanced client;data mining;databases middleware statistics classification tree analysis classification algorithms decision trees data mining probability predictive models file servers;sql databases;data size;classification algorithms;data intensive operations;statistics;middleware;predictive models;relational databases;classification tree analysis;decision trees;microsoft sql server 7	We identify data-intensive operations that are common to classifiers and develop a middleware that decomposes and schedules these operations efficiently using a backend SQL database. Our approach has the added advantage of not requiring any specialized physical data organization. We demonstrate the scalability characteristics of our enhanced client with experiments on Microsoft SQL Server 7.0 by varying data size, number of attributes and characteristics of decision trees.	data-intensive computing;decision tree;experiment;microsoft sql server;middleware;scalability	Surajit Chaudhuri;Usama M. Fayyad;Jeff Bernhardt	1999		10.1109/ICDE.1999.754963	statistical classification;data transformation services;data definition language;pl/sql;business intelligence markup language;stored procedure;open database connectivity;computer science;query by example;in-memory processing;decision tree;user-defined function;data mining;database;language integrated query;world wide web	DB	-31.876082081352678	1.5445366608607012	182786
d2e1e385bd89670491a27b1dc0f9c35acfffed74	mining adaptation rules from cases in cbr systems	raisonnement base sur cas;razonamiento fundado sobre caso;sistema experto;adquisicion del conocimiento;information retrieval;acquisition connaissances;similitude;recherche information;knowledge acquisition;similarity;recuperacion informacion;similitud;systeme expert;case based reasoning;expert system	Case based reasoning (CBR) is a well known framework to cope with ill-structured tasks, where no enough domain knowledge is available [2]. The main objective of CBR is to address the knowledge acquisition bottleneck. Namely, in CBR the reasoner does not make effort to build an abstract model for domain knowledge to solve the problem, instead, during the problem solving, it relies on the past similar cases, and attempts to find the appropriate solution for the problem at hand, by modifying the past similar solutions. However, CBR systems also require substantial knowledge acquisition effort (e.g. acquiring cases, case vocabulary, retrieval knowledge, adaptation knowledge [5]). This knowledge traditionally is derived from a domain expert. Accordingly, although the expert can not propose an abstract model to support the domain, s/he attempts to define some regularities in the domain, that makes it possible to reason with the cases. In fact, the knowledge necessary to solve the problem has been depicted in the cases. The expert defines the skill of using the right case in the right place (similarity), and also defines a formalism to modify the old cases such that address the new problem (adaptation). Defining this knowledge, if precision matters needs an accurate model of domain, proposed by expert (which is normally not available in CBR domains), and if precision is not assumed as a requirement, may fail to demonstrate acceptable reasoning skill. However, since in CBR there is no well-defined background knowledge, defining this knowledge by expert, constrains the system with a static incomplete reasoning skill, that does not evolve with new experiences, even though this skill can be acquired through the new cases. Furthermore, adaptation is a compensatory part of CBR [1]. Adaptation process can compensate the shortcomings in other steps of reasoning process, by appropriately modifying a weak match to obtain a reasonable solution. Accordingly automating the process of acquiring adaptation knowledge, in this work, has been proposed in the means of increasing the reasoning skill.	case-based reasoning	Shadan Saniepour;Behrouz Homayoun Far	1999		10.1007/3-540-46846-3_46	case-based reasoning;similarity;computer science;artificial intelligence;similitude;data mining;expert system	Robotics	-21.077812462640846	-5.708807314297986	182813
6b2f8d84ec199901f9772f2bc4e03e998f6b40b6	multi-strategy selection supported automated negotiation system based on bdi agent	protocols;electronic commerce;human computer interaction;belief desire intention model;computer architecture cognition decision making protocols proposals speech atmospheric modeling;multi agent system;agent architecture automated negotiation negotiation agent multi agent system belief desire intention model;belief desire intention;conference;speech;inference mechanisms;software agents;purchasing;computer architecture;software architecture;bdi agents;decision making process;negotiation protocol;cognition;negotiation agent;atmospheric modeling;agent architecture;systems science;proposals;aircraft purchase negotiation process multistrategy selection automated negotiation system bdi agent negotiation protocol strategy design agent independent decision making process operationalization negotiation decision making model design software architecture goal directed reasoning reactive response automated negotiation agent abstract architecture model belief desire intention agent multistrategy negotiation;software architecture aircraft decision making electronic commerce inference mechanisms purchasing software agents;automated negotiation;aircraft	Research in automated negotiation has traditionally been focusing on the negotiation protocol and strategy design, but little on the implementation related issues such as how to select the best negotiation strategy, especially for problems involving multi-strategy selection. The lack of such study has hampered the development in applying automated negotiation to real world problems. This research focuses on operationalizing agent's independent decision-making process through the design of a negotiation decision-making model and the software architecture, based on an abstract architecture model that can support both goal-directed reasoning and reactive response. We formally define the automated negotiation agent's abstract architecture model and propose an algorithm for the architecture and the decision-making model. Grounded on the theory of Belief-Desire-Intention, the model can support the agent's multi-strategy negotiation. A prototype of the model is built and applied to an aircraft purchase negotiation process to demonstrate the effectiveness of our model.	agent architecture;algorithm;computation;e-commerce;experiment;formal specification;prototype;smoothing;software agent;software architecture;software development kit;testbed	Mukun Cao	2012	2012 45th Hawaii International Conference on System Sciences	10.1109/HICSS.2012.442	systems science;e-commerce;agent architecture;communications protocol;software architecture;atmospheric model;decision-making;simulation;cognition;computer science;knowledge management;speech;artificial intelligence;software agent;multi-agent system	AI	-19.312850883189597	-9.52097159848796	182955
0c43fb6b89f8e35dfe1a837c3f0751addedc85d1	the foundations and architecture of autotutor	representacion conocimientos;architecture systeme;systeme tutoriel intelligent;intelligent tutoring system;systeme apprentissage;learning systems;semantic processing;sistema cognitiva;intelligent tutoring systems;arquitectura sistema;systeme cognitif;cognitive system;analisis semantico;knowledge representation;system architecture;analyse semantique;representation connaissances;semantic analysis	The Tutoring Research Group at the University of Memphis is developing an intelligent tutoring system which takes advantages of recent technological advances in the areas of semantic processing of natural language, world knowledge representation, multimedia interfaces, and fuzzy descriptions. The tutoring interaction is based on in-depth studies of human tutors, both skilled and unskilled. Latent semantic analysis will be used to semantically process and provide a representation for the student's contributions. Fuzzy production rules select appropriate topics and tutor dialogue moves from a rich curriculum script. The production rules will implement a variety of di erent tutoring styles, from a basic untrained tutor to one which uses sophisticated pedagogical strategies. The tutor will be evaluated on the naturalness of its interaction, with Turing-style tests, by comparing di erent tutoring styles, and by judging	commonsense knowledge (artificial intelligence);interaction;knowledge representation and reasoning;latent semantic analysis;natural language processing;pedagogical agent;prototype;selection rule;turing	Peter M. Wiemer-Hastings;Arthur C. Graesser;Derek Harter	1998		10.1007/3-540-68716-5_39	natural language processing;knowledge representation and reasoning;semantic memory;computer science;artificial intelligence;systems architecture	AI	-25.659610958132003	-8.223752600149338	183249
401f3d5103b41de0e119ffdb9cb804730e827e17	efficiency considerations for scalable information retrieval servers	information retrieval;text database;indexation;collection efficiency;parallel processing	We review a variety of techniques to improve efficiency in information retrieval. Given the increasing volumes of data that are available electronically, understanding and using such techniques is critical. We address several efficiency concerns, but our primary focus is on index processing since it dominates the computational demands of information retrieval. Given the importance of index processing, in addition to a general overview, we include some recent index maintenance results. These results demonstrate that by delaying the updating of the index when additional documents are introduced to the collection, efficiency is improved without noticeably degrading the effectiveness of information retrieval. We conclude with an overview of parallel processing in information retrieval. Since users cannot tolerate lengthy response times, searching large text databases requires vast computational resources. Parallel processing is currently the only means to support these demands. We focus on only those approaches that are currently commercially viable.	algorithm;analysis of algorithms;central processing unit;computation;computational resource;data compression;database;distributed computing;information retrieval;inverted index;parallel computing;parallel processing (dsp implementation);relational database management system;run time (program lifecycle phase);tf–idf;time complexity;web search engine	Ophir Frieder;David A. Grossman;Abdur Chowdhury;Gideon Frieder	2000	J. Digit. Inf.		parallel processing;computer science;data mining;database;world wide web;information retrieval;human–computer information retrieval	DB	-29.49995574849548	2.3983519382955807	183535
1f2107a38ee296112ec50fcf65f95b4e959b8ba4	probabilistic complex actions in golog	action language;cognitive robotics;uncertainty;robot control;sensors and actuators;knowledge representation	Uncertainty seems to be inherent in most robotic applications. This is because a robot's sensors and actuators are in general imprecise and prone to error. The logic-based action language GOLOG was introduced for the purpose of high-level robot control, but its usefulness was limited because it did not address uncertainty. bIn this paper we show how this deficiency can be overcome.	situation calculus	Henrik Grosskreutz;Gerhard Lakemeyer	2003	Fundam. Inform.		knowledge representation and reasoning;simulation;uncertainty;action language;computer science;artificial intelligence;robot control;cognitive robotics	ML	-20.074205396858286	-2.8481166753158256	183626
2c569536d2ed42db746acddefb106cab102bcb18	consultant-2: pre- and post-processing of machine learning applications	langage description donnee;mlt tools;sistema experto;systeme intelligent;architecture systeme;learning;gollete estrangulamiento;data description language;sistema inteligente;base connaissance;lenguaje descripcion dato;aprendizaje;resolucion problema;goulot etranglement;machine learning tools;apprentissage;machine learning;knowledge acquisition;intelligent system;acquisition;base conocimiento;arquitectura sistema;systeme expert;system architecture;bottleneck;adquisicion;problem solving;resolution probleme;knowledge base;expert system;knowledge engineering	The knowledge acquisition bottleneck in the development of large knowledge based applications has not yet been resolved. One approach which has been advocated is the systematic use of Machine Learning (ML) techniques. However, ML technology poses difficulties to domain experts and knowledge engineers who are not familiar with it. This paper discusses Consultant-2, a system which makes a first step towards providing system support for a “Preand PostProcessing” methodology where a cyclic process of experiments with an ML tool, its data, data description language and parameters attempts to optimise learning performance. Consultant-2 has been developed to support the use of the Machine Learning Toolbox (MLT), an integrated architecture of ten ML tools, and has evolved from a series of earlier systems. Consultant-O and Consultant-1 had knowledge only about how to choose an ML algorithm based on the nature of the domain data. Consultant-2 is the most sophisticated. It, additionally, has knowledge about how ML experts and domain experts pre-process domain data before a run with the ML algorithm, and how they further manipulate the data and reset parameters after a run of the selected ML algorithm, to achieve a more acceptable result. How these several KBs were acquired and encoded is described. In fact, this knowledge has been acquired by interacting both with the ML algorithm developers and with domain experts who had been using the MLT toolbox on real-world tasks. A major aim of the MLT project was to enable a domain expert to use the toolbox directly; i.e. without necessarily having to involve either a ML specialist or a knowledge engineer. Consultant’s principal goal was to provide specific advice to ease this process. ∗Seconded to the MLT project at the University of Aberdeen, 1990/91; permanent address, The Robert Gordon University, Aberdeen	algorithm;data definition language;experiment;interaction;knowledge acquisition;knowledge engineer;machine learning;preprocessor;subject-matter expert;video post-processing	Derek H. Sleeman;Michalis Rissakis;Susan Craw;Nicolas Graner;Sunil Sharma	1995	Int. J. Hum.-Comput. Stud.	10.1006/ijhc.1995.1035	knowledge base;simulation;computer science;artificial intelligence;knowledge engineering;military acquisition;expert system;domain knowledge;algorithm	AI	-26.0470038097129	-5.357109397756544	183863
d9ffd111eb92e80e5f55591ee87ec1627e62cba9	methods of data processing and communication for a web-based wind flow visualization		This paper presents methods for the reduction and compression of meteorological data for web-based wind flow visualizations, which are tailored to the flow visualization technique. Flow data sets represent a large amount of data and are therefore not well suited for mobile networks with low data throughput rates and high latency. Using the mechanisms introduced in this paper, an efficient transfer of thinned out and compressed data can be achieved, while keeping the accuracy of the visualized information almost at the same quality level as for the original data.	data compression;desktop computer;http/2;hypertext transfer protocol;lossless compression;lossy compression;thinning;throughput;web application	Marc Skutnik;Luigi Lo Iacono;Christian Neuhaus	2016		10.5220/0005846800320041	computer science;database;world wide web;information retrieval	Visualization	-31.75242297323405	-3.047318257215551	183929
fb398222610c5ef9a2f69524a3829afbe2bd4d35	dynamo-mas: a multi-agent system for building and evolving ontologies from texts		Building and evolving an ontology are a complex problems: they involve numerous entities (terms, concepts, relations), the environment of the ontology is dynamic (addition of new documents, ontologist’s actions) and we cannot predict all ontology evolution possibilities. That is why a unique entity or system to solve these problems cannot list all the possible situations to which it can be confronted as well as the actions it has to take in such situations. This compels to distribute the problem on several autonomous entities that have a local perception of each situation that can arise during the system functioning and that have simple, generic and local behaviors in order to self-adapt to these situations. We propose in that sense DYNAMO-MAS, a tool based on a Multi-Agent System (MAS) enabling the co-construction and the evolving of an ontology. It takes as input a corpus of texts and provides as output an ontology.		Zied Sellami;Valérie Camps	2012		10.1007/978-3-642-28786-2_38	theoretical computer science	AI	-21.955220835913583	-9.524108066145697	184104
543fbef117f6606225c9407823f925480380856f	difference computation of large models	high dimensionality;search trees;version management;differences;versioning;models;software engineering practices	Modern software engineering practices lead to large models which exist in many versions. Version management systems should offer a service to compare, and possibly merge, these models. The computation of a difference between large models is a big challenge; current algorithms are too inefficient here. We present a new technique for computing differences between models. In practical tests, this technique has been an order of magnitude faster than currently known algorithms. The main idea is to use a high-dimensional search tree for efficiently finding similar model elements. Individual elements are mapped onto a vector of numerical values using a collection of metrics for models and a numerical representation of the names which occur in a model.	algorithm;array data structure;class diagram;computation;ibm pc compatible;in-memory database;lexicon;numerical analysis;range query (data structures);search tree;software engineering;software metric;substring	Christoph Treude;Stefan Berlik;Sven Wenzel;Udo Kelter	2007		10.1145/1287624.1287665	computer science;software versioning;theoretical computer science;data mining;database	SE	-30.647108830733718	2.4864414885726993	184107
47146d7b57368a21167b418f9be69cc663bea9e7	collection, storage and application of human knowledge in expert system development	petroleum exploration;fuzzy logic;knowledge representation;expert system design;expert system	Expert systems are programs that analyze data by mimicking the thought processes of an expert. Two expert systems were developed by the Reservoir Evaluation and Advanced Computational Techniques group to aid in oil prospecting for two New Mexico formations, leading to the development of a third customizable fuzzy expert system. Knowledge engineering is a major part of the development of these expert systems, in which expert knowledge is solicited, analyzed, converted to rules stored in a system’s knowledge base, and used by the computer to produce expert judgment. Numerical versions of the rules are used to analyze data and produce an evaluation of the user’s prospect. In addition, the knowledge base preserves expert knowledge for future workers. This is especially important in the petroleum industry, as there is a cyclical trend in employment relating to the price of oil, retirements and people leaving and entering the industry.	computation;expert system;knowledge base;knowledge engineering	Robert Balch;Susan Schrader;Tongjun Ruan	2007	Expert Systems	10.1111/j.1468-0394.2007.00439.x	fuzzy logic;legal expert system;knowledge base;expert elicitation;computer science;knowledge management;artificial intelligence;knowledge-based systems;data mining;subject-matter expert;expert system	AI	-32.054070187822276	-7.884941730532135	184252
eff8e75809fa67cb6ae1cbda1aff77d06ebf5893	transportation security decision support system for emergency response: a training prototype	emergency response;organizational learning;unfolding;sistema de transporte;modele entreprise;sistema experto;recuperacion;securite;deploiement;systeme aide decision;accesibilidad;informacion incompleta;computer aided instruction;mock drills;despliegue;prise de decision;sistema ayuda decision;recovery;modelo empresa;emergency;business model;incomplete information;support system;systeme incertain;decision support system;accessibility;safety;information incomplete;urgencia;computer based training;urgence;systeme transport;recuperation;systeme expert;toma decision;sistema incierto;department of transportation;seguridad;instruction assistee;uncertain system;transportation system;accessibilite;effective action;expert system	During emergencies, decision making is a challenging task requiring immediate and effective action from responders under the pressures of incomplete and erroneous information. Identification of appropriate resources and personnel, proper lines of communication, and timely accessibility to relevant procedures can minimize after effects. To achieve emergency response and recovery effectiveness, responders need to be prepared and trained for various emergency situations and decision support systems. To address some of the decision making needs experienced by responders, a low-cost computer computer-based training prototype with a decision support system tool was developed. The emergency training prototype was designed for the Indiana Department of Transportation. Emergency responders' capabilities, collaboration with other agencies, deployment of resources and personnel, implementation of response plans, and use of the chain of command were evaluated. The usefulness of prototype and potential decision making systems for the transportation agency are validated based on several mock drills.	decision support system;prototype	Sang Won Yoon;Juan D. Velásquez;B. K. Partridge;Shimon Y. Nof	2008	Decision Support Systems	10.1016/j.dss.2008.06.002	business model;effective action;organizational learning;recovery;simulation;decision support system;emergency;computer science;artificial intelligence;operations management;accessibility;operations research;expert system;complete information	Mobile	-24.330422085167147	-5.486285206284658	184383
a214584d8066efbf285582f113be1818ceb39e2f	possibilistic networks for information retrieval	busqueda informacion;modelizacion;bayesian network;computacion informatica;information retrieval;interrogation base donnee;pertinencia;interrogacion base datos;intelligence artificielle;probabilistic approach;modelisation;reseau bayes;red bayes;recherche information;ciencias basicas y experimentales;enfoque probabilista;approche probabiliste;pertinence;term weighting;bayes network;artificial intelligence;possibility theory;possibilistic networks;entropy;inteligencia artificial;relevance;grupo a;modeling;database query;teoria posibilidad;bayesian networks;theorie possibilite	This paper proposes an information retrieval (IR) model based on possibilistic directed networks. The relevance of a document w.r.t a query is interpreted by two degrees: the necessity and the possibility. The necessity degree evaluates the extent to which a given document is relevant to a query, whereas the possibility degree evaluates the reasons of eliminating irrelevant documents. This new interpretation of relevance led us to revisit the term weighting scheme by explicitly distinguishing between informative and non-informative terms in a document. Experiments carried out on three standard TREC collections show the effectiveness of the model.	information retrieval	Mohand Boughanem;Asma Brini;Didier Dubois	2009	Int. J. Approx. Reasoning	10.1016/j.ijar.2008.10.005	computer science;artificial intelligence;machine learning;bayesian network;data mining	AI	-20.491911949489413	-0.7885746471689207	184494
0078c0d8aa10d5f2e5f7e492d8b0064123e2c452	computer aided evaluation of trainee skills on a simulator network	developpement logiciel;concepcion sistema;cognitive agents;sistema n niveles;simulator;software architecture;simulador;systeme n niveaux;system design;desarrollo logicial;software development;multilevel system;computer aid;simulateur;asistencia ordenador;pedagogical platoon training system;assistance ordinateur;conception systeme;architecture logiciel	The aim of the PPTS project (Pedagogical Platoon Training System) is to design and implement an evaluation environment for strategic and tactical skills, coupled to a network of full-scale simulators of the LECLERC tank. The three-level architecture adopted here is modelled on the levels of expertise, technical, tactic and strategic. The technical level is carried out by reactive agents, the monitors. They point out the important events of the exercise. The evaluation of tactical skills is carried out by cognitive agents, the analysts, which use the facts generated by the monitors to evaluate the tactical skills. The software architecture at analyst level provides a flexible solution to the problem of time in the evaluation of the exercise, whether the transitions between phases are sudden or gradual, whether the exercise is slow or rapid. We will complete this study with the strategic level.	expression (computer science);full scale;pedagogical agent;simulation;software architecture	Michelle Joab;Odette Auzende;Michel Futtersack;Brigitte Bonnet;Patrice Le Leydour	2002		10.1007/3-540-47987-2_54	software architecture;simulation;artificial intelligence;software development;operating system;systems design	AI	-24.340006989698676	-5.830480992683114	184540
8d2ef540d1b26bea0798d13132e898f3cec87ccd	scalable similarity search in metric spaces	similarity search;indexation;range query;metric space	Similarity search in metric spaces represents an important paradigm for content-based retrieval of many applications. Existing centralized search structures can speed-up retrieval, but they do not scale up to large volume of data because the response time is linearly increasing with the size of the searched file. The proposed GHT* index is a scalable and distributed structure. By exploiting parallelism in a dynamic network of computers, the GHT* achieves practically constant search time for similarity range queries in data-sets of arbitrary size. The amount of replicated routing information on each server increases logarithmically. At the same time, the potential for interquery parallelism is increasing with the growing data-sets because the relative number of servers utilized by individual queries is decreasing. All these properties are verified by experiments on a prototype system using real-life data-sets.	addressing scheme;centralized computing;computer;data structure;experiment;information retrieval;inverted index;organizing (structure);parallel computing;programming paradigm;prototype;range query (data structures);real life;response time (technology);routing;scalability;search algorithm;server (computing);similarity search;space partitioning;spaces	Michal Batko;Claudio Gennaro;Pasquale Savino;Pavel Zezula	2004			scalability;dynamic network analysis;range query (data structures);theoretical computer science;metric space;indexation;response time;nearest neighbor search;server;mathematics	DB	-29.6570763681443	0.7668804384178555	184928
d4120a07a73c17d04ccdf1ef8b57ddf85e6de90e	parallel interval-based reasoning in medical knowledge-based system clinaid	knowledge based system;knowledge base	In a series of papers and a monograph [33], we have described the conceptual structures as well as the basic architecture of a knowledge-based system CUN^tD. Its architecture is a imed at support ing not only diagnosis but also other types of clinical activity and decision making in diverse clinical and/or hospital environments. T h e aim of the generic architecture of CLINArD is to support knowledge-based decision making under conditions of risk and uncertainty, both of which are present in clinical medicine. Such a system has to operate in a multi-environmental situation and make decisions within a multiplicity of contexts. The basic architecture consists of the following co-operating units (basic shell substratum): 1. Diagnostic Unit (comprized of several parallel co-operating centres). 2. Trea tment Recommendat ion Unit. 3. Patient Clinical Record Unit. 4. Co-ordination and Planning Unit.	knowledge-based systems	Ladislav J. Kohout;Isabel Stabile;Hasan Kalantar;F. San-Andres;John Anderson	1995	Reliable Computing	10.1007/BF02384051	legal expert system;knowledge base;knowledge integration;computer science;knowledge management;artificial intelligence;body of knowledge;model-based reasoning;knowledge-based systems;open knowledge base connectivity;data mining;mathematics;procedural knowledge;knowledge extraction;domain knowledge	AI	-30.21682447771845	-6.99543364115571	184929
2ad654707ad35ef6d75fd9a1d51945bbd4deac70	improving security architecture development based on multiple criteria decision making	content management;multicriteria analysis;sistema multiple;architecture systeme;red www;systeme aide decision;multiple criteria decision making;reseau web;multiple system;gestion contenido;sistema ayuda decision;prise decision;securite donnee;resolucion problema;decision support system;multiple decision;gestion contenu;security architecture;world wide web;decision multiple;arquitectura sistema;analisis multicriterio;information system;analyse multicritere;system architecture;toma decision;security of data;systeme information;problem solving;resolution probleme;sistema informacion;systeme multiple	This paper describes an effort to improve security architecture development of information systems based on the multiple criteria decision making (MCDM) techniques. First, we introduce the fundamental of MCDM, describe how the security architecture is developed and analyze the main problems in the development. Finally, this paper shows how the MCDM techniques were applied to solve two problems in security architecture development. And an approach which could assist in prioritizing threats and selecting security technologies is illustrated. The practices indicate that MCDM techniques are valuable in formulating and solving problems in security architecture development.		Fang Liu;Kui Dai;Zhiying Wang	2004		10.1007/978-3-540-30483-8_26	computer security model;decision support system;content management;computer science;artificial intelligence;operations research;information system;enterprise information security architecture	SE	-26.768558920177643	-4.466318191028274	184947
a603c906488330e65854edb043746a27ca6efe86	visreduce: fast and responsive incremental information visualization of large datasets	online aggregation;end to end approach visreduce responsive incremental information visualization large datasets visual analytics system exploratory data analysis mapreduce style algorithm compressed columnar data store database querying data warehouse systems postgresql cloudera impala mapreduce based apache hive;query processing;sql;incremental visualization;information visualization;data visualisation;data analysis;columnar storage;columnar storage incremental visualization online aggregation information visualization mapreduce;mapreduce;data warehouses;data visualization databases arrays aggregates visualization acceleration java;sql data analysis data visualisation data warehouses query processing	Performance and responsiveness of visual analytics sytems for exploratory data analysis of large datasets has been a long standing problem. We propose a method for incrementally computing visualizations in a distributed fashion by combining a modified MapReduce-style algorithm with a compressed columnar data store, resulting in significant improvements in performance and responsiveness for constructing commonly encountered information visualizations, e.g. bar charts, scatterplots, heat maps, cartograms and parallel coordinate plots. We compare our method with one that queries three other readily available database and data warehouse systems - PostgreSQL, Cloudera Impala and the MapReduce-based Apache Hive - in order to build visualizations. We show that our end-to-end approach allows for greater speed and guaranteed end-user responsiveness, even in the face of large, long-running queries.	algorithm;apache hive;chart;column-oriented dbms;data store;end-to-end principle;feedback;heat map;information visualization;interactive visualization;mapreduce;parallel coordinates;postgresql;relational database;responsiveness;scalability;visual analytics	Jean-Francois Im;Felix Giguere Villegas;Michael J. McGuffin	2013	2013 IEEE International Conference on Big Data	10.1109/BigData.2013.6691710	computer science;data science;data mining;database	Visualization	-32.57140354804679	-0.05728948381215777	184953
fa288a96af2b8a08b84947458f22c6e02ec2c8c0	some practical aspects of fuzzy database techniques: an example	congres;outil logiciel;base relacional dato;software tool;imprecis;fuzzy set;criminologie;implementation;informacion incompleta;criminologia;logique floue;logica floja;ensemble flou;relational database;congreso;fuzzy logic;ejecucion;incomplete information;estudio caso;herramienta controlada por logicial;information incomplete;etude cas;base donnee relationnelle;criminology;conjunto flojo;information system;fuzzy database;systeme information;sistema informacion;congress	Basically there are two approaches for dealing with incomplete or imprecise information in the framework of (relational) databases. Buckles and Petry introduced fuzzy similarity relations in order to estimate to what extent possible values of an attribute can be regarded as interchangeable. In the approach of Dubois, Prade and Testemale possibility distributions were used to represent all possible kinds of incompletely or fuzzily known values. This paper describes the application of a combination of both techniques. We focus on the very specific domain of criminal investigation, especially on criminal identification by means of a personal description. However, the developed method can be applied to a lot of other domains, where a similar sort of fuzziness and uncertainty shows up. For most attributes the fuzzy values are considered as primitive notions. Each domain is provided with a supB-transitive likeness relation in order to represent an existing kind of overlapping between the possible fuzzy values. Moreover, in order to restrict the number of attributes we allow multivalued ones: their values are represented as possibility distributions on the power class of the domain. The pattern-matching process uses a Prade-Testemale-like technique based on possibility distributions. Key nor& Fuzzy databases, fuzzy pattern matching, criminal identification.	database;fuzzy set;pattern matching;relational model	R. Vandenberghe;Antoine A. van Schooten;Rita M. M. De Caluwe;Etienne E. Kerre	1989	Inf. Syst.	10.1016/0306-4379(89)90014-8	fuzzy logic;relational database;computer science;artificial intelligence;data mining;fuzzy set;implementation;complete information;information system;algorithm	DB	-22.34769490582097	2.6542076806688404	184979
f97fd55faf7e57a16f86c780f25202d353401d24	describing morphological phenomena of modern greek using a unification grammar formalism	grammar;tratamiento lenguaje;greek;unification;griego;analisis morfologico;language processing;grammaire;directed graph;graphe oriente;traitement langage;morphological analysis;analyse morphologique;grafo orientado;procesador;grec;processeur;gramatica;processor;unificacion	Abstract   The  unification-based formalism , suitable for encoding a wide variety of grammars for computational applications and the linguistic research, can also be applied to the area of computational morphology. The  morphological processor , presented in this paper, is based on the PATR II formalism and provides the system with a context-sensitive approach as opposed to the more classical finite state automaton approach. It has been designed as a tool to describe a range of linguistic models, in which unification plays the centrol role. Our software system can be used to describe various morphological phenomena of  Modern Greek  (and any other highly inflected language) such as inflection, derivation, accentuation, composition, etc. Stored morphological information representing a word, is operationally divided into syntactic and semantic parts and is described in terms of  attribute-value pairs . The unification-based formalism provides a  unified  approach to deal with the stratified levels of linguistic information, namely morphology, syntax and semantics.	formal grammar;semantics (computer science);unification (computer science)	Yannis Kotsanis;Yannis Maistros	1991	Inf. Syst.	10.1016/0306-4379(91)90024-4	natural language processing;computer science;artificial intelligence;greek;programming language;algorithm	NLP	-22.255651165807006	0.24108480298895538	185091
3216e44ce9d7299dee87189e94496b4ce1cb4830	an initial response to the oas'03 challenge problem	bdi agents;ontology;agent applications;semantic web	We present our initial response to the OAS '03 Challenge Problem. The Challenge Problem proposes an agent-assisted travel scenario, and asks what the role of ontologies would be to support the agent's activity. We discuss a belief-desire-intention (BDI) approach to the problem using our Nuin agent platform, and illustrate various ways in which ontology reasoning supports BDI-oriented problem solving and communications by the agents in the system.	belief–desire–intention software model;ontology (information science);oracle application server;problem solving	Ian Dickinson;Michael Wooldridge	2003			semantic web;ontology;ontology (information science);psychology;knowledge management	AI	-26.33603644323312	-9.420535616896052	185405
3382c401bb04d68f478691a17e5f24ac67b2d88c	open information systems semantics for distributed artificial intelligence	distributed system;systeme reparti;etude theorique;semantics;intelligence artificielle;systeme ouvert;semantica;semantique;sistema repartido;distributed artificial intelligence;estudio teorico;artificial intelligence;inteligencia artificial;information system;theoretical study;open systems;sistema abierto;systeme information;sistema informacion	Abstract    Distributed Artificial Intelligence  (henceforth called DAI) deals with issues of large-scale  Open Systems  (i.e. systems which are always subject to unanticipated outcomes in their operation and which can receive new information from outside themselves at any time).  Open Information Systems  (henceforth called OIS) are Open Systems that are implemented using digital storage, operations, and communications technology. OIS Semantics aims to provide a scientific foundation for understanding such large-scale OIS projects and for developing new technology.  The literature of DAI speaks of many important concepts such as  commitment, conflict, negotiation, cooperation, distributed problem solving, representation , etc. However there is currently no framework for comparing the usage of such concepts in one publication with usage in other publications.  Open Information System Semantics  (henceforth called OIS Semantics) is a step toward remedying this problem by providing a framework which integrates methods from Sociology with methods from Concurrent Systems Science into a foundation that provides a framework for analyzing previous work in DAI and a powerful foundation for its further development.  Deduction, one of the most powerful and well-understood methods for information systems, has recently been applied to foundational issues in DAI thereby raising important new issues and problems above and beyond those of applying deduction to the problems of classical AI. OIS Semantics provides answers to many important questions about the uses of deduction in OIS. It provides a characterization of Deduction that encompasses  N th-Order Logics, Meta-theories, Modal Logics, Circumscription, Default Logic, Autoepistemic Logic, Restricted Scope Nonmonotonic Logics, etc. OIS Semantics develops the concept of  Deductive Indecision  as a fundamental aspect of Deductive systems, thereby characterizing the scope and limits of Deduction for operational and representational activities in OIS.  Negotiations play a fundamental role in OIS Semantics. They are creative processes that go beyond the capabilities of Deduction. OIS Semantics characterizes the role of Negotiation as a powerful method for increasing understanding of large-scale OIS projects.  The ambitious goal of OIS Semantics for DAI is to provide an integrated foundation for Sociology and Concurrent Systems Science. This paper provides a snapshot of where we currently stand in the process of developing these foundations	distributed artificial intelligence;open system (computing)	Carl Hewitt	1991	Artif. Intell.	10.1016/0004-3702(91)90051-K	computer science;artificial intelligence;mathematics;semantics;open system;information system;algorithm	AI	-24.329126733248707	-2.932034223598104	185516
0ee74992dcb4cffcf0d6dd833604b684d136a6a6	a revised r*-tree in comparison with related index structures	split;hilbert r tree;index structure;performance comparison;multidimensional data;revised r tree;r tree;three dimensional;data distribution;multi dimensional;index structures;rr tree;multi dimensional data;choosesubtree	In this paper we present an improved redesign of the R*-tree that is entirely suitable for running within a DBMS. Most importantly, an insertion is guaranteed to be restricted to a single path because re-insertion could be abandoned. We re-engineered both, subtree choice and split algorithm, to be more robust against specific data distributions and insertion orders, as well as peculiarities often found in real multidimensional data sets. This comes along with a substantial reduction in CPU-time.  Our experimental setup covers a wide range of different artificial and real data sets. The experimental comparison shows that the search performance of our revised R*-tree is superior to that of its three most important competitors. In comparison to its predecessor, the original R*-tree, the creation of a tree is substantially faster, while the I/O cost required for processing queries is improved by more than 30% on average for two- and three-dimensional data. For higher dimensional data, particularly for real data sets, much larger improvements are achieved.	algorithm;central processing unit;concurrency (computer science);input/output;insertion sort;line level;mathematical optimization;multiversion concurrency control;online and offline;perimeter;r* tree;r+ tree;r-tree;requirement;synthetic intelligence;tree (data structure)	Norbert Beckmann;Bernhard Seeger	2009		10.1145/1559845.1559929	r-tree;segment tree;three-dimensional space;hilbert r-tree;computer science;data mining;interval tree;database;fractal tree index;split;algorithm	DB	-24.91812954035566	1.2225944557452657	185523
34f1dff7f0d79bc173395fab311fdddf5003ab12	attribution of knowledge to artificial agents and their principals	artificial agent x;legal theory;pragmatic analysis;artificial agent;legal principal;knowledge attribution	We consider the problem of attribution of knowledge to artificial agents and their legal principals. When can we say that an artificial agent X knows p and that its principal can be attributed the knowledge of p? We offer a pragmatic analysis of knowledge attribution and apply it to the legal theory of artificial agents and their principals.	intelligent agent;software agent;source-to-source compiler;text corpus;trine	Samir Chopra;Laurence White	2005			knowledge management;artificial intelligence	AI	-26.13189968472683	-9.532713146981157	185620
b089c04fd84f28134505fdd28f6c747c8116fc13	rox: relational over xml	performance perspective;rox scenario;native format;xml format;good performance;natively-stored xml data;sql engine;utilizing xml;xml document;initial performance result;query optimization	"""An increasing percentage of the data needed by business applications is being generated in XML format. Storing the XML in its native format will facilitate new applications that exchange business objects in XML format and query portions of XML documents using XQuery. This paper explores the feasibility of accessing natively-stored XML data through traditional SQL interfaces, called Relational Over XML (ROX), in order to avoid the costly conversion of legacy applications to XQuery. It describes the forces that are driving the industry to evolve toward the ROX scenario as well as some of the issues raised by ROX. The impact of denormalization of data in XML documents is discussed both from a semantic and performance perspective. We also weigh the implications of ROX for manageability and query optimization. We experimentally compared the performance of a prototype of the ROX scenario to today’s SQL engines, and found that good performance can be achieved through a combination of utilizing XML's hierarchical storage to store relations """"pre-joined"""" as well as creating indices over the remaining join columns. We have developed an experimental framework using DB2 8.1 for Linux, Unix and Windows, and have gathered initial performance results that validate this approach."""	business object;column (database);denormalization;experiment;hierarchical storage management;linux;mathematical optimization;microsoft windows;prototype;query optimization;sql;unix;xml;xquery	Alan Halverson;Vanja Josifovski;Guy M. Lohman;Hamid Pirahesh;Mathias Mörschel	2004			xml catalog;xml validation;binary xml;xml encryption;query optimization;simple api for xml;xml;relax ng;xml schema;streaming xml;computer science;document structure description;xml framework;data mining;xml database;xml schema;database;xml signature;world wide web;xml schema editor;cxml;efficient xml interchange;sgml	DB	-32.458155870904775	3.035900459866763	185723
dfb10200270462d306599ed97c0e2df4a6978be1	multi-layer logic — a predicate logic including data structure as knowledge representation language	conceptual model;satisfiability;system performance;human interface;design knowledge;system design;information management;information processing;knowledge representation;data structure;problem solving	A new generation computer is expected to be the knowledge processing system of the future. However, many aspects are yet unknown regarding this technology, and a number of fundamental concepts, directly concerning knowledge processing system design need investigation, such as knowledge, data, inference, communication, information management, learning, and human interface. These concepts are closely related to knowledge representation. In particular, methodology to materialize such concepts as above in computers are completely dependent upon them. Thus, knowledge representation is a key concept in the design of knowledge processing systems and, consequently, of new generation computer systems. Knowledge representation design is a very important task affecting the performance of new generation computer systems to be developed. We should first investigate the requirements for precise knowledge representation, considering its effects on system performance, then design knowledge representations to satisfy these requirements. This paper discusses (1) a new style of information processing, (2) requirements for knowledge representation and (3) a knowledge representation satisfying these requirements, a knowledge processing system designed on this basis and a new style of problem solving using this system.	computer;data structure;information management;information processing;knowledge representation and reasoning;problem solving;requirement;systems design;user interface	Setsuo Ohsuga;Hiroyuki Yamauchi	1985	New Generation Computing	10.1007/BF03037079	natural language processing;knowledge representation and reasoning;knowledge base;information;knowledge integration;data structure;information processing;computer science;knowledge management;conceptual model;theoretical computer science;body of knowledge;mathematical knowledge management;knowledge-based systems;open knowledge base connectivity;procedural knowledge;knowledge extraction;information management;programming language;domain knowledge;human interface device;satisfiability;systems design	AI	-25.988232995286374	-3.799220875310809	185796
284abaf01bfb4e2ebff5e61dabca81a6fb5895e1	o-plan: the open planning architecture	embedding;architecture systeme;nonlinear programming;search space;heuristic method;metodo heuristico;intelligence artificielle;operations research;planificacion;research and development;logistics;design and implementation;control architecture;data structures;scheduling;management planning;artificial intelligence;arquitectura sistema;domains;planning;o plan;methode heuristique;inteligencia artificial;planification;system architecture;architecture computers;constraints	O Plan is an AI planner based on previous experience with the Nonlin planner and its deriva tives Nonlin and other similar planning systems had limited control architectures and were only partially successful at limiting their search spaces O Plan is a design and implementation of a more exible system aimed at supporting planning research and development opening up new planning methods and supporting strong search control heuristics O Plan takes an engineering approach to the construction of an e cient domain independent planning system which includes a mixture of AI and numerical techniques from Operations Research The main contributions of the work are centred around the control of search within the O Plan planning framework and this paper outlines the search control heuristics employed within the planner These involve the use of condition typing time and resource constraints and domain constraints to allow knowledge about an application domain to be used to prune the search for a solution The paper also describes aspects of the O Plan user interface domain description language Task Formalism or tf and the domains to which O Plan has been applied History and Technical In uences O Plan was initially conceived as a project to provide an environment for speci cation generation interaction with and execution of activity plans There are three distinct components see gure the planner s workstation or user interface the plan generator and the execution monitoring sys tem The main e ort has been concentrated in the area of plan generation Plan generation is di cult and it is a classic example of a search problem in AI The main target of this research is therefore search space control The outputs of this study are a better understanding of the re quirements of planning methods improved heuristics and techniques for search space control and a demonstration system embodying these results in an appropriate framework and representational scheme The story of O Plan starts from a software engineering viewpoint namely how to build an open architecture for an AI planning project with the aim of incrementally developing a system resilient to change It was our aim at the start of the project to build a system where it was possible to experiment with and integrate developing ideas Further the nal system was to be tailorable to suit particular applications Section brie y describes the components of the system in which the overall controller and some of the internal knowledge sources can be customised or replaced by the end user O Plan is intended to be a domain independent general planning and control framework with the ability to embed detailed knowledge of the domain Of course exibility can never be achieved The user does have to live with some basic design features and planning philosophies The primary limitation being the least commitment approach taken by the system O Plan grew out of the experiences of other research into AI planning particularly with Nonlin and blackboard systems We have included a taxonomy of earlier planning systems gure which places O Plan in relation to the in uences on its design It is assumed that the reader is familiar with these works as the bibliography does not cover all of them see for an introduction to the literature of AI planning The main AI planning techniques which have been used or extended in O Plan are A hierarchical planning system which can produce plans as partial orders on actions as suggested by Sacerdoti though O Plan is exible concerning the order in which parts of the plan are expanded An agenda based control architecture in which each control cycle can post pending tasks dur ing plan generation These pending tasks are then picked up from the agenda and processed by appropriate handlers hearsay ii uses the term Knowledge Source for these handlers The notion of a plan state which is the data structure containing the emerging plan the aws remaining in it and the information used in building the plan This is similar to the work of McDermott Constraint posting and least commitment on object variables as seen in molgen Temporal and resource constraint handling shown to be valuable in realistic domains by Deviser has been extended to provide a powerful search space pruning method The algorithms for this are incremental versions of Operational Research methods O Plan has integrated ideas from or and AI in a coherent and constructive manner O Plan is derived from the earlier Nonlin planner from which we have taken and extended the ideas of Goal Structure Question Answering qa and typed preconditions We have maintained Nonlin s style of task description language Task Formalism or tf and extended it for O Plan As with most planning systems intended to operate in realistic domains control of the search and the management of con ict between competing actions has been the focus of the work The eventual aim of all such systems is to incorporate techniques which should scale up to tackle the expected complexities	algorithm;application domain;artificial intelligence;automated planning and scheduling;blackboard system;coherence (physics);data structure;drew mcdermott;heuristic (computer science);naruto shippuden: clash of ninja revolution 3;numerical analysis;open architecture;open road tolling;operations research;plan 9 from bell labs;precondition;question answering;search problem;semantics (computer science);software engineering;user interface;workstation	Ken Currie;Austin Tate	1991	Artif. Intell.	10.1016/0004-3702(91)90024-E	planning;logistics;simulation;nonlinear programming;computer science;artificial intelligence;embedding;scheduling	AI	-19.990767744283836	-6.119948413862535	185859
f43dcdceda5d7fc991c82780fcec932415f7d4fa	when optimizer chooses table scans: how to make them more responsive		Recent studies show that table scans are increasingly more common than using secondary indices. Given that the optimizer may choose table scans when the selectivity is as low as 0.5% with large data, it is important to make initial query results faster for interactive data explorations. We formulate it as a query result timeliness problem, and propose two complementary approaches. The first approach builds lightweight statistics and judiciously determines an access order to data blocks for a given query. The second approach performs adaptive microscopic tuple reordering online without relying on pre-built statistics. Our systematic experimental evaluation further verifies the efficiency and efficacy of our approaches.	mathematical optimization;responsiveness;selectivity (electronic)	Lijian Wan;Tingjian Ge	2018		10.1145/3269206.3271773	data mining;information retrieval;tuple;computer science	DB	-24.9643469430809	3.80805906530876	186404
880f9e43040d64f3e5ded2426a20d43401e8182f	towards autonomy, self-organisation and learning in holonic manufacturing	autoapprentissage;multiagent system;holonic manufacturing systems;learning;production system;systeme production;intelligence artificielle;sistema produccion;holon;autodidactismo;emergency;self learning;control proceso;control system;urgencia;process control;autoorganizacion;urgence;artificial intelligence;self organization;inteligencia artificial;sistema multiagente;dynamic adaptation;bookpart;commande processus;autoorganisation;systeme multiagent	This paper intends to discuss self-organisation and learning capabilities in autonomous and cooperative holons that are part of a holonic manufacturing control system. These capabilities will support the dynamic adaptation of the manufacturing control to the manufacturing evolution and emergency, specially the agile reaction to unexpected disturbances.	agile software development;algorithm;autonomous robot;autonomy;control flow;control system;global optimization;holon (philosophy);linear system;machine learning;mathematical optimization;nonlinear system;programming paradigm;self-organization;software propagation	Paulo Leitão;Francisco Restivo	2003		10.1007/3-540-45023-8_52	self-organization;simulation;emergency;computer science;control system;artificial intelligence;process control;computer-integrated manufacturing;production system	Robotics	-22.86686976593921	-6.413728585593999	186589
c9e9e5ac1d94b2094ed6f0f86ce7e073ead76f5b	integrating expert systems and decision support systems		Expert systems are emerging as a powerful tool for decision making. Integrating expert systems with decision support systems may enhance the quality and efficiency of both computerized systems. This article examines possible connections between the two technologies and discusses some issues related to their integration.	decision support system;expert system	Efraim Turban;Paul R. Watkins	1986	MIS Quarterly		legal expert system;clinical decision support system;decision support system;intelligent decision support system;decision engineering;computer science;systems engineering;knowledge management;artificial intelligence;management information systems;data mining;subject-matter expert;interactivity;expert system;information system	ECom	-31.429891324217735	-8.92828201148859	186655
94ae83dc16f1b9bf08c26ac17c24c1dd804f9429	a knowledge-based methodology for developing knowledge acquisition tools	knowledge acquisition;software methodology;knowledge base;knowledge engineering			Chih-Cheng Chien;Cheng-Seen Ho	1995			knowledge base;knowledge integration;software mining;idef3;computer science;knowledge management;artificial intelligence;mathematical knowledge management;knowledge engineering;open knowledge base connectivity;data mining;knowledge extraction;domain knowledge	HCI	-31.775220632907697	-6.307654483811525	186761
6ebbb192c967b43dca22548d0de67805bc0fb4f6	cooperation and group utility	systeme intelligent;systeme cooperatif;systeme apprentissage;systeme aide decision;sistema inteligente;sistema ayuda decision;learning systems;planificacion;decision support system;cooperative systems;intelligent system;planning;planification	In this paper, we propose an definition of cooperation to shared plans that takes into account the benefit of the whole group, where the group’s benefit is computed by considering also the consequences of an agent’s choice in terms of the actions that the other members of the group will do. In addition, the members of a group consider whether to adopt the goals of their partners: an agent should adopt these goals only when the adoption results in an increase of the group’s benefit.	decision theory;expected utility hypothesis;identifier;point of view (computer hardware company);rationality;refinement (computing);simulation	Guido Boella;Rossana Damiano;Leonardo Lesmo	1999		10.1007/10719619_24	planning;decision support system;computer science;artificial intelligence;operations research	AI	-22.77871846137868	-7.34640242882805	187256
c7bc144ba52d87bcf6cd127d601b95f516663b87	strategies for exploring large scale data	database indexing;query processing;search algorithm;time series;data mining;parallel processing large scale multidimensional time series data querying temporal pattern association business data scientific data demographic data simulation data information discovery knowledge discovery indexing techniques search algorithms temporal range value querying linear space data structures temporal window data structure optimal asymptotic bounds multidimensional objects serial processing;large scale;large scale systems multidimensional systems indexing data structures testing demography parallel processing parallel architectures;data structures;indexation;temporal pattern;data mining query processing temporal databases very large databases data structures database indexing time series;time series data;temporal databases;very large databases;linear space;data structure;parallel processing	Summary form only given. We consider the problem of querying large scale multidimensional time series data to discover events of interest, test and validate hypotheses, or to associate temporal patterns with specific events. This type of data currently dominates most other types of available data, and will very likely become even more prevalent in the future given the current trends in collecting time series of business, scientific, demographic, and simulation data. The ability to explore such collections interactively, even at a coarse level, will be critical in discovering the information and knowledge embedded in such collections. We develop indexing techniques and search algorithms to efficiently handle temporal range value querying of multidimensional time series data. Our indexing uses linear space data structures that enable the handling of queries in I/O time that is essentially the same as that of handling a single time slice, assuming the availability of a logarithmic number of processors as a function of the temporal window. A data structure with provably almost optimal asymptotic bounds is also presented for the case when the number of multidimensional objects is relatively small. These techniques improve significantly over standard techniques for either serial or parallel processing, and are evaluated by extensive experimental results that confirm their superior performance.	central processing unit;data structure;embedded system;interactivity;parallel computing;preemption (computing);search algorithm;serial port;simulation;time series	Joseph JáJá	2004	7th International Symposium on Parallel Architectures, Algorithms and Networks, 2004. Proceedings.	10.1109/ISPAN.2004.1300447	computer science;data science;data mining;database	DB	-28.27634710351993	-0.46181510773815454	187629
34465f5c7ac438a9cc6160a7c5c24a08adc4414b	thrombophilia screening - an artificial neural network approach		Thrombotic disorders have severe consequences for the patients and for the society in general, being one of the main causes of death. These facts reveal that it is extremely important to be preventive; being aware of how probable is to have that kind of syndrome. Indeed, this work will focus on the development of a decision support system that will cater for an individual risk evaluation with respect to the surge of thrombotic complaints. The Knowledge Representation and Reasoning procedures used will be based on an extension to the Logic Programming language, allowing the handling of incomplete and/or default data. The computational framework in place will be centered on Artificial Neural Networks.	artificial intelligence;artificial neural network;case-based reasoning;computation;computational complexity theory;decision support system;decoding methods;genetic programming;information;international symposium on fundamentals of computation theory;knowledge representation and reasoning;logic programming;neural networks;oe-cake!;predispositioning theory;problem solving;programming language;swarm	João Vilhena;Maria Rosário Martins;Henrique Vicente;Luís Nelas;José Machado;José Neves	2015			knowledge representation and reasoning;computer science;artificial intelligence;machine learning;data mining;logic programming	AI	-28.07643409128726	-9.170640458148817	187915
373c3631dc4ecef3b45188b721fb923912fb7ef0	rough sets in approximate spatial reasoning	spatial reasoning;rough set theory;teoria conjunto;qualitative spatial reasoning;theorie ensemble;intelligence artificielle;raisonnement qualitatif;set theory;spatial relation;raisonnement spatial;artificial intelligence;razonamiento calitativo;inteligencia artificial;qualitative reasoning;rough set;ensemble approximatif	In spatial reasoning the qualitative description of relati ons between spatial regions is of practical importance and has been wide ly studied. Examples of such relations are that two regions may meet only at their b oundaries or that one region is a proper part of another. This paper shows how sy stems of relations between regions can be extended from precisely known region s to approximate ones. One way of approximating regions with respect to a part ition of the plane is that provided by rough set theory for approximating subse ts of a set. Relations between regions approximated in this way can be described by an extension of the RCC5 system of relations for precise regions. Two techni ques for extending RCC5 are presented, and the equivalence between them is p roved. A more elaborate approximation technique for regions (boundary s ensitive approximation) takes account of some of the topological structure of r egions. Using this technique, an extension to the RCC8 system of spatial relati ons is presented.	approximation algorithm;mereology;region connection calculus;rough set;set theory;spatial–temporal reasoning;turing completeness	Thomas Bittner;John G. Stell	2000		10.1007/3-540-45554-X_55	discrete mathematics;rough set;computer science;artificial intelligence;machine learning;mathematics;algorithm	Theory	-21.232622718461812	-0.05562639947276991	188095
3191806b9c809bfb064ef6d51cc2344e6f6d618a	compressing relations and indexes	compression algorithm;tree data structures relation compression index compression relational database records cardinality fields numeric fields page level compression decision support systems fact tables disk throughput gzip tuple decompression buffer pool memory utilization index structures b trees r trees lossy compression;compression algorithms databases decision support systems shafts application software throughput memory management packaging information retrieval insulation;indexation;compression ratio;relational databases	We propose a new compression algorithm that is tailored to database applications It can be applied to a collection of records and is especially e ective for records with many low to medium cardinality elds and numeric elds In addition this new technique sup ports very fast decompression Promising application domains include decision sup port systems DSS since fact tables which are by far the largest tables in these applications contain many low and medium cardinality elds and typically no text elds Further our decompression rates are faster than typical disk throughputs for sequential scans in con trast gzip is slower This is important in DSS appli cations which often scan large ranges of records An important distinguishing characteristic of our algorithm in contrast to compression algorithms pro posed earlier is that we can decompress individual tu ples even individual elds rather than a full page or an entire relation at a time Also all the infor mation needed for tuple decompression resides on the same page with the tuple This means that a page can be stored in the bu er pool and used in compressed form simplifying the job of the bu er manager and improving memory utilization Our compression algorithm also improves index structures such as B trees and R trees signi cantly by reducing the number of leaf pages and compressing in dex entries which greatly increases the fan out We can also use lossy compression on the internal nodes of an index	algorithm;application domain;data compression;dex;fan-out;lossy compression;naruto shippuden: clash of ninja revolution 3;netbsd gzip / freebsd gzip;r-tree	Jonathan Goldstein;Raghu Ramakrishnan;Uri Shaft	1998		10.1109/ICDE.1998.655800	data compression;data compression ratio;relational database;computer science;theoretical computer science;compression ratio;data mining;database;lossless compression	DB	-29.199092101710296	1.8935537335541466	188416
afdda933c7c8cf1caec4fbc8bc36606807c8c21a	comparing and analyzing the computational complexity of fca algorithms	comparative analysis;implementation;algorithm;efficient implementation;computational complexity;graph model;formal concept analysis	"""We introduce results of efficiency comparative analysis of algorithms for building the sets of formal concepts. The main goals of research are ascertainment current state of algorithmization of basic tasks in Formal Concept Analysis (FCA), discovering most efficient implementations of algorithms, negotiation of common view on the future of computer aided FCA task solving. Using integrated research environment """"Graph Model Workshop"""" we practically estimate the asymptotic computational complexity on different collections of formal contexts with increasing number of objects and attributes. One of the results is construction of database of comparative analysis. We plan to expand database and invite all concerned researchers to participate in this process."""	algorithm;analysis of algorithms;asymptotic computational complexity;computational complexity theory;formal concept analysis;qualitative comparative analysis	Fedor Strok;Alexey Neznanov	2010		10.1145/1899503.1899557	probabilistic analysis of algorithms;computer science;theoretical computer science;data mining;asymptotic computational complexity	DB	-23.284131407228585	1.5914868389950414	188489
94f42f699c9ea896a22d02c067bb868d2b625ea2	similarity searching: towards bulk-loading peer-to-peer networks	carbon;m tree;metric space model;networks;metric space;distributed structure similarity searching bulk loading peer to peer network paradigm similarity metric space model m tree m chord metric based structured p2p network;peer to peer network;query processing;metric data;information retrieval;data locality;bulk loading similarity searching p2p m tree m chord p2p;oxygen;similarity metric space model;bulk loading;index structure;network organization;data processing;p2p;tree data structures;bulk loading peer to peer network;searching;phosphorus;peer to peer computing information retrieval;metric based structured p2p network;indexes;m chord metric based structured p2p network;organizing;p2p network;database systems;distributed structure;multimedia databases;similarity;distributed databases;digital data complexity;informatics;m chord;exponential growth;experimental evaluation;peer split;p2p networks;very large databases;peer to peer computing;collection efficiency;similarity searching;indium;extraterrestrial measurements;peer to peer;algorithm design and analysis;similarity search;buildings;m chord similarity searching bulk loading peer to peer network digital data complexity metric space model distributed structure metric data m tree metric based structured p2p network;bulk loading peer to peer network paradigm;very large databases distributed databases peer to peer computing query processing tree data structures;peer to peer computing buildings database systems costs informatics organizing image retrieval data processing multimedia databases extraterrestrial measurements;data models;partitioning algorithms;image retrieval	Due to the exponential growth of digital data and its complexity, we need a technique which allows us to search such collections efficiently. A suitable solution seems to be based on the peer-to-peer (P2P) network paradigm and the metric-space model of similarity. During the building phase of the distributed structure, the peers often split as new peers join the network. During a peer split, the local data is halved and one half is migrated to the new peer. In this paper, we study the problem of efficient splits of metric data locally organized by an M-tree and we propose a novel algorithm for speeding the splits up. In particular, we focus on the metric-based structured P2P network called the M-Chord. In experimental evaluation, we compare the proposed algorithm with several straightforward solutions on a real network organizing 10 million images. Our algorithm provides a significant performance boost.	algorithm;baseline (configuration management);digital data;istituto di scienza e tecnologie dell'informazione;m-tree;mpeg-7;organizing (structure);peer-to-peer;programming paradigm;real life;time complexity	Vlastislav Dohnal;Jan Sedmidubský;Pavel Zezula;David Novak	2008	2008 IEEE 24th International Conference on Data Engineering Workshop	10.1109/SISAP.2008.9	m-tree;carbon;database index;data modeling;algorithm design;exponential growth;phosphorus;similarity;data processing;image retrieval;metric space;computer science;theoretical computer science;peer-to-peer;data mining;database;oxygen;indium;tree;informatics;distributed database	DB	-29.92385789636054	0.6545162038937271	188793
b6853a2d6086a84e2fbc85a7e4cfbcf11322321a	hug the elephant: migrating a legacy data analytics application to hadoop ecosystem	computer architecture;data analysis;big data;ecosystems;distributed databases;relational databases;data models	"""Big data applications that rely on relational databases gradually expose limitations on scalability and performance. In recent years, Hadoop ecosystem has been widely adopted as an evolving solution. This paper presents the migration of a legacy data analytics application in a provincial data center. The target platform follows """"no one size fits all"""" method. Considering different workloads, data storage is hybrid with distributed file system (HDFS) and distributed NoSQL database. Beyond the architecture re-design, we focus on the problem of data model transformation from relational database to NoSQL database. We propose a query-aware approach to free developers from tedious manual work. The approach generates query-specific views (NoView) for NoSQL and re-structures the views to align with NoSQL's data model. Our results show that the migrated application achieves high scalability and high performance. We believe that our practice provides valuable insights (such as NoSQL data modeling methodology), and the techniques can be easily applied to other similar migrations."""	align (company);analysis of algorithms;apache hadoop;big data;clustered file system;computer data storage;data center;data model;data modeling;digital subscriber line;domain-specific language;ecosystem;fits;heuristic;model transformation;nosql;relational database;sql;scalability;software development;software maintenance	Feng Zhu;Jie Liu;Sa Wang;Jiwei Xu;Lijie Xu;Jixin Ren;Dan Ye;Jun Wei;Tao Huang	2016	2016 IEEE International Conference on Software Maintenance and Evolution (ICSME)	10.1109/ICSME.2016.14	data modeling;ecosystem;big data;relational database;computer science;database model;data mining;xml database;database;data analysis;view;world wide web;database design;elasticity	DB	-33.28420089442783	0.6304144103993876	188812
2914bc2fb9210077dffeed2f06316d047d118c8e	a query processor for prediction-based monitoring of data streams	anonymization bias;performance evaluation;data stream;satisfiability;management strategy;data privacy;industrial application	Networks of sensors are used in many different fields, from industrial applications to surveillance applications. A common feature of these applications is the necessity of a monitoring infrastructure that analyzes a large number of data streams and outputs values that satisfy certain constraints.  In this paper, we present a query processor for monitoring queries in a network of sensors with prediction functions. Sensors communicate their values according to a threshold policy, and the proposed query processor leverages prediction functions to compare tuples efficiently and to generate answers even in the absence of new incoming tuples. Two types of constraints are managed by the query processor: window-join constraints and value constraints. Uncertainty issues are considered to assign probabilistic values to the results returned to the user. Moreover, we have developed an appropriate buffer management strategy, that takes into account the contributions of the prediction functions contained in the tuples. We also present some experimental results that show the benefits of the proposal.	sensor	Sergio Ilarri;Ouri Wolfson;Eduardo Mena;Arantza Illarramendi;A. Prasad Sistla	2009		10.1145/1516360.1516409	information privacy;computer science;theoretical computer science;data mining;database;satisfiability	DB	-23.987102497192566	3.092324122025023	189136
097a77b5dd13143c526e5bdc12f7efdbb1c272d8	interval indexing and querying on key-value cloud stores	query processing;tree data structures;tree data structures cloud computing indexing query processing;vegetation buildings query processing web pages indexing crawlers;indexing;hive query execution tool interval querying cloud key value store data access data intensive application temporal querying temporal analytics interval index structure mrsegmenttree mrst key value representation endpoints index epi column family index mapreduce job scalable index maintenance hbase hadoop query processing algorithm real data set;cloud computing	Cloud key-value stores are becoming increasingly more important. Challenging applications, requiring efficient and scalable access to massive data, arise every day. We focus on supporting interval queries (which are prevalent in several data intensive applications, such as temporal querying for temporal analytics), an efficient solution for which is lacking. We contribute a compound interval index structure, comprised of two tiers: (i) the MRSegmentTree (MRST), a key-value representation of the Segment Tree, and (ii) the Endpoints Index (EPI), a column family index that stores information for interval endpoints. In addition to the above, our contributions include: (i) algorithms for efficiently constructing and populating our indices using MapReduce jobs, (ii) techniques for efficient and scalable index maintenance, and (iii) algorithms for processing interval queries. We have implemented all algorithms using HBase and Hadoop, and conducted a detailed performance evaluation. We quantify the costs associated with the construction of the indices, and evaluate our query processing algorithms using queries on real data sets. We compare the performance of our approach to two alternatives: the native support for interval queries provided in HBase, and the execution of such queries using the Hive query execution tool. Our results show a significant speedup, far outperforming the state of the art.	algorithm;apache hbase;apache hadoop;apache hive;attribute–value pair;data-intensive computing;full table scan;interval arithmetic;inverted index;key-value database;mapreduce;multitier architecture;performance evaluation;population;routing;sql;scalability;segment tree;speedup;throughput;whole earth 'lectronic link	George Sfakianakis;Ioannis Patlakas;Nikos Ntarmos;Peter Triantafillou	2013	2013 IEEE 29th International Conference on Data Engineering (ICDE)	10.1109/ICDE.2013.6544876	search engine indexing;cloud computing;computer science;operating system;data mining;database;tree;information retrieval	DB	-30.70613481476913	0.4787360665072985	189233
35c627ed177de6ebcbe4d721cc5717e9279d4b45	improving html compression	static dictionary;hypermedia markup languages;document handling;image coding;web pages;transforms data compression document handling hypermedia markup languages;data compression;html document compression;compression algorithms;frequent alphanumerical phrase;compressed file html document compression lossless html transform static dictionary frequent alphanumerical phrase binary encoding semistatic lht fixed english dictionary;fixed english dictionary;html;internet;lossless html transform;compressors;compressed file;dictionaries;transforms;xml;dictionary;lossless;computer science;compression;semistatic lht;html compression algorithms dictionaries internet data compression web pages compressors xml image coding computer science;binary encoding;dictionary lossless compression html	"""The primary objective of our research was to design an efficient way of compressing HTML documents, which will reduce Internet's traffic or will reduce storage requirements of HTML data. In our work we present the lossless HTML transform (LHT) aiming to improve lossless HTML compression in combination with existing general purpose compressors. The main components of our algorithm are: a static dictionary or a semi-static dictionary of frequent alphanumerical phrases, and binary encoding of popular patterns, like numbers, dates or IP addresses. Alphanumerical phrases are not limited to """"words"""" in a conventional sense as they can be XML tags, XML entities, URL addresses, e-mails, and runs of spaces. We have developed two versions of LHT: static and semi-static. Both algorithms have some disadvantages. Static LHT uses a fixed English dictionary required for compression and decompression. Semi-static LHT does not support streams as input (offline compression) as it requires two passes over an input file. Semi-static LHT creates a dictionary in a first pass and stores it within the compressed file."""	algorithm;binary file;data compression;data dictionary;email;entity;html;lossless compression;online and offline;requirement;semiconductor industry;xml	Przemyslaw Skibinski	2008	Data Compression Conference (dcc 2008)	10.1109/DCC.2008.74	data compression;computer science;database;world wide web;information retrieval;statistics	DB	-27.311842521502246	-2.2041141967341815	189604
fd8e164c9022dcd00ab74bab8df077d5cb77ba19	use of decision-tree induction for process optimization and knowledge refinement of an industrial process	modelizacion;optimisation;sistema experto;decision tree;aplicacion;optimizacion;superviseur;learning;base connaissance;intelligence artificielle;arbol decision;aprendizaje;modelisation;apprentissage;induccion;supervisor;induction;process optimization;acquisition;artificial intelligence;base conocimiento;optimization;inteligencia artificial;systeme expert;decision tree induction;application;modeling;arbre decision;adquisicion;knowledge base;expert system		decision tree;mathematical optimization;process optimization;refinement (computing)	A. Famili	1994	AI EDAM	10.1017/S0890060400000469	knowledge base;systems modeling;computer science;engineering;artificial intelligence;machine learning;decision tree;process optimization;operations research;expert system	AI	-24.18291328958331	-4.945704821057378	189679
ee28627cb5e515c3edbf80db3ba125fcb614c637	extending error recovery capability in manufacturing by machine reasoning	declarative programming;fabrication;error recovery;erreur;recuperacion;systeme intelligent;error correction codes;supervisory control;flexible manufacturing systems;fabricacion;system modeling;sistema inteligente;inference mechanisms;recovery;raisonnement;robot industriel;control system;assembly planning;assembling;industrial robots;robot industrial;manufacturing;intelligent system;razonamiento;error correction codes control systems error correction robotic assembly industrial control manufacturing industries supervisory control level control assembly systems gears;industrial application;cost effectiveness;montage;recuperation;error;inference mechanisms assembling error correction codes flexible manufacturing systems industrial robots;reasoning;montaje;gear plate;industrial robot;robotic assembly declarative programming error recovery capability machine reasoning industrial applications error recovery codes flexible control system symbolic level task level control system model assembly plan	In industrial applications, the quantity of error recovery codes far exceeds that required for supervisory control. Declarative programming techniques can be integrated with the traditional procedural control to reduce the efforts in developing error recovery codes and to provide flexibility for future modifications as new error types are encountered. A flexible control system that employs reasoning for both control and error recovery should distribute its functions among hierarchical levels. A symbolic level for task level control and error recovery reasoning is composed of a system model, an assembly plan and a reasoning system. A device level includes all the devices and modular control programs. A robotic assembly of a gear plate is presented as an example application of the intelligent error recovery system. The results from two implemented versions of the example application show that incremental revisions to such a system can be made easily and cost effectively, but the system requires a relatively large initial declarative programming investment. >	automated reasoning	Frank DiCesare;Geoffrey C. Goldbogen;Douglas Feicht;Doo Yong Lee	1993	IEEE Trans. Systems, Man, and Cybernetics	10.1109/21.214780	recovery;declarative programming;simulation;systems modeling;cost-effectiveness analysis;control system;artificial intelligence;machine learning;control theory;supervisory control;manufacturing;fabrication;reason	Robotics	-23.227762360608757	-4.691323724166707	189780
7f78dd625e2da235d97ef3f1b035b641fe071d57	a decision case-based system, that reasons in uncertainty conditions	representacion conocimientos;raisonnement base sur cas;razonamiento fundado sobre caso;decision tree;case base reasoning;incertidumbre;systeme aide decision;uncertainty;rough set theory;sistema ayuda decision;teoria decision;artificial intelligent;decision support system;theorie ensemble approximatif;theorie decision;decision under uncertainty;decision theory;incertitude;information system;case based reasoning;knowledge representation;rough set;representation connaissances;ensemble approximatif;systeme information;sistema informacion	Generally, most Decision Systems do not consider the uncertainty that might be present in knowledge. On many occasions, this leads to proposed solutions that are sometimes inconsistent with the expected results.#R##N##R##N#Case-Based Reasoning is one of the techniques of Artificial Intelligence used in the solution of decision-making problems. Consequently, Case-Based Systems, must consider imperfection in the available knowledge about the world.#R##N##R##N#In this paper, we present a model to make case-based decisions under uncertainty conditions. The model uses Decision Trees and Rough Set Theory to assure an efficient access and an adequate retrieval of cases.		Iliana Gutiérrez Martínez;Rafael Bello	2002		10.1007/3-540-36079-4_5	rough set;optimal decision;decision support system;computer science;artificial intelligence;data mining;mathematics;algorithm	ML	-21.137630216951734	-2.375634941805396	189781
dd6ab163c77e5a26bdda2ed998122178a9d6e17f	answer mining by combining extraction techniques with abductive reasoning.	abductive reasoning		abductive reasoning	Sanda M. Harabagiu;Dan I. Moldovan;Christine Clark;Mitchell Bowden;John Williams;Jeremy Bensley	2003			abductive reasoning;computer science;artificial intelligence;model-based reasoning;machine learning;data mining;reasoning system;deductive reasoning;abductive logic programming	AI	-27.755764317039944	-7.480534033911419	189984
d67f949a92df2b262451659ce654a56705f5a662	knowledge representation on the connection machine	system modeling;semantic network;data mining;computer vision;artificial intelligent;neural net;knowledge representation;expert system	A primary motivation for the development of the Connection Machine (CM) was to create a vehicle for artificial intelligence research. The original design was largely based upon Fahlman's NETL machine [Fah79], the primary purpose of which was to effect large semantic networks, a paradigm of artificial intelligence. To date, however, only a small amount of AI research is being conducted on the CM. Discounting neural net and computer vision research, the amount is miniscule. The lack of AI tools for the CM is a primary cause for this dearth of research. AI researchers proposing to use the CM must first develop the necessary AI tools before beginning work on their projects. For example, there are no existing inference systems, knowledge representation packages, expert system toolkits, etc. PARKA, a pseudo-acronym for “Parallel Knowledge Representation and Association”, was developed as the prototype of one such tool: a knowledge representation system modeled on a frame-based representation language (FDL) [Tou87] paradigm.	artificial intelligence;artificial neural network;attribute-value system;computer vision;connection machine;expert system;gnu free documentation license;knowledge representation and reasoning;list of toolkits;programming paradigm;prototype;semantic network	Matthew P. Evett;Lee Spector;James A. Hendler	1989	Proceedings of the 1989 ACM/IEEE Conference on Supercomputing (Supercomputing '89)	10.1145/76263.76294	applications of artificial intelligence;systems modeling;computer science;artificial intelligence;machine learning;data mining;semantic network;expert system	HPC	-28.590020551931666	-6.663784099851483	190007
0ed6c3de683f58a11f61ae034abc30bf33be5598	intelligent support for interactive configuration of mass-customized products	reconfiguration;arbre graphe;technical information;reconfiguracion;intelligent interfaces;red www;system configuration;tree graph;mass customization;interface intelligente;base connaissance;produccion pequena serie;organisation systeme;small series production;constraint satisfaction;organizacion sistema;interfase;resolucion problema;satisfaction contrainte;interface;automated problem solving;world wide web;base conocimiento;reseau www;satisfaccion restriccion;informacion tecnica;arbol grafo;production petite serie;problem solving;resolution probleme;knowledge base;information technique	Mass customization of configurable products made knowledge-based (and web-based) product configuration systems an important tool to support the sales engineer or end user when configuring systems according to the customer’s needs. Configuration problems are often modeled as Constraint Satisfaction Problems, where the configuration process is an interactive search process. During this search process, the user may encounter situations, where a path in the search tree is reached, where no solution can be found. In this paper we show how modelbased diagnosis techniques can be employed to assist the user to recover from such situations by calculating adequate (or optimal) reconfiguration or recovery actions.	algorithm;constraint satisfaction;constraint satisfaction problem;knowledge-based configuration;knowledge-based systems;library (computing);prototype;search tree;user interface;web application	Alexander Felfernig;Gerhard Friedrich;Dietmar Jannach;Markus Zanker	2001		10.1007/3-540-45517-5_82	configuration management;knowledge base;simulation;constraint satisfaction;mass customization;computer science;artificial intelligence;control reconfiguration;interface;programming language;tree	AI	-24.995898896943114	-4.521738498572079	190198
ea306ebf6b7a618ab7cb313c00f720a54e9d7cb2	possibilistic neural networks for process modelling			artificial neural network;process modeling	Andreas Kanstein	1999				ML	-28.41796529542864	-7.407699636234734	190414
0b9b48d77a4e0c4486f4a5fa0f9063e4087f7e81	flexible learning of problem solving heuristics through adaptive search	flexible learning;learning system;domain specificity;problem solving	Noting that the methods employed by existing learning systems are often bound to the intended task domain and have little applicability outside that domain, this paper considers an alternative learning system design that offers greater flexibility without sacrificing performance. An operational prototype, constructed around a powerful adaptive search technique, is presented and applied to the problem of acquiring problem solving heuristics through experience. Some performance results obtained with the system in a poker betting domain are reported and compared with those of a previously investigated learning system in the same domain. It is seen that comparable levels of performance are achieved by the two systems, despite the latter's dependence on a considerable amount of domain specific knowledge for effective operation.	experience;heuristic (computer science);problem solving;prototype;systems design	Stephen F. Smith	1983			simulation;computer science;artificial intelligence;domain engineering;machine learning	AI	-21.08674843517914	-6.098028860722119	190469
3a21d9e0ac2f0a173528362c5485386b88c022d8	fuzzy logic and hyperknowledge a new, effect paradigm for active dss	problem solving fuzzy logic hyperknowledge active decision support system decision making;fuzzy logic;decision support system;decision support systems;knowledge based systems fuzzy logic decision support systems problem solving;fuzzy logic decision support systems problem solving decision making knowledge management instruments operations research environmental management humans application software;knowledge based systems;problem solving	We have shown that by combining fuzzy logic and hyperknowledge we will get a new paradigm for the development and use of active DSS, which is a more effective and productive way to build decision support systems. We demonstrate that this will help us to gain a better understanding of decision making, problem solving and planning processes in environments, which have not been accessible for systematic studies with traditional research instruments. We also show that active DSS tools can be used as research instruments.	fuzzy logic	Christer Carlsson	1997		10.1109/HICSS.1997.663189	fuzzy logic;r-cast;decision support system;intelligent decision support system;decision analysis;decision engineering;computer science;knowledge management;artificial intelligence;machine learning;management science;business decision mapping	EDA	-31.310913570882594	-8.957818973421208	190481
084a63494892b71e93eb732c9868a41db26d0950	use of formal ontologies to support error checking in specifications	modelizacion;representacion conocimientos;systeme intelligent;formal specification;adquisicion del conocimiento;sistema inteligente;software systems;acquisition connaissances;specification formelle;modelisation;especificacion formal;knowledge acquisition;intelligent system;logic programs;knowledge representation;representation connaissances;modeling;formal ontology	This paper explores the possibility of using formal ontologies to support detection of conceptual errors in speciications. We deene a conceptual error as a misunderstanding of the application domain knowledge which results in undesirable behaviour of the software system. We explain how to use formal ontologies, and in particular ontological constraints , to tackle this problem. We present a exible architecture based on meta interpretation in logic programming in which the speciication is viewed as a multilayer design. We illustrate the signiicance of this approach for the software and ontology engineering community via an example case in the domain of ecological modelling.	application domain;ecosystem model;logic programming;ontology (information science);ontology engineering;software system	Yannis Kalfoglou;David Stuart Robertson	1999		10.1007/3-540-48775-1_13	knowledge representation and reasoning;idef5;systems modeling;computer science;artificial intelligence;data mining;formal specification;database;algorithm;software system	SE	-24.102606734982086	-3.474809567819103	190765
1bc709c2fee908de0e1b8e40c19f147e31e5753b	long-term spatial representations from pictorial and textual input	optimal solution;cognitive map;representacion espacial;spatial reasoning;carte cognitive;apprentissage conceptuel;large scale;aprendizaje conceptual;mapa cognitiva;spatial representation;concept learning;representation spatiale;information system;theorie information;systeme information;information theory;spatial information;mental model;sistema informacion;teoria informacion	Spatial information is typically represented in memory in a form that facilitates operations like spatial reasoning, mental navigation, search, and perspective shifts. This apparently happens whether learning is through primary experience or description, and it appears to happen whether the learned configuration is large- or small-scale. Typical methods used in the cognitive map and mental models literatures diverge, however, in several ways that might affect representation (e.g., medium of input at learning, size of the referent situation, and delay from learning to test). The cognitive map literature shows that spatial representation of large-scale environments persists, though often biased toward spatial simplification over time. This represents one optimal solution to the problem of retaining information in a form that facilitates likely cognitive operations while minimizing resources devoted to storing the information. The current studies investigate whether mental models learned from description are subject to the same tendencies or whether they revert with time to a less spatial format that better optimizes cognitive economy. Secondly, these studies investigate the extent to which visual input at learning affects how the information is represented.	image	Todd Federico;Nancy Franklin	1997		10.1007/3-540-63623-4_55	concept learning;information theory;cognitive map;artificial intelligence;machine learning;mathematics;spatial analysis;spatial intelligence;information system;statistics	HCI	-24.870187414357133	-9.290261000836116	190843
ac2694a2bfeceeecc6c2f40142a3f2b0eb1f83b5	a knowledge-based expert system for automatic analysis and synthesis in cad	knowledge base;expert system		computer-aided design;expert system	Mehmet Dincbas	1980			data mining;knowledge base;expert system;legal expert system;computer science	EDA	-28.535784092437574	-7.336774482475677	190927
4fe6a3d11efa276e04654941c4b5be6fff40162f	efficient lca based keyword search in xml data	query processing;transitive nearest neighbor;low frequency;efficient algorithm;multi channel access;query optimization;keyword search;indexation;lowest common ancestor;xml document;approximate nearest neighbor;experimental evaluation;ke ywords;high frequency	Keyword search in XML documents based on the notion of lowest common ancestors (LCAs) and modifications of it has recently gained research interest [10, 14, 22]. In this paper we propose an efficient algorithm called Indexed Stack to find answers to keyword queries based on XRank's semantics to LCA [10]. The complexity of the Indexed Stack algorithm is O(kd|S1| log |S|) where k is the number of keywords in the query, d is the depth of the tree and |S1| (|S|) is the occurrence of the least (most) frequent keyword in the query. In comparison, the best worst case complexity of the core algorithms in [10] is O(kd|S|). We analytically and experimentally evaluate the Indexed Stack algorithm and the two core algorithms in [10]. The results show that the Indexed Stack algorithm outperforms in terms of both CPU and I/O costs other algorithms by orders of magnitude when the query contains at least one low frequency keyword along with high frequency keywords. This is important in practice since the frequencies of keywords typically vary significantly.	best, worst and average case;central processing unit;dual in-line package;experiment;indexed grammar;input/output;search algorithm;worst-case complexity;xml	Yu Xu;Yannis Papakonstantinou	2008		10.1145/1353343.1353408	query optimization;xml;computer science;high frequency;data mining;database;low frequency;information retrieval;lowest common ancestor	DB	-30.77575415234849	3.680979144599039	191143
3e917067e303084adf018a590f16679c1b6f66b1	document ranking on weight-partitioned signature files	information retrieval;search strategy;term frequency;signature file;superimposed coding;text retrieval;document retrieval;access method	A signature file organization, called the weight-partitioned signature file, for supporting document ranking is proposed. It employs multiple signature files, each of which corresponds to one term frequency, to represent terms with different term frequencies. Words with the same term frequency in a document are grouped together and hashed into the signature file corresponding to that term frequency. This eliminates the need to record the term frequency explicitly for each word. We investigate the effect of false drops on retrieval effectiveness if they are not eliminated in the search process. We have shown that false drops introduce insignificant degradation on precision and recall when the false-drop probability is below a certain threshold. This is an important result since false-drop elimination could become the bottleneck in systems using fast signature file search techniques. We perform an analytical study on the performance of the weight-partitioned signature file under different search strategies and configurations. An optimal formula is obtained to determine for a fixed total storage overhead the storage to be allocated to each partition in order to minimize the effect of false drops on document ranks. Experiments were performed using a document collection to support the analytical results.	archive;digital signature;exptime;elegant degradation;experiment;hp-il;hash function;heuristic (computer science);horseland;lh (complexity);overhead (computing);precision and recall;ranking (information retrieval);tf–idf	Dik Lun Lee;Liming Ren	1996	ACM Trans. Inf. Syst.	10.1145/226163.226164	document retrieval;computer science;data mining;database;tf–idf;access method;world wide web;information retrieval	Web+IR	-28.36906774450473	3.4434675313563745	191342
3497280b548a1cd67f6db3ef056047652f7c746f	omcat: optimal maintenance of continuous queries' answers for trajectories	continuous queries;management system;moving objects databases;moving object database;continuous query;object relational databases;triggers;query answering;switching cost	We present our prototype system, OMCAT, which optimizes the reevaluation of a set of pending continuous spatio-temporal queries on trajectory data, when some of the trajectories are affected by traffic abnormalities reported. The key observation that motivates OMCAT is that an abnormality in a given geographical region may cause changes to the answers of queries pertaining to future portions of affected trajectories. We investigate the sources of context-switching costs at various levels and propose solutions that utilize the correlation of several context dimensions to orchestrate the reevaluation of the queries. OMCAT, fully implemented on top of an existing Object Relational Database Management System - Oracle 9i, demonstrates that our techniques can substantially reduce the response time during query answer update.	object-relational database;optimal maintenance;prototype;relational database management system;response time (technology)	Hui Ding;Goce Trajcevski;Peter Scheuermann	2006		10.1145/1142473.1142575	computer science;data mining;management system;database;conjunctive query;information retrieval;spatial query	DB	-26.299251740664587	0.8891278447134373	191428
8eca44d354083fc15c91edbd6311df64e8a3e532	transferring problem solving strategies from the expert to the end users - supporting understanding	information science;communication studies	If knowledge sharing between people in an organisation is to be encouraged, new types of systems are needed to transfer domain knowledge and problem-solving strategies from an expert to the end users and, thereby, make the knowledge available and applicable in a specific domain. If it is to be possible to apply the knowledge in the organisation, the systems will need a means of illustrating the reasoning strategies involved in interpreting the knowledge to arrive at the conclusions drawn. One solution is to incorporate different diagrams in knowledge management systems to assist the user to comprehend the reasoning strategies and to better understand the knowledge required and gained. This paper describes the manners by which knowledge management systems can facilitate transfer of problem-solving strategies from a domain expert to different kinds of end users. With this objective in mind, we suggest using visualization, graphical diagrams and simulation in conjunction to support the transfer of problem-solving strategies from a domain expert to the end users. Visualization can support end users, enabling them to follow the reasoning strategy of the system more easily. The visualization discussed here includes static and dynamic presentation of the rules and facts in the knowledge base that are used during execution of the system. The static presentation illustrates how different rules are related statically in a sequence diagram in the Unified Modeling Language (UML). The dynamic presentation, in contrast, visualizes rules used and facts relevant to a specific consultation, i.e., this presentation depends on the input inserted by the users and is illustrated in a collaboration diagram in the UML. Utilising these diagrams can support the sharing and reuse of the knowledge and strategies used for handling routine tasks and problems more efficiently and profitably whilst minimizing potential for loss of knowledge. This is important when experts are not available on the spot. These diagrams can also be used for the organisation and the disseminating of knowledge by locating experts in an organisation, which is important when these are to be relocated in large organisations or geographically distributed.	code reuse;communication diagram;graphical user interface;knowledge base;knowledge management;problem solving;reasoning system;sequence diagram;simulation;subject-matter expert;unified modeling language;while	Anne Håkansson	2005				HCI	-29.732377028486013	-5.06153598445459	191648
2f0bdefbc4b607497d811fe14e493ab57ac3a6ba	learning user plan preferences obfuscated by feasibility constraints	hierarchical task network;user preferences;graduate student	It has long been recognized that users can have complex preferences on plans. Non-intrusive learning of such preferences by observing the plans executed by the user is an attractive idea. Unfortunately, the executed plans are often not a true representation of user preferences, as they result from the interaction between user preferences and feasibility constraints. In the travel planning scenario, a user whose true preference is to travel by a plane may well be frequently observed traveling by car because of feasibility constraints (perhaps the user is a poor graduate student). In this work, we describe a novel method for learning true user preferences obfuscated by such feasibility constraints. Our base learner induces probabilistic hierarchical task networks (pHTNs) from sets of training plans. Our approach is to rescale the input so that it represents the user’s preference distribution on plans rather than the observed distribution on plans.	best, worst and average case;obfuscation (software);user (computing)	Nan Li;William Cushing;Subbarao Kambhampati;Sung Wook Yoon	2009			simulation;computer science;knowledge management;artificial intelligence;data mining;hierarchical task network	AI	-19.146591646223495	-5.7330480315135395	191671
bc569249e438c1475971abffe0309ee6456605e9	mediator join indices	distributed algorithms;query processing;update anomalies;n way inter database joins;mediator join indices;anomaly handling overhead;query evaluation;indexation;distributed algorithms distributed databases query processing;query processing indexing database systems indexes;distributed databases;efficient mediator query processing;query scrubbing algorithms;efficient mediator query processing mediator join indices n way inter database joins data transfer query scrubbing algorithms query evaluation query anomalies update anomalies materialized views anomaly handling overhead;materialized views;data transfer;query anomalies	A mediator join index (MJI) is proposed to speed up Nway inter-database joins by reducing the amount of data transfer during evaluation. A family of algorithms, the Query Scrubbing Algorithms (QSA), are developed to maintainMJI and to evaluate queries usingMJI. QSA algorithms use query scrubbing to cope with update and query anomalies related to materialized views in the mediator context. Compared with existing algorithms, QSA algorithms incur less overhead in handling the anomalies and makes MJI a promising technique for efficient mediator query processing.	algorithm;analysis of algorithms;database;matchware mediator;materialized view;mediator pattern;memory scrubbing;overhead (computing);qtscript;qualified security assessor	Ling-Ling Yan;M. Tamer Özsu;Ling Liu	1997		10.1109/RIDE.1997.583698	computer science;data mining;database;information retrieval	DB	-29.182307855273244	0.7231323949407217	191776
427a18c5dd812655cac884acfe1beff2c06fa66e	mdht: a multi-level-indexed dht algorithm to extra-large-scale data retrieval on hdfs/hadoop architecture	distributed hash table;algorithm;cloud storage;data retrieval;multi level index	Corresponding to the storing and fast searching needs of an extra-large scale of energy monitoring and statistics data, we propose a multi-level-indexed distributed hash table (mDHT) algorithm and complete a MapReduce implementation of the algorithm on the open-standard HDFS/Hbase platform. Such an approach uses a columnar storage structure for energy consumption data storage and creates a hashed index table to provide a quick search and retrieval method for extra-large-scale data processing systems. Such a hashed indexing scheme is implemented on a 3-node Hadoop cluster, and the simulation experiments at a scale up to 48 million data records indicate that, when the data volume reaches the scale of 12 million to 48 millions, the proposed mDHT algorithm presents an outstanding performance in data writing operation, compared to that of traditional SQL Server implementation. Even compared to the single-indexed DHT (sDHT) application, the mDHT solution outperforms by reducing the data retrieval time by 24.5–48.6 %. The multi-level-indexed DHT algorithm presented in this paper contributes a key technique to developing a fast search engine to the extra-large scale of data on the cloud storage architecture.	apache hbase;apache hadoop;boyer–moore string search algorithm;cloud storage;computer data storage;data retrieval;distributed hash table;experiment;mapreduce;microsoft sql server;simulation;single-index model;web search engine	Yu Tang;Aihua Fan;Yingjie Wang;Yuanzhe Yao	2014	Personal and Ubiquitous Computing	10.1007/s00779-014-0784-1	computer science;chord;data mining;database;world wide web;data retrieval	DB	-30.77292421328154	0.8419926586197135	191802
8398b47ae487414bbbfd9d5e27e457df4a6b176d	adaptive index structures	total cost;adaptive index structure;minimal update overhead;large number;adaptive b;query characteristic;overall query cost;re-structure adaptive index;update algorithm;query processing;adaptive index;random access;indexation	Traditional indexes aim at optimizing the node accesses during query processing, which, however, does not necessarily minimize the total cost due to the possibly large number of random accesses. In this paper, we propose a general framework for adaptive indexes that improve overall query cost. The performance gain is achieved by allowing index nodes to contain a variable number of disk pages. Update algorithms dynamically re-structure adaptive indexes depending on the data and query characteristics. Extensive experiments show that adaptive Band R-trees significantly outperform their conventional counterparts, while incurring minimal update overhead.	algorithm;cpu cache;cryptographic hash function;database;dataspaces;experiment;gibbs sampling;hoc (programming language);overhead (computing);r* tree	Yufei Tao;Dimitris Papadias	2002			sargable;query optimization;computer science;theoretical computer science;data mining;database;random access	DB	-28.175863995329404	2.874901163575748	191818
b4dc66665188d70bd47e68573af149397e62bb2c	agent-based control of spatially distributed chemical reactor networks	chemical reactors;distributed system;distribucion espacial;reseau capteur;multiagent system;haute performance;systeme reparti;transducer network;monitoring control system;agent based;ingenierie connaissances;adaptive control;systeme controle commande;intelligence artificielle;commande repartie;sistema control mando;repartition spatiale;large scale;control system;flujo red;local structure;red sensores;sistema repartido;control structure;control adaptativo;flow rate;reacteur chimique;spatial distribution;agent intelligent;commande adaptative;parameter space;intelligent agent;sensors and actuators;sensor array;debit;alto rendimiento;reseau transducteur;autoorganizacion;artificial intelligence;self organization;agente inteligente;estructura local;inteligencia artificial;structure locale;control repartido;network flow;sistema multiagente;spatial configuration;high performance;reactor quimico;article;distributed control;flot reseau;autoorganisation;gasto;systeme multiagent;chemical reactor;red transductora;knowledge engineering	Large-scale spatially distributed systems provide a unique and difficult control challenge because of their nonlinearity, spatial distribution and generally high order. The control structure for these systems tend to be both discrete and distributed as well and contain discrete and continuous elements. A layered control structure interfaced with complex arrays of sensors and actuators provides a flexible supervision and control system that can deal with local and global challenges. Traditionally, research on control of nonlinear distributed processes has focused on distributed parameter systems involving mathematically complex model reduction and controller synthesis methodologies. So-called hybrid control systems combine process dynamics and discrete control elements through the use of multiple linear models at different operating points. One alternative approach is based on a hierarchical agent-based system with local and global control structures that has been demonstrated on a network of interconnected continuous stirred tank reactors (CSTRs). Reactor networks exhibit highly complex behavior, with multiple steady state operating regimes, and have a large pool of candidates for manipulated variables. We use autocatalytic reactions in these networks to formulate surrogates for predator-prey, virus propagation in a distributed population, multiple species of animals that rely on the same resources, or chemical manufacturing problems.	as-interface;agent-based model;control flow;control system;distributed computing;game controller;linear model;lotka–volterra equations;nonlinear system;prey;reactor (software);sensor;software propagation;steady state;surrogates	Eric Tatara;Michael J. North;Cindy Hood;Fouad Teymour;Ali Cinar	2005		10.1007/11734697_17	adaptive control;computer science;networked control system;control system;artificial intelligence;chemical reactor;knowledge engineering;operations research;intelligent agent	ML	-22.657718549570216	-6.408265933327684	192090
020fbf9b07df897bc1ae13b3c3c92a1aa2d10d90	disjoint interval partitioning	temporal data;interval data;query processing;join;anti-join;aggregation	In databases with time interval attributes, query processing techniques that are based on sort-merge or sort-aggregate deteriorate. This happens because for intervals no total order exists and either the start or end point is used for the sorting. Doing so leads to inefficient solutions with lots of unproductive comparisons that do not produce an output tuple. Even if just one tuple with a long interval is present in the data, the number of unproductive comparisons of sort-merge and sort-aggregate gets quadratic. In this paper we propose disjoint interval partitioning ( $$\mathcal {DIP}$$ DIP ), a technique to efficiently perform sort-based operators on interval data. $$\mathcal {DIP}$$ DIP divides an input relation into the minimum number of partitions, such that all tuples in a partition are non-overlapping. The absence of overlapping tuples guarantees efficient sort-merge computations without backtracking. With $$\mathcal {DIP}$$ DIP the number of unproductive comparisons is linear in the number of partitions. In contrast to current solutions with inefficient random accesses to the active tuples, $$\mathcal {DIP}$$ DIP fetches the tuples in a partition sequentially. We illustrate the generality and efficiency of $$\mathcal {DIP}$$ DIP by describing and evaluating three basic database operators over interval data: join, anti-join and aggregation.	aggregate data;backtracking;computation;database;merge sort;quadratic function;sorting	Francesco Cafagna;Michael H. Böhlen	2017	The VLDB Journal	10.1007/s00778-017-0456-7	database;algorithm	DB	-25.349263575726887	2.555323479981166	192119
7e47ce1208cf4659e50094f3db71e0243540d6ff	20 years of decision making and decision support research published by the journal of decision systems	decision support		decision support system	Frédéric Adam	2012	Journal of Decision Systems	10.1080/12460125.2012.695890	clinical decision support system;r-cast;decision support system;decision analysis;computer science;artificial intelligence;data mining;business decision mapping	ECom	-30.967667009548727	-9.183872912093097	192162
66e57698c3641d14d7e177b381bef88aaf3fa2ce	queryable sepa message compression by xml schema subtraction	xml schema	In order to standardize the electronic payments within and between the member states of the European Union, SEPA (Single Euro Payments Area) – an XML based standard format – was introduced. As the financial institutes have to store and process huge amounts of SEPA data each day, the verbose structure of XML leads to a bottleneck. In this paper, we propose a compressed format for SEPA data that removes that data from a SEPA document that is already defined by the given SEPA schema. The compressed format allows all operations that have to be performed on SEPA data to be executed on the compressed data directly, i.e., without prior decompression. Even more, the queries being used in our evaluation can be processed on compressed SEPA data with a speed that is comparable to ADSL2+, the fastest ADSL standard. In addition, our tests show that the compressed format reduces the data size to 11% of the original SEPA messages on average, i.e., it compresses SEPA data 3 times stronger than other compressors like gzip, bzip2 or XMill – although these compressors do not allow the direct query processing of the compressed data.	archive;asymmetric digital subscriber line;auxiliary memory;computer data storage;data compression;database;experiment;fastest;netbsd gzip / freebsd gzip;throughput;xml schema;bzip2	Stefan Böttcher;Rita Hartel;Christian Messinger	2010			xml validation;xml encryption;xml;relax ng;xml schema;streaming xml;computer science;xs3p;document definition markup language;document structure description;xml framework;xml database;xml schema;database;document schema definition languages;schematron;xml signature;world wide web;xml schema editor;information retrieval;efficient xml interchange	DB	-30.571446533098513	-1.0335408271029765	192366
7381daca0c46a3b9092d09bcafcbbc6d992f24f0	operational decision support: context-based approach and technological framework	sensibilidad contexto;modelizacion;gestion informacion;metodo adaptativo;decision support;ontologie;context aware;hospital;information sources;systeme aide decision;ingenierie connaissances;information source;source information;methode adaptative;sistema ayuda decision;domain knowledge;resolucion problema;modelisation;hopital;dynamic environment;decision support system;object oriented;contexto;information management;adaptive method;context management;contexte;on the fly;oriente objet;ontologia;knowledge integration;information system;gestion information;sensibilite contexte;modeling;orientado objeto;ontology;context;systeme information;fuente informacion;problem solving;resolution probleme;sistema informacion;knowledge engineering	The paper presents a context-based approach to operational decision support. The approach focuses on modelling and solving a problem considering changes in the dynamic environment. The problem is modelled by abstract and operational contexts integrating information provided by information sources and domain knowledge. The approach involves ontology management operations for knowledge integration, context management techniques for information organisation, and object-oriented constraint network mechanisms for problem definition and solving. The approach is tested as an adaptive service for on-the-fly portable hospital configuration.		Alexander V. Smirnov;Mikhail Pashkin;Nikolay Shilov;Tatiana Levashova	2005		10.1007/11508373_36	systems modeling;decision support system;computer science;knowledge management;artificial intelligence;object-oriented programming;operations research;information system;domain knowledge	Vision	-24.11727563264242	-4.3164452804855875	192815
7c4a6194858bb121d3eaf745e8f7cbb370b1920e	knowledge management in mcda domain	knowledge management;additives;zarządzanie wiedzą;yttrium;taxonomy;ontologies;europe;utility theory	Multi-criteria decision analysis (MCDA) methods have become increasingly popular in decision-making. Numerous methods in this field were developed to solve real-world decision problems including various engineering and scientific areas. Unfortunately, the proper use of each method is difficult due to the dispersion of the domain knowledge and lack of knowledge databases in this area. The paper presents research focused on knowledge management aspects in MCDA domain. In order to achieve a high level of practicality on different levels of decision making, the ontology as a form of conceptualization is implemented.	conceptualization (information science);database;decision analysis;decision problem;high-level programming language;knowledge management	Jaroslaw Watrobski;Jaroslaw Jankowski	2015	2015 Federated Conference on Computer Science and Information Systems (FedCSIS)	10.15439/2015F295	food additive;computer science;knowledge management;ontology;artificial intelligence;yttrium;data mining;management science;utility;domain knowledge;taxonomy	AI	-32.85822861499194	-7.453255089779398	193080
4e3a5f105daa0186789b6fc9c7df14271e0af219	machine learning applications in anthropology: automated discovery over kinship structures	representation des connaissances;social interaction;learning;lien de parente;logic;inductive logic programming;intelligence artificielle;algorithme;algorithm;incomplete data;contexte culturel;apprentissage;cultural context;machine learning;recherche appliquee;organisation sociale;genealogie;artificial intelligence;anthropologie;anthropology;logique;algoritmo;knowledge base	A common problem in anthropological field work is generalizing rules governing social interactions and relations (particularly kinship) from a series of examples. One class of machine learning algorithms is particularly well-suited to this task: inductive logic programming systems, as exemplified by FOIL. A knowledge base of relationships among individuals is established, in the form of a series of singlepredicate facts. Given a set of positive and negative examples of a new relationship, the machine learning programs build a Horn clause description of the target relationship. The power of these algorithms to derive complex hypotheses is demonstrated for a set of kinship relationships drawn from the anthropological literature. FOIL extends the capabilities of earlier anthropology-specific learning programs by providing a more powerful representation for induced relationships, and is better able to learn in the face of noisy or incomplete data.	algorithm;anthropological literature;foil (programming language);field research;horn clause;inductive logic programming;inductive reasoning;interaction;knowledge base;machine learning	Sally Jo Cunningham	1996	Computers and the Humanities	10.1007/BF00057936	natural language processing;knowledge base;computer science;artificial intelligence;machine learning;sociology;logic;algorithm	AI	-23.393473895763872	-1.8854519955556184	193105
7150c20901adb1ceab5b44875ed2b37dd8109269	consumer-oriented product conceptualization via a web-based data mining approach	axiomatic product conceptualization;design knowledge management;laddering technique;restricted coulomb energy neural network;web-based data mining	Rapid advancing information technology (IT) has significantly improved the efficiency and effectiveness of capturing consumers’ concerns of a product and increased the importance of its role in new product development (NPD). To ride on this trend, this work proposes an approach that aims at establishing an axiomatic product conceptualization system (APCS) to meet the demand of consumer-oriented product concept development. The proposed prototype APCS comprises three modules, namely, knowledge elicitation module using laddering technique; knowledge representation module using design knowledge hierarchy (DKH); and knowledge synthesis module using restricted Coulomb energy (RCE) neural network. Accordingly, this system offers a method of making design decisions via a web-based data mining product conceptualization approach. A case study on golf wood club design is used for system illustration.	conceptualization (information science);data mining	Chun-Hsien Chen;Wei Yan;Nai-Feng Chen	2012		10.1007/978-1-4471-4426-7_80	conceptualization;new product development;web application;knowledge representation and reasoning;design knowledge;laddering;data mining;hierarchy;information technology;computer science	ML	-32.96904978550121	-9.271289228210826	193128
6848746678d777021607656a6064235ac28a8c11	autonomous cloud federation for high-throughput queries over voluminous datasets	hybrid clouds cloud computing distributed file systems distributed data structures distributed architectures;distributed architectures;cloud computing data structures query processing instruments metadata aerospace electronics indexes;instruments;metadata;query processing;hybrid clouds;autonomous cloud federation query resolution structure data structure distributed storage framework hybrid cloud high throughput query parameter;query processing cloud computing data structures;indexes;distributed data structures;data structures;aerospace electronics;distributed file systems;cloud computing	The breadth and depth of information being generated and stored continues to grow rapidly, causing an information explosion. Observational devices and remote sensing equipment are no exception here, giving researchers new avenues for detecting and predicting phenomena at a global scale. To cope with increasing storage loads, hybrid clouds offer an elastic solution that also satisfies processing and budgetary needs. In this article, the authors describe their algorithms and system design for dealing with voluminous datasets in a hybrid cloud setting. Their distributed storage framework autonomously tunes in-memory data structures and query parameters to ensure efficient retrievals and minimize resource consumption. To circumvent processing hotspots, they predict changes in incoming traffic and federate their query resolution structures to the public cloud for processing. They demonstrate their framework's efficacy on a real-world, petabyte dataset consisting of more than 20 billion files.	algorithm;cloud computing;clustered file system;data structure;federated identity;in-memory database;information explosion;input device;petabyte;sensor;systems design;throughput	Matthew Malensek;Sangmi Lee Pallickara;Shrideep Pallickara	2016	IEEE Cloud Computing	10.1109/MCC.2016.65	database index;data structure;cloud computing;computer science;operating system;data mining;database;metadata;world wide web;computer security	HPC	-30.70681540861128	0.05221718862634849	193202
b3c4e1ad2821f2119df4fd8a11c8b948c6fc0c6b	expert systems in medicine and moral responsibility	sistema experto;aplicacion medical;concepcion sistema;etica;responsibility;moral;prise decision;moral science;ethics;responsabilidad;system design;responsabilite;ethique;medical expertise;medical application;systeme expert;morale;toma decision;peritaje medical;conception systeme;expertise medicale;application medicale;expert system	Abstract   The use of computerized expert systems as an aid in medicine may cause new moral problems. Medical decision making is inherently normative. This fact may be overlooked through an increased dependence on expert systems or the expert base will have to include ethical decision criteria. However, the construction of rational decision criteria for ethical decisions as a base for expert systems in medicine can be questioned, especially in a multicultural context. Reliance on expert systems may threaten the responsibility of the physician. The ability to take responsibility presupposes   1.   (1) that the decision maker is autonomous, and   2.   (2) the possibility for the decision maker to see the consequences of his or her actions.    The use of expert systems must not impede the fulfillment of these conditions, or physicians will become dependent on the technical system.	expert system	Göran Collste	1992	Journal of Systems and Software	10.1016/0164-1212(92)90076-V	ethics;computer science;management;operations research;expert system;systems design	OS	-25.34183811169183	-5.476110838106427	193300
20fbf7ab2f31eb3e12c0b79772b71e4a7b195bf8	evidential reasoning in expert systems: computational methods	knowledge based system;expert systems;computational method;automated reasoning;evidential reasoning;dempster shafer theory;knowledge based systems;expert system		computation;expert system	Jiwen Guan;Z. Guan;David A. Bell	1994			knowledge representation and reasoning;legal expert system;dempster–shafer theory;computer science;artificial intelligence;model-based reasoning;knowledge-based systems;machine learning;data mining;reasoning system;automated reasoning;evidential reasoning approach;expert system	AI	-27.860821636346813	-7.837822593850724	193392
b00aed5a7fd09b853c69a9d229b3dc25c293cda9	guidelines for developing effective decision support systems	decision support system		decision support system	Cyril H. P. Brookes	1986	Australian Computer Journal		clinical decision support system;r-cast;decision support system;intelligent decision support system;decision analysis;decision engineering;computer science;business decision mapping	DB	-31.08598023286915	-9.143674514129785	193482
5cf75387f11ba09576a9a33a50624816ad8a29e5	pluralistic evaluation of belief plausibility and its application to nonmonotonic reasoning	nonmonotonic reasoning	Abstract   A pluralistic evaluation of belief plausibility is introduced by extending the notion of plausibility index introduced by Rescher. The properties of the extended plausibility are analyzed by comparing its properties with those of probability theoretic indexing and Rescher's indexing of beliefs. Also it is shown that the new indexing method can be effectively utilized in truth maintenance of beliefs in complex and dynamically changing situations. First, the reason we focus our attention on Rescher's plausibility index of beliefs and the necessity for its extension are clarified by referring to the method of knowledge organization and maintenance called ATMS (assumption-based truth maintenance system) introduced by De Kleer, which is an extension of Doyle's TMS. Second, Rescher's plausibility index is briefly reviewed. Third, the index is compared with the probability theoretic index of beliefs by referring to a system of logic called preference logic that reflects essential features of the indices. The comparison clarifies the monotonic, inflexible nature of Rescher's index and the need to extend it in order to cope with the nonmonotonic nature of truth maintenance in dynamically changing situations. An extension of Rescher's plausibility index is introduced, and its properties are examined. Finally, an application of the indexing method to ATMS and to default reasoning is presented, showing that the proposed indexing method can be effectively utilized in truth maintenance of beliefs in complex and dynamically changing situations.	non-monotonic logic;plausibility structure	Osamu Katai;Sosuke Iwai	1989	Int. J. Approx. Reasoning	10.1016/0888-613X(89)90007-8	computer science;artificial intelligence;non-monotonic logic	AI	-19.634994356445453	3.7442601811010623	194255
e8eb6f64f6b73894e1f7d078fe981a5bece874c5	modular construction of logic knowledge bases: an algebraic approach	algebraic approach;representacion conocimientos;information systems;technology;logic;base connaissance;intelligence artificielle;modularite;journal article;category theory;science technology;theorie categorie;artificial intelligence;base conocimiento;modularity;inteligencia artificial;computer science;knowledge representation;modularidad;teoria categoria;representation connaissances;logique;logica;knowledge base	Abstract   Modularity, reusability, extension capabilities and transformations are among the main facilities that must be considered in knowledge engineering targeted to the development of knowledge bases in the large. Moreover, fast and efficient techniques for updating, querying and inferencing with knowledge bases are also important issues. It seems that current knowledge representation approaches, namely the logic and the structural approaches, do not cope with the essential aspects that must characterize a knowledge engineer workbench. Herein, an algebraic approach allowing the modular construction of knowledge bases is proposed. The main building blocks of knowledge bases are the semantic primitives defined by theory morphisms, mappings taking knowledge bases into knowledge bases, and an interpretation functor. The application of a semantic primitive is the value of the interpretation functor for the chosen interpretations of the argument knowledge bases of the theory morphisms involved. A knowledge base becomes a structured theory from which we can retrace the respective construction sequence. Incidentally, the approach allows the description of a new architecture for knowledge engineering supporting two alternative representations of knowledge bases: a data base representation and a theory representation. Fast, concurrent and dumb updating and querying of knowledge bases can be targeted to the data base representation, whereas intelligent accesses can be directed to the theory representation. The structure of the knowledge base makes possible the selection of the relevant fragment for the purposes at hand. The E-R modeling concepts are adopted for illustrating the approach. The structuring aspects are discussed upon an E-R example of a fragment of a stock management knowledge base.	knowledge base;linear algebra	Cristina Sernadas;José Luiz Fiadeiro;Amílcar Sernadas	1990	Inf. Syst.	10.1016/0306-4379(90)90015-H	knowledge base;computer science;artificial intelligence;body of knowledge;knowledge-based systems;machine learning;open knowledge base connectivity;data mining;database;modularity;knowledge extraction;logic;domain knowledge;algorithm;category theory;technology	AI	-23.831497302902786	-1.3903799809164645	194732
9f6f82274fc969e19382f38926d403cf0890653d	triggered moving range queries over rfid monitored objects	location uncertainty;monitored objects;moving range query;probability;rfid	As a promising technology for monitoring and tracing the product flows and human activities, Radio Frequency Identification (RFID) has received much attention within database community. Moving range query over RFID data streams is one of the most important spatio-temporal queries to support valuable information analysis. However, the uncertainty of the monitored object location challenges the query strategy. Novel models and methods are desired. In this paper, we propose a probability evaluation model in the RFID-enabled monitoring environments and discuss the query optimization techniques under the scenarios of triggered moving range query, which can also be applied into more situations. The extensive experimental evaluation verifies the efficiency and effectiveness of our proposed model and methods.	mathematical optimization;query optimization;radio frequency;radio-frequency identification;range query (database);real-time clock	Yu Gu;Ge Yu;Na Guo	2013	J. Inf. Sci. Eng.		computer science;data mining;database;world wide web	DB	-25.64331568034098	0.40579211358489664	194806
38b75a4e4e85264722a9d73eaae2c734b2750087	consodilation of fingerprint databases: a malaysian case study	databases;ners;mafis;fingerprint identification cloud computing computer forensics database management systems;standards;computer forensics;ners finger print database multimodal biometric framework afis mafis biofis;hybrid intelligent systems;multimodal biometric framework;database management systems;hybrid intelligent system;decision fusion fingerprint database consolidation malaysian case study cloud computing classifier;thumb;fingerprint recognition databases feature extraction standards hybrid intelligent systems thumb;fingerprint recognition;biofis;feature extraction;afis;finger print database;fingerprint identification;cloud computing	This paper presents the challenges of consolidating fingerprint databases in Malaysia as well as approaches that can be taken to solve some of these challenges. Solutions leverages on opportunities presented by standard bodies, advances in cloud computing as well as a framework which leverages on classifiers and decision fusion in order to reduce the large search space expected of a consolidated fingerprint database.	cloud computing;database;fingerprint	Chiung Ching Ho;Chikkannan Eswaran	2011	2011 11th International Conference on Hybrid Intelligent Systems (HIS)	10.1109/HIS.2011.6122148	engineering;data mining;world wide web;computer security	Robotics	-33.63084569859923	2.980602384707171	194836
36f57fe38ed38176139157c748fa24a29c51c503	a flexible architecture for autonomous agents	inf;autonomous agent;decision under uncertainty;other	The authors would like to thank Integral Solutions Ltd for project management, Masons Solicitors and Lloyds Register for their stimulating studies of sociolegal issues and software safety, and Paul Krause of the Imperial Cancer Research Fund and Simon Parsons of Queen Mary and Westteld College, for many helpful discussions. We would particularly like to remember Mike Clarke who shared in the conception of the project but who sadly died during its course. 1 A exible architecture for autonomous agents 2 Abstract A generic architecture for autonomous agents is presented. In common with other current proposals the agent is capable of reacting to and reasoning about events which occur in its environment, execute actions and plans in order to achieve goals in its environment, and communicate with other agents. The work described here proposes certain advances on other systems, notably the ability to reason about and make decisions under uncertainty, including decisions about competing beliefs and alternative actions. The framework is grounded in a non-classical decision model, the 'domino' model. This is formalised to ensure continuity with classical decision theory and avoid ad hoc features. The domino model is embodied in a well-deened knowledge representation language, R 2 L, which explicitly supports the central concepts of decisions and plans, and associated constructs of goals, arguments, commitments, obligations and constraints. The availability of such a language provides a sound basis for building knowledge based agents for practical applications. A major issue for such applications , however, is how to ensure their safe operation. This is a central issue whether the agents are used in an advisory role (e.g. decision support systems) or an autonomous one (e.g. in a robot). Techniques for explicit management of safety are described and some broader theoretical implications are discussed.	autonomous agent;autonomous robot;decision support system;decision theory;edmund m. clarke;hoc (programming language);knowledge representation and reasoning;scott continuity	Subrata Kumar Das;John Fox;D. Elsdon;Peter Hammond	1997	J. Exp. Theor. Artif. Intell.	10.1080/095281397146979	computer science;knowledge management;artificial intelligence;autonomous agent	AI	-20.64177757835639	-9.096180415085957	194984
6e7510e9c211ee65a600f486ca088397e2116902	changes of dimension data in temporal data warehouses	base donnee;analisis datos;base donnee temporelle;temporal data;database;base dato;multidimensional analysis;almacen dato;data model;multi dimensional;data analysis;temporal database;analyse n dimensionnelle;analisis n dimensional;multidimensional data base;analyse donnee;base donnee multidimensionnelle;entrepot donnee;data warehouse	Time is one of the dimensions we frequently find in data warehouses allowing comparisons of data in different periods. In current multi-dimensional data warehouse technology changes of dimension data cannot be represented adequately since all dimensions are (implicitly) considered as orthogonal. We propose an extension of the multidimensional data model employed in data warehouses allowing to cope correctly with changes in dimension data: a temporal multi-dimensional data model allows the registration of temporal versions of dimension data. Mappings are provided to transfer data between different temporal versions of the instances of dimensions and enable the system to correctly answer queries spanning multiple periods and thus different versions of	ats;computation;correctness (computer science);data model;database schema;economic complexity index;file spanning;online analytical processing;schema evolution;temporal database;transformation matrix	Johann Eder;Christian Koncilia	2001		10.1007/3-540-44801-2_28	computer science;data science;data warehouse;data mining;database;temporal database	DB	-27.599251221270297	3.596025375646171	195111
197c9f54ac83b264560d70272ad8d3bc5d705c35	automated product pricing using argumentation	decision maker;autonomous agent;decision making process;agent systems;point of view	This paper describes an argumentation-based approach for automating the decision making process of an autonomous agent for pricing products. Product pricing usually involves different decision makers with different possibly conflicting points of view. Moreover, when considering firms in the retail business sector, they have hundreds or thousands of products to apply a pricing policy. Our approach allows for applying a price policy to each one of them by taking into account different points of view expressed through different arguments and the dynamic environment of the application. This is done because argumentation is a reasoning mechanism based on the construction and the evaluation of interacting conflicting arguments. We also show how we conceived and developed our agent using the Agent Systems Engineering Methodology (ASEME).	autonomous agent;autonomous robot;interaction;privacy policy;systems engineering	Nikolaos I. Spanoudakis;Pavlos Moraitis	2009		10.1007/978-1-4419-0221-4_38	decision-making;decision analysis;computer science;knowledge management;artificial intelligence;management science;business decision mapping	AI	-20.23209952971607	-8.687110079424817	195223
46584443cd65d2435fb6684b8afcf12f233e7b7b	ontological approach to development of computing with words based systems	regle inference;representacion conocimientos;ontologie;sistema experto;architecture systeme;computacion informatica;computing with words;generalized constraints;web semantique;fuzzy relation;customization;personnalisation;semantics;intelligence artificielle;semantica;semantique;artificial intelligent;inference rule;internet;civil engineering;ciencias basicas y experimentales;web semantica;intelligent system;personalizacion;representation connaissance;semantic web;artificial intelligence;ontologia;arquitectura sistema;fuzzy relations;genie civil;inference engine;inteligencia artificial;systeme expert;ontology of fuzzy relations;knowledge representation;relation floue;grupo a;system architecture;ingenieria civil;ontology;relacion difusa;computing with words based system architecture;regla inferencia;expert system	Computing with words introduced by Zadeh becomes a very important concept in processing of knowledge represented in the form of propositions. Two aspects of this concept – approximation and personalization – are essential to the process of building intelligent systems for human-centric computing. For the last several years, Artificial Intelligence community has used ontology as a means for representing knowledge. Recently, the development of a new Internet paradigm – the Semantic Web – has led to introduction of another form of ontology. It allows for defining concepts, identifying relationships among these concepts, and representing concrete information. In other words, an ontology has become a very powerful way of representing not only information but also its semantics. The paper proposes an application of ontology, in the sense of the Semantic Web, for development of computing with words based systems capable of performing operations on propositions including their semantics. The ontology-based approach is very flexible and provides a rich environment for expressing different types of information including perceptions. It also provides a simple way of personalization of propositions. An architecture of computing with words based system is proposed. A prototype of such a system is described. 2008 Published by Elsevier Inc.	approximation;artificial intelligence;computing with words and perceptions;personalization;programming paradigm;prototype;semantic web	Marek Reformat;Cuong Ly	2009	Int. J. Approx. Reasoning	10.1016/j.ijar.2008.03.004	computing with words and perceptions;the internet;computer science;ontology;artificial intelligence;theoretical computer science;machine learning;semantic web;ontology;data mining;semantics;expert system;inference engine;rule of inference	AI	-22.810132518094626	-1.0431193127337608	195433
4334382c1b8a08f0e18e7137d5b1f33125ece70c	highway design by constraint specification	systeme intelligent;design process;highway layout;implementation;sistema inteligente;coaccion;contrainte;base connaissance;conception;prototipo;ejecucion;constraint;intelligent system;diseno;base conocimiento;design;prototype;knowledge base	This paper describes the application of AI techniques to the problem of designing civil engineering objects like highways or roads. These design processes must cope with a complex environment, and constraints and alternatives are not predefined, but chosen during the process of designing. Several abstract models of design are presented and their applicability to road design is discussed. Finally, a partially new model is defined and applied to this problem. Outputs and inputs of an implemented prototype are shown.	prototype	José-Luis Pérez-de-la-Cruz;Ricardo Conejo;Rafael Morales Bueno;J. Puy-Huarte	1995	AI in Engineering	10.1016/0954-1810(94)00013-U	design;knowledge base;simulation;design process;computer science;systems engineering;engineering;artificial intelligence;prototype;constraint;implementation	AI	-24.34165550213011	-3.947699716344448	195600
2473071cd377ee348b0260773b62ef161a281995	the darpa advanced logistics project	next generation;information system;defense advanced research project agency	Logistics deals with the problem of getting the right “stuff” (people, materials, supplies) to the right “place” at the right “time”. Major transportation vendors such as Fedex, UPS, Emery Worldwide, require the efficient solution of logistics problems on a minute by minute basis. Major corporate organizations such as Ford and GM, need to keep inventories at optimal levels, while supporting the smooth inflow of production materials, and smooth outflow of finished products. During the last few years, it has been widely recognized that the next generation of logistics products will, in reality, be powerful information systems that manipulate massive, distributed, logistics databases, and enable logisticians to perform a variety of functions such as tracking the status of supplies and materials, planning based on the current status, efficiently tracking status changes as they occur, and replanning as needed in order to accomplish the mission(s) at hand. In 1996, the Defense Advanced Research Projects Agency (DARPA) started an 80 million dollar research effort called the Advanced Logistics Project (ALP) aimed at developing the next generation of logistics systems. In this paper, we will describe the goals of ALP, describe the multi-agent logistics architecture proposed by ALP, and show how this architecture supports the achievement of ALP's goals.	algorithm;brute-force search;business process;columbus;cache (computing);computation;computational problem;constraint satisfaction problem;data model;data structure;database;decision problem;entity;full scale;global variable;information system;interaction;inventory;logistics;lookup table;multi-agent system;next-generation network;polling (computer science);requirement;scalability;speedup;uninterruptible power supply;user agent	Sibel Adali;Leo Pigaty	2003	Annals of Mathematics and Artificial Intelligence	10.1023/A:1021563826306	logistics;simulation;humanitarian logistics;computer science;artificial intelligence;operations research;information system	AI	-30.981488143178687	-2.786791044912255	195616
8a2be87e23721056858184c1b9db645f6a31b4fa	indirectly driven knowledge modelling in ecology	modelizacion;representacion conocimientos;ontologie;semantic tagging;scientific observation;ecologia;information scientifique technique;metodo formal;methode formelle;semantics;ecologie;ecology;semantica;semantique;formal method;modelisation;analyse syntaxique;knowledge modelling;marcacion;analisis sintaxico;syntactic analysis;marquage;indirectly driven knowledge modelling;representation connaissance;ecological knowledge models;ontologia;ontologies;scientific technical information;knowledge representation;informacion cientifica tecnica;modeling;ecological metadata;ontology;tagging	We describe collaborative efforts among a group of Knowledge Representation (KR) experts, domain scientists, and scientific information managers in developing knowledge models for ecological and environmental concepts. The development of formal, structured approaches to KR used by the group (i.e., ontologies) can be informed by evidence marshalled from unstructured approaches to KR and semantic tagging already in use by the community.	ecology;feedback;formal ontology;interaction;knowledge representation and reasoning;marshalling (computer science);ontology (information science);prototype	Deana D. Pennington;Ioannis N. Athanasiadis;Shawn Bowers;Sergey Krivov;Joshua S. Madin;Mark Schildhauer;Ferdinando Villa	2008	IJMSO	10.1504/IJMSO.2008.023569	systems modeling;computer science;knowledge management;ontology;artificial intelligence;parsing;ontology;data mining;semantics	Web+IR	-23.373485829250615	-3.6144592392627795	195825
d5e6b2fb7b5fe0d78621991e87fe8d47a471235e	hobi: hierarchically organized bitmap index for indexing dimensional data	internet auction;indexation	In this paper we propose a hierarchically organized bitmap index (HOBI) for optimizing star queries that filter data and compute aggregates along a dimension hierarchy. HOBI is created on a dimension hierarchy. The index is composed of hierarchically organized bitmap indexes, one bitmap index for one dimension level. It supports range predicates on dimensional values as well as roll-up operations along a dimension hierarchy. HOBI was implemented on top on Oracle10g and evaluated experimentally. Its performance was compared to a native Oracle bitmap join index. Experiments were run on a real dataset, coming from the biggest East-European Internet auction platform  Allegro.pl  . The experiments show that HOBI offers better star query performance than the native Oracle bitmap join index.	bitmap index	Jan Chmiel;Tadeusz Morzy;Robert Wrembel	2009		10.1007/978-3-642-03730-6_8	computer science;bitmap index;data mining;database;world wide web	DB	-30.301997967740924	1.7379522311034679	195829
3d1e6b67f9f2a19ea7f4708f86474fc71fee5de8	automatic generation of an operation procedure presentation system reusing user's input data		Users use software applications to achieve a goal. Occasionally they make mistakes in the operation path due to the complexity of large-scale applications, which requires them to back track to the appropriate operation step and reenter previously input data. This is burdensome for users. Herein a method is proposed to generate an operation support system that reuses previously input data in an inappropriate operation path as much as possible by navigating users to the appropriate operation path. Specifically, our method has an input reuse function for copying previously input data to similar input items as well as an operation procedure presentation function to highlight the operation procedure from the current step to the goal. Our integrated operation support can minimize users’ rework. To generate our system, developers must create an ontology, including concepts of label names of input items, correspondence between input items and label names, an activity diagram of the target application, and the operation procedure. Our system uses this information to compute the similarity of label names between input items, copy input data for similar input items, and present operation procedures to users.	activity diagram;rework (electronics)	Shimon Nakamura;Hajime Iwata;Junko Shirogane;Yoshiaki Fukazawa	2018		10.5220/0006628001240131	reuse;theoretical computer science;machine learning;activity diagram;computer science;artificial intelligence	HCI	-29.35386271467283	-4.698260167549342	195929
823831d3ada9645e098090ff7a69f317aea89bc1	intra-agent explanation using temporal and extended causal maps	multi agent system;reasoning	multi agent system;reasoning	causal filter;map	Aroua Hedhili Sbaï;Wided Lejouad Chaari;Khaled Ghédira	2013		10.1016/j.procs.2013.09.100	computer science;knowledge management;artificial intelligence;machine learning;data mining;algorithm	AI	-27.03164226640567	-7.874972369020578	196285
3cb89169bea65dc6c6c8e3433eb1c3e9fa7977ed	generalizing inconsistency learning for constraint satisfaction	learning experience;constraint satisfaction;artificial intelligent;learning methods;cost effectiveness;constraint satisfaction problem	Constraint satisfaction problems, where values are sought for problem variables subject to restrictions on which combinations of values are acceptable, have many applications in artificial intelligence. Conventional learning methods acquire individual tuples of inconsistent values. These learning experiences can be generalized. We propose a model of generalized learning, based on inconsistency preserving mappings, which is sufficiently focused so as to be computationally cost effective. Rather than recording an individual inconsistency that led to a failure, and looking for that specific inconsistency to recur, we observe the context of a failure, and then look for a related context in which to apply our experience opportunistically. As a result we leverage our learning power. This model is implemented, extended and evaluated using two simple but important classes of constraint problems.	artificial intelligence;constraint satisfaction problem	Eugene C. Freuder;Richard J. Wallace	1995			cost-effectiveness analysis;constraint satisfaction;constraint learning;computer science;constraint graph;artificial intelligence;machine learning;data mining;constraint satisfaction dual problem;mathematics;complexity of constraint satisfaction;constraint satisfaction problem;hybrid algorithm;local consistency;backtracking	AI	-20.062268948724924	-5.216172422096604	196348
c1c6b94436c533cc85485b555a2a5fa3dabbaf32	online expansion of largescale data warehouses.		Modern data warehouses store exceedingly large amounts of data, generally considered the crown jewels of an enterprise. The amount of data maintained in such data warehouses increases significantly over time—often at a continuous pace, e.g., by gathering additional data or retaining data for longer periods to derive additional business value, but occasionally also precipitously, e.g., when consolidating disparate data warehouses and Data Marts into a single database. Having to expand a data warehouse with 100’s of TB of data by a substantial portion, e.g., 100% or more is a complex and disruptive maintenance operation as it typically involves some sort of dumping and reloading of data which requires substantial downtime. In this paper we describe the methodology and mechanisms we developed in Greenplum Database to expand largescale data warehouses in an online fashion, i.e., without noticeable downtime. At the core of our approach is a set of robust and transactionally consistent primitives that enable efficient data movement. Special emphasis was put on usability and control that lets an administrator tailor the expansion process to specific operational characteristics via priorities and schedules. We present a number of experiments to quantify the impact of an on-going expansion on query workloads.	computer engineering;crown group;downtime;experiment;pivotal greenplum database;scheduling (computing);terabyte;usability	Jeffrey Cohen;John Eshleman;Brian Hagenbuch;Joy Kent;Christopher Pedrotti;Gavin Sherry;Florian Waas	2011	PVLDB		computer science;data warehouse;data mining;database	DB	-31.25362369951811	-2.150633631366447	196368
3e2dab351a1eccfe2c21837948751f8e93388ba3	estimating bucket accesses: a practical approach	approximation algorithms;indexes;probability distribution;indexes probability distribution algorithm design and analysis ink approximation algorithms;ink;algorithm design and analysis	In optimizing database queries one inevitably encounters two important estimation problems. The first problem is to estimate the number of page accesses when selecting k tuples from a relation. The other problem is to estimate the number of different equijoin values remaining after selecting k tuples from a relation. The estimated values strongly depend on how the tuples are distributed over the pages (first problem) and how the equijoin values are distributed over the relation (second problem). It appears to be possible to find restrictive upper and lower limits for these problems in many practical situations. Results derived elsewhere appear to fall significantly outside these limits. Finally, a (time) efficient algorithm to approximate the values to be estimated, is proposed.	approximation algorithm;database;join (sql)	Alle IJbema;Henk M. Blanken	1986	1986 IEEE Second International Conference on Data Engineering	10.1109/ICDE.1986.7266203	probability distribution;database index;algorithm design;computer science;theoretical computer science;data mining;database;approximation algorithm;algorithm;statistics	DB	-25.107166610824876	2.3256686918296174	196383
c13bddff2d096a116f859f568567d212ee9ff4f3	qualitative data modeling: application of a mechanism for interpreting graphical data	well testing;data interpretation;qualitative data;intelligence artificielle;raisonnement qualitatif;raisonnement;interpretacion;experto;interpretation de donnees;razonamiento;pattern recognition;interpretation;test de puits;artificial intelligence;razonamiento calitativo;reconnaissance des formes;inteligencia artificial;reconnaissance forme;expert;qualitative reasoning;reasoning;reconocimiento patron	ion. 112 COMPUT. INTELL. VOL. 5 . 1989 FIG. 1. Characteristic shapes for gas well buildup curves: (u) ideal; (b) positive skin, wellbore storage, and partial penetration; (c) negative skin; (d) naturally fractured; (e) stratified layers; (I) well in bounded reservoir. This paper describes the notion of qualitative data modeling (QDM) for visually interpreting graphical data. The approach involves quantization of numeric behaviour models into a discrete qualitative representation. However, instead of using this representation for predicting or explaining behaviour, it is used as a standard for modeling other collected data. Observed data is automatically transformed into a qualitative description that characterizes its shape. This abstraction is then matched by shape with existing models. Once a model has been selected, a prioritized curve fitting is performed to estimate parameter values for that model. As with many qualitative reasoning theories, the concept of qualitative data modeling is the result of formalizing and implementing the qualitative reasoning performed by experts in a particular domain (Yip 1987). This paper continues with a motivating account of the problem of interpreting data from a well test. A description of the technical approach employed by QDM in capturing the visual interpretation skills of well test engineers is then provided. Specific attention is given to the representation of both the data and the physical system models. This is followed by the design of the’ knowledge-based system architecture into which QDM is incorporated. The specific topics of qualitative domain abstraction and model identification follow from the architecture. A brief account of the implementation using the ART development environment is then provided. The paper concludes with a discussion of related work, an evaluation of QDM, and speculation on extensions to the system.	behavioral modeling;curve fitting;data modeling;emoticon;knowledge-based systems;quantization (signal processing);system identification;systems architecture;test engineer;theory	Sheila A. McIlraith	1989	Computational Intelligence	10.1111/j.1467-8640.1989.tb00320.x	qualitative property;well test;qualitative reasoning;interpretation;computer science;artificial intelligence;data analysis;reason;algorithm	AI	-25.391756601516295	-4.758154921660657	196600
c359f5b2c015312f061e82ed556ddeaa78202d6a	pluralistic multi-agent decision support system: a framework and an empirical test	decision support;design principle;multiagent system;multi agent system;agent based;systeme aide decision;intelligence artificielle;sistema ayuda decision;prise decision;multi agent systems;decision support system;intelligent agents;critiquing systems;agent intelligent;decision support systems;pluralistic models;intelligent system;intelligent agent;artificial intelligence;agente inteligente;inteligencia artificial;sistema multiagente;toma decision;human decision making;problem solving;systeme multiagent	Recent research in decision support systems (DSSs) has focused on building active cooperative intelligent systems. Research in agent-based decision support is a promising stream in this direction. This paper proposes a framework for a pluralistic multiagent decision support system (MADSS). The distinguishing feature of the proposed approach is its organization around human decision making process. The framework builds upon the decision support pyramid with agents organized into groups according to the phases of the problem solving model. We outline the design principles and develop architecture for MADSS. The framework is illustrated through an investment MADSS prototype. The results of the empirical test are presented. # 2003 Elsevier B.V. All rights reserved.	agent-based model;artificial neural network;complexity;component-based software engineering;decision support system;experiment;expert system;multi-agent system;problem solving;prototype	Rustam M. Vahidov;Bijan Fazlollahi	2004	Information & Management	10.1016/j.im.2003.08.017	r-cast;decision support system;intelligent decision support system;computer science;engineering;knowledge management;artificial intelligence;operations research	AI	-26.89649594276841	-4.893839947312122	196647
7549a6e03afdd42ae94c19235db076319eeb34f8	do we need specialized graph databases?: benchmarking real-time social networking applications		With the advent of online social networks, there is an increasing demand for storage and processing of graph-structured data. Social networking applications pose new challenges to data management systems due to demand for real-time querying and manipulation of the graph structure. Recently, several systems specialized systems for graph-structured data have been introduced. However, whether we should abandon mature RDBMS technology for graph databases remains an ongoing discussion. In this paper we present an graph database benchmarking architecture built on the existing LDBC Social Network Benchmark. Our proposed architecture stresses the systems with an interactive transactional workload to better simulate the real-time nature of social networking applications. Using this improved architecture, we evaluated a selection of specialized graph databases, RDF stores, and RDBMSes adapted for graphs. We do not find that specialized graph databases provide definitively better performance.	benchmark (computing);graph (abstract data type);graph database;real-time locating system;real-time transcription;relational database management system;resource description framework;simulation;social network	Anil Pacaci;Alice Zhou;Jimmy Lin;M. Tamer Özsu	2017		10.1145/3078447.3078459	computer science;rdf;architecture;wait-for graph;database;relational database management system;benchmarking;data mining;graph database;social network;data management	DB	-32.518997806771296	0.7544942613569792	196936
1afc565f01ae90ff0c7d9f5cb2f021756f3215a9	language processing and commonsense reasoning	commonsense reasoning;language processing;knowledge acquisition;natural language	The goal of this project is to develop the technology to construct intelligent, natural-languagecapable agents. Such agents will have natural language and reasoning capabilities that facilitate interaction with untrained users who are seeking information pertinent to tasks in which they are engaged. Emphasis is given to techniques for creating such agents for new domains, focusing specifically on portability and knowledge acquisition. In particular, we have been developing DIRC (Domain Independent Retargetable Consultant), a kind of intelligent, naturai-language-capable consultation shell, and a number of mechanisms for world and language knowledge acquisition.	commonsense reasoning;knowledge acquisition;natural language;relevance;software portability	Robert Wilensky	1991			natural language processing;commonsense reasoning;universal networking language;question answering;verbal reasoning;computer science;model-based reasoning;linguistics;reasoning system;natural language;commonsense knowledge;language technology	AI	-25.689621481877857	-8.147396962450806	197137
7472d809da7e339f3c63d481ebcf92250c9514bd	involving the human user in the control architecture of an autonomous agent	plan execution;autonomous agent;replanning;situation awareness	The paper presents an architecture for an autonomous robotic agent, which carries on a plan in a partially observable environment. A Supervisor module is in charge of assuring the correct execution of the plan, possibly by inferring alternative recovery plans when unexpected contingencies occur. In the present paper we describe a control strategy where a human user is directly involved in the control loop, and plays the role of advisor by helping the robotic agent both for reducing ambiguity in the robot’s observations, and for selecting the preferred recovery plan.	autonomous agent;autonomous robot;computation;control system;control theory;disaster recovery plan;interaction;partially observable system	Roberto Micalizio;Giancarlo Nuzzolo;Enrico Scala;Pietro Torasso	2010		10.1007/978-3-642-15286-3_2	simulation;engineering;knowledge management;autonomous agent;operations management	AI	-19.60541151581325	-8.709923247938638	197203
5a1843df61138ff6927f4d18c2ee896fb0f79bd4	enhancing cellular automata by an embedded generalized multi-layer perceptron	simulation ordinateur;sistema hibrido;neural networks;multilayer perceptrons;perceptron multicouche;hybrid approach;a priori knowledge;red multinivel;complex system;automate cellulaire;emergent behavior;hybrid system;evolutionary strategy;algorithme evolutionniste;dynamic simulation;multi layer perceptron;algoritmo evolucionista;simulacion computadora;multilayer network;evolutionary algorithm;reseau multicouche;reseau neuronal;modelling and simulation;cellular automata;computer simulation;cellular automaton;red neuronal;artificial neural network;neural network;systeme hybride;automata celular	Database Workshop is a relational database manager and provides the ability to create, edit and analyze database files in IDRISI using the Microsoft ADO and Access Jet Engines. Both the Calculate and Filter operations are supported through the use of Structured Query Language (SQL). Image Calculator is an interactive mathematical modeling tool that allows you to enter a model as a full algebraic equation using a calculator-like interface. It also supports mathematical expressions and logical queries.	algebraic equation;cellular automaton;mathematical model;perceptron;query language;relational database;sql;terrset geospatial monitoring and modeling software	Giuseppe A. Trunfio	2005		10.1007/11550822_54	a priori and a posteriori;computer science;artificial intelligence;machine learning;evolution strategy;multilayer perceptron;artificial neural network;algorithm;hybrid system;emergence	DB	-27.63173856794255	-5.558174511168594	197338
153a1c800097c8f8a4446248952f05f69788d119	improving index performance through prefetching	topology;linkages among documents;information retrieval;data management;data bases;metasearch;distributed collection;indexation;optimization;experimental evaluation;preprocessing;memory devices;data transfer	This paper proposes and evaluate Prefetching B+-Trees (pB+-Trees), which use prefetching to accelerate two important operations on B+-Tree indices: searches and range scans. To accelerate searches, pB+-Trees use prefetching to effectively create wider nodes than the natural data transfer size: e.g., eight vs. one cache lines or disk pages. These wider nodes reduce the height of the B+-Tree, thereby decreasing the number of expensive misses when going from parent to child without significantly increasing the cost of fetching a given node. Our results show that this technique speeds up search and update times by a factor of 1.21-1.5 for main-memory B+-Trees. In addition, it outperforms and is complementary to “Cache-Sensitive B+-Trees.” To accelerate range scans, pB+-Trees provide arrays of pointers to their leaf nodes. These allow the pB+-Tree to prefetch arbitrarily far ahead, even for nonclustered indices, thereby hiding the normally expensive cache misses associated with traversing the leaves within the range. Our results show that this technique yields over a sixfold speedup on range scans of 1000+ keys. Although our experimental evaluation focuses on main memory databases, the techniques that we propose are also applicable to hiding disk latency.	b+ tree;cpu cache;cache (computing);computer data storage;database;link prefetching;pointer (computer programming);speedup;tree (data structure)	Shimin Chen;Phillip B. Gibbons;Todd C. Mowry	2001		10.1145/375663.375688	data management;computer science;theoretical computer science;data mining;database;preprocessor	DB	-28.239116094052086	1.6602345853064093	197387
