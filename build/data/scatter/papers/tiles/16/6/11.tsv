id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
d7c5d8905e62eb7c18844222d797e67dca71285d	efficient dynamic load balancing for association rule mining	dynamic load balancing;association rule mining		association rule learning	Hyun-min Kim;Jihye Kim;Chang Wook Ahn;Rudrapatna S. Ramakrishna	2002			association rule learning;computer science	DB	-27.69404192901204	47.53694760867037	42348
1bba4cc6df083d312e2265005c844ae8be92d15a	dynamo: design, implementation, and evaluation of cooperative persistent object management in a local area network	persistent object management;cooperative caching;system architecture	"""SUMMARY In light of advances in processor and networking technology, especially the emergence of network attached disks, the traditional client-server architecture of persistent storage systems has become suboptimal for many computation/data intensive applications. In this paper, we introduce a revised architecture for persistent object management employing network attached storage: the dynamic object server environment (Dynamo). Dynamo introduces two main architectural innovations:(1) To provide high scalability, the object management functions are mainly performed cooperatively by the clients in the system. Furthermore, data is transferred directly to the client's cache from network-attached disks, thus avoiding copies from a disk to the server buuer and then over the network to the client. Other systems have an a priori assignment of \object management"""" functions to speciic nodes. (2) Dynamo uses a cooperative cache management which employs a decentralized lottery-based page replacement strategy with a novel technique which attempts to put a page replaced from one node into the cache of another node which is likely to access the page in the near future. We show via performance benchmarks run on the Dynamo system and simulation results how this architecture increases the system's adaptability, scalability and cost performance. Introduction High speed local area networks and increasingly powerful desktop machines have led to the notion of clusters of PCs as servers. But servers, whether they be clusters or mainframes are conngured a priori and, with growing workloads or in the case of workloads with \\oating skew"""", a server can become a bottleneck. One approach to more scalable architecture is to use the client platforms to implement a cluster server; so each client executes not only the usual client side software but also, part of its resources are used to implement one node of a cluster server. The motivation is clearly that the \server resources"""" grow in direct proportion to the number of clients which leads to a scalable system. There are also challenges to be overcome to make this approach work in practice. One challenge is to build a system that adapts to a changing connguration. Client machines may fail but also, new machines may be added or decommissioned and the system needs to automatically adapt to these changes. In addition, it is becoming more common for portable machines to be connected in the morning and disconnected in the evening. Particularly as this latter mode of operation becomes more popular"""	block cipher mode of operation;cpu cache;client-side;client–server model;computation;computer cluster;data-intensive computing;desktop computer;emergence;mainframe computer;network-attached storage;page replacement algorithm;paging;persistence (computer science);scalability;server (computing);simulation	Jiong Yang;Wei Wang;Silvia Nittel;Richard R. Muntz;Vince Busam	2000	Softw., Pract. Exper.	10.1002/(SICI)1097-024X(20000410)30:4%3C419::AID-SPE305%3E3.0.CO;2-3	local area network;real-time computing;simulation;computer science;operating system;distributed computing;client–server model;systems architecture	OS	-21.604037357205513	51.05133992864758	42502
b5a4ea0571597b594206cef643bf42abb41834d0	unified hardware abstraction layer with device masquerade		Device drivers are a major concern for existing and new operating systems (OSs) in terms of the development cost and code quality. Unfortunately, the OS-dependent nature of device drivers makes their reuse or unification complicated because the execution environments of device drivers are tightly coupled with the hosting OS kernel. Previous studies of porting device drivers from major OSs suffer from various conflicts and engineering cost, and unmodified reuse of device drivers with virtual machines (VMs) incurs non-negligible overhead. In this paper, we present the design and implementation of a unified hardware abstraction layer that uses a thin hypervisor, on which an OS kernel runs with a single device driver for each device class, thereby reducing the development costs of device drivers for OSs that run on bare-metal machines. Our key technique is device masquerade; rather than virtualizing devices with fat software layers, a thin hypervisor converts physical devices into standardized abstract devices with minimum efforts. We exploit a de facto standard interface to hardware devices that allows clean separation of the abstraction layer implementation from OS kernels and easy deployment in practical use. To reduce virtualization overhead, the hypervisor supports only a single VM and allows pass-through access to already standardized devices such as interrupt controllers. The experimental results confirmed that the performance of our system was comparable to that on a bare-metal machine without any hypervisors.	abstraction layer;bare machine;byte;device driver;gigabit;hardware abstraction;hypervisor;linux;msi protocol;mathematical optimization;operating system;overhead (computing);performance tuning;prototype;software deployment;software quality;stack machine;unification (computer science);virtual machine;zero-copy	Iori Yoneji;Takaaki Fukai;Takahiro Shinagawa;Kazuhiko Kato	2018		10.1145/3167132.3167250	porting;interrupt;virtualization;real-time computing;software deployment;hypervisor;hardware abstraction;abstraction layer;virtual machine;computer science	OS	-26.292254146617342	50.780473603236466	42556
a92619e61af82fea85a3ae72fb6ae6012a131780	an optimal algorithm for testing for safety and detecting deadlocks in locked transaction systems	optimal solution;concurrency control;optimal algorithm	Various notions are introduced for the closure of a set of rectilinearly oriented rectangles on the plane. Optimal algorithms are presented for computing the closures. These algorithms imply optimal solutions to several problems related to database concurrency control.	algorithm;closure (computer programming);concurrency (computer science);concurrency control;deadlock;sensor	Eljas Soisalon-Soininen;Derick Wood	1982		10.1145/588111.588130	real-time computing;computer science;theoretical computer science;concurrency control;database;distributed computing	DB	-23.244111650447124	47.370184168117184	42559
630c8a99829ee2252996ee411937b9e563316654	understanding application-level caching in web applications: a comprehensive introduction and survey of state-of-the-art approaches		A new form of caching, namely application-level caching, has been recently employed in web applications to improve their performance and increase scalability. It consists of the insertion of caching logic into the application base code to temporarily store processed content in memory and then decrease the response time of web requests by reusing this content. However, caching at this level demands knowledge of the domain and application specificities to achieve caching benefits, given that this information supports decisions such as what and when to cache content. Developers thus must manually manage the cache, possibly with the help of existing libraries and frameworks. Given the increasing popularity of application-level caching, we thus provide a survey of approaches proposed in this context. We provide a comprehensive introduction to web caching and application-level caching, and present state-of-the-art work on designing, implementing, and managing application-level caching. Our focus is not only on static solutions but also approaches that adaptively adjust caching solutions to avoid the gradual performance decay that caching can suffer over time. This survey can be used as a start point for researchers and developers, who aim to improve application-level caching or need guidance in designing application-level caching solutions, possibly with humans out-of-the-loop.	cache (computing);library (computing);response time (technology);scalability;web application;web cache	Jhonny Mertz;Ingrid Nunes	2017	ACM Comput. Surv.	10.1145/3145813	web application;data mining;cache;scalability;computer science;reuse;distributed computing;response time	Web+IR	-22.506975558818542	54.25300611166862	42562
2fadeb44dcc5b4b73a7233fac4bac7c9caeee612	towards a cross platform cloud api - components for cloud federation		Cross platform APIs for cloud computing are emerg ing due to the need of the application developer to combine the features exposed by different cloud providers and to por t the codes from one provider environment to another. Such APIs are allowing nowadays the federation of c louds to an infrastructure level, requiring a certain knowledge of programming the infrastructure. We de scribe a new approach for a cross platform API that encompass all cloud service levels. We expect that the i mplementation of this approach will offer a higher degree of portability and vendor independence for Cloud bas ed pplications.	application programming interface;cloud computing;code;power-on reset	Dana Petcu;Ciprian Craciun;Massimiliano Rak	2011			distributed computing;computer science;database;vendor;cloud computing;software portability;cross-platform	SE	-30.46498176824456	54.33866529389058	42596
68a386e6011c2ed2b6ee43d4a37cfab53e1d48a6	disaggregated operating system		Recently, there is an emerging trend to move towards a disaggregated hardware architecture that breaks monolithic servers into independent hardware components that are connected to a fast, scalable network [1, 2]. The disaggregated architecture offers several benefits over traditional monolithic server model, including better resource utilization, ease of hardware deployment, and support for heterogeneity. Our vision of the future disaggregated architecture is that each component will have its own controller to manage its hardware and can communicate with other components through a fast network.	operating system;scalability;server (computing);software deployment	Yizhou Shan;Sumukh Hallymysore;Yutong Huang;Yilun Chen;Yiying Zhang	2017		10.1145/3127479.3131617	real-time computing;scalability;architecture;computer science;software deployment;embedded system;control theory;server;hardware architecture	Arch	-28.282249649420933	57.23757774509098	42713
fefb8d1fbaf5ec16c45250b18acfa62fab119f97	scammdb: facing challenge of mass data processing with mmdb	data transmission;data processing;test bed;data distribution;peer to peer;open source	Main memory database(MMDB) has much higher performance than disk resident database(DRDB), but the architecture of hardware limits the scalability of memory capacity. In OLAP applications, comparing with data volume, main memory capacity is not big enough and it is hard to extend. In this paper, ScaMMDB prototype is proposed towards the scalability of MMDB. A multi-node structure is established to enable system to adjust total main memory capacity dynamically when new nodes enter the system or some nodes leave the system. ScaMMDB is based on open source MonetDB which is a typical column storage model MMDB, column data transmission module, column data distribution module and query execution plan re-writing module are developed directly in MonetDB. Any node in ScaMMDB can response user's requirements and SQL statements are transformed automatically into extended column operating commands including local commands and remote call commands. Operation upon certain column is pushed into the node where column is stored, current node acts as temporarily mediator to call remote commands and assembles the results of each column operations. ScaMMDB is a test bed for scalability of MMDB, it can extend to MMDB cluster, MMDB replication server, even peer-to-peer OLAP server for further applications.		Yansong Zhang;Yanqin Xiao;Zhanwei Wang;Xiaodong Ji;Yunkui Huang;Shan Wang	2009		10.1007/978-3-642-03996-6_1	real-time computing;data processing;computer science;operating system;data mining;database;distributed computing;world wide web;data transmission;testbed	DB	-20.515509981615303	51.919049161391314	42757
be34daacabe24ece9dbda74bc476937d272d4769	on the apparent continuity of processing in a paging environment	working ensemble;demand paging multiprogramming paging storage hierarchies time sharing working ensemble;time sharing;storage hierarchies;multiprogramming;paging;high speed;demand paging	"""The page-faulting rate is one of the important criteria measuring the performance of a virtual storage under demand paging. In a multiprogrammed and/or time-sharing environment, a job is not expected to be run from the beginning to the end without interruption. In effect, it is segmented into phases and run piecewise. When running in this """"segmented mode,"""" a job phase upon initiation will most likely not possess the storage contents left by its preceding job phase (from the same job). Further, a job running in a paging environment at time t has a nonzero probability of referencing information brought to the high-speed store before t. Hence, for a given job, the segmented mode will yield a higher page faulting rate than the """"continuous mode"""" in which the job runs continuously from the beginning to the end. By saving and retrieving its """"working ensemble"""" upon the termination and initiation of a job phase, a job can be made to possess an """"apparent continuity"""" insofar as the storage activity is concerned. The performance of the """"apparently continuous"""" mode is hoped to approximate that of the continuous mode with an insignificant amount of overhead. Based on this observation, a storage organization oriented toward the minimization of overhead, is proposed. Simulation has been done for verification."""	approximation algorithm;c date and time functions;interrupt;job scheduler;overhead (computing);page fault;paging;scott continuity;simulation;time-sharing	Chin Tung	1970	IEEE Transactions on Computers	10.1109/T-C.1970.222831	demand paging;embedded system;parallel computing;real-time computing;simulation;computer multitasking;computer science;operating system;job stream;distributed computing;time-sharing;paging	HPC	-19.66692576334283	50.51996053875834	42945
056279f491cbd82b7449736eb3e7bc0d95ed6bf5	migrating medical communications software to a multi-tenant cloud environment	medical computing;ibcn;technology and engineering;windows azure public cloud environment medical communication software migration multitenant cloud environment cloud computing legacy applications net based medical communications application;databases cloud computing computer architecture servers hospitals security;medical computing cloud computing;cloud computing	The rise of cloud computing has paved the way for many new applications. Many of these new cloud applications are also multi-tenant, ensuring multiple end users can make use of the same application instance. While these technologies make it possible to create many new applications, many legacy applications can also benefit from the added flexibility and cost-savings of cloud computing and multi-tenancy. In this paper, we describe the steps required to migrate a .NET-based medical communications application to the Windows Azure public cloud environment, and the steps required to add multi-tenancy to the application. We then discuss the advantages and disadvantages of our migration approach. We found that the migration to the cloud itself requires only a limited amount of changes to the application, but that this also limited the benefits, as individual instances would only be partially used. Adding multi-tenancy requires more changes, but when this is done, it has the potential to greatly reduce the cost of running the application.	cloud computing;cloud storage;microsoft azure;microsoft windows;multitenancy;software as a service	Pieter-Jan Maenhaut;Hendrik Moens;Marino Verheye;Piet Verhoeve;Stefan Walraven;Eddy Truyen;Wouter Joosen;Veerle Ongenae;Filip De Turck	2013	2013 IFIP/IEEE International Symposium on Integrated Network Management (IM 2013)		cloud computing security;single-chip cloud computer;cloud computing;computer science;operating system;cloud testing;distributed computing;utility computing;world wide web;computer security	Arch	-30.786514340321883	54.84451478589776	43296
555cdac07beda0c9258873bb6d772ae063656240	avoiding disruptive failovers in transaction processing systems with multiple active nodes	high availability;total ordering algorithm;computer driven trading;fault tolerance;transaction processing	We present a highly available system for environments such as stock trading, where high request rates and low latency requirements dictate that service disruption on the order of seconds in length can be unacceptable. After a node failure, our system avoids delays in processing due to detecting the failure or transferring control to a back-up node. We achieve this by using multiple primary nodes which process transactions concurrently as peers. If a primary node fails, the remaining primaries continue executing without being delayed at all by the failed primary. Nodes agree on a total ordering for processing requests with a novel low overhead wait-free algorithm that utilizes a small amount of shared memory accessible to the nodes and a simple compare-and-swap like protocol which allows the system to progress at the speed of the fastest node. We have implemented our system on an IBM z990 zSeries eServer mainframe and show experimentally that our system performs well and can transparently handle node failures without causing delays to transaction processing. The efficient implementation of our algorithm for ordering transactions is a critically important factor in achieving good performance. © 2013 Elsevier Inc. All rights reserved.	backup;compare-and-swap;denial-of-service attack;experiment;fastest;ibm system z;mainframe computer;non-blocking algorithm;overhead (computing);paging;requirement;sensor;shared memory;transaction processing system	Gong Su;Arun Iyengar	2013	J. Parallel Distrib. Comput.	10.1016/j.jpdc.2013.01.007	embedded system;fault tolerance;parallel computing;real-time computing;transaction processing;telecommunications;computer science;operating system;distributed computing;high availability;computer network	OS	-20.465909154471625	50.3311369988931	43352
606cdc3734738b9eb6a763fa580b51251cea65dc	causalspartanx: causal consistency and non-blocking read-only transactions		Causal consistency is an intermediate consistency model that can be achieved together with high availability and performance requirements even in presence of network partitions. In the context of partitioned data stores, it has been shown that implicit dependency tracking using timestamps is more efficient than explicit dependency tracking. Existing time-based solutions depend on monotonic psychical clocks that are closely synchronized. These requirements make current protocols vulnerable to clock anomalies. In this paper, we propose a new time-based algorithm, CausalSpartanX, that instead of physical clocks, utilizes Hybrid Logical Clocks (HLCs). We show that using HLCs, without any overhead, we make the system robust on physical clock anomalies. This improvement is more significant in the context of query amplification, where a single query results in multiple GET/PUT operations. We also show that CausalSpartanX decreases the visibility latency for a given data item compared with existing time-based approaches. In turn, this reduces the completion time of collaborative applications where two clients accessing two different replicas edit same items of the data store. CausalSpartanX also provides causally consistent distributed read-only transactions. CausalSpartanX read-only transactions are non-blocking and require only one round of communication between the client and the servers. Also, the slowdowns of partitions that are unrelated to a transaction do not affect the performance of the transaction. Like previous protocols, CausalSpartanX assumes that a given client does not access more than one replica. We show that in presence of network partitions, this assumption (made in several other works) is essential if one were to provide causal consistency as well as immediate availability to local updates.		Mohammad Roohitavaf;Murat Demirbas;Sandeep Kulkarni	2018	CoRR			DB	-21.824307768217274	49.24818704213619	43414
11c7788b2097b26262c1c59720e63970f1fe7ad4	automatically wrapping legacy software into services: a grid case study	p2p system;legacy software;computational grid;service orientation;p2p;large scale;content distribution;service oriented computing;resource sharing;file sharing;computer animation;peer to peer;grid system	The computational grid is rapidly evolving into a service-oriented computing infrastructure that facilitates resource sharing for solving large-scale data and computationally intensive problems. Peer-to-peer (P2P) systems have emerged as an infrastructure enabling technologies for enhanced scalability and reliability in file sharing and content distribution. It is envisioned that P2P enabled service-oriented grid systems would virtualize various resources as services with high scalability and reliability. Many legacy software resources exist nowadays, but making them grid aware services for effective resource sharing has become an issue of vital importance. This paper presents GSLab, a toolkit for automatically wrapping legacy software into services that can be published, discovered and reused in grid environments. GSLab employs Sun Grid Engine (SGE) to enhance its performance in execution of wrapped services. Using GSLab, we have automatically wrapped a legacy computer animation rendering code written in C as a service that can be discovered and accessed in a SGE environment. The evaluation results show that the performance of GSLab improves with an increase in the number of computing nodes involved.	legacy system;wrapping (graphics)	Maozhen Li;Bin Yu;Man Qi;Nick Antonopoulos	2008	Peer-to-Peer Networking and Applications	10.1007/s12083-008-0011-9	shared resource;semantic grid;computer science;operating system;service-oriented architecture;peer-to-peer;database;distributed computing;computer animation;world wide web;computer security;legacy system;file sharing;grid computing;computer network	HPC	-31.230639158332988	52.4244616303674	43484
bf23b85bf6b23166cf77df3e7e209b38da5e2947	failure-aware checkpointing in fine-grained cycle sharing systems	fault tolerant;cycle sharing;checkpointing;fault tolerance;performance reliability;cycle sharing systems;resource availability	Fine-Grained Cycle Sharing (FGCS) systems aim at utilizing the large amountof idle computational resources available on the Internet. Such systems allow guest jobs to run on a host if they do not significantly impact the local users of the host. Since the hosts are typically provided voluntarily, their availability fluctuates greatly. To provide fault tolerance to guest jobs without adding significant computational overhead, we propose failure-aware checkpointing techniques that apply the knowledge of resource availability to select checkpoint repositories and to determine checkpoint intervals. We present the schemes of selecting reliable and efficient repositories from the non-dedicated hosts that contribute their disk storage. These schemes are formulated as 0/1 programming problems to optimize the network overhead of transferring checkpoints and the work lost due to unavailability of a storage host when needed to recover a guest job. We determine the checkpoint interval by comparing the cost of checkpointing immediately and the cost of delaying that to a later time, which is a function of the resource availability. We evaluate these techniques on an FGCS system called iShare, using trace-based simulation. The results show that they achieve better application performance than the prevalent methods which use checkpointing with a fixed periodicity on dedicated checkpoint servers.	application checkpointing;computational resource;disk storage;fault tolerance;heuristic;internet;overhead (computing);quasiperiodicity;software repository;testbed;trace-based simulation;transaction processing system;unavailability	Xiaojuan Ren;Rudolf Eigenmann;Saurabh Bagchi	2007		10.1145/1272366.1272372	fault tolerance;parallel computing;real-time computing;computer science;operating system;distributed computing;computer security	HPC	-22.156371976489336	53.662902143489376	43507
cc88683e8232880a8bccfdb43c5e1a496a0f50f3	a covert timing channel-free optimistic concurrency control scheme for multilevel secure database management systems	mandatory access control;controle acces;tratamiento transaccion;entrada salida;base donnee;mise a jour;sistema temporizado;control inteligente;securite informatique;timed system;database;base dato;sistema n niveles;intelligence artificielle;satisfiability;intelligent control;input output;actualizacion;computer security;refinement method;serialisabilidad;systeme n niveaux;seguridad informatica;retard;concurrency control;multilevel system;serializability;version management;optimistic concurrency control;systeme temporise;serialisabilite;deadlock;interbloqueo;artificial intelligence;commande intelligente;controle concurrence;access control;inteligencia artificial;control concurrencia;systeme gestion base donnee;interblocage;methode raffinement;transaction processing;retraso;sistema gestion base datos;database management system;metodo afinamiento;multilevel security;gestion version;updating;traitement transaction;reading and writing;entree sortie	This paper presents a set of multilevel-secure optimistic concurrency control (MLS/OCC) scheme that has several desirable properties: If lower-level transactions were somehow allowed to continue with its execution in spite of the conflict of high-level transactions, covert timing channel-freeness would be satisfied. This sort of optimistic approach for conflict insensitiveness and the properties of non-blocking and deadlock freedom make the optimistic concurrency control scheme especially attractive to multilevel-secure transaction processing. Unlike pessimistic approaches, the MLS/OCC scheme never delays or rejects an operation submitted by a lower-level transaction which is passed the mandatory access control. Instead, the read and write operations are processed freely without updating the actual database. Therefore, it is reasonable to assert that MLS/OCC scheme is allowed to avoid the abort of lower-level transactions in order to close covert timing channel, nevertheless guaranteeing conflict-preserving serializability. The basic refinement philosophy for the solution on starvation problem is an incorporation of multiple versions of low-level datainto MLS/OCC. This kind of intelligent channel-free concurrency control scheme satisfies the B3 or higher level of the US TCSEC requirements.	management system;multilevel security;optimistic concurrency control;timing channel	Sukhoon Kang;Yong-Rak Choi	2005		10.1007/11553939_45	input/output;optimistic concurrency control;transaction processing;computer science;access control;deadlock;concurrency control;database;distributed computing;serializability;computer security;intelligent control;satisfiability	DB	-23.722344395256748	46.90264899368849	43792
34aca22a99c10bd3bf83ce1608e754e9b5b86855	application of snapshot isolation protocol to concurrent processing of long transactions	long transaction;concurrent;snapshot isolation;processing;isolation;bepress selected works;snapshot;long;transaction processing;application;transactions;concurrent process;application snapshot isolation protocol concurrent processing long transactions;protocol	Snapshot Isolation (SI) protocol is a database transaction processing algorithm used by some of commercial database systems to manage the concurrent executions of database transactions. SI protocol is a special case of multi-version algorithm. It avoids many of the anomalies typical for the concurrent processing of database transactions. Unfortunately, SI protocol does not guarantee correct serialization of database transactions under certain conditions. A recent work [3] proposed a formal solution, which characterizes the correctness of transactions running under SI protocoL However, the protocol is inefficient when it comes to processing long transactions. In this paper, we show that the limitations imposed on the structures of long transactions improve performance of SI protocol. A different way to characterize the serializability of schedule under SI dynamically is proposed and proved.	algorithm;communications protocol;computer multitasking;correctness (computer science);database transaction;serializability;serialization;snapshot isolation;transaction processing	Yang Yang;Janusz R. Getta	2006			real-time computing;two-phase commit protocol;database transaction;distributed transaction;computer science;database;distributed computing;online transaction processing;compensating transaction;serializability;acid;snapshot isolation	DB	-23.10465464494853	48.12516593216759	43949
e7c0970f5ed60c4a694981ddfe4e8bf07797751c	performance visualization of grid applications based on ocm-g and paraver	universiteitsbibliotheek	In grid computing where data processing takes place on many machines it is essential to have possibility to see a whole execution  workflow. It enables the user to visually assess the computing-to-communication time ratio, frequency of executed probes or  library functions, delay and volume of communication, or simply to observe how all nodes are actually working. In this paper  we introduce the Ocm2Prv tool that makes it possible to visually analyze the execution of parallel applications developed  with MPI library. The tool couples the functionality of two flexible environments - the OCM-G grid-enabled monitoring system  and performance visualization tool Paraver. Implications coming from other Grid models, especially, GCM are also addressed.  		Wlodzimierz Funika;Maciej Zientarski;Rosa M. Badia;Jesús Labarta;Marian Bubak	2008		10.1007/978-0-387-09457-1_10	grid;grid computing;workflow;visualization;distributed computing;time ratio;data processing;gcm transcription factors;computer science	HPC	-21.354270032193416	53.5883400947722	44168
54aade2baf833e0b55eddfa602dea1c069a0186f	crowdtuning: systematizing auto-tuning using predictive modeling and crowdsourcing		Software and hardware co-design and optimization of HPC systems has become intolerably complex, ad-hoc, time consuming and error prone due to enormous number of available design and optimization choices, complex interactions between all software and hardware components, and multiple strict requirements placed on performance, power consumption, size, reliability and cost. We present our novel long-term holistic and practical solution to this problem based on customizable, plugin-based, schema-free, heterogeneous, open-source Collective Mind repository and infrastructure with unified web interfaces and online advise system. This collaborative framework distributes analysis and multiobjective off-line and on-line auto-tuning of computer systems among many participants while utilizing any available smart phone, tablet, laptop, cluster or data center, and continuously observing, classifying and modeling their realistic behavior. Any unexpected behavior is analyzed using shared data mining and predictive modeling plugins or exposed to the community at cTuning.org for collaborative explanation, top-down complexity reduction, incremental problem decomposition and detection of correlating program, architecture or run-time properties (features). Gradually increasing optimization knowledge helps to continuously improve optimization heuristics of any compiler, predict optimizations for new programs or suggest efficient run-time (online) tuning and adaptation strategies depending on end-user requirements. We decided to share all our past research artifacts including hundreds of codelets, numerical applications, data sets, models, universal experimental analysis and auto-tuning pipelines, self-tuning machine learning based meta compiler, and unified statistical analysis and machine learning plugins in a public repository to initiate systematic, reproducible and collaborative R&D with a new publication model where experiments and techniques are validated, ranked and improved by the community.	cognitive dimensions of notations;compiler;computer cluster;crowdsourcing;data center;data mining;experiment;heuristic (computer science);hoc (programming language);holism;interaction;laptop;machine learning;mathematical optimization;numerical analysis;online and offline;open-source software;pipeline (computing);plug-in (computing);predictive modelling;reduction (complexity);requirement;self-tuning;smartphone;tablet computer;top-down and bottom-up design;user interface;user requirements document	Abdul Wahid Memon;Grigori Fursin	2013		10.3233/978-1-61499-381-0-656	theoretical computer science;software;laptop;compiler;parallel computing;data center;heuristics;computer science;plug-in;ranking;crowdsourcing	ML	-19.493428059950485	56.84424522547154	44181
1e265951d2d2715b3f77d99efd2f7180576e5928	mobile agent platforms for web databases: a qualitative and quantitative assessment	distributed computing models;employment;information resources;distributed database;performance evaluation;web databases;mobile agents;aglets;distributed computing;software performance evaluation;software agents;concordia;computational modeling;software performance evaluation distributed programming software agents information resources distributed databases java internet;internet;mitsubishi;distributed programming;performance analysis;distributed databases;world wide web;robustness;computer science;mobile agent;mobile agents distributed computing distributed databases performance analysis computational modeling java internet computer science employment robustness;performance evaluation mobile agent web databases java ibm aglets mitsubishi concordia distributed computing models distributed database world wide web;java;ibm	In this paper we present practical experiences gathered from the employment of two popular Java-based mobile-agent platforms, IBM's Aglets and Mitsubishi's Concordia. We present some basic distributed computing models and describe their adaptation to the mobileagent paradigm. Upon these models we develop a set of frameworks for distributed database access over the World-Wide Web, using IBM's Aglets and Mitsubishi's Concordia platforms. We compare the two platforms both quantitatively and qualitatively. For the quantitative comparison, we propose, employ, and validate an approach to evaluate and analyze mobile-agent framework performance. For the qualitative assessment, we present our observations about the programmability and robustness of, and mobility provided by, the two plat-	aglets;benchmark (computing);distributed computing;distributed database;java;mobile agent;naruto shippuden: clash of ninja revolution 3;programming paradigm;world wide web	George Samaras;Marios D. Dikaiakos;Constantinos Spyrou;Andreas Liverdos	1999		10.1109/ASAMA.1999.805393	the internet;software agent;mobile agent;java;computational model;distributed database;ibm;robustness	Web+IR	-33.240459768127764	47.00971814190094	44247
1f21c3d03726264e48d156f0430ec6357d6fa642	perturbation analysis of database queries		We present a system, Perada, for parallel perturbation analysis of database queries. Perturbation analysis considers the results of a query evaluated with (a typically large number of) different parameter settings, to help discover leads and evaluate claims from data. Perada simplifies the development of general, ad hoc perturbation analysis by providing a flexible API to support a variety of optimizations such as grouping, memoization, and pruning; by automatically optimizing performance through run-time observation, learning, and adaptation; and by hiding the complexity of concurrency and failures from its developers. We demonstrate Perada’s efficacy and efficiency with real workloads applying perturbation analysis to computational journalism.	application programming interface;computation;computational journalism;concurrency (computer science);database;hoc (programming language);mathematical optimization;memoization;performance tuning;perturbation theory;usability	Brett Walenz;Jun Yang	2016	PVLDB	10.14778/3007328.3007330	computer science;theoretical computer science;data mining;database;programming language	DB	-23.20494850072169	52.328506267679685	44633
97d03a52bd04335c158a99fb678b1c91c2a13247	scalable distributed concurrency services for hierarchical locking	protocols;corba concurrency service;concurrent computing;resource allocation middleware concurrency control workstation clusters very large databases distributed object management;resource allocation;resource management;distributed computing;linux cluster;computer applications;concurrent computing scalability protocols distributed computing peer to peer computing middleware concurrency control delay computer applications resource management;very large database;large scale;distributed objects;col;very large database environment;distributed environment;peer to peer computing environment;resource sharing;distributed object management;concurrency control;application sharing;very large database environment scalable distributed concurrency service distributed environment resource sharing middleware concurrency control protocol linux cluster corba concurrency service embedded computing peer to peer computing environment;middleware concurrency control protocol;middleware;scalability;workstation clusters;very large databases;peer to peer computing;scalable distributed concurrency service;embedded computing	"""! #$ % & ' # $ ( ) # !) * + ' , . $ # #$ # / ' 0 . %1 2 '% % ! 3#$ 4/56! $ 87 ' 8 9 ;:<#' = !>+ # 3? ' # @ 3 A !&+A 6B 8 #$ C! $ @ 1 '!>+ BD E F % %1#$ + ) & ' !&+ ! ' G 3#-!&H6 , ' JIK L+ BD * )7 !> 3 1 @ NM O'PQ4SR/# IK BD > T + U '% % ! 3#$ V $! !) W + + ' . ) # !) X +0 BD Y $ / %1#$ Z J [8 )7 ) 6 Y #,:\ !> 3#$ ]! #' !) 8 -4E^"""" B6 3#$ _ $% % #D '!>+ _:\#$ _ ! $ 87 ' `! # ! ) !) a ) B6 ! ( #J #$ ( G + G% #$ b * + $ c#$:d ' 87e Z '% % ! 3#$ 4 f + 2Ig#$ H.!)# ) / E ' 0 . % ) 6 & # , ) & $ #':G * #'B h%1 ) 7Q #$7e%1 ) K% #$ #-!)# i:\#$ / X! # ! !) > B ! ) CI` 8 +W % %1#' F:<#$ C ' 7Q ] 4j E% #' #$7 !)# ` $ ? #$ ! F 8 & ' #$ k + #$ +l+ ) & !>+ ! $ 2 #-!>H-7 m 8 L *+ 3 $+ 8 ] ! $ 3 $ 3 T:<#' 3 ' ] _ $ 0 8 . 1# T% #$ 8 n o B ) # i4;p q $ 6 + C% # 3 ) r#': % #$ 8 9 s B ) # b t % %1#' k nIK#o#$:* + u # C% # 87 . + ) 8 & $ ! E% #' #-! # vl^(wKx(^y ' 0 o^Gz ^d4{x""""| %1 ) !)# } ~ + L ! $ 3 $ 3 8 9 #$:{#$ a% #' #-! # ( ]! # ' !) # qI` + 8 , % %1#' .:<#' F% #' 4^"""" #$ 8 ?  [F + # I` #T 8 Y q% 3!> & $ * $ 0 E 1# 0 , %1# * 4*5 % 7 %1#' {#$:^(wKx(^= 1# 0 m% #' n X B ) # X K X L !) g:< $ #-!>H6 4gpo+ 3 Y + J^mz9^k # #$ E #2 #' / h ) `:\ #$ c:< $ #-!>H6 8 ( 1# (# X% #$ 8 n B ) # T 3 ! 0 $ 8 # $ ) $ =# BD ) + h4 FIK#' HW %0 '!) , $ 87Q = $% % ! '7 #$  + ' { #$ ! ) """" $!) #$ d 3 ' $ g 3 J 6B6 8 # & ' $ 3 Y:\ # ;+ 3 > & ' !&+ ! $ 3#-!&H6 Y * ' 7Q 2 & $ 0 ' ( $ 0 ' & ' 0 $ 2 & $ $!) # K #a L# 6 !) K 6B 8 #$ m 3 3 )7e ! ' 3 a _ 1 T 4 1. Scalable Hierarchical Locking p /} ` 3 #6 ! /# ` $ J% #$ #-! #$  #a% # B6 3 J+ ) & ' !&+ 87 ! $ 0 #-!&H a X Y 3 L 6B6 8 # `M O Ph $ * + * )|6 0 8 G Y % %1#$ """":\#$ G% 3#' 8 9 a[4""""56 i ) & $ 8 _ [6 8 + 2 !) # 1 > 9IK ) #-!>HL #6 """" #Y !) ' / + {%1#$ 3 $ :\#$ X! #$ ! !> -4V.# X 3 += tJZ #-!>H6 ' 0 FI{ 8 ,<pt/ #-!&H JI` 8 +q%0 ' & ' 3 ) g ' 0 E )| ! BD L '! ! i )7 %1 )!) BD 8 -4""""2 _ % $ & $ /Yi #-!&HZ )% G ' _ )| ! 3B g #-!&HF +0 ' J:<# # IK q 6 F X [J:<#' Z% #$ #$ # F #, I{ 8 #-!&H14G % ' & 2 #-!&H6 K BD# 3 L! #$  3!> g 1 ) nIK L + m + ' ( 7 ) 0 _ #Y #6 : .<I{ 8 G# 6 !> K ': ) (+0 B6 3 Y+ 3 L L #-!>H14 R/ ) & ' !&+ ! ' #-!&H / !&+ ) d + $ ! (% ' & ' 3 ; $ Z !) #-!&H6 3 *# BD ) + X 3 + 3 * #-!&H #6 ( 0 $ T#$ T + !) `#': ' & % & ' #$ i4mxG 6 8 ( ' ( + {+ $+ ) ( BD #':h $ & ' 3 ' 8 9 * ' 2 $ #-!) LI` 8 +L 6 ) 6 #$ LBD ) # `z9V $ 0 zeptg#$:"""" + Y ':\#$ ) 6 #$ . #-!&H #6 Ys $ 0 Tpt>4 zn U# T% #$ #-!)#  #6 :\#$  ] # ! ' E !> F X ' 6 & ' + ) / #-! $ """"% ' /%1# ) 4 f + Z #-#$ #6 Z+ $ + m #$HD Z ' 0 J i ):\ ) Y #2 $ i + """" # HD Y #6 $ ' 3 -#' + )  #6 a >:< ) . #T ' / # 7Q #$HD , #6 4{p Y ) , !> 3 1 Y + 2 ` 1 # IY 0 $# BD ) * + Ig#$ H6 $ /#':d + J% #$ #-! #$ v f + CIg#$ HWIK $ q % %1#' W b%0 sY5  ' & $ uw{wg`7   O     4 Xd\.  .#6 (lG ' 0 _]K ' K ' 3 a # 1 J&'  ¡ ¢$£¤Q¥ ¦3§iI` 8 + '!>+q#$ + > Z :K $ C#$ 8 C :( + ) C L #' Z ! #$ 0 !) Y $! ! #' & #L! #$ %0 3 3 8 n X 8| #': f '  ̈DQ D{ ' 0 qM ©'Pe4 a -«G¬­\® ­\ ̄1¬0C° #-!&Hu±y $ 3 = #C 1 E2&£3 $ ́6μ-§>3_ +0 ' l #-!&H ¶ 8:/ + :<#$ > L! #$ & $ * + ' T#$:/! # ! !) u# BD ) + 2 3 ' ) 4Gzn X#$ + > KIK#' & ±o K! # % ' `I` 8 +T:\ )IK ) `#' + ) #6 ) 2 + $ ¶ 4 f + Z#' & ) /#$:m #-!>H. ' + 2 3 )} 6 + /:<# # I` T 3 [0 3#$ v ·o ̧l16oW ̧VoW ̧U»o1⁄4o1 1⁄23⁄4 ̧U1⁄2   ̈  a -«G¬­\® ­\ ̄1¬ ¿  #6 {±V """" $ 3 _ #YÀ  ¦ Ág + g #-!>H_Â(Ã. 3 _ #6 ]Äu 8:1±u d 3 { `!) 8 ! $ 6 !> 3#$ *% #$ !) a J + g #-!>H1 6¤eÅ<§ Å ': ) ±s+0 ' 2 $! [8 X + Y #-!&H. ' 0 1 >:<#$ Z±t $ 2 8 4 a -«G¬­\® ­\ ̄1¬]Æ  #6 K±V G $ 3 _ # $ÇG ́/ + K #-!&H*Â Ã 3 _ #6 ]È= :G]Èl { + J #$ $ 2 #6 J 1 L+ 3 T 6 T ' #6 Y + / J #-#$ 3 . #6 Z±Z4 Xd\,¿ o #6 2 0 K J [m:<#' K + 2 #-!>H* 3 X #6  Ã #X 8 /% ' / :( ' 0 ,# 8 , 8:G + Z #6 a# I` / + Z #-!&H. F #6  È I`+ ) J È  ̧  Ã Q ' 0  È X 1 · > #$ / È $ . Ã ' X !)# %0 4E / + ) I` """" + #-! ' K! #$%6 ) a a %h $ T + Y!) 8 3! $  )!) # , 2 ) TI` 8 + # 2 ' 4 Xd\qÆ ̈ 4{É # 7Q # HD F #6 a# I` Â Ã C #6 L È ! $ E $ & ' Y [6 g:\#$ `Â(Ãq X #6 /]Ãq 8:d]È] $ T]Ã ' /! # L7 %0 ' 3 Z ' 0 . ÈlÊ  Ã 4  4 f + _ #$HD E #6 *# I` ,Â(ÃV q #6 L]ÈU! ' q ' & $ Z [/:<#$ aÂ Ã C #6 * Ã 8:{ È ' 0 C Ã L! # L7 %0 ' 3 $4 f + Y#$%1 ) & ' #$ 0 $ i% #' #-! # i %1 ! 8}0! # ,:< + ) { [6 8 v ­<¬UË$Ì0Í ]0 + [) ` 1 ! #$ 2 *!&+ 3 #$:"""" + J #6 Î ­<¬UË$Ì0Í C¿ :g #6 J ' L! #$ %0 3 L $ q 8:` È  ̧  Ã  + # HD l * & $ :< ) l #q + T [6 ) 4 f + [) 1 ! # ) d + K )I= # HD _ #6 { $ _%0 G#$:1 + K#$ $ # HD l #6 4zn:Y ÈÊ  Ã K + T [> ! BD T $ & $ _! # %a:\ # b + K # HD _ #6 ` $ 0 _ 1 ! # ) ( J!&+ 3 #$:"""" + J #$HD , #6 4 e5 Z2 Y©L:<#' / + Y! $ ZI`+ ) E È $ 0 .]Ãu ' Y !)# %0 4  f ' i4K ̈  1d )% !) """" D ' # I` * #6 m=G#$:1 # 7Q #$HD a #6 ) :<#' a $ & $ , #6 L]L $!)! #$ & 3 . #,2 LO6 d 0 ! ' q 6 + J $ ) ! Y#':m $ ,Ï 4 Xd\EÐ ̈ 4Kzn:/ F # 7Q # HD u #6 I` 8 +=%1 0 3 F [a:<#$ * #6 lY! ' #' Z $ & ' 6 a T )Ib [6 Y:<#' _]$ i Z ) + ) Y:<#' 7 I{ ' & XQGJ + * [) a #. Z% ' a#$ a #-! $ 8 u[eÑYK $! ! #' & L # f $ 3 L ̈ ! >4  4Kzn: + ` # HD X #6 2! $ #$ K $ & $ K Z [6 8 KI` 3 h[+ / [6 ` #-! $ 8 T D & `#$:"""" 8 {%1 Z [) 4 2 g©J """" % % ) 6 Z Y + g:<#$ 3 # I` Z# %1 > & ' # $ %1 ! 87 }0! ' # v ° #-! $ 8 *[J ["""" ' g! # ) Z:<#$ G ' & $ I`+ ) E + _%1 ) 0 X [6 J!)# 2 + # +E#$ J X ) 3 $ L 7 $ $ Z 3 { !) BD 4 Xd\qÒ ̈ 4gpo+ a + g #$HD _ #6 ( $ ( #-!&H_#' m )! BD """" ' :\ # b#$ {#':h G!&+ 3  D 8 m! # 3 ) """" + { #-! ' L[[K:\#$ / $ & $ * 0 ) `! #$ & $ `#$:G2 YO64 /I` T #6 Z]ÈKv Q Kz9 !)# %0  1g/#Xwg+ 3 / & $ ! {Ñ $ #' I{ ' & Q K ? X.#6 / ' f #$HD / [6 ` #6 Y]Ã zn   z p z9   z p z9   zep p z9   z p /#* #-!>H ·      z9 {2 z9 Ñ     z9Z a a zep 2 X  Ñ    Z  zna Z  /% ' &    Ñ Ñ Ñ  z9Z  zn Kp 8 Xzep    Ñ  znp znp z9Z znp"""	binary prefix;blu-ray;lock (computer science);mpeg transport stream;scalability;shebang (unix);signed zero;spherical basis	Nirmit Desai;Frank Mueller	2003		10.1109/ICDCS.2003.1203504	shared resource;timestamp-based concurrency control;communications protocol;optimistic concurrency control;real-time computing;scalability;isolation;concurrent computing;computer cluster;resource allocation;computer science;resource management;operating system;concurrency control;middleware;database;distributed computing;multiversion concurrency control;computer applications;non-lock concurrency control;serializability;concurrent object-oriented programming;distributed computing environment;very large database;distributed concurrency control	Theory	-29.63147745245285	47.048213985947974	44661
9633a15d7d3796b9a1b411889a8a4551fce35cd1	online analysis and runtime steering of dynamic workflows in the askalon grid environment	runtime environment performance analysis distributed computing grid computing contracts quality of service fuzzy logic optimization methods chemistry graphics;material chemistry;runtime optimisations;runtime steering;runtime environment;distributed computing;software performance evaluation;contracts;formal method;event correlation;fuzzy logic;dynamic environment;online analysis;chemistry;workflow management software fuzzy logic grid computing software performance evaluation;performance analysis;event correlation technique;workflow management software;dynamic workflows;online analysis dynamic workflows askalon grid environment distributed performance analysis event correlation technique fuzzy logic rescheduling runtime optimisations material chemistry graphics rendering runtime steering;rescheduling;quality of service;graphics rendering;grid computing;graphics;askalon grid environment;distributed performance analysis;optimization methods	We present a new distributed performance analysis service of the ASKALON integrated Grid environment for computing runtime overheads of dynamic workflows in realtime based on event correlation techniques. We illustrate a formal method to express precise overhead correlation rules, including several performance contracts as quality of service parameters based on fuzzy logic to be enforced in dynamic environments though rescheduling, various runtime optimisations, and steering techniques. We demonstrate experimental results for two real applications from material chemistry and graphics rendering domains.	computer cluster;data parallelism;event correlation;event monitoring;extensibility;formal methods;fuzzy logic;high- and low-level;overhead (computing);production (computer science);quality of service;real-time clock;rendering (computer graphics);scalability	Radu Prodan	2007	Seventh IEEE International Symposium on Cluster Computing and the Grid (CCGrid '07)	10.1109/CCGRID.2007.76	fuzzy logic;real-time computing;formal methods;quality of service;computer science;graphics;event correlation;theoretical computer science;operating system;distributed computing;grid computing	HPC	-26.385940385123465	58.043775151732014	44696
099b157a8a26fc004e019a97e21178983ddc2461	cost-based optimization of integration flows		Integration flows are increasingly used to specify and execute data-intensive integration tasks between several heterogeneous systems and applications. There are many different application areas such as (near) real-time ETL (Extraction Transformation Loading) and data synchronization between operational systems. For the reasons of (1) an increasing amount of data, (2) typically highly distributed IT infrastructures, and (3) high requirements for data consistency and up-to-dateness, many instances of integration flows—with rather small amounts of data per instance—are executed over time by the central integration platform. Due to this high load as well as blocking synchronous source systems or client applications, the performance of the central integration platform is crucial for an IT infrastructure. As a result, there is a need for optimizing integration flows. Existing approaches for the optimization of integration flows tackle this problem with rule-based optimization in the form of algebraic simplifications or static rewriting decisions during deployment. Unfortunately, rule-based optimization exhibits two major drawbacks. First, we cannot exploit the full optimization potential because the decision on rewriting alternatives often depends on dynamically changing costs with regard to execution statistics such as cardinalities, selectivities and execution times. Second, there is no re-optimization over time and hence, the adaptation to changing workload characteristics is impossible. In conclusion, there is a need for adaptive cost-based optimization of integration flows. This problem of cost-based optimization of integration flows is not as straight-forward as it may appear at a first glance. The differences to optimization in traditional data management systems are manifold. First, integration flows are reactive in the sense that they process remote, partially non-accessible data that is received in the form of message streams. Thus, proactive optimization such as dedicated physical design is impossible. Second, there is also the problem of missing knowledge about data properties of external systems because, in the context of loosely coupled applications, statistics are non-accessible or do not exist at all. Third, in contrast to traditional declarative queries, integration flows are described as imperative flow specifications including both data-flow-oriented and controlflow-oriented operators. This requires awareness with regard to semantic correctness when rewriting such flows. Additionally, further integration-flow-specific transactional properties such as the serial order of messages, the cache coherency problem when interacting with external systems, and the compensation-based rollback must be taken into account when optimizing such integration flows. In conclusion, the cost-based optimization of integration flows is a hard but highly relevant problem in today’s IT infrastructures. In this thesis, we introduce the concept of cost-based optimization of integration flows that relies on incremental statistics maintenance and inter-instance plan re-optimization. As a foundation, we propose the concept of periodical re-optimization and present how to integrate such a cost-based optimizer into the system architecture of an integration platform. This includes integration-flow-specific (1) prerequisites such as the dependency analysis and a cost model for interaction-, control-flowand data-flow-oriented operators as well as (2) specific statistic maintenance strategies, optimization algorithms and optimization techniques. While this architecture was inspired by cost-based optimizers	algorithm;analysis of algorithms;blocking (computing);cache coherence;cardinality (data modeling);correctness (computer science);data synchronization;data-intensive computing;dataflow;dependence analysis;imperative programming;integration platform;interaction;logic programming;loose coupling;mathematical optimization;operational system;optimizing compiler;physical design (electronics);real-time clock;requirement;rewriting;software deployment;systems architecture;transaction processing	Matthias Boehm	2011			computer science	DB	-20.866182127516065	54.777242154752365	45020
906173ac6b09264695c62ddf9d410a9947df2fca	islet: an isolated, scalable, & lightweight environment for training	training;docker;security;containers	In this paper we present ISLET; the Isolated, Scalable, & Lightweight Environment for Training. ISLET overcomes many of the distribution, scaling and security challenges of providing mass training to students requiring an interactive GNU/Linux command-line environment. This Docker-based solution is evaluated and lessons learned from real world experience with ISLET are discussed.	command-line interface;docker;gnu;linux;scalability	Jonathan Schipp;Jeannette Dopheide;Adam J. Slagell	2015		10.1145/2792745.2792762	real-time computing;simulation;engineering;world wide web	OS	-29.75538659566122	54.64110564404833	45052
81fc87ffe7940f7a06467f3d3aaaef9b2424120e	building efficient and highly run-time adaptable virtual machines	metaobject protocols;performance;virtual machines;reflection	Programming language virtual machines (VMs) realize language semantics, enforce security properties, and execute applications efficiently. Fully Reflective Execution Environments (EEs) are VMs that additionally expose their whole structure and behavior to applications. This enables develop- ers to observe and adapt VMs at run time. However, there is a belief that reflective EEs are not viable for practical usages because such flexibility would incur a high performance overhead. To refute this belief, we built a reflective EE on top of a highly optimizing dynamic compiler. We introduced a new optimization model that, based on the conjecture that variability of low-level (EE-level) reflective behavior is low in many scenarios, mitigates the most significant sources of the performance overheads related to the reflective capabilities in the EE. Our experiments indicate that reflective EEs can reach peak performance in the order of standard VMs. Concretely, that a) if reflective mechanisms are not used the execution overhead is negligible compared to standard VMs, b) VM operations can be redefined at language-level without incurring in significant overheads, c) for several software adaptation tasks, applying the reflection at the VM level is not only lightweight in terms of engineering effort, but also competitive in terms of performance in comparison to other ad-hoc solutions.	benchmark (computing);compiler;definition;dynamic compilation;elegant degradation;emoticon;experiment;garbage collection (computer science);google sites;graal;heart rate variability;high- and low-level;hoc (programming language);indirection;just-in-time compilation;mathematical optimization;memory management;metaobject;overhead (computing);programming language;proxy server;read-only memory;run time (program lifecycle phase);spatial variability;synthetic intelligence;universal instantiation;virtual machine	Guido Chari;Diego Garbervetsky;Stefan Marr	2016		10.1145/2989225.2989234	embedded system;real-time computing;simulation;reflection;performance;computer science;virtual machine	PL	-26.61827420608014	55.82445264888418	45076
c4938e775210c3eba1cfcd8456e1290a845d8665	m-lock: accelerating distributed transactions on key-value stores through dynamic lock localization	protocols;protocols concurrency control distributed databases;distributed key value stores;protocols cloud computing concurrency control delays distributed databases system recovery concurrent computing;distributed key value stores transactions;lock localization m lock distributed transactions key value stores dynamic lock localization scalable distributed data stores transactional support entity groups multiversion concurrency control lock based protocol network overhead application oriented locks clustering transactions latency locks migration tpc c benchmark;concurrency control;distributed databases;transactions	Scalable distributed data-stores are increasingly used for storing large datasets in diverse applications. The need for transactional support in these applications has motivated several recent efforts. A common theme underlying these efforts is the creation of disjoint groups of objects (entity-groups) on which efficient local transactional support is provided using multi-version concurrency control. A lock-based protocol is used to support distributed transactions across entity-groups. A significant drawback of this scheme is that the latency of distributed transactions increases with the number of entity-groups it operates on. This is due to the commit overhead of local transactions, and network overhead due to distributed locks. We address this problem using lock-localization -- locks for distributed objects are dynamically migrated and placed in distinct entity-groups in the same datastore. This reduces the overhead of multiple local transactions while acquiring locks. Application-oriented clustering of locks in these new entity-groups leads to a decrease in network overhead. Separating locks from data in this manner, however, affects the latency of local transactions. To account for this, we propose protocols and policies for selective, adaptive, and dynamic migration of locks. Using TPC-C benchmark, we provide detailed evaluation of the system.	benchmark (computing);cluster analysis;concurrency (computer science);data store;database transaction;distributed object;distributed transaction;ibm tivoli storage productivity center;lock (computer science);multiversion concurrency control;overhead (computing);tpc-w;throughput	Naresh Rapolu;Srimat Chakradhar;Adnan Hassan;Ananth Grama	2013	2013 IEEE Sixth International Conference on Cloud Computing	10.1109/CLOUD.2013.95	lock;communications protocol;optimistic concurrency control;real-time computing;computer science;operating system;concurrency control;database;distributed computing;lock convoy;serializability;distributed database;acid;distributed concurrency control	DB	-20.85757295100976	50.9641382249809	45371
17e727c34569306682c367e718c32c7134ccb1d6	the nonkernel: a kernel designed for the cloud	fundamental shift;cloud computing platform;nonkernel operating system;machine virtualization;hypervisor involvement;cloud computing;underlying resource;cpu cycle;system kernel construction;computing resource	Infrastructure-as-a-Service (IaaS) cloud computing is causing a fundamental shift in the way computing resources are bought, sold, and used. We foresee a future whereby every CPU cycle, every memory word, and every byte of network bandwidth in the cloud would have a constantly changing market-driven price. We argue that, in such an environment, the underlying resources should be exposed directly to applications without kernel or hypervisor involvement. We propose the nonkernel, an architecture for operating system kernel construction designed for such cloud computing platforms. A nonkernel uses modern architectural support for machine virtualization to securely provide unprivileged user programs with pervasive access to the underlying resources. We motivate the need for the nonkernel, we contrast it against its predecessor the exokernel, and we outline how one could go about building a nonkernel operating system.	byte;cloud computing;exokernel;hypervisor;instruction cycle;kernel (operating system);operating system;privilege (computing)	Muli Ben-Yehuda;Omer Peleg;Orna Agmon Ben-Yehuda;Igor Smolyar;Dan Tsafrir	2013		10.1145/2500727.2500737	real-time computing;cloud computing;computer science;operating system;distributed computing	OS	-28.894953118677236	57.15647820754554	45379
a81009270c96688bf0fbfce3d5ff0cdc242fdbcb	protocols for multimedia systems		Replicating Web documents at a worldwide scale can help reduce userperceived latency and wide-area network traffic. This paper presents the design of Globule, a platform that automates all aspects of such replication: server-to-server peering negotiation, creation and destruction of replicas, selection of the most appropriate replication strategies on a per-document basis, consistency management and transparent redirection of clients to replicas. Globule is initially directed to support standard Web documents. However, it can also be applied to streamoriented documents. To facilitate the transition from a non-replicated server to a replicated one, we designed Globule as a module for the Apache Web server. Therefore, converting Web documents should require no more than compiling a new module into Apache and editing a configuration file.	compiler;inter-server;network traffic control;peering;server (computing);web page;web server	Jan van Leeuwen;Marten van Sinderen	2001		10.1007/3-540-45481-0	multimedia;computer science	Web+IR	-26.41360421270176	53.871478796676165	45437
3cef2127019c01e5dbf05e7994bf147a73da2af5	a cloud server based on i/o virtualization in hardware		With the advent of Internet services and big data, cloud computing has generated much research interest, especially on cloud servers. In view of the development of lightweight server processors, i.e., x86 single-chip processors and ARM64 processors, and the high-performance interconnect fabric, an approach building a cloud server on top of virtualized I/O is presented in this paper. Its advantage is to provide high performance/cost, performance/Watt, high-density and high scalability compared with the existing method, to better meet the demands of cloud computing.		Yang You;Gongbo Li;Xiaojun Yang;Bowen Qi;Bingzhang Wang	2015		10.1007/978-3-662-49283-3_13	embedded system;full virtualization;virtualization;virtual machine;operating system;hardware virtualization;storage virtualization;computer network	Crypto	-28.318026777028006	56.78994571476844	45652
95f7fd4522284793f260a7cd772bf567c781f9e4	cross-platform resource scheduling for spark and mapreduce on yarn	spark on yarn resource scheduling cross platform application performance reservation aware executor placement dependency aware resource adjustment locality aware task assignment;sparks resource management yarn job shop scheduling computer science processor scheduling big data	While MapReduce is inherently designed for batch and high throughput processing workloads, there is an increasing demand for non-batch processes on big data, e.g., interactive jobs, real-time queries, and stream computations. Emerging Apache Spark fills in this gap, which can run on an established Hadoop cluster and take advantages of existing HDFS. As a result, the deployment model of Spark-on-YARN is widely applied by many industry leaders. However, we identify three key challenges to deploy Spark on YARN, inflexible reservation-based resource management, inter-task dependency blind scheduling, and the locality interference between Spark and MapReduce applications. The three challenges cause inefficient resource utilization and significant performance deterioration. We propose and develop a cross-platform resource scheduling middleware, iKayak, which aims to improve the resource utilization and application performance in multi-tenant Spark-on-YARN clusters. iKayak relies on three key mechanisms: reservation-aware executor placement to avoid long waiting for resource reservation, dependency-aware resource adjustment to exploit under-utilized resource occupied by reduce tasks, and cross-platform locality-aware task assignment to coordinate locality competition between Spark and MapReduce applications. We implement iKayak in YARN. Experimental results on a testbed show that iKayak can achieve 50 percent performance improvement for Spark applications and 19 percent performance improvement for MapReduce applications, compared to two popular Spark-on-YARN deployment models, i.e., YARN-client model and YARN-cluster model.	apache hadoop;apache spark;big data;computation;interference (communication);job stream;locality of reference;mapreduce;middleware;multitenancy;real-time transcription;schedule (project management);scheduling (computing);software deployment;testbed;throughput	Dazhao Cheng;Xiaobo Zhou;Palden Lama;Jun Wu;Changjun Jiang	2017	IEEE Transactions on Computers	10.1109/TC.2017.2669964	job shop scheduling;real-time computing;computer science;parallel computing;resource management;software deployment;spark (mathematics);big data;scheduling (computing);testbed;middleware;distributed computing	OS	-20.553635798014984	60.05148271547189	45772
a9b54afb9eadd1f61ac24adfdffa51402e0d95b8	disco: a computing platform for large-scale data analytics	cluster;fault tolerant;distributed computing;data processing;distributed filesystems;large scale;design and implementation;mapreduce;task scheduling;clustered data	We describe the design and implementation of Disco, a distributed computing platform for MapReduce style computations on large-scale data. Disco is designed for operation in clusters of commodity server machines, and provides both a fault-tolerant scheduling and execution layer as well as a distributed and replicated storage layer. Disco is implemented in Erlang and Python; Erlang is used for the implementation of the core aspects of cluster monitoring, job management, task scheduling and distributed filesystem, while Python is used to implement the standard Disco library.  Disco has been used in production for several years at Nokia, to analyze tens of terabytes of data daily on a cluster of over 100 nodes. With a small but very functional codebase, it provides a free, proven, and effective component of a full-fledged data analytics stack.	clustered file system;commodity computing;computation;computer cluster;distributed computing;erlang (programming language);fault tolerance;mapreduce;python;requirement;scheduling (computing);server (computing);terabyte	Prashanth Mundkur;Ville H. Tuulos;Jared Flatow	2011		10.1145/2034654.2034670	parallel computing;computer science;operating system;database	OS	-19.684995898229058	51.653145672989034	45864
0e9d6f3748e1fd71c19051ec17c484e52731a876	deduplicated distributed file system using lightweight cryptography	encryption;computer architecture;servers;operating systems;hardware	Current user demands from a storage service increase every day in complexity and availability. Together with the expansion of Cloud Computing services we find an important problem existing in nowadays datacenters - the problem of efficient, secure and reliable storage backbone that can serve both virtual infrastructures and users. In this paper we present a different approach in the design of a file system. We propose the ELightFS file system, that meets the requirements needed to be installed and run over a Cloud Service Provider infrastructure - it has a native and transparent implementation that can be run on top of various hardware infrastructures, ARM or x86, it is distributed, has a file deduplication feature and stores data in encrypted form.	cloud computing;clustered file system;complexity;cryptography;cryptosystem;data deduplication;encryption;internet backbone;key management;requirement;x86	Alecsandru Patrascu;Mihai Togan;Victor Valeriu Patriciu	2015	2015 IEEE International Conference on Intelligent Computer Communication and Processing (ICCP)	10.1109/ICCP.2015.7312710	self-certifying file system;computer science;stub file;operating system;ssh file transfer protocol;distributed computing;open;distributed file system;world wide web;virtual file system	Embedded	-28.42366131345336	57.947552450356866	45891
3b4e942f720fabde460caaaab1e0ab6a97b6c644	economic model for grid resource sharing	economic model;resource sharing	The main objective of grid computing is to aggregate an important number of geographically distributed heterogeneous resources, owned by different organizations, in order to enable their utilization in a transparent way by the grid users. In such an environment, resource owners and users have different objectives and strategies that have to be satisfied. Because they often need a complete state information about the grid, traditional approaches can not be used in this context. An approach based on economic models used in human economy can be a solution to deal with decentralization and	aggregate data;grid computing	Bernard Miegemolle;Rémi Sharrock;Thierry Monteil	2007			chemical engineering;frost (temperature);dissolution;grid;urea;distributed computing;computer science;electrolyte;shared resource;guanidine	HPC	-31.310531378410776	53.763779491968826	46090
152d7c663bac2a2aaa01a478023a5737f557059b	classification of different approaches for e-science applications in next generation computing infrastructures	scientific application;groupware;natural sciences computing grid computing groupware;juser websearch publications database;e science;collaboration;materials;high performance computing grid computing computational modeling scientific computing production soil physics computing application software computer science computer simulation;grid;evolution biology;computational modeling;graphical user interfaces;next generation;scientific computing;middleware;interoperability;natural sciences computing;e science infrastructures e science applications next generation computing infrastructures scientific computing collaboration grid computing;grid computing;next generation networking;science e science infrastructure grid interoperability scientific computing;infrastructure	Simulation and thus scientific computing is the third pillar alongside theory and experiment in todays science and engineering. The term e-science evolved as a new research field that focuses on collaboration in key areas of science using next generation infrastructures to extend the powers of scientific computing. This paper contributes to the field of e-science as a study of how scientists actually work within currently existing Grid and e-science infrastructures. Alongside numerous different scientific applications, we identified several common approaches with similar characteristics in different domains. These approaches are described together with a classification on how to perform e-science in next generation infrastructures. The paper is thus a survey paper which provides an overview of the e-science research domain.	am broadcasting;autodock;bi-directional text;computation;computational science;computational steering;disk staging;e-science;gaussian (software);high- and low-level;interoperability;logical equality;middleware;next-generation network;open grid services architecture;plug-in (computing);portals;real-time clock;reference model;simulation;unix;x.690	Morris Riedel;Achim Streit;Felix Wolf;Thomas Lippert;Dieter Kranzlmüller	2008	2008 IEEE Fourth International Conference on eScience	10.1109/eScience.2008.56	computational science;human–computer interaction;computer science;theoretical computer science	HPC	-30.187390249410427	50.36146043848681	46196
d6d3d7e8ef01ea863491a0a4fafa903fe286de8b	belle-dirac setup for using amazon elastic compute cloud	cloud;integration;grid;dirac;interoperability	The Distributed Infrastructure with Remote Agent Control (DIRAC) software framework allows a user community to manage computing activities in a distributed environment. DIRAC has been developed within the Large Hadron Collider Beauty (LHCb) collaboration. After successful usage over several years, it is the final solution adopted by the experiment. The Belle experiment at the Japanese High Energy Accelerator Research Organization (KEK) has the purpose of studying matter/anti-matter asymmetries using B mesons. During its lifetime, Belle detector has collected about 5,000 terabytes of real and simulated data. The analysis of this data requires an enormous amount of computing intensive Monte Carlo simulation. The Belle II experiment, which recently published its technical design report, will produce 50 times more data. Therefore it is interested to determine if commercial computing clouds can reduce the total cost of the experiment’s computing solution. This paper describes the setup prepared to evaluate the performance and cost of this approach using real 2010 simulation tasks of the Belle experiment. The setup has been developed using DIRAC as the overall management tool to control both the tasks to be executed and the deployment of virtual machines using the Amazon Elastic Compute Cloud as service provider. At the same time, DIRAC is also used to monitor the execution, collect the necessary statistical data, and finally upload the results of the simulation to Belle resources on the Grid. The results of a first test using over 2000 days of cpu time show that over 90% efficiency in the use of the resources can easily be achieved.	amazon elastic compute cloud (ec2);belle (chess machine);cpan;central processing unit;cloud computing;cloud storage;distributed computing;experiment;grid computing;large hadron collider;microsoft outlook for mac;monte carlo method;purchasing;run time (program lifecycle phase);simulation;software deployment;software framework;terabyte;the australian;upload;virtual community;virtual machine	Ricardo Graciani Diaz;Adrian Casajus Ramo;Ana Carmona Agüero;Thomas Fifield;Martin Sevior	2010	Journal of Grid Computing	10.1007/s10723-010-9175-7	interoperability;real-time computing;simulation;cloud computing;computer science;operating system;dirac;database;distributed computing;grid	HPC	-25.084479423837134	58.65649814523188	46284
5b8f71b08f4779c989b4941fd0552c0df7a6abe5	cloud federation and the evolution of cloud computing	resources;cloud federation;inter clouds;cloud computing	To satisfy the demand for collective and collaborative cloud use, academia and industry want to interconnect heterogeneous clouds to form a federated system. This approach is promising but also faces significant challenges.	cloud computing	Dimitrios G. Kogias;Michael G. Xevgenis;Charalampos Z. Patrikakis	2016	Computer	10.1109/MC.2016.344	cloud computing security;simulation;cloud computing;computer science;operating system;distributed computing;world wide web;resource	HPC	-29.008023855207256	57.559214111096026	46369
4af46d10b40371d97f2786cec4ad2c820722c3e6	enabling performance predictions of complex storage systems	storage system			Diana Lubow;Chuck Milligan	2003			performance prediction;control engineering;computer data storage;computer science	HPC	-19.363329625059517	52.16486605465079	46390
af8ed10eada89f7c45e18b7fb5ade392bdf05360	a comparison of two approaches to build reliable distributed file servers	sun s nfs protocol;dual ported disks;file servers;protocols;reliability;reliable distributed file servers;maintenance;deceit;distributed processing;protocols computer networks distributed processing fault tolerant computing file servers;computer networks;network servers;fault tolerant computing;ha nfs;distributed file system;dual ported disks reliable distributed file servers reliability server replication dual ported disks deceit ha nfs sun s nfs protocol;file servers file systems network servers protocols computer science costs concrete hardware maintenance;computer science;server replication;file systems;concrete;hardware	Several existing distributed le systems provide reliability by server replication. An alternative approach is to use dual-ported disks accessible to a server and a backup. We compare the two approaches by examining an example of each. Deceit is a replicated le server that emphasizes exibility. HA-NFS is an example of the second approach that emphasizes e ciency and simplicity. The two le servers run on the same hardware and implement SUN's NFS protocol. The comparison shows that replicated servers are more exible and tolerant of a wider variety of faults. On the other hand, the dual-ported disks approach is more efcient and simpler to implement. When tolerating single failure, dual-ported disks also give somewhat better availability.	backup;server (computing)	Anupam Bhide;E. N. Elnozahy;Stephen P. Morgan;A. Siegel	1991		10.1109/ICDCS.1991.148734	file server;communications protocol;real-time computing;concrete;computer science;operating system;reliability;database;distributed computing;distributed file system;server;computer network	OS	-22.242430206184565	50.40278525113703	46396
180dd84700bca7cb1e75ceb10a8edba469e4ccc9	black-box and gray-box strategies for virtual machine migration	virtual machine;data center	Virtualization can provide significant benefits in data centers by enabling virtual machine migration to eliminate hotspots. We present Sandpiper, a system that automates the task of monitoring and detecting hotspots, determining a new mapping of physical to virtual resources and initiating the necessary migrations. Sandpiper implements a black-box approach that is fully OSand application-agnostic and a gray-box approach that exploits OSand application-level statistics. We implement our techniques in Xen and conduct a detailed evaluation using a mix of CPU, network and memory-intensive applications. Our results show that Sandpiper is able to resolve single server hotspots within 20 seconds and scales well to larger, data center environments. We also show that the gray-box approach can help Sandpiper make more informed decisions, particularly in response to memory pressure.		Timothy Wood;Prashant J. Shenoy;Arun Venkataramani;Mazin S. Yousif	2007			data center;real-time computing;simulation;computer science;virtual machine;operating system	Networks	-23.53530617711846	58.47564734113161	46684
d05a14821f618df18eb5de66ca7f0963f7f3864f	a two-tier architecture to handle number portability and in-service interaction			multitier architecture;software portability	Markus Lehmacher;Wolfgang Lautenschlager;Heinrich Hintzen;Stephan Rupp	1998			computer architecture;multitier architecture;portability testing;software portability;computer science	Vision	-30.399977803373513	56.02657870918373	46690
5c0ee5e71daead36b7ac3ca13812b341499c5a62	modular concurrency control and failure recovery	computers;digital computers;database decomposition;protocols;generalized serialisability theory;control theory;system recovery database theory distributed databases fault tolerant computing modules;general and miscellaneous mathematics computing and information science;correct execution of transactions;oceans;systems analysis 990210 supercomputers 1987 1989;concurrent computing;concurrency control transaction databases protocols concurrent computing computer science safety oceans statistics maintenance engineering processor scheduling;modular concurrency control;processor scheduling;maintenance engineering;failure analysis;computer architecture;fault tolerant computing;system recovery;transaction databases;failure safety modular concurrency control database decomposition transaction decomposition fault tolerance generalized serialisability theory correct execution of transactions failure recovery consistency;fault tolerance;safety;concurrency control;distributed databases;schedules;statistics;transaction decomposition;system analysis;computer science;system failure analysis;failure recovery;management;failure safety;programming;data base management;database theory;modules;supercomputers;consistency;parallel processing	This paper presents an approach to concurrency control based on the decomposition of both the database and the individual transactions. This approach is a generalization of serializability theory in that the set of permissible transaction schedules contains all the serializable schedules. In addition to providing a higher degree of concurrency than that provided by serializability theory, this approach retains three important properties associated with serializability: the consistency of the database is preserved, the individual transactions are executed correctly, and the concurrency control approach is modular, a concept formalized in this paper. The associated failure recovery procedure is also presented as is the concept of failure safety. Index Terns-Concurrency control, consistency, correctness, failure recovery, modularity.	correctness (computer science);global concurrency control;recovery procedure;schedule (computer science);serializability	Lui Sha;John P. Lehoczky;E. Douglas Jensen	1988	IEEE Trans. Computers	10.1109/12.2144	maintenance engineering;global serializability;embedded system;timestamp-based concurrency control;communications protocol;parallel processing;programming;failure analysis;database theory;optimistic concurrency control;fault tolerance;parallel computing;real-time computing;isolation;concurrent computing;commitment ordering;schedule;computer science;two-phase locking;operating system;concurrency control;modular programming;database;distributed computing;system analysis;multiversion concurrency control;consistency;non-lock concurrency control;programming language;serializability;snapshot isolation;schedule;distributed concurrency control	DB	-23.78921278447188	46.95365237790656	46807
eee6026e73c9df8ca5994bff4930e7931881f724	everware toolkit. supporting reproducible science and challenge-driven education.		Modern science clearly demands for a higher level of reproducibility and collaboration. To make research fully reproducible one has to take care of several aspects: research protocol description, data access, environment preservation, workflow pipeline, and analysis script preservation. Version control systems like git help with the workflow and analysis scripts part. Virtualization techniques like Docker or Vagrant can help deal with environments. Jupyter notebooks are a powerful platform for conducting research in a collaborative manner. We present project Everware that seamlessly integrates git repository management systems such as Github or Gitlab, Docker and Jupyter helping with a) sharing results of real research and b) boosts education activities. With the help of Everware one can not only share the final artifacts of research but all the depth of the research process. This been shown to be extremely helpful during organization of several data analysis hackathons and machine learning schools. Using Everware participants could start from an existing solution instead of starting from scratch. They could start contributing immediately. Everware allows its users to make use of their own computational resources to run the workflows they are interested in, which leads to higher scalability of the toolkit.	care-of address;comparison of version control software;computational resource;control system;data access;docker;emoticon;gitlab;hackathon;ipython;machine learning;open-source software;pipeline (computing);scalability	Andrey Ustyuzhanin;Timothy Daniel Head;Igor Babuschkin;Alexander Tiunov	2017	CoRR	10.1088/1742-6596/898/7/072051	knowledge management;virtualization;software engineering;scalability;human–computer interaction;management system;scratch;extremely helpful;computer science;workflow;data access;scripting language	HCI	-28.564359041573386	52.57443930436334	47501
3a47c600c8651a7e3f6b2db05f03085fc9878968	reducing database locking contention through multi-version concurrency		In multi-version databases, updates and deletions of records by transactions require appending a new record to tables rather than performing in-place updates. This mechanism incurs non-negligible performance overhead in the presence of multiple indexes on a table, where changes need to be propagated to all indexes. Additionally, an uncommitted record update will block other active transactions from using the index to fetch the most recently committed values for the updated record. In general, in order to support snapshot isolation and/or multi-version concurrency, either each active transaction is forced to search a database temporary area (e.g., rollback segments) to fetch old values of desired records, or each transaction is forced to scan the entire table to find the older versions of the record in a multi-version database (in the absence of specialized temporal indexes). In this work, we describe a novel kV-Indirection structure to enable efficient (parallelizable) optimistic and pessimistic multi-version concurrency control by utilizing the old versions of records (at most two versions of each record) to provide direct access to the recent changes of records without the need of temporal indexes. As a result, our technique results in higher degree of concurrency by reducing the clashes between readers and writers of data and avoiding extended lock delays. We have a working prototype of our concurrency model and kV-Indirection structure in a commercial database and conducted an extensive evaluation to demonstrate the benefits of our multi-version concurrency control, and we obtained orders of magnitude speed up over the single-version concurrency control.	database;in-place algorithm;indirection;lock (computer science);multiversion concurrency control;overhead (computing);prototype;random access;snapshot (computer storage);snapshot isolation;wiki	Mohammad Sadoghi;Mustafa Canim;Bishwaranjan Bhattacharjee;Fabian Nagel;Kenneth A. Ross	2014	PVLDB	10.14778/2733004.2733006	database index;timestamp-based concurrency control;optimistic concurrency control;real-time computing;isolation;computer science;data mining;database;distributed computing;multiversion concurrency control;serializability;acid	DB	-21.168513342047998	49.05912555372506	47531
6fbc80d0cb04bd30844efe3c358cb6b5a254c084	supermon: a high-speed cluster monitoring system	performance measure;protocols;performance evaluation;heterogeneous environment;client server systems;heterogeneous alpha processor based node cluster high speed scalable cluster monitoring supermon node behavior monitoring data protocol symbolic expressions heterogeneous environment performance measurements;monitoring system;monitoring;client server systems workstation clusters monitoring performance evaluation protocols;workstation clusters;high speed	Supermon is a flexible set of tools for high speed, scalable cluster monitoring. Node behavior can be monitored much faster than with other commonly used methods (e.g., rstatd). In addition, Supermon uses a data protocol based on symbolic expressions (Sexpressions) at all levels of Supermon, from individual nodes to entire clusters. This contributes to Supermon’s scalability and allows it to function in a heterogeneous environment. This paper presents the Supermon architecture and discuss initial performance measurements on a cluster of heterogeneous Alpha-processor based nodes.	s-expression;scalability	Matthew J. Sottile;Ronald Minnich	2002		10.1109/CLUSTR.2002.1137727	embedded system;communications protocol;real-time computing;computer science;operating system;computer network	HPC	-21.458110235832944	51.62934101301807	47790
d720977eb2376c095cef1eabb0902659bbe18410	jet-lag, java heterogeneous log analysis on the grid: architecture, implementation and performance evaluation			java;log analysis;performance evaluation	Flavio Lombardi;Roberto Puccinelli	2005			grid;architecture;lag;java;computer science;distributed computing	HPC	-29.70793489743927	46.67989403060738	47822
3fae7ef35e98f2e8ab511fbb22bd86e193427c76	research on performance comparison of data center between pm and vm		Cloud computing can realize resources unified scheduling, integrate resources and platforms, speed up the development of value-added products and application’s cycle, reduce costs, and improve business agility by using technology of virtualization. Telecom operators need to virtualize their data centers and build cloud infrastructure for better using resources. So they should move data centers from physical machines to virtual machines for implementing the virtualization. But there are some risks such as the loss of computing ability, performance decline and so on. In this paper, we do a series of experiments to test performance of rational databases and Hadoop in physical machines and virtual machines. We also explore the difference between rational databases and Hadoop when we meet different amounts of data sets. Through these experiments, we find that disk I/O performance of rational database and Hadoop deployed in physical machines is better than that in virtual machines. We also find that Hadoop will have a better performance than rational databases when the data size reaches GB level.	data center	Jiangdong Deng;E Haihong;Qian Chang;Qin Shu;Jun Yang;Haiou Jiang	2014		10.1007/978-3-319-15554-8_19	business agility;scheduling (computing);virtualization;speedup;relational database;data center;cloud computing;distributed computing;virtual machine;computer science	HPC	-26.154516586041442	59.89953863908859	47893
364da1cce93f3de5770ac99e7cebabf1c6233424	an autonomic and scalable management system for private clouds	virtual machines cloud computing data privacy fault tolerant computing power aware computing public domain software;fault tolerance autonomic computing cloud computing resource management scalability;resource management;public domain software;computer architecture;power aware computing;fault tolerant computing;cloud computing scalability resource management computer architecture fault tolerance fault tolerant systems monitoring;monitoring;virtual machines;data privacy;fault tolerant systems;fault tolerance;scalability;power management techniques scalable management system autonomic management system private clouds snooze autonomic virtual machine management framework open source scalable virtual machine management framework energy efficient virtual machine management framework virtualized resources self organizing hierarchical architecture distributed vm management fault tolerance;autonomic computing;cloud computing	Snooze is an open-source scalable, autonomic, and energy-efficient virtual machine (VM) management framework for private clouds. It allows users to build compute infrastructures from virtualized resources. Particularly, once installed and configured, it allows its users to submit and control the life-cycle of a large number of VMs. For scalability, the system relies on a self-organizing hierarchical architecture. Moreover, it implements self-healing mechanisms in case of failure to enable high availability. It also performs energy-efficient distributed VM management through consolidation and power management techniques. This poster focuses on the experimental validation of two main properties of Snooze: scalability and fault-tolerance.	autonomic computing;fault tolerance;high availability;management system;open-source software;openvms;organizing (structure);power management;scalability;self-organization;semiconductor consolidation;virtual machine	Matthieu Simonin;Eugen Feller;Anne-Cécile Orgerie;Yvon Jégou;Christine Morin	2013	2013 13th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing	10.1109/CCGrid.2013.44	fault tolerance;real-time computing;scalability;cloud computing;computer science;virtual machine;resource management;operating system;distributed computing;public domain software;autonomic computing	HPC	-27.670443694966355	55.17832323184026	47915
760ae1f5614c1931fe4a85ba67ce2d68496187ad	special issue: performance evaluation of communications in distributed systems and web-based service architectures			distributed computing;performance evaluation;service-oriented architecture	Lynda Mokdad;Mirela Sechi Moretti Annoni Notare	2010	Concurrency and Computation: Practice and Experience	10.1002/cpe.1575	computer science;distributed computing	HPC	-30.221845213996048	46.93212671912087	47918
0be4df9ee9949549b966c844113d7a8f32d1b367	is cloud computing really ready for prime time?	vendor observers industry observers cloud computing;iaas;virtualization;vendor observers;cloud computing costs computer networks internet scalability operating systems personal communication networks portable computers smart phones web server;data center;internet;paas;blades;grids;blades cloud computing virtualization saas paas iaas data centers grids;industry observers;cloud computing;saas;data centers	Even though the technology faces several significant challenges, many vendors and industry observers predict a bright future for cloud computing.	cloud computing	Neal Leavitt	2009	Computer	10.1109/MC.2009.20	cloud computing security;data center;cloud computing;computer science;operating system;cloud testing;distributed computing;law;world wide web	Arch	-32.119366535383996	55.501073648205455	48015
0542407be89c6e6aeec31951a6c67f4824b012df	taming aggressive replication in the pangaea wide-area file system	data sharing;end user computing;heterogeneous environment;file system;randomized algorithm;distributed file system	Pangaea is a wide-area file system that supports data sharing among a community of widely distributed users. It is built on a symmetrically decentralized infrastructure that consists of commodity computers provided by the end users. Computers act autonomously to serve data to their local users. When possible, they exchange data with nearby peers to improve the system's overall performance, availability, and network economy. This approach is realized by aggressively creating a replica of a file whenever and wherever it is accessed.This paper presents the design, implementation, and evaluation of the Pangaea file system. Pangaea offers efficient, randomized algorithms to manage highly dynamic and potentially large groups of file replicas. It applies optimistic consistency semantics to replica contents, but it also offers stronger guarantees when required by the users. The evaluation demonstrates that Pangaea outperforms existing distributed file systems in large heterogeneous environments, typical of the Internet and of large corporate intranets.	computer cluster;intranet;pangaea (data library);randomized algorithm	Yasushi Saito;Christos T. Karamanolis;Magnus Karlsson;Mallik Mahalingam	2002		10.1145/844128.844131	self-certifying file system;real-time computing;torrent file;device file;computer science;stub file;versioning file system;operating system;unix file types;ssh file transfer protocol;end-user computing;journaling file system;database;open;distributed file system;randomized algorithm;file system fragmentation;global namespace;world wide web;computer security;replication	OS	-24.464444304070895	54.48207751464648	48113
02a22a7212c6876537bd79f9856072c9771606a2	timely long tail identification through agent based monitoring and analytics	analytical models;effective mitigation strategy timely long tail identification agent based monitoring straggling tasks offline analytic agents model task execution patterns online analytic agents large scale production cloud data enters data skew batch processes large scale distributed systems;system monitoring cloud computing data analysis software agents;agent based;long tail;datacenter;runtime;data analysis;servers;monitoring;stragglers;cloud computing long tail stragglers distributed systems data analysis agent based datacenter;analytical models runtime monitoring production quality of service cloud computing servers;production;quality of service;distributed systems;cloud computing	"""The increasing complexity and scale of distributed systems has resulted in the manifestation of emergent behavior which substantially affects overall system performance. A significant emergent property is that of the """"Long Tail"""", whereby a small proportion of task stragglers significantly impact job execution completion times. To mitigate such behavior, straggling tasks occurring within the system need to be accurately identified in a timely manner. However, current approaches focus on mitigation rather than identification, which typically identify stragglers too late in the execution lifecycle. This paper presents a method and tool to identify Long Tail behavior within distributed systems in a timely manner, through a combination of online and offline analytics. This is achieved through historical analysis to profile and model task execution patterns, which then inform online analytic agents that monitor task execution at runtime. Furthermore, we provide an empirical analysis of two large-scale production Cloud data enters that demonstrate the challenge of data skew within modern distributed systems, this analysis shows that approximately 5% of task stragglers caused by data skew impact 50% of the total jobs for batch processes. Our results demonstrate that our approach is capable of identifying task stragglers less than 11% into their execution lifecycle with 98% accuracy, signifying significant improvement over current state-of-the-art practice and enables far more effective mitigation strategies in large-scale distributed systems worldwide."""	distributed computing;emergence;execution pattern;long tail;online and offline;run time (program lifecycle phase)	Peter Garraghan;Xue Ouyang;Paul Townend;Jie Xu	2015	2015 IEEE 18th International Symposium on Real-Time Distributed Computing	10.1109/ISORC.2015.39	embedded system;data center;real-time computing;simulation;quality of service;cloud computing;computer science;operating system;distributed computing;data analysis;long tail;server	OS	-23.22857429346296	59.40943455473679	48249
b4b689cb0ffa1bccff70082447cf962b725b0707	abecity: a middleware for autonomous, parallel, and distributed agent-based processing.	distributed agents;middleware		middleware	Hanh Pham	2003			embedded system;middleware;real-time computing;computer science;object request broker;operating system;middleware;distributed computing	HPC	-30.307416621449953	46.82980549661198	48526
a0aa0364c4d629dc9e42ce5b4bb72ae471127e68	operational information systems: an example from the airline industry	application development;electronic commerce;open system;airline industry;information system	Our research is motivated by the scaleability, availability, and extensibility challenges in deploying open systems based, enterprise operational applications. We present Delta's mid-tier Operational Information Systems (OIS) as an approach for leveraging its legacy operational OLTP infrastructure, to participate in the emerging world of electronic commerce, as well as enable new applications. The approach is to place minimally intrusive 'taps' into the legacy OLTP systems to capture transactions as they occur for consistent replay in the mid-tier OIS. One important issue addressed by our work is the processing, and dissemination of information in the mid-tier system itself, potentially serving hundreds of thousands of access and display points, distributed across a highly geographically distributed system (e.g. airports world wide), and also involving large 'working sets' of operational data, used by applications that require rapid response and also rapid recovery from failures. To address the scaleability, availability, and cost of this OIS infrastructure, we are researching cluster computing techniques, as well as, devising replication and failover techniques. To address the communications scaleability requirements, we are experimenting with novel event-based implementations of information transport and processing, that include reliable multicast variations.	complex event processing;computer cluster;database engine;distributed computing;e-commerce;experiment;extensibility;failover;information system;multicast;multitier architecture;online transaction processing;operational system;requirement	Van Oleson;Karsten Schwan;Greg Eisenhauer;Beth Plale;Calton Pu;Dick Amin	2000			real-time computing;simulation;engineering;computer security	DB	-24.75351812587396	52.71143208292912	48693
557f89269aadfcce9089914199da4555c3f87eea	elastic si-cache: consistent and scalable caching in multi-tier architectures	replication;snapshot isolation;multi version caching;provisioning;elastic computing;fault tolerance;scalability	The new vision of cloud computing demands scalable, available and autonomic software platforms in order to deploy applications and services accessible anywhere and anytime. Multi-tier architectures are an important building block for many applications that are deployed in the cloud. This paper presents a novel caching and replication infrastructure that facilitates the scalable and elastic deployment of multi-tier architectures. Our Elastic SI-Cache is a novel multi-version cache that attains high performance and consistency in multi-tier systems. In contrast to most existing caches, Elastic SI-Cache provides snapshot isolation coherently across all tiers. Furthermore, Elastic SI-Cache supports scalable replication of the different tiers where replicas can be added or removed dynamically as needed, making the cache amenable for cloud computing environments. Elastic SI-Cache has been implemented and integrated into an open source JEE application server and its performance evaluated with the industrial benchmark SPECjAppServer.	anytime algorithm;application server;autonomic computing;benchmark (computing);cache (computing);cloud computing;concurrency (computer science);database;interrupt;isolation (database systems);jonas;java platform, enterprise edition;multitier architecture;open-source software;scalability;server (computing);snapshot (computer storage);snapshot isolation;software deployment	Francisco Perez-Sorrosal;Marta Patiño-Martínez;Ricardo Jiménez-Peris;Bettina Kemme	2011	The VLDB Journal	10.1007/s00778-011-0228-8	fault tolerance;replication;parallel computing;real-time computing;scalability;computer science;database;distributed computing;provisioning;snapshot isolation	OS	-24.90153334320396	53.927473759183414	48768
2de2a47ba2cf44b170ce02c750ad3fea47c3e372	prometheus: a flexible toolkit for the experimentation with virtualized infrastructures			algorithm;component-based software engineering;experiment;google cloud platform;prometheus;server (computing);software deployment;testbed	Cosimo Anglano;Massimo Canonico;Marco Guazzone	2018	Concurrency and Computation: Practice and Experience	10.1002/cpe.4400	virtualization;distributed computing;computer science;resource management	SE	-30.177468408571492	54.872281848567525	48940
363d26ac158e028e2cd4e92d8890fcab40cd6670	a hybrid scheme for controlling transactional composite services	databases;runtime analysis;protocols;sql;sql statements runtime analysis transactional composite services control hybrid concurrency control;transactional composite services control;probes;information gathering;concurrency control probes protocols databases servers time factors algorithm design and analysis;composite services;servers;conference item;time factors;sql statements runtime analysis;web services;concurrency control;web services concurrency control sql;composite services concurrency control;algorithm design and analysis;hybrid concurrency control	This paper proposes a hybrid concurrency control scheme for transactional composite services. The scheme uses the information gathered from the workflow specifications of the composite services to reduce the overhead in detecting cycles in the serialization graph. The scheme carries out runtime analysis of the SQL statements used by the composite services to determine whether the clients that execute composite services depend on each other more accurately. As a result, it reduces the response time to some users. The proposed scheme also tackles the repeated rollback problem facing many concurrency control schemes.	analysis of algorithms;concurrency (computer science);concurrency control;online and offline;overhead (computing);response time (technology);sql;sensor;serialization	Xinfeng Ye;Yi Chen	2010	2010 IEEE International Conference on Web Services	10.1109/ICWS.2010.51	web service;communications protocol;algorithm design;sql;real-time computing;computer science;concurrency control;database;distributed computing;server	DB	-25.112197715585573	49.21500992010759	49083
9e99534bec9fb495da10c9159dd827c820b9299e	policy-based hybrid workflow management system for advanced heart disease identification	heart disease;policy based hybrid workflow management system;service level agreement policy based hybrid workflow management system advanced heart disease identification physiological problem grid computing genomic analysis drug discovery qos constrained medical grid application quality of service ecg analysis physio grid e health platform virtual heart simulation workflow application sla;heart;drug discovery;grid applications;genomic analysis;genome analysis;electrocardiography;engines;sla;workflow management software diseases electrocardiography grid computing health care medical diagnostic computing quality of service;advanced heart disease identification;physio grid e health platform;virtual heart simulation workflow application;user requirements;diseases;workflow management software;middleware;workflow management system;service level agreement;medical application;quality of service;physiological problem;grid computing;medical diagnostic computing;ecg analysis;workflow management software cardiac disease computer networks pharmaceutical technology medical services biomedical equipment grid computing genomics bioinformatics drugs;qos constrained medical grid application;health care	As computer and network technology grows, medical application is become more complex to solve the physiological problems within expected time. Workflow management systems (WMS) in Grid computing are becoming more important to solve the sophisticated problem such as genomic analysis, drug discovery, disease identification, etc. Although existing WMS can provide basic management functionality in Grid environment, consideration of user requirements such as performance, reliability and interaction with user is missing. In this paper, we discuss how to guarantee different user requirements according to user SLA in Grid workflow management system. A hybrid workflow management system for QoS constrained medical Grid application is designed and implemented to provide QoS awareness WMS. The proposed system is applied to Physio-Grid e-health platform to identify human heart disease with ECG analysis and Virtual Heart Simulation (VHS) workflow applications. The experimental result shows that the proposed system can flexibly enhance the Physio-Grid platform in terms of SLA guarantee of different user.	average-case complexity;grid computing;human–computer interaction;quality of service;requirement;service-level agreement;simulation;user requirements document	Woo Ram Jung;Chan-Hyun Youn;Hoeyoung Kim	2009	2009 22nd IEEE International Symposium on Computer-Based Medical Systems	10.1109/CBMS.2009.5255368	quality of service;computer science;bioinformatics;user requirements document;operating system;middleware;data mining;database;world wide web;workflow management system;drug discovery;heart;health care;workflow engine;grid computing;workflow technology	HPC	-33.30736767077083	53.92085425811146	49119
4227a665f22cc9465973b76e414059135b3894ab	a performance model for a bpi middleware	thread pool tuning;capacity planning;benchmark;e commerce;capacity sizing;e commerce workload;layered queueing model;queueing model;performance model;layered queueing network;middleware;performance modeling;performance tuning;business process;business process integration middleware;business process integration;synchronous and asynchronous requests	With today's fast changing business environment and distributed global organizations, business processes of enterprises need to be able to quickly adapt to the new business requirements. BPI middlewares are thus developed to facilitate the integration of various types of enterprise applications that operate within or across enterprise boundaries. Due to the varying complexity and distributed nature of the business processes and the wide range of enterprise applications' characteristics, capacity planning and performance tuning of BPI middleware has been a challenge for engineers who develop and deploy BPI solutions. This poster presents a layered queueing network-based performance model for a BPI middleware to address this challenge.	business process interoperability;business requirements;enterprise software;global serializability;layered queueing network;middleware;performance tuning;queueing theory;requirement	Te-Kai Liu;Amir Behroozi;Santhosh Kumaran	2003		10.1145/779928.779969	e-commerce;real-time computing;benchmark;computer science;operations management;middleware;layered queueing network;distributed computing;business process	HPC	-24.787558843132445	57.96657265158145	49174
2392121fe2557ea32cb53004a8df19dcecb2edd4	the impact of multilevel security on database buffer management	buffer management;multilevel security	Multilevel security introduces new constraints on methods for DBMS buffer management. Design issues include buffer allocation across security levels, secure page replacement, and reader/writer synchronization. We present a client/buffer manager interface with a set of synchronization guarantees that does not delay low writers in the presence of concurrent high readers, an allocation scheme that partitions slots by security level but allows buffers, underutilized at the low level, to be used by subjects at high levels using a technique we call slot stealing. We also propose a general page replacement algorithm and methods of synchronizing readers and writers that involve varying degrees of page replication. We use simulation to investigate the performance characteristics of the various solutions.	multilevel security	Andrew Warner;Qiang Li;Thomas F. Keefe;Shankar Pal	1996		10.1007/3-540-61770-1_41	computer science;database;computer network	DB	-22.34302362395423	47.535168477280116	49272
3b65492f8737637c785d1edd1e21e1b3ae608add	towards simulating billions of agents in thousands of seconds	databases;distributed system;multiagent system;multi agent system;simulation;distributed computing;scaling up;design and implementation;scalability;distributed systems	Building multiagent systems that can scale up to very large number of agents is a challenging research problem. In this paper, we present Distributed Multi Agent System Framework (DMASF), a system which can simulate billions of agents in thousands of seconds. DMASF utilizes distributed computation to gain performance as well as a database to manage the agent and environment state. We briefly present the design and implementation of DMASF and present experimental results. DMASF is a generic and versatile tool that can be used for building massive multi agent system applications.	agent-based model;computation;distributed computing;multi-agent system;simulation	I. V. Aprameya Rao;Manish Jain;Kamalakar Karlapalem	2007		10.1145/1329125.1329299	real-time computing;scalability;simulation;computer science;multi-agent system;distributed computing;distributed design patterns	AI	-29.148513779789837	49.31501824090689	50210
4d50a383ce3408cafb49e8d8aba1055a35656791	towards real-time performance in a scalable, continuously available telecom dbms	real time	A database for telecom applications must satisfy the same demanding requirements as the other components in the telecom network with respect to availability , scalability, and real-time performance. Unfortunately it is diicult to provide state-of-the-art real-time service without sacriicing the other. ClustRa is a parallel and distributed telecom DBMS designed as a careful balance between these requirements. The-losophy has been to provide suucient real-time capabilities for telecom applications. This paper describes the architecture of ClustRa, its real-time features and a performace evaluation.	database;parallel computing;real-time clock;real-time transcription;requirement;scalability	Øystein Torbjørnsen;Svein-Olaf Hvasshovd;Young-Kuk Kim	1996			scalability;database;computer science	Embedded	-28.065871586407265	59.30082739300463	50224
d07542e00595d88768ae2ba74f883c4a798daabc	distributed travelling salesman algorithms for distributed database operation.	distributed database		distributed database;travelling salesman problem	Mario Gerla;Aksenti Grnarov;Douglas Stott Parker	1980			distributed algorithm;computer science;theoretical computer science;database;distributed computing	DB	-28.04243841158197	47.094518084435094	50231
14e382d82477efe46c2da6e63416239a59c12df0	framework for high-performance data transfers optimization in large distributed systems	distributed system;data intensive application;topology;high energy physics instrumentation computing;optical packet switching;next generation network;monalisa;resource allocation;high speed networks;monalisa high performance data transfer optimization large distributed system intelligent data transfer handling next generation network hybrid high speed network network resource high energy physics agent monitoring large integrated services architecture grid system large hadron collider;hybrid high speed network;large hadron collider;distributed computing;system monitoring;network resource;remote monitoring bandwidth distributed computing optical packet switching wide area networks large hadron collider environmental management intelligent networks next generation networking high speed networks;intelligent data transfer handling;fdt;network topology;fdt data transfers scheduling optimizations distributed systems monalisa;scheduling algorithm;monitoring;optimal scheduling;high energy physics;scheduling;optimizations;distributed databases;next generation;bandwidth;large integrated services architecture;high performance data transfer optimization;intelligent networks;remote monitoring;system monitoring data handling grid computing high energy physics instrumentation computing resource allocation;data handling;agent monitoring;quality of service;distributed systems;data transfers;environmental management;grid computing;high performance;large distributed system;next generation networking;grid system;data transfer;wide area networks	Handling high-performance data transfers is a hot topic today, in the context of the numerous data-intensive applications which begin to be used not only in local clusters, but also in large distributed systems. For this purpose, we have developed a framework based on MONALISA that allows intelligent handling of data transfers in large environments, both over current and next-generation, hybrid high-speed networks. This paper presents its design and two policies for handling the data transfers contention to network resources. We argue for its effectiveness by presenting our experiences with the framework, using the FDT data transfer application, in the context of the ALICE Grid and the US LHCNet network.	algorithm;data-intensive computing;distributed computing;experiment;heterogeneous element processor;high- and low-level;mathematical optimization;powerflasher fdt;program optimization;real life;scheduling (computing);testbed;web service	Catalin Cirstoiu;Ramiro Voicu;Nicolae Tapus	2008	2008 International Symposium on Parallel and Distributed Computing	10.1109/ISPDC.2008.12	parallel computing;real-time computing;next-generation network;computer science;operating system;database;distributed computing;scheduling;distributed database;computer network	HPC	-26.34442641423004	54.50127591226727	50416
89401a6fa331c157cac975ac1f19a62b6ff16577	top-down development of a decentralized single address space management	distributed environment;memory management;top down;operating system	 This paper presents concepts and implementation of a memory management to organize virtual single address spaces for fine-grain concurrent computation in parallel and distributed environments. The distributed address space is adaptively partitioned by a dynamic set of cooperating managers. The partitioning scheme is decentralized and scales with growing system configuration. Crucial for the efficiency of this approach is a top-down construction of all operating system entities involved in... 	address space	Christian Rehn	2000			memory management;computer science;address space;real-time computing;distributed computing;system configuration;distributed computing environment;top-down and bottom-up design;configuration management (itsm)	SE	-20.823296513616715	51.36702736241755	50597
6964c4366170d04cec319813be128e940bd11549	a hybrid cloud framework for scientific computing	clouds overlay networks atmospheric modeling computational modeling cloud computing time measurement supercomputers;overlay network hybrid cloud framework scientific computing cloud services cloud resources performance variation organic grid heterogeneous computing resources;scientific information systems cloud computing	Cloud services are transforming many computing tasks, but the unique requirements of scientific computing have caused it to lag behind in cloud adoption because of the performance variation of cloud resources. Based on our experience with the Organic Grid, we propose a framework for a hybrid cloud that will intelligently distribute work to appropriate computing resources to mitigate the impact of performance variation. We describe a cloud framework that integrates with specialized hardware and distributes work intelligently among heterogeneous computing resources. Our approach is to organize a set of computing nodes in an overlay network, to allow each node as an individual agent to position itself within the network to maximize its productivity. An application finds the resources and decides which task to run on which cloud nodes. Our simulations demonstrate that our methods can significantly reduce communication burdens of the most overworked nodes, especially on networks with the highest task to node ratios.	cloud computing;computational science;heterogeneous computing;overlay network;platform as a service;requirement;simulation	Brian Peterson;Gerald Baumgartner;Qingyang Wang	2015	2015 IEEE 8th International Conference on Cloud Computing	10.1109/CLOUD.2015.57	real-time computing;cloud computing;computer science;theoretical computer science;cloud testing;distributed computing;utility computing;grid computing	HPC	-21.447741622328262	58.76685259050198	50727
478ed1e6ecea2c7ae690f136a4677154b00e3c62	distributed, heterogeneous resource management using artificial immune systems	distributed system;heterogeneous resource management;scheduling artificial immune systems grid computing;artificial immune system;high performance computing;processor scheduling;grid computing distributed resource management heterogeneous resource management artificial immune systems distributed computing scheduling;resource manager;resource management;distributed computing;protection;large scale;telecommunication traffic;scheduling;immune system;resource management artificial immune systems distributed computing processor scheduling immune system grid computing high performance computing large scale systems protection telecommunication traffic;distributed resource management;network theory;grid computing;high performance;artificial immune systems;resource management system;geographic distribution;large scale systems	As high performance and distributed computing become more important tools for enabling scientists and engineers to solve large computational problems, the need for methods to fairly and efficiently schedule tasks across multiple, possibly geographically distributed, computing resources becomes more crucial. Given the nature of distributed systems and the immense numbers of resources to be managed in distributed and large-scale cluster environments, traditional centralized schedulers will not be extremely effective at providing timely scheduling information. In order to manage large numbers of resources quickly, less computationally intensive methods for scheduling tasks must be explored. This paper proposes a novel resource management system based on the immune system metaphor, making use of the concepts in Immune Network Theory and Danger Theory. By emulating various elements in the immune system, the proposed manager could efficiently execute tasks on very large systems of heterogeneous resources across geographic and/or administrative domains. The distributed nature of the immune system is also exploited in order to allow efficient scheduling of tasks, even in extremely large environments, without the use of a centralized or hierarchical scheduler.	artificial immune system;centralized computing;computation;computational problem;distributed computing;emulator;extreme programming practices;network theory;scheduling (computing)	Lucas A. Wilson	2008	2008 IEEE International Symposium on Parallel and Distributed Processing	10.1109/IPDPS.2008.4536363	real-time computing;computer science;resource management;theoretical computer science;operating system;distributed computing;artificial immune system	HPC	-20.050021459936904	58.02661555184574	50846
2cc734f58c1c1e82b6e2532fb47df02c163cf282	analysis of remote execution models for grid middleware	distributed application;multiplication operator;grid middleware;hybrid model;workflow;task scheduling;grid computing;use case	Grid computing applications, and distributed applications in general, often experience performance deterioration due to the latencies inherent in the execution of remote operations. Here we analyze three approaches for reducing latencies: an asynchronous model which executes operations in a thread to hide the remote latency of an operation, a bulk model which bundles multiple operations together in a single remote operation, and a pipelining model which executes remote operations in a pipeline-parallel mode. We analyze the performance, parameters and technical requirements of each model, and identify general properties which can help determine which model is the most suitable.Our results show that, depending on the use case scenario, any of the three models can offer the best performance, and we conclude by presenting a hybrid model that combines all three approaches, potentially providing the benefit of each.	distributed computing;grid computing;middleware;pipeline (computing);requirement	Andrei Hutanu;Stephan Hirmer;Gabrielle Allen;André Merzky	2006		10.1145/1186675.1186687	use case;multiplication operator;workflow;parallel computing;real-time computing;computer science;operating system;database;distributed computing;grid computing	HPC	-21.605195091831124	54.329422952634474	50913
b29c71b1e9e2b956bd8e600b6397e301cd71644f	cloudware: an emerging software paradigm for cloud computing	micro service;cloudware;software paradigm;cloud computing	Software paradigm is a driving force for the evolution of software technology. With the continuous improvement in the current cloud computing and the Internet environment, software will develop further into Cloudware, which is emerging as a new software paradigm. This paper defines the concept of Cloudware, and discusses it in the context of software paradigm. Then, based on a loosely coupled von Neumann computing model, we propose a new method of constructing a Cloudware PaaS system which can directly deploy software into the cloud without any modification. By using micro-service architecture, we can achieve high performance, scalable deployment, faults tolerance and flexible configuration. Finally, we evaluate this method by carrying out an interactive delay experiment that directly focuses on users' experience, which shows the effectiveness of our method.	cloud computing;data center;data compression;graphics processing unit;hardware virtualization;internet;loose coupling;platform as a service;programming paradigm;real-time computing;real-time transcription;scalability;software deployment;streaming media;user experience;von neumann architecture;x86 virtualization	Dong Guo;Wei Wang;Jingxuan Zhang;Qiao Xiang;Chenxi Huang;Jinda Chang;Liqing Zhang	2016		10.1145/2993717.2993718	computing;real-time computing;software sizing;crowdsourcing software development;computer science;backporting;software framework;component-based software engineering;software development;software design description;software engineering;multitenancy;software construction;cloud testing;software as a service;distributed computing;resource-oriented architecture;software deployment;software system	SE	-29.18285320032575	55.705970697803124	51343
6e28960cfdceea98b7ee4a1e460e97bf0386469f	a synchronization mechanism for an object oriented distributed system	shared objects;concurrent computing interference constraints object oriented programming computer networks distributed computing physics computing encapsulation delay condition monitoring message passing;encapsulation;distributed system;synchronisation distributed processing object oriented programming;persistent objects;concurrent computing;synchronization mechanism;distributed processing;distributed computing;typed objects;object oriented programming;physics computing;computer networks;synchronisation;condition monitoring;synchronized objects persistent objects semaphore synchronization mechanism object oriented distributed system shared objects typed objects control clauses inheritance transactions;object oriented;semaphore;message passing;synchronized objects;inheritance;interference constraints;transactions;control clauses;object oriented distributed system	A mechanism for synchronizing shared objects in a distributed system based on persistent, typed objects is presented. Synchronization constraints are expressed as separate control clauses and are factorized for a class of objects. The interference of this mechanism with inheritance and transactions is examined and solutions are proposed. Examples of synchronized objects are provided, and a semaphore-based implementation of the mechanism is described. >	distributed computing	Dominique Decouchant;P. Le Dot;Michel Riveill;Cécile Roisin;Xavier Rousset de Pina	1991		10.1109/ICDCS.1991.148657	real-time computing;concurrent computing;computer science;theoretical computer science;distributed computing;programming language;object-oriented programming	Robotics	-24.232050935381984	46.524838911866176	51545
062ffed5d9991661cae071f6b47db57ccd63b581	the ieee intercloud testbed -- creating the global cloud of clouds	protocols;cloud exchange;standards working groups;cloud broker;computer bootstrapping cloud computing;cloud testbed cloud computing;cloud ontology;intercloud;computer architecture;cloud computing protocols interoperability standards working groups computational modeling computer architecture;multi cloud;computational modeling;cloud arbitrage;interoperability;cloud testbed;cloud federation;bootstrap global cloud ieee intercloud testbed project active research topic standards working group ieee p2302 structured organization organizational structure exchange functions;cloud spot market;cloud broker cloud exchange cloud spot market;cloud computing;computer bootstrapping	"""This paper presents the current work of the IEEE Intercloud Testbed project. The notion of an Intercloud has been an active research topic. Within the IEEE several researchers formed a Standards Working Group (IEEE P2302) where a specific set of conventions, formats, and protocols were proposed. It was decided by those in the Standards Working Group that due to the scale, variability of component Compute Clouds, and lack of insight into extremely large Compute Cloud operational issues, such a system could not realistically be fully defined without live experimentation. Therefore it was decided to set up a specifically structured organization within the IEEE in parallel to the Standards Working Group, to provide a structure for a live, experimental testbed. This paper describes the innovative organizational structure and various policies were used to provide the desire context. Also covered are how we sorted the issues around governance of the namespace, and the technical details of reference """"Root"""" and """"Exchange"""" functions. Ongoing work includes the plan to bootstrap the new testbed."""	booting;heart rate variability;intercloud;testbed	David Bernstein;Yuri Demchenko	2013	2013 IEEE 5th International Conference on Cloud Computing Technology and Science	10.1109/CloudCom.2013.102	communications protocol;interoperability;simulation;cloud computing;computer science;operating system;distributed computing;computational model;world wide web	Visualization	-32.05282133489071	54.65062838384133	51844
5047a0677451cf8096961d37c32921327a84bfce	practicality of non-interfering checkpoints in distributed database systems			distributed database	Sang Hyuk Son;Ashok K. Agrawala	1986			database tuning;distributed computing;distributed concurrency control;replication (computing);distributed database;computer science;database testing	Embedded	-27.87480221788312	46.91279082936433	52206
7ea30eb93a80181cbe95a4de53afed6f808fe4e8	data distribution strategies for providing database scalability in e-commerce applications	electronic commerce;distributed database;information security;electronic commerce applications;availability;e commerce;user expectations;software performance evaluation electronic commerce distributed databases quality of service;software performance evaluation;data distribution;back end parallel database servers;back end distributed database servers;scalability distribution strategy quality of service delay database systems distributed databases business data security information security availability;efficient data placement;database systems;business;distributed databases;database scalability;data distribution strategies;scalability;quality of service;user class information;distribution strategy;data placement;service quality;database performance;user class information data distribution strategies database scalability electronic commerce applications service quality database performance efficient data placement back end parallel database servers back end distributed database servers user expectations;data security	The number of users of e-commerce applications is increasing, and users are becoming more and more sensitive to the quality of the ofSered services. This paper discusses performance and scalability issues for back-end parallel or distributed database servers used in ecommerce applications. We argue that database scalability rannot be achieved without considering eflcient data placement. In particular data distribution strategies yhould consider the specqics of e-cornmerce applications and user expectations in ternis of quality of service. We propose a generic data distribution strategy integrating user class information.	database server;distributed database;e-commerce payment system;quality of service;scalability	Haiwei Ye;Brigitte Kerhervé;Gregor von Bochmann;Don Bourne	2001		10.1109/WECWIS.2001.933909	computer science;data mining;database;world wide web	DB	-25.32483077477173	52.17980555083552	52365
2bd3f35cd245112cc9a3ea688e283a435a3d3566	jupyter notebooks in science gateways	jupyter notebooks;high performance computing;scientific gateways	Jupyter Notebooks empower scientists to create executable documents that include text, equations, code and figures. Notebooks are a simple way to create reproducible and shareable workflows. The Jupyter developers have also released a multi-user notebook environment: Jupyterhub. Jupyterhub provides an extensible platform for handling user authentication and spawning the Notebook application to each user. I developed a plugin for Jupyterhub to spawn notebooks on a Supercomputer and integrated the authentication with CILogon and XSEDE. Scientists can authenticate on their browser and connect to a Jupyter Notebook instance running on the computing node of a Supercomputer, in my test deployment SDSC Comet. Jupyterhub can benefit Science Gateways by providing an expressive interface to a centralized environment with many software tools pre-installed and allow scientists to access Gateway functionality via web API. Scientists can then define their own workflows with maximum flexibility: they can mix data processing with a programming language of their choice, e.g. Python, R or Julia, add their own software modules and call the Gateway web API for the more resource-intensive operations. Workflows written as notebooks and executed in such a standard environment boost reproducibility with minimal effort from the scientists. Such notebooks can be both attached to publications to ensure reproducibility of past research and also modified and improved by other interested research groups. In this talk I will introduce the functionalities of Jupyterhub, give an overview of the architecture of the interface with XSEDE authentication and with Comet and finally different scenarios where Jupyter Notebooks can be integrated into Science Gateways.	apl;application programming interface;authentication;centralized computing;crosstalk;executable;ipython;julia;multi-user;pre-installed software;programming language;python;r language;san diego supercomputer center;software deployment;spawn (computing);web api	Andrea Zonca	2016	PeerJ PrePrints	10.7287/peerj.preprints.2577v2	computational science;supercomputer;computer science;world wide web	PL	-32.83862059770038	51.53089460468275	52389
c3c6bee2ead8ecd0ab656cf932b5e313e0b023bd	monitoring community clouds: the lightweight network management protocol	protocols;random access memory;community cloud monitoring lightweight network management protocol ubiquitous networking applications energy frugal devices intermittent power supply lnmp network monitoring process flexible platform portable code;community cloud computing;ubiquitous computing cloud computing computer network management protocols;monitoring communities protocols random access memory cloud computing linux;sigar;lnmp;snmp;monitoring;computer network management;ubiquitous computing;linux;sigar snmp lnmp cloud computing community cloud computing;communities;cloud computing	The simple network monitoring protocol (SNMP) is currently seen as the de-facto standard for network monitoring in IT settings. It is usually used in conjunction with tools such as Cacti, Zabbix and Zenoss to provide most of the features that IT professionals require to efficiently monitor network systems. However, SNMP is not be suitable for the emerging ubiquitous networking applications where energy frugal devices with limited storage and processing are usually deployed unattended sometimes with intermittent power supply. Furthermore, the client server model underlying SNMP may create traffic bottlenecks and a single point of failure for the underlying monitoring systems. Building upon these limitations, this paper proposes the lightweight network management protocol (LNMP) and demonstrates its relative efficiency compared to SNMP. Preliminary experimental results reveal that LNMP simplifies the network monitoring process and results in a more flexible and platform portable code for community clouds.	client–server model;power supply;reliability engineering;server (computing);simple network management protocol;single point of failure;ubiquitous computing;zabbix	Taariq Mullins;Antoine B. Bagula	2013	2013 IEEE 10th International Conference on Ubiquitous Intelligence and Computing and 2013 IEEE 10th International Conference on Autonomic and Trusted Computing	10.1109/UIC-ATC.2013.31	embedded system;communications protocol;network management station;cloud computing;computer science;operating system;database;distributed computing;network management application;simple network management protocol;world wide web;computer security;ubiquitous computing;linux kernel;computer network	Embedded	-30.201198550456766	58.192742021119706	52638
e71589d9fbed12c548019b7deaf09c65439ca4d5	management of heterogeneous atm networks based on integration of mobile agents with legacy systems	pvc configuration management;protocols;investments;simple network management protocol;heterogeneous atm networks management;enterprise network connectivity providers;mobile agents;distributed management paradigm;distributed processing;centralized client server paradigm;enterprise networks;distributed computing;mobile agent technology;research and development management;client server systems;telecommunication computing;business communication;client server based network management systems;atm networks;software agents;mib database;telecommunication computing asynchronous transfer mode telecommunication network management distributed programming software agents client server systems protocols business communication distributed processing;mobile agents technology;research and development;client server;snmp;common management information protocol;legacy systems;distributed programming;information management;computer network management;network management architecture;mobile agents telecommunication network management protocols scalability information management investments research and development research and development management distributed computing computer network management;cost effectiveness;network management;scalability;mib database heterogeneous atm networks management mobile agents legacy systems systems integration centralized client server paradigm network management architecture simple network management protocol snmp data networks common management information protocol cmip telecommunication networks enterprise network connectivity providers research and development distributed management paradigm distributed computing mobile agents technology legacy code client server based network management systems pvc configuration management;cmip;mobile agent;data networks;legacy system;systems integration;configuration management;legacy code;asynchronous transfer mode;telecommunication networks;network management system;distributed management	Regardless of the well-known performance and scalability deficiencies, the centralized client-server paradigm based network management architecture, coupled with Simple Network Management Protocol (SNMP) for data networks and Common Management Information Protocol (CMIP) for telecommunication networks, remains the prevailing solution in the network management arena. The main reason for this are the huge investments that Telcos and the enterprise network connectivity providers have invested. Although significant research and development efforts have been spent in recent years in building alternative solutions, based on distributed management paradigm, their role is still minor. We propose a third solution, combining the distributed computing benefits based on mobile agents technology with the legacy code of existing client-server based network management systems. We believe that such model provides a cost-effective solution, while permitting significantly to reduce the bottlenecks of client-server based solutions. We demonstrate our solution by applying it to the tasks of PVC configuration management in heterogeneous ATM networks.	atm turbo;legacy system;mobile agent	André Ribeiro Cardoso;Joaquim Celestino;Ricardo A. R. Celestino	2002		10.1109/NOMS.2002.1015633	network management;fcaps;element management system;real-time computing;systems management;network management station;computer science;distributed computing;telecommunications management network;network management application;simple network management protocol;structure of management information;information management;common management information protocol;business communication;computer security;legacy system;computer network	Robotics	-33.0549483722944	48.109988073542766	52651
6010cd9348aad0f2c441b0c1cb4d68a4b6436a16	guest editor's foreword: bigger and faster and smaller	storage system;consumer electronics;design and implementation;enterprise system;power consumption	In recent years storage systems have evolved dramatically as a result of both social and technical advances. Storage systems and storage devices are found in almost any computing installation from large centralized and distributed enterprise systems to a variety of mobile consumer electronic devices. Such a wide deployment of storage has created a need to re-evaluate basic solutions in storage systems design and implementation. As part of this ongoing process of technology evolution, it is critical to find a framework to identify, understand, and evaluate a range of issues. The reliability, availability, scalability, performance, and power consumption characteristics of storage systems must be considered in a variety of traditional and emerging computing environments.	centralized computing;computer performance;enterprise system;scalability;software deployment;systems design	Alma Riska;Erik Riedel	2006	SIGMETRICS Performance Evaluation Review	10.1145/1138085.1138088	enterprise system;simulation;converged storage;computer science;operating system;enterprise storage	OS	-27.21848126944443	55.53289571360145	52681
ef8465c888a0691a27974e95a9a1d8ac8eeadc13	similarity-based load adjustment for static real-time transaction systems	air traffic control;air traffic control data integrity concurrency control real time systems database management systems performance evaluation;performance evaluation;data integrity;database management systems;real time;real time databases;real time data;simulation experiment;load adjustment;real time systems frequency transaction databases concurrency control air traffic control guidelines processor scheduling aerospace electronics error correction degradation;concurrency control;serializability;data similarity;theoretical foundation;data consistency;air traffic control example similarity based load adjustment static real time transaction systems application semantics performance correctness criteria semantica based concurrency control algorithms data consistency system workload simulation experiments;reading and writing;real time systems	How to exploit application semantics to improve the performance of a real-time data-intensive application has been an active research topic in the past few years. Weaker correctness criteria and semantica-based concurrency control algorithms were proposed to provide more flexibility in reordering read and write events. Distinct from past work, this paper exploits the trade-off between data consistency and system workload. The definition of similarity is combined with the idea of transaction skipping to provide a theoretical foundation for reducing the workload of a transaction system. We also propose guidelines to adjust the execution frequencies of a static set of transactions and prove their correctness. The strengths of this work were verified by simulation experiments on an air traffic control example.	real-time transcription	Tei-Wei Kuo;Shao-Juen Ho	2000	IEEE Trans. Computers	10.1109/12.833108	embedded system;real-time data;parallel computing;real-time computing;computer science;operating system;air traffic control;concurrency control;data integrity;database;distributed computing;data consistency;serializability	Embedded	-23.717797868330262	48.3102231994942	52932
b6364fd40e7420089f5b061141fdf1e0e68df176	container-based orchestration in cloud: state of the art and challenges	kernel;virtualization;system level virtualization;containers cloud computing orchestration system level virtualization;virtualisation big data cloud computing computer centres internet of things resource allocation virtual machines;vm container based orchestration cloud computing big data internet of things iot cloud data center virtual machine;containers virtualization linux hardware kernel cloud computing;linux;orchestration;containers;cloud computing;hardware	How to effectively manage increasingly complex enterprise computing environments is one of the hardest challenges that most organizations have to face in the era of cloud computing, big data and IoT. Advanced automation and orchestration systems are the most valuable solutions helping IT staff to handle large-scale cloud data centers. Containers are the new revolution in the cloud computing world, they are more lightweight than VMs, and can radically decrease both the start up time of instances and the processing and storage overhead with respect to traditional VMs. The aim of this paper is to provide a comprehensive description of cloud orchestration approaches with containers, analyzing current research efforts, existing solutions and presenting issues and challenges facing this topic.	big data;cloud computing;data center;enterprise software;image file formats;interoperability;openvms;overhead (computing);provisioning;software system;uptime	Andrea Tosatto;Pietro Ruiu;Antonio Attanasio	2015	2015 Ninth International Conference on Complex, Intelligent, and Software Intensive Systems	10.1109/CISIS.2015.35	cloud computing security;embedded system;cloud computing;computer science;operating system;cloud testing;world wide web	HPC	-30.355220863418868	56.97505280943566	53019
89f664ed82495b014c28869fcc42044a9c8b4a4f	operating system-level virtual organization support in xtreemos	software;computational grid;performance evaluation;virtual organization support;virtual enterprises;xtreemos;security infrastructure;authentication;grid middleware;virtual enterprises grid computing linux middleware operating systems computers;multiple linux unix nodes;xtreemos authentication authorization access control virtual organization support;operating system;virtual organization;resource sharing;middleware;linux;authorization;access control;computational grids;nas parallel benchmarks;security;grid computing;security infrastructure operating system level virtual organization support xtreemos computational grids multiple linux unix nodes grid middleware;operating systems computers;support function;operating system level virtual organization support;linux software performance data security distributed computing grid computing middleware kernel authentication switches computers	In computational grids, a virtual organization (VO) is a dynamic coupling of multiple Linux/Unix nodes for resource sharing under specific polices. Currently, VO support functionalities are generally implemented as grid middleware. However, the usability of grids is often impaired by the complexity of configuring and maintaining a new layer of security infrastructure as well as adapting to new interfaces of security enabled services. In this paper, we present an OS-level approach to provide native VO support functionalities, which is a part of XtreemOS project [18]. Our approach adopts pluggable frameworks existing in current OS as extension points to implement VO support, avoiding modification of kernel codes and easily turning traditional OSes into grid-aware ones. The performance evaluation of NAS parallel benchmarks (NPB) shows that our current implementation incurs trivial overhead on original systems.	code;linux;middleware;nas parallel benchmarks;operating system;overhead (computing);performance evaluation;unix;usability;virtual organization (grid computing)	An Qin;Haiyan Yu;Chengchun Shu;Xiaoqian Yu;Yvon Jégou;Christine Morin	2008	2008 Ninth International Conference on Parallel and Distributed Computing, Applications and Technologies	10.1109/PDCAT.2008.48	shared resource;support function;parallel computing;computer science;information security;access control;operating system;middleware;authentication;database;distributed computing;authorization;linux kernel;grid computing;computer network	HPC	-30.11782381776753	53.82675865027856	53041
fa893eece1de34e081a7c009f1478a4f8b769915	designing workflows for grid enabled internet instruments	workflow design;geii;business process execution language;instruments;security of data business data processing grid computing internet open systems;internet instruments telescopes ip networks telecommunication traffic grid computing computerized monitoring large scale systems computer architecture secure storage;grid enabled internet instruments;bpel geii workflow client honey pot;telescopes;secure storage;computing;client honey pot;computerized monitoring;computer architecture;large scale;telecommunication traffic;malicious activity;internet;business data processing;gridcc;business process execution language workflow design grid enabled internet instruments malicious activity grid computing interoperability cima gridcc;workflow;ip networks;bpel;interoperability;open systems;cima;grid computing;security of data;large scale systems;open source	To analyse malicious activity on the Internet, instruments such as network telescopes and honeypots are effective tools that can be deployed. Such tools can be deployed in large scale using Grid computing. Manual deployment of instruments wastes resources because common tasks and solutions are reinvented by different deployers and the resulting architectures are often not interoperable or sufficiently scalable. Research is underway to develop a framework for scalable and automated deployment, with Grid technologies providing a promising basis. The integration of Grid technology with instrumentation has two initiatives. These are CIMA and GRIDCC, with GRIDCC being available as open source. A key area is workflow within the framework, for which BPEL (Business Process Execution Language) is used in GRIDCC and considered for initial use for Grid Enabled Internet Instruments. We have found BPEL has limitations when implementing such as framework, particularly in the areas of concurrency and statefullness. We propose implementation independent workflows and identify extensions to BPEL in order to realise them. We believe that BPEL with modification can be used to implement a framework for Internet instruments-Grid computing integration.	business process execution language;cima: the enemy;computational science;concurrency (computer science);grid computing;honeypot (computing);internet;interoperability;iteration;middleware;open-source software;requirement;scalability;service-orientation;software deployment	David Stirling;Ian Welch;Peter Komisarczuk	2008	2008 Eighth IEEE International Symposium on Cluster Computing and the Grid (CCGRID)	10.1109/CCGRID.2008.112	business process execution language;computer science;operating system;database;distributed computing;world wide web	HPC	-33.308827356991735	50.212840351828426	53167
407b34d1638b4bdad96f7690c7b6e539ff648897	efficient deadlock resolution for lock-based concurrency control schemes	directed graphs;distributed algorithms;distributed system;control systems;lock based concurrency control schemes;system recovery computational complexity distributed processing operating systems computers;detection algorithms;brazil council;time complexity;distributed processing;traffic control;time complexities;object oriented distributed systems;waits for graph directed graphs lock based concurrency control schemes distributed algorithm object oriented distributed systems semantic lock model message traffic redundant messages time complexities;system recovery;object oriented;computational complexity;semantic lock model;concurrency control;redundant messages;message traffic;computer science;waits for graph;distributed algorithm;system recovery concurrency control object oriented modeling distributed algorithms control systems computer science traffic control detection algorithms brazil council delay;operating systems computers;object oriented modeling	We propose a distributed algorithm for detection and resolution of resource deadlocks in object-oriented distributed systems. In particular the algorithm can be used in conjunction with concurrency control algorithms which are based on the semantic lock model. In order to drastically reduce message traffic, our algorithm properly identifies and eliminates redundant messages. We show that its worst and average time complexities are O(ne), where e is the number of edges in the waits-for-graph.	concurrency (computer science);concurrency control;deadlock;distributed algorithm;distributed computing;waits	Marina Roesler;Walter A. Burkhard;Kenneth B. Cooper	1988		10.1109/DCS.1988.12521	giant lock;lock;time complexity;distributed algorithm;real-time computing;isolation;directed graph;computer science;theoretical computer science;operating system;concurrency control;database;distributed computing;object-oriented programming;computational complexity theory;deadlock prevention algorithms;distributed concurrency control	DB	-22.810494814588395	46.748645536345556	53390
fc729d78f6feaed9efaa1407b6cda079f5b02b4d	brb: better batch scheduling to reduce tail latencies in cloud data stores		A common pattern in the architectures of modern interactive web-services is that of large request fan-outs, where even a single end-user request (task) arriving at an application server triggers tens to thousands of data accesses (sub-tasks) to different stateful backend servers. The overall response time of each task is bottlenecked by the completion time of the slowest sub-task, making such workloads highly sensitive to the tail of latency distribution of the backend tier. The large number of decentralized application servers and skewed workload patterns exacerbate the challenge in addressing this problem. We address these challenges through BetteR Batch (BRB). By carefully scheduling requests in a decentralized and task-aware manner, BRB enables low-latency distributed storage systems to deliver predictable performance in the presence of large request fan-outs. Our preliminary simulation results based on production workloads show that our proposed design is at the 99th percentile latency within 38% of an ideal system model while offering latency improvements over the state-of-the-art by a factor of 2.	data store;internet slang;job scheduler;scheduling (computing)	Waleed Reda;P. Lalith Suresh;Marco Canini;Sean Braithwaite	2015	Computer Communication Review	10.1145/2829988.2790023	data center;parallel computing;real-time computing;computer science;load balancing;operating system;computer security;computer network	Networks	-22.782069528925714	59.321850005034655	53551
10520b648b60c985b48d2900ad840e8597d95a4e	developing power-aware scheduling mechanisms for computing systems virtualized by xen	power aware scheduling;virtualization;cloud computing		operating-system-level virtualization;scheduling (computing)	Shenyuan Ren;Ligang He;Huanzhou Zhu;Zhuoer Gu;Wei Song;Jiandong Shang	2017	Concurrency and Computation: Practice and Experience	10.1002/cpe.3888	parallel computing;real-time computing;virtualization;cloud computing;computer science;operating system;utility computing	HPC	-20.403797684323255	60.01299956307161	53910
bb430783f53556174ee1b82bb3bbf7f35fc01386	a survey on database performance in virtualized cloud environments	database management systems dbms;virtualization;performance;online transaction processing oltp;cloud computing	Cloud Computing emerged as a major paradigm over the years. Major challenges it poses to computer science are related to latency, scale, and reliability issues. It leverages strong economical aspects and provides sound answers to questions like energy consumption, high availability, elasticity, or efficient computing resource utilization. Many Cloud Computing platform and solution providers resort to virtualization as key underlying technology. Properties like isolation, multi-virtual machine parallelism, load balancing, efficient resource utilization, and dynamic pre-allocation besides economic factors make it attractive. It not only legitimates the spread of several types of data stores supporting a variety of data modes, but also inherently requires different types of load: i analytical; ii Transactional/Update-intensive; and iii mixed real-time feed processing. The authors survey how database systems can best leverage virtualization properties in cloud scenarios. The authors show that read mostly database systems and especially column stores profit from virtualization in analytical and search scenarios. Secondly, cloud analytics virtualized database systems are efficient in transactional scenarios such as Cloud CRM virtualized database systems lag. The authors also explore how the nature of mixed cloud loads can be best reflected by virtualization properties like load balancing, migration, and high availability.		Todor Ivanov;Ilia Petrov;Alejandro P. Buchmann	2012	IJDWM	10.4018/jdwm.2012070101	cloud computing security;real-time computing;virtualization;cloud computing;performance;computer science;data virtualization;operating system;cloud testing;data mining;database	DB	-23.408919752630805	59.56672837119366	54438
45208423b65bd6504bfdc2c989972f10dfe815d1	on a storage system software stack for extreme scale data centric computing		This paper presents an overview of a storage system software stack designed specifically for extreme scale data centric computing. The storage system software stack is designed to work for next generation storage system architectures that will need to incorporate multiple data storage device technologies. We also envision such data centric storage systems to have in-storage compute capability to make it suitable for dealing with data analytics, and, pre/post processing steps in data centric workflows. The storage system software stack indeed needs to accommodate such capability.	computer data storage;ibm websphere extreme scale;next-generation network	Sai Narasimhamurthy	2017	2017 46th International Conference on Parallel Processing Workshops (ICPPW)	10.1109/ICPPW.2017.41	distributed computing;information repository;database;software;computer science;data analysis;workflow;database-centric architecture;computer data storage;converged storage	HPC	-27.320949115085725	55.245652677701756	55050
050535a2b08182a721c0f67d6a3c53cbb50e1f10	cannyfs: opportunistically maximizing i/o throughput exploiting the transactional nature of batch-mode data processing		We introduce a user mode file system, CannyFS, that hides latency by assuming all I/O operations will succeed. The user mode process will in turn report errors, allowing proper cleanup and a repeated attempt to take place. We demonstrate benefits for the model tasks of extracting archives and removing directory trees in a real-life HPC environment, giving typical reductions in time use of over 80%. This approach can be considered a view of HPC jobs and their I/O activity as transactions. In general, file systems lack clearly defined transaction semantics. Over time, the competing trends to add cache and maintain data integrity have resulted in different practical tradeoffs. High-performance computing is a special case where overall throughput demands are high. Latency can also be high, with non-local storage. In addition, a theoretically possible I/O error (like permission denied, loss of connection, exceeding disk quota) will frequently warrant the resubmission of a full job or task, rather than traditional error reporting or handling. Therefore, opportunistically treating each I/O operation as successful, and part of a larger transaction, can speed up some applications that do not leverage asynchronous I/O. E-mail: jessica.nettelblad@it.uu.se Uppsala Multidisciplinary Center for Advanced Computational Science, Uppsala University Box 335 SE-751 05 Uppsala Sweden · E-mail: carl.nettelblad@it.uu.se Division of Scientific Computing, Department of Information Technology, Science for Life Laboratory, Uppsala University Box 335 SE-751 05 Uppsala Sweden	archive;asynchronous i/o;batch processing;bottleneck (software);computation;computational science;data integrity;exception handling;experiment;input/output;interrupt latency;job stream;real life;science for life laboratory;thread-local storage;throughput;transaction processing;user space	Jessica Nettelblad;Carl Nettelblad	2016	CoRR		real-time computing;data mining;distributed computing	HPC	-20.10589683540263	49.97624473236378	55351
ce01db3920a71d23573bbf38add7de56719d6f67	coordination avoidance in distributed databases	computer science coordination avoidance in distributed databases university of california berkeley joseph m hellerstein;ion stoica bailis peter david;computer science	The rise of Internet-scale geo-replicated services has led to upheaval in the design of modern data management systems. Given the availability, latency, and throughput penalties associated with classic mechanisms such as serializable transactions, a broad class of systems (e.g., NoSQL) has sought weaker alternatives that reduce the use of expensive coordination during system operation, often at the cost of application integrity. When can we safely forego the cost of this expensive coordination, and when must we pay the price?In this thesis, we investigate the potential for coordination avoidance---the use of as little coordination as possible while ensuring application integrity---in several modern data-intensive domains. We demonstrate how to leverage the semantic requirements of applications in data serving, transaction processing, and web services to enable more efficient distributed algorithms and system designs. The resulting prototype systems demonstrate regular order-of-magnitude speedups compared to their traditional, coordinated counterparts on a variety of tasks, including referential integrity and index maintenance, transaction execution under common isolation models, and database constraint enforcement. A range of open source applications and systems exhibit similar results.	database;distributed computing	Peter Bailis	2015			computer science;artificial intelligence;operations research	DB	-21.76824327505863	49.766398193391474	55430
c2de320e2d01fcc896ffffe35f636f32fb90f8bb	agent-based approach to monitoring and control of distributed computing environment	expert systems;distributed computing environments;multi agent systems;monitoring;high performance computer centre	This paper discusses a problem of monitoring heterogeneous distributed computing environments which consist of loosely coupled multiplatform computing resources. We propose an approach to the organization of the meta-monitoring system which collects data from existing local monitoring systems and own software sensors, unifies and analyzes data, generates necessary control actions. The approach is based on web-technologies, multi-agent technologies, expert systems, methods of decentralized processing and distributed storage of data.	clustered file system;distributed computing environment;expert system;loose coupling;multi-agent system;sensor;software agent	Igor Bychkov;Gennady A. Oparin;Alexei P. Novopashin;Ivan Sidorov	2015		10.1007/978-3-319-21909-7_24	distributed algorithm;real-time computing;computer science;multi-agent system;data mining;distributed computing;utility computing;distributed design patterns;expert system;autonomic computing	HPC	-30.93939254616377	47.552333413727204	55503
6bfc30477aed2ae52ce79ee776cabca9191d9b64	market-oriented cloud computing: opportunities and challenges	market-oriented cloud computing	"""Computing is being transformed to a model consisting of services that are commoditised and delivered in a manner similar to utilities such as water, electricity, gas, and telephony. In such a model, users access services based on their requirements without regard to where the services are hosted. Several computing paradigms have promised to deliver this utility computing vision. Cloud computing is the most recent emerging paradigm promising to turn the vision of """"computing utilities"""" into a reality. Cloud computing has emerged as one of the buzzwords in the IT industry. Several IT vendors are promising to offer storage, computation and application hosting services, and provide coverage in several continents, offering Service-Level Agreements (SLA) backed performance and uptime promises for their services. It delivers infrastructure, platform, and software (application) as services, which are made available as subscription-based services in a pay-as-you-go model to consumers. The price that Cloud Service Providers charge can vary with time and the quality of service (QoS) expectations of consumers. This keynote talk will cover (a) 21st century vision of computing and identifies various IT paradigms promising to deliver the vision of computing utilities; (b) the architecture for creating market-oriented Clouds by leveraging technologies such as VMs; (c) market-based resource management strategies that encompass both customer-driven service management and computational risk management to sustain SLA-oriented resource allocation; (d) Aneka, a software system for rapid development of Cloud applications and their deployment on private/public Clouds with resource provisioning driven by SLAs and user QoS requirements, (e) experimental results on deploying Cloud applications in engineering, gaming, and health care domains (integrating sensors networks, mobile devices), ISRO satellite image processing on elastic Clouds, and (f) need for convergence of competing IT paradigms for delivering our 21st century vision along with pathways for future research. 2013 17th IEEE International Enterprise Distributed Object Computing Conference 1541-7719/13 $26.00 © 2013 IEEE DOI 10.1109/EDOC.2013.41 3"""	cloud computing;computation;emoticon;enterprise distributed object computing;image processing;mobile device;openvms;programming paradigm;provisioning;quality of service;requirement;risk management;sensor;service-level agreement;software deployment;software system;three utilities problem;uptime;utility computing	Rajkumar Buyya	2013		10.1109/EDOC.2013.41	simulation;cloud computing;systems engineering;engineering;operating system;software engineering;data mining;database;utility computing;services computing;management;world wide web;computer security	HPC	-31.617207639337384	54.90099978179038	55617
af2870e2a9434fe9956c78a812258a48ed0dcfb8	resource elasticity for distributed data stream processing: a survey and future directions.		Under several emerging application scenarios, such as in smart cities, operational monitoring of large infrastructures, and Internet of Things, continuous data streams must be processed under very short delays. Several solutions, including multiple software engines, have been developed for processing unbounded data streams in a scalable and efficient manner. This paper surveys state of the art on stream processing engines and mechanisms for exploiting resource elasticity features of cloud computing in stream processing. Resource elasticity allows for an application or service to scale out/in according to fluctuating demands. Although such features have been extensively investigated for enterprise applications, stream processing poses challenges on achieving elastic systems that can make efficient resource management decisions based on current load. This work examines some of these challenges and discusses solutions proposed in the literature to address them.	cloud computing;elasticity (cloud computing);elasticity (data store);enterprise software;internet of things;scalability;smart city;software engine;stream processing	Marcos Dias de Assunção;Alexandre Da Silva Veith;Rajkumar Buyya	2017	CoRR		data science;data stream;elasticity (economics);data mining;computer science	OS	-27.954364312274116	60.10151371647709	55667
03fd6135ed6815ccc5cddbac9a59efc2ad3dc8f5	vidas: object-based virtualized data sharing for high performance storage i/o	data sharing;hpc;storage i o virtualization;cloud computing	With scientific computing in the cloud gaining popularity and using every time larger data sets, high performance storage I/O in virtualized environments is substantially increasing in importance. However, exploiting the performance potential of the storage I/O on today's virtualized architectures is complex, due to the limitations of POSIX standard for storage I/O and the lack of integration of related mechanisms such as data sharing, storage I/O coordination, relaxing the consistency semantics, and data locality awareness. In this paper we propose VIDAS (Virtualized DAta Sharing), an object-based virtualized data store that targets to integrate the above mechanisms through a simple and powerful interface. VIDAS can be used to efficiently and consistently share access to externally stored data in virtualized environments based on a shared pool of storage objects. We show how VIDAS can be used for straightforwardly implementing I/O coordination and data sharing for two common high-performance patterns: inter-domain write-reader and inter-domain collective I/O. We present the implementation and evaluation of VIDAS for the Xen virtualization solution. In addition, we present a novel mechanism for efficiently sharing memory among an arbitrary number of virtual machines.	asynchronous i/o;cloud computing;computational science;data store;input/output;inter-domain;locality of reference;object-based language;posix;virtual machine	Pablo Llopis;Francisco Javier García Blas;Florin Isaila;Jesús Carretero	2013		10.1145/2465848.2465851	supercomputer;real-time computing;cloud computing;computer science;operating system;database	HPC	-21.35124799854266	52.86924731215997	55681
3aac2b38c21e03487c172373b57b441e62d50d66	trilogy: data placement to improve performance and robustness of cloud computing	workload-aware data placement;data management;cloud computing	Infrastructure as a Service, one of the most disruptive aspects of cloud computing, enables configuring a cluster for each application for each workload. When the workload changes, a cluster will be either underutilized (wasting resources) or unable to meet demand (incurring opportunity costs). Consequently, efficient cluster resizing requires proper data replication and placement. Our work reveals that coarse-grain, workload-aware replication addresses over-utilization but cannot resolve under-utilization. With fine-grain partitioning of the dataset, data replication can reduce both under- and over-utilization. In our empirical studies, compared to a näive uniform data replication a coarse-grain workload-aware replication increases throughput by 81% on a highly-skewed workload. A fine-grain scheme further reaches 166% increase. Furthermore, a surprisingly small increase in granularity is sufficient to obtain most benefits. Evaluations also show that maximizing the number of unique partitions per node increases robustness to tolerate workload deviation while minimizing this number reduces storage footprint.	branch misprediction;cloud computing;disk partitioning;distributed computing;hpcc;load balancing (computing);replication (computing);software deployment;throughput	Chin-Jung Hsu;Vincent W. Freeh;Flavio Villanustre	2017	2017 IEEE International Conference on Big Data (Big Data)	10.1109/BigData.2017.8258202	throughput;machine learning;workload;robustness (computer science);artificial intelligence;real-time computing;replication (computing);cloud computing;opportunity cost;granularity;distributed database;computer science	HPC	-19.195285879583967	58.410288392463336	56033
7510afe3416f372366e027c878be93bed409e049	improving real-time collaboration with highlighting	real time;collaborative editing;multi user;highlighting;group awareness;operational transformation;consistency maintenance;undo	Highlighting is a common tool in most single-user editors. It provides users with a mechanism of communication and collaboration between the author and readers by emphasizing some important text. This tool is also necessary and even more valuable in multi-user collaborative editors. However, it is non-trivial to extend it from single-user environment to multi-user environment because of the following challenges: (1) the need to differentiate highlights performed by different users, (2) the need to tackle inconsistency problems caused by concurrent operations and non-deterministic communication latency, and (3) the need to provide a flexible undo facility with the capability of undoing any highlighting operation at any time. In this paper, we will systematically address these issues and offer our solutions. These solutions lay a foundation for handling update operations similar in nature to highlighting, which update attributes of text instead of inserting/deleting text. © 2003 Elsevier B.V. All rights reserved.	algorithm;collaborative real-time editor;color;concurrency (computer science);concurrency control;entity–relationship model;interactive programming;microsoft word for mac;multi-user;operational transformation;pair programming;real-time clock;real-time transcription;reduce;text editor;undo;user interface	Haifeng Shen;Chengzheng Sun	2004	Future Generation Comp. Syst.	10.1016/S0167-739X(03)00176-6	computer science;artificial intelligence;operating system;data mining;database;distributed computing;multimedia;world wide web;computer security	HCI	-25.552145951257263	47.72208084323315	56266
4347e0b2b98996b457bd1181b5ec84ec819c8747	a flexible support framework for widely distributed computing			distributed computing	Michael Baentsch	1997				HPC	-29.49714999146281	46.63853700896978	56536
bf989394f4ac4f3d24f056f633c8762411a18c66	p2p application for file sharing	structured;directory server;time measurement;decentralized systems p2p application peer to peer paradigm resource sharing users sharing file sharing application centralized systems central server;peer to peer computing servers time measurement size measurement ip networks resource management containers;resource manager;resource management;p2p;size measurement;centralized systems;decentralized system;hybrid;peer to peer computing file organisation;servers;indexation;resource sharing;structured peer to peer file sharing centralized systems directory server hybrid;ip networks;file sharing;peer to peer computing;peer to peer;containers;file organisation	The novelty of the peer-to-peer (P2P) paradigm relies on two main concepts: cooperation among users and resource sharing. There are many applications based on peer-to-peer paradigm, but the most popular one is the file sharing. We can classify the file sharing application into centralized systems, (having a central server), and decentralized systems. Another classification would be structured and unstructured systems, based on the way of managing the indexing information. In this paper we implement a centralized peer-to-peer application for file sharing. And we evaluate the application by means of simulation.	centralized computing;file sharing;peer-to-peer;programming paradigm;server (computing);simulation	Hala Amin;Mohamed Khaled Chahine;Gianluca Mazzini	2012	2012 19th International Conference on Telecommunications (ICT)	10.1109/ICTEL.2012.6221249	shared resource;self-certifying file system;directory service;hybrid;many-to-many;decentralised system;computer science;resource management;operating system;ssh file transfer protocol;peer-to-peer;database;distributed computing;open;world wide web;file sharing;server;time	DB	-24.738915934178184	52.19332798037332	56727
ff516b109e5cb11e85a132dd5b75bf8f2c2a4438	towards a universal cdar device: a high-performance adapter-based inline media encryptor		As the rate at which digital data is generated continues to grow, so does the need to ensure that data can be stored securely. The use of an NSA-certified Inline Media Encryptor (IME) is often required to protect classified data, as its security properties can be fully analyzed and certified with minimal coupling to the environment in which it is embedded. However, these devices are historically purpose-built and must often be redesigned and recertified for each target system. This tedious and costly (but necessary) process limits the ability for an information system architect to leverage advances made in storage technology. Our universal Classified Data At Rest (CDAR) architecture represents a modular approach to reduce this burden and maximize interface flexibility. The core module is designed around NVMe, a high-performance storage interface built directly on PCIe. Interfacing with non-NVMe interfaces such as SATA is achieved with adapters which are outside the certification boundary and therefore can be less costly and leverage rapidly evolving commercial technology. This work includes an analysis for both the functionality and security of this architecture. A prototype was developed with peak throughput of 23.9 Gb/s at a power consumption of 8.5W, making it suitable for a wide range of storage applications.	digital data;embedded system;field-programmable gate array;gigabyte;information system;input method;pci express;prototype;serial ata;software deployment;throughput;unicom system architect	Benjamin Nahill;Aaron Mills;Martin Kiernicki;David A. Wilson;Michael Vai;Roger I. Khazan;John Sherer	2017	MILCOM 2017 - 2017 IEEE Military Communications Conference (MILCOM)	10.1109/MILCOM.2017.8170815	digital data;computer network;throughput;architecture;computer science;nvm express;pci express;real-time computing;encryption;modular design;adapter (computing)	EDA	-32.941006807363856	59.213036952901845	56874
af4e3af40b9208841b3d024eb87b9def50445def	lightweight data mining based database system self-optimization: a case study	database system;data mining;system performance;neural network	With the system becoming more complex and workloads becoming more fluctuating, it is very hard for DBA to quickly analyze performance data and optimize the system, self optimization is a promising technique. A data mining based optimization scheme for the lock table in database systems is presented. After trained with performance data, a neural network become intelligent enough to predict system performance with newly provided configuration parameters and performance data. During system running, performance data is collected continuously for a rule engine, which chooses the proper parameter of the lock table for adjusting, the rule engine relies on the trained neural network to precisely provide the amount of adjustment. The selected parameter is adjusted accordingly. The scheme is implemented and tested with TPC-C workload, system throughput increases by about 16 percent.	data mining;mathematical optimization	Xiongpai Qin;Wei Cao;Shan Wang	2009		10.1007/978-3-642-00672-2_61	real-time computing;computer science;operating system;machine learning;data mining;database;computer performance;world wide web;artificial neural network	ML	-20.535612283542132	57.13154601645254	56944
657f86ea19b5694129340501cda7ae253848059b	mobile database procedures in mdbas	management system;performance evaluation;query processing;distributed transactions;client server systems;mobile database;software agents;multi agent systems;internet;system integration;commercial database mobile database procedures mdbas multidatabase management system mobile agents autonomous databases global database scheme transparent distributed execution user requests distributed transactions performance evaluation internet;distributed databases;mobile agent;internet software agents performance evaluation multi agent systems distributed databases client server systems query processing;distributed databases mobile agents transaction databases sections employment java information retrieval distributed computing software engineering mathematics	MDBAS is a prototype of a multidatabase management system based on mobile agents. The system integrates a set of autonomous databases distributed over a network, enables users to create a global database scheme, and manages transparent distributed execution of user requests an d procedures including distributed transactions. This paper highlights the issues related to mobile database procedures, especially the MDBAS execution strategy. In order to adequately assess MDBAS’s qualities and bottlenecks, we have carried out complex performance evaluation with real databases distributed in real Internet. The evaluatio n included a comparison to a commercial database with distributed database capabilities. The most interesting resu lts are presented and commented.	autonomous robot;commitment ordering;distributed database;distributed transaction;mobile agent;performance evaluation;prototype;stored procedure	Richard Vlach	2001		10.1109/DEXA.2001.953118	database theory;the internet;distributed data store;database transaction;database tuning;distributed transaction;mobile database;computer science;software agent;concurrency control;multi-agent system;mobile agent;management system;database;distributed computing;serializability;world wide web;database schema;distributed database;acid;database testing;replication;system integration;distributed concurrency control	DB	-27.039848605230674	46.71070568673824	57041
2b0e7462fcbd338822b35c403cdab525acf4345a	reducing the energy footprint of a distributed consensus algorithm	servers nominations and elections clustering algorithms algorithm design and analysis proposals fault tolerance fault tolerant systems;distributed consensus;distributed computing;raft algorithm distributed computing fault tolerant computing green computing distributed consensus paxos algorithm;workstation clusters distributed algorithms meta data power aware computing;paxos algorithm;servers;fault tolerant computing;fault tolerant systems;fault tolerance;nominations and elections;raft algorithm;clustering algorithms;proposals;algorithm design and analysis;data loss energy footprint reduction distributed consensus algorithm raft consensus algorithm lightweight servers metadata cluster availability;green computing	The Raft consensus algorithm is a new distributed consensus algorithm that is both easier to understand and more straightforward to implement than the older Paxos algorithm. Its major limitation is its high energy footprint. As it relies on majority consensus voting for deciding when to commit an update, Raft requires five participants to protect against two simultaneous failures. We propose two methods for reducing this huge energy footprint. Our first proposal consists of adjusting Raft quorums in a way that would allow updates to proceed with as few as two servers while requiring a larger quorum for electing a new leader. Our second proposal consists of replacing one or two of the five Raft servers with witnesses, that is, lightweight servers that maintain the same metadata as other servers but hold no data and can therefore run on very low-power hosts. We show that these substitutions have little impact on the cluster availability but very different impacts on the risks of incurring a data loss.	chandra–toueg consensus algorithm;computer cluster;consensus (computer science);low-power broadcasting;paxos (computer science);raft (computer science)	Jehan-François Pâris;Darrell D. E. Long	2015	2015 11th European Dependable Computing Conference (EDCC)	10.1109/EDCC.2015.25	paxos;green computing;algorithm design;fault tolerance;real-time computing;consensus;raft;computer science;operating system;database;distributed computing;cluster analysis;computer security;server	HPC	-21.057504748711867	49.62390929825092	57223
3e1de77b39cd64524965e9a2ea0ae8203c332916	injecting realistic burstiness to a traditional client-server benchmark	management system;performance evaluation;capacity planning;e commerce;index of dispersion;system performance;autonomic system;client server;client server benchmarks;performance evaluation of self managed systems;burstiness	The design of autonomic systems often relies on representative benchmarks for evaluating system performance and scalability. Despite the fact that experimental observations have established that burstiness is a common workload characteristic that has deleterious effects on user-perceived performance, existing client-server benchmarks do not provide mechanisms for injecting burstiness into the workload. In this paper, we introduce a new methodology for generating workloads that emulate the temporal surge phenomenon in a controllable way, thus provide a mechanism that enables testing and evaluation of client-server system performance under reproducible bursty workloads. This new methodology allows to inject different amounts of burstiness into the arrival stream using the index of dispersion, a single parameter that is as simple to use as a turnable knob.  We exemplify the effectiveness of this new methodology by introducing a new module into the TPC-W, a benchmark that is routinely used for capacity planning of e-commerce systems. This new module injects burstiness into the arrival process of clients in a controllable manner, and hence, enables understanding system performance degradation due to burstiness. Detailed experimentation on a real system shows that this benchmark modification can stress the system under different degrees of burstiness, making a strong case for the usefulness of this modification for capacity planning of autonomic systems.	autonomic computing;benchmark (computing);client–server model;control knob;e-commerce;elegant degradation;exemplification;perceived performance;scalability;server (computing);tpc-w	Ningfang Mi;Giuliano Casale;Ludmila Cherkasova;Evgenia Smirni	2009		10.1145/1555228.1555267	e-commerce;embedded system;real-time computing;simulation;computer science;index of dispersion;operating system;management system;database;distributed computing;computer performance;management;computer security;client–server model;computer network	Metrics	-24.125615706356527	57.58558706735821	57240
8a535066df65a48ef76421bb62b9974698064697	cloud driven design of a distributed genetic programming platform	genetic programming;machine learning;distributed evolutionary computation;cloud computing	We describe how we design FlexGP, a distributed genetic programming (GP) system to efficiently run on the cloud. The system has a decentralized, fault-tolerant, cascading startup where nodes start to compute while more nodes are launched. It has a peer-to-peer neighbor discovery protocol which constructs a robust communication network across the nodes. Concurrent with neighbor discovery, each node launches a GP run differing in parameterization and training data from its neighbors. This factoring of parameters across learners produces many diverse models for use in ensemble learning.	genetic programming	Owen Derby;Kalyan Veeramachaneni;Una-May O'Reilly	2013		10.1007/978-3-642-37192-9_51	real-time computing;computer science;theoretical computer science;distributed computing	Robotics	-26.089976229898358	53.08257990556872	57763
404d41299352dddaa829f30cad191ad7247fed00	augmenting the cave: an initial study into close focused, inward looking, exploration in ipt systems	distributed coordination;termination detection;federated simulation;computational modeling;proposals aggregates computational modeling real time systems discrete event simulation context modeling optimization methods;aggregates;interactive simulation;interdependencies analysis;critical infrastructure;agent based modeling and simulation;proposals;context modeling;logical process;real time systems;optimization methods;discrete event simulation	In this paper we study how to reuse checkpoints taken in an uncorrelated manner during the forward execution phase in an optimistic simulation system in order to construct global consistent snapshots which are also committed (i.e. the logical time they refer to is lower than the current GVT value). This is done by introducing a heuristic-based mechanism relying on update operations applied to local committed checkpoints of the involved logical processes so to eliminate mutual dependencies among the final achieved state values. The mechanism is lightweight since it does not require any form of (distributed) coordination to determine which are the checkpoint update operations to be performed. At the same time it is likely to reduce the amount of checkpoint update operations required to realign the consistent global state exactly to the current GVT value, taken as the reference time for the snapshot. Our proposal can support, in a performance effective manner, termination detection schemes based on global predicates evaluated on a committed and consistent global snapshot, which represent an alternative as relevant as classical termination check only relying on the current GVT value. Another application concerns interactive simulation environments, where (aggregate) output information about committed and consistent snapshots needs to be frequently provided, hence requiring lightweight mechanisms for the construction of the snapshots.	aggregate data;application checkpointing;direct inward dial;heuristic;information processes and technology;mutual exclusion;simulation;snapshot (computer storage);transaction processing system;x86 virtualization	Rob Aspin;Kien Hoang Le	2007		10.1109/DS-RT.2007.9	real-time computing;simulation;computer science;distributed computing	SE	-21.570604571596082	47.539961929247106	57900
3772b35794d2fdfa4154937befb0042ba818e981	crumbling walls: a class of practical and efficient quorum systems	distributed system;quorum system;fault tolerant;distributed computing;data replication;mutual exclusion	 A quorum system is a collection of sets (quorums) every two of which intersect. Quorum systems have been used for many applications in the area of distributed systems, including mutual exclusion, data replication and dissemination of information. In this paper we introduce a general class of quorum systems called Crumbling Walls and study its properties. The elements (processors) of a wall are logically arranged in rows of varying widths. A quorum in a wall is the union of one full row and a representative from every row below the full row. This class considerably generalizes a number of known quorum system constructions. The best crumbling wall is the CWlog quorum system. It has small quorums, of size O(lg n), and structural simplicity. The CWlog has optimal availability and optimal load among systems with such small quorum size. It manifests its high quality for all universe sizes, so it is a good choice not only for systems with thousands or millions of processors but also for systems with as few as 3 or 5 processors. Moreover, our analysis shows that the availability will increase and the load will decrease at the optimal rates as the system increases in size.	central processing unit;display resolution;distributed computing;mutual exclusion;quorum (distributed computing);replication (computing)	David Peleg;Avishai Wool	1995	Distributed Computing	10.1007/s004460050027	fault tolerance;parallel computing;real-time computing;mutual exclusion;computer science;engineering;distributed computing;replication	Theory	-20.87858769605522	47.02808146983463	57921
2a9c885e5a3a965c7d90fb5972247a584b29c953	using services and service compositions to enable the distributed execution of legacy simulation applications	web services;simulation workflows;bpel;distributed simulations;service compositions	In the field of natural and engineering science, computer simulations play an increasingly important role to explain or predict phenomena of the real world. Although the software landscape is crucial to support scientists in their every day work, we recognized during our work with scientific institutes that many simulation programs can be considered legacy monolithic applications. They are developed without adhering to known software engineering guidelines, lack an acceptable software ergonomics, run sequentially on single workstations and require tedious manual tasks. We are convinced that SOA concepts and the service composition technology can help to improve this situation. In this paper we report on the results of our work on the serviceand service compositionbased re-engineering of a legacy scientific application for the simulation of the ageing process in copper-alloyed. The underlying general concept for a distributed, service-based simulation infrastructure is also applicable to other scenarios. Core of the concept is a resource manager that steers server work load and handles simulation data.	automatic parallelization;computer simulation;enterprise service bus;graphical user interface;human factors and ergonomics;overhead (computing);parallel computing;provisioning;server (computing);service composability principle;software engineering;speedup;workstation	Mirko Sonntag;Sven Hotta;Dimka Karastoyanova;David Molnar;Siegfried Schmauder	2011		10.1007/978-3-642-24755-2_23	real-time computing;simulation;computer science;software engineering	HPC	-28.925332660552748	53.41848767922797	58006
66fe619f9c579d5d35439f0b774ff82cd7473265	enabling the autonomic data center with a smart bare-metal server platform	autonomic data center;management system;resource manager;provisioning;common information model;discovery;manageability standards;service location protocol;pre boot environment;data center;dynamic data;bare metal;asset inventory;service level agreement;it management;business process	Ever increasing data center complexity poses a significant burden on IT administrators. This burden can become unbearable without the help of self-managed systems that monitor themselves and automatically modify their state in order to carry out business processes according to high level objectives set by service level agreements (SLA) and policies. Among the key IT management tasks that must be automated and enhanced to realize the idea of an autonomic and highly dynamic data center, are discovery, configuration, and provisioning of new servers. In this direction, this paper describes pre-boot capabilities endowing the bare metal server with the ability to be discovered, queried, configured, and provisioned at time zero using industry standards like Common Information Model (CIM), CIM-XML, and Service Location Protocol (SLP). The capabilities are implemented as a payload of an Intel® Extensible Firmware Interface (EFI)-compliant BIOS, the Intel® Rapid Boot Toolkit (IRBT), allowing a resource manager to discover a new server during pre-boot, possibly in a bare-metal state, and then perform an asset inventory, configure the server including CPU-specific settings, and provision it with the most appropriate image. All these tasks may be carried out based on decisions taken by the resource manager according to server capabilities, application requirements, SLAs, and high-level policies. Additionally, this system uses reliable protocols, thus minimizing error possibilities.	apple–intel architecture;autonomic computing;bios;bare machine;bare-metal server;business process;central processing unit;computer-integrated manufacturing;data center;dynamic data;high- and low-level;high-level programming language;information model;payload (computing);provisioning;rapid boot;requirement;server (computing);service-level agreement;superword level parallelism;unified extensible firmware interface;xml	Arzhan Kinzhalin;Rodolfo Kohn;David Lombard;Ricardo Morin	2009		10.1145/1555228.1555257	data center;real-time computing;dynamic data;information technology management;resource management;operating system;service location protocol;management system;database;distributed computing;business process;management;world wide web;computer security;provisioning;computer network;server farm	OS	-30.92683758775423	54.62719834053604	58016
003fa2b27bf5e7c404bd074751c7b35d08e7629a	optimizing data management in grid environments	location service;data management;peer to peer;data transfer;flash crowds	Grids currently serve as platforms for numerous scientific as well as business applications that generate and access vast amounts of data. In this paper, we address the need for efficient, scalable and robust data management in Grid environments. We propose a fully decentralized and adaptive mechanism comprising of two components: A Distributed Replica Location Service (DRLS) and a data transfer mechanism called GridTorrent. They both adopt Peer-to-Peer techniques in order to overcome performance bottlenecks and single points of failure. On one hand, DRLS ensures resilience by relying on a Byzantine-tolerant protocol and is able to handle massive concurrent requests even during node churn. On the other hand, GridTorrent allows for maximum bandwidth utilization through collaborative sharing among the various data providers and consumers. The proposed integrated architecture is completely backwards-compatible with already deployed Grids. To demonstrate these points, experiments have been conducted in LAN as well as WAN environments under various workloads. The evaluation shows that our scheme vastly outperforms the conventional mechanisms in both efficiency (up to 10 times faster) and robustness in case of failures and flash crowd instances.	backward compatibility;byzantine fault tolerance;distributed hash table;experiment;middleware;optimizing compiler;peer-to-peer;planetlab;reliability engineering;replication (computing);scalability	Antonis Zissimos;Katerina Doka;Antony Chazapis;Dimitrios Tsoumakos;Nectarios Koziris	2009		10.1007/978-3-642-05148-7_38	real-time computing;computer science;distributed computing;world wide web	HPC	-24.97270472062802	54.38011842212278	58353
3565d3ee3c087b0b99c8d7d9f49cf8b11fe17f3b	benefits of software rejuvenation on hpc systems	os kernel rejuvenation;hpc system;kernel;software reliability availability numerical models hardware kernel;availability;checkpoint restart mechanism;object oriented programming;rejuvenation scheduling;system component repair;checkpointing;failure mitigation;operating system kernels checkpointing object oriented programming;system component replacement;numerical models;operating system kernels;software rejuvenation;software reliability;system component resetting;checkpoint restart mechanism software rejuvenation hpc system failure mitigation system component replacement system component repair system component resetting os kernel rejuvenation rejuvenation scheduling;hardware	Rejuvenation is a technique expected to mitigate failures in HPC systems by replacing, repairing, or resetting system components. Because of the small overhead required by software rejuvenation, we primarily focus on OS/kernel rejuvenation. In this paper, we propose three rejuvenation scheduling techniques. Moreover, we investigate the claim that software rejuvenation prolongs failure times in HPC systems. Also, we compare the lost computing times of the checkpoint/restart mechanism with and without rejuvenation after each checkpoint.	application checkpointing;operating system;overhead (computing);scheduling (computing);software rejuvenation;transaction processing system	Nichamon Naksinehaboon;Narate Taerat;Chokchai Leangsuksun;Clayton Chandler;Stephen L. Scott	2010	International Symposium on Parallel and Distributed Processing with Applications	10.1109/ISPA.2010.82	availability;parallel computing;kernel;real-time computing;computer science;operating system;programming language;object-oriented programming;software quality	Arch	-22.973152929714292	50.54537951723682	58588
19174d4682ef734799ac9d958e20b9f1e6a06d3c	towards timely acid transactions in dbms	databases;timely transactions;information infrastructure;failure detection;real time;performance;data management;programming model;temporal properties;transaction processing;time constraint	On time data management is becoming a key difficulty faced by organizations. In spite of the importance of timeliness requirements in database applications, commercial DBMS do not assure the detection of the cases when a transaction takes longer than the expected/desired time. This paper discusses the problem of timing failure detection in database applications and proposes a transaction programming approach to help developers in programming database applications with time constraints	acid;database server;oracle database;real-time clock;requirement;server (computing);timed event system;timing failure;trusted computing base	Marco Vieira;António Casimiro Costa;Henrique Madeira	2006	2006 12th Pacific Rim International Symposium on Dependable Computing (PRDC'06)	10.1007/978-3-540-71703-4_24	information infrastructure;real-time computing;database transaction;transaction processing;rollback;performance;distributed transaction;data management;computer science;data mining;database;online transaction processing;programming paradigm;acid;consistency	Arch	-23.535069804434766	48.560497869604106	58651
b5ce249ed13b3fd9003a6a818b750de9e7b0a7cc	interactive hpc gateways with jupyter and jupyterhub		MSI at the University of Minnesota has adopted a goal of supporting Interactive HPC as a first class service. This paper describes the implementation of a gateway for user-friendly, reproducible computing in an HPC environment using the Jupyter notebook server and Jupyterhub. For this project, we developed novel components and techniques enabling interoperability of Jupyter with the distinct requirements of an institutional HPC center. These include modules providing integration with batch job scheduling, control of job profiles, and integration with a central authentication service. To achieve software sustainability, we designed these elements for generic applicability, and several have now been accepted as components of the Jupyterhub open source project.	batch processing;central authentication service;first-class function;ipython;interoperability;job scheduler;open-source software;requirement;scheduling (computing);server (computing);usability	Michael B. Milligan	2017		10.1145/3093338.3104159	batch processing;scheduling (computing);first class;software;interoperability;default gateway;central authentication service;computer science;supercomputer;distributed computing	HPC	-32.414217202167514	52.85346129552108	58784
f4b408a93760bab3d126ca6be29bf1720a3c0f95	evaluation of container orchestration systems for deploying and managing nosql database clusters		Container orchestration systems, such as Docker Swarm, Kubernetes and Mesos, provide automated support for deployment and management of distributed applications as sets of containers. While these systems were initially designed for running load-balanced stateless services, they have also been used for running database clusters because of improved resilience attributes such as fast auto-recovery of failed database nodes, and location transparency at the level of TCP/IP connections between database instances. In this paper we evaluate the performance overhead of Docker Swarm and Kubernetes for deploying and managing NoSQL database clusters, with MongoDB as database case study. As the baseline for comparison, we use an OpenStack IaaS cloud that also allows attaining these improved resilience attributes although in a less automated manner.	baseline (configuration management);cloud computing;distributed computing;docker;internet protocol suite;load balancing (computing);mongodb;nosql;overhead (computing);software deployment;stateless protocol;swarm	Eddy Truyen;Matt Bruzek;Dimitri Van Landuyt;Bert Lagaisse;Wouter Joosen	2018	2018 IEEE 11th International Conference on Cloud Computing (CLOUD)	10.1109/CLOUD.2018.00066	stateless protocol;orchestration (computing);software deployment;swarm behaviour;nosql;cloud computing;location transparency;database;computer science;distributed database;distributed computing	DB	-25.445560050580067	57.62142072660999	59153
785c9baa945f0c8d9633b4127a4b7cbd0ac39ac5	cloud software performance metrics collection and aggregation for auto-scaling module		Cloud computing made a big impact on software architecture evolution. The demand to serve multiple tenants, to include continuous delivery practice into the development process as well as increased system load influenced the style of cloud based software architecture. Microservice architecture is preferred architecture despite its complexity when scalability is an essential attribute of quality of service. Microservices should be managed, i.e., hardware resources should be adjusted based on application load, as well as resiliency should be ensured. Popular IaaS and PaaS providers such as Amazon, Azure or OpenStack ensure auto-scaling and elasticity at the infrastructure level. This approach has the following limitations: (1) Scaling and resiliency is a part of the infrastructure and not emerging from application nature; (2) The software is locked in with a specific vendor; (3) It might be difficult to run and ensure smooth scalability by running software on different vendors at the same time. We are creating auto-scaling module for microservice-based applications. Collecting metrics both at infrastructure and application levels is one important task for auto-scaling. We’ve chosen to investigate ELK stack and build appropriate architecture around it.		Aurimas Cholomskis;Olesia Pozdniakova;Dalius Mazeika	2018		10.1007/978-3-319-99972-2_10	real-time computing;architecture;scalability;continuous delivery;microservices;cloud computing;software architecture;autoscaling;computer science;software performance testing	Arch	-29.646460737339236	56.172080038279205	59375
ec0a110d6d15b0304b33e9b990bace184797f93b	unicore deployment within the deisa supercomputing grid infrastructure	computational grid;graphic user interface;middleware;interoperability;unicore;middleware deployment	DEISA is a consortium of leading national supercomputing centers that is building and operating a persistent distributed supercomputing environment with continental scope in Europe. To integrate their resources, the DEISA partners have adopted the most advanced middleware and applications currently available. The consortium decided to embrace UNICORE as a job submission interface for the DEISA grid infrastructure. UNICORE is the foremost European grid technology able to hide the complexity of the underlying resources providing a user-friendly graphical user interface for job submission. This paper presents the deployment solution and strategies implemented by DEISA in order to adapt UNICORE for their infrastructure.	supercomputer;unicore	Luca Clementi;Michael Rambadt;Roger Menday;Johannes Reetz	2006		10.1007/978-3-540-72337-0_26	interoperability;computer science;operating system;middleware;graphical user interface;database;world wide web	HPC	-30.95167414851411	52.05020701813583	59547
0daad057a90235279c4c565f8776097f5a7df4de	attack surface metrics and automated compile-time os kernel tailoring.		The economy of mechanism security principle states that program design should be kept as small and simple as possible. In practice, this principle is often disregarded to maximize user satisfaction, resulting in systems supporting a vast number of features by default, which in turn offers attackers a large code base to exploit. The Linux kernel exemplifies this problem: distributors include a large number of features, such as support for exotic filesystems and socket types, and attackers often take advantage of those. A simple approach to produce a smaller kernel is to manually configure a tailored Linux kernel. However, the more than 11,000 configuration options available in recent Linux versions make this a time-consuming and non-trivial task. We design and implement an automated approach to produce a kernel configuration that is adapted to a particular workload and hardware, and present an attack surface evaluation framework for evaluating security improvements for the different kernels obtained. Our results show that, for real-world server use cases, the attack surface reduction obtained by tailoring the kernel ranges from about 50% to 85%. Therefore, kernel tailoring is an attractive approach to improve the security of the Linux kernel in practice.	attack surface;computer user satisfaction;kernel (operating system);linux;operating system;server (computing)	Anil Kurmus;Reinhard Tartler;Daniela Dorneanu;Bernhard Heinloth;Valentin Rothberg;Andreas Ziegler;Wolfgang Schröder-Preikschat;Daniel Lohmann;Rüdiger Kapitza	2013			theoretical computer science;distributed computing;computer network	Security	-26.963072410723523	58.86194598216119	59843
0564e53ca7ecf0e7bfb65af42903d4a3e9b66ffb	co-existence of cloud and grid: a case study in service oriented grid garuda	application development;software;portals;service oriented grid;security garuda cloud computing grid computing services job submission;service oriented architecture cloud computing grid computing;service orientation;garuda infrastructure;service oriented architecture service oriented grid cloud computing garuda infrastructure internet;job submission;structural change;garuda;internet;software security;services;communities;service oriented architecture;security;grid computing;credit cards;cloud computing;cloud computing grid computing communities portals security credit cards software	In this paper we discuss how a Cloud Computing offering can co-exist in the GARUDA infrastructure. GARUDA is now an established Grid, having undergone two phases of evolution. Providing Cloud Computing support is essential to increase the outreach of Grid to more application developers and users. Grids may rely on private networks, whereas Clouds rely on Internet and the Service-oriented Architecture. We describe how GARUDA differs from a Cloud, and propose the structural changes required in GARUDA in order to support Cloud, while at the same time servicing the current Grid Computing users. So, we suggest an architecture that allows GARUDA to be viewed as Grid by some users, and as a Cloud by the others.	cloud computing;grid computing;internet;private network;service-oriented architecture;service-oriented device architecture	Vineeth Simon Arackal;B. Arunachalam;Payal Saluja;B. B. Prahlada Rao	2011	2011 Eighth International Conference on Information Technology: New Generations	10.1109/ITNG.2011.186	software security assurance;the internet;service;cloud computing;computer science;information security;operating system;service-oriented architecture;structural change;database;distributed computing;rapid application development;law;world wide web;computer security;grid computing	HPC	-32.42122847158715	54.83720965629955	59902
7aa0151c3f01121e510bfa59e5cff7c64a0e1ce1	handling and resolving conflicts in real time mobile collaboration	distributed system;systeme reparti;resolucion conflicto;real time;sistema repartido;internet;resolution conflit;temps reel;tiempo real;operational transformation;conflict resolution;mobile network	Real time group editors allow two or more users at different locations to work on a shared document at the same time. In a mobile network environment with nondeterministic communication latency, a replicated architecture is usually adopted for the storage of the shared document in order to provide high responsiveness. A conflict occurs when two or more users have different intentions for editing the same part of the replicated document. Conflict can be categorised into two types: exclusive and non-exclusive conflicts. An exclusive conflict occurs when the conflicting operations cannot be realised at the same time, and if serially executed, the effect of the later operation will override the earlier operation. In contrast, a non-exclusive conflict occurs when the conflicting operations can be realised at the same time and both operations can be applied to the target without one overriding the other. The approaches adopted by the existing algorithms to resolve conflicts can be categorised into: (1) locking approaches [1], (2) operational transformation approaches [2], and (3) multi-versioning approaches [3]. The locking approach is a conflict prevention approach rather than a conflict resolution approach, and it does not promote concurrency as only one person can modify an object at one time. The operational transformation approach is optimistic and is able to handle non-exclusive conflicts, but it cannot handle exclusive conflicts. The multi-versioning approach handles exclusive conflicts by creating different object versions with each version realising each conflicting intention. Xue et al. [4] combine multi-versioning, operational transformation, and post-locking to handle both types of conflict and to restrict the number of object versions. However, while post-locking simplifies the conflict resolution process by locking versions to protect them from further modification, it suffers from a partial intention problem insofar as conflicts could be better resolved if the conflicting intentions have been completely realised and thus every user can see the full intention of all other users.	algorithm;concurrency (computer science);exclusive or;lock (computer science);logical equality;operational transformation;responsiveness	Sandy Citro;Jim McGovern;Caspar Ryan	2006		10.1007/11915034_10	cellular network;real-time computing;the internet;simulation;computer science;artificial intelligence;operating system;conflict resolution;database;distributed computing;operations research;computer security	PL	-23.832031286568853	46.7674020495363	60023
ec9ba36edc2dfd280128ab6d71cc283fd16fbf2e	rebooting computing: new strategies for technology scaling	special issues and sections moore s law scalability high performance computing computer architecture supercomputers;high performance computing;moore s law;scalable computing;computer architecture;rebooting computing;supercomputing moore s law rebooting computing scalable computing high performance computing computer architecture;supercomputing	"""Year-over-year exponential computer performance scaling has ended. Complicating this is the coming disruption of the """"technology escalator"""" underlying the industry: Moore's law. To fundamentally rethink and restart the performance-scaling trend requires bold new ways to compute and a commitment from all stakeholders."""	scalability	Thomas M. Conte;Elie K. Track;Erik DeBenedictis	2015	IEEE Computer	10.1109/MC.2015.363	supercomputer;parallel computing;computer science;theoretical computer science;utility computing;moore's law;unconventional computing	Visualization	-27.867491078040526	55.86272586536479	60153
112e6437e80bf8bca94a06874a823c5d17eaefaa	exploring models and mechanisms for exchanging resources in a federated cloud	outsourcing;delays context outsourcing cloud computing computational modeling availability educational institutions;task outsourcing cloud computing cloud federation cometcloud tuple space;availability;tuple space;qa75 electronic computers computer science;computational modeling;task outsourcing;resource allocation cloud computing computer centres;cometcloud;us model exploration mechanism exploration resource exchange federated cloud cloud systems service delivery resource availability data centres cloud providers vendor lock in prevention in house capacity specialist capability cometcloud based implementation specialist gateways uk;cloud federation;context;delays;cloud computing	One of the key benefits of Cloud systems is their ability to provide elastic, on-demand (seemingly infinite) computing capability and performance for supporting service delivery. With the resource availability in single data centres proving to be limited, the option of obtaining extra-resources from a collection of Cloud providers has appeared as an efficacious solution. The ability to utilize resources from multiple Cloud providers is also often mentioned as a means to: (i) prevent vendor lock in, (ii) to enable in house capacity to be combined with an external Cloud provider, (iii) combine specialist capability from multiple Cloud vendors (especially when one vendor does not offer such capability or where such capability may come at a higher price). Such federation of Cloud systems can therefore overcome a limit in capacity and enable providers to dynamically increase the availability of resources to serve requests. We describe and evaluate the establishment of such a federation using a CometCloud based implementation, and consider a number of federation policies with associated scenarios and determine the impact of such policies on the overall status of our system. CometCloud provides an overlay that enables multiple types of Cloud systems (both public and private) to be federated through the use of specialist gateways. We describe how two physical sites, in the UK and the US, can be federated in a seamless way using this system.	auction algorithm;centralized computing;cloud computing;common criteria;darknet market;emergence;experiment;ibm notes;itil;job stream;oracle call interface;outsourcing;seamless3d;service-level agreement;tuple space;vendor lock-in	Ioan Petri;Thomas H. Beach;Mengsong Zou;Javier Diaz Montes;Omer F. Rana;Manish Parashar	2014	2014 IEEE International Conference on Cloud Engineering	10.1109/IC2E.2014.9	cloud computing security;simulation;cloud computing;cloud testing;business;world wide web;computer security	HPC	-29.805690824673007	59.336065149403225	60167
881fe6f8eda7d0eec0be3388d6bd8eb425ac3b97	investigating resource interference and scaling on multitenant paas clouds		Platform as a Service (PaaS) clouds are capable of both transparently allocating computing resources as well as providing part of the software stack and related services to tenant applications that execute on a subset of available cloud VMs. To deal with increased load, PaaS clouds enable applications to scale out, by creating extra instances, or scale up, by adding resources to the existing instances. However, good scalability is not necessarily attainable; in this paper we investigate the reasons for this. In particular, we propose a mathematical model that describes CPU allocation per tenant depending on interference from other tenants on the same VM, which we use to make predictions and confirm them in a variety of experimental situations. Furthermore, using a set of cloud tenants that target specific resources, we propose and evaluate a methodology for profiling the resource-intensiveness of cloud applications that uses slowdown in the presence of a resource-intensive cloud burner.		Panagiotis Patros;Stephen A. MacKay;Kenneth B. Kent;Michael Dawson	2016			slowdown;software;scalability;cloud computing;profiling (computer programming);real-time computing;computer science;scaling;multitenancy	HPC	-22.728120082901768	60.43056023124962	60394
ca24b16b898ecc0835794c2d55f458011a4c8fb3	join and multi-join processing in data integration systems	memory management;data integrity;query processing;symmetric hash join;multi user;multi join;initial response time;blocking execution model;data transfer;data integration	Query processing in a data integration system is complicated by a lack of quality statistics about the data, unpredictable and bursty data transfer rates, and slow or unavailable data sources. Conventional query processing algorithms, which are based on a blocking execution model, are no longer attractive because of their long initial response time. Moreover, the execution engine may be stalled by slow data delivery rates or unavailable data sources. In this paper, we adopt a non-blocking execution model for evaluating queries. We propose a symmetric partition-based join algorithm, called AJoin, that can operate with small memory requirement, produce first few answer tuples quickly, and blocks only when all available data have been examined. We also examine heuristics to manage the partitions and address the memory management issues of AJoin. To evaluate multi-join query plans, we also proposed two new strategies, m-AJoin and Pm-AJoin. Both strategies evaluate each join operation using AJoin. While m-AJoin accesses data from remote sources in its entirety, Pm-AJoin accesses remote data in chunks of smaller partitions. Our performance study shows the effectiveness of the proposed approaches for join and multi-join processing in a multi-user data integration system.		Kian-Lee Tan;Pin-Kwang Eng;Beng Chin Ooi;Ming Zhang	2002	Data Knowl. Eng.	10.1016/S0169-023X(01)00055-6	hash join;recursive join;real-time computing;computer science;data integration;data integrity;data mining;database;memory management	DB	-21.08154450431092	50.342124845143395	60526
04b40f57698d2402ba7272e514ceff4882954d3b	performance impact analysis of two generic group communication apis	jgroups;pattern clustering;performance analysis application software software engineering performance gain resource management load management power system reliability computer applications software performance marketing and sales;application software;spread;four node cluster;middleware system generic group communication application program interface four node cluster;pattern clustering application program interfaces;resource management;application program interface;group communication;software engineering;computer applications;software performance;impact analysis;application program interfaces;performance analysis;load management;performance gain;generic group communication;middleware;power system reliability;jgcs;hedera;middleware system;hedera performance analysis group communication jgroups spread jgcs;marketing and sales	This paper presents an analysis of the performance impact of two generic group communication APIs, namely Hedera and jGCS. The analysis was carried out in a four-node cluster by comparing the performance of different configurations of two well-known group communication middleware systems (i.e., JGroups and Spread), under different message sizes, both in standalone mode and when used as plug-ins for the two generic APIs. The results show that there are significant variations in the performance overhead imposed by each generic API on both JGroups and Spread, and that those differences are strongly related to variations in message size and the way each generic API is implemented. The paper also discusses whether it would be worth migrating from a standalone group communication implementation to a generic API.	application programming interface;jgroups;middleware;overhead (computing)	Leandro Sales;Henrique Teofilo;Jonathan D'Orleans;Nabor das Chagas Mendonça;Rafael G. Barbosa;Fernando Trinta	2009	2009 33rd Annual IEEE International Computer Software and Applications Conference	10.1109/COMPSAC.2009.128	embedded system;application software;real-time computing;simulation;software performance testing;application programming interface;communication in small groups;computer science;resource management;operating system;software engineering;middleware;database;computer applications;computer security	SE	-32.57939660744307	48.899609791441925	60732
6247b3a04870333560959e660ee29bd343ba6d97	an application-semantics-based relaxed transaction model for internetware	static checking;internetware service composition relaxed atomicity relaxed transaction mode;transaction management;service composition;internetware;huang tao ding xiaoning wei jun 应用语义学 非严格事务模型 机器翻译 网构软件 an application semantics based relaxed transaction model for internetware;satisfiability;relaxed atomicity;relaxed transaction mode;exception handling;composite structure;transaction processing;failure recovery;service composition relaxed atomicity relaxed transaction mode internetware	An internetware application is composed by existing individual services, while transaction processing is a key mechanism to make the composition reliable. The existing research of transactional composite service (TCS) depends on the analysis to composition structure and exception handling mechanism in order to guarantee the relaxed atomicity. However, this approach cannot handle some application-specific requirements and causes lots of unnecessary failure recoveries or even aborts. In this paper, we propose a relaxed transaction model, including system mode, relaxed atomicity criterion, static checking algorithm and dynamic enforcement algorithm. Users are able to define different relaxed atomicity constraint for different TCS according to application-specific requirements, including acceptable configurations and the preference order. The checking algorithm determines whether the constraint can be guaranteed to be satisfied. The enforcement algorithm monitors the execution and performs transaction management work according to the constraint. Compared to the existing work, our approach can handle complex application requirements, avoid unnecessary failure recoveries and perform the transaction management work automatically.	algorithm;atomicity (database systems);business logic;correctness (computer science);exception handling;requirement;service composability principle;service-oriented modeling;transaction processing	Tao Huang;Xiaoning Ding;Jun Wei	2006	Science in China Series F: Information Sciences	10.1007/s11432-006-2028-0	exception handling;real-time computing;transaction processing;distributed transaction;computer science;database;distributed computing;atomicity;satisfiability	AI	-24.00955105308609	48.00036494352781	60780
caa71fa9f463090c2abdd578734add49c2b67170	bio-inspired monitoring of pervasive environments	topology;resource monitoring;p2p overlay networks;bioinspired selforganization mechanisms;pervasive environments;network topology;computer architecture;monitoring;pervasive computing paradigm;high level policy based management operations bioinspired monitoring pervasive environments pervasive computing paradigm resource discovery mechanisms p2p overlay networks resource monitoring bioinspired selforganization mechanisms;ubiquitous computing;middleware;optimization;bioinspired monitoring;peer to peer computing;resource discovery mechanisms;context;monitoring network topology context topology middleware computer architecture optimization;high level policy based management operations;ubiquitous computing peer to peer computing	Successful deployment of the pervasive computing paradigm is based on the exploitation of the multitude of participating devices and associated data. It becomes therefore evident that there is a necessity to provide optimal resource discovery mechanisms, the effectiveness of which will constitute the foundation for the efficient operation of pervasive computing environments. To mitigate the drawbacks brought by the inherent nature of pervasive environments, i.e. dynamicity, heterogeneity and scalability of the network infrastructure, we propose to employ P2P overlay networks that lead to more manageable topologies and optimize resource monitoring by scaling down the degree of complexity. In particular, we take advantage of bio-inspired self-organization mechanisms to construct reliable P2P overlays and thus provide more robust and adaptive monitoring solutions. High-level policy-based management operations driven by monitored context information enable a further level of runtime adaptation and optimization, subject to the overall application requirements. We report here on our ongoing work in this research area.	algorithm;british informatics olympiad;data structure;experiment;image scaling;mathematical optimization;overhead (computing);overlay network;pervasive informatics;programming paradigm;requirement;scalability;self-organization;skolem normal form;software deployment;ubiquitous computing	Apostolos Malatras;Fei Peng;Béat Hirsbrunner	2011	2011 IEEE International Conference on Pervasive Computing and Communications Workshops (PERCOM Workshops)	10.1109/PERCOMW.2011.5766908	real-time computing;context-aware pervasive systems;human–computer interaction;computer science;operating system;middleware;distributed computing;ubiquitous computing;network topology;computer network	DB	-30.083907932285626	57.60233645848442	61042
232fca6c5a705f68122614bd4cfbd826d9aad3ec	collaborative and integrated network and systems management: management using grid technologies		Current Internet trends are moving towards decentralization of computation, storage, and resources. Supporting network management for such a vast and a highly complex system has become a challenging issue. A management platform has to sufficiently support decentralization, collaboration, and integration. Grid technologies have the potential to serve as management architecture due to the support of the above features. In this paper, we developed a collaborative network management architecture leveraging the key features of grid technology. Benefiting from this integration, we were able to show that multiple management tasks can be integrated and completed in parallel. This assures the management scalability and efficiency. We also showed that the management information at different networking domains can freely consume the computational resources provided through the grid interface while being executed. Grid interface has guaranteed scalability and reliability for the network management tasks. We have simulated the system prototype and closely studied its efficiency.	collaborative network;complex system;computation;computational resource;generic programming;geographic coordinate system;grid computing;management information system;prototype;scalability;simulation;software deployment;systems management;virtual organization (grid computing)	Mohammad Hassan	2013	Int. Arab J. Inf. Technol.		element management system;systems management;simulation;network management station;computer science;knowledge management;operating system;distributed computing;structure of management information;computer network	HPC	-30.595528209249945	51.629197665868986	61089
4fd46fe297ad3d260ebd1e754dab9aa4f333899d	developing a cloud computing charging model for high-performance computing resources	front end;measurement;resource allocation;biological system modeling;data storage capacity;cloud computing charging model;batch interface;resource allocation grid computing;data storage;supercomputing resource provider;computational modeling;high performance computing resources;system utilization charging model;clouds;cloud interface;high performance computer;grid interface;grid computing;cloud interface cloud computing charging model high performance computing resources supercomputing resource provider system utilization charging model data storage capacity batch interface grid interface;supercomputers;memory;cloud computing;clouds cloud computing computational modeling supercomputers biological system modeling measurement memory	This paper examines the economics of cloud computing charging from the perspective of a supercomputing resource provider offering its own resources. To evaluate the competitiveness of our computing center with cloud computing resources, we develop a comprehensive system utilization charging model similar to that used by Amazon EC2 and apply the model to our current resources and planned procurements. For our current resource, we find that charging for computational time may be appropriate, but that charging for data traffic between the supercomputer and the storage/front-end systems would result in negligible additional revenue. Similarly, charging for data storage capacity at currently typical commercial rates yields insufficient revenue to offset the acquisition and operation of the storage. However, when we extend the analysis to a capacity cluster scheduled for deployment in the first half of 2010 that will be made available to users through batch, Grid, and cloud interfaces, we find that the resource will be competitive with current and anticipated cloud rates.	amazon elastic compute cloud (ec2);cloud computing;computer data storage;software deployment;supercomputer;time complexity	Matthew Woitaszek;Henry M. Tufo	2010	2010 10th IEEE International Conference on Computer and Information Technology	10.1109/CIT.2010.72	real-time computing;simulation;online charging system;cloud computing;resource allocation;computer science;front and back ends;operating system;computer data storage;distributed computing;utility computing;memory;computational model;grid computing;measurement	HPC	-25.26185426899555	59.687647064466695	61198
931aa54bf61d53335c4f6e62a3533ba4f4aaeb4c	magcat: an agent-based metadata service for data grids	attractive technology;discovery data;grid middleware;increasing demand;agent-based metadata service;extended data set;efficient access;implementation challenge;computer grid;data discovery;magcat multi-agent system;data grids;middleware;authentication;data sets;multi agent system;computer architecture;multi agent systems;database systems;meta data;data grid;data security;grid computing;distributed environment;protocols;publishing;environmental management;distributed computing;mobile agent	Facing the increasing demand for data sharing in distributed environments, computer grids became an attractive technology to manage extended data sets, providing efficient access, publishing, and data discovery. In this paper, we present the structure and implementation of MagCat multi-agent system, that provides a service for publishing and discovery data in grids through metadata. This service is being developed in context of MAG, a grid middleware that explores the mobile agent paradigm as a way to overcome the design and implementation challenges of constructing a grid middleware	agent-based model;authentication;authorization;computer data storage;data access;data model;grid computing;heuristic (computer science);lightweight directory access protocol;middleware;mobile agent;multi-agent system;ontology (information science);open grid services architecture;programming paradigm;scalability;xslt/muenchian grouping	Bysmarck Barros de Sousa;Francisco José da Silva e Silva;Mário Meireles Teixeira;Gilberto Cunha Filho	2006	Sixth IEEE International Symposium on Cluster Computing and the Grid (CCGRID'06)	10.1109/CCGRID.2006.153	communications protocol;computer science;operating system;multi-agent system;middleware;data grid;mobile agent;authentication;database;publishing;distributed computing;data security;metadata;world wide web;data set;grid computing;distributed computing environment	HPC	-31.35478073751437	50.77820851626428	61252
0fac09dac70b07532437572b365c08de4b087df5	parallelizing a defect detection and categorization application	disk resident dataset;parallelizing data mining algorithm;resource allocation;distributed processing;scientific data;physics computing molecular dynamics method resource allocation data mining data analysis;molecular dynamics simulation;data mining;physics computing;molecular dynamics method;data analysis;data mining algorithm;high level interfaces;molecular dynamic;load balance;categorization application;md simulation;parallelizing data mining algorithm defect detection categorization application molecular dynamics simulation disk resident dataset high level interfaces load balance;defect detection	This paper presents a case study in creating a parallel and scalable implementation of a scientific data analysis application. We focus on a defect detection and categorization application which analyzes datasets produced by molecular dynamics (MD) simulations. In parallelizing this application, we had the following three goals. First, we obviously wanted to achieve high parallel efficiency. Second, we wanted to create an implementation that can scale to disk-resident datasets. Third, we wanted to create an easy to maintain and modify implementation, which is possible only through using high-level interfaces. We used a number of techniques for organizing the input data, achieving load balance, and efficiently parallelizing the step for updating and matching with the defect catalog. To meet our third goal, we used a system called FREERIDE (FRamework for Rapid Implementation of Datamining Engines), which was originally developed for parallelizing data mining algorithms. We have carried out a detailed evaluation of our implementation. The main observations from our experiments are as follows: 1) our implementation achieves high parallel efficiency, 2) the execution time remains proportional to the amount of computation even as the dataset becomes disk-resident, and 3) our scheme for load balancing and the method we use for parallelizing updating and matching of the defect catalog are crucial for parallel efficiency of the defect categorization phase.	algorithm;automatic parallelization;categorization;computation;data mining;experiment;high- and low-level;load balancing (computing);molecular dynamics;organizing (structure);parallel computing;run time (program lifecycle phase);scalability;simulation;software bug;speedup	Leonid Glimcher;Gagan Agrawal;Sameep Mehta;Ruoming Jin;Raghu Machiraju	2005	19th IEEE International Parallel and Distributed Processing Symposium	10.1109/IPDPS.2005.332	molecular dynamics;parallel computing;resource allocation;computer science;load balancing;theoretical computer science;operating system;data mining;database;distributed computing;data analysis;data	HPC	-20.529143489307387	54.10009311659108	61294
749a202750e455281597fa9d8f0bb3c3ec6a675a	combining genetic strategy with least-loaded to improve dynamic load balancing in corba	genetics		common object request broker architecture;load balancing (computing)	Francisco Javier Luna Rosas;Rogelio Alcántara Silva	2005			parallel computing;computer science;distributed computing;common object request broker architecture;dynamic load testing	Robotics	-28.683554932545178	46.91488186269421	61645
c08ff4cc80e9fb73fb847ba7c75c16ab27a0e075	trading replication consistency for performance and availability: an adaptive approach	distributed algorithms;replication system;protocols;optimisation;replication;electronic commerce;collaborative work;network condition;data integrity;application software;building block;availability;perforation;prototypes;performance;wide area application;system availability;suboptimal performance;prototype system;adaptation model;window protocol;consistency level;adaptation;performance analysis;bounded consistency;prototype system replication system wide area application suboptimal performance system availability consistency level heterogeneous workload network condition window protocol;optimisation replicated databases wide area networks data integrity distributed algorithms;availability protocols application software delay adaptation model proposals performance analysis prototypes collaborative work electronic commerce;proposals;replicated databases;wide area networks;heterogeneous workload	The wide area data consistency become increasingly important in the GTEO as a result of using caching (Amiri,2003), middleware (Emmanuel,2004)(Jimenez,2003), edge server and database replication (Bettina,2000)(Decker,2003)(Zhang,2003) technologies. It is difficult for enterprise usage copies with local / remote ODS object partial dependency relationships to maintain their consistency. Today’s data integration solution based on document and application centralize conceptual will take more effort in application system. In addition, more and more database middleware API can’t truly reduce coding effort and maintenance costs. A novel application platform was proposed which consists of active, real time, automation, and global routing properties. The result can expand enterprise database system with ODS dependency replica correction maintenance abilities and provide adjustable update routing policies based on ODS dependency replicas usage mining results.	application programming interface;centralisation;content delivery network;data mining;database;middleware;operational data store;real-time computing;replication (computing);routing;server (computing)	Chi Zhang;Zheng Zhang	2003		10.1109/ICDCS.2003.1203520	e-commerce;communications protocol;distributed algorithm;availability;replication;application software;real-time computing;performance;computer science;consistency model;operating system;data integrity;database;distributed computing;prototype;eventual consistency;computer security;computer network;adaptation	DB	-24.732053140618213	52.17025006309721	61665
8c62470c7d2b93eb281d818d121c0b41465c847d	managing and analysing genomic data using hpc and clouds		Database management techniques using distributed processing services have evolved to address the issues of distributed, heterogeneous data collections held across dynamic, virtual organisations [1-3]. These techniques, originally developed for data grids in domains such as high-energy particle physics [4], have been adapted to make use of the emerging cloud infrastructures [5].		Bartosz Dobrzelecki;Amrey Krause;Michal Piotrowski;Neil P. Chue Hong	2011		10.1007/978-3-642-20045-8_13	bioinformatics	HPC	-30.290580045281462	50.699052686071134	61943
916eaf126e77e738cc3191ea65cb7df4ee5d01ce	parastack: efficient hang detection for mpi programs at large scale		While program hangs on large parallel systems can be detected via the widely used timeout mechanism, it is difficult for the users to set the timeout - too small a timeout leads to high false alarm rates and too large a timeout wastes a vast amount of valuable computing resources. To address the above problems with hang detection, this paper presents ParaStack, an extremely lightweight tool to detect hangs in a timely manner with high accuracy, negligible overhead with great scalability, and without requiring the user to select a timeout value. For a detected hang, it provides direction for further analysis by telling users whether the hang is the result of an error in the computation phase or the communication phase. For a computation-error induced hang, our tool pinpoints the faulty process by excluding hundreds and thousands of other processes. We have adapted ParaStack to work with the Torque and Slurm parallel batch schedulers and validated its functionality and performance on Tianhe-2 and Stampede that are respectively the world's current 2nd and 12th fastest supercomputers. Experimental results demonstrate that ParaStack detects hangs in a timely manner at negligible overhead with over 99% accuracy. No false alarm is observed in correct runs taking 66 hours at scale of 256 processes and 39.7 hours at scale of 1024 processes. ParaStack accurately reports the faulty process for computation-error induced hangs.	computation;fastest;hang (computing);overhead (computing);scalability;slurm;supercomputer;torque	Hongbo Li;Zizhong Chen;Rajiv Gupta	2017		10.1145/3126908.3126938	hang;parallel computing;false alarm;distributed computing;scalability;timeout;real-time computing;computer science;computation	HPC	-22.908264548376465	55.14241212216581	62135
236b23efa8af0e4527f487b9152c66e89617d199	speaker: split-phase execution of application containers		Linux containers have recently gained more popularity as an operating system level virtualization approach for running multiple isolated OS distros on a control host or deploying large scale microservicebased applications in the cloud environment. The wide adoption of containers as an application deployment platform also attracts attackers’ attention. Since the system calls are the entry points for processes trapping into the kernel, Linux seccomp filter has been integrated into popular container management tools such as Docker to effectively constrain the system calls available to the container. However, Docker lacks a method to obtain and customize the set of necessary system calls for a given application. Moreover, we observe that a number of system calls are only used during the short-term booting phase and can be safely removed from the long-term running phase for a given application container. In this paper, we propose a container security mechanism called SPEAKER that can dramatically reduce the number of available system calls to a given application container by customizing and differentiating its necessary system calls at two different execution phases, namely, booting phase and running phase. For a given application container, we first separate its execution into booting phase and running phase and then trace the invoked system calls at these two phases, respectively. Second, we extend the Linux seccomp filter to dynamically update the available system calls when the application is running from the booting phase into the running phase. Our mechanism is non-intrusive to the application running in the container. We evaluate SPEAKER on the popular web server and data store containers from Docker hub, and the experimental results show that it can successfully reduce more than 50% and 35% system calls in the running phase for the data store containers and the web server containers, respectively, with negligible performance overhead.	attack surface;booting;cloud computing;data store;docker;lxc;linux;operating system;operating-system-level virtualization;overhead (computing);server (computing);software deployment;system call;usb hub;web container;web server;seccomp	Lingguang Lei;Jianhua Sun;Kun Sun;Chris Shenefiel;Rui Ma;Yuewu Wang;Qi Li	2017		10.1007/978-3-319-60876-1_11	system call;software deployment;virtualization;split-phase electric power;real-time computing;web server;cloud computing;booting;computer science	OS	-26.93881644623116	58.58143261505723	62149
190788671c96e79a3b4a28c98acf31bd1928e450	low power mode in cloud storage systems	instruments;storage system;low power mode;availability;probability density function;redundancy mechanism;full power operation;greedy algorithms;cloud data services low power mode cloud storage systems large scale distributed storage systems redundancy mechanism low utilization time intervals full power operation;distributed storage system;spectrum;data mining;large scale;low power;internet;redundancy;energy consumption;system design;clouds;large scale distributed storage systems;energy storage;clouds energy consumption power system reliability large scale systems costs redundancy availability energy storage instruments energy management;distributed databases;normal operator;power system reliability;cloud storage systems;digital storage;power consumption;redundancy digital storage internet large scale systems;low utilization time intervals;power demand;cloud data services;meteorology;large scale systems;energy management	We consider large scale, distributed storage systems with a redundancy mechanism; cloud storage being a prime example. We investigate how such systems can reduce their power consumption during low-utilization time intervals by operating in a low-power mode. In a low power mode, a subset of the disks or nodes are powered down, yet we ask that each data item remains accessible in the system; this is called full coverage. The objective is to incorporate this option into an existing system rather than redesign the system. When doing so, it is crucial that the low power option should not affect the performance or other important characteristics of the system during full-power (normal) operation. This work is a comprehensive study of what can or cannot be achieved with respect to full coverage low power modes. The paper addresses this question for generic distributed storage systems (where the key component under investigation is the placement function of the system) as well as for specific popular system designs in the realm of storing data in the cloud. Our observations and techniques are instrumental for a wide spectrum of systems, ranging from distributed storage systems for the enterprise to cloud data services. In the cloud environment where low cost is imperative, the effects of such savings are magnified by the large scale.	cloud computing;cloud storage;clustered file system;data item;imperative programming;low-power broadcasting;power supply	Danny Harnik;Dalit Naor;Itai Segall	2009	2009 IEEE International Symposium on Parallel & Distributed Processing	10.1109/IPDPS.2009.5161231	embedded system;spectrum;availability;probability density function;greedy algorithm;parallel computing;real-time computing;the internet;computer science;operating system;distributed computing;redundancy;energy storage;distributed database;normal operator;systems design;energy management	HPC	-26.014579278429338	60.103849592305316	62274
8ad3876aeb1576c29068b10239212d7d0b24e826	revising openstack to operate fog/edge computing infrastructures	databases;prototypes;servers;buildings;cloud computing	Academic and industry experts are now advocating for going from large-centralized Cloud Computing infrastructures to smaller ones massively distributed at the edge of the network. Among the obstacles to the adoption of this model is the development of a convenient and powerful IaaS system capable of managing a significant number of remote data-centers in a unified way. In this paper, we introduce the premises of such a system by revising the OpenStack software, a leading IaaS manager in the industry. The novelty of our solution is to operate such an Internet-scale IaaS platform in a fully decentralized manner, using P2P mechanisms to achieve high flexibility and avoid single points of failure. More precisely, we describe how we revised the OpenStack Nova service by leveraging a distributed key/value store instead of the centralized SQL backend. We present experiments that validate the correct behavior and gives performance trends of our prototype through an emulation of several data-centers using Grid'5000 testbed. In addition to paving the way to the first large-scale and Internet-wide IaaS manager, we expect this work will attract a community of specialists from both distributed system and network areas to address the Fog/Edge Computing challenges within the OpenStack ecosystem.	centralized computing;cloud computing;distributed computing;ecosystem;edge computing;emulator;experiment;linux;national research and education network;nosql;open-source software;peer-to-peer;prototype;redis;reliability engineering;sql;scalability;scheduling (computing);single point of failure;testbed;trusted computer system evaluation criteria	Adrien Lèbre;Jonathan Pastor;Anthony Simonet;Frédéric Desprez	2017	2017 IEEE International Conference on Cloud Engineering (IC2E)	10.1109/IC2E.2017.35	simulation;engineering;world wide web;computer security	HPC	-25.47253582150421	54.574950089840854	62281
754cc5d677f94cfd75cb704a452137c93184bb68	combined vertical and horizontal autoscaling through model predictive control		Meeting performance targets of co-located distributed applications in virtualized environments is a challenging goal. In this context, vertical and horizontal scaling are promising techniques; the former varies the resource sharing on each individual machine, whereas the latter deals with choosing the number of virtual machines employed. State-of-the-art approaches mainly apply vertical and horizontal scaling in an isolated fashion, in particular assuming a fixed and symmetric load balancing across replicas. Unfortunately this may result unsatisfactory when replicas execute in environments with different computational resources.	autoscaling	Emilio Incerto;Mirco Tribastone;Catia Trubiani	2018		10.1007/978-3-319-96983-1_11	horizontal and vertical;shared resource;load balancing (computing);scaling;model predictive control;autoscaling;virtual machine;computer science;distributed computing	AI	-19.757858840073737	59.09601825657468	62367
3a7555b95a0293ee89187b995c3403aca04a3a98	enabling large-scale biomolecular conformation search with replica exchange statistical temperature molecular dynamics (restmd) over hpc and cloud computing resources	histograms;temperature distribution heuristic algorithms computational modeling scalability histograms benchmark testing;restmd;lammps;enhanced conformational sampling;computational modeling;sampling methods bioinformatics cloud computing data handling molecular biophysics molecular dynamics method parallel processing;heuristic algorithms;scalability;lammps enhanced conformational sampling restmd hadoop mapreduce;temperature distribution;hadoop mapreduce;benchmark testing;geni distributed computing environment large scale biomolecular conformation search replica exchange statistical temperature molecular dynamics restmd cloud computing resources parallel tempering version lammps distributed cyber infrastructure amazon ec2 distributed computing infrastructure high performance computing cluster systems hpc cluster systems advanced sampling protocol replica exchange algorithm charmm public molecular dynamics packages hadoop mapreduce based restmd nationwide network virtual organization	We present the latest development and experimental simulation studies of Statistical Temperature Molecular Dynamics (STMD) and its parallel tempering version, Replica Exchange Statistical Temperature Molecular Dynamics (RESTMD). Our main contributions are i) introduction of newly implemented STMD in LAMMPS, ii) use of large scale distributed cyber infrastructure including Amazon EC2 and the nationwide distributed computing infrastructure GENI, in addition to High-performance Computing (HPC) cluster systems, and iii) benchmark and simulation results highlighting advantages and potentials of STMD and RESTMD for challenging large-scale bio molecular conformational search. In this work, we attempt to provide convincing evidence that RESTMD, combining two advanced sampling protocols, STMD and the replica exchange algorithm, offers various advantages over not only conventional ineffective approaches but also other enhanced sampling methods. Interestingly, RESTMD has benefits over the most popular Replica Exchange Molecular Dynamics (REMD) as an application maximizing its capacity in HPC environments. For example, RESTMD alleviates the need of a large number of replicas which is unavoidable in REMD and is flexible in order to exploit the maximum amount of available computing power of a cluster system. Continuing our recent effort in which RESTMD was implemented with a community molecular dynamics package, CHARMM, and the Hadoop MapReduce, in this work, we report latest development outcomes. First of all, we plugged the implementation of STMD into LAMMPS, one of the most popular public molecular dynamics packages. This is expected to position STMD and RESTMD appealing to investigators from a broad range of life science fields. Secondly, Hadoop MapReduce-based RESTMD is now able to run on Amazon EC2 and the nationwide network virtual organization, the GENI distributed computing environment. Thirdly, in order to find optimized parameters for RESTMD simulations, simulation results using test systems, water and solvated cram bin, were obtained and presented. These results, despite of relatively small sizes and short time scale trajectories, serve to underscore merits and potentials of STMD and RESTMD with respect to the strength in algorithmic advantages as well as efficient utilization of distributed resources.	amazon elastic compute cloud (ec2);apache hadoop;benchmark (computing);british informatics olympiad;charmm;cloud computing;clustered file system;distributed computing environment;greedy algorithm;large-scale atomic/molecular massively parallel simulator;mapreduce;molecular dynamics;ncr cram;parallel tempering;sampling (signal processing);simulation;virtual organization (grid computing)	Nayong Kim;Richard Platania;Wei Huang;Chris Knight;Tom Keyes;Seung-Jong Park;Joohyun Kim	2015	2015 IEEE 29th International Conference on Advanced Information Networking and Applications Workshops	10.1109/WAINA.2015.115	benchmark;parallel computing;scalability;computer science;theoretical computer science;operating system;database;histogram;distributed computing;computational model;statistics	HPC	-21.302696445539347	54.693913644168845	62758
0aab94e0bdb02dceb3600cae0339d63dcfed9fb5	empirical analysis of database server scalability using an n-tier benchmark with read-intensive workload	distributed system;performance evaluation;empirical analysis;database replication;application server;rubbos;n tier applications;consistency management;interaction pattern;middleware;experimental evaluation;distributed systems;database management system;bottleneck	The performance evaluation of database servers in N-tier applications is a serious challenge due to requirements such as non-stationary complex workloads and global consistency management when replicating database servers. We conducted an experimental evaluation of database server scalability and bottleneck identification in N-tier applications using the RUBBoS benchmark. Our experiments are comprised of a full scale-out mesh with up to nine database servers and three application servers. Additionally, the fourtier system was run in a variety of configurations, including two database management systems (MySQL and PostgreSQL), two hardware node types (normal and low-cost), and two database replication techniques (C-JDBC and MySQL Cluster). In this paper we present the analysis of results generated with a read-intensive interaction pattern (browse-only workload) in the client emulator. These empirical data can be divided into two kinds. First, for a relatively small number of servers, we find simple hardware resource bottlenecks. Consequently, system throughput increases with an increasing number of database (and application) servers. Second, when sufficient hardware resources are available, non-obvious database related bottlenecks have been found that limit system throughput. While the first kind of bottlenecks shows that there are similarities between database and application/web server scalability, the second kind of bottlenecks shows that database servers have significantly higher sophistication and complexity that require in-depth evaluation and analysis.	application server;benchmark (computing);bottleneck (software);browsing;database server;emulator;experiment;full scale;jdbc;multitier architecture;mysql cluster;performance evaluation;postgresql;replication (computing);requirement;scalability;server (computing);stationary process;throughput;web server	Simon Malkowski;Deepal Jayasinghe;Markus Hedwig;Junhee Park;Yasuhiko Kanemasa;Calton Pu	2010		10.1145/1774088.1774449	real-time computing;database server;database transaction;database tuning;computer science;operating system;middleware;database;distributed computing;world wide web;distributed database;physical data model;database testing;application server;server;replication	DB	-22.779272261823657	54.240334874862036	62959
16902f6f0fa2200b458c7827295979c07470b3af	specifying data availability in multi-device file systems	file system			John Wilkes;Raymie Stata	1990		10.1145/504136.504139	fork;self-certifying file system;torrent file;indexed file;device file;computer file;zap file;class implementation file;stub file;versioning file system;unix file types;ssh file transfer protocol;journaling file system;open;distributed file system;data file;file system fragmentation;design rule for camera file system;file area network;file control block	OS	-19.17316610464751	51.779704017647994	63137
daa11c7519081df28005b0c5eff6275604a90d78	real-time application processing for fpga-based resilient embedded systems in harsh environments		Real-time embedded systems nowadays get employed in harsh environments such as space, nuclear sites to carry out critical operations. Along with the traditional software based (CPU) execution, FPGAs are now also emerging as a bright prospect to accomplish such routines. However, these platforms are often get plagued by faults generated due to the high radiations in such environments. As a result, the real-time applications running on the platform could also get jeopardized. Thus, efficient execution of a set of hard real-time applications on reconfigurable systems with anomaly detection and recovery mechanism is inevitable. This work aims at tackling such problem with a “healing” approach for extreme environments. Initially, the applications are intelligently partitioned for hardware and software execution, then attempts have been made to schedule hardware applications with intermittent preemption point. Upon detecting any abnormality on such distinct points, our approach orchestrates a healing mechanism to remediate the scenario without hampering the pre-determined schedule. Experimental validation of our proposed method reveals its effectiveness.	anomaly detection;bitstream;central processing unit;embedded system;field-programmable gate array;list of code lyoko episodes;preemption (computing);real-time clock;real-time computing;real-time transcription;schedule (computer science);scheduling (computing);sensor;simulation	Sangeet Saha;Shoaib Ehsan;Adrian Stoica;Rustam Stolkin;Klaus D. McDonald-Maier	2018	2018 NASA/ESA Conference on Adaptive Hardware and Systems (AHS)	10.1109/AHS.2018.8541449	real-time computing;field-programmable gate array;anomaly detection;embedded system;scheduling (computing);schedule;software;computer science;preemption	Embedded	-22.05996364919544	54.98745359261233	63545
eace5d4cf9421b61fbc244e308d8a0caebf1d93f	a simulation study of database application in a distributed system: data replication	data replication;distributed system		distributed computing;replication (computing);simulation	Thomas J. Liu	2003			database application;computer science;distributed computing;database;replication (computing)	DB	-28.57965760329077	47.010161027222026	63592
c82406278a9b9c52da2edd50b2dc75ae44d07031	processing of medical images in virtual distributed environment	virtualization;pacs;p2p;grid;distribution function;pilot project;medical image;operating system;distributed environment;communication channels;high performance;high speed	The processing of medical images within a PACS system depends on high capacity of communication channels and high performance of computational resources. We introduce pilot project utilizing grid technology to distribute functionality of PACS system to several machines located in distant places which allows economizing utilization of network channels. We also discuss benefits and disadvantages of virtualization techniques allowing to separate physical machine capabilities from the operating system. We compare this pilot project utilizing high speed CESNET 2 network with similar mature projects based mainly on P2P secure connection, centralized system and proprietary protocols.	centralized computing;computational resource;cryptographic protocol;operating system;peer-to-peer;picture archiving and communication system	Tomás Kulhánek;Milan Sárek	2009		10.1145/1551722.1551732	embedded system;real-time computing;virtualization;simulation;computer science;operating system;distribution function;peer-to-peer;database;picture archiving and communication system;grid;world wide web;distributed computing environment;computer network;channel	HPC	-25.117531993851486	50.46826161794805	63986
1482aaf86f01ebbbf575a60cf0540b564a2eb292	relational database support for event-based middleware functionality	distributed application;database system;distributed transactions;database;data type;relational database;access control policy;distributed environment;publish subscribe;complex event processing;relational database management system;middleware;enterprise service bus;service oriented architecture;queues	Many of the popular relational database management systems (RDBMS) provide features for operating in a distributed environment, such as remote table queries and updates, and support for distributed transactions. In practice, however, much application software targets a more minimal set of functionality than is offered by the SQL standards. Independently of the database tier, engineering concepts such as the enterprise service bus and service oriented architecture have led to the development of communication middleware to support distributed applications. For applications that require reliable delivery of messages, complex event processing, and integrated archiving of data, impedance mismatches are likely to emerge between the database system and the communications middleware---for example with respect to data-types, event filtering that is based on information in the database, and in terms of coordinating access control policy. This paper describes event-based middleware functionality that is supported directly within the database system. In contrast to previous approaches (e.g. being able to name remote tables in SQL statements), the programming of event-based communication operations within the database is explicit. We present initial performance results that compare an augmented PostgreSQL database system to an environment in which a database and an event-based middleware package are used side-by-side. These results demonstrate the viability of our approach.	access control;archive;characteristic impedance;complex event processing;distributed computing;distributed transaction;enterprise integration;enterprise service bus;impedance matching;middleware;multitier architecture;postgresql;relational database management system;reliable messaging;sql;service-oriented architecture	David M. Eyers;Luis Vargas;Jatinder Singh;Ken Moody;Jean Bacon	2010		10.1145/1827418.1827455	data definition language;middleware;database theory;relational database management system;database server;intelligent database;database transaction;database tuning;data type;distributed transaction;relational database;computer science;complex event processing;operating system;database model;service-oriented architecture;middleware;data mining;database;distributed computing;publish–subscribe pattern;view;database schema;distributed database;queue;alias;object-relational impedance mismatch;database testing;database design;distributed computing environment;spatiotemporal database	DB	-27.12047989998494	46.69777549001548	64185
a6a1b70305b27c556aac779fb65429db9c2e1ef2	eventually returning to strong consistency		Eventually and weakly consistent distributed systems have emerged in the past decade as an answer to scalability and availability issues associated with strong consistency semantics, such as linearizability. However, systems offering strong consistency semantics have an advantage over systems based on weaker consistency models, as they are typically much simpler to reason about and are more intuitive to developers, exhibiting more predictable behavior. Therefore, a lot of research and development effort is being invested lately into the re-engineering of strongly consistent distributed systems, as well as into boosting their scalability and performance. This paper overviews and discusses several novel directions in the design and implementation of strongly consistent systems in industries and research domains such as cloud computing, data center networking and blockchain. It also discusses a general trend of returning to strong consistency in distributed systems, when system requirements permit so.	bitcoin;cloud computing;consistency model;data center;distributed computing;eventual consistency;linearizability;requirement;scalability;strong consistency;system requirements	Marko Vukolic	2016	IEEE Data Eng. Bull.		data mining;strong consistency;computer science	OS	-25.540804217641035	54.20811593638403	64351
70a7aa1c8a95881b0448883bf879bf74411168f2	a qos-aware composition method supporting cross-platform service invocation in cloud environment	service composition;cloud environment;cross platform service invocation;journal article;qos	With the increasing popularity of cloud computing technologies, more and more service composition processes are enacted and executed in could environment. Compared with the various and approximately infinite application requirements from end users, the web services held by a cloud platform is usually limited. Therefore, it is often a challenging effort to develop a service composition, in such a situation that only part of the functional qualified candidate services could be found inside a cloud platform. In this situation, the absent services will be invocated in a cross-platform way outside the cloud platform. In view of this challenge, a QoS-aware composition method is investigated for supporting cross-platform service invocation in cloud environment. Furthermore, some experiments are deployed to evaluate the method presented in this paper.	cloud computing;experiment;ibm notes;quality of service;requirement;scalability;service composability principle;web service;world sudoku championship	Lianyong Qi;Wan-Chun Dou;Xuyun Zhang;Jinjun Chen	2012	J. Comput. Syst. Sci.	10.1016/j.jcss.2011.12.016	real-time computing;quality of service;computer science;cloud testing;distributed computing;world wide web	SE	-31.49717184185441	56.09689502947988	64416
145c4d94d04255a3e2ae1cf2fbb48b458d6e6c6d	expressing and enforcing distributed resource sharing agreements	alinear-programming model;information source;chained agreement;transitive availability;case study modeling resource;transitive agreement;existing resource management infrastructure;allocation problem;compatible resource;isp-level web proxy;increasing importance	Advances in computing and networking technology, and an explosion in information sources has resulted in a growing number of distributed systems being constructed out of resources contributed by multiple sources. Use of such resources is typically governed by sharing agreements between owning principals, which limit both who can access a resource and in what quantity. Despite their increasing importance, existing resource management infrastructures offer only limited support for the expression and enforcement of sharing agreements, typically restricting themselves to identifying compatible resources. In this paper, we present a novel approach building on the concepts of tickets and currencies to express resource sharing agreements in an abstract, dynamic, and uniform fashion. We also formulate the allocation problem of enforcing these agreements as alinear-programming model, automatically factoring the transitive availability of resources via chained agreements. A case study modeling resource sharing among ISP-level web proxies shows the benefits of enforcing transitive agreements: worst-case waiting times of clients accessing these proxies improves by up to two orders of magnitude.	best, worst and average case;distributed computing;information source;integer factorization;programming model;proxy server	Tao Zhao;Vijay Karamcheti	2000	ACM/IEEE SC 2000 Conference (SC'00)		distribution;shared resource;availability;parallel computing;real-time computing;computer science;resource management;operating system;soap;database;distributed computing;programming paradigm;programming language;java;information system;bandwidth;computer network;resource	HPC	-31.67627063222118	57.071990125906865	64440
57c167adf6c399f6f32aa147f291dc3fa819d36a	towards scalable and reliable grid networks	self organized network system;resource discovery;resource allocation;computer networks distributed computing grid computing high performance computing concurrent computing large scale systems complex networks computer network reliability throughput computational modeling;parallel and distributed computing;load balancing framework;grid networks;resource discovery grid networks collaborative computing environment load balancing framework self organized network system load distribution;self organization;load distribution;resource availability;load balance;networked systems;collaborative computing;grid computing;resource allocation computer network reliability grid computing;collaborative computing environment;computer network reliability	Improvements in computer and networking technologies over the past decades produced new type of collaborative computing environment called grid networks. Grid is a parallel and distributed computing network system that provides the ability to perform higher throughput computing by taking advantage of many computing resources available in the network. Therefore, to achieve a scalable and reliable grid network system, we need to efficiently distribute the load among the resources accessible on the network. In this paper, we present a distributed and scalable load- balancing framework for grid networks. The generated network system is self-organized and depends only on local information for load distribution and resource discovery. Simulation results show that the generated network system provides an effective, scalable, and reliable load-balancing scheme for the distributed resources accessible on grid networks.	distributed computing;grid network;high-throughput computing;load balancing (computing);parallel computing;scalability;self-organization;simulation;throughput	Osama Abu-Rahmeh;Princy Johnson	2008	2008 IEEE/ACS International Conference on Computer Systems and Applications	10.1109/AICCSA.2008.4493543	real-time computing;self-organization;semantic grid;resource allocation;computer science;weight distribution;load balancing;theoretical computer science;distributed computing;grid computing	HPC	-19.87787139704181	58.499299177863044	64460
51a4b2aa4c27ce213acde68a0e653b1e5c98230e	the design and demonstration of the ultralight testbed	grid computing sites;telecommunication network management computerised monitoring;data processing;ultralight testbed;testing computer networks physics computing next generation networking monitoring resource management data processing computer architecture kernel bandwidth;particle physics experiments;particle physics;next generation;network architecture;grid computing sites ultralight testbed particle physics experiments next generation global system;data intensive computing;computerised monitoring;grid computing;telecommunication network management;next generation global system;dynamic configuration	In this paper we present the motivation, the design, and a recent demonstration of the UltraLight testbed at SC|05. The goal of the Ultralight testbed is to help meet the data-intensive computing challenges of the next generation of particle physics experiments with a comprehensive, network- focused approach. UltraLight adopts a new approach to networking: instead of treating it traditionally, as a static, unchanging and unmanaged set of inter-computer links, we are developing and using it as a dynamic, configurable, and closely monitored resource that is managed from end-to-end. To achieve its goal we are constructing a next-generation global system that is able to meet the data processing, distribution, access and analysis needs of the particle physics community. In this paper we will first present early results in the various working areas of the project. We then describe our experiences of the network architecture, kernel setup, application tuning and configuration used during the bandwidth challenge event at SC|05. During this Challenge, we achieved a record-breaking aggregate data rate in excess of 150 Gbps while moving physics datasets between many Grid computing sites.	aggregate data;autonomous robot;central processing unit;communications protocol;computation;data rate units;data-intensive computing;distributed computing;egi;end-to-end principle;experiment;grid computing;heterogeneous element processor;köppen climate classification;large hadron collider;national research and education network;network architecture;next-generation network;offset binary;service-oriented device architecture;terabyte;testbed;uncompressed video	Harvey B. Newman;Dimitri Bourilkov;Julian J. Bunn;Richard Cavanaugh;Iosif Legrand;Steven H. Low;Shawn McKee;Dan Nae;Sylvain Ravot;Conrad Steenberg;Xun Su;Michael Thomas;Frank van Lingen;Yang Xia	2006	2006 3rd International Conference on Broadband Communications, Networks and Systems	10.1109/BROADNETS.2006.4374312	simulation;network architecture;data processing;computer science;theoretical computer science;operating system;data-intensive computing;distributed computing;grid computing;computer network	HPC	-30.63381400108307	50.79900218190797	64649
087f2d6a6278eb5612071c5e8b830548f93fa758	agent server technology for managing millions of agents	memory management;programming model;execution environment	In this paper, we describe technologies for an agent server capable of hosting millions of agents. The agent server needs a thread management mechanism, a memory management mechanism, and a recovery management mechanism. We have developed a framework and agent execution environment named Caribbean. First, we describe the programming model of Caribbean. Following the description, we explain technologies for managing millions of agents. Some application scenarios of real commercial systems using the technology are also introduced. We describe what we learned from the development of the real applications.		Gaku Yamamoto	2004		10.1007/11512073_1	real-time computing;simulation;engineering;database	ECom	-28.8451996016903	49.51193356238689	64699
01ee5e617677f3429ac0ec79ca16bd1b30138ae1	transferring a petabyte in a day		Extreme-scale simulations and experiments can generate large amounts of data, whose volume can exceed the compute and/or storage capacity at the simulation or experimental facility. With the emergence of ultra-high-speed networks, it becomes feasible to consider pipelined approaches in which data are passed to a remote facility for analysis. Here we examine the case of an extreme-scale cosmology simulation that, when run on a large fraction of a leadership-scale computer, generates data at a rate of one petabyte per elapsed day. Writing those data to disk is inefficient and impractical, and in situ analysis poses its own difficulties. Thus we implement a pipeline in which data are generated on one supercomputer and then transferred, as they are generated, to a remote supercomputer for analysis. We use the Swift scripting language to instantiate this pipeline across Argonne National Laboratory and the National Center for Supercomputing Applications, which are connected by a 100 Gb/s network, and demonstrate that by using the Globus transfer service we can achieve a sustained rate of 93 Gb/s over a 24-hour period and thus achieve our performance goal of one petabyte moved in 24 hours. This paper describes the methods used and summarizes the lessons learned in this demonstration.	24-hour clock;emergence;end-to-end principle;experiment;gigabyte;ibm websphere extreme scale;national center for supercomputing applications;petabyte;pipeline (computing);scripting language;simulation;supercomputer;swift (programming language)	Rajkumar Kettimuthu;Zhengchun Liu;David Wheeler;Ian T. Foster;Katrin Heitmann;Franck Cappello	2018	Future Generation Comp. Syst.	10.1016/j.future.2018.05.051	swift;petabyte;distributed computing;computer science;scripting language;in situ;supercomputer	HPC	-27.678957966945433	52.78218760967445	64755
9751b99b6240f26b50cfeb60b739849988ee0007	teaching cloud computing	computing education;cloud;web services;cloud computing	Just how do you teach cloud computing? It requires substantial hands-on applications and is constantly evolving--thus it presents some unique challenges for educators.	cloud computing;hands-on computing	Scott Campbell	2016	Computer	10.1109/MC.2016.286	cloud computing security;cloud computing;computer science;operating system;end-user computing;distributed computing;utility computing;multimedia;services computing;world wide web	Arch	-32.260387432881316	55.6304622320521	65131
f2e409d913245f525f2fca3d8080cedea80042fd	san fermín: aggregating large data sets using a binomial swap forest	large data sets;peer to peer system;binomial tree;large scale distributed systems;analytical model	San Fermı́n is a system for aggregating large amounts of data from the nodes of large-scale distributed systems. Each San Fermı́n node individually computes the aggregated result by swapping data with other nodes to dynamically create its own binomial tree. Nodes that fall behind abort their trees, thereby reducing overhead. Having each node create its own binomial tree makes San Fermı́n highly resilient to failures and ensures that the internal nodes of the tree have high capacity, thereby reducing completion time. Compared to existing solutions, San Fermı́n handles large aggregations better, has higher completeness when nodes fail, computes the result faster, and has better scalability. We analyze the completion time, completeness, and overhead of San Fermı́n versus existing solutions using analytical models, simulation, and experimentation with a prototype built on peer-to-peer system deployed on PlanetLab. Our evaluation shows that San Fermı́n is scalable both in the number of nodes and in the aggregated data size. San Fermı́n aggregates large amounts of data significantly faster than existing solutions: compared to SDIMS, an existing aggregation system, San Fermı́n computes a 1MB result from 100 PlanetLab nodes in 61–76% of the time and from 2-6 times as many nodes. Even if 10% of the nodes fail during aggregation, San Fermı́n still includes the data from 97% of the nodes in the result and does so faster than the underlying peer-to-peer system recovers from failures.	aggregate function;binomial heap;binomial options pricing model;distributed computing;overhead (computing);paging;peer-to-peer;planetlab;prototype;scalability;simulation	Justin Cappos;John H. Hartman	2008			computer science;theoretical computer science;operating system;database;distributed computing;binomial options pricing model;computer security;algorithm	Networks	-22.216015806121913	51.37795105967448	65208
91b34aef725cc1477242d6623e08345e970846c7	autonomous decentralized database system self configuration technology for high response				Carlos Perez Leguizamo	2016	IEICE Transactions		real-time computing;computer science;mobile agent;database;distributed computing;autonomy;decentralization	DB	-30.314222962150463	46.80832277482399	65457
6ef12473fa99cc47f4f4bfad4b0df0d6149b3a6b	an efficient industrial big-data engine		Current trends in industrial systems opt for the use of different big-data engines as a means to process huge amounts of data that cannot be processed with an ordinary infrastructure. The number of issues an industrial infrastructure has to face is large and includes challenges such as the definition of different efficient architecture setups for different applications, and the definition of specific models for industrial analytics. In this context, this paper explores the development of a medium size big-data engine (i.e., implementation) able to improve the performance in map-reduce computing by splitting the analytic into different segments that may be processed by the engine in parallel using a hierarchical model. This type of facility reduces end-to-end computation time for all segments with their results then merged with other information from other segments after their processing in parallel. This type of setup increases the performance of current clusters improving I/O operations remarkably as empirical results revealed.	big data;computation;end-to-end principle;hierarchical database model;input/output;mapreduce;time complexity	Pablo Basanta-Val	2018	IEEE Transactions on Industrial Informatics	10.1109/TII.2017.2755398	architecture;computer science;real-time computing;computation;hierarchical database model;big data;analytics;benchmark (computing)	DB	-19.57843882306389	57.602898499948246	65467
070dea0070ba87d66b4819f57fca11825924a8f8	fabric convergence implications on systems architecture	protocols;pattern clustering;physical layer;evaluation method;i o consolidation;protocols pattern clustering;data storage;leverage ensemble level resource sharing;leverage ensemble level resource sharing fabric convergence implications systems architecture cluster networking physical layer similarities i o consolidation application agnostic designs;systems architecture;resource sharing;communication protocol;physical layer similarities;cross layer;system architecture;fabric convergence implications;cluster networking;application agnostic designs	Converged fabrics that support data, storage, and cluster networking in a unified fashion are desirable for their cost and manageability advantages. Recent trends towards higher-bandwidths in commodity networks, physical-layer similarities across different communication protocols, and the adoption of blade servers along with the corresponding availability of dasiabackplanespsila to implement new networking methods, motivate revisiting this idea. We discuss various aspects of fabric convergence, and present some evaluation results from our experiments in the context of a specific I/O consolidation case study. Based on the insights from these experiments, we discuss opportunities for future research - in new instrumentation and evaluation methods, new cross-layer and application-agnostic designs for fabric convergence solutions, and new system architectures that leverage ensemble-level resource sharing. Our goal, through the discussions in this position paper, is to initiate a more general examination of these issues in the broader academic community.	experiment;input/output;semiconductor consolidation;systems architecture	Kevin Leigh;Parthasarathy Ranganathan;Jaspal Subhlok	2008	2008 IEEE 14th International Symposium on High Performance Computer Architecture	10.1109/HPCA.2008.4658624	communications protocol;parallel computing;real-time computing;simulation;computer science;operating system;distributed computing;computer network;systems architecture	Arch	-27.34762211911296	56.58269296498785	65535
e41d2dadff14bbde4b470042cec58fb5cbb00e85	agent based self-healing system for grid computing	agent based;abss;gridway;gram;gt4;next generation;large scale distributed systems;grid computing;grid system	Increasing complexity of large scale distributed systems is becoming unmanageable because of the manual style adopted for the management today. The manual management techniques are time-consuming, un-secure and more prone to errors. A new paradigm for self-management is pervading over the old manual system to begin the next generation of computing. In this paper we have, proposed and implemented the automated multiagent based approach to self-heal the grid system from faults. This approach automatically monitor and handle the faults occurred in grid environment while executing the job.	agent-based model;distributed computing;grid computing;programming paradigm;self-management (computer science)	I. Chopra;Maninder Singh	2010		10.1145/1741906.1741912	real-time computing;semantic grid;computer science;database;distributed computing;grid computing	HPC	-31.277403602165876	48.204804306480945	65653
225bdf151e3b0b37a894abd5d69bcdcc0f4c3ed2	a failure detection system for large scale distributed systems	failure detection;fault tolerance;large scale distributed systems;gossip strategies;clustering mechanisms	Failure detection is a fundamental building block for ensuring fault tolerance in large scale distributed systems. In this paper we present an innovative solution to this problem. The approach is based on adaptive, decentralized failure detectors, capable of working asynchronous and independent on the application flow. The proposed failure detectors are based on clustering, the use of a gossip-based algorithm for detection at local level and the use of a hierarchical structure among clusters of detectors along which traffic is channeled. In this we present result proving that the system is able to scale to a large number of nodes, while still considering the QoS requirements of both applications and resources, and it includes the fault tolerance and system orchestration mechanisms, added in order to asses the reliability and availability of distributed systems in an autonomic manner.	algorithm;autonomic computing;cluster analysis;distributed computing;fault tolerance;quality of service;requirement;sensor	Andrei Lavinia;Ciprian Dobre;Florin Pop;Valentin Cristea	2010	2010 International Conference on Complex, Intelligent and Software Intensive Systems	10.4018/jdst.2011070105	fault tolerance;real-time computing;computer science;distributed computing;computer security	HPC	-24.978230694119162	53.38499817107174	65714
082788d87c73e8d2e539607e48bbcdfb2e407115	native cloud applications - why virtual machines, images and containers miss the point!		Due to the current hype around cloud computing, the term “native cloud application” becomes increasingly popular. It suggests an application to fully benefit from all the advantages of cloud computing. Many users tend to consider their applications as cloud native if the application is just bundled in a virtual machine image or a container. Even though virtualization is fundamental for implementing the cloud computing paradigm, a virtualized application does not automatically cover all properties of a native cloud application. In this work, we propose a definition of a native cloud application by specifying the set of characteristic architectural properties, which a native cloud application has to provide. We demonstrate the importance of these properties by introducing a typical scenario from current practice that moves an application to the cloud. The identified properties and the scenario especially show why virtualization alone is insufficient to build native cloud applications. Finally, we outline how native cloud applications respect the core principles of service-oriented architectures, which are currently hyped a lot in the form of microservice architectures.	application programming interface;cloud computing;continuous delivery;electronic billing;hardware virtualization;microservices;multitenancy;native (computing);native cloud application;programming paradigm;service-oriented architecture;service-oriented device architecture;software as a service;virtual machine	Frank Leymann	2016			computer hardware;computer science;theoretical computer science;computer graphics (images)	HPC	-29.883988322201635	55.953522343389935	65849
9af0342e9f6ea8276a4679596cf0e5fbbd7f9996	the skiplist-based lsm tree		Log-Structured Merge (LSM) Trees provide a tiered data storage and retrieval paradigm that is aractive for write-optimized data systems. Maintaining an ecient buer in memory and deferring updates past their initial write-time, the structure provides quick operations over hot data. Because each layer of the structure is logically separate from the others, the structure is also conducive to opportunistic and granular optimization. In this paper, we introduce the SkiplistBased LSM Tree (sLSM), a novel system in which the memory buer of the LSM is composed of a sequence of skiplists. We develop theoretical and experimental results that demonstrate that the breadth of tuning parameters inherent to the sLSM allows it broad exibility for excellent performance across a wide variety of workloads.	computer data storage;data system;mathematical optimization;programming paradigm;skip list	Aron Szanto	2018	CoRR		memory buffer register;database;merge (version control);skip list;computer science;data system;computer data storage;distributed computing	DB	-20.678823053054025	49.53827369228778	65936
38e7b67d3df9ad003e844c9b7df19e9088ebb531	computing all the best swap edges distributively	distributed system;systeme reparti;distributed computing;sistema repartido;distributed environment;optimality criteria;calculo repartido;calcul reparti	Recently great attention has been given to point-of-failure swap rerouting, an efficient technique for routing in presence of transient failures. According to this technique, a message follows the normal routing table information unless the next hop has failed; in this case, it is redirected towards a precomputed link, called swap; once this link has been crossed, normal routing is resumed. The amount of precomputed information required in addition to the routing table is rather small: a single link per each destination. Several efficient serial algorithms have been presented to compute this information; none of them can unfortunately be efficiently implemented in a distributed environment. In this paper we present protocols, based on a new strategy, that allow the efficient distributed computation of all the optimal swap edges under several optimization criteria. In systems allowing long messages, we develop solution protocols based on the same strategy that use only O(n) messages without increasing the total amount of transmitted data items. Research partially supported by NSERC Canada, TECSIS Co., and the Swiss BBW 03.0378-1 for EC contract 001907 (DELIS), and “Progetto ALINWEB: Algoritmica per Internet e per il Web”, MIUR, Programmi di Ricerca Scientifica di Rilevante Interesse Nazionale. A preliminary version of this paper has been presented at the 8th International Conference on Principles of Distributed Systems (OPODIS 2004). University of Ottawa, Canada, flocchin@site.uottawa.ca Università di Pisa, Italy, pagli@di.unipi.it Corresponding author: Università di Pisa, Dipartimento di Informatica, Largo Bruno Pontecorvo, 3, 5600 Pisa, Italy, email: prencipe@di.unipi.it, Tel: +39 05	algorithm;computation;distributed computing;download;email;mathematical optimization;pc bruno;paging;precomputation;routing table;shortest path problem;switzerland;uniform resource identifier	Paola Flocchini;Linda Pagli;Giuseppe Prencipe;Nicola Santoro;Peter Widmayer;Tranos Zuva	2004		10.1007/11516798_11	computer science;theoretical computer science;distributed computing;algorithm;distributed computing environment	DB	-20.33199196409721	49.2850837218164	66026
b6613dbcdf4b29ede9c32f2468aa1a9ea62f8a04	execution and composition of e-science applications using the ws-resource construct	analytical models;scientific application;globus toolkit;natural sciences computing digital simulation internet;service orientation;globus toolkit e science applications service oriented architectures computational simulation models wsrf set web services resource framework;telecommunication computing;service oriented architectures;computer applications;computer architecture;computational modeling;internet;web services;web services service oriented architecture object oriented modeling computational modeling grid computing informatics computer applications telecommunication computing analytical models computer architecture;informatics;computational simulation models;web services resource framework;natural sciences computing;service oriented architecture;grid computing;wsrf set;e science applications;object oriented modeling;digital simulation	Service oriented architectures are emerging as the recommended paradigm for developing dispersed e-science environments. In this paper we analyze the characteristics and requirements of a common class of scientific applications, namely computational simulation models, and define a generic service-oriented framework for their execution and composition. Finally we present the work done so far towards the implementation of such framework based on the WSRF set of specifications and the Globus toolkit	computation;data transformation services;e-science;finalize (optical discs);flat file database;grid computing;interaction;programming paradigm;requirement;service-oriented architecture;service-oriented device architecture;service-oriented programming;simulation;web service;wireless access point;xml;glite	Evangelos Floros;Yannis Cotronis	2006	Proceedings 20th IEEE International Parallel & Distributed Processing Symposium	10.1109/IPDPS.2006.1639658	computer science;theoretical computer science;service-oriented architecture;database;distributed computing	HPC	-31.410779315702396	49.70209489002978	66048
bb9e8f88d2895982827f55a5c06b49d5976854fe	grid systems and their applications to biomedical science	eksperymenty in silico;distributed computing;large scale applications;aplikacje wielkiej skali;systemy gridowe;grid systems;in silico experiments;obliczenia rozproszone		grid systems corporation	Maciej Malawski;Tomasz Szepieniec;Irena Roterman-Konieczna	2006	Bio-Algorithms and Med-Systems		computational biology;computational science;computer science;bioinformatics	Logic	-30.054262500447805	50.58178260684727	66300
daed3a446a716c57a0e8c10784c7f54c70b5a667	flight plan management in a distributed air traffic control system	distributed system;communication system traffic control;distributed air traffic control system;control systems;poles and towers;air traffic control;availability;loosely coupled distributed system flight plan management distributed air traffic control system large scale replication;air traffic computer control;flight plan management;loosely coupled distributed system;weak consistency;large scale;air traffic control control systems aircraft aerospace control communication system traffic control workstations availability airborne radar poles and towers displays;aerospace control;displays;workstations;large scale replication;airborne radar;local area networks air traffic computer control;local area networks;geographic distribution;aircraft	We explore how large-scale replication may enhance the availability of data in a loosely coupled distributed system. We are especially concerned with Air Traac Control systems which are intrinsically geographically distributed and impose strict constraints of availability. In this framework, we propose a replication system providing a weak consistency of data. After a study of speciic properties of ight plan data, we discuss possible replication strategies and present our own approach , which is characterized by a split between the propagation function and the end-user service.	control system;distributed computing;fault tolerance;loose coupling;replicator (stargate);response time (technology);server (computing);software propagation;weak consistency	Philippe Quéinnec;Gérard Padiou	1993		10.1109/ISADS.1993.262687	local area network;availability;weak consistency;real-time computing;simulation;workstation;computer science;control system;operating system;air traffic control;distributed computing	OS	-26.526743417114677	53.42133140412166	66359
5e1ce75e649cef37f6fbd257a2fedce273569edf	the design of scaleable batch transaction processing application using an open tp monitor.	transaction processing		teleprocessing monitor;transaction processing	Pat V. Crain;Craig D. Hanson	1997			parallel computing;real-time computing;computer science;database;transaction processing system	ML	-19.731876068729136	51.633931361719725	66377
04cfe3ffbb111e10a138ddd040a6acf52d7ead6a	the adaptive heartbeat design of high availability raid dual-controller	dual controller hot standby system;high availability;data read write request;storage system;self adaptive heart ability adaptive heartbeat design high availability raid dual controller storage systems design data protection dual controller hot standby system high availability system self adaptive heartbeat model data read write request dual controller raid storage system stochastic petri net model fault detection;stochastic petri net;raid;self adaptive heart ability;storage systems design;self adaptive heartbeat model;stochastic petri net model;stochastic processes petri nets raid;high availability system;stochastic processes;fault detection;heart beat availability control systems hardware multimedia systems data engineering design engineering pervasive computing computer science face detection;dual controller raid storage system;petri nets;adaptive heartbeat design;high availability raid dual controller;data protection;reading and writing	Designing storage systems to provide high availability in the face of failures needs the use of various data protection techniques, such as dual-controller RAID. The failure of RAID controller may cause RAID storage system to fail to respond to ongoing requests and to no longer be available to new requests. Heartbeat is used to detect controllers whether survival. So, the heartbeat cycle's impact on the high availability of a dual-controller hot-standby system has become the key of current research. To address the problem of fixed setting heartbeat in building high availability system currently, a self-adaptive heartbeat model of dual-controller, which can adjust heartbeat cycle based on the frequency of data read-write request, is designed to improve the high availability of dual-controller RAID storage system. Based on this model, the high availability stochastic Petri net model of fault detection was established and used to evaluate the effect of the availability. In addition, we define a SHA (self-adaptive heart ability) parameter to scale the ability of system heartbeat cycle to adapt to the environment when high availability system is at a changing environment of read and write requests. The results show that, relatively speaking with fixed configuration, the design can enhance dual controller RAID system high availability.	computer data storage;disk array controller;fault detection and isolation;high availability;hot spare;information privacy;raid;read-write memory;real-time transcription;stochastic petri net	Yaping Wan;Dan Feng;Tianming Yang;Ze Deng;Li Liu	2008	2008 International Conference on Multimedia and Ubiquitous Engineering (mue 2008)	10.1109/MUE.2008.31	stochastic process;embedded system;real-time computing;stochastic petri net;computer hardware;computer science;operating system;data protection act 1998;high availability;computer security;petri net;fault detection and isolation;raid	DB	-24.661991419291137	50.12755768946324	66612
8bcce140cf8f00e6b7a90f190c0dd4428a888fbf	managing service level agreement contracts in ogsa-based grids	open grid services architecture;open grid service architecture;dynamic environment;execution environment;service level agreement;utility computing;execution management services;quality of service;network services	Grids and mobile Grids can form the basis and the enabling technology for pervasive and utility computing due to their ability to be open, highly heterogeneous and scalable. However, the process of selecting the appropriate resources and initiating the execution of a job is not enough to provide quality in a dynamic environment such as a mobile Grid, where changes are numerous, highly variable and with unpredictable effects. In this paper we present a scheme for advancing and managing Quality of Service (QoS) attributes contained in Service Level Agreement (SLA) contracts of Grids that follow the Open Grid Services Architecture (OGSA). In order to achieve this, the execution environment of the Grid infrastructure establishes and exploits the synergies between the various modules of the architecture that participate in the management of the execution and the enforcement of the SLA contractual terms. We introduce an Execution Management Service which is in collaboration with both the application services and the network services in order to provide an adjustable quality of the requested services. The components that manage and control the execution in the Grid environment interact with the suit of the SLA-related services exchanging information that is used to provide the quality framework of the execution with respect to the agreed contractual terms. The described scheme has been implemented in the framework of the Akogrimo IST project.	open grid services architecture;service-level agreement	Antonios Litke;Kleopatra Konstanteli;Vassiliki Andronikou;Sotirios P. Chatzis;Theodora A. Varvarigou	2008	Future Generation Comp. Syst.	10.1016/j.future.2007.06.004	real-time computing;quality of service;computer science;operating system;database;distributed computing;utility computing;services computing	HPC	-31.548969986881485	53.49945515205066	66917
aefda5d5625b061dd29c01638a919ccaab84fd7f	virtual machine migration in heterogeneous clouds: from openstack to vmware		Adopting a Cloud solution means binding to a specific platform and vendor, with proprietary protocols, standards and tools so usually running into a vendor lock-in. The fear of vendor lock-in is often cited as a major impediment to Cloud service adoption. In this work, we focus on the Virtual Machine (VM) migration between homogeneous and heterogeneous Cloud platforms. We focused on the technical parameters that are essential to be tuned, depending on the various types of virtualization engines used by the Cloud environments. The key difference is the configuration requirements mainly related with image formats and used hypervisors. To demonstrate heterogeneous VM migration we develop a tool that works for OpenStack and VMWare platforms. The experimental analysis demonstrates the effectiveness of our solution with regards to the HTTP response times, especially between the heterogeneous Cloud platforms. The tool achieves approximately two milliseconds response time for HTTP requests to Cloud APIs, excluding the time required to download and upload image files.	backup;cloud computing;download;hypertext transfer protocol;hypervisor;image file formats;interoperability;operating system;process migration;requirement;response time (technology);upload;vmdk;vendor lock-in;virtual machine	Dimitrios Kargatzis;Stelios Sotiriadis;Euripides G. M. Petrakis	2017	2017 IEEE 38th Sarnoff Symposium	10.1109/SARNOF.2017.8080393	full virtualization;virtualization;computer network;cloud computing;hypervisor;operating system;vendor;computer science;virtual machine;upload;hardware virtualization	Metrics	-24.767905432590407	58.97024204080523	66923
cc26fa1ae36297f710e56274a97f446b88dc9c61	study on supporting technology for operational procedure design of it systems in cloud-era datacenters	checklists;procedural parts;templates;operational procedures	Datacenters are now widely used, and their sizes are increasing due to the rapid spread of cloud computing. Meanwhile, the cost of IT-system operations occupies 60% of the total cost in a datacenter. So far, improving operations has been focused on automating operations with operation management middleware. However, designing operational procedures, including composing and modifying operational manuals and checklists, is also a big issue since it is time consuming. Supporting technology that improves the efficiency of IT-system operational procedure design has been studied by focusing on the commonly used procedural parts. Existing checklists were analyzed to extract the iterated patterns of the procedures in order to evaluate the technology. The evaluation results showed that common operations such as the ones on management middleware and the remote log-in were extracted as procedural parts. The number of checklist composition steps was reduced by 45% on average and that of the modification steps was reduced by 87% on average by using these procedural parts. A prototyping tool was developed, and the checklists of a sample system different from the analyzed one were implemented on the tool.	cloud computing;data center;iteration;login;middleware;procedural programming	Hiroaki Shikano;Machiko Asaie;Junji Yamamoto;Tatsuya Saito;Shunsuke Ota;Keitaro Uehara	2013		10.1145/2480362.2480443	template;simulation;operating system;database	SE	-30.79412118577943	54.88662754795639	67058
ee2be83ab84c94b3c764d263efe23e79f37dee8b	tailor-made concurrency control - distributed transactions as a case	correctness;distributed databases;concurrency control;distributed transactions	This paper applies a model for distributed databases and transactions with a distinction between global and local correctness criteria. The global requirements per system are weaker than the local requirements per site. The paper presents an application which suits such a two-level division. The main motivation for our investigation is based on the fact that the commonly used correctness criteria for concurrency control and recovery, serializability and total recoverability, are very strict criteria. The use of more relaxed criteria (allowing more true parallel behaviour and more true partial behaviour) is therefore very appealing - as long as this can be achieved without compromising safety or applicability. The main paradigm in our approach is based on the observation that relatively little knowledge about the databases and transactions can lead to major gains in system throughput. This allows specific systems to have more tailormade correctness criteria.rnWe analyse a specialized type of distributed database, the skeleton-database, and a specialized type of distributed transaction, the wander-transaction. Wander-transactions accessing a skeleton-database allow breaks with both the common serializability criterion and the common total recoverability criterion. Our main emphasis here is on the nonserializability aspect. The primary goal of this work is to designate correctness criteria for controlling local and global parallelism. The secondary goal is to specify priority rules for handling local and global criteria breaks. Wander-transactions accessing a skeleton-database experience dynamic priorities. Our resulting concept, priority serializability, gives increased parallelism without compromising safety.	concurrency control;distributed transaction	Mads Nygård	1995	Australasian J. of Inf. Systems		global serializability;correctness;real-time computing;commitment ordering;distributed transaction;computer science;two-phase locking;concurrency control;database;distributed computing;serializability;distributed database;schedule;distributed concurrency control	DB	-22.79322123356455	46.87566374127962	67399
39ce93b5c12f97c02b7513c4254402e1c0dc7b79	efficient resource management for running multiple concurrent jobs in a computational grid environment	optimal resource allocation;computational grid;concurrent jobs;resource manager;grid;design and implementation;resource broker;resource brokering	Managing resources in Grid is an intricate issue and is not possible at the application or user level. Grid provides uniform access to heterogeneous resources owned by multiple organizations. Any time a new resource can join the Grid or an existing resource can withdraw. The system loads and status of the resources also change frequently. Thus, a resource broker is an essential component in a Grid environment that can assist in the selection of a right resource provider for a job in all aspects. This paper discusses the design and implementation of resource brokering strategies within the multi-agent framework. These strategies help in finding out an optimal allocation of resources for executing multiple concurrent jobs in a Grid environment. Different stages in resource brokering and their implementation within the framework are discussed. The paper also presents results of evaluating the strategies and demonstrates their effectiveness.	grid computing	Sarbani Roy;Nandini Mukherjee	2011	Future Generation Comp. Syst.	10.1016/j.future.2011.04.008	semantic grid;resource allocation;computer science;knowledge management;resource management;database;distributed computing;grid;drmaa	HPC	-30.296872927324785	52.50588348193869	67412
76c366916b33eba604de7754dfcbca6aed825584	fault tolerance management for a hierarchical gridrpc middleware	optimal fault detector fault tolerance management hierarchical gridrpc middleware tcp automatic checkpoints;fault tolerance management;transport protocols checkpointing grid computing middleware software fault tolerance;service provider;fault tolerant;distributed algorithm gridrpc fault tolerant failure detector checkpoint;optimal fault detector;tcp;software fault tolerance;indexing terms;checkpointing;distributed scheduling;transport protocols;large scale;fault tolerance middleware grid computing high performance computing detectors processor scheduling network servers libraries large scale systems fault detection;failure detector;high per formance computing;checkpoint;middleware;hierarchical gridrpc middleware;automatic checkpoints;long range;distributed algorithm;grid computing;gridrpc;grid system	The GridRPC model is well suited for high performance computing on grids thanks to efficiently solving most of the issues raised by geographically and administratively split resources. Because of large scale, long range networks and heterogeneity, Grids are extremely prone to failures. GridRPC middleware are usually managing failures by using 1) TCP or other link network layer provided failure detector, 2) automatic checkpoints of sequential jobs and 3) a centralized stable agent to perform scheduling. Most recent developments have provided some new mechanisms like the optimal Chandra & Toueg & Aguillera failure detector, most numerical libraries now providing their own optimized checkpoint routine and distributed scheduling GridRPC architectures. In this paper we aim at adapting to these novelties by providing the first implementation and evaluation in a grid system of the optimal fault detector, a novel and simple checkpoint API allowing to manage both service provided checkpoint and automatic checkpoint (even for parallel services) and a scheduling hierarchy recovery algorithm tolerating several simultaneous failures. All those mechanisms are implemented and evaluated on a real grid in the DIET middleware.	algorithm;application checkpointing;application programming interface;centralized computing;failure cause;failure detector;fault tolerance;gridrpc;library (computing);middleware;numerical analysis;scheduling (computing);supercomputer;transaction processing system	Aurelien Bouteiller;Frédéric Desprez	2008	2008 Eighth IEEE International Symposium on Cluster Computing and the Grid (CCGRID)	10.1109/CCGRID.2008.14	service provider;distributed algorithm;fault tolerance;parallel computing;real-time computing;index term;computer science;operating system;transmission control protocol;middleware;distributed computing;transport layer;failure detector;software fault tolerance;grid computing	HPC	-24.6186737096349	53.36404903004232	67520
20ce5ecdb4ab5138296b59ee93c94c6888f4dee1	transparent recovery in distributed systems	distributed system	"""We are investigating transparent optimistic solutions to problems in dist r ibuted systems such as recovery [6], replication [3], parallelization [2], and concurrent competing alternatives [4]. By a transparent solution to such a problem we mean that a program is transformed automatically, and that the behavior of the program is equivalent to a possible behavior of the untransformed program; in addition, the programmer and the end-user need not be aware of the transformation. Transparent solutions to such problems are relatively straightforward if synchronization is relied upon, but performance of such methods is generally poor or the implementat ion is too expensive, and they do not scale. Our approach is to use optimistic methods in which we guess that synchronization is unnecessary, and verify this asynchronously while the program continues execution. We track inter-process dependencies and log non-deterministic events so that we can roll back a computat ion that depends upon an incorrect guess. Where virtual memory virtualizes the space of a process, we virtualize time, """"paging in"""" a previous process state when a """"time fault"""" (or incorrect guess) occurs."""	distributed computing;information needs;inter-process communication;paging;parallel computing;process state;programmer;synchronization (computer science);virtual machine	David F. Bacon	1990		10.1145/504136.504150	computer science;distributed computing	PL	-19.213248535227528	48.67770006672379	67677
42aced3415391e3b0dcbf4a664e422ab134fa7a4	towards a taxonomy of cloud recovery strategies	protocols;system recovery cloud computing;virtual machining;runtime virtual machining fault tolerance fault tolerant systems cloud computing taxonomy protocols;runtime;sporadic operations;fault tolerant systems;fault tolerance;taxonomy;consumer initiated;failure recovery rolling upgrade life cycle phase cloud sporadic operations cloud recovery strategies;sporadic operations cloud recovery consumer initiated;cloud recovery;cloud computing	Recovering from failures of sporadic operations such as rolling upgrade or migration is complicated by the fact that the application being upgraded or migrated must continue to provide service. This means that recovery strategies for sporadic operations must include facilities for recovering from normal operations as well. As a step in deriving methods for recovering from failures in sporadic operations, we classify existing methods into four categories according to their purposes and the life cycle phase for which they are applicable. Not only does this taxonomy facilitate the research on recoverability of cloud sporadic operations but also it can help better understand the existing cloud recovery strategies.	cloud computing;enterprise life cycle;serializability;system migration;taxonomy (general)	Min Fu;Leonard J. Bass;Anna Liu	2014	2014 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks	10.1109/DSN.2014.70	communications protocol;fault tolerance;real-time computing;simulation;cloud computing;computer science;operating system;distributed computing;taxonomy	DB	-24.323713495075307	50.29629878040404	67731
57435709f5170b054f2f78be2d17ecff088697be	a cloud computing application in land resources information management	cloud cache pool;land resource;processor scheduling;land resources cloud computing platform;loading;soa;cloud computing tiles spatial databases information management clouds processor scheduling loading;land resources information management;clouds;information management;spatial databases;mapreduce land resource cloud computing soa cache pool;map reduce;land use planning cloud computing information management;resources oriented techniques land resources information management land resources cloud computing platform job scheduling cloud cache pool map reduce;tiles;resources oriented techniques;cache pool;mapreduce;job scheduling;land use planning;cloud computing	Based on the characteristics of land resources information management, this research firstly designs a land resources cloud computing platform. Four main strategies, namely job scheduling, cloud cache pool, Map Reduce and resources-oriented techniques are then discussed in details. A prototype system named Land Cloud is then introduced, followed by two experiments which are conducted to test the performance of the system. The results show enough evidence of the usefulness of cloud computing services in managing land resources data.	algorithm;cloud computing;experiment;information management;job scheduler;mapreduce;prototype;scheduling (computing)	Fang Lei;Shen-jun Yao;Liu Ting;Ren-yi Liu	2010	2010 Ninth International Conference on Grid and Cloud Computing	10.1109/GCC.2010.81	land-use planning;cloud computing security;real-time computing;cloud computing;computer science;job scheduler;operating system;service-oriented architecture;cloud testing;database;distributed computing;utility computing;information management	HPC	-26.140381784027092	58.17633371703209	67893
2d62d0b8c965235870b4b71bd3a5ea6612c5a6c6	persistent storage for distributed applications	distributed application	Research work in supporting distributed applications has traditionally focussed on the dynamic part of their interactions—that is, network communication paths. It’s our belief that although these are important, their very transience means that they are much less valuable in the long term than the persistent state that these applications manipulate and leave behind. Such state can be enormous—tens of terabytes are not uncommon for large-scale commercial applications, which are frequently constructed from suites of federated, distributed applications. Such systems are themselves classic examples of a distributed application composed from a set of cooperating pieces. As with network communications, the persistent storage medium must be well-behaved, in the sense of providing predictable behavior, so that applications do not interfere with each other. We believe that quality of service (QoS) guarantees, and ways to automatically reason about resource provision to meet them, is the key to building effective and useful storage services.	algorithm;best-effort delivery;block (data storage);computer data storage;data synchronization;database;distributed computing;fair-share scheduling;interaction;mathematical optimization;open research;persistence (computer science);provisioning;quality of service;requirement;scheduling (computing);terabyte;transaction processing	Richard A. Golding;John Wilkes	1998		10.1145/319195.319204	distributed computing;computer science;persistence (computer science)	OS	-24.516447004396785	52.87540930262721	68001
4d0ff88dd2970cbe26e98364c8243087e24d0d63	performance and resource modeling in highly-concurrent oltp workloads	oltp;multi tenancy;performance prediction transactions muti tenancy;performance predictions;database as a service	"""Database administrators of Online Transaction Processing (OLTP) systems constantly face difficult questions. For example, """"What is the maximum throughput I can sustain with my current hardware?"""", """"How much disk I/O will my system perform if the requests per second double?"""", or """"What will happen if the ratio of transactions in my system changes?"""". Resource prediction and performance analysis are both vital and difficult in this setting. Here the challenge is due to high degrees of concurrency, competition for resources, and complex interactions between transactions, all of which non-linearly impact performance.  Although difficult, such analysis is a key component in enabling database administrators to understand which queries are eating up the resources, and how their system would scale under load. In this paper, we introduce our framework, called DBSeer, that addresses this problem by employing statistical models that provide resource and performance analysis and prediction for highly concurrent OLTP workloads. Our models are built on a small amount of training data from standard log information collected during normal system operation. These models are capable of accurately measuring several performance metrics, including resource consumption on a per-transaction-type basis, resource bottlenecks, and throughput at different load levels. We have validated these models on MySQL/Linux with numerous experiments on standard benchmarks (TPC-C) and real workloads (Wikipedia), observing high accuracy (within a few percent error) when predicting all of the above metrics."""	approximation error;experiment;ibm tivoli storage productivity center;interaction;iterative and incremental development;iterative method;linux;lock (computer science);maximum throughput scheduling;mysql;online transaction processing;oracle database;profiling (computer programming);random-access memory;statistical model;vendor lock-in;web server;wikipedia	Barzan Mozafari;Carlo Curino;Alekh Jindal;Samuel Madden	2013		10.1145/2463676.2467800	real-time computing;computer science;multitenancy;data mining;database;online transaction processing	DB	-23.00635938327243	58.70521920363898	68023
e54d431f077691039aa3de654436dd5ef91d4561	managing heterogeneous resources in data mining applications on grids using xml-based metadata	heterogeneous resource management;software tool;execution plans;application software;resource allocation;distributed data mining applications;resource management;knowledge management;data mining;data intensive simulations;computer networks;knowledge grid;data mining application;distributed environment;resource management data mining application software knowledge management grid computing software tools computer networks environmental management computer network management relational databases;data sources;grid based data mining applications;computer network management;dynamic heterogeneous distributed environments;grid infrastructure;xml;meta data;grid infrastructure dynamic heterogeneous distributed environments network facilities software tools data intensive simulations heterogeneous resource management grid based data mining applications xml based metadata data sources knowledge grid execution plans distributed data mining applications;software tools;relational databases;distributed data mining;environmental management;grid computing;resource allocation data mining xml meta data grid computing;network facilities;xml based metadata	The Grid supports the sharing and coordinated use of resources in dynamic heterogeneous distributed environments. The effective use of a Grid requires the definition of an approach to manage the heterogeneity of the involved resources that can include computers, data, network facilities and software tools provided by different organizations. This issue get more importance when complex applications, such as data-intensive simulations and data mining applications, executed on a Grid. This paper is concerned with heterogeneous resource management in Grid-based data mining applications. It discusses how resources are represented and managed in the KNOWLEDGE GRID and how XML-based metadata are used to describe data mining tools, data sources, mining models and execution plans, and how those metadata are used for the design and execution of distributed data mining applications on Grids.	computer;data mining;data-intensive computing;grid computing;heterogeneous computing;metadata modeling;simulation;xml	Carlo Mastroianni;Domenico Talia;Paolo Trunfio	2003		10.1109/IPDPS.2003.1213204	application software;xml;semantic grid;resource allocation;relational database;computer science;resource management;operating system;data grid;data mining;database;data stream mining;metadata;world wide web;data element;grid computing;distributed computing environment	HPC	-31.102693077442748	50.35472097085369	68029
a43a619c9fe2fe9c32f7047d4c8c253675cc911e	luc model : a timestamp ordering based view consistency model for distributed shared memory	view consistency model;timestamp ordering protocol;distributed shared memory	Excessive locking and cumulative updates in Distributed Shared Memory (DSM) not only reduces the parallelism for block access but also causes a serious degradation in response time for a dense network. This paper proposes a new consistency model in DSM named Last Update Consistency (LUC) model, where the model uses logical clock counter to keep the DSM consistent. The logical clock always increases never decreases. So the increasing order of the logical clock value is used to provide the request to the DSM. In this model, multiple nodes can perform READ operations over the same block at a time. For WRITE operation over the same block, only the last modification will exist and the earlier WRITE operations will be treated as obsolete WRITE and should be discarded. The experimental and analytical analysis showed that the proposed model effectively reduces the unnecessary network traffic and cumulative block updates that exist in the Sequential Consistency Model and Release Consistency Model.	consistency model;distributed shared memory;elegant degradation;lock (computer science);logical clock;network packet;parallel computing;release consistency;response time (technology);sequential consistency;timestamp-based concurrency control	Rubaiyat Islam Rafat;Kazi Sakib	2010	JCP	10.4304/jcp.5.12.1828-1838	distributed shared memory;cache coherence;real-time computing;computer science;consistency model;operating system;database;distributed computing;computer security	OS	-20.13586126271982	48.8443071327016	68114
abd8c4d1d23dd76aac92a042edff266b461ed865	a persistent snapshot device driver for linux	persistent snapshot device driver;data capacity;large enterprise;online backup;online backup demand;layered device driver;conventional off-line backup;design issue;backup window time frame;snapshot device driver;online backup capability	Web servers and large enterprises demand online backup capability to protect data that must be available continuously and eliminate the down-time needed to perform conventional off-line backups. Online backup demand is fueled by growing data capacities that have lengthened backup window time frames and by the significant loss in productivity that occurs when servers must be taken offine.#R##N##R##N#Snapshots allow applications to take online backup. This paper discusses the design and implementation of a layered device driver in Linux for persistent snapshots. This paper also discusses the design issues involved in developing a snapshot device driver in a clustered environment.	device driver;linux;snapshot isolation	Suresh Siddha	2001			backup software;embedded system;real-time computing;incremental backup;engineering;operating system	OS	-23.7318061159857	50.94837354751994	68266
4ae291eb01cb6593576cbfa91788bf028903cf7e	dynamic window-based adaptive fault monitoring for ubiquitous computing systems	fault diagnosis ubiquitous computing fault tolerant computing system monitoring;fault management dynamic window based adaptive fault monitoring ubiquitous computing systems fault tolerance failure detection;fault tolerant;failure detection;pull and push based monitoring;system monitoring;interval computations;fault tolerant computing;fault tolerant corba;pull and push based monitoring dynamic window elapsed time fault monitoring and detection fault tolerant corba;fault detection;detection algorithm;ubiquitous computing;dynamic window;ubiquitous computing fault detection condition monitoring computerized monitoring fault tolerance fault tolerant systems biomedical monitoring object detection computer crashes computer networks;fault management;fault monitoring and detection;elapsed time;fault diagnosis	A number of different kinds of applications in large and complex ubiquitous computing systems need fault tolerance. Here failure detection is one of the key elements for efficient fault management. There exist various fault monitoring and detection algorithms employing a timeout-based mechanism. However, they are occasionally inaccurate in unstable or overloaded system. The goal of the proposed scheme is to enhance the accuracy of fault detection by properly handling the monitoring interval. This is achieved by dynamically adjusting the window of the past elapsed times included in the prediction of the monitoring interval. Computer simulation reveals that the proposed algorithm allows much more elastic and accurate prediction of monitoring interval than the existing algorithms regardless of load condition of the system.	algorithm;computer simulation;control theory;fault detection and isolation;fault tolerance;ubiquitous computing	Su Myeong Lee;Hee Yong Youn	2005	11th Pacific Rim International Symposium on Dependable Computing (PRDC'05)	10.1109/PRDC.2005.33	embedded system;system monitoring;fault tolerance;real-time computing;computer science;engineering;fault management;distributed computing;computer security;ubiquitous computing;fault detection and isolation	Arch	-23.19205532087316	51.09494329068074	68393
37e8096d76347c97b7a010bcc8ef748fefcb9c2f	performance evaluation of scientific workflow on openstack and openvz		Cloud computing is capturing attention of the market by providing infrastructure, platform and software as a services. Using virtualization technology, resources are shared among multiple users to improve the resource utilization. By leasing the infrastructure from public cloud, users can save money and time to maintain the expensive computing facility. Therefore, it gives an option for cluster and grid computing technology which is used for industrial application or scientific workflow. Virtual machine enables more flexibility for consolidation of the underutilized servers. However, containers are also competing with virtual machine to improve the resource utilization. Therefore, to adopt cloud computing for scientific workflow, scientist needs to understand the performance of virtual machine and container. We have used cloud computing with different virtualization technologies like KVM and container to test the performance of scientific workflow. In this work, we analyze the performance of scientific workflow on OpenStack’s virtual machine and OpenVZ’s container. Our result shows that container gives better and stable performance than virtual machine.	openvz;performance evaluation	Amol Jaikar;Syed Asif Raza Shah;Sangwook Bae;Seo-Young Noh	2015		10.1007/978-3-319-38904-2_13	operating system;database;world wide web	HPC	-24.79011599150117	59.58254259490276	68409
dc97c252fa486cc6e2fde4ccd937bb5f1916fef0	interestcast: adaptive event dissemination for interactive real-time applications		Many networked applications use push-based many-to-many communication. Especially real-time applications, such as online games and other virtual reality applications, need to synchronize state between many participants under strict latency requirements. Those applications typically exchange frequent state updates and therefore require an appropriate dissemination mechanism. Centralized and server-based solutions do not minimize latency, since they always need an extra round-trip to the server. In addition, a server infrastructure constitutes a potential performance bottleneck and thus a scalability limitation. Direct communication between event source and destination is often latency-minimal but quickly exceeds the capacities especially of poorly connected participants because each one needs to communicate individually with many others.#R##N#Our proposed solution, InterestCast, provides a decentralized event dissemination mechanism that uses peer-to-peer event forwarding, allowing powerful participants to help weak participants with the event multiplication and dissemination. To obtain forwarding configurations that best fit the current situation and application needs, InterestCast adapts them dynamically and continuously during runtime. The application’s needs are passed as utility functions, which determine the utility of events with a given latency for a given interest level. Interest levels serve as an abstraction for the importance of events from a specific source, allowing a more fine-grained prioritization than an all-or-nothing subscription model. This is particularly useful if the importance of updates depends on virtual reality distance or another application-specific metric.#R##N#InterestCast runs an incremental local optimization algorithm that repeatedly evaluates all possible rerouting operations from the point of view of the respective local node. In each iteration, the best operation is chosen based on the application’s utility functions and a system model that predicts the effects of a given operation. As this optimization process is run on each node independently, it scales well with the number of participants. The prediction only uses local knowledge#R##N#as well as information from the local neighborhood in up to two hops, which is provided by a neighborhood information exchange protocol.#R##N#Our evaluation shows that the results of InterestCast’s distributed optimization are close to the global optima computed by a integer program solver. Computing the optimum for a given situation globally at runtime, however, is infeasible due to its computational complexity, even with a highly simplified network model. In detailed network simulations, we further demonstrate the superiority of InterestCast over a purely direct event dissemination in online gaming scenarios. In comparison with the direct dissemination, InterestCast significantly reduces the traffic of weak nodes and almost quadruples the possible number of participants for the same average delivery latency of high-interest events.	real-time clock	Max Lehn	2015			real-time computing;simulation;computer science;distributed computing	Robotics	-19.59402866771081	55.96682078020637	68438
76ca67311be5d82cc842c993b331aea9126c71d6	agent-based platform to support the execution of parallel tasks	parallel computing;multi agent system;agent based;mobile agents;parallel computer;mobile agent;grid computing	Research highlights? We present an agent-based architecture to execute parallel tasks in heterogeneous networks. ? It exploits state of the art agent mobility capabilities easing task management. ? Agents provide a high level of flexibility introducing a minimum overhead. ? The platform has been applied to execute a complex knowledge acquisition task. ? Its throughput scales linearly with regards to the available resources. Parallel computing has been radically evolving in the recent years from the supercomputer multi-processor centralised point of view to the modern distributed approaches such as grid computing. The availability of relatively obsolete or underused hardware and the increasing LAN and WAN interconnection speed have motivated the success of those new paradigms. In this paper, we propose the use of agent technology to improve the management, flexibility and reusability of grid-like parallel computing architectures. We present a general purpose agent-based architecture which is able to manage and execute independent parallel tasks through one or several heterogeneous computer networks - or even Internet nodes - exploiting state of the art agent mobility capabilities. A particular application of the proposed architecture to support the execution of a complex knowledge acquisition task is also introduced, showing a high scalability and a very low overhead.		David Sánchez;David Isern;Ángel Rodríguez-Rozas;Antonio Moreno	2011	Expert Syst. Appl.	10.1016/j.eswa.2010.11.073	parallel computing;real-time computing;embarrassingly parallel;computer science;multi-agent system;mobile agent;distributed computing;grid computing	DB	-28.829724320366857	49.05355190494856	68703
95ad5edf04af9a61bec86cfb01f83c7b224e4c68	building replicated internet services using tact: a toolkit for tunable availability and consistency tradeoffs	databases;continents;information resources;staleness;replication;tact;quality of service replicated internet services tact scalability fault tolerant systems replica consistency system availability unseen writes uncommitted writes staleness tunable availability and consistency tradeoffs performance;availability;web and internet services;performance;system availability;software performance evaluation;software fault tolerance;qos;adaptivity;fault tolerant system;vents;urban areas;internet;adaptive systems;tunable availability and consistency tradeoffs;fault tolerant systems;replica consistency;replicated internet services;internet services;uncommitted writes;unseen writes;scalability;computer science;quality of service;web and internet services availability quality of service computer science fault tolerant systems urban areas continents vents adaptive systems databases;high performance;replicated databases;consistency;replicated databases internet information resources software fault tolerance quality of service software performance evaluation	An ultimate goal for modern Internet services is the development of scalable, high-performance, highly-available and fault-tolerant systems. Replication is an important approach to achieve this goal. However, replication introduces the issue of consistency among replicas, which is further complicated by network partitions. Generally, higher consistency levels result in lower system availability in the presence of network partitions. Thus, there is a fundamental tradeoff between consistency and availability in building replicated Internet services. In this paper, we argue that Internet services can benefit from dynamically choosing availability/consistency tradeoffs. With three consistency metrics, Unseen Writes , Uncommitted Writesand Staleness , we show how consistency can be meaningfully quantified for many Internet services. We present the design of the TACT (Tunable Availability and Consistency Tradeoffs) toolkit that allows Internet services to flexibly and dynamically choose their own availability/consistency tradeoffs, enabling differentiated availability/consistency quality of service. Further, TACT makes it possible for Internet services to dynamically trade consistency for performance.	computer architecture simulator;eventual consistency;fault tolerance;high- and low-level;internet;prototype;quality of service;requirement;scalability;web service	Haifeng Yu;Amin Vahdat	2000		10.1109/WECWIS.2000.853861	real-time computing;computer science;database;distributed computing;eventual consistency	OS	-24.842933688456068	52.10784480710363	68771
054786fbef7fa6f1821d952f144034c480a93903	dhalion in action: automatic management of streaming applications		In a world where organizations are being inundated with data from various sources, analyzing data and gaining actionable insights in real-time has become a key service differentiator. Over the last few years, several stream processing frameworks have been developed to address the need for large-scale, real-time analytics. A crucial challenge in these environments is the complexity of configuring, managing and deploying long-running streaming applications. Operators must carefully tune these systems to balance competing objectives such as resource utilization and performance. At the same time, they must also account for external shocks such as unexpected load variations and service degradations. In this demonstration, we show how operators can maintain healthy streaming applications without manual intervention while still meeting their performance objectives. We use Dhalion, an open-source library that sits on top of the streaming application, observes its behavior and automatically takes actions to keep the application in a healthy state. In particular, through various Dhalion policies that are configured by the attendees, we demonstrate how a streaming application can meet its performance objective by automatically configuring the amount of resources needed at the various application stages. We also demonstrate Dhalion’s modularity and extensibility that greatly simplifies the process of developing new policies which address different application requirements. PVLDB Reference Format: Ashvin Agrawal and Avrilia Floratou. Dhalion in Action: Automatic Management of Streaming Applications. PVLDB, 11 (12): 2050-2053, 2018. DOI: https://doi.org/10.14778/3229863.3236257	computer performance;differentiator;extensibility;open-source software;real-time clock;real-time computing;real-time locating system;requirement;stream processing;streaming media	Ashvin Agrawal;Avrilia Floratou	2018	PVLDB	10.14778/3229863.3236257	data mining;computer science	DB	-24.469359518725977	59.233373867400985	68989
39b8ed2d99fe3c98c332775e77bfb4d979405a99	performance issues of grid computing based on different architecture cluster computing platforms	workstation clusters grid computing performance evaluation computer architecture;performance evaluation grid computing computing architecture cluster computing computing infrastructures performance analysis supercomputing grid technology system performance cpu architecture;cluster computing;performance evaluation cpu architecture cluster computing grid computing;performance evaluation;system performance;computer architecture;research paper;cpu architecture;grid computing computer architecture distributed computing computer networks high performance computing concurrent computing computer science central processing unit workstations biology computing;performance analysis;workstation clusters;grid computing	This research paper discusses performance issues of cluster and grid computing platforms, and reasons to support the implementation of these computing infrastructures. A number of benchmark programs are executed in these computing systems, in order to perform performance analysis of experimental results. We are able to show that cluster platforms are excellent alternatives to access to supercomputing, due to its cost/performance, scalability and commodity components factors. In addition, we also show that grid technology is viable by increasing total system performance at no additional cost.	benchmark (computing);computer cluster;grid computing;profiling (computer programming);scalability;supercomputer	Hsun-Chang Chang;Kuan-Ching Li;Yaw-Ling Lin;Chao-Tung Yang;Hsiao-Hsi Wang;Liang-Teh Lee	2005	19th International Conference on Advanced Information Networking and Applications (AINA'05) Volume 1 (AINA papers)	10.1109/AINA.2005.276	fabric computing;space-based architecture;computer architecture;parallel computing;cloud computing;computer cluster;computer science;operating system;end-user computing;data-intensive computing;computer performance;utility computing;grid computing;computing with memory;unconventional computing;autonomic computing	HPC	-20.131719673734285	53.949593575684545	69011
a6d6dad952d35658d5e9a5c481401f7d3a5d7a7d	optimal operator state migration for elastic data stream processing		A cloud-based data stream management system (DSMS) handles fast data by utilizing the massively parallel processing capabilities of the underlying platform. An important property of such a DSMS is elasticity, meaning that nodes can be dynamically added to or removed from an application to match the latter’s workload, which may fluctuate in an unpredictable manner. For an application involving stateful operations such as aggregates, the addition / removal of nodes necessitates the migration of operator states. Although the importance of migration has been recognized in existing systems, two key problems remain largely neglected, namely how to migrate and what to migrate, i.e., the migration mechanism that reduces synchronization overhead and result delay during migration, and the selection of the optimal task assignment that minimizes migration costs. Consequently, migration in current systems typically incurs a high spike in result delay caused by expensive synchronization barriers and suboptimal task assignments. Motivated by this, we present the first comprehensive study on efficient operator states migration, and propose designs and algorithms that enable live, progressive, and optimized migrations. Extensive experiments using real data justify our performance claims.	aggregate data;algorithm;cloud computing;elasticity (data store);experiment;overhead (computing);parallel computing;state (computer science);stream processing	Jianbing Ding;Tom Z. J. Fu;Richard T. B. Ma;Marianne Winslett;Yin Yang;Zhenjie Zhang;Hongyang Chao	2015	CoRR		parallel computing;real-time computing;simulation;computer science;operating system;distributed computing;algorithm	DB	-20.124654330691936	54.9047557840407	69025
79ee074a672d6d26200b82e476ba3e1f92478462	the method to secure scalability and high density in cloud data-center	high density;data center;scalability;power;cloud computing	Recently IT infrastructures change to cloud computing, the demand of cloud data center increased. Cloud computing is a model for enabling ubiquitous, convenient, on-demand network access to a shared computing resources that can be rapidly provisioned and released with minimal management effort, the interest on data centers to provide the cloud computing services is increasing economically and variably. This study analyzes the factors to improve the power efficiency while securing scalability of data centers and presents the considerations for cloud data center construction in terms of power distribution method, power density per rack and expansion unit separately. The result of this study may be used for making rational decisions concerning the power input, voltage transformation and unit of expansion when constructing a cloud data center or migrating an existing data center to a cloud data center. & 2014 Elsevier Ltd. All rights reserved.	access network;cloud computing;data center;grid computing;performance per watt;scalability	YoungJin Choi;SangHak Lee;JinHwan Kim;YongJu Kim;HyeonGyu Pak;GyuYoung Moon;JongHei Ra;Yong-Gyu Jung	2015	Inf. Syst.	10.1016/j.is.2014.05.013	data center;scalability;cloud computing;computer science;operating system;power;cloud testing;distributed computing;world wide web;computer security	HPC	-26.112998846491923	60.140275579287916	69141
e94b2c0274943389f16be1354f8369775550e5c7	dfs-container: achieving containerized block i/o for distributed file systems		Today BigData systems commonly use resource management systems such as TORQUE, Mesos, and Google Borg to share the physical resources among users or applications. Enabled by virtualization, users can run their applications on the same node with low mutual interference. Container-based virtualizations (e.g., Docker and Linux Containers) offer a lightweight virtualization layer, which promises a near-native performance and is adopted by some Big-Data resource sharing platforms such as Mesos. Nevertheless, using containers to consolidate the I/O resources of shared storage systems is still at an early stage, especially in a distributed file system (DFS) such as Hadoop File System (HDFS). To overcome this issue, we propose a distributed middleware system, DFS-Container, by further containerizing DFS. We also evaluate and analyze the unfairness of using containers to proportionally allocate the I/O resource of DFS. Based on these analyses and evaluations, we propose and implement a new mechanism, IOPS-Regulator, which improve the fairness of proportional allocation by 74.4% on average.	apache hadoop;big data;borg (star trek);clustered file system;dce distributed file system;docker;fairness measure;input/output;interference (communication);lxc;linux;middleware;torque	Dan Huang;Qing Liu;Xuhong Zhang;Xunchao Chen;Jian Zhou	2017		10.1145/3127479.3132568	computer science;distributed file system;real-time computing;virtualization;big data;resource management;file system;input/output;shared resource;computer network;middleware	OS	-21.71384866476117	59.32199174975192	69172
1bfaed6a3ae32c39f5b9a61fa528a8d23ff48801	softscale: stealing opportunistically for transient scaling	transient scaling;additional capacity;load spike;possible load spike;multi-tier data center;data center load;additional resource;dynamic capacity;additional capacity online;extreme load spike;28-server multi-tier	Dynamic capacity provisioning is a well studied approach to handling gradual changes in data center load. However, abrupt spikes in load are still problematic in that the work in the system rises very quickly during the setup time needed to turn on additional capacity. Performance can be severely affected even if it takes only 5 seconds to bring additional capacity online. In this paper, we propose SOFTScale, an approach to handling load spikes in multi-tier data centers without having to over-provision resources. SOFTScale works by opportunistically stealing resources from other tiers to alleviate the bottleneck tier, even when the tiers are carefully provisioned at capacity. SOFTScale is especially useful during the transient overload periods when additional capacity is being brought online. Via implementation on a 28-server multi-tier testbed, we investigate a range of possible load spikes, including an artificial doubling or tripling of load, as well as large spikes in real traces. We find that SOFTScale can meet our stringent 95th percentile response time Service Level Agreement goal of 500ms without using any additional resources even under some extreme load spikes that would normally cause the system (without SOFTScale) to exhibit response times as high as 96 seconds.	baseline (configuration management);data center;flip-flop (electronics);interference (communication);load profile;load/store architecture;manycore processor;memcached;multitier architecture;period-doubling bifurcation;provisioning;response time (technology);service-level agreement;state (computer science);stateless protocol;testbed;tracing (software)	Anshul Gandhi;Timothy Zhu;Mor Harchol-Balter;Michael A. Kozuch	2012		10.1007/978-3-642-35170-9_8	real-time computing;simulation;engineering;operations management	OS	-23.08782606948756	59.341094006576455	69242
ebb6b386acebd6b48de8d0bdf9d35b9ce6ceafcd	an extensible architecture for network-attached device management	extensible management architecture network attached device management multi function equipment;human interaction;computer network management computer architecture;computer architecture;operating systems network servers testing computer architecture hardware energy management humans environmental management bandwidth technological innovation;network attached device management;operating system;computer network management;multi function equipment;extensible management architecture;hardware implementation	The development of network-attached devices has ushered in an era of autonomous, multi-function equipment demanding minimal human interaction: the only requirements are data and electricity. Despite these advances, these machines continue underutilized in network environments due to operating system limitations regarding the management of these devices. These limitations force the use of these devices via other network hardware, such as a server, that manage the device access and data. While effective, this results in increased resource consumption and ignores the capabilities presented by network-attached devices. In order to facilitate optimal utilization of these devices, we have designed a new, extensible management architecture for all network-attached devices. This architecture, presented here, supports the central management of network-attached devices while allowing client machines access to the device without intermediate server hardware. Implementation of this paradigm on test networks has decreased resource consumption - especially bandwidth - considerably.	autonomous robot;networking hardware;operating system;programming paradigm;requirement;server (computing)	Michael J. McMahon;Sergiu M. Dascalu;Frederick C. Harris;Juan C. Quiroz	2007	International Conference on Software Engineering Advances (ICSEA 2007)	10.1109/ICSEA.2007.87	out-of-band management;embedded system;element management system;interpersonal relationship;intelligent platform management interface;simulation;network management station;computer science;applications architecture;operating system;software engineering;enterprise architecture management;network management application;data architecture;computer engineering	Mobile	-28.125220912176932	57.36007624905163	69245
e69133ec7699636227dc0931a69263c71bcdea33	enhancing data-intensive applications performance by tuning the distributed storage policies	distributed storage			Zeyad Ali;Qutaibah M. Malluhi	2004			computer science;distributed computing;distributed data store	HPC	-19.509093884506097	52.403002226791735	69283
2c638e7b84061fc9d65d77639ceb1a26f256d1da	a scalable architecture for crowd simulation: implementing a parallel action server	databases;computers;interactive rates;crowd simulations;database management systems;semantic database;crowd simulation;virtual reality cooperative systems database management systems rendering computer graphics software architecture user interfaces;virtual reality;hardware platform;action server;large scale crowd simulation scalable architecture crowd simulation parallel action server intelligent agents autonomous agents interactive rates action server semantic database hardware platform;parallel action server;scalable architectures;computer architecture;large scale;software architecture;large scale crowd simulation;servers;computational modeling;intelligent agents;autonomous agent;cooperative systems;servers computational modeling computer architecture computers scalability proposals databases;scalable architecture;scalable architectures crowd simulations;intelligent agent;scalability;virtual environment;autonomous agents;system architecture;rendering computer graphics;proposals;user interfaces;virtual worlds	Crowd simulation can be considered as a special case of virtual environments where avatars are intelligent agents instead of user-driven entities. These applications require both rendering visually plausible images of the virtual world and managing the behavior of autonomous agents. Although several proposals have focused on the software architectures for these systems, the scalability of crowd simulation is still an open issue. In this paper, we propose a scalable architecture that can manage large crowds of autonomous agents at interactive rates. This proposal consists of enhancing a previously proposed architecture through the efficient parallelization of the action server and the distribution of the semantic database. In this way, the system bottleneck is removed, and new action servers (hosted each one on a new computer) can be added as necessary. The evaluation results show that the proposed architecture is able to fully exploit the underlying hardware platform, regardless of both the number and the kind of computers that form the system. Therefore, this system architecture provides the scalability required for large-scale crowd simulation.	autonomous agent;autonomous robot;computer;crowd simulation;distributed computing;entity;intelligent agent;multi-core processor;parallel computing;scalability;server (computing);software architecture;systems architecture;virtual reality;virtual world	Guillermo Vigueras;Miguel Lozano;Carlos Perez;Juan M. Orduña	2008	2008 37th International Conference on Parallel Processing	10.1109/ICPP.2008.20	space-based architecture;simulation;computer science;applications architecture;autonomous agent;theoretical computer science;operating system;distributed computing;intelligent agent	Robotics	-27.722739832026654	49.42962157924746	69290
824d6a41e368866570b366dc392f478e241a6762	mechanisms for quality of service in web clusters	web system;distributed system;front end;performance evaluation;differentiated service;web service;satisfiability;simulation experiment;settore ing inf 05 sistemi di elaborazione delle informazioni;load sharing;load balance;quality of service;distributed systems	The new generation of Web systems provides more complex services than those related to Web publishing sites. Users are increasingly reliant on the Web for up-to-date personal and business information and services. Web architectures able to guarantee the quality of service (QoS) that rules the relationship between users and Web service providers require a large investment in new algorithms and systems for dispatching, load balancing, and information consistency. In this paper, we consider Web cluster architectures composed of multiple back-end server nodes and one front-end dispatcher and we analyze how to provide differentiated service levels to various classes of users. We demonstrate through simulation experiments under realistic workload models that the proposed mechanisms are able to satisfy QoS requirements of the most valuable users classes, without impacting too negatively on the other users.	quality of service;web server	Valeria Cardellini;Emiliano Casalicchio;Michele Colajanni;Salvatore Tucci	2001	Computer Networks	10.1016/S1389-1286(01)00252-3	web service;real-time computing;web analytics;quality of service;differentiated service;computer science;load balancing;front and back ends;operating system;ws-policy;distributed computing;world wide web;computer security;computer network;satisfiability	Theory	-25.992602004132635	58.95204624424474	69292
ef64656656bea818fa925ea29b3c5106b19450f1	dynamic resource allocation for load balancing in fog environment		Fog computing is emerging as a powerful and popular computing paradigm to perform IoT (Internet of Things) applications, which is an extension to the cloud computing paradigm to make it possible to execute the IoT applications in the network of edge. The IoT applications could choose fog or cloud computing nodes for responding to the resource requirements, and load balancing is one of the key factors to achieve resource efficiency and avoid bottlenecks, overload, and low load. However, it is still a challenge to realize the load balance for the computing nodes in the fog environment during the execution of IoT applications. In view of this challenge, a dynamic resource allocation method, named DRAM, for load balancing in fog environment is proposed in this paper. Technically, a system framework for fog computing and the load-balance analysis for various types of computing nodes are presented first. Then, a corresponding resource allocation method in the fog environment is designed through static resource allocation and dynamic service migration to achieve the load balance for the fog computing systems. Experimental evaluation and comparison analysis are conducted to validate the efficiency and effectiveness of DRAM.		Xiaolong Xu;Shucun Fu;Qing Cai;Wei Tian;Wenjie Liu;Wan-Chun Dou;Xingming Sun;Alex X. Liu	2018	Wireless Communications and Mobile Computing	10.1155/2018/6421607	distributed computing;computer science;resource efficiency;cloud computing;load balancing (computing);internet of things;resource allocation	SE	-20.873238854962715	59.65693664270364	69408
85e60994cb801856fd43a1bfac693dc71f363210	the hesiod name server		Hesiod, the Athena name server, provides naming for services and data objects in a distributed network environment. More specifically, it replaces databases that heretofore have had to be duplicated on each workstation and timesharing machine (e.g., remote file system information, /etc/printcap, /etc/services, /etc/passwd, /etc/group) and provides a flexible mechanism to supply new information as the need arises.	database;group identifier;hesiod (name service);passwd;remote file sharing;server (computing);system information (windows);time-sharing;workstation	Stephen P. Dyer	1988			world wide web;hesiod;name server;computer science	DB	-22.151122544475054	51.547925608317776	69483
7a2645562165423f56370b07832a9dca93e90553	stateful container migration employing checkpoint-based restoration for orchestrated container clusters		Recently, container-leveraged cloud computing technology is being adopted in a wide range of ICT areas. For a more effective management of diversified containers over multiple machine nodes, several container orchestrators including Kubernetes have been utilized in production cloud environments. Due to reliability reasons, stateful containers in an orchestrated container cluster are sometimes required to be migrated to other clusters. Typically, a storage-based migration approach is commonly adopted despite its considerable downtime, which also requires the application-level implementation of storage components for state save and restore. Thus, in this paper, we propose a checkpoint-based approach of stateful container migration for orchestrated container clusters. By checkpointing and restoring in-memory states of processes that are running in the containers with CRIU (Checkpoint/Restore in Userspace) feature, container migration can be realized without the application-level implementation of storage components. We then verify the proposed approach at a PoC(Proof-of-Concept) level by managing the associated migration procedure with the container orchestrator.	application checkpointing;circuit restoration;cloud computing;downtime;external storage;in-memory database;iterative method;master of science in information technology;prototype;software as a service;state (computer science);stateful firewall;system migration;transaction processing system	Seungyong Oh;JongWon Kim	2018	2018 International Conference on Information and Communication Technology Convergence (ICTC)	10.1109/ICTC.2018.8539562	virtual machining;stateful firewall;cloud computing;cluster (physics);real-time computing;downtime;computer science;information and communications technology	HPC	-27.481597145826697	58.08193163027182	69573
c73daa95770a5e0071bebd3f3540ace72b5261e7	specification and complexity of collaborative text editing	collaborative text editing;eventual consistency	Collaborative text editing systems allow users to concurrently edit a shared document, inserting and deleting elements (e.g., characters or lines). There are a number of protocols for collaborative text editing, but so far there has been no precise specification of their desired behavior, and several of these protocols have been shown not to satisfy even basic expectations. This paper provides a precise specification of a replicated list object, which models the core functionality of replicated systems for collaborative text editing. We define a strong list specification, which we prove is implemented by an existing protocol, as well as a weak list specification, which admits additional protocol behaviors.  A major factor determining the efficiency and practical feasibility of a collaborative text editing protocol is the space overhead of the metadata that the protocol must maintain to ensure correctness. We show that for a large class of list protocols, implementing either the strong or the weak list specification requires a metadata overhead that is at least linear in the number of elements deleted from the list. The class of protocols to which this lower bound applies includes all list protocols that we are aware of, and we show that one of these protocols almost matches the bound.	collaborative real-time editor;communications protocol;correctness (computer science);overhead (computing);text editor	Hagit Attiya;Sebastian Burckhardt;Alexey Gotsman;Adam Morrison;Hongseok Yang;Marek Zawirski	2016		10.1145/2933057.2933090	computer science;operating system;self-organizing list;data mining;database;distributed computing;eventual consistency;programming language;world wide web	PL	-25.359044088264476	47.54097390681924	69575
0b6304f00d5d28d6b31f1a757200a211752decdb	recent advances of the cloud platform delivered in the infrastructure as a service model for the pl-grid scientific communities		This paper describes the work done to provide production-grade Cloud system for the scientific communities of the PLGrid Plus project. Our goal was both to show generic architecture of the Open-Nebula-based platform as well as enhancements we had to provide in order to meet the needs of our users and the requirements of the platform. Numerous solutions have been developed and integrated such as the authentication and authorization mechanism working with the standard project accounts and based on X.509 proxy certificates, ai¾źgroup synchronization solution, flexible way to access cloud instances despite the need to conserve IPv4 resources either through Network Address Translation NAT mechanism or the Virtual Private Network VPN. We also describe some security implications, which are crucial in the case of cloud systems.	cloud computing;google cloud platform;polish grid infrastructure pl-grid	Jan Meizner;Maciej Nabozny;Marcin Radecki;Tomasz Szepieniec;Milosz Zdybal	2014		10.1007/978-3-319-10894-0_4	simulation;engineering;data science;world wide web;converged infrastructure	HPC	-32.02510261090959	54.67691979210045	69585
1e4198ea46fbc35138767562ec8fa712403404f6	investigation of containerizing distributed petri net simulations		Reference nets combine the power of an object-oriented programming language with the formalism of Petri nets. Using them makes it possible to model complex concurrent systems easier. By doing so with a tool like Renew, we achieve a directly executable (local) simulation of the system. In recent publications, ideas have been presented to transpose these simulations into one distributed simulation. However, being a genuinely concurrent system, the problem arises, that some nodes are less utilized than others slowing down the simulation in its entirety. In this paper, we define requirements to a distributed simulation and evaluate how specific technical approaches harmonize with them. We present a first reference net based prototype for an approach using containerization and Docker for the execution of a single net simulation truly distributed on several (virtual) machines. As an example of a workload with differing complexity, the prototype calculates parts of the Mandelbrot set in a distributed manner. A significant advantage of the approach is that inherent complexity of a problem may be unknown without restraining the method.	autoscaling;bare machine;complexity;computer simulation;concurrency (computer science);distributed computing;docker;executable;mandelbrot set;petri net;programming language;prototype;requirement;semantics (computer science);virtual machine	Jan Henrik Röwekamp;Daniel Moldt;Matthias Feldmann	2018			petri net;computer science;distributed computing	HPC	-28.85808872826397	53.44222366652743	69964
223c64598e74823438399e4bfa60c2c371db3a14	serializable executions with snapshot isolation and two-phase locking: revisited	databases;silicon;data integrity;database management systems concurrency control data integrity;database management systems;silicon optical wavelength conversion databases concurrency control servers throughput benchmark testing;pivot_ordered2pl serializable executions snapshot isolation two phase locking concurrency control mechanism open resources platforms database integrity constraints;servers;concurrency control;optical wavelength conversion;benchmark testing;throughput	Snapshot Isolation (SI) is a concurrency control mechanism that has been implemented by several commercial and open resources platforms. However, under SI, a set of program may experience a non-serializable execution, in which database integrity constraints can be violated. An elegant approach from Fekete (2005) shows how to guarantee serializable execution on platforms that offer both SI and traditional two-phase locking (2PL) concurrency control, by running some transactions (pivots) with 2PL and the rest at SI. While Fekete's Pivot 2PL technique performs better than running all transactions at 2PL, it often loses much performance compared to SI for all transactions. In this paper we identify causes that harm performance of Pivot 2PL, and we propose an improved approach, called Pivot Ordered2PL, in which a few transactions are rewritten (without changing their functionality). We evaluate Pivot Ordered2PL and find it ensures serializable execution with performance close to that of SI.	concurrency (computer science);concurrency control;data integrity;fekete polynomial;lock (computer science);pivot table;serializability;snapshot isolation;two-phase locking	Mohammad Alomari	2013	2013 ACS International Conference on Computer Systems and Applications (AICCSA)	10.1109/AICCSA.2013.6616497	timestamp-based concurrency control;optimistic concurrency control;parallel computing;real-time computing;isolation;computer science;database;multiversion concurrency control;non-lock concurrency control;serializability;acid;snapshot isolation;distributed concurrency control	DB	-22.63140093097963	48.27861835152188	70029
b91a2ada136b9419a11851ad9089138ef4013c6e	research and improvement on pxe security of dynamic distributed network of non-fixed disk	sims secure information management system;diskless network;pxe pre boot execution environment;anthropic factor	PXE(Pre-boot Execution Environment) tech. has played great roles for the powerful compatibility and ease for maintenance. Internet requirement for data safety are more and more important while PXE methods is lacked. The applies diskless tech and dynamic distributed security system on data storage has been analyzed. According the PXE tech. improvement and design thus reduce the risk of letting out and dependability of the network data under the strict data management. The function may bring huge economic benefit with the popularization of the applications and low maintenance cost.	computer data storage;dependability;diskless node;hard disk drive;preboot execution environment	Guanli Huang;Bin Yang	2012	JCP	10.4304/jcp.7.7.1681-1687	real-time computing;simulation;computer science;artificial intelligence;operating system;world wide web;computer security	DB	-30.014478855361563	58.122897019383466	70155
dba26410d14c0b092d118eb31b704e2f6954c812	editor's message to special issue on network services and distributed processing				Kazuhiko Kushima	2016	JIP	10.2197/ipsjjip.24.182	computer science;theoretical computer science;distributed computing;message broker	HPC	-30.0259467576943	47.04005165541731	70268
0a101e82feb1bfbadddf0ba79001dcbdb45b6add	cloud network architecture design patterns	infrastructure as a service;patterns;cloud computing	"""During the past decade the cloud service market is one of the fastest growing segments around the world. The amount of companies that turned to the cloud has been steadily growing, since paying for a """"shared"""" Cloud service over a given period of time reduces the capital expenditures and turned out to be better than using a dedicated hardware. This paper is focused on the architecture and the design of a shared public cloud infrastructure as a service."""	cloud computing;computer security;design pattern;fastest;high availability;load balancing (computing);multitenancy;network architecture;requirement	Vassil Gourov;Elissaveta Gourova	2015		10.1145/2855321.2855323	cloud computing security;simulation;cloud computing;engineering;distributed computing;data as a service;computer security	Arch	-28.629090250919468	57.66933686072865	70310
bf569c5414ac87f5b95e18d824af12a6692c495f	distributed algorithms and educational simulation/visualisation in collaborative environments	publikationer;konferensbidrag;collaborative environment;artiklar;rapporter;distributed algorithm	This thesis examines a general class of applications called collaborative environmentswhich allow in real-time multiple users to share and modify information in spite of not being present at the same physical location. Two views on collaborative environments have been taken by examining algorithms supporting the implementation of collaborative environments as well as using collaboration to support educational visualisation/simulation environments in the context of distributed computing. Important algorithmic aspects of collaborative environments are to provide scalable communication which allows interest management of processes and deal with modification of information among collaborators in an efficient and consistent manner. However, to support real-time interactions one may need to trade strong reliability guarantees for efficiency and scalability. This work examines lightweight decentralised peer-to-peer algorithms which can scale to a large number of processes and offer reliability expressed with probabilistic guarantees. Besides proposing and evaluating peer-to-peer algorithms on support for large scale event dissemination, this work considers the impact of buffer management and membership in achieving stable performance even under critical system parameters. Moreover, we examine consistency management as well as interest management in combination with lightweight peer-to-peer dissemination schemes. Lightweight cluster consistency management allows a dynamic set of processes to perform updates on a set of processes which is observed in optimistic causal order. Topic-aware membership management supports lightweight dissemination schemes to propagate events which correspond only to their interest by providing a fair work distribution are the same time. The second part deals with collaboration in the context of educational simulation/visualisation. We present and evaluate a visualisation/simulation environment for distributed algorithms called LYDIAN which helps studying distributed algorithms and protocols. Besides looking at what environments such as LYDIAN should provide in order to be successfully used in class, we consider the use of collaboration in providing more interactivity in simulation environments as well as a possibility for new visualisations of learning concepts to evolve.	causal filter;critical system;distributed algorithm;distributed computing;infographic;interaction;interactivity;multi-user;peer-to-peer;real-time clock;real-time transcription;scalability;scientific visualization;simulation	Boris Koldehofe	2005			simulation;computer science;data mining;distributed computing	DB	-26.360353612969725	49.21197773900237	70365
2b1b1e68dd574f9ef6a0dadbfb093c37348ffdab	d2t: doubly distributed transactions for high performance and distributed computing	protocols;acid compliance d 2 t protocol doubly distributed transactions high performance computing distributed computing exascale computing projections monolithic simulation scientific discovery process online workflow inter step operations dynamic load balancing fault tolerance techniques database like transactions distributed servers;resource allocation;distributed processing;io tuning;resource allocation distributed processing fault tolerant computing natural sciences computing protocols;transaction;io;data staging;servers;exascale;fault tolerant computing;computational modeling;hpc;fault tolerant systems;fault tolerance;servers protocols fault tolerance fault tolerant systems scalability couplings computational modeling;io tuning hpc exascale io data staging transaction;scalability;couplings;natural sciences computing	Current exascale computing projections suggest rather than a monolithic simulation running for the majority of the machine, a collection of components comprising the scientific discovery process will be employed in an online workflow. This move to an online workflow scenario requires knowledge that inter-step operations are completed and correct before the next phase begins. Further, dynamic load balancing or fault tolerance techniques may dynamically deploy or redeploy resources for optimal use of computing resources. These newly configured resources should only be used if they are successfully deployed. Our D2T system offers a mechanism to support these kinds of operations by providing database-like transactions with distributed servers and clients. Ultimately, with adequate hardware support, full ACID compliance is possible for the transactions. To prove the viability of this approach, we show that the D2T protocol has less than 1.2 seconds of overhead using 4096 clients and 32 servers with good scaling characteristics using this initial prototype implementation.	acid;database;distributed computing;distributed transaction;fault tolerance;image scaling;load balancing (computing);overhead (computing);prototype;simulation	Jay F. Lofstead;Jai Dayal;Karsten Schwan;Ron Oldfield	2012	2012 IEEE International Conference on Cluster Computing	10.1109/CLUSTER.2012.79	communications protocol;fault tolerance;supercomputer;parallel computing;real-time computing;scalability;resource allocation;computer science;operating system;distributed computing;coupling;computational model;server;computer network	HPC	-23.449238882679285	54.297776831694385	70396
054ad2a04f117e849634c063462b8b0ba3c79423	fun academic cloud computing	fun academic cloud computing learning experience cloud services computing power learning institutes public cloud computing system learning games cloud enabled services game based learning developers facc framework public cloud computing services delivery models deployment models end users online services computational resources scalable storage learn and play;human computer interaction;human computer interaction cloud computing game based learning;game based learning;lead organizations games servers face yttrium;computer games cloud computing computer aided instruction;cloud computing	Cloud Computing is considered as a provider of scalable storage and computational resources, which supports various types of online services for end users. This paper discusses cloud computing concepts, deployment models, and delivery models. Different public cloud computing services are compared. The Fun Academic Cloud Computing (FACC) framework is proposed to help game-based-learning developers determine how best to employ cloud-enabled services. FACC promotes building efficient, cost effective, and powerful learning games that can be used by many users anytime and anywhere. We discuss how to link the proposed framework with the appropriate public cloud computing system. Using this framework, learning institutes can access increased computing power through cloud services and enhance the learning experience.	anytime algorithm;cloud computing;computational resource;e-services;scalability;software deployment	Shoug Alfadhli;Asmaa Alsumait	2015	2015 Second International Conference on Computer Science, Computer Engineering, and Social Media (CSCESM)	10.1109/CSCESM.2015.7331826	cloud computing security;simulation;cloud computing;computer science;cloud testing;utility computing;multimedia;services computing;world wide web	HPC	-33.577443380042226	57.87968736597855	70416
576646da3c337641a1ce7b76545bbdd701f1e311	decisionqos: an adaptive, self-evolving qos arbitration module for storage systems	unregulated competition;performance guarantee;rule based policy;policy management system;system administrators;storage system;storage systems;large centralized data centers;resource arbitration problem;application software;adaptive qos arbitration module;resource allocation;qos adaptation;rule based;qos guarantee;resource management;resource management quality of service computer networks environmental management power system management storage area networks application software switches quadrature amplitude modulation monitoring;client server systems;excessive complexity;self evolving qos arbitration module;system performance;computer networks;storage area networks;expressive power;incomplete information;data center;decision support systems quality of service resource allocation learning artificial intelligence open systems client server systems computer network management file organisation;machine learning;monitoring;qos guarantees;open enterprise systems;power system management;policy management system decisionqos adaptive qos arbitration module self evolving qos arbitration module storage systems computing infrastructures storage infrastructures networking infrastructures large centralized data centers resource sharing open enterprise systems unregulated competition rule based solutions resource arbitration problem excessive complexity multiple competing clients qos guarantees system administrators system performance resource allocation decision workload requirements machine learning rule based policy;computer network management;computing infrastructures;decision support systems;resource sharing;enterprise system;learning artificial intelligence;quality of service;storage infrastructures;switches;open systems;environmental management;networking infrastructures;quadrature amplitude modulation;multiple competing clients;workload requirements;rule based solutions;resource allocation decision;policy management;file organisation;decisionqos	As a consequence of the current trend towards consolidating computing, storage and networking infrastructures into large centralized data centers, applications compete for shared resources. Open enterprise systems are not designed to provide performance guarantees in the presence of sharing; unregulated competition is very likely to result in a free-for-all where some applications monopolize resources while others starve. Rule-based solutions to the resource arbitration problem suffer from excessive complexity, brittleness, and limitations in their expressive power. We present DecisionQoS, a novel approach for arbitrating resources among multiple competing clients while enforcing QoS guarantees. DecisionQoS requires system administrators to provide a minimal, declarative amount of information about the system and the workloads running on it. That initial input is continuously refined and augmented at run time, by monitoring the system's performance and its reaction to resource allocation decisions. When faced with incomplete information, or with changes in the workload requirements or system capabilities, DecisionQoS adapts to them by applying machine learning techniques; the resulting scheme is highly resilient to unforeseen events. Moreover, it overcomes significant shortcomings of pre-existing, rule-based policy management systems.	centralized computing;complexity;computer data storage;data center;declarative programming;disk array;enterprise system;event condition action;expressive power (computer science);high- and low-level;intrinsic dimension;logic programming;machine learning;network switch;novell open enterprise server;programming paradigm;provisioning;quality of service;requirement;rule-based system;run time (program lifecycle phase);scheduling (computing);system administrator	Sandeep Uttamchandani;Guillermo A. Alvarez;Gul A. Agha	2004	Proceedings. Fifth IEEE International Workshop on Policies for Distributed Systems and Networks, 2004. POLICY 2004.	10.1109/POLICY.2004.1309151	real-time computing;distributed computing;business;computer security	OS	-23.870220695643567	60.10861050758525	70749
676f95b06cb00854d0da3387c652c252f40dec09	distributed state monitoring for iaas cloud with continuous observation sequence	measurement;anomaly detection;training;hmm;hidden markov models;monitoring;markov processes;cloud computing;data models	Cloud computing has become increasing popular by freeing users from the low-level task of setting up the hardware and managing the system software. Anomaly detection is an effective approach to enhancing availability and reliability of Cloud infrastructures. In this paper, we propose a supervised online anomaly detection scheme that analyses monitoring data within current sliding data window and judging the current working state of the monitored component in Cloud based on Hidden Markov Model (HMM). What makes our method different from existing anomaly detecting is that it determines current state in context of recent continuous monitoring data, instead of isolated data point. Besides, our algorithm is basically distributed and runs locally on each computing machine on the Cloud in order to achieve high scalability. Experiments performed on real data sets validate the fact that our algorithm can effectively detect performance anomalies while imposing low overhead to the infrastructure in Cloud.	algorithm;anomaly detection;black box;cloud computing;data point;experiment;hidden markov model;high- and low-level;markov chain;overhead (computing);scalability;sensor;sparse matrix;unsupervised learning	Bin Hong;Yazhou Hu;Fuyang Peng;Bo Deng	2015	2015 IEEE 12th Intl Conf on Ubiquitous Intelligence and Computing and 2015 IEEE 12th Intl Conf on Autonomic and Trusted Computing and 2015 IEEE 15th Intl Conf on Scalable Computing and Communications and Its Associated Workshops (UIC-ATC-ScalCom)	10.1109/UIC-ATC-ScalCom-CBDCom-IoP.2015.193	data modeling;real-time computing;cloud computing;computer science;machine learning;data mining;distributed computing;markov process;hidden markov model;measurement	Metrics	-22.960632287194873	55.80567482376961	70781
a2408c13ce831b9aadca03f458b423cb28fb8a8a	instrumenting cloud caches for online workload monitoring: the case of online miss rate curve estimation in memcached		Fast and efficient algorithms to estimate miss rate curves have recently been proposed, yet these have not been incorporated into cloud caches. Numerous applications that could benefit from these techniques are relying on less useful cache metrics or incomplete information. We study how to instrument cloud caches to obtain online miss rate curves (MRCs). Our approach leverages state-of-the-art algorithms and data structures, thus incurring in negligible overhead. We also propose an alternative design that makes it easier to change the MRC estimation algorithm, as well as plug-in other monitoring techniques. We implemented our designs in one of the top cloud caches: Memcached. We show via experimentation, that our implementation is efficient. Finally, we discuss how our solution can be used to improve the management of cloud caches; in particular, our code can be used by caching middleware to auto-adapt to changes in workload and maximize performance.	algorithm;amazon web services;cloud computing;cloud storage;data structure;design pattern;instrumentation (computer programming);memcached;middleware;overhead (computing);plug-in (computing);redis;stream processing	Jorge R. Murillo;Gustavo Totoy;Cristina L. Abad	2017		10.1145/3152881.3152884	complete information;workload;cache;cloud computing;operating system;middleware;data structure;computer science	Metrics	-20.9970093182864	55.218122539284295	70939
61cad3a81effde709fbebce3f4ae960264e8c3be	workload management for dynamic mobile device clusters in edge femtoclouds		Edge computing offers an alternative to centralized, in-the-cloud compute services. Among the potential advantages of edge computing are lower latency that improves responsiveness, reduced wide-area network congestion, and possibly greater privacy by keeping data more local. In our previous work on Femtoclouds, we proposed taking advantage of clusters of devices that tend to be co-located in places such as public transit, classrooms or coffee shops. These clusters can perform computations for jobs generated from within or outside of the cluster. In this paper, we address the full requirements of workload management in Femtoclouds. These functions enable a Femtocloud to provide a service to job initiators that is similar to that provided by a centralized cloud service. We develop a system architecture that relies on the cloud to efficiently control and manage a Femtocloud. Within this architecture, we develop adaptive workload management mechanisms and algorithms to manage resources and effectively mask churn. We implement a prototype of our Femtocloud system on Android devices and utilize it to evaluate the overall system performance. We use simulation to isolate and study the impact of our workload management mechanisms and test the system at scale. Our prototype and simulation results demonstrate the efficiency of the Femtocloud workload management mechanisms especially in situations with potentially high churn. For instance, our mechanisms can reduce the average job completion time by up to 26% compared to similar mechanisms used in traditional cloud computing systems when used in situations that suggest high churn.	algorithm;android;centralized computing;cloud computing;computation;edge computing;job stream;mobile device;network congestion;prototype;requirement;responsiveness;simulation;systems architecture	Karim Habak;Ellen W. Zegura;Mostafa H. Ammar;Khaled A. Harras	2017		10.1145/3132211.3134455	workload;computer security;computer science;architecture;real-time computing;utility computing;mobile cloud computing;systems architecture;cloud computing;mobile computing;distributed computing;edge computing	HPC	-21.367367614990318	59.87037371362775	71032
83cf0c25564f92ac6e4f2f535680477d0d542acd	cic portal: a collaborative and scalable integration platform for high availability grid operations	high availability grid operation;operational point;cic operations portal;essential egee;egee infrastructure management tool;daily operational need;cic portal;lcg core service;largest grid production infrastructure;scalable integration platform;web portal;overall daily monitoring;grid computing;web interface;high availability;internet;it management	"""EGEE, along with its sister project LCG, manages the world's largest grid production infrastructure which is spreading nowadays over 260 sites in more than 40 countries. Just as building such a system requires novel approaches; its management also requires innovation. From an operational point of view, the first challenge we face is to provide scalable procedures and tools able to monitor the ever expanding infrastructure and the constant evolution of the needs. The second is to ensure that all these tools strongly interact with one another, even though their development is spread out worldwide. Consequently, our goal is to provide a homogeneous way to access tools and analyze data for daily operational needs. To implement this concept into LCG/EGEE infrastructure management tools, 1N2P3 Computing Centre proposed a web portal, named """"CIC operations portal"""", conceived and built as an integration platform for existing features and new requirements. Firstly, we describe the initial needs that led us to the present architecture of this portal. We then emphasize a specific feature for the operations efficiency which is the web interface dedicated to EGEE overall daily monitoring. We also deal with the high availability mechanism put in place by INFN-CNAF to address failover and replication issues. We finally present how the CIC portal has become one of the essential EGEE and LCG core services."""	egi;failover;high availability;integration platform;requirement;scalability;user interface;worldwide lhc computing grid	Osman Aidel;Alessandro Cavalli;Hélène Cordier;Cyril L'Orphelin;Gilles Mathieu;Alfredo Pagano;Sylvain Reynaud	2007	2007 8th IEEE/ACM International Conference on Grid Computing		the internet;information technology management;computer science;operating system;data mining;database;high availability;user interface;world wide web;grid computing	HPC	-31.27042322773274	51.72559798422227	71125
603048ef2d1ce1ff741ef9844baf63376c115525	hypervisor performance analysis for real-time workloads	kernel;virtualization;virtual machine monitors;linux;program processors;cloud computing;real time systems	Virtualization has become a key technology used in modern data centers. What began as a tool for server consolidation and energy efficiency has grown into an enabler for cloud computing. Cloud computing has become an accepted best practice for data centers. Virtualization is also becoming a key component of embedded and real-time systems in automotive systems, game consoles, and industrial settings. Many applications that run on embedded systems are bound to hard real-time requirements, meaning that deadlines must be met. While hypervisor performance for non-real-time workloads has been well-documented, comparison of performance across hypervisors for real-time workloads has not been systematically studied. In this paper, we fill that gap by characterizing Xen (Credit and Real-Time-Deferrable-Server schedulers) and Wind River's low-latency KVM for hard real-time workloads in one and two virtual machine (VM) cases. We compare each hypervisor to a non-virtualized base system and evaluate the relative merits of each hypervisor for a number of synthetic workloads with a number of varying characteristics. When using a single VM, and therefore no resource contention, we find that all configurations are capable of completing over 99.8% of their jobs within their deadline. This demonstrates that a virtualized environment can support some real-time applications, such as those with relaxed constraints. We also find that Wind River's low-latency KVM and Xen-RTDS scheduler are capable of supporting hard real-time constraints over a variety of task set characteristics. However, a use case with multiple VMs creates a need for resource sharing. The sharing of resources results in contention between guests for usage of the said resources. When two VMs are present, the XenRTDS and tuned Xen-credit scheduler are superior to the other configurations. Furthermore, we find that the default Xen-credit scheduler provides poor support for hard real-time constraints, regardless of whether resource contention is present. However, by tuning the credit scheduler's timeslice parameter to benefit latency-sensitive tasks, we observe that the performance of the credit scheduler is indeed able to support hard real-time constraints for the given workloads in both single VM and two VM cases. These developments in real-time virtualization prove to be an exciting step towards real-time enabled clouds.	best practice;cloud computing;data center;embedded system;hypervisor;profiling (computer programming);real-time clock;real-time computing;real-time transcription;requirement;resource contention;scheduling (computing);semiconductor consolidation;server (computing);synthetic intelligence;virtual machine	Geoffrey Phi C. Tran;Yu-An Chen;Dong-In Kang;John Paul Walters;Stephen P. Crago	2016	2016 IEEE High Performance Extreme Computing Conference (HPEC)	10.1109/HPEC.2016.7761610	embedded system;full virtualization;real-time computing;virtualization;storage hypervisor;cloud computing;computer science;virtual machine;operating system	Embedded	-21.717992742994127	60.40891524060474	71252
ba0b15e23381c9631ae01920049dbd93c7484348	object-oriented distributed transaction scheduling with load balancing in workflow management systems	object oriented;workflow management system;load balance;distributed transactions		distributed transaction;load balancing (computing);management system;scheduling (computing)	Tomoko Baba;Keiji Kaneyoshi;Sachiko Okuda;Sayako Okuda;Seiko Takahashi;Motoyasu Nagata	2003			distributed computing;workflow management system;computer science;workflow technology;database;workflow engine;scheduling (computing);load balancing (computing);distributed transaction;object-oriented programming	DB	-28.915525081799057	46.672850467527404	71276
d9e997532340c19224d20b97a002358e6b9067cc	cyber infrastructure for community remote sensing	communities distributed databases standards organizations organizations data systems remote sensing computer architecture;integrated rule oriented data system;data grid middleware community remote sensing cyber infrastructure cyber infrastructure integrated rule oriented data system;cyber infrastructure;standards organizations;data grids;remote sensing grid computing middleware;community remote sensing;computer architecture;irods;remote sensing;distributed databases;data systems;middleware;irods cyber infrastructure community remote sensing data grids;organizations;communities;data grid middleware;grid computing;data grid	Community Remote Sensing (CRS) is an emerging field where information is collected about the environment by the general public and then integrated into collections to provide a holistic view of the environment with local details. We argue the need for a common architecture for the cyber-infrastructure that will be necessary to cater to the needs of Community Remote Sensing systems. We identify the challenges that such a cyber infrastructure (CRS-CI) has to meet and also proposed five principles as solutions to meet these challenges. Finally, we also describe the integrated Rule Oriented Data System, a data grid middleware that is built upon these principles which provides an ideal and exemplar implementation for CRS-CI.	data system;holism;ibm notes;middleware;programming paradigm	Arcot Rajasekar;Reagan Moore;Mike Wan;Wayne Schroeder	2010	2010 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2010.5650855	computer science;organization;middleware;data grid;data mining;database;data system;distributed database;computer security;grid computing;remote sensing	Mobile	-30.98299875815281	50.89726108768459	71342
d9dc0e64b3701ef3babf68f36a66c8d55b159b7c	toward a general distributed messaging framework for online transaction processing applications		This paper presents a general distributed messaging framework for online transaction processing applications in e-business industry, and we name it as Metamorphosis (MetaQ). Specifically, this messaging framework has features of high availability, scalability, and high data throughput. The current implementation of MetaQ supports distributed XA transactions, asynchronous messaging, and multiple ways for storing message offsets. As a consequence, it is suited to the application contexts having a large quantity of messages, transaction-support, and real-time requirements. More important, another branch of MetaQ implementation, i.e., RocketMQ has been deployed and used in Taobao.com and Alipay.com. The real usages in both typical online transaction applications have proven that the nature of MetaQ can perform well for such big Internet applications.	database transaction;electronic business;high availability;message-oriented middleware;online transaction processing;real-time clock;real-time computing;requirement;scalability;taobao marketplace;throughput;x/open xa	Jianwei Liao;Xiaodan Zhuang;Renyi Fan;Xiaoning Peng	2017	IEEE Access	10.1109/ACCESS.2017.2717930	messaging pattern;online transaction processing;computer network;real-time computing;scalability;middleware (distributed applications);computer science;distributed computing;distributed transaction;transaction processing system;message broker;transaction processing	DB	-27.088930664139983	54.77049263984603	71505
000d3c264725ef70000b19468d6665922fe1e4e1	a general approach to real-time workflow monitoring	log analysis;workflow performance statistics;scientific workflows;qa75 electronic computers computer science;system recovery;monitoring;workflow management software;interoperability;data handling;open systems;real time systems	Scientific workflow systems support different workflow representations, operational modes and configurations. However, independent of the system used, end users need to track the status of their workflows in real time, be notified of execution anomalies and failures automatically, perform troubleshooting and automate the analysis of the workflow to help categorize and qualify the results. In this paper, we describe how the Stampede monitoring infrastructure, which was previously integrated in the Pegasus Workflow Management System, was employed in Triana in order to add generic real time monitoring and troubleshooting capabilities across both systems. Stampede is an infrastructure that attempts to address interoperable monitoring needs by providing a three-layer model: a common data model to describe workflow and job executions; high-performance tools to load workflow logs conforming to the data model into a data store, and a querying interface for extracting information from the data store in a standard fashion. The resulting integration demonstrates the generic nature of the Stampede monitoring infrastructure that has the potential to provide a common platform for monitoring across scientific workflow engines.	categorization;common platform;data model;data store;global variable;interoperability;management system;multitier architecture;pegasus;real-time computing;real-time transcription	Karan Vahi;Ian Harvey;Taghrid Samak;Dan Gunter;Kieran Evans;David H. Rogers;Ian J. Taylor;Monte Goode;Fabio Silva;Eddie Al-Shakarchi;Gaurang Mehta;Andrew Jones;Ewa Deelman	2012	2012 SC Companion: High Performance Computing, Networking Storage and Analysis	10.1109/SC.Companion.2012.26	interoperability;workflow;xpdl;computer science;operating system;workflow management coalition;group method of data handling;data mining;database;distributed computing;windows workflow foundation;open system;world wide web;workflow management system;workflow engine;workflow technology	HPC	-30.439331497770308	53.16263436698068	71659
11ad7a20d1d74d9ba5b802a48b0cc520d9f2dc76	the effects of metadata corruption on nfs	reliability;fault tolerant;nfs;retry;silent failure;fault tolerance;distributed file system;fault injection;metadata corruption;inconsistency	Distributed file systems need to be robust in the face of failures. In this work, we study the failure handling and recovery mechanisms of a widely used distributed file system, Linux NFS. We study the behavior of NFS under corruption of important metadata through fault injection. We find that the NFS protocol behaves in unexpected ways in the presence of these corruptions. On some occasions, incorrect errors are communicated to the client application; inothers, the system hangs applications or crashes outright; in a few cases, success is falsely reported when an operation has failed. We use the results of our study to draw lessons for future designs and implementations of the NFS protocol.	client (computing);clustered file system;crash (computing);dce distributed file system;failure;fault injection;linux	Swetha Krishnan;Giridhar Ravipati;Andrea C. Arpaci-Dusseau;Remzi H. Arpaci-Dusseau;Barton P. Miller	2007		10.1145/1314313.1314324	real-time computing;computer science;distributed computing;computer security	OS	-22.706726062249675	50.02935354221244	71712
16f99977471dff0001eb30c0c7483562dfc5c8f4	transparent process migration: design alternatives and the sprite implementation	distributed system;migration;transparence;distributed processing;simulation;experience;computer networks;operating system;workstations;interprocessor communication;load sharing;distributed systems;process migration;operating systems computers;word processing;operating systems	The Sprite operating system allows executing processes to be moved between hosts at any time. We use this process migration mechanism to offload work onto idle machines, and also to evict migrated processes when idle workstations are reclaimed by their owners. Sprite’s migration mechanism provides a high degree of transparency both for migrated processes and for users. Idle machines are identified, and eviction is invoked, automatically by daemon processes. On Sprite it takes up to a few hundred milliseconds on SPARCstation 1 workstations to perform a remote exec, whereas evictions typically occur in a few seconds. The pmake program uses remote invocation to invoke tasks concurrently. Compilations commonly obtain speed-up factors in the range of three to six; they are limited primarily by contention for centralized resources such as file servers. CPU-bound tasks such as simulations can make more effective use of idle hosts, obtaining as much as eight-fold speed-up over a period of hours. Process migration has been in regular service for over two years.	central processing unit;centralized computing;daemon (computing);distributed object communication;file server;operating system;process migration;remote procedure call;simulation;sprite;workstation	Fred Douglis;John K. Ousterhout	1991	Softw., Pract. Exper.	10.1002/spe.4380210802	parallel computing;real-time computing;process migration;workstation;computer science;human migration;operating system;distributed computing	OS	-20.323039617743195	50.47753825710174	71739
1477864821ddc29257dd9604f2e231fa3213f9a6	combining generality and practicality in a conit-based continuous consistency model for wide-area replication	databases;model design;sensor systems and applications;wide area replication;data integrity;generality;application independent consistency protocols;application software;prototypes;performance;collaboration;software performance evaluation;metrics;data integrity replicated databases software performance evaluation wide area networks;consistency model;large scale;per write weight specification conit based continuous consistency model wide area replication large scale replication wide area networks consistency performance replicated databases generality practicality application specific consistency semantics application independent consistency protocols metrics;programming profession;large scale replication;access protocols;application software programming profession computer science prototypes access protocols hafnium databases collaboration sensor systems and applications large scale systems;application specific consistency semantics;computer science;relaxed consistency;per write weight specification;practicality;replicated databases;consistency;wide area network;wide area networks;large scale systems;conit based continuous consistency model;hafnium	Replication is a key approach to scaling wide-area applications. However, the overhead associated with largescale replication quickly becomes prohibitive across widearea networks. One effective approach to addressing this limitation is to allow applications to dynamically trade reduced consistency for increased performance and availability. Although extensive study has been performed on relaxed consistency models in traditional replicated databases, none of the models can simultaneously achieve the following two typically conflicting requirements imposed by wide-area applications:generality(capturing applicationspecific consistency semantics) and practicality (enabling efficient application-independent consistency protocols to be designed and providing natural ways to express appli-	application programming interface;consistency model;database;eventual consistency;image scaling;overhead (computing);prototype;requirement	Haifeng Yu;Amin Vahdat	2001		10.1109/ICDSC.2001.918973	application software;performance;computer science;theoretical computer science;consistency model;release consistency;data integrity;data mining;database;prototype;consistency;eventual consistency;hafnium;metrics;sequential consistency;collaboration	OS	-24.831861807541124	51.83752115088851	71829
6613eb71a81ffb0492ebc2fe5c68329674fcf5db	reparallelization and migration of openmp applications in grid environments		Grid computing opens up a new computing infrastructure and users gain access to a vast landscape of computing resources that are spread over the world and that are instantly accessible through the Internet. In the future, computational Grids are expected to deliver high-end computing power at the user’s finger tips. Today, typical users of a computational Grid only exploit a single cluster and are faced with the job schedulers that assign computing resources to applications. Job schedulers expect a resource estimation to exclusively allocate parts of the computing system for the application. Users typically over-estimate the resource limits of their applications to avoid premature termination of the application by the job scheduler. Over-estimating resource limits has negative effects on both the clusters’ schedules as well as on the waiting time until an application eventually executes. In this work, we present a solution that alleviates the need of estimating the resource limits for OpenMP applications. A reparallelization of OpenMP applications is automatically computed, when new computing resources become available or are withdrawn. The application can be migrated between clusters of the Grid if an allocated resource is about to be exceeded. Programmers do not need to change the application to enable reparallelization and migration. Instead, our compiler transparently prepares the applications for reparallelization and migration. It adds code to the application to enable reparallelization and augments the application with a platformindependent, coordinated checkpointing algorithm for migration. A prototype implementation of a migration framework automatically discovers free resources and migrates the application to these resources. Measurements show that the overhead of reparallelization and migration is roughly 4%, which we consider a negligible cost compared to the gain of flexibility.	algorithm;application checkpointing;compiler;grid computing;job scheduler;openmp;overhead (computing);programmer;prototype;scheduling (computing);system migration	Michael Klemm	2009				HPC	-23.342886200786257	58.81886841915071	71912
028119b03a1c2403103c74ec5e682395958924e3	dynamically adapting file domain partitioning methods for collective i/o based on underlying parallel file system locking protocols	parallel file system;dynamic file;o benchmarks;partitioning method;appropriate partitioning method;file domain;right partitioning;file domain partitioning;modern parallel file system;dynamically adapting file domain;single partitioning;layout;protocols;collaboration;parallel processing;data mining;message passing;data consistency;data integrity;servers	Collective I/O, such as that provided in MPI-IO, enables process collaboration among a group of processes for greater I/O parallelism. Its implementation involves file domain partitioning, and having the right partitioning is a key to achieving high-performance I/O. As modern parallel file systems maintain data consistency by adopting a distributed file locking mechanism to avoid centralized lock management, different locking protocols can have significant impact to the degree of parallelism of a given file domain partitioning method. In this paper, we propose dynamic file partitioning methods that adapt according to the underlying locking protocols in the parallel file systems and evaluate the performance of four partitioning methods under two locking protocols. By running multiple I/O benchmarks, our experiments demonstrate that no single partitioning guarantees the best performance. Using MPI-IO as an implementation platform, we provide guidelines to select the most appropriate partitioning methods for various I/O patterns and file systems.	benchmark (computing);centralized computing;clustered file system;degree of parallelism;experiment;input/output;lock (computer science);parallel computing;socket.io	Wei-keng Liao;Alok N. Choudhary	2008	2008 SC - International Conference for High Performance Computing, Networking, Storage and Analysis	10.1145/1413370.1413374	layout;self-certifying file system;communications protocol;parallel processing;parallel computing;message passing;petascale computing;device file;computer science;class implementation file;operating system;data integrity;distributed computing;data consistency;file system fragmentation;server;collaboration	HPC	-20.78277990814057	51.65961765809202	71969
98efb1037ee96a5df2ffa29df13a7c4dc5031906	scratching the surface of windows server 2016 and system center configuration manager current branch		Lehigh University has set a goal to implement System Center Configuration Manager by the end of 2017. This project is being spearheaded by one of our Senior Computing Consultants who has been researching and trained in the Microsoft Virtualization stack. We will discuss our roadmaps, results from our proof-of-concept environments, and discussions in driving this project.	microsoft windows;plan	Muhammed Naazer Ashraf	2017		10.1145/3123458.3123462	virtualization;vbscript;server message block;computer science;group policy;operating system;microsoft transaction server;lan manager;windows server;microsoft office live meeting	OS	-27.971842099199403	51.5092923234701	71979
38fefc4ceaaea2456ee8597baa243b4926c36f90	an event-based monitoring tool for parallel and distributed applications on transputer networks	distributed application		transputer	Shiping Chen;Tom Hintz	1996			parallel computing;computer science;transputer;distributed computing;distributed design patterns	HPC	-29.336120141423336	46.41907502977192	71980
71de3a0591583b5403ab5543d644c54775ef7269	performance evaluation of two-phase locking algorithms in a system for distributed databases	distributed database		algorithm;performance evaluation;two-phase commit protocol;two-phase locking	Costantino Thanos;Carlo Carlesi;Elisa Bertino	1983			distributed computing;distributed database;computer science;two-phase locking	DB	-27.758772158514855	47.09633680094681	72042
b0d101de134373e1b46f5bb0595819a4e25bd8dd	toward a progress indicator for machine learning model building and data mining algorithm execution: a position paper	machine learning;automatic administration;data mining;load management;progress indicator	For user-friendliness, many software systems offer progress indicators for long-duration tasks. A typical progress indicator continuously estimates the remaining task execution time as well as the portion of the task that has been finished. Building a machine learning model often takes a long time, but no existing machine learning software supplies a non-trivial progress indicator. Similarly, running a data mining algorithm often takes a long time, but no existing data mining software provides a nontrivial progress indicator. In this article, we consider the problem of offering progress indicators for machine learning model building and data mining algorithm execution. We discuss the goals and challenges intrinsic to this problem. Then we describe an initial framework for implementing such progress indicators and two advanced, potential uses of them, with the goal of inspiring future research on this topic	algorithm;data mining;estimated;machine learning;manufactured supplies;progress indicator;run time (program lifecycle phase);software system;usability	Gang Luo	2017	SIGKDD explorations : newsletter of the Special Interest Group (SIG) on Knowledge Discovery & Data Mining	10.1145/3166054.3166057	software system;position paper;data mining;computer science;software;machine learning;algorithm;model building;load management;artificial intelligence	ML	-23.09604573034795	55.181599681652976	72175
cfc27ec841d6784c99d12c7a4230a5e989067b64	cloud and edge computing		Cloud and edge computing are currently undergoing a substantive transformation on several fronts, from applications to hardware, and from architectures to devices. This issue of the Cloud and Edge Computing Series solicited articles in the area of cloud and edge computing to address the main issues concerned with evolving processes and supporting pedagogies and applications in cloud computing, networking, and storages technologies. There have been advances on both the cloud and edge computing fronts that have affected this line of technology. Paradigms such as computing in virtualization-based architectures, issues on geographical constraints for deploying clouds, and the use of SDN/NFV in clouds, for example, were of special interest for this issue of the Series. Nevertheless, our attention was also focused on data center network (DCN) architectures, security, load balancing, and application data streaming supported by evidence from simulations, analysis, or experiments. The last but not least important factor we expected	edge computing	Nader Mir;Salvatore Loreto	2017	IEEE Communications Standards Magazine	10.1109/MCOMSTD.2017.8258597	special interest group;virtualization;network functions virtualization;cloud computing;data center;distributed computing;load balancing (computing);edge computing;computer science	Vision	-28.82241604869966	57.49647857295571	72205
0948ff7fea8440185e5a8117138c0887c878d467	porting the sgi xfs file system to linux	file system;open source	In late 1994, SGI released an advanced, journaled file system called XFS on IRIX, their System-V-derived version of UNIX. Since that time, XFS has proven itself in production as a fast, highly scalable file system suitable for computer systems ranging from the desktop to supercomputers. In early 1999, SGI announced that XFS would be released under an open source license and integrated into the Linux kernel. In this paper, we outline the history of XFS, its current architecture and implementation, our porting strategy for migrating XFS to Linux, and future plans, including coordinating our work with the Linux hacker community.	desktop computer;hacker;irix;linux;open-source license;open-source software;scalability;supercomputer;unix	Jim Mostek;Bill Earl;Steven Levine;Steve Lord;Russell Cattelan;Ken McDonell;Ted Kline;Brian Gaffey;Rajagopal Ananthanarayanan	2000			self-certifying file system;parallel computing;memory-mapped file;device file;computer file;resolv.conf;computer science;operating system;fstab;unix file types;ssh file transfer protocol;database;open;everything is a file;file system fragmentation;squashfs;configfs;virtual file system	OS	-27.54602189623606	51.86549435643933	72221
4f0014309bacae609770111f2a3caf2ea74b4038	grid computing usability heuristics in practice	usability evaluation;distributed computing;grid computing applications;usability evaluation method;usability checklist distributed computing computational resource sharing grid computing usability evaluation method usability heuristics;usability checklist;software reusability;usability heuristics;software reusability grid computing;grid computing;grid computing usability green products documentation graphics inspection satellites;usability checklist grid computing applications usability heuristics	Grid Computing is a relatively new, distributed computing technology, based on sharing different types of computational resources, located in various geographic locations. Usability evaluation for Grid Computing applications brings new challenges. There is a need for new usability evaluation methods or at least for the use of traditional evaluations in novel ways. A set of specific usability heuristics was defined and validated. A usability checklist to be used when applying Grid Computing heuristics is also proposed.	computational resource;distributed computing;grid computing;heuristic (computer science);usability	Silvana Roncagliolo;Virginica Rusu;Cristian Rusu;Gonzalo Tapia;Danae Hayvar;Dorian Gorgan	2011	2011 Eighth International Conference on Information Technology: New Generations	10.1109/ITNG.2011.33	pluralistic walkthrough;web usability;component-based usability testing;cognitive walkthrough;usability;human–computer interaction;agile usability engineering;computer science;system usability scale;usability engineering;database;tree testing;heuristic evaluation;world wide web;usability lab;grid computing;usability inspection	HPC	-30.914520617984458	48.874582569394775	72246
1af5703d8485051f678ea9d30021ecb57c256bb1	neighboring graphs as alternative organizations for information retrieval (poster)	conceptual evocative links;information retrieval;conceptual graphs;auto adaptativity;conceptual evocative engine;hypertext;conceptual evocation	"""SQL statements but extending the gateway process to other database systems is quite straightforward. Existing dynamic pages are passive in nature. When the content of a database is changed, the corresponding dynamic HTML page does not get refreshed. To maintain the coherence of a dynamic page with the database, a user has to manually reload the page, i.e., rerun the retrieval query. This is not acceptable especially when there are time-critical data, such as stock data because a user would have no idea and no control on when the database will be updated. """" Push-based """" updating scheme from a Web server to a Web client is not applicable either due to the stateless and connectionless nature of a Web server. Our prototype employs a """" pull-based """" update scheme. The gateway process associates a timestamp, called refresh time (RT), with each database item. In our current prototype, a database item is implemented at the granularity of a tuple. In other words, each tuple, f of T, will be associated with a refresh time, RT t. The refresh time is estimated according to the update probabil%y of tuple t utilizing some statistical operators. RT-t will be transmitted to a Web client along with t. RT-t governs the duration that the associated tuple t could be considered valid. When RT-t oft expires, a Web client will initiate a query to the Web server for the updated values of tuple t. In our prototype, a Web client contains two processes, Monitor and CacheStorage. Monitor is implemented as a Java Applet. It maintains a collection of thread, one for each tuple, t, of T. Each thread sleeps for RT-t duration and initiates a query to the Web server for the updated values oft. Monitor also passes the updated values to CacheStorage which is implemented as a Plugin of Netscape. CacheStorage maintains a local cache file for a dynamic HTML page capturing relation T. Monitor could retrieve cached database items from a Web client's local storage when a previously dynamic HTML page is visited. Notice that we need to implement our own storage cache as a Plugin since Java prevents accessing the local storage of a Web client. We are currently experimenting with the effectiveness of this """" pull-based """" coherence scheme which could be measured by the number of """" unnecessary refreshes """" encountered and total """" incoherent duration """" browsed. These two …"""	coherence (physics);connectionless communication;database;dynamic html;dynamic web page;experiment;information retrieval;java applet;plug-in (computing);prototype;sql;server (computing);stateless protocol;thread-local storage;web server;window of opportunity;world wide web	Fei Song	1997		10.1145/263690.264348	computer science;data mining;world wide web;information retrieval	DB	-26.14523868058992	49.440543042563924	72248
972bf3fbd8c7c6bc35d29a383f3805cca5ddd583	constructing services with interposable virtual hardware	virtual machine;virtual machine monitor;fault tolerant;intrusion detection;reverse engineering	Virtual machine monitors (VMMs) have enjoyed a resurgence in popularity, since VMMs can help to solve difficult systems problems like migration, fault tolerance, code sandboxing, intrusion detection, and debugging. Recently, several researchers have proposed novel applications of virtual machine technology, such as Internet Suspend/Resume [25, 31] and transparent OS-level rollback and replay [13]. Unfortunately, current VMMs do not export enough functionality to budding developers of such applications, forcing them either to reverse engineer pieces of a black-box VMM, or to reimplement significant portions of a VMM. In this paper, we present the design, implementation, and evaluation of μDenali, an extensible and programmable virtual machine monitor that has the ability to run modern operating systems. μDenali allows programmers to extend the virtual architecture exposed by the VMM to a virtual machine, in effect giving systems programmers the ability to dynamically assemble a virtual machine out of either default or custombuilt virtual hardware elements. μDenali allows programmers to interpose on and modify events at the level of the virtual architecture, enabling them to easily perform tasks such as manipulating disk and network events, or capturing and migrating virtual machine state. In addition to describing and evaluating our extensible virtual machine monitor, we present an application-level API that simplifies writing extensions, and we discuss applications of virtual machines that we have built using this API.	application programming interface;black box;debugging;fault tolerance;hypervisor;independence day: resurgence;intrusion detection system;modern operating systems;operating system;programmer;reverse engineering;rollback (data management);sandbox (computer security);virtual machine manager	Andrew Whitaker;Richard S. Cox;Marianne Shaw;Steven D. Gribble	2004			intrusion detection system;embedded system;fault tolerance;full virtualization;real-time computing;temporal isolation among virtual machines;computer science;virtual machine;kernel virtual address space;operating system;database;distributed computing;hypervisor;programming language;computer security;reverse engineering	OS	-26.976332335687175	50.73181349897415	72321
53d22ed280026b46cec8525a505b6f0a369de936	an application meta-model to support the execution and benchmarking of scientific applications in multi-cloud environments		Cloud computing has proven its importance to scientists around the globe on many occasions already. However, as it is still a rather new technology for many users, the cloud represents another layer of complexity in any workflow. As a lot of research confirms, especially the efficient provision and management of resources in the cloud is a very complex but also very rewarding task. Upon surveying the research in this area we observed many differences in applied methodologies and application cases which impede not only the comparison of these approaches but also the collective usage of the obtained results, e.g. for more accurate resource estimation algorithms that require less additional benchmarks. We propose a novel application and resource meta-model to model not only applications but also the underlying resource infrastructure for application benchmarks in a generic manner. We show how the meta-model is defined and how it can be used to model an application, using a simple web application as an example. We conclude with highlighting the potential benefits of applying this model in different scenarios but also its limits and how it could be expanded in the future.	algorithm;benchmark (computing);cloud computing;metamodeling;web application	Markus Ullrich;Jörg Lässig;Kento Aida;Jingtao Sun;Tomoya Tanjo;Martin Gaedke	2017	2017 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computed, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)	10.1109/UIC-ATC.2017.8397633	distributed computing;computer science;resource management;data mining;scalability;web application;benchmarking;cloud computing;benchmark (computing);workflow;data modeling	HPC	-22.6399450170307	58.6543632531721	72472
978f3560998f7c5b8c76cea1489fea76f48920d3	scidp: support hpc and big data applications via integrated scientific data processing		Modern High Performance Computing (HPC) applications, such as Earth science simulations, produce large amounts of data due to the surging of computing power, while big data applications have become more compute-intensive due to increasingly sophisticated analysis algorithms. The needs of both HPC and big data technologies for advanced HPC and big data applications create a demand for integrated system support. In this study, we introduce Scientific Data Processing (SciDP) to support both HPC and big data applications via integrated scientific data processing. SciDP can directly process scientific data stored on a Parallel File System (PFS), which is typically deployed in an HPC environment, in a big data programming environment running atop Hadoop Distributed File System (HDFS). SciDP seamlessly integrates PFS, HDFS, and the widely-used R data analysis system to support highly efficient processing of scientific data. It utilizes the merits of both PFS and HDFS for fast data transfer, overlaps computing with data accessing, and integrates R into the data transfer process. Experimental results show that SciDP accelerates analysis and visualization of a production NASA Center for Climate Simulation (NCCS) climate and weather application by 6x to 8x when compared to existing solutions.	algorithm;apache hadoop;big data;blu-ray;cns;clustered file system;comparison of command shells;dce distributed file system;data system;ecosystem;forward secrecy;hierarchical data format;ibm notes;integrated development environment;microsoft customer care framework;netcdf;r language;requirement;spark;simulation;speedup;supercomputer	Kun Feng;Xian-He Sun;Xi Yang;Shujia Zhou	2018	2018 IEEE International Conference on Cluster Computing (CLUSTER)	10.1109/CLUSTER.2018.00023	computer science;data visualization;distributed file system;distributed computing;visualization;big data;file system;data modeling;data processing;supercomputer	HPC	-28.245381397154514	52.80663098943355	72517
e3a1ef0d3ab3a0685baf5ecb0fe49bcf23aa631f	an adaptively speculative execution strategy based on real-time resource awareness in a multi-job heterogeneous environment		MapReduce (MRV1), a popular programming model, proposed by Google, has been well used to process large datasets in Hadoop, an open source cloud platform. Its new version MapReduce 2.0 (MRV2) developed along with the emerging of Yarn has achieved obvious improvement over MRV1. However, MRV2 suffers from long finishing time on certain types of jobs. Speculative Execution (SE) has been presented as an approach to the problem above by backing up those delayed jobs from low-performance machines to higher ones. In this paper, an adaptive SE strategy (ASE) is presented in Hadoop-2.6.0. Experiment results have depicted that the ASE duplicates tasks according to real-time resources usage among work nodes in a cloud. In addition, the performance of MRV2 is largely improved using the ASE strategy on job execution time and resource consumption, whether in a multi-job environment.	adaptive server enterprise;apache hadoop;backup;cloud computing;cloud storage;job stream;mapreduce;open-source software;programming model;real-time clock;real-time transcription;run time (program lifecycle phase);speculative execution	Qi Liu;Tom Weidong Cai;Qiang Liu;Jian Shen;Zhangjie Fu;Xiaodong Liu;Nigel Linge	2017	TIIS	10.3837/tiis.2017.02.004	distributed computing;resource consumption;real-time computing;speculative multithreading;cloud computing;speculative execution;programming paradigm;computer science	HPC	-20.350580706793306	59.933199001256746	72567
5cae2d4f2de2da1ef6ce403d861e54d7057004e0	nuboinc: boinc extensions for community cycle sharing	computers;software;virtual machine;boinc;programming language;statistical software;community cycle sharing;probability density function;cycle sharing;boinc extensions;data mining;servers;cyclesharing platform;internet;community computing;registers;software package;community computing cycle sharing boinc;cyclesharing platform community cycle sharing boinc extensions internet software packages;software packages internet;infrastructure development;application software computer languages software packages java conferences internet virtual machining concurrent computing distributed computing data processing;software packages;java	Currently, cycle sharing over the Internet is a one-way deal. Computer owners only have one role in the process: to donate their computers' idle time. This is due to the fact that it is difficult for an ordinary user to install the required infrastructure, develop the processing applications and gather enough computer cycle donors. In this paper we describe a set of BOINC extensions that allow any user to create and submit jobs that can take advantage of remote idle cycles. These jobs are processed by commonly available software (e.g. programming language interpreters or virtual machines, statistical software) that is installed in the remote donating computers. In order to submit their jobs, users only have to provide the input files, select the processing application and define the command line to provide to that application. Later, users of the same software packages will contact the server, receive a set of jobs, and process them using the already installed commodity application. These users can later take advantage of other peoplepsilas computer cycles.This system allows an expressive definition of jobs providing considerable speed gains, while leveraging a cyclesharing platform and widely available commodity applications, in a truly global communal computer cycle market.	boinc;boinc client–server technology;burst mode (computing);central processing unit;centralized computing;command-line interface;computer;fairness measure;idle scan;instruction cycle;internet;interpreter (computing);job stream;list of statistical packages;one-way function;preemption (computing);programming language;reputation system;scheduling (computing);self-replicating machine;server (computing);surround sound;user interface;virtual machine	João Nuno de Oliveira e Silva;Luís Veiga;Paulo José Azevedo Vianna Ferreira	2008	2008 Second IEEE International Conference on Self-Adaptive and Self-Organizing Systems Workshops	10.1109/SASOW.2008.66	probability density function;real-time computing;the internet;computer science;virtual machine;operating system;database;distributed computing;processor register;programming language;java;server	Arch	-28.443479928039626	54.53520925572787	72589
f7ded3861172915f3aed5802c686e616a13f1876	a framework to support survivable web services	distributed application;n modular redundancy concept;protocols;application software;availability;web and internet services;helium;computer network reliability internet open systems replicated databases security of data;nuclear magnetic resonance;message ordering protocols;web service;group communication;message ordering protocols distributed application systems interoperability survivable web service framework replication scheme n modular redundancy concept inter group communication;inter group communication;service model;internet;n modular redundancy;web services;web services application software protocols web and internet services delay nuclear magnetic resonance space technology availability data security helium;space technology;interoperability;replication scheme;distributed application systems;open systems;security of data;replicated databases;computer network reliability;data security;survivable web service framework	Web services have gained high popularity in the development of distributed application systems. Some critical applications also consider using Web services paradigm due to the benefits of interoperability, reusability, and adaptability. To support critical applications, existing Web service model needs to be extended to assure survivability. In this paper, we introduce the design of a novel survivable Web service (SWS) framework that supports continuous operation even in the presence of failures and security attacks. The replication scheme and n-modular redundancy (NMR) concept are used as the basis for achieving survivability. Due to replication, communication among multiple Web services becomes costly. We develop protocols, including the inter-group communication (SWS-IGC) and message ordering (SWS-MO) protocols, to guarantee efficient and correct communication. Preliminary performance studies show that the SWS framework only incurs a moderate overhead due to the incorporation of survivability assurance.	algorithm;communications protocol;continuous operation;distributed computing;fault tolerance;high availability;international parallel and distributed processing symposium;interoperability;overhead (computing);programming paradigm;scalability;sinewave synthesis;triple modular redundancy;web service;world wide web	Wei Li;Jiang He;Qingkai Ma;I-Ling Yen;Farokh B. Bastani;Raymond A. Paul	2005	19th IEEE International Parallel and Distributed Processing Symposium	10.1109/IPDPS.2005.27	web service;computer science;operating system;database;distributed computing;world wide web;computer network	Embedded	-32.6146961027715	47.62750931778758	73015
6dd9deccf6e2ff52e31a8d737a2deb7a8244cf31	castor: a distributed storage resource facility for high performance data processing at cern	cache storage;mass storage systems;storage system;data processing scalability large hadron collider cache storage throughput production laboratories buildings testing memory;large hadron collider;data processing;distributed storage;production environment distributed storage resource facility data processing mass storage systems cern advanced storage system disk cache management layer large hadron collider;distributed storage resource facility;disk cache management layer;mass storage system;production environment;high performance;cache management;cern advanced storage system	Mass storage systems at CERN have evolved over time to meet growing requirements, in terms of both scalability and fault resiliency. The CERN advanced storage system (CASTOR) and its new disk cache management layer (CASTOR2) have been developed to meet the challenges raised by the experiments using the new accelerator that CERN is building: the large hadron collider (LHC) [4]. This system must be able to cope with hundreds of millions of files, tens of petabytes of storage and handle a constant throughput of several gigabytes per second. In this paper, we detail CASTOR's architecture and implementation and present some operational aspects. We finally list the performance levels achieved by the current version both in a production environment and during internal tests.	authentication;authorization;backup;computer data storage;deployment environment;experiment;gigabyte;grid computing;high availability;large hadron collider;mass storage;petabyte;requirement;scalability;software deployment;system reference manual;tape drive;throughput;voms;virtual organization	Giuseppe Lo Presti;Olof Bärring;Alasdair Earl;Rosa Maria Garcia Rioja;Sebastien Ponce;Giulia Taurelli;Dennis Waldron;Miguel Coelho Dos Santos	2007	24th IEEE Conference on Mass Storage Systems and Technologies (MSST 2007)	10.1109/MSST.2007.7	parallel computing;storage area network;converged storage;engineering;operating system;database	HPC	-27.129988865222465	54.63093832533552	73081
375824adf7fc135013e049643e36e5c4ab6b8423	moobi: a thin server management system using bittorrent	management system;bittorrent	We describe a tool that provides a method for running dataless caching clients – a hybrid combination of imaging systems with traditional diskless nodes. Unlike imaging systems, it is a single boot to get to a running system; unlike diskless systems, it is more robust and scalable as it does not continuously depend on central servers. The tool, Moobi, uses the peer-to-peer protocol BitTorrent to provide efficient distribution of the image cache, and combines standard diskless tools to provide the basis for the running system. Moobi makes it possible to run large installations of ‘‘thin server’’ farms.	bittorrent;diskless node;management system;peer-to-peer;scalability;server (computing)	Chris McEniry	2007			embedded system;real-time computing;bittorrent;computer science;operating system;management system;sociology	Networks	-21.81413751764837	52.4127946880219	73128
3db1d62716a1dc30dd63f5011de7ba7730c577a6	big data availability: selective partial checkpointing for in-memory database queries	databases;checkpointing;big data;fault tolerant systems;fault tolerance;middleware;algorithm design and analysis	Fault tolerance is an important challenge for supporting critical big data analytic operations. Most existing solutions only provide fault tolerant data replication, requiring failed queries to be restarted. This approach is insufficient for long-running time-sensitive analytic queries, due to lost query progress. Several solutions provide intra-query fault tolerance. However, these focus on distributed or row-oriented databases and are not suitable for use with the column-oriented in-memory databases increasingly used for highperformance workloads. We propose a new approach for intra-query checkpointing that produces an optimal checkpoint solution for a fixed checkpointing budget to minimise overhead on in-memory column-oriented database clusters. We describe a modified architecture for fault tolerant query execution using this approach. We present a general model for the problem, in which an adversary is free to terminate the execution of the query, eliminating all unsaved work. We present an algorithm that represents a first step towards producing checkpoint plans by optimally placing a single checkpoint. Our analysis shows this approach allows reduced checkpoint overheads while providing resilience for long-running queries.	adversary (cryptography);algorithm;application checkpointing;big data;column-oriented dbms;database schema;extrapolation;fault tolerance;global optimization;high availability;in-memory database;mathematical optimization;monetdb;overhead (computing);parallel computing;query plan;replication (computing);run time (program lifecycle phase);selection algorithm;terminate (software);time complexity;transaction processing system	Daniel Playfair;Amitabh Trehan;Barry McLarnon;Dimitrios S. Nikolopoulos	2016	2016 IEEE International Conference on Big Data (Big Data)	10.1109/BigData.2016.7840926	real-time computing;computer science;database;distributed computing	DB	-19.698168652539387	55.06210738098139	73177
93b5ba6a25d50006e8dd10a2619433d32b903d71	virtual private grid: a command shell for utilizing hundreds of machines efficiently	grid applications;distributed supercomputing;collaborative grid applications;peer to peer computing;geographic distribution	We describe design and implementation of virtual private grid (VPG), a shell that can utilize many machines distributed over multiple subnets. VPG works around common security policies (e.g., firewall, private IP, DHCP) that restrict communication between machines and even break uniqueness of IP addresses. VPG provides the following functions: (1) a unique nickname to each machine that does not depend on a DNS name or a fixed IP address; (2) job submissions to any nicknamed machine; (3) redirections from/to a file on any nicknamed machine; (4) pipes between commands executed on any nicknamed machine. VPG implements the above functions by constructing a self-stabilizing spanning tree among machines and forwarding messages via a path in the tree. We ran VPG on about 100 nodes (270 CPUs) and measured a turn around time of a small job submission with VPG and other tools: rsh, SSH, and globus-job-run. The experimental result shows that VPG can submit a job faster than SSH and globus-job-run, since VPG performs authentication only when it constructs a tree.		Kenji Kaneda;Kenjiro Taura;Akinori Yonezawa	2003	Future Generation Comp. Syst.	10.1016/S0167-739X(03)00036-0	parallel computing;real-time computing;computer science;operating system;database;distributed computing;world wide web;computer security	HPC	-26.243028575896783	52.54462289743059	73279
91fbedc2781dd6a083f5d1a82fe6ae306d7975ac	improving reliability of jxta-overlay p2p platform: a comparison study for two fuzzy-based systems	fuzzy logic;jxta overlay;network reliability;article;intelligent algorithm;p2p systems	In P2P systems, each peer has to obtain information of other p e rs and propagate the information to other peers through neighbori ng peers. Thus, it is important for each peer to have some number of neighbor peers. M oreover, it is more significant to discuss if each peer has reliable neighbor pee rs. In reality, each peer might be faulty or might send obsolete, even incorrect infor mation to the other peers. We have implemented a P2P platform called JXTA-Orver lay, which defines a set of protocols that standardize how different devices ma y communicate and collaborate among them. It abstracts a new layer on the top of JXT A through a set of primitive operations and services that are commonly used in JXTA-based applications and provides a set of primitives that can be used by othe r applications, which will be built on top of the overlay, with complete independen c . JXTA-Overlay provides a set of basic functionalities, primitives, inten d d to be as complete as possible to satisfy the needs of most JXTA-based applicatio ns. In this paper, we present two fuzzy-based systems (called FRS1 and FRS2) to im pr ve the reliability of JXTA-Overlay P2P platform. We make a comparison study bet we n the fuzzybased reliability systems. Comparing the complexity of FRS 1 and FRS2, the FRS2 is more complex than FRS1. However, it considers also the sec urity; therefore, it can be used in real application for secure systems. 1ryui1010@gmail.com 2shinji.t.sakamoto@gmail.com 3matuo-k7@fku.ed.jp 4makoto.ikd@acm.org 5barolli@fit.ac.jp 6fatos@lsi.upc.edu	experiment;jxta;simulation	Yi Liu;Shinji Sakamoto;Keita Matsuo;Makoto Ikeda;Leonard Barolli;Fatos Xhafa	2015	J. High Speed Networks	10.3233/JHS-150506	fuzzy logic;fuzzy electronics;computer science;data mining;database;reliability;world wide web;computer network	Web+IR	-33.28023658657814	49.57010500636	73294
33bfe9d4ff95c523c27da82043f4f49972089f9c	avalanche dynamics in grids: indications of soc or hot?	networks and distributed systems;highly optimized tolerance hot;complex systems;distributed systems;grid computing;self organized criticality soc	Complex systems such as those in evolution, growth and depinning models do not evolve slowly and gradually, but exhibit avalanche dynamics or punctuated equilibria. Self-Organized Criticality (SOC) and Highly Optimized Tolerance (HOT) are two theoretical models that explain such avalanche dynamics. We have studied avalanche dynamics in two vastly different grid computing systems: Optimal Grid and Vishva. Failures in optimal grid cause an avalanche effect with respect to the overall computation. Vishva does not exhibit failure avalanches. Interestingly, Vishva exhibits load avalanche effects at critical load density, wherein a small load disturbance in one node can cause load disturbances in several other nodes. The avalanche dynamics of grid computing systems implies that grids can be viewed as SOC systems or as HOT systems. An SOC perspective suggests that grids may be sub-optimal in performance, but may be robust to unanticipated uncertainties. A HOT perspective suggests that grids can be made optimal in performance, but would then be sensitive to unanticipated perturbations. An ideal approach for grid systems research is to explore a combination of SOC and HOT as a basis for design, resulting in robust yet optimal systems.	ant colony;avalanche effect;complex systems;computation;distributed computing;failure cause;grid computing;highly optimized tolerance;mathematical optimization;randomness;resultant;scalability;self-organized criticality;systems theory	Vijay Srinivas Agneeswaran;D. Jana Kiram;Motakatla Venkateswara Reddy	2005			real-time computing;simulation;engineering;distributed computing	HPC	-19.2997525812364	59.30489188440333	73498
f66b166381cb5715a46ae845630af426dbf081ba	the sharegrid peer-to-peer desktop grid: infrastructure, applications, and performance evaluation	volunteer computing;performance evaluation;peer to peer systems;desktop grids;p2p;peer to peer system;bag of tasks applications;bag of tasks;middleware;experimental evaluation;peer to peer	Peer-to-Peer (P2P) Desktop Grids are computing infrastructures that aggregate a set of desktop-class machines in which all the participating entities have the same roles, responsibilities, and rights. In this paper, we present ShareGrid, a P2P Desktop Grid infrastructure based on the OurGrid middleware, that federates the resources provided by a set of small research laboratories to easily share and use their computing resources. We discuss the techniques and tools we employed to ensure scalability, efficiency, and usability, and describe the various applications used on it. We also demonstrate the ability of ShareGrid of providing good performance and scalability by reporting the results of experimental evaluations carried out by running various applications with different resource requirements. Our experience with ShareGrid indicates that P2P Desktop Grids can represent an effective answer to the computing needs of small research laboratories, as long as they provide both ease of management and use, and good scalability and performance.	aggregate data;desktop computer;entity;federation (information technology);middleware;ourgrid;peer-to-peer;requirement;scalability;usability	Cosimo Anglano;Massimo Canonico;Marco Guazzone	2010	Journal of Grid Computing	10.1007/s10723-010-9162-z	computer science;operating system;peer-to-peer;middleware;database;distributed computing;world wide web	HPC	-30.791416937502206	52.1881922455541	73861
f169797e6ff4ed82ad2334b516d5916dba422340	adding monitoring and reconfiguration facilities for service-based applications in the cloud	service containers;reconfiguration;biomedical monitoring;monitoring subscriptions servers biomedical monitoring scalability java containers;storage management cloud computing computerised monitoring resource allocation service oriented architecture;nonfunctional property cloud computing information technology economic model virtual resource provisioning service oriented architecture soa service based applications memory consumption reconfiguration facilities scalable microcontainer services business;servers;monitoring;subscriptions;component model;scalability;service containers cloud computing monitoring reconfiguration component model scalability;containers;cloud computing;java	Cloud computing is a recent paradigm in information technology enabling an economic model for virtual resources provisioning. For this paradigm, service oriented Architecture (SOA) is a pillar block to build applications. To efficiently manage service-based applications in the cloud, monitoring and reconfiguration remain important tasks but it is still a challenge to find a solution that reconciles the scalability, the memory consumption, and the efficiency. In this paper, we propose a framework that adds monitoring and reconfiguration facilities to services of a service-based application and deploys them in the cloud using a scalable micro-container. Unlike the existing initiatives, our framework adds dynamically these facilities to services that were not designed to be monitored or reconfigured. This makes the developers' task easier, letting them focusing on the business of their services instead of the non functional property of monitoring and reconfiguration. Moreover, our framework uses a scalable micro-container for services' deployment in the cloud to be inline with the scalability of this environment. The evaluation that we performed proves the efficiency and the flexibility of our approach for service-based applications in the cloud.	apply;cloud computing;programming paradigm;provisioning;scalability;service-oriented architecture;software deployment;state (computer science)	Mohamed Mohamed;Djamel Belaïd;Samir Tata	2013	2013 IEEE 27th International Conference on Advanced Information Networking and Applications (AINA)	10.1109/AINA.2013.46	real-time computing;scalability;cloud computing;computer science;control reconfiguration;operating system;component object model;database;distributed computing;java;computer security;server;computer network	HPC	-30.32394675074782	55.99315469130272	73960
00dbc789e0a1ab195892ab1ec981837277d824e8	adaptive online compression in clouds—making informed decisions in virtual machine environments	i o performance;cloud computing;adaptive compression	Infrastructure as a Service clouds often use virtual machines to host different customers on the same physical hardware. This form of resource sharing can lead to unpredicatable performance degradations for the individual customer, especially with regard to data-intensive applications, which heavily depend on stable I/O characteristics. One traditional approach to cope with I/O fluctuations is adaptive online compression. In this paper we present a new scheme for adaptive online compression which has been explicitly designed to work in co-located virtual machine environments. In contrast to existing adaptive online compression schemes, the decision model of our approach does not rely on the system metrics CPU utilization and I/O bandwidth, which we demonstrate to be often displayed inaccurately inside XEN, KVM, and Amazon EC2-based virtual machines. Instead, it only considers the application data rate. Without requiring any calibration or training phase our adaptive compression scheme can improve the I/O throughput of virtual machines up to a factor of four as shown through extended experimental evaluations.	adaptive compression;amazon elastic compute cloud (ec2);amazon machine image;amazon simple storage service;baseline (configuration management);central processing unit;cloud computing;data center;data rate units;data-intensive computing;device driver;distributed computing;experiment;fastest;gentoo linux;gigabyte;hard disk drive;identifier;input/output;random-access memory;scsi;seagate barracuda;serial ata;throughput;ubuntu version history;uncompressed video;virtual machine	Matthias Hovestadt;Odej Kao;Andreas Kliem;Daniel Warneke	2013	Journal of Grid Computing	10.1007/s10723-013-9249-4	real-time computing;simulation;cloud computing;computer science;theoretical computer science;operating system;database;distributed computing	HPC	-22.523732693773873	59.586348875711316	74007
6340c819b1b0865f0ad8c7ddbb666f0a9a02179f	toward a framework for preparing and executing adaptive grid programs	resource selection;electrical capacitance tomography;information resources;degradation;identity based encryption;application software;grid applications;resource allocation;application software electrical capacitance tomography computer vision grid computing lifting equipment identity based encryption degradation contracts monitoring information resources;computer and information science;adaptive grid;contracts;computer vision;monitoring;lifting equipment;data och informationsvetenskap;grid computing	This paper describes the program execution framework being developed by the Grid Application Development Software (GrADS) Project. The goal of this framework is to provide good resource allocation for Grid applications and to support adaptive reallocation if performance degrades because of changes in the availability of Grid resources. At the heart of this strategy is the notion of a configurable object program, which contains, in addition to application code, strategies for mapping the application to different collections of resources and a resource selection model that provides an estimate of the performance of the application on a specific collection of Grid resources. This model must be accurate enough to distinguish collections of resources that will deliver good performance from those that will not. The GrADS execution framework also provides a contract monitoring mechanism for interrupting and remapping an application execution when performance falls below acceptable levels.	adaptive mesh refinement;executable;interrupt	Ken Kennedy;Mark Mazina;John M. Mellor-Crummey;Keith D. Cooper;Linda Torczon;Francine Berman;Andrew A. Chien;Holly Dail;Otto Sievert;Dave Angulo;Ian T. Foster;Ruth A. Aydt;Daniel A. Reed;Dennis Gannon;S. Lennart Johnsson;Carl Kesselman;Jack J. Dongarra;Sathish S. Vadhiyar;Richard Wolski	2002		10.1109/IPDPS.2002.1016570	application software;parallel computing;real-time computing;simulation;degradation;semantic grid;resource allocation;computer science;theoretical computer science;operating system;distributed computing;grid computing;lifting equipment	HPC	-32.686464532966184	49.40858218413402	74431
23fda68035075412729653f9bbdcbf5d486aaa2a	an object infrastructure for computational steering of distributed simulations	distributed application;computational steering;control network;distributed environment;sensors and actuators;distributed simulation;data structure;monitoring and control;geographic distribution	This paper presents a brief overview of a framework for the interactive steering of distributed applications that addresses three key issues: (1) Definition of Interaction Objects that provide sensors and actuators for interrogation and control. These objects encapsulate existing computation data-structures and can be distributed (spanning many processors) and dynamic (be created, deleted, or migrated to another processor). (2) Definition of a control network of interaction agents that enable the interactive steering of distributed interaction objects. (3) Definition of an Interaction Gateway that provides a proxy to the entire application and can be accessed via a web server. This research is part of an ongoing effort to develop a web-based computational collaboratory that enables geographically distributed scientists/engineers to collaboratively monitor, and control distributed applications.	as-interface;central processing unit;computation;computational steering;computer simulation;distributed computing;file spanning;server (computing);web application;web server	Rajeev Muralidhar;Manish Parashar	2000		10.1109/HPDC.2000.10004	control network;distributed algorithm;simulation;data structure;computer science;theoretical computer science;operating system;distributed computing;distributed object;distributed design patterns;distributed computing environment;distributed concurrency control	HPC	-30.47324020186157	47.37071660313231	74603
00bd7d20a79a0a93407b1a83be85c96ca6874062	the case for non-transparent replication: examples from bayou	data management;data replication;propagation delay	Applications that rely on replicated data have different requirements for how their data is managed For example, some applications may require that updates propagate amongst replicas with tight tim constraints, whereas other applications may be able to tolerate longer propagation delays. Som applications only require replicas to interoperate with a few centralized replicas for data synchroniza tion purposes, while other applications need communication between arbitrary replicas. Similarly, th type of update conflicts caused by data replication varies amongst applications, and the mechanisms resolve them differ as well. The challenge faced by designers of replicated systems is providing the right interface to support coo eration between applications and their data managers. Application programmers do not want to b overburdened by having to deal with issues like propagating updates to replicas and ensuring eventu consistency, but at the same time they want the ability to set up appropriate replication schedules a to control how update conflicts are detected and resolved. The Bayou system was designed to mitig this tension between overburdening and underempowering applications. This paper looks at tw Bayou applications, a calendar manager and a mail reader, and illustrates ways in which they utiliz Bayou’s features to manage their data in an application-specific manner.	centralized computing;eventual consistency;god of war: chains of olympus;interoperability;programmer;propagation delay;replication (computing);requirement;software propagation	Douglas B. Terry;Karin Petersen;Mike Spreitzer;Marvin Theimer	1998	IEEE Data Eng. Bull.		database;replication (computing);data management;interoperability;real-time computing;propagation delay;data synchronization;computer science;complex data type;eventual consistency;schedule	OS	-22.65957226547744	49.29835895528169	74715
66089fba5b8369a357d65bbdc5e483aabe5df5c2	gravevine: an exercise in distributed computing (summary).	virtual memory;vax;distributed computing;large scale;paging;research and development;distributed computation;perq;networking;unix;network;operating systems;inter process communication	Grapevine is a distributed, replicated system running on a large internet within the Xerox research and development community. The internet extends from coast to coast in the USA, to Canada and to Europe, and contains more than 50 Ethernet local networks linked by leased telephone lines. Over 1500 computers are attached to the internet. Most computers are used an personal workstations, but some are used as servers providing access to shared facilities such as printers, large-scale secondary storage, or data bases. Computers on the internet are uniformly addressable using the PUP family of protocols.	auxiliary memory;computer data storage;database;distributed computing;internet;leased line;potentially unwanted program;telephone line;workstation	Andrew Birrell;Roy Levin;Roger M. Needham;Michael D. Schroeder	1981		10.1145/800216.806606	real-time computing;computer science;virtual memory;operating system;distributed computing;unix;computer security;paging;inter-process communication	Metrics	-27.28997126577252	51.42912621682764	74748
12fc61c4bb5060fed078a226d0ebdec15215e512	an authorization model for multi-tenancy services in cloud	computer centres authorisation cloud computing;convenient authorization service authorization model multitenancy services cloud computing internet data centers software instance software cost;authorization cloud computing multi tenancy;authorisation;computer centres;authorization cloud computing digital signatures knowledge based systems computational modeling;cloud computing	Cloud computing is a set of resources and services offered through the Internet. The services of cloud are delivered from data centers located all over the world. Multi-tenancy is the common feature of public cloud and private cloud. Multi-tenancy service is defined as the capability of a single software instance to provide its service to several parties simultaneously. In cloud computing, multi-tenancy service has many advantages. Compared with traditional softwares, multi-tenancy service can lower the software's cost, get more profit and have obvious advantages and prospects. Under this condition, the security problems of multi-tenancy in cloud become important. This paper studies the multi-tenancy service in cloud, and presents an authorization model and the architecture of the model. This authorization model can provide a convenient authorization service for multi-tenancy services in cloud.	application programming interface;authorization;cloud computing;data center;digital signature;internet;multitenancy	Zhaohai Zhang;Qiaoyan Wen	2012	2012 IEEE 2nd International Conference on Cloud Computing and Intelligence Systems	10.1109/CCIS.2012.6664408	cloud computing security;cloud computing;computer science;operating system;cloud testing;utility computing;authorization;internet privacy;world wide web;computer security	HPC	-32.172216842636125	59.617431198396424	74965
f8abe4d3bd50f13482f56f1ae5ba35254c5c9fbf	automated parameterization of performance models from measurements	arrival processes;conference paper;demand estimation	Estimating parameters of performance models from empirical measurements is a critical task, which often has a major influence on the predictive accuracy of a model. This tutorial presents the problem of parameter estimation in queueing systems and queueing networks. The focus is on reliable estimation of the arrival rates of the requests and of the service demands they place at the servers. The tutorial covers common estimation techniques such as regression methods, maximum-likelihood estimation, and moment-matching, discussing their sensitivity with respect to data and model characteristics. The tutorial also demonstrates the automated estimation of model parameters using new open source tools.	computer performance;estimation theory;open-source software	Giuliano Casale;Simon Spinner;Weikun Wang	2016		10.1145/2851553.2858666	econometrics;real-time computing;simulation;computer science	Metrics	-22.094539844860968	56.53019063360638	75051
2378cc85685d5e512c071bc6a3f94df5f355866a	password-based protection of clustered segments in distributed memory systems		Abstract With reference to a distributed system consisting of nodes connected by a local area network, we consider the problems related to the distribution, verification, review and revocation of access permissions. We propose the organization of a protection system that takes advantage of a form of protected pointer, the handle, to reference clusters of segments allocated in the same node. A handle is expressed in terms of a selector and a password. The selector specifies the segments, the password specifies an access right, read or write. Two primary passwords are associated with each cluster, corresponding to an access permission for all the segments in that cluster. A handle weakening algorithm takes advantage of a parametric one-way function to generate secondary passwords corresponding to less segments. A small set of protection primitives makes it possible to allocate and delete segments in active clusters, and to use handles to access remote segments both to read and to write. The resulting protection environment is evaluated from a number of viewpoints, which include handle forging, review and revocation, the memory costs for handle storage, the execution times for handle validation and the network traffic generated by the execution of the protection primitives. An indication of the flexibility of the handle concept is given by applying handles to the solution of a variety of protection problems.	distributed memory;password	Lanfranco Lopriore	2018	J. Parallel Distrib. Comput.	10.1016/j.jpdc.2018.01.003	pointer (computer programming);password;revocation;distributed memory;local area network;small set;distributed computing;software;parametric statistics;computer science	HPC	-22.090880098991303	47.45616835710708	75127
d75c4f870617c8de4a8fc8fce79db5cf917ff3d5	building an online computing service over volunteer grid resources	amazon ec2 online computing service volunteer grid resource volunteer computing grid parallel workload data processing scientific experiment clouds quality of service on demand dynamic expansion resource pool scheduling mechanism online interactive computing service gridbot system genetic linkage analysis super link online internet access supercomputer superlink technion volunteer grid;reliability servers communities computers throughput resource management dynamic scheduling;scheduling cloud computing grid computing quality of service;scheduling;quality of service;grid computing;cloud computing	Volunteer computing grids have traditionally been used for massively parallel workloads, such as processing data from large scientific experiments. We argue that the domain of volunteer grids can be extended well beyond this specific niche, by enhancing them with built-in mechanisms for integration with with standard clusters, grids and clouds, to compensate for unexpected fluctuations in resource availability and quality of service. The resulting capabilities for on-demand dynamic expansion of the resource pool, together with sophisticated scheduling mechanisms will turn volunteer grids into a powerful execution platform for on-line interactive computing services. We will show our experience with the GridBoT system, which implements these ideas. GridBoT is part of a production high performance online service for genetic linkage analysis, called Super link-online. The system enables anyone with the Internet access to submit genetic data, and easily and quickly analyze it as if using a supercomputer. The analyses are automatically parallelized and executed via GridBoT on over 45,000 non-dedicated machines from the Superlink@Technion volunteer grid, as well as on 9 other grids and clouds, including the Aamazon EC2. Since 2009 the system has served more than 300 geneticists from leading research institutions worldwide, and executed over 6500 different real analysis runs, with about 10 million tasks consumed over 420 CPU years.	central processing unit;experiment;interactive computing;internet access;linkage (software);niche blogging;online and offline;online service provider;parallel computing;quality of service;scheduling (computing);supercomputer;volunteer computing	Mark Silberstein	2011	2011 IEEE International Symposium on Parallel and Distributed Processing Workshops and Phd Forum	10.1109/IPDPS.2011.352	parallel computing;real-time computing;computer science;distributed computing;utility computing;grid computing	HPC	-21.100757712222148	58.72876806703462	75332
e8619405277413dcb8f1075c209e81fa21a3eb61	gridfs: a web-based data grid with p2p concepts and writable replicas	distributed system;systeme reparti;dato;red www;relation interpair;digital library;data;reseau web;p2p;grid;peer relation;educational resource;biblioteca electronica;sistema repartido;donnee;rejilla;relacion interpar;replique;grille;world wide web;electronic library;web caching;replica;bibliotheque electronique;data grid	We merge concepts from P2P (Peer-to-Peer) into Data Grid and present a web-based Data Grid, called GridFS (Grid File System), for the distributed sharing of educational resources. GridFS extends Data Grid by making replicas writable in practice. We propose a push-version-number consistency (PVNC) model and a writer/reader protocol to implement it. PVNC eliminates nearly all read latency and lessens the network utilization. We believe GridFS and PVNC would benefit hierarchical web cache systems and digital libraries.		Qinghu Li;Jianmin Wang;Kwok-Yan Lam;Jia-Guang Sun	2003		10.1007/978-3-540-24594-0_65	grid file;digital library;semantic grid;computer science;operating system;peer-to-peer;data grid;database;distributed computing;grid;world wide web;grid computing;data	HPC	-26.054654391361144	48.493948677783415	75469
44285141d9e3d7dd9076b78a5765d46acd7e3922	controlling quality of service in multi-tier web applications	databases;yarn;service level;routing;web and internet services;actuators;quality of service web server actuators network servers databases web and internet services computer architecture routing yarn delay;computer architecture;network servers;service differentiation;internet services;web server;quality of service;coarse grained;analytical model	The need for service differentiation in Internet services has motivated interest in controlling multi-tier web applications. This paper describes a tier-to-tier (T2T) management architecture that supports decentralized actuator management in multi-tier systems, and a testbed implementation of this architecture using commercial software products. Based on testbed experiments and analytic models, we gain insight into the value of coordinated exploitation of actuators on multiple tiers, especially considerations for control efficiency and control granularity. For control efficiency, we show that more effective utilization of tiers can be achieved by using actuators on the bottleneck tier rather than only using actuators on the entry tier. For granularity of control (the ability to achieve a wide range of service level objectives) we show that a fine granularity of control can be achieved through a coordinated, cross-tier exploitation of coarse grained actuators (e.g., multiprogramming level), an approach that can greatly reduce controllerinduced variability.	autonomic computing;centralized computing;commercial software;computer multitasking;control theory;database server;distributed control system;experiment;heart rate variability;ibm websphere application server;inter-rater reliability;memory controller;multitier architecture;quality of service;service-level agreement;spatial variability;testbed;web application;web service	Yixin Diao;Joseph L. Hellerstein;Sujay S. Parekh;Hidayatullah Shaikh;Maheswaran Surendra	2006	26th IEEE International Conference on Distributed Computing Systems (ICDCS'06)	10.1109/ICDCS.2006.23	web service;routing;quality of service;service level;computer science;operating system;database;distributed computing;world wide web;computer security;web server;computer network;actuator	Robotics	-24.043853498392874	58.255852635573476	75570
cd6ff9e5e3d8d20337ec166043b904818e6000f7	database pointers: a predictable way of manipulating hot data in hard real-time systems	relational data model;technology;real time;computer and information science;teknikvetenskap;natural sciences;control system;engineering and technology;teknik och teknologier;hard real time system;vehicle control system;data access;datavetenskap datalogi;computer science;database management system;data structure;real time database;high frequency;hard real time	Traditionally, control systems use ad hoc techniques such as shared internal data structures, to store control data. However, due to the increasing data volume in control systems, these internal data structures become increasingly diÆcult to maintain. A real-time database management system can provide an eÆcient and uniform way to structure and access data. However the drawback with database management systems is the overhead added when accessing data. In this paper we introduce a new concept called database pointers, which provides fast and deterministic accesses to data in hard real-time database management systems compared to traditional database management systems. The concept is especially bene cial for hard real-time control systems where many control tasks each use few data elements at high frequencies. Database pointers can co-reside with a relational data model, and any updates made from the database pointer interface are immediately visible from the relational view. We show the eÆciency with our approach by comparing it to tuple identi ers and relational processing.	control system;data model;data structure;hoc (programming language);overhead (computing);pointer (computer programming);real-time clock;real-time computing;real-time locating system;real-time operating system;relational database;relational model	Dag Nyström;Aleksandra Tesanovic;Christer Norström;Jörgen Hansson	2003		10.1007/978-3-540-24686-2_28	data access;embedded system;real-time computing;data structure;computer science;control system;operating system;high frequency;data mining;database;technology	DB	-23.58518091871289	49.20707497224369	75768
7080a612803be1e131ccca72482ce11e1c068936	an autonomic cross-platform operating environment for on-demand internet computing		The heterogeneous nature of the Internet makes it difficult to support on demand Internet computing. Promising remedies are self-managing Internet application environments that maintain themselves without particular user or application intervention. We have developed a self-managing approach which supports on demand Internet computing in an autonomic cross-platform operating environment. The realization in Java is based on the Crossware Development Kit (XDK) and a Crosslet Engine built on top of it. An Internet Application Workbench has been developed which provides a full-featured graphical desktop interface and can be used to demonstrate various applications of the cross-platform operating environment, such as nomadic desktop computing, on demand application hosting and ad hoc execution migration. 1. On Demand Internet Computing An ongoing trend is the use of Internet applications on demand, i.e. applications which are not explicitly installed on a certain node by a platform administrator but dynamically deployed and run as they are requested by the application user. From this point of view, the realization of on demand Internet computing encounters various challenges which reflect the spontaneous use of computer devices and applications in unmanaged and heterogeneous environments. An important subject is the unpredictable constellation of application requirements and platform capabilities which makes it difficult to provide an all-purpose application and platform configuration, as shown in fig. 1. Fig. 1. On Demand Internet Computing Required resources cannot be provided and allocated in advance but have to be determined and requested in the moment they are needed. In turn, resources are alternately utilized in different application scenarios, and the relationship of device user, application installation and employed computer device is untied and replaced by a dynamic assignment. Due to its large extent and diversity, the Internet also renders precautionary configuration attempts of application deployers and platform administrators unfeasible. This results in Internet application systems which cannot be addressed by administered approaches. Instead, a suitable approach must be able to manage dynamic and unspecified scenarios without manual user intervention. Promising remedies are autonomic computing systems that are supposed to maintain themselves and dynamically adjust the operating environment according to the current application scenario. 2. Autonomic Cross-Platform Operating Environment In the project CROSSWARE [1], the overall goal is the development of an autonomic crossplatform operating environment which enables on demand computing in the Internet [2]. A particular objective is the separated management of platform configurations and application requirements, as shown in fig. 2. Fig. 2. Autonomic Cross-Platform Operating Environment Each time a customer wants to start an application on demand, a suitable application is prepared by using existing features and resources of the current platform installation. This is achieved without particular user intervention but by evaluating given platform configurations and application requirements in a self-managing way. The Java realization consists of a Crossware Development Kit (XDK) and a Crosslet Engine which can be run by any legacy Java Runtime Environment (JRE) [3], as shown in fig. 3. Fig. 3. Java Realization The crosslet engine supports distributed deployment, dynamic composition, adaptive hosting, nomadic customization, transparent interconnection and ad hoc migration of Java-based applications. Besides of specific applications, so called crosslets, most regular Java applications may be run without modifications as long as they do not interfere with the self-managed operations of the engine, e.g. by using a custom class loader or installing a different security manager.	autonomic computing;cloud computing;desktop computer;graphical user interface;hoc (programming language);interconnection;java classloader;java virtual machine;operating environment;point of view (computer hardware company);rendering (computer graphics);requirement;rich internet application;self-management (computer science);software deployment;spontaneous order;system administrator;utility computing;workbench;xbox development kit;xfig	Stefan Paal	2010			autonomic computing;personalization;operating environment;the internet;operating system;security management;computer science;workbench;java;distributed computing;cross-platform	HPC	-31.31626751144984	52.23482488918518	75930
951e1b3e04809a586d7a096e3b75e8b6d8970db9	maintaining coherency of dynamic data in cooperating repositories	system-wide communication;maintaining coherency;data item;low communication;dynamic data;certain point;coherency maintenance;stock price;efficient dissemination tree;careful dissemination;dynamic data item;real time	In this paper , we considertechniquesfor disseminating dynamic data—suchas stock prices and real-time weatherinformation—fromsourcesto a setof repositories.We focuson theproblemof maintainingcoherency of dynamicdataitemsin anetwork of cooperatingrepositories. We show thatcooperation amongrepositories— whereeachrepositorypushesupdatesof dataitems to otherrepositories—helps reducesystem-widecommunicationandcomputationoverheadsfor coherency maintenance.However, contraryto intuition, we alsoshow that increasingthe degreeof cooperationbeyond a certainpointcan,in fact,bedetrimentalto thegoalof maintaining coherency at low communicationandcomputational overheads. We presenttechniques(i) to derive the“optimal” degreeof cooperationamongrepositories, (ii) to constructanefficientdisseminationtreefor propagatingchangesfrom sourcesto cooperatingrepositories, and(iii) to determinewhento pushan updatefrom one repositoryto anotherfor coherency maintenance.We evaluatethe efficacy of our techniquesusingreal-world tracesof dynamicallychangingdataitems(specifically, stockprices)andshow thatcarefuldisseminationof updatesthrougha network of cooperatingrepositoriescan substantiallylower thecostof coherency maintenance.	artificial intuition;dynamic data;overlay network;real-time transcription	Shetal Shah;Krithi Ramamritham;Prashant J. Shenoy	2002			dynamic data;computer science;data mining;database;distributed computing	DB	-21.422996234772313	50.46760700116695	76250
04b3b1e1a80f5194081450bba734dfcbe09dbaab	soar-dsgrid: service-oriented architecture for distributed simulation on the grid	resource management;distributed computing;computer networks;computational modeling;service oriented architecture computational modeling computer simulation distributed computing grid computing resource management middleware web services costs computer networks;web services;next generation;middleware;service oriented architecture;distributed simulation;grid computing;computer simulation	Simulation is a low cost alternative to experimentation on real-world physical systems. Grid technology enables coordinated use of and secure access to distributed computing resources and data sources. The service-oriented architecture (SOA) is an ideal paradigm for next generation computing. The loose coupling among services in the SOA relieves service consumers from detailed knowledge of implementation, implementation language, and execution platform of the services to be consumed. In this paper, we propose a framework for developing a component-based distributed simulation and executing the simulation in a service-oriented architecture on the Grid. This framework consists of the schemas for developing simulation components and simulation applications, and underlying base component service modules for constructing a simulation component as a service. The use of component interfaces and schemas enables collaborative development of simulation applications, and the deployment of simulation components as services takes advantage of the SOA (e.g., loose coupling, heterogeneity, and transport neutrality). This paper discusses the motivation for developing such a framework and describes the details of its development.	component-based software engineering;distributed computing;loose coupling;next-generation network;object language;programming paradigm;service-oriented architecture;service-oriented device architecture;simulation;soar (cognitive architecture);software deployment	Xinjun Chen;Wentong Cai;Stephen John Turner;Yong Wang	2006	20th Workshop on Principles of Advanced and Distributed Simulation (PADS'06)	10.1109/PADS.2006.33	computer simulation;web service;real-time computing;computer science;resource management;theoretical computer science;operating system;service-oriented architecture;middleware;distributed computing;utility computing;services computing;computational model;grid computing	HPC	-31.45740515485033	49.22129350580362	76537
411c3c55361f2287b1a66cc52a93df1e7ed94863	log-based abnormal task detection and root cause analysis for spark		Abstract-Application delays caused by abnormal tasks arecommon problems in big data computing frameworks. Anabnormal task in Spark, which may run slowly withouterror or warning logs, not only reduces its resident node’sperformance, but also affects other nodes’ efficiency.Spark log files report neither root causes of abnormal tasks,nor where and when abnormal scenarios happen. AlthoughSpark provides a “speculation” mechanism to detect stragglertasks, it can only detect tailed stragglers in each stage. Sincethe root causes of abnormal happening are complicated, thereare no effective ways to detect root causes.This paper proposes an approach to detect abnormality andanalyzes root causes using Spark log files. Unlike commononline monitoring or analysis tools, our approach is a pureoff-line method that can analyze abnormality accurately. Ourapproach consists of four steps. First, a parser preprocessesraw log files to generate structured log data. Second, ineach stage of Spark application, we choose features relatedto execution time and data locality of each task, as well asmemory usage and garbage collection of each node. Third,based on the selected features, we detect where and whenabnormalities happen. Finally, we analyze the problems usingweighted factors to decide the probability of root causes. In thispaper, we consider four potential root causes of abnormalities,which include CPU, memory, network, and disk. The proposedmethod has been tested on real-world Spark benchmarks.To simulate various scenario of root causes, we conductedinterference injections related to CPU, memory, network,and Disk. Our experimental results show that the proposedapproach is accurate on detecting abnormal tasks as well asfinding the root causes	apache spark;big data;central processing unit;data logger;garbage collection (computer science);ibm notes;locality of reference;log analysis;run time (program lifecycle phase);sensor;simulation;system monitoring;vii	Siyang Lu;BingBing Rao;Xiang Wei;Byung-Chul Tak;Long Wang	2017	2017 IEEE International Conference on Web Services (ICWS)	10.1109/ICWS.2017.135	computer science;feature extraction;big data;spark (mathematics);real-time computing;garbage collection;abnormality;root cause analysis	OS	-23.61353126063043	55.29627967452302	76586
fa71b0b2543670fbd4a104a0073989e129727f00	data analysis of cyber-activity within high performance computing environments		High performance computing (HPC) environments are becoming the norm for daily use. However, the resilience of these systems is questionable because their complex infrastructure makes troubleshooting both the location and cause of failures extremely difficult. These same reasons make HPCs prone to virulent activity. This paper presents a data analysis framework for analyzing ranges of failure observations as a result of malicious activity. Taking into account the internal reliability infrastructure, data network extrapolation is performed as a preprocessing tool that accurately calculates the normalized failure rates. Next, nonlinear regression is performed on the spectrum of observations taking into account the magnitude, growth rate, and midpoint behavior. Additionally, influence analysis is performed that considers outlying observations. The empirical results using a simulated supercomputing modeling and simulation framework show improvement, in terms of characterization performance, where approximately 91% of the nodes were properly characterized. The results of this work can be applied to develop robust task-scheduling frameworks within supercomputing architectures.	extrapolation;nonlinear system;preprocessor;scheduling (computing);simulation;supercomputer	L. Ji;S. Kolhe;A. D. Clark	2017	2017 IEEE 8th Annual Ubiquitous Computing, Electronics and Mobile Communication Conference (UEMCON)	10.1109/UEMCON.2017.8249003	reliability engineering;human–computer interaction;extrapolation;troubleshooting;magnitude (mathematics);normalization (statistics);computer science;modeling and simulation;supercomputer;nonlinear regression	HPC	-23.648183603646306	57.55742927418466	76635
d264bd703139c0b72a56f53fcdb832142eb6fbcb	servogrid complexity computational environments (cce) integrated performance analysis	servogrid complexity computational environments;integrated performance analysis;geographic information systems;distributed application;data mining;geographic information system;service oriented architecture;internet;grid computing	In this paper we describe the architecture and initial performance analysis results of the SERVOGrid complexity computational environments (CCE). The CCE architecture is based on a lightly coupled, service oriented architecture approach that is suitable for distributed applications that are tolerant of Internet latencies. CCE focuses on integrating diverse Web and grid services for coupling scientific applications to geographical information systems. The services and coupling/orchestrating infrastructure are mapped to problems in geophysical data mining, pattern informatics, and multiscale geophysical simulation.	certified computer examiner;data mining;distributed computing;geographic information system;grid computing;informatics;multiscale modeling;profiling (computer programming);service-oriented architecture;simulation;world wide web	Galip Aydin;Mehmet S. Aktas;Geoffrey C. Fox;Harshawardhan Gadgil;Marlon E. Pierce;Ahmet Sayar	2005	The 6th IEEE/ACM International Workshop on Grid Computing, 2005.		the internet;computer science;data science;service-oriented architecture;data mining;database;distributed computing;geographic information system;world wide web;grid computing	HPC	-30.90206047042045	50.10848184368288	76650
b86f718e148ed63590d36f4d66830507aaa7c0d0	special issue on load balancing in distributed systems: introduction	distributed system;load balance	We are witnessing the explosion of the Internet and the WWW. They make information, computation, and communication resources available worldwide. Because of the open nature of the Internet, its exponential growth, and the popularity of some of its resources, congestion and bottlenecks can appear anytime, anyplace and they are most of the time unpredictable and uncontrollable. Load balancing in distributed systems, such as the Internet, tries to bring order in the use of computational and communication resources. It is thus and will remain a very important research issue in the area of distributed systems. In this special issue, we examine several aspects of load balancing, contribute some new results, and point out open problems where fruitful research can be expected. The papers that appear in this special issue report on research work conducted under LYDIA 8144, and IT Long Term Research project funded by the European Union. The purpose of LYDIA is to design, analyze and experiment with load balancing algorithms for numerically intensive parallel computations and for On-Line Transaction Processing (OLTP) systems. Workload characterization for distributed OLTP applications can produce insights on the data access and CPU consumption behavior of transactions. Transactions executing precompiled programs can exhibit statistically predictable behavior. Therefore, investigation of the workload that they generate can improve the data affinity and the nodes utilization.	anytime algorithm;bottleneck (software);central processing unit;computation;data access;distributed computing;internet;load balancing (computing);network congestion;numerical analysis;online transaction processing;processor affinity;time complexity;www	Christos Nikolaou;Lutz Richter	1997	Inf. Sci.	10.1016/S0020-0255(96)00170-3	computer science;load balancing	DB	-20.497859260897915	58.54740524448302	76671
a4a582c6739c8f6d88f3ad01671b5b6733eb464c	bridges: a uniquely flexible hpc resource for new communities and data analytics	database;gpu;hpc;big data;data analytics;hadoop;usability;architecture;applications	In this paper, we describe Bridges, a new HPC resource that will integrate advanced memory technologies with a uniquely flexible, user-focused, data-centric environment to empower new research communities, bring desktop convenience to HPC, connect to campuses, and drive complex workflows. Bridges will differ from traditional HPC systems and support new communities through extensive interactivity, gateways (convenient web interfaces that hide complex functionality and ease access to HPC resources) and tools for gateway building, persistent databases and web servers, high-productivity programming languages, and virtualization. Bridges will feature three tiers of processing nodes having 128GB, 3TB, and 12TB of hardware-enabled coherent shared memory per node to support memory-intensive applications and ease of use, together with persistent database and web nodes and nodes for logins, data transfer, and system management. State-of-the-art Intel® Xeon® CPUs and NVIDIA Tesla GPUs will power Bridges' compute nodes. Multiple filesystems will provide optimal handling for different data needs: a high-performance, parallel, shared filesystem, node-local filesystems, and memory filesystems. Bridges' nodes and parallel filesystem will be interconnected by the Intel Omni-Path Fabric, configured in a topology developed by PSC to be optimal for the anticipated data-centric workload. Bridges will be a resource on XSEDE, the NSF Extreme Science and Engineering Discovery Environment, and will interoperate with other advanced cyberinfrastructure resources. Through a pilot project with Temple University, Bridges will develop infrastructure and processes for campus bridging, consisting of offloading jobs at periods of unusually high load to the other site and facilitating cross-site data management. Education, training, and outreach activities will raise awareness of Bridges and data-intensive science across K-12 and university communities, industry, and the general public.	bridging (networking);central processing unit;coherence (physics);cross-site scripting;cyberinfrastructure;data-intensive computing;database;desktop computer;exception handling;graphics processing unit;ibm notes;interactivity;interoperability;nvidia tesla;omni-path;parallel computing;programming language;shared memory;systems management;terabyte;usability;user interface;web server	Nicholas A. Nystrom;Michael J. Levine;Ralph Roskies;J. Ray Scott	2015		10.1145/2792745.2792775	computer science;operating system;database;world wide web	HPC	-28.557730842688787	52.33649620528576	76764
5a44a4c0a58a7d443d403f0d0ddca6039cfc90bf	multipi: a java implementation of multiparty interaction for distributed computing	distributed computing		distributed computing;java	James Sinnamon;Peiyi Tang	2004			distributed concurrency control;computer science;theoretical computer science;distributed computing;distributed algorithm;csiv2;distributed design patterns;java	HPC	-29.349745105860173	46.66815553099197	76976
7b0fa5176f9ad12b02b7e03594456defae3572a6	a sharable storage service for distributed computing systems in combination of remote and local storage	sharable storage;broadband network;distributed computing systems;application server;distributed computing;iscsi;preloading;distributed computing system;data management system;pervasive storage;data transfer	Traditional Grid data management systems have been developed on the supposition that the network bandwidth is unconstrained with dedicated broadband network lines and the storage space is enough for storing replicas. This paper proposes a storage service for distributed computing systems without the supposition, by adopting the concept of pervasive storage in the distributed computing. The  Sharable Storage  space is recognized as the application server's local storage. Whenever the user commits, data transfer is automatically performed using iSCSI protocol. The storage service is destined for numerous data transfer of relatively small files, and simulated under the condition of a restrictive network bandwidth. File preloading function could make the applications start early without dedicated broadband network lines. The computing sites could benefit from the proposed storage service than waiting until the whole file is transferred for replication and keeping the replica stored without the users' explicit note for reuse.	distributed computing;thread-local storage	MinHwan Ok	2009		10.1007/978-3-642-03095-6_52	parallel computing;storage area network;converged storage;computer science;direct-attached storage;operating system;database;distributed computing;information repository;application server;computer network;broadband networks	HPC	-22.212456637699955	51.04355125700946	77269
53a6d5fd01190125124d2027b1cc3189ea50db52	ssd performance modeling using bottleneck analysis		Solid-State Drives (SSDs) are widely deployed for high throughput and low latency. However, the unpredictable access latency of SSDs makes it difficult to satisfy quality-of-service requirements and fully achieve the performance potential. In fact, it has been a fundamental challenge to accurately predict the access latency of modern SSDs performing many non-disclosed, device-specific intra-SSD optimizations. In this paper, we propose SSDcheck, a novel SSD performance model which accurately predicts the latency of future SSD accesses. After first identifying write buffer (WB) and garbage collection (GC) as the key components in modeling the access latency, we develop diagnosis snippets to identify the target SSDs critical intra-SSD parameters (e.g., WB size). Finally, we construct the SSDs access-latency model with the identified parameters. Our system-level evaluations using five commodity SSDs show that SSDcheck achieves up to 93 percent prediction accuracy. Our real-world prototype applying an SSDcheck-aware system-level request scheduling can significantly improve both throughput and tail latency by up to 2.1x and 1.46x, respectively.	black box;garbage collection (computer science);prototype;quality of service;requirement;scheduling (computing);solid-state drive;throughput;write buffer	Jihun Kim;Joonsung Kim;Pyeongsu Park;Jong Kim;Jangwoo Kim	2018	IEEE Computer Architecture Letters	10.1109/LCA.2017.2779122	parallel computing;throughput;latency (engineering);real-time computing;latency (engineering);feature extraction;write buffer;computer science;scheduling (computing);garbage collection;bottleneck	OS	-21.288530525108126	56.37698358660516	77657
f29977e0569dc7e5d92580ef860c7680b5d2a52d	cloud server benchmarks for performance evaluation of new hardware architecture		Adding new hardware features to a cloud computing server requires testing both the functionalities and the performance of the new hardware mechanisms. However, commonly used cloud computing server workloads are not well-represented by the SPEC integer and floating-point benchmark and Parsec suites typically used by the computer architecture community. Existing cloud benchmark suites for scale-out or scale-up computing are not representative of the most common cloud usage, and are very difficult to run on a cycle-accurate simulator that can accurately model new hardware, like gem5. In this paper, we present PALMScloud, a suite of cloud computing benchmarks for performance evaluation of cloud servers, that is ready to run on the gem5 cycle-accurate simulator. We demonstrate how our cloud computing benchmarks are used in evaluating the cache performance of a new secure cache called Newcache as a case study. We hope that these cloud benchmarks, ready to run on a dual-machine gem5 simulator or on real machines, can be useful to other researchers interested in improving hardware micro-architecture and cloud server performance.	application security;benchmark (computing);cloud computing;computer architecture simulator;microarchitecture;performance evaluation;scalability;security testing;server (computing);simulation;virtual private server;x86	Hao Wu;Fangfei Liu;Ruby B. Lee	2016	CoRR		parallel computing;real-time computing;single-chip cloud computer;cloud computing;computer science;operating system;cloud testing	Arch	-27.96087968309208	56.40883565708995	77753
c0c8bb39585532b3ec066c75709cdddd04fc28fe	usable computing on open distributed systems	computing paradigm;best effort;quality of service;grid computing;autonomic computing;open distributed system	An open distributed system provides a best-effort guarantee on the quality of service provided to applications. This has worked well for throughput-based applications of the kind typically executed in Condor or BOINCstyle environments. For other applications, the absence of timeliness of correctness guarantees limit the utility or appeal of this environment. Computational results that are too late or erroneous are not usable to the application. We present techniques designed to efficiently promote usable computing in open distributed systems.	best-effort delivery;computation;correctness (computer science);distributed computing;quality of service;throughput	Jon B. Weissman	2008		10.1145/1383529.1383539	best-effort delivery;real-time computing;quality of service;computer science;operating system;database;distributed computing;utility computing;distributed design patterns;grid computing;autonomic computing	HPC	-24.507453130489793	53.034937404713176	77880
4f027d520cae3984cf735569739a503f65b76a60	efficient allocation of hierarchically-decomposable tasks in a sensor web contract net	distributed system;explanation;protocols;sensor systems;multi agent system;system configuration;virtual training;operant conditioning;contract net protocol;resource management;environmental conditions;data processing;weather forecasting;theory of mind;contracts;sensor network;contracts sensor systems intelligent agent intelligent sensors resource management multiagent systems protocols data processing scalability weather forecasting;intelligent agent;resource availability;scalability;sensor web;intelligent sensors;multiagent systems	In large, distributed systems, such as a sensor web, allocating resources to tasks that span multiple providers presents significant challenges. Individual subtasks associated with a task could potentially be assigned to a number of agents (e.g., when there is overlap in sensor or data processing capability among constituent sensor networks). This problem is further compounded by the dynamic nature of a sensor web, in which both desired tasks and resource availability change with time and environmental conditions. This paper presents a novel variation of the contract net protocol (CNP) for subtask allocation, which employs brokers to limit communication overhead in a two-phase CNP and aggregate domain information from groups of agents. Experimental results using this subtask allocation approach verify its efficiency and scalability. These results also suggest specific refinements and appropriate parameters for a variety of system configurations and operating conditions in sensor webs and other large multi-agent systems.	aggregate data;contract net protocol;distributed computing;multi-agent system;overhead (computing);scalability;sensor web;two-phase commit protocol	John S. Kinnebrew;Gautam Biswas	2009	2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology	10.1109/WI-IAT.2009.154	sensor web;communications protocol;real-time computing;scalability;simulation;wireless sensor network;weather forecasting;computer science;artificial intelligence;operant conditioning;multi-agent system;data mining;distributed computing;contract net protocol;world wide web;intelligent sensor	AI	-25.813788925774183	56.47286498339277	77889
59252997658f25e6535602df7f1e02f188c70f62	a framework for enforcing application policies in database systems	database system;policy;system configuration;data collection;total cost of ownership;system performance;access control;self managing	As database systems have grown in terms of scale and complexity, administration tasks have become increasingly difficult and time consuming. A scarcity of skilled database professionals has meant that human costs have begun to dominate the total cost of ownership (TCO) of a database system. Database vendors are under immense pressure to provide solutions that make their products easy to administer in areas such as problem diagnostics, monitoring, query tuning, access control and system configuration.  To address this issue, we have built a framework that allows control over many administration operations via the use of policies. Users can uniformly define, manage and enforce policies to affect disparate aspects of the system.  In our framework, policies are declarative constructs that are comprised of type, scope, condition and action. Policy groups cover query monitoring and tuning, query prioritization, system configuration, access control, report generation, etc. Policy scope defines the domain over which policies apply. Policy actions are performed if certain conditions are true. This framework has been fully integrated into DB2 for z/OS V9. Using detailed system performance evaluations, we report that enforcement of policies is largely a function of data-collection granularity. Under the setting for normal monitoring with minimal report, the overhead on system performance is very low (0.1%).	access control;complexity;database;declarative programming;operating system;overhead (computing);system configuration;total cost of ownership;z/os	Lin Qiao;Basuki Soetarman;Gene Fuh;Adarsh Pannu;Baoqiu Cui;Thomas Beavin;William Kyu	2007		10.1145/1247480.1247597	computer science;access control;data mining;database;computer performance;data collection	DB	-32.623690872311755	57.90417866547584	77906
bc2d6ceec7746a6b2d9971dbcb3c77b828005f29	on the design and implementation of a simulator for parallel file system research	simulation;client server systems;servers;computational modeling;i o scheduling simulation parallel file system;scheduling algorithms;scheduling;distributed shared memory systems;script based interface parallel file system high performance computing hpc center pfs design i o optimization hpc system pfssim trace driven simulator distributed storage system i o scheduler network structure workload subsystem modeling client data server;scheduling client server systems distributed shared memory systems file organisation parallel processing;parallel file system;distributed databases;servers file systems throughput computational modeling scheduling algorithms distributed databases data models;parallel processing;i o scheduling;file systems;data models;throughput;file organisation	Due to the popularity and importance of Parallel File Systems (PFSs) in modern High Performance Computing (HPC) centers, PFS designs and I/O optimizations are active research topics. However, the research process is often time-consuming and faces cost and complexity challenges in deploying experiments in real HPC systems. This paper describes PFSsim, a trace-driven simulator of distributed storage systems that allows the evaluation of PFS designs, I/O schedulers, network structures, and workloads. PFSsim differentiates itself from related work in that it provides a powerful platform featuring a modular design with high flexibility in the modeling of subsystems including the network, clients, data servers and I/O schedulers. It does so by designing the simulator to capture abstractions found in common PFSs. PFSsim also exposes script-based interfaces for detailed configurations. Experiments and validation against real systems considering sub-modules and the entire simulator show that PFSsim is capable of simulating a representative PFS (PVFS2) and of modeling different I/O scheduler algorithms with good fidelity. In addition, the simulation speed is also shown to be acceptable.	algorithm;clustered file system;computer data storage;experiment;forward secrecy;i/o scheduling;input/output;modular design;parallel virtual file system;parallel computing;scheduling (computing);server (computing);simulation;systems design;systems modeling	Yonggang Liu;Renato J. O. Figueiredo;Yiqi Xu;Ming Zhao	2013	2013 IEEE 29th Symposium on Mass Storage Systems and Technologies (MSST)	10.1109/MSST.2013.6558438	parallel computing;real-time computing;computer science;operating system	HPC	-23.011989705327206	56.357130835091475	78056
a0c4767eea5efb02347b8e52cbf54547178ae77f	a global timestamp-based approach to enhanced data consistency and fairness in collaborative virtual environments	estensibilidad;systeme temps reel;distributed system;on line systems;hierarchical system;fairness;online game;systeme cooperatif;realite virtuelle;realidad virtual;real time distributed systems;implementation;real time;systeme hierarchise;conceptual analysis;distributed computing;virtual reality;real time processing;simultaneidad informatica;object oriented programming;analisis conceptual;proof of concept;partage des ressources;equite;decentralized system;equidad;tratamiento tiempo real;sistema jerarquizado;traitement temps reel;concurrency;equity;cooperative systems;object oriented;systeme en ligne;resource sharing;particion recursos;sistema descentralizado;calculo repartido;tmo;real time system;sistema tiempo real;extensibilite;scalability;systeme decentralise;analyse conceptuelle;network game;implementacion;programmation orientee objet;data consistency;simultaneite informatique;collaborative virtual environment;calcul reparti;hierarchical model;global timestamp	Collaborative virtual environments are rapidly gaining in popularity for the implementation of intuitive workspaces and networked gaming environments. Primary challenges that have to be addressed by these systems are maintaining systemwide data consistency, enabling fair resource sharing and interaction between users, and compensating network latency jitters. This paper investigates two major concurrency problems - (1) fair request handling and (2) systemwide data consistency. A global timestamp-based approach is presented in combination with different implementation models. Both centralized and decentralized approaches to achieving systemwide consistency and fairness are analyzed. To improve the scalability of the approach, hierarchical models are discussed. Implementation of a proof-of-concept system based on a high-level object-oriented real-time programming scheme called TMO is presented.	application programming interface;bayesian network;best, worst and average case;centralized computing;collaborative virtual environment;communications satellite;concurrency (computer science);fairness measure;high- and low-level;ibm notes;microcomputer;multicast;nest (neural simulation tool);real-time cmix;real-time clock;real-time computing;real-time transcription;scalability;virtual reality;workspace	Sung-Jin Kim;Falko Kuester;K. H. Kim	2004	Multimedia Systems	10.1007/s00530-004-0153-4	real-time computing;simulation;computer science;operating system;distributed computing;virtual reality;object-oriented programming	HPC	-24.55928504416385	46.70731520178231	78069
730f674eea5f2d0ba35d608c7091c3ba3fb08c04	redundant virtual machine placement for fault-tolerant consolidated server clusters	virtual machine;high availability;fault tolerant;redundant virtual machine placement;placement algorithm;virtual machining;placement algorithm virtual machine fault tolerant redundant configuration;virtual machining fault tolerance platform virtualization resource virtualization fault tolerant systems laboratories national electric code protection hardware resource management;host server failures redundant virtual machine placement fault tolerant consolidated server clusters;host server failures;servers;fault tolerant computing;time factors;redundant configuration;virtual machines;fault tolerant systems;fault tolerance;fault tolerant consolidated server clusters;optimization;virtual machines fault tolerant computing;load modeling	Consolidated server systems using server virtualization involves serious risks of host server failures that induce unexpected downs of all hosted virtual machines and applications. To protect applications requiring high-availability from unpredictable host server failures, redundant configuration using virtual machines can be an effective countermeasure. This paper presents a virtual machine placement method for establishing a redundant configuration against host server failures with less host servers. The proposed method estimates the requisite minimum number of virtual machines according to the performance requirements of application services and decides an optimum virtual machine placement so that minimum configurations survive at any k host server failures. The evaluation results clarify that the proposed method achieves requested fault-tolerance level with less number of hosting servers compared to the conventional N+M redundant configuration approach.	algorithm;computer cluster;fault tolerance;free-form deformation;high availability;requirement;server (computing);virtual machine;virtual private server	Fumio Machida;Masahiro Kawato;Yoshiharu Maeno	2010	2010 IEEE Network Operations and Management Symposium - NOMS 2010	10.1109/NOMS.2010.5488431	fault tolerance;real-time computing;computer science;virtual machine;operating system;distributed computing;server farm	Arch	-23.300458133075416	60.08259911221052	78455
855d781d8250342697850ae2131b25b880d522a0	unicore and grip: experiences of grid middleware development	grid middleware;scientific computing;middleware;qa76 computer software	We describe our experiences with the UNICORE Grid environment. Several lessons of general applicability can be drawn in regard to user uptake and security. The principal lesson is that more effort should be taken to be made to meet the needs of the target user community of the middleware development. Novel workflow strategies, in particular, should not be imposed on an existing community.	middleware;unicore;virtual community	Denis A. Nicole	2005			middleware;computer science;operating system;middleware;database;world wide web;grid computing	HCI	-32.45429767166015	52.16965838615473	78575
40fe8c719bbd990256121b8f572527ed9a12960e	performance prediction and analysis of boinc projects: an empirical study with emboinc	volunteer computing;world community grid;empirical study;simulation;emulation;model complexity;user interfaces and human computer interaction;management of computing and information systems;middleware;performance prediction;docking home;computer science;processor architectures	Middleware systems for volunteer computing convert a set of computers that is large and diverse (in terms of hardware, software, availability, reliability, and trustworthiness) into a unified computing resource. This involves a number of scheduling policies and parameters, which have a large impact on the throughput and other performance metrics. How can we study and refine these policies? Experimentation in the context of a working project is problematic, and it is difficult to accurately model complex middleware in a conventional simulator. Instead, we use an approach in which the policies being studied are “emulated”, using parts of the actual middleware. In this paper we describe EmBOINC, an emulator based on the BOINC middleware system. EmBOINC simulates a population of volunteered clients (including heterogeneity, churn, availability, and reliability) and emulates the BOINC server components. After describing the design of EmBOINC and its validation, we present three case studies in which the impact of different scheduling policies are quantified in terms of throughput, latency, and starvation metrics.	boinc;computer;emulator;fabric computing;job stream;middleware;performance prediction;scheduling (computing);server (computing);simulation;throughput;tracing (software);trust (emotion);user interface;volunteer computing;world community grid	Trilce Estrada;Michela Taufer;David P. Anderson	2009	Journal of Grid Computing	10.1007/s10723-009-9126-3	emulation;real-time computing;simulation;computer science;boinc credit system;operating system;middleware;database;distributed computing;empirical research;grid computing	Metrics	-22.943196462176502	57.31329628763135	78625
398fc8df425075251c49d0812ac341452ed4ee4e	hardfs: hardening hdfs with selective and lightweight versioning	hardfs detects;hardening hdfs;fail-silent behavior;fail-silent fault;random bit;random memory corruption;new approach;memory corruption;targeted corruption;real software bug;lightweight versioning	We harden the Hadoop Distributed File System (HDFS) against fail-silent (non fail-stop) behaviors that result from memory corruption and software bugs using a new approach: selective and lightweight versioning (SLEEVE). With this approach, actions performed by important subsystems of HDFS (e.g., namespace management) are checked by a second implementation of the subsystem that uses lightweight, approximate data structures. We show that HARDFS detects and recovers from a wide range of fail-silent behaviors caused by random bit flips, targeted corruptions, and real software bugs. In particular, HARDFS handles 90% of the fail-silent faults that result from random memory corruption and correctly detects and recovers from 100% of 78 targeted corruptions and 5 real-world bugs. Moreover, it recovers orders of magnitude faster than full reboot by using micro-recovery. The extra protection in HARDFS incurs minimal performance and space overheads.	apache hadoop;approximation algorithm;dce distributed file system;data structure;fail-stop;memory corruption;reboot (computing);software bug;xojo	Thanh Do;Tyler Harter;Yingchao Liu;Haryadi S. Gunawi;Andrea C. Arpaci-Dusseau;Remzi H. Arpaci-Dusseau	2013			parallel computing;real-time computing;operating system;database;distributed computing	OS	-21.786699912764544	48.65841997760392	78674
e36fcd86516790c2ccdb97db4ae4792a88cfa6a2	an approach to object sharing in distributed datbase systems	database system;distributed database system;distributed environment	This paper describes DODM, a simple model for object sharing in distributed database systems. The model provides a small set of operations for object definition, manipulation, and retrieval in a distributed environment. Relationships among oojects can be established across database boundarres, objects are relocatable within the distributed environment, and mechanisms are provided for object sharing among individual databases. An object naming convention supports location transparent object references; that is, objects can be referenced by user-defined names rather than by address. The primitive operations introduced can be used as the basis for the specification and stepwise development of database models and database systems of increasing complexity. An example is provided to illustrate the use of DODM in the design of a distributed database system supporting a semantically expressive database model. 1. Int reduction Distributed computing systems are becoming increasingly common. This trend is largely caused by the decreasing cost of hardware: not only are powerful personal computers becoming so Inexpensive today that individuals can afford them for personal use, but the cost of computer networks that enable computer systems to exchange information at a very high rate is decreasing drastically. Decentralization overcomes many of the limitations and deficiencies of centralized systems. A network ‘This research was supported, in part, by the Joint Sewlees Electronics Program through the Air Force Office of ScientWic Research under contract F49820-El-C.0070. of computers simply provides a higher level of performance, availability, reliability, fault tolerance, and security than a centralized computer system. In addition to the technical advantages that make decentralized systems feasible, social attitudes tend to indicate that a collection of smaller, autonomous computer systems are preferable to large central systems. The growing popularity of distributed computing establishes a need for mechanisms that allow individual users to communicate with each other and share both hardware and software resources. Individual users also need access to the growing number of “public” databases, which contain a variety of information such as grocery prices, the values of stocks, and the histories of bank accounts. Of course, sharing mechanisms decrease the autonomy of the components of the distributed environment, and affect the performance, availability, reliability, fault tolerance, and security of the total system. Sharing and communication mechanisms alsc introduce data transmission and naming problems. Most current approaches to distributed database management system design fail to adequately address issues concerning location transparency (the ability to reference data by name rather than by address), logical decentralization, catalog management, and the uniform handling of meta-data and user-data. Logically centralized database systems [Rothnie 80, Stonebraker 77, Andler 821 provide the users with a single integrated database schema describing all the data in the physically centralized or distributed environment. Recent research has also resulted in approaches to support the integration of heterogeneous as well as homogeneous (preexisting) databases [Motro 81, Smith 81, Litwin 81, Kimbleton 791. However, a critical remaining problem is accommodating informalion sharing among individual, autonomous databases. Finally, existing distributed database system architectures that emphasize the autonomy of the individual databases [Heimbigner 82, Williams 81, Tsichritzis 821 require centralized or complex catalog management. The aim of the research described in this paper is to define a simple model for object sharing in distributed database systems. This is done by stepwise development of a series of object-oriented models. First, a simple model called ODM (for object-orienfed databsse model) is defined. ODM provides a	autonomous robot;centralisation;centralized computing;commitment ordering;computer performance;computer security;database model;database schema;distributed computing;distributed database;fault tolerance;ontology definition metamodel;personal computer;reliability engineering;stepwise regression;systems design	Peter Lyngbæk;Dennis McLeod	1983			distributed algorithm;database theory;distributed data store;database transaction;database tuning;computer science;database model;database;distributed computing;distributed object;distributed system security architecture;distributed design patterns;database schema;distributed database;database testing;database design;replication;distributed computing environment;distributed concurrency control	DB	-28.58606081290187	48.53790890460362	78812
3d70e44a2084886d5dce1ea62193514c38ef9030	synchronization of distributed simulation using braodcast algorithms			algorithm;simulation	J. Kent Peacock;Eric G. Manning;Johnny W. Wong	1979			distributed algorithm;synchronization;computer science;distributed computing	ML	-29.301770791577276	46.850722769177224	78851
86ab1acb937b9b3f8c865716d3d903d23c53c64b	mind my value: a decentralized infrastructure for fair and trusted iot data trading		Internet of Things (IoT) data are increasingly viewed as a new form of massively distributed and large scale digital assets, which are continuously generated by millions of connected devices. The real value of such assets can only be realized by allowing IoT data trading to occur on a marketplace that rewards every single producer and consumer, at a very granular level. Crucially, we believe that such a marketplace should not be owned by anybody, and should instead fairly and transparently self-enforce a well defined set of governance rules. In this paper we address some of the technical challenges involved in realizing such a marketplace. We leverage emerging blockchain technologies to build a decentralized, trusted, transparent and open architecture for IoT traffic metering and contract compliance, on top of the largely adopted IoT brokered data infrastructure. We discuss an Ethereum-based prototype implementation and experimentally evaluate the overhead cost associated with Smart Contract transactions, concluding that a viable business model can indeed be associated with our technical approach.	centralized computing;data infrastructure;digital asset;ethereum;experiment;fairness measure;internet of things;interoperability;open architecture;overhead (computing);prototype;smart contract	Paolo Missier;Shaimaa Bajoudah;Angelo Capossele;Andrea Gaglione;Michele Nati	2017		10.1145/3131542.3131564	distributed computing;computer security;computer science;open architecture;leverage (finance);smart contract;producer–consumer problem;internet of things;overhead (business);business model;corporate governance	Mobile	-29.67728334708534	59.40349056959618	78879
c6791e3bc3e0cd451e6f78e0af4cb3072c08b3b7	design and development of community aware distributed content networking framework using jxta	application development;protocols;information systems;fault tolerant;design and development;availability;virtual community;information network;computer networks;fault tolerance;resource sharing;knowledge sharing;bandwidth;ip networks;information system;computational intelligence society information systems protocols fault tolerance peer to peer computing computer networks ip networks bandwidth buildings availability;peer to peer computing;peer to peer;buildings;geographic distribution;computational intelligence society	Community Information Systems (CIS)1 is the best place to share the information related to common interest of communities [1]. The requirement of better infrastructure in terms of huge storage space, computational resources and better bandwidth is the bottleneck for effective utilization of community-based systems. CIS spread over different geographical regions portrays region specific information. Networking of CIS and their distributed content resources help in development of communities through knowledge sharing, get acquainted about geographically diversified communities having similar interests and reduce the requirement of sophisticated infrastructure. The new developments in algorithms related to formation of virtual communities, distributed resource-sharing, content replication using peer-topeer techniques provides better solution for the interconnection of geographically distributed CIS. The aim of this paper is to describe the design and development of scalable, fault tolerant, community aware content networking framework, in order to support interconnection of CIS using JXTA [2] Peerto- Peer framework. Application developers can use this framework to establish a network of information systems using peer-to-peer techniques.	algorithm;computational resource;content delivery network;dvd region code;fault tolerance;information system;interconnection;jxta;list of code lyoko episodes;peer-to-peer;scalability;virtual community	N. Satyanarayana;N. Subramanian;Neelanarayanan Venkataraman;E. Usha Rani	2006	Advanced Int'l Conference on Telecommunications and Int'l Conference on Internet and Web Applications and Services (AICT-ICIW'06)	10.1109/AICT-ICIW.2006.77	fault tolerance;computer science;operating system;database;distributed computing;world wide web;computer security;information system;computer network	HPC	-30.72779811378076	50.98670281764678	79251
e5a14014076a1a10eb5acc1a006c1d9eebd6447b	using message semantics to reduce rollback in the time wrap mechanism	time warp;optimization technique;out of order;discrete event simulation	The time warp mechanism is one of the most favorite paradigm to carry out a distributed discrete event simulation, where the synchronization of events can be determined a priori when the program is being executed. The forward computation performed optimistically has to be rolled back when an out-of-order synchronization message is received. This excessive amount of rollback often haunts the performance of a time warp mechanism. We propose in this paper that the semantics of the synchronization messages being sent in a time warp execution can be exploited to reduce the amount of rollback. In particular, we illustrate with some examples how the commutativity and dependency properties of messages can be used to avoid certain types of rollback. For example, two commutative messages can be processed in any order and the resultant computation remains valid. We present a protocol for message handling in the time warp mechanism. We also give a correctness argument for the protocol, leaving the formal proof in the full paper. In the discussion, we suggest several optimization techniques that can be used in our protocol for a better improvement.		Hong Va Leong;Divyakant Agrawal;Jonathan R. Agre	1993		10.1007/3-540-57271-6_44	parallel computing;real-time computing;computer science;distributed computing	Arch	-21.7137217357579	47.20601857343432	79421
de4b431e1fdb60887323e209018c310ae87ab7e0	a cloudification methodology for multidimensional analysis: implementation and application to a railway power simulator	migration;railway simulator;mapreduce;cloud computing;many task computing	Many scientific areas make extensive use of computer simulations to study complex real-world processes. These computations are typically very resource-intensive and present scalability issues as experiments get larger even in dedicated clusters, since these are limited by their own hardware resources. Cloud computing raises as an option to move forward into the ideal unlimited scalability by providing virtually infinite resources, yet applications must be adapted to this new paradigm. This process of converting and/or migrating an application and its data in order to make use of cloud computing is sometimes known as cloudifying the application. We propose a generalist cloudification method based in the MapReduce paradigm to migrate scientific simulations into the cloud to provide greater scalability. We analysed its viability by applying it to a real-world railway power consumption simulatior and running the resulting implementation on Hadoop YARN over Amazon EC2. Our tests show that the cloudified application is highly scalable and there is still a large margin to improve the theoretical model and its implementations, and also to extend it to a wider range of simulations. We also propose and evaluate a multidimensional analysis tool based on the cloudified application. It generates, executes and evaluates several experiments in parallel, for the same simulation kernel. The results we obtained indicate that out methodology is suitable for resource intensive simulations and multidimensional analysis, as it improves infrastructure’s utilization, efficiency and scalability when running many complex experiments.		Silvina Caíno-Lores;Alberto García Fernández;Félix García Carballeira;Jesús Carretero	2015	Simulation Modelling Practice and Theory	10.1016/j.simpat.2015.04.002	real-time computing;simulation;cloud computing;computer science;human migration;operating system;distributed computing	EDA	-21.495954027461423	58.405200000957976	79694
5f68600b40d5aaf35ac63f89a9bb6c08437d6194	multiversion concurrency control for multilevel secure database systems	database system;database management systems;covert channel;concurrency control;data conflict secure multiversion concurrency control multilevel secure database systems multiversion schedulers transparency schedulers protocol;concurrency control database systems data security transaction databases access protocols processor scheduling timing computer science application software multilevel systems;transaction processing concurrency control database management systems security of data;transaction processing;database management system;security of data;multilevel security	In this paper we consider the application of mu1 tiversion schedulers in Multilevel Secure Database Management Systems (MLSDBMSs). Transactions are vital for MLSDBMSs because they provide transparency to c o n m n c y and failure. Concurrent execution of transactions may lead to contention among subjects for access to data, which in MLSPBMSs may lead to security problems. Multiversion schedulers reduce the contention for access to data by maintaining multiple versions. We describe the relation between schedules produced in MLSDBMSs and those which are multiversion serializable. We also propose a secure multiversion scheduler. We show that the scheduling protocol outputs correct schedules and is free of covert channels due to contention for access to data, i.e.. the scheduler is Data Conflict-Secure @CSecure).	covert channel;multilevel security;multiversion concurrency control;scheduling (computing);serializability	Thomas F. Keefe;Wei-Tek Tsai	1990		10.1109/RISP.1990.63865	optimistic concurrency control;real-time computing;isolation;transaction processing;rollback;covert channel;computer science;concurrency control;database;distributed computing;multiversion concurrency control;acid;snapshot isolation	Security	-23.393268961893323	47.89169545608808	79710
3c81e7649ff30a236d57135d43a1983c8255f9e5	satin: simple and efficient java-based grid programming	parallel computer;divide and conquer;network performance;load balance	Grid programming environments need to be both portable and efficient to exploit the computational power of dynamically available resources. In previous work, we have presented the divide-and-conquer based Satin model for parallel computing on clustered wide-area systems. In this paper, we present the Satin implementation on top of our new Ibis platform which combines Java’s write once, run everywhere with efficient communication between JVMs. We evaluate Satin/Ibis on the testbed of the EU-funded GridLab project, showing that Satin’s load-balancing algorithm automatically adapts both to heterogeneous processor speeds and varying network performance, resulting in efficient utilization of the computing resources. Our results show that when the wide-area links suffer from congestion, Satin’s load-balancing algorithm can still achieve around 80% efficiency, while an algorithm that is not grid aware drops to 26% or less.	algorithm;java;load balancing (computing);multiprocessing;network congestion;network performance;parallel computing;simulation;testbed;write once, run anywhere	Rob van Nieuwpoort;Jason Maassen;Thilo Kielmann;Henri E. Bal	2005	Scalable Computing: Practice and Experience	10.12694/scpe.v6i3.334	embedded system;real-time computing;engineering;distributed computing	HPC	-19.55193663766307	58.85803849776645	79933
84a837b5ac5ef13bc9d410e966c7b8151285ca89	cloudbft: elastic byzantine fault tolerance	relational database cloudbft elastic byzantine fault tolerance cloud computing computational resources outsourcing virtual machines vm physical machines pm cloud based fault tolerance schemes synchronization overheads communication overheads elastic critical services standard three tiered system relational data parallel accesses cloud based server;dependability distributed systems fault tolerant algorithms byzantine faults security;virtual machines cloud computing file servers relational databases software fault tolerance synchronisation;fault tolerant algorithms;dependability;byzantine faults;distributed systems;security;databases fault tolerance fault tolerant systems servers peer to peer computing virtual machining computer architecture	Cloud computing is increasingly important, with the industry moving towards outsourcing computational resources as a means to reduce investment and management costs, while improving security, dependability and performance. Cloud operators use multi-tenancy, by grouping virtual machines (VMs) into a few physical machines (PMs), to pool computing resources, thus offering elasticity to clients. Although cloud-based fault tolerance schemes impose communication and synchronization overheads, the cloud offers excellent facilities for critical applications, as it can host varying numbers of replicas in independent resources. Given these contradictory forces, determining whether the cloud can host elastic critical services is a major research question. We address this challenge from the perspective of a standard three-tiered system with relational data. We propose to tolerate Byzantine faults using groups of replicas placed on distinct physical machines, as a means to avoid exposing applications to correlated failures. To improve the scalability of our system, we divide data to enable parallel accesses. Using a realistic setup, this setting can reach speedups largely exceeding the number of partitions. Even for a wide variation of the load, the system preserves latency and throughput within reasonable bounds. We believe that the elasticity we observe demonstrates the feasibility of tolerating Byzantine faults in a cloud-based server using a relational database.	benchmark (computing);byzantine fault tolerance;cloud computing;computational resource;data item;data model;dependability;elasticity (cloud computing);elasticity (data store);experiment;ibm tivoli storage productivity center;multitenancy;outsourcing;pool (computer science);relational database;relational model;scalability;server (computing);speedup;synchronization (computer science);throughput;virtual machine	Rodrigo Nogueira;Filipe Araújo;Raul Barbosa	2014	2014 IEEE 20th Pacific Rim International Symposium on Dependable Computing	10.1109/PRDC.2014.31	reliability engineering;embedded system;real-time computing;cloud computing;computer science;information security;quantum byzantine agreement;operating system;dependability;distributed computing;byzantine fault tolerance;computer security;software fault tolerance;computer network	OS	-24.181257904820008	54.30949102688913	80103
f56ec0e68717a5e67ebb375be6b4013055b17f0c	integrity problems in distributed accounting systems with semantic acid properties	integrity problem;semantic acid property;accounting system	Many major companies have a physically distributed sales and/or production organization. In such an organization a distributed ERP (Enterprise Resource Planning) system may optimize performance and response time by storing data locally in the locations where they normally are used. If traditional ACID properties (Atomicity, Consistency, Isolation and Durability) are implemented in such a system, the availability of the system will be reduced, because often updating transactions can only be executed if they have access to both local and remote databases. This problem can be reduced by using only semantic ACID properties, i.e. from an application point of view, the system should function as if all the traditional ACID properties had been implemented. This paper illustrates how the integrity problems caused by using semantic ACID properties may be solved in distributed accounting systems. However, some of the techniques described in this paper may also be used to improve performance and availability in centralized accounting systems.	acid	Lars Frank	1999			reliability engineering;real-time computing;distributed computing	Networks	-23.92632624158119	50.09031302774295	80396
96a92893d92faa6ad5eab5b4376a58b11e9540c5	decomposed process mining with divideandconquer		Many known process mining techniques scale badly in the number of activities in an event log. Examples of such techniques include the ILP Miner and the standard replay, which also uses ILP techniques. To alleviate the problems these techniques face, we can decompose a large problem (with many activities) into a number of small problems (with few activities). Expectation is, that the run times of such a decomposed setting will be faster than the run time of the original setting. This paper presents the DivideAndConquer tool, which allows the user to decompose a large problem into small problems, to run the desired discovery or replay technique on each of these decomposed problems, and to merge the results into a single result, which can then be shown to the user.	run time (program lifecycle phase)	Eric Verbeek	2014			real-time computing;engineering;data mining;algorithm	Networks	-19.798515969184418	55.29219950497324	80500
33b1a62399c69a61d549b98e0bf52df5de412fae	distributed real-time nested transactions	nested transaction;real time	We present the concepts of real-time nested transactions and priority propagation for a distributed transaction processing environment. Real-time nested transactions incorporate the deadline requirements in the hierarchical structure of nested transactions. Priority propagation addresses issues related to transaction aborts in real-time nested transaction processing. The notion of priority ceiling has been used to avoid the priority inversion problem. The proposed protocols exhibit freedom from deadlock and have tightly bounded waiting periods. Both of these properties make them very suitable for distributed real-time transaction processing environments.	nested transaction;real-time clock	Waqar Hague;Johnny S. Wong	1994	Journal of Systems and Software	10.1016/0164-1212(94)90023-X	real-time computing;database transaction;transaction processing;distributed transaction;computer science;database;distributed computing;online transaction processing;compensating transaction;serializability;transaction processing system;priority ceiling protocol;nested transaction	Embedded	-23.72636512901701	47.39732374630132	80669
d30a946075cccf3b4c2e901567047a15461acbea	usgpa: a user-centric and secure grid portal architecture for high-performance computing	high performance computing portal security grid computing;grid portal;portals;access grid;portal;high performance computing;user centric grid portal architecture;user interfaces grid computing portals security of data software architecture;secure grid portal architecture;portals computer architecture grid computing application software data security biology computing computer networks supercomputers concurrent computing distributed computing;usgpa;access grid systems;computer architecture;software architecture;servers;cryptography;high performance computer;security portlet model;security portlet model usgpa user centric grid portal architecture secure grid portal architecture high performance computing access grid systems;security;grid computing;user interfaces;security of data;data models	a grid portal is one of the most important ways to access grid systems. The disadvantages existing in current grid portals are that they are designed for specific applications, difficult to add new applications. Besides, the security issues are not considered fully in these portals. In this paper, we proposed a user-centric and secure grid portal architecture-USGPA based on portlet. In USGPA, a security portlet model is proposed in which security issues for sensitive data and the portal server are solved. Based on the model, the application portlet model is proposed with which new applications can be encapsulated into portal quickly. Then all portlets for high-performance computing-HPC are designed based on these two models. With these portlets, users can not only manage jobs via portal but also can custom the portal by selecting and managing the applications they required. Last but important, a prototype is implemented to evaluate the scalability, security and efficiency of the architecture.	access grid;ajax (programming);data security;desktop computer;out-of-order execution;portals;portlet;prototype;scalability;server (computing);supercomputer;usability	Rongqiang Cao;Xuebin Chi;Zongyan Cao;Zhihui Dai;Haili Xiao	2009	2009 IEEE International Symposium on Parallel and Distributed Processing with Applications	10.1109/ISPA.2009.90	software architecture;enterprise portal;computer science;cryptography;information security;operating system;database;world wide web;grid computing	HPC	-33.2426702765602	53.83472285252584	80690
7cb19d4498f98d8efb73289ca6f339fa7701788a	brief announcement: network traffic can optimize consolidation during transformation to virtualization	virtualization;consolidation;network traffic;network dimensioning;optimization	Under the consolidation scenario in Clouds, the network dimension should be considered as important as the computing power of machines. Traditional consolidation procedure is usually made according to the experience, which mainly focused on the hardware capability of the target system, like CPU, Memory and etc. Along with the consolidation of the computing power, the network communication among machines is also consolidated. The consolidation procedure needs to cover this change and avoid network problems after moving the applications into target virtualized system. This paper presents a novel approach to provide optimization taking network traffic into account during consolidation.	central processing unit;mathematical optimization;network traffic control;semiconductor consolidation	Kewei Sun;Ying Li;Jing Luo	2010		10.1145/1835698.1835764	consolidation;embedded system;real-time computing;virtualization;simulation;computer science;operating system	HPC	-20.198758660567755	60.201843165485144	80741
26e20530ceff30d0d1816098556d199ade3887a0	managing large-scale workflow execution from resource provisioning to provenance tracking: the cybershake example	southern california earthquake center;software tool;resource management;collaboration;hazards;earthquakes;large scale;geophysics computing;geology;resource management large scale systems earthquakes buildings hazards production collaboration geophysics computing environmental management geology;production;resource availability;seismic hazard;environmental management;buildings;large scale systems;los angeles	This paper discusses the process of building an environment where large-scale, complex, scientific analysis can be scheduled onto a heterogeneous collection of computational and storage resources. The example application is the Southern California Earthquake Center (SCEC) CyberShake project, an analysis designed to compute probabilistic seismic hazard curves for sites in the Los Angeles area. We explain which software tools were used to build to the system, describe their functionality and interactions. We show the results of running the CyberShake analysis that included over 250,000 jobs using resources available through SCEC and the TeraGrid.	computation;interaction;provisioning;teragrid	Ewa Deelman;Scott Callaghan;Edward H Field;Hunter Francoeur;Robert Graves;Nitin Gupta;Vipin Gupta;Thomas H. Jordan;Carl Kesselman;Philip Maechling;John Mehringer;Gaurang Mehta;David Okaya;Karan Vahi;Lu Zhao	2006	2006 Second IEEE International Conference on e-Science and Grid Computing (e-Science'06)	10.1109/E-SCIENCE.2006.99	simulation;hazard;resource management;seismic hazard;collaboration	HPC	-29.86075106631501	51.35743992728484	80766
a0541db5a6098f8020a83d299b591769233af247	cloud computing platform within grid infrastructure	virtual enterprises cloud computing grid computing;virtual enterprises;cloud computing platform distributed heterogeneous computing resources large scale calculations data processing remote computers resource merging resource sharing european grid infrastructure ukrainian national grid cloud in grid approach grid sites multiple virtual organizations application deployment acceleration computing as a service efficient hardware utilization manageability flexibility collaboration groups large hadron collider experiments;virtualization grid computing cloud computing htc distributed computing;grid computing;cloud computing virtual machining hardware large hadron collider organizations europe;cloud computing	Both grid and cloud are used to organize large scale calculations and data processing on remote computers. Grid which became a basic computing infrastructure for the Large Hadron Collider experiments provides unified technical solutions for sharing and merging distributed heterogeneous computing resources within big collaboration groups. Cloud became popular among data centers and computing service providers because of flexibility, manageability and efficient hardware utilization. Both approaches share common ideology “computing as a service”, so one can expect additional benefits from their integration. The paper describes our approach to the integration. We propose to use cloud within grid sites for acceleration of application deployment and easy support of multiple virtual organizations by grid sites. The cloud in grid approach has been implemented and tested in Ukrainian National Grid, a part of European Grid Infrastructure.	cloud computing;computer;data center;egi;experiment;heterogeneous computing;large hadron collider;software deployment	Vladislav Falfushinsky;Olena Skarlat;Vadim Tulchinsky	2013	2013 IEEE 7th International Conference on Intelligent Data Acquisition and Advanced Computing Systems (IDAACS)	10.1109/IDAACS.2013.6663018	fabric computing;cloud computing security;cloud computing;semantic grid;computer science;operating system;cloud testing;database;distributed computing;utility computing;world wide web;drmaa;grid computing	HPC	-30.581773743511242	52.262218942302745	81054
dfc9b8c44604a33247dce55864c54fffa348959d	editing any version at any time: a consistency maintenance mechanism in internet-based collaborative environments	groupware;document handling;collaborative work;data integrity;prototypes;real time;collaboration;collaborative editing;collaborative environment;internet computing;internet;data integrity groupware internet real time systems java text editing document handling;internet collaboration prototypes collaborative work australia real time systems java collaborative software delay humans;polo consistency maintenance mechanism multi version approach internet based real time collaborative editing systems user intentions convergent document states java;humans;consistency maintenance;text editing;australia;collaborative software;java;real time systems	This paper investigates the multi-version approach to consistency maintenance in Internet-based real-time collaborative editing systems. It proposes a new multi-versioning scheme that is able to preserve individual users' intentions while guaranteeing convergent document states. The scheme has been implemented in Java in a prototype called POLO.	internet	Liyin Xue;Mehmet A. Orgun;Kang Zhang	2002		10.1109/ICPADS.2002.1183380	computer science;operating system;database;distributed computing;multimedia;world wide web;collaborative software	AI	-25.712027516490288	47.4712669615233	81447
00f367e95e8adfebb2e04e68bed24e56736bfb6f	out-of-band detection of boot-sequence termination events	virtual machine;out of band;virtualization;boot detection;file system	The popularization of both virtualization and CDP technologies mean that we can now watch disk accesses of systems from entities which are not controlled by the OS. This is a rich source of information about the system's inner workings. In this paper, we explore one way of mining the stream of data, to determine if the system had finished booting. Systems which we detect as failing to boot (or taking too long to boot) are flagged for further manual or automatic remediation. By performing this detection out-of-band, we gain a head start on any detection scheme that runs within the OS, and therefore must wait for the boot event to finish. Additionally, our scheme is agnostic to file-system layout and to kernel architecture.  We implemented our solution for the x86 architecture under two different virtualization platforms, and tested it on both Windows and Linux virtual machines. Under a variety of workloads and configurations, our detector managed to successfully identify the boot termination event, in most cases within 5 seconds of the event.	booting;entity;failure;hardware virtualization;information source;linux;microsoft windows;operating system;out-of-band agreement;virtual machine;x86	Naama Parush;Dan Pelleg;Muli Ben-Yehuda;Paula Ta-Shma	2009		10.1145/1555228.1555251	out-of-band management;embedded system;real-time computing;virtualization;computer science;virtual machine;operating system;database;distributed computing;volume boot record;computer security;computer network	OS	-24.16914908857061	55.818448796697126	81500
f7c95d2287a32e9e462f685744c1dbd1f46a6261	enabling advanced loading strategies for data intensive web services	data intensive services;loading business servers semantics simple object access protocol;adaptation web services performance middleware data intensive services;performance;semantics;loading;servers;adaptation;business;web services;middleware;simple object access protocol	Improving performance of Web services interactions is an important factor to burst the adoption of SOAin mission-critical applications, especially when they deal with large business objects whose transfer time is not negligible. Designing messages dynamic granularity (offloading) is a key challenge for achieving good performances. This requires the server being able to predict the pieces of data actually used by clients in order to send only such data. However, exact prediction is not easy, and consequently lazy interactions are needed to transfer additional data whenever the prediction fails. To preserve semantics, lazy accesses to the results of a Web service interaction need to work on a dedicated copy of the business object stored as application state. Thus, dynamic offloading can experience an overhead due to a prediction failure, which is the sum of round-trip and storage access delays, which could compromise the benefits of the technique. This paper improves our previous work enabling dynamic offloading for both IN and OUT parameters, and analyses how attributes copies impact on the technique, by comparing the overheads introduced by different storage technologies in a real implementation of a Web services framework that extends CXF. More specifically, we quantitatively characterize the execution contexts that make dynamic offloading effective, and the expected accuracy of the predictive strategy to have a gain in term of response time compared to plain services invocations. Finally, the paper introduces the Attribute Loading Delegation technique that enables optimized data-transfers for those applications where data-intensive multiple-interactions take place.	apache cxf;application programming interface;business object;coupling (computer programming);data-intensive computing;interaction;lazy evaluation;mathematical optimization;middleware;mission critical;network topology;overhead (computing);overlay network;play;performance;response time (technology);scalability;server (computing);service-orientation;service-oriented device architecture;software deployment;state (computer science);web application;web service	Quirino Zagarese;Gerardo Canfora;Eugenio Zimeo;Françoise Baude	2012	2012 IEEE 19th International Conference on Web Services	10.1109/ICWS.2012.30	web service;real-time computing;performance;computer science;operating system;soap;middleware;database;semantics;services computing;world wide web;server;adaptation	HPC	-25.33585888564723	55.178791834350854	81984
fd037539a42d7a243305a87ad7c19baf76276a44	beeflow: a workflow management system for in situ processing across hpc and cloud systems		In this paper, we propose BeeFlow – an in situ analysis enabled workflow management system across multiple platforms using Docker containers. BeeFlow can support both traditional workflows as well as workflows with in situ analysis. BeeFlow leverages Docker containers to provide a portable, flexible, and reproducible workflow management system across HPC and cloud platforms. We showcase how current in situ visualization workflows can apply BeeFlow with DOE production codes VPIC and Flecsale.	code;docker;management system;usability	Jieyang Chen;Qiang Guan;Zhao Zhang;Xin Liang;Louis James Vernon;Allen McPherson;Li-Ta Lo;Patricia Grubel;Tim Randles;Zizhong Chen;James P. Ahrens	2018	2018 IEEE 38th International Conference on Distributed Computing Systems (ICDCS)	10.1109/ICDCS.2018.00103	task analysis;database;workflow management system;visualization;distributed computing;cloud computing;computer science;workflow;data modeling;in situ	HPC	-28.892797514615577	53.21838049223075	82209
0c579b8b2224b802bacbcd0bf4291ebbe453a972	composition of a dids by integrating heterogeneous idss on grids	grid middleware;system integration;intrusion detection systems;grids;systems integration;intrusion detection system	This paper considers the composition of a DIDS (Distributed Intrusion Detection System) by integrating heterogeneous IDSs (Intrusion Detection Systems). A Grid middleware is used for this integration. In addition, an architecture for this integration is proposed and validated through simulation.	intelligent decision support system;intrusion detection system;middleware;simulation	Paulo F. Silva;Carlos Becker Westphall;Carla Merkle Westphall;Marcos Dias de Assunção	2006		10.1145/1186675.1186688	intrusion detection system;real-time computing;computer science;operating system;database;computer security;system integration	ML	-31.13728252231854	47.02677947089464	82232
ef4661bd3f916130e2b258d0f79f97d1330e6600	anomaly detection in clouds: challenges and practice		Cloud computing is an important infrastructure for many enterprises. After 10 years of development, cloud computing has achieved a great success, and has greatly changed the economy, society, science and industries. In particular, with the rapid development of mobile Internet and big data technology, almost all of the online services and data services are built on the top of cloud computing, such as the online banking services provided by banks, the electronic services provided by the news media, the government cloud information systems provided by the government departments, the mobile services provided by the communications companies. Besides, tens of thousands of Start-ups rely on the provision of cloud computing services. Therefore, ensuring cloud reliability is very important and essential. However, the reality is that the current cloud systems are not reliable enough.  On February 28th 2017, Amazon Web Services, the popular storage and hosting platform used by a huge range of companies, experienced S3 service interruption for 4 hours in the Northern Virginia (US-EAST-1) Region, and then quickly spread other online service providers who rely on the S3 service [2]. This failure caused a huge economic loss. It is because cloud computing service providers typically set a Service Level Agreement (SLA) with customers. For example, when customers require 99.99% availability, it means that 99.99% of the time must meet the requirement for 365 days per year. If the service breaks more than 0.01%, compensation is required.  In fact, with the continuous development and maturity of cloud computing, a large number of traditional business systems have been deployed on the cloud platform. Cloud computing integrates existing hardware resources through virtualization technology to create a shared resource pool that enables applications to obtain computing, storage, and network resources on demand, effectively enhancing the scalability and resource utilization of traditional IT infrastructures and significantly reducing the operation cost of the traditional business systems. However, with the growing number of applications running on the cloud, the scale of cloud data center has been expanding, the current cloud computing system has become very complex, mainly reflected in: 1) Large scale. A typical data center involves more than 100,000 servers and 10,000 switches, more nodes usually mean higher probability of failure; 2) Complex application structure. Web search, e-commerce and other typical cloud program has a complex interactive behavior. For example, an Amazon page request involves interaction with hundreds of components [7], error in any one component will lead to the whole application anomalies; 3) Shared resource pattern. One of the basic features of cloud computing is resource sharing, a typical server in Google Cloud data center hosts 5 to 18 applications simultaneously, each server runs about 10.69 applications [5]. Resource competition will interfere with each other and affect application performance. The complexity of these cloud computing systems, the complexity of application interaction structure and the inherent sharing pattern of cloud platforms make cloud systems more prone to performance anomalies than traditional platforms. It can be said that anomaly is a normal state in cloud computing [3].  For further analysis, resource competition, resource bottlenecks, misconfiguration, software defects, hardware failures, and external attacks can cause cloud system anomalies or failures. Performance anomaly refers to any sudden degradation of performance that deviates from the normal behavior of the system. Unlike outages that cause the system to stop running immediately, performance anomalies typically result in a decrease in system efficiency. The reasons such as misconfiguration, software defects, hardware failures, can often cause performance anomalies. For cloud computing systems, it is not enough to detect outages or other functional anomalies, because those anomalies often cause service interruption and can be resolved by simply restarting or replacing hardware. While performance anomalies caused by resource sharing and interference are more worthy of attention [4], because the performance anomalies can be eliminated before service interruption to ensure continued services.  If the performance anomalies of cloud computing system are not timely handled, it may cause very serious consequences, which not only affect the business system to run normally, but also hinder the enterprise to deploy their services on cloud systems. Especially for the those latency-sensitive cloud applications, it is extremely important to eliminate performance anomalies in a timely manner. For example, Amazon found a 1% decline in sales per 100ms latency, Google found a 20% drop in traffic for every 0.5s latency in search page, and stock traders found that it would cause a loss of 400 Million dollars if their electronic trading platform lagged behind the competitors by 5 ms. Other research also shows that the average maximum time of cloud data center failure is about 100 hours, which seriously affects the experience of cloud service users. In the cloud environment, as a large number of business systems are deployed in the cloud data center, cloud data center failure will affect a large number of users, such as the previously mentioned Amazon S3 failure, resulting in serious economic losses.  Thus, timely and accurate detection of the cloud computing anomalies is very important. Anomaly detection is an effective means to help cloud platform administrators monitor and analyze cloud behaviors and improve cloud reliability. It helps to identify unusual behavior of the system so that cloud platform administrators can take proactive operations before a system crash or service failure. However, due to the characteristics such as large-scale, complex and resource sharing, it is very difficult to accurately detect anomalies in cloud computing. If the anomalies can not be accurately detected, the further recovery will be out of the question. Due to the importance of the problem, current mainstream cloud computing service providers usually provide online monitoring services.  Amazon developed CloudWatch [1] for its EC2 users to monitor virtual machine instance status, resource usage, network traffic, disk read and write status, etc. Google developed Dapper framework [6] to provide state monitoring for Google search engines. But those monitoring services only provide simple data presentation, lack of in-depth analysis of monitoring data (such as cross-level correlation analysis), and is not intelligent enough for anomaly reasoning (such as cross-node fault source localization). In cloud data center, as the size of detected objects is very large and interrelated, the object being detected itself is in a high dynamic environment, it is very challenging to detect anomalies in an accurate, real-time and adaptive way. The existing anomaly detection solution lacks effective discovery and reasoning of the anomaly, which leads to the inability to locate and eliminate the anomaly in time. This is also the main reason that causes the current cloud platform accident frequently.  In this talk, we introduce our solution for anomaly detection in clouds.  1) Anomaly detection. In order to efficiently detect the potential anomalies, we perform large-scale offline performance testing and also create an online detection method. i) Offline testing. The purpose is to find the key performance bottleneck and quantify comparison between difference hardware and software. We first propose a three-layer benchmarking methodology to fully evaluates cloud performance and then present a new benchmark suite -- Virt-B [11] -- that measures various scenarios, such as single machine virtualization, server consolidation, VM mapping, VM live migration, HPC virtual clusters and Hadoop virtual cluster. Finally, we introduce a performance testing toolkit to automate the benchmarking process. ii) Online detection. The purpose is to monitor applications in real time and quickly detect potential faults. We propose a quantile regression based online anomaly detection method and did a case study on 67 real Yahoo! anomaly traffic datasets.  2) Anomaly inference. We propose a dependency graph based anomaly inference method. Dependency reflects interaction relationship and execution path, and can be used for fault localization. There are usually three methods that can be used to fetch the dependency graph: instrumentation, extract configuration files and analyze network traffic. We create light-weight agents to monitor the traffic and use sampling technique to reduce the overheads. The advantages of our solution include: supports VMs (Xen/KVM), accuracy guarantee based on probability theory, dynamic dependency construction, focuses on the PM/VM layer, and low overheads.  3) Anomaly recovery. The traditional recovery methods like Checkpoint/Restart has high overheads and are not suitable for latency-sensitive applications. While we propose two solutions: cache-aware fault VM isolation and migration-based fault recovery.  i) Cash-Aware Fault Isolation [8]. We first give a quantitative definition of isolation, then we propose a VM-core scheduling method to improve the fault isolation. ii) Fault Recovery based on Migration [9, 10, 12]. We propose a fault recovery method based on live migration. The main advantages include: online service, low overheads, and can be used in large-scale cloud datacenters.	amazon simple storage service;amazon web services;anomaly detection;apache hadoop;application checkpointing;benchmark (computing);big data;bottleneck (software);cpu cache;capability maturity model;cloud computing;dapper;data center;e-commerce;e-services;electronic trading;elegant degradation;fault detection and isolation;google cloud messaging;google search;information system;interference (communication);interrupt;multitier architecture;network switch;network traffic control;online and offline;online banking;online service provider;openvms;real-time clock;sampling (signal processing);scalability;scheduling (computing);semiconductor consolidation;server (computing);service-level agreement;software bug;software performance testing;traders;virtual machine;web search engine;web service;x86 virtualization	Kejiang Ye	2017		10.1145/3129457.3129497	scalability;real-time computing;anomaly detection;service-level agreement;computer science;big data;cloud computing;data center;data as a service;government	Metrics	-24.333139677589983	59.17184573281851	82253
3e6990ce9a37eba58832a0a0df67ab2240bc94ac	shaft: supporting transactions with serializability and fault-tolerance in highly-available datastores	high availability;shafts fault tolerance fault tolerant systems protocols proposals partitioning algorithms distributed databases;isolation;storage management data handling fault tolerant computing protocols;transaction;concurrency control;two phase locking procedure shaft fault tolerant property transaction semantics datastores transactional replication protocol paxos algorithm serializability;isolation concurrency control transaction high availability consistency;consistency	Guaranteeing transaction semantics in a highly available and fault tolerant manner is desirable to application developers. Besides, it is a very valuable feature for database-backed applications. In this paper, we propose SHAFT to support transactions with serializability in highly-available datastores, which partition, distribute and replicate data across datacenters. SHAFT is a transactional replication protocol guaranteeing Serializability, High Availability and Fault Tolerance simultaneously for transactions. Laying its basis on the Paxos algorithm, SHAFT guarantees serializability by a two-phase locking procedure in a fault-tolerant manner. Different from other transactional replication protocols like MDCC, SHAFT allows a client to actively abort a transaction. SHAFT also allows flexible data partition, replication and distribution, a proper combination of which can reduce costs and improve performance. SHAFT performs well even under failures. Our experiments show that SHAFT outperforms MDCC, which outperforms other synchronous transactional replication protocols, e.g. Megastore.	algorithm;blocking (computing);commitment scheme;concurrency (computer science);concurrency control;experiment;fault tolerance;high availability;lock (computer science);non-blocking algorithm;paxos (computer science);replication (computing);self-replicating machine;serializability;server (computing);transaction processing;two-phase commit protocol;two-phase locking	Yuqing Zhu;Yilei Wang	2015	2015 IEEE 21st International Conference on Parallel and Distributed Systems (ICPADS)	10.1109/ICPADS.2015.95	global serializability;parallel computing;real-time computing;isolation;commitment ordering;distributed transaction;computer science;two-phase locking;concurrency control;database;distributed computing;high availability;consistency;serializability	DB	-22.301958215239818	48.714420024539535	82323
4a89b2b1d99293ebc4b0998fd107e186f35aedae	the challenges and opportunities in transparent computing	software;communication networks;sequentially virtual computing;difference operator;software resources;neumann architecture;storage servers;hardware resources;computer architectures;computer networks;software programs;computer network;transparent computing;computer architecture;servers;operating system;service sharing;operating system technologies;community networks;operating systems computers computer architecture;sequentially virtual computing transparent computing service sharing computer networks hardware resources software resources operating system technologies computer architectures software programs storage servers execution servers meta os neumann architecture;operating systems embedded computing pervasive computing computer networks computer science education hardware computer architecture network servers ubiquitous computing communication system software;operating systems computers;execution servers;meta os;conferences;operating systems;hardware	In this talk, we present some new research results about our Transparent Computing which is a new computing paradigm for service sharing. Rapid development in computer networks let people easily share the hardware and software resources, however, because the slow progress in Operating system technologies and few changes in computer architectures in several decades, it is still not easy to share services from different software and hardware platforms for users. Transparent Computing paradigm separates the storages and executions of software programs and data including what of operating systems in different computers connecting with communication networks. In this paradigm, data and programs are stored in the storage servers and they are executed in either clients or execution servers according to the required services, respectively. We propose a new mechanism called Meta OS to manage this separation. This mechanism extends the Neumann architecture special-temporally, so that a sequentially virtual computing can be effectively obtained and the individual service sharing can be easily performed underlying different Operating System platforms. We will introduce the idea, the mechanism and algorithms of proposed Meta OS for the separation of the programs and data including what of Operating Systems. Moreover, we also present some examples about the applications of this computing paradigm.	algorithm;computer architecture;operating system;programming paradigm;telecommunications network;transparency (human–computer interaction)	Yaoxue Zhang	2008	2008 IEEE/IFIP International Conference on Embedded and Ubiquitous Computing	10.1109/EUC.2008.188	embedded system;real-time computing;computer science;operating system;distributed computing;utility computing;server;computer network	HPC	-28.946169634965315	55.0130085089453	82734
68f79a7f63b0742a459ffcad563be3d2b3f85719	accelerating critical os services in virtualized systems with flexible micro-sliced cores		Consolidating multiple virtual machines into a single server has been widely adopted in cloud computing to improve system utilization. However, the sharing of physical CPUs among virtual machines in consolidated systems poses a new challenge in providing an illusion of continuous CPU execution to the guest operating systems (OS). Due to the time-sharing of physical CPUs, the execution of a guest OS is periodically interrupted, while the guest OS may not be aware of the discontinuity of virtual time against the real time. The virtual time discontinuity problem causes the delayed processing of critical OS operations, such as interrupt handling and lock processing. Although there have been several prior studies to mitigate the problem, they address only a subset of symptoms, require the modification of guest OSes, or change the processor architecture. This paper proposes a novel way to comprehensively reduce the inefficiency of guest OS execution in consolidated systems. It migrates the short-lived critical OS tasks to dedicated micro-sliced cores, minimizing the delays caused by time sharing. The hypervisor identifies the critical OS tasks without any OS intervention, and schedules the critical code sections onto the dynamically partitioned cores at runtime. The dedicated micro-sliced cores employ a short sub-millisecond quantum to minimize the response latencies for consolidated virtual machines. By readily servicing the critical tasks, the proposed scheme can minimize the adverse artifact of virtual machine consolidation without any modification of guest OSes.	central processing unit;cloud computing;hypervisor;interrupt;operating system;pointer (computer programming);program counter;reflections of signals on conducting lines;run time (program lifecycle phase);semiconductor consolidation;server (computing);time-sharing;virtual machine	Jeongseob Ahn;Chang Hyun Park;Taekyung Heo;Jaehyuk Huh	2018		10.1145/3190508.3190521	interrupt;computer science;virtualization;lock (computer science);real-time computing;cloud computing;hypervisor;scheduling (computing);virtual machine;microarchitecture	OS	-21.839257998435812	55.16777402822386	82826
5e598b5fd97d4f84eb9be1e5a4f4c89aec5c8b41	unification of replication and transaction processing in three-tier architectures	high availability;database system;replication;fault tolerant;application software;coordination failure;availability;software infrastructure;database servers;logic;application server;database servers software infrastructure replication transaction processing fault tolerance corba application servers business logic processing;corba;network servers distributed object management fault tolerant computing transaction processing replicated databases;computer architecture;protection;network servers;fault tolerant computing;commercial off the shelf;transaction databases;fault tolerant systems;application software computer architecture transaction databases availability fault tolerance fault tolerant systems database systems protection business logic;fault tolerance;database systems;business;distributed object management;object transaction service;application servers;transaction processing;business logic processing;replicated databases;logical process	In this paper we describe a software infrastructure that unifies replication and transaction processing in three-tier architectures and, thus, provides high availability and fault tolerance for enterprise applications. The infrastructure is based on the Fault Tolerant CORBA and CORBA Object Transaction Service standards, and works with commercialoff-the-shelf application servers and database systems. The infrastructure replicates the application servers to protect the business logic processing. In addition, it replicates the transaction coordinator, which renders the twophase commit protocol non-blocking and, thus, avoids potentially long service disruptions caused by coordinator failure. The infrastructure handles the interactions between the application servers and the database servers through replicated gateways that prevent duplicate requests from reaching the database servers. The infrastructure implements client-side automatic failover mechanisms, which guarantees that clients know the outcome of the requests that they have made. The infrastructure starts the transactions at the application servers, and retries aborted transactions, caused by process or communication failures, automatically on the behalf of the clients.	application server;blocking (computing);business logic;client–server model;common object request broker architecture;database server;distributed transaction;enterprise software;failover;fault tolerance;han unification;high availability;interaction;multitier architecture;non-blocking algorithm;rendering (computer graphics);retry;transaction processing	Wenbing Zhao;Louise E. Moser;P. M. Melliar-Smith	2002		10.1109/ICDCS.2002.1022266	fault tolerance;real-time computing;distributed transaction;computer science;operating system;database;distributed computing;online transaction processing;converged infrastructure;application server;server;computer network	DB	-26.52416868258849	46.48382151774551	83082
9dcd04151a03418546cf77775f4a5df14881d71b	using multicast communication to reduce deadlock in replicated databases	multicast communication;probability;broadcasting replicated databases concurrency control multicast communication probability;replica update multicast communication deadlock reduction replicated databases distributed replicated database update transactions one copy serializability deadlock probabilities replica management protocol atomic broadcast;multicast communication system recovery transaction databases distributed databases broadcasting multicast protocols computer science analytical models predictive models distributed computing;concurrency control;atomic broadcast;broadcasting;replicated databases	Obtaining good performance from a distributed repli-cated database that allows update transactions to originate at any site while ensuring one-copy serializability is a challenge. A popular analysis of deadlock probabilities in replicated databases shows that the deadlock rate for the system is high and increases as the third power of the number of replicas. We show how a replica management protocol that uses atomic broadcast for replica update reduces the occurrence of deadlocks and the dependency on the number of replicas. The analysis is connrmed by simulation experiments.	atomic broadcast;database;deadlock;experiment;multicast;serializability;simulation	JoAnne Holliday;Divyakant Agrawal;Amr El Abbadi	2000		10.1109/RELDI.2000.885407	multicast;atomic broadcast;computer science;concurrency control;probability;database;distributed computing;source-specific multicast;broadcasting;xcast;statistics;computer network	OS	-22.19040865149144	49.046344028025594	83096
25229a5a6aaadcf7047cbafcd8d57c45a3ee6bfd	query execution optimization based on incremental update in database distributed middleware		Big data is often generated incrementally in the real word. Existing incremental query optimization is mainly used in the streaming data environment. Due to the constraints of real-time streaming data applications, existing incremental execution mechanisms is difficult to directly apply to large business-oriented data in a distributed environment. This paper proposes a query execution optimization method based on incremental update in database distributed middleware. First, the proposed method defines the Reference-Graph according to tables and their foreign key relationships, based on which a data partition strategy is provided to reduce data transmission quantity during query operation. In addition, the proposed method proposes an incremental update query execution strategy and incremental intermediate result preservation mechanism in distributed environment for non-aggregate and aggregate query respectively. The combination of data partition and incremental updating strategy reduces the query execution cost and enhance the performance of complex query operation significantly. Finally, the experimental results conducted on the benchmark dataset test and verify the effectiveness of the proposed method.	middleware;windows update	Wei Ye;Mei Wang;Jiajin Le	2015		10.1007/978-3-319-27119-4_18	sargable;middleware;query optimization;middleware;database;distributed computing;programming language;view	DB	-19.25754977475854	54.25074923220492	83138
b0a534cbb6acde056a4d2e0199626f3de35faac3	an open-source azure solution for scalable genomics workflows		We present an open-source Azure solution for running scalable genomics workflows. It benefits from state-of-art distributed workflow framework, container and cloud technologies and allows users to create a cluster that is scaled to suit their workload in minutes. We describe the design decisions, solution testing and automation options to support a variety of users for their genomic data analytics. The solution demonstrates a generic and customizable approach to run genomic data analytics workflows on a cloud environment.	cloud computing;microsoft azure;open-source software;scalability	Fan Yang-Turner;Lawrence Gripper;Jeremy Swann;T Do;Dona Foster;Denis Volk;Anita Ramanan;Marcus Robinson;Tim E. A. Peto;Derrick W. Crook	2018	2018 IEEE World Congress on Services (SERVICES)	10.1109/SERVICES.2018.00033	genomics;automation;data mining;database;workload;computer science;scalability;data analysis;cloud computing;workflow	Visualization	-28.615881871958788	53.694077300846274	83210
0909d173bdacb6a61a896e48cbd4fde715fc82d6	towards online schedulers based on pre-analysis locking	phase locking;online scheduling;concurrency control	Locking is one familiar mechanism for a database concurrency control to achieve safe transaction systems. Pre-Analysis Locking bases on an algorithm which analyses a set of transactions to determine the conflicting actions. Different to known locking policies, e.g. 2-Phase Locking, the position of lock operations depends only on the location of the conflicting actions within the transactions. Therefore, depending on the structure of the transactions, Pre-Analysis Locking allows for a higher potential degree of concurrency than 2-Phase Locking. Until now Pre-Analysis Locking requires the knowledge of the complete set of transactions to be executed in advance. In this paper, iterative Pre-Analysis Locking is proposed, which manages the case of previously unknown transactions. Further, dynamic Pre-Analysis Locking is introduced, which additionally is able to forget finished transactions and therefore is an appealing new approach to online scheduling. Safety of the policies is proven and the issue of deadlock freedom is discussed.	lock (computer science)	Georg Lausen;Eljas Soisalon-Soininen;Peter Widmayer	1986		10.1007/3-540-17187-8_40	parallel computing;real-time computing;computer science;concurrency control;database;distributed computing;non-lock concurrency control	DB	-22.440729032314437	47.47405728935025	83266
5f7655007da85940d0c437ed84532fde1b0b7043	permutation clustering: an approach to on-line storage reorganization	quasistatic organization;exhibit improved paging performance;on-line storage reorganization;dynamic reorganization algorithm;desirable systems property;permutation clustering;large data base application	A class of dynamic reorganization algorithms is described which embodies a number of desirable systems properties. Experiments on a trace taken from a large data base application indicate that a member of this class may be used to obtain time-varying or quasistatic organizations that exhibit improved paging performance.		Brian T. Bennet;Peter A. Franaszek	1977	IBM Journal of Research and Development	10.1147/rd.216.0528	computer science;theoretical computer science;distributed computing;algorithm	Theory	-19.120899065289343	47.48894346040073	83657
6b6efd93f5248e2cb0677fc61168db637d94c6d6	virtualization: virtually at the desktop	resource utilization;virtualization;academic computing;maintenance cost;higher education institution;development tool;desktop virtualization;operating system;lessons learned;funding;software package;power consumption;fault isolation;high performance;off the shelf;notebook computer	We have witnessed low resource utilization of high performance graphics workstations in our instructional computer laboratories. The low utilization statistics indicate that workstation consolidation could achieve great savings in infrastructure, networking, power consumption, and maintenance costs. In addition, we would spend less time in deployment, security, and fault isolation without compromising performance.  The basic enabler for workstation consolidation in our instructional computing environment is the ability to allow multiple separate operating system instances and associated software packages to share a single hardware server. We have successfully utilized existing off the shelf products and developed tools and protocols to migrate processing tasks from the desktop level to the virtual desktop level running on remote hardware and returning the processing results back to the desktop level for display. Since all processing is done at the server level, we no longer need high performance graphics workstation class machines at the desktop. This allows us to offer high performance graphics workstation capabilities to any desktop, including lower-end commodity class desktop machines, notebook computers, or even thin-clients.  While server consolidation through virtualization is not new, desktop workstation virtualization seemed a natural and novel extension of the server virtualization framework. Indeed, the general trend is towards applying virtualization techniques to almost all Information Technology infrastructure machinery, and we should expect to see more virtualization, virtually everywhere in higher education institutions.  In this report, we will present our approach, framework, implementation challenges, lessons learned and next steps.	computer lab;desktop computer;fault detection and isolation;graphics;hardware virtualization;laptop;operating system;semiconductor consolidation;server (computing);software deployment;virtual desktop;virtual private server;workstation	Karissa Miller;Mahmoud Pegah	2007		10.1145/1294046.1294107	embedded system;full virtualization;real-time computing;virtualization;desktop management interface;computer science;operating system;hardware virtualization;virtual desktop	OS	-27.431532902688428	52.821524298025984	83676
719ccc756bb4e90a935d35976c5f672133e9bdd7	scalable real-time visualization using the cloud	cloud based interactive visualization scalable real time visualization big data visualization cloud computing computationally intensive task scalable computational power usage patterns graphics hardware software service software architectures;cloud;information visualization;cloud computing data visualization rendering computer graphics computer architecture graphics processing units scalability big data;scalable systems;computer architecture;big data cloud information visualization cloud computing scalable systems;big data;graphics processing units;data visualization;scalability;interactive systems big data cloud computing data visualisation;rendering computer graphics;cloud computing	Visualizing big data is a computationally intensive task that most users undertake on an exploratory basis. The need for scalable computational power coupled with varied usage patterns makes it a strong candidate for cloud computing. With the recent availability of graphics hardware as infrastructure in the cloud it is now realistic to execute interactive visualization remotely as a software service. The authors review software architectures that can deliver cloud-based interactive visualization, highlight the key systems challenges to support this, and predict how scalable computing power in the cloud will make transformative changes to the way visualization is implemented and delivered to users.	big data;cloud computing;graphics hardware;interactive visualization;real-time transcription;scalability;software as a service	Nicolas S. Holliman;Paul Watson	2015	IEEE Cloud Computing	10.1109/MCC.2015.131	information visualization;big data;interactive visualization;cloud computing;computer science;theoretical computer science;operating system;cloud testing;world wide web;data visualization;computer graphics (images)	Visualization	-28.491539482973135	52.96657911337045	83762
ccf8eb4547f4d102b501fc3fdc05e3074833502d	a failure index for hpc applications	system volatility;high performance computing;inequality measures;adequate level of performance;resilience;failure index fi	This paper conducts an examination of log files originating from High Performance Computing (HPC) applications with known reliability problems. The results of this study further the maturation and adoption of meaningful metrics representing HPC system and application failure characteristics. Quantifiable metrics representing the reliability of HPC applications are foundational for building an application resilience methodology critical in the realization of exascale supercomputing. In this examination, statistical inequality methods originating from the study of economics are applied to health and status information contained in HPC application log files. The main result is the derivation of a new failure index metric for HPC-a normalized representation of parallel application volatility and/or resiliency to complement existing reliability metrics such as mean time between failure (MTBF), which aims for a better presentation of HPC application resilience. This paper provides an introduction to a Failure Index (FI) for HPC reliability and takes the reader through a use-case wherein the FI is used to expose various run-time fluctuations in the failure rate of applications running on a collection of HPC platforms. A novel metric called Failure Index (FI) that can be used in the evaluation of High Performance Computing failure or resilience information.Modeling resource allocation schemes leveraging batches and queues and a history of successful and unsuccessful jobs.Index estimates used to construct a reliability-aware metascheduler tasked with processing incoming jobs.		Andrei Paun;Clayton Chandler;Chokchai Leangsuksun;Mihaela Paun	2016	J. Parallel Distrib. Comput.	10.1016/j.jpdc.2016.04.009	parallel computing;real-time computing;telecommunications;computer science;operating system;distributed computing;algorithm;psychological resilience;computer network	HPC	-22.836329697494524	53.46872554166268	84345
b2304f575225490a78050a50af630f2231110fa2	asdf: an automated, online framework for diagnosing performance problems		Performance problems account for a significant percentage of documented failures in large-scale distributed systems, such as Hadoop. Localizing the source of these performance problems can be frustrating due to the overwhelming amount of monitoring information available. We automate problem localization using ASDF an online diagnostic framework that transparently monitors and analyzes different time-varying data sources (e.g., OS performance counters, Hadoop logs) and narrows down performance problems to a specific node or a set of nodes. ASDF’s flexible architecture allows system administrators to easily customize data sources and analysis modules for their unique operating environments. We demonstrate the effectiveness of ASDF’s diagnostics on documented performance problems in Hadoop; our results indicate that ASDF incurs an average monitoring overhead of 0.38% of CPU time and achieves a balanced accuracy of 80% at localizing problems to the culprit node. Distributed systems are typically composed of communicating components that are spatially distributed across nodes in the system. Performance problems in such systems can be hard to diagnose and to localize to a specific node or a set of nodes. There are many challenges in problem localization (i.e., tracing the problem back to the original culprit node or nodes) and root-cause analysis (i.e., tracing the problem further to the underlying code-level fault or bug, e.g., memory leak, deadlock). First, performance problems can originate at one node in the system and then start to manifest at other nodes as well, due to the inherent communication across components–this can make it hard to discover the original culprit node. Second, performance problems can change in their manifestation over time–what originally manifests as a memory-exhaustion problem can ultimately escalate to resemble a node crash, making it hard to discover the underlying root-cause of the problem. Obviously, the larger the system and the more complex and distributed the interactions, the more difficult it is to diagnose the origin and root-cause of a problem. ? ASDF stands for Automated System for Diagnosing Failures ?? This work is supported by the NSF CAREER Award CCR-0238381, grant CNS-0326453, and the General Motors Collaborative Laboratory at CMU.	asdf;apache hadoop;black box;central processing unit;deadlock;distributed computing;ibm notes;interaction;internationalization and localization;issue tracking system;jira (software);memory leak;online algorithm;operating system;overhead (computing);system administrator	Keith A. Bare;Soila Kavulya;Jiaqi Tan;Xinghao Pan;Eugene Marinelli;Michael P. Kasick;Rajeev Gandhi;Priya Narasimhan	2009		10.1007/978-3-642-17245-8_9	embedded system;real-time computing;computer science;distributed computing	Networks	-23.87025177776695	55.38989627809762	84556
5205e9f2f315b2d29e56fe5c9b0885b5cd5b3fac	a case study on ivr applications' provisioning as cloud computing services	audio user interfaces;performance measurements ivr application provisioning cloud computing services interactive voice response multifaceted paradigm infrastructure as a service iaas platform as a service paas software as a service saas ivr substrate virtualization infrastructure resource usage;speech processing;virtualisation audio user interfaces cloud computing software performance evaluation speech processing;software performance evaluation;cloud computing virtualization detectors software as a service interactive services speech synthesis;virtualisation;cloud computing	Interactive Voice Response (IVR) applications (e.g. automated attendant, automated survey) have become ubiquitous. However, very few, if any, IVR applications are provisioned today as cloud computing services despite all the potential benefits. Cloud computing is an emerging multi-faceted paradigm, whose main facets are Infrastructure as a Service-IaaS, Platform as a Service-PaaS, and Software as a Service-SaaS. This paper proposes a case study on IVR applications' provisioning as cloud computing services. It identifies the IVR substrates (e.g. key detector, voice recorder and dialog manager) on which the IVR applications rely and proposes at the IaaS level an IVR substrate virtualization infrastructure. Substrates' virtualization enables a more efficient resource usage, a key benefit inherent to cloud computing. The article also proposes at the PaaS level an accompanying platform to develop and manage the IVR applications which use the identified virtualized substrates. A few typical IVR applications are also developed and managed to illustrate the SaaS facet of cloud computing. The implementation is described, along with the prototype and the performance measurements.	algorithm;cloud computing;coherence (physics);dialog manager;drag and drop;faceted classification;graphical user interface;interaction;interactive voice response;overhead (computing);platform as a service;programming paradigm;prototype;provisioning;quality of service;service-level agreement;software as a service;software design	Fatna Belqasmi;Christian Azar;Roch H. Glitho;Mbarka Soualhia;Nadjia Kara	2014	IEEE Network	10.1109/MNET.2014.6724104	embedded system;cloud computing;computer science;operating system;cloud testing;speech processing;utility computing;world wide web;computer security;computer network	HPC	-30.852751685318772	56.5133324881152	84814
cbd75fc633a261d95c68aed9b3f59da29edb9f8e	deferred maintenance of replicated objects in single-site databases	pointer based architectures;lazy update propagation;critical read intensive object types;database object replication;primary objects;concurrent computing;timestamps;retrieval speed;abstract data types;conflicting clustering preferences;relational database;recovery;materialized view;tree graphs;pointer based architectures single site databases replicated objects deferred maintenance database object replication performance enhancement conflicting clustering preferences many to many relationship storage lazy update propagation obsolete replica main memory hash table timestamps primary objects concurrency recovery retrieval speed critical read intensive object types redundancy storage penalty materialized view relational databases;concurrency;obsolete replica;system recovery;redundancy;marine vehicles;main memory hash table;hash table;data structures;object replication;spatial databases;database systems;concurrency control;storage penalty;replicated objects;single site databases;relational databases;object oriented databases;materialized views;performance enhancement;many to many relationship storage;replicated databases;deferred maintenance;marine vehicles spatial databases tree graphs concurrency control object oriented databases relational databases concurrent computing database systems	Replication of database objects is suggested as a means of enhancing performance in cases where conflicting preferences for clustering occur, such as in the storage of many-to-many relationships. A novel feature is lazy propagation of updates: the primary objects are kept up-to-date, but an obsolete replica is not updated until it is next retrieved. The core of the method is a main-memory hash table, containing timestamps of primary objects. Replication causes almost no overhead in regard to updates, concurrency or recovery. Yet, the retrieval speed can be increased considerably. Replication should be applied only for critical, read-intensive object types, to gain actual advantage from redundancy, compensating the storage penalty. The author's scheme could be used for implementing materialized views in relational databases. However, the method is described in terms of pointer-based architectures.		Jukka Teuhola	1996		10.1109/DEXA.1996.558597	materialized view;concurrent computing;relational database;computer science;theoretical computer science;data mining;database;distributed computing;programming language;world wide web	DB	-21.26596212966436	49.171851409106885	85165
273995a0f69998ef6d9aa4f15579c3a6fc02d311	a dynamic shadow approach for mobile agents to survive crash failures	crash failure;dynamic shadow method;performance evaluation;fault tolerant;computer crashes;information retrieval;performance evaluation dynamic shadow method mobile agents agent server crash failure recovery fault tolerance exception handling;mobile agents;fault tolerant computing;mobile agent system;system recovery;internet;guidelines;design guideline;fault tolerance;performance analysis;system recovery mobile agents fault tolerant computing exception handling performance evaluation;exception handling;web server;experimental evaluation;computer science;mobile agent;server crash failures;agent server crash failure recovery;mobile agents computer crashes fault tolerance web server information retrieval performance analysis computer science performance evaluation guidelines internet	Fault tolerance schemes for mobile agents to survive agent server crash failures are complex since developers normally have no control over remote agent servers. Some solutions modify the agent server platform, e.g. a replica of the mobile agent is injected into stable storage upon its arrival at an agent server. However in the event of an agent server crash the replica remains unavailable until the agent server recovers. This paper presents a failure model and a revised exception handling framework for mobile agent systems. An exception handler design is presented for mobile agents to survive agent server crash failures. A replica mobile agent operates at the agent server visited prior to its master’s current location. If a master crashes its replica is available as a replacement. Experimental evaluation is performed and performance results are used to suggest some useful design guidelines.	exception handling;experiment;fault tolerance;information retrieval;mobile agent;process group;server (computing);simulation;stable storage	Simon Pears;Jie Xu;Cornelia Boldyreff	2003		10.1109/ISORC.2003.1199243	embedded system;fault tolerance;real-time computing;computer science;operating system;mobile agent;distributed computing;programming language;computer security	AI	-22.695563786897658	49.977727486100136	85262
813f2ce5f0278044167893b4db8ed75c60b9c9a6	a standard for benchmarking big data systems	benchmark testing big data industries standards measurement availability hardware;benchmark models big data system benchmarking big data technologies hadoop enterprise it ecosystem tpc express benchmark hs tpcx hs operating system commercial apache hadoop file system api compatible software distributions energy efficiency metrics;standards application program interfaces benchmark testing big data data handling parallel processing	Big Data technologies like Hadoop have become an important part of the enterprise IT ecosystem. TPC Express Benchmark™HS (TPCx-HS) was developed to provide an objective measure of hardware, operating system and commercial Apache Hadoop File System API compatible software distributions, and to provide the industry with verifiable performance, price-performance, and availability and energy efficiency metrics. The benchmark models a continuous system availability of 24 hours a day, 7 days a week.	apache hadoop;application programming interface;benchmark (computing);big data;data system;ecosystem;file system api;formal verification;ibm tivoli storage productivity center;operating system	Raghunath Othayoth Nambiar	2014	2014 IEEE International Conference on Big Data (Big Data)	10.1109/BigData.2014.7004472	sdet;computer science;operating system;data mining;database	DB	-27.33009802749611	57.347699339493495	85390
88c48dfc8e8388c47f3cb6b18ff8d5024f26d324	adaptive workflow processing and execution in pegasus	resource allocation;adaptive grid;pegasus workflow management system;adaptive workflow;grid middleware;computational resources;workflow management software middleware;grid;large scale;adaptivity;scheduling algorithm;pegasus;scheduling;grid middleware adaptive workflow processing computational resources workflow management system pegasus workflow management system resource allocation;workflow management software;middleware;workflow management system;workflow processing;experimental evaluation;institutional repository research archive oaister;grid workflow processing adaptivity pegasus scheduling;adaptive workflow processing;resource management workflow management software concrete engines pervasive computing parallel processing feedback runtime environment programmable control adaptive control	Workflows are widely used in applications that require coordinated use of computational resources. Workflow definition languages typically abstract over some aspects of the way in which a workflow is to be executed, such as the level of parallelism to be used or the physical resources to be deployed. As a result, a workflow management system has responsibility for establishing how best to execute a workflow given the available resources. The Pegasus workflow management system compiles abstract workflows into concrete execution plans, and has been widely used in large-scale e-Science applications. This paper describes an extension to Pegasus whereby resource allocation decisions are revised during workflow evaluation, in the light of feedback on the performance of jobs at runtime. The contributions of this paper include: (i) a description of how adaptive processing has been retrofitted to an existing workflow management system; (ii) a scheduling algorithm that allocates resources based on runtime performance; and (Hi) an experimental evaluation of the resulting infrastructure using grid middleware over clusters.	algorithm;autonomic computing;computational resource;e-science;experiment;information sciences institute;job stream;middleware;montagejs;parallel computing;pegasus;run time (program lifecycle phase);scheduling (computing)	Kevin Lee;Norman W. Paton;Rizos Sakellariou;Ewa Deelman;Alvaro A. A. Fernandes;Gaurang Mehta	2008	2008 The 3rd International Conference on Grid and Pervasive Computing - Workshops	10.1109/GPC.WORKSHOPS.2008.30	workflow;real-time computing;xpdl;computer science;operating system;workflow management coalition;database;distributed computing;windows workflow foundation;scheduling;workflow management system;workflow engine;workflow technology	HPC	-30.175419321650306	52.87818983902188	85654
520a631c90005edbd3dd3bf841ccbd6fc353c788	service orientation paradigm in future network architectures	graph theory;protocols;service oriented architecture graph theory internet protocols;service orientation;protocol graphs;service oriented network architecture service orientation paradigm future network architectures internet soa paradigm self contained building blocks dynamic protocol graphs composition methods selection methods;protocol graphs soa network architectures service orientation building blocks;network architectures;soa;runtime;computer architecture;internet;rna;building blocks;service oriented architecture;architecture;protocols computer architecture service oriented architecture runtime architecture rna	The Internet can not keep up with changing application requirements and new network technologies as its network architecture makes it hard to introduce new functionality because existing functionalities in the Architecture are inherently tightly coupled. This article describes how the principles of Service Oriented Architecture (SOA) can help to develop more flexible network architecture. We argue that the SOA paradigm can be applied to networks by utilizing the concepts of self-contained building blocks, dynamic protocol graphs and selection and composition methods. In order to make use of flexible networks, applications must be decoupled from the protocols they use. We give a brief overview, of how some of these concepts are already implemented, by presenting few approaches. Finally we describe some challenges of service oriented network architecture.	computer performance;interdependence;internet;loose coupling;network architecture;overhead (computing);programming paradigm;protocol stack;requirement;service-oriented architecture;software engineering	M. Rahamatullah Khondoker;Abbas Ali Siddiqui;Bernd Reuther;Paul Müller	2012	2012 Sixth International Conference on Innovative Mobile and Internet Services in Ubiquitous Computing	10.1109/IMIS.2012.30	enterprise architecture framework;reference architecture;space-based architecture;real-time computing;network architecture;computer science;applications architecture;graph theory;theoretical computer science;service-oriented architecture;service;solution architecture;distributed computing;law;computer network	Mobile	-33.27359238861966	46.905704272180095	86143
0769f799b7c128e9a63a56adda68b10a80a9aa52	replication in the harp file system	distribution;files records;reliability;networks;distributed networks;communication and radio systems;commerce;design and implementation;file system;capacity quantity;storage;unix file system;reaction time;disks;guarantees	This paper describes the design and implementation of the Harp file system. Harp is a replicated Unix file system accessible via the VFS interface. It provides highly available and reliable storage for files and guarantees that file operations are executed atomically in spite of concurrency and failures. It uses a novel variation of the primary copy replication technique that provides good performance because it allows us to trade disk accesses for network communication. Harp is intended to be used within a file service in a distributed network; in our current implementation, it is accessed via NFS. Preliminary performance results indicate that Harp provides equal or better response time and system capacity than an unreplicated implementation of NFS that uses Unix files directly.	channel capacity;concurrency (computer science);harp;response time (technology);unix	Barbara Liskov;Sanjay Ghemawat;Robert Gruber;Paul Johnson;Liuba Shrira;Michael Williams	1991		10.1145/121132.121169	distribution;fork;self-certifying file system;mental chronometry;real-time computing;computer file;resolv.conf;network file system;computer science;stub file;versioning file system;operating system;fstab;unix file types;ssh file transfer protocol;journaling file system;reliability;database;file descriptor;open;distributed file system;everything is a file;data file;file system fragmentation;computer security;file area network;file control block;virtual file system	OS	-20.639802200936156	50.461316999379854	86203
3827b0ba243c7458ce8cb2dc78893df8a222bb4e	service oriented web based meta resource sharing platform - the cbweb portal	middleware;resource sharing;heterogeneous computing;pervasive computing;grid computing;cluster computing	Grid computing and cluster computing platforms are becoming the primary development platforms for pervasive computing. The primary problem associated with both computational grids and clustering systems is the latency or bandwidth required for applying effective job submission and processing by remote systems. The control over the jobs submitted also poses problems for effective communication and remote computation. In this paper we discuss techniques in which different types of heterogeneous computing platforms and their resources are utilized over a decentralised interconnected network such as the Internet. The model is a web portal called the Cluster Based Web submission system or CBWeB which primarily focuses on offering GRID service components or resources to the end user without the additional complexities of installing special middleware components for utilizing remote resources. A Beowulf cluster was built and the model is currently being implemented. This model is an extension to a previous architectural model called the CBReM (9). The portal provides access to distributed resources and is used to simplify the process of using such resources.	websphere portal	Avinash Shankar;Daniel Saffioti;Ian Piper;Ashwin Shankar	2004			grid computing;web application;ubiquitous computing;computer science;utility computing;distributed computing;semantic grid;shared resource;database;computer cluster;middleware	Robotics	-30.886895053807326	52.09732345935907	86493
612e5a02c0751b53b072f499260e08a10d34bb7e	on correctness of non-serializable executions	computer aided design	In a number of application environments (e.g., computer aided design), serializability, the traditionally accepted notion of correctness has been found to be too restrictive, and a number of alternate criteria have been proposed in the literature. One such criterion is predicate-wise serializability (PWSR), which requires only restrictions of schedules that access subsets of the database over which integrity constraints are de ned, to be serializable. In this paper, we identify restrictions on the structure of transaction programs, their concurrent execution and their access characteristics under which PWSR schedules preserve database consistency.	acid;autonomous robot;autonomy;computer-aided design;concurrency (computer science);correctness (computer science);cursor (databases);data integrity;distributed computing;distributed database;global serializability;hoc (programming language);mathematical induction;requirement;transaction processing;verse protocol;waits	Rajeev Rastogi;Sharad Mehrotra;Yuri Breitbart;Henry F. Korth;Abraham Silberschatz	1993		10.1145/153850.153859	global serializability;parallel computing;real-time computing;database transaction;rollback;distributed transaction;computer science;two-phase locking;database;online transaction processing;serializability;acid;consistency;atomicity;schedule	DB	-23.28102787273217	47.560683572609506	86638
6931cd443ef0dbd5b69107850bd61fb87777fea3	application of organization model based on ldap directory server to workflow management system	file servers;management system;heterogeneous systems;organization model;authorisation;biological system modeling;lightweight directory access protocol;ldap directory server;role based access control;resource access;servers;adaptation model;model utilization;rbac;lightweight directory access protocol organization model ldap directory server workflow management system model utilization role based access control resource access;synchronization;workflow management software access protocols authorisation file servers organisational aspects;access protocols;workflow management software;workflow management system;organizations;workflow management software access control technology management relational databases finance access protocols conferences computational intelligence computer industry electronics industry;ldap;workflow management system ldap rbac organization model;data models;organisational aspects	As an important part of workflow management system, the organization model is being challenged by some difficulties such as cross-region, heterogeneous systems and so on. Firstly, this article proposes an organization model hosted in the LDAP directory server. This scheme expands the scope of model utilization, gives the technical method which is capable of dealing with the heterogeneity, distribution of the organization model, and the transparency of the model to its users. Then the role-based access control (RBAC) technology has been used in this article to implement the control of resource access in the workflow management system according to the rules in the organization model. At the end, this article proposes an idea of designing an organization model information platform in workflow management systems and a prototype system. After experiments and analysis, we proved this organization model has good compatibility, extendibility and high operating efficiency. Thanks to the role-based strategy, the model facilitated the management authority and suits the independence in the work flow management system for the use of organization model.	directory (computing);directory service;experiment;lightweight directory access protocol;management system;prototype;requirement;role-based access control;server (computing)	Zhiyi Wang;Zheng Wang;Jiaxi Luo	2008	2008 IEEE Pacific-Asia Workshop on Computational Intelligence and Industrial Application	10.1109/PACIIA.2008.383	workflow;lightweight directory access protocol;computer science;knowledge management;operating system;role-based access control;database;world wide web;workflow management system;workflow engine;workflow technology	DB	-32.84238636048184	50.400046593248874	86877
b535b65690a92b66d358aa7d8f8831a3b24c0093	designing next generation clusters with infiniband and 10ge/iwarp: opportunities and challenges	commodity multicore processors;parallel file systems;protocols;multi core processor;openfabrics software stack;infiniband;storage area networks;lan based clusters;data center;servers;openfabrics software stack infiniband 10 ge iwarp commodity multicore processors commodity networking technologies hpc clusters servers parallel file systems multitier data centers protocols petascale clusters san based clusters lan based clusters wan based clusters;parallel file system;petascale clusters;next generation;multitier data centers;cost effectiveness;wan based clusters;wide area networks local area networks protocols storage area networks;san based clusters;10 ge iwarp;commodity networking technologies;high performance;reliability availability and serviceability;local area networks;wide area networks;hpc clusters	Clusters with commodity multi-core processors and commodity networking technologies are providing cost-effective solutions for building next generation high-end systems including HPC clusters, servers, parallel file systems and multi-tier data-centers. The talk focus on two emerging networking technologies (InfiniBand and 10 GE/iWARP) and their associated protocols for designing such systems. In this talk, we critically examine the current and future trends of these technologies and their applicability for designing next generation petascale clusters. The talk start with the motivations behind these technologies and then focus on their architectural aspects and applicability to SAN, LAN and WAN-based clusters. Designing next generation clusters with high performance, scalability and RAS (reliability, availability and serviceability) capabilities by using these technologies will be examined. Current and future trends of InfiniBand and iWARP products was highlighted. The emerging OpenFabrics software stack, focusing both these technologies in an integrated manner, was presented. Finally, a set of case studies in designing various clusters with these networking technologies was presented to outline the associated opportunities and challenges.	iwarp;infiniband;next-generation network	Dhabaleswar K. Panda	2008		10.1109/CLUSTR.2008.4663772	local area network;reliability, availability and serviceability;multi-core processor;communications protocol;data center;parallel computing;real-time computing;storage area network;cost-effectiveness analysis;computer science;operating system;server;computer network	HPC	-27.3566221550337	56.51943406444851	86929
c786f399a09f7919678a17678d580d3dfcecbf25	performance modeling in mapreduce environments: challenges and opportunities	user needs;service level;performance evaluation;capacity planning;service provider;performance management;large dataset;resource allocation;distributed computing;workload profiling;web service;profiles techniques;large scale;pay as you go;side effect;performance analysis;performance model;cost efficiency;measurements;mapreduce;business intelligence;job scheduling;spam detection;cloud computing;open source	"""Unstructured data is the largest and fastest growing portion of most enterprise's assets, often representing 70% to 80% of online data. These steep increase in volume of information being produced often exceeds the capabilities of existing commercial databases. MapReduce and its open-source implementation Hadoop represent an economically compelling alternative that offers an efficient distributed computing platform for handling large volumes of data and mining petabytes of unstructured information. It is increasingly being used across the enterprise for advanced data analytics, business intelligence, and enabling new applications associated with data retention, regulatory compliance, e-discovery and litigation issues.  However, setting up a dedicated Hadoop cluster requires a significant capital expenditure that can be difficult to justify. Cloud computing offers a compelling alternative and allows users to rent resources in a """"pay-as-you-go"""" fashion. For example, a list of offered Amazon Web Services includes MapReduce environment for rent. It is an attractive and cost-efficient option for many users because acquiring and maintaining complex, large-scale infrastructures is a difficult and expensive decision. One of the open questions in such environments is the amount of resources that a user should lease from the service provider. Currently, there is no available methodology to easily answer this question, and the task of estimating required resources to meet application performance goals is the solely user's responsibility. The users need to perform adequate application testing, performance evaluation, capacity planning estimation, and then request appropriate amount of resources from the service provider. To address these problems we need to understand: """"What do we need to know about a MapReduce job for building an efficient and accurate modeling framework? Can we extract a representative job profile that reflects a set of critical performance characteristics of the underlying application during all job execution phases, i.e., map, shuffle, sort and reduce phases? What metrics should be included in the job profile?"""" We discuss a profiling technique for MapReduce applications that aims to construct a compact job profile that is comprised of performance invariants which are independent of the amount of resources assigned to the job (i.e., the size of the Hadoop cluster) and the size of the input dataset. The challenge is how to accurately predict application performance in the large production environment and for processing large datasets from the application executions that being run in the smaller staging environment and that process smaller input datasets.  One of the major Hadoop benefits is its ability of dealing with failures (disk, processes, node failures) and allowing the user job to complete. The performance implications of failures depend on their types, when do they happen, and whether a system can offer some spare resources instead of failed ones to the running jobs. We discuss how to enhance the MapReduce performance model for evaluating the failure impact on job completion time and predicting a potential performance degradation.  Sharing a MapReduce cluster among multiple applications is a common practice in such environments. However, a key challenge in these shared environments is the ability to tailor and control resource allocations to different applications for achieving their performance goals and service level objectives (SLOs). Currently, there is no job scheduler for MapReduce environments that given a job completion deadline, could allocate the appropriate amount of resources to the job so that it meets the required SLO. In MapReduce environments, many production jobs are run periodically on new data. For example, Facebook, Yahoo!, and eBay process terabytes of data and event logs per day on their Hadoop clusters for spam detection, business intelligence and different types of optimization. For the production jobs that are routinely executed on the new datasets, can we build on-line job profiles that later are used for resource allocation and performance management by the job scheduler? Wediscuss opportunities and challenges for building the SLO-based Hadoop scheduler.  The accuracy of new performance models might depend on the resource contention, especially, the network contention in the production Hadoop cluster. Typically, service providerstend to over provision network resources to avoid undesirable side effects of network contention. At the same time, it is an interesting modeling question whether such a network contention factor can be introduced, measured, and incorporated in theMapReduce performance model. Benchmarking Hadoop, optimizing cluster parameter settings, designing job schedulers with different performance objectives, and constructing intelligent workload management in shared Hadoop clusters create an exciting list of challenges and opportunities for the performance analysis and modeling in MapReduce environments."""	amazon web services;anti-spam techniques;apache hadoop;cloud computing;cost efficiency;database;deployment environment;disk staging;distributed computing;electronic discovery;elegant degradation;fastest;job scheduler;job stream;mapreduce;mathematical optimization;need to know;online and offline;open-source software;performance evaluation;petabyte;process (computing);resource contention;scheduling (computing);spamming;terabyte;web testing	Ludmila Cherkasova	2011		10.1145/1958746.1958752	service provider;web service;performance management;real-time computing;service level;cloud computing;resource allocation;computer science;job scheduler;data mining;database;business intelligence;side effect;measurement;cost efficiency	HPC	-24.210809570736057	60.14162540272068	87048
456e4ebf5f0a4833ce06f4250cd3627a6e3a81b5	scaling up classifiers to cloud computers	ensemble learning;probability density function;learning artificial intelligence data mining;training;data management;data mining;scaling up;accuracy;large scale;machine learning;distributed data mining ensemble learning cloud computing scalability;distributed databases;scalability;distributed data mining;learning artificial intelligence;datasets cloud computers machine learning algorithms computing infrastructures data management scalable data mining;meteorology;cloud computing data mining distributed computing scalability partitioning algorithms large scale systems computer science data engineering usa councils machine learning algorithms;cloud computing	As the size of available datasets has grown from Megabytes to Gigabytes and now into Terabytes, machine learning algorithms and computing infrastructures have continuously evolved in an effort to keep pace. But at large scales, mining for useful patterns still presents challenges in terms of data management as well as computation. These issues can be addressed by dividing both data and computation to build ensembles of classifiers in a distributed fashion, but trade-offs in cost, performance, and accuracy must be considered when designing or selecting an appropriate architecture. In this paper, we present an abstraction for scalable data mining that allows us to explore these trade-offs. Data and computation are distributed to a computing cloud with minimal effort from the user, and multiple models for data management are available depending on the workload and system configuration. We demonstrate the performance and scalability characteristics of our ensembles using a wide variety of datasets and algorithms on a Condor-based pool with Chirp to handle the storage.	algorithm;chirp;cloud computing;computation;converge;data mining;data parallelism;ensembles of classifiers;expect;gigabyte;machine learning;megabyte;multi-core processor;multiprocessing;scalability;seamless3d;software deployment;system configuration;terabyte;workstation	Christopher Moretti;Karsten Steinhaeuser;Douglas Thain;Nitesh V. Chawla	2008	2008 Eighth IEEE International Conference on Data Mining	10.1109/ICDM.2008.99	probability density function;scalability;cloud computing;data management;computer science;data science;theoretical computer science;machine learning;data mining;accuracy and precision;ensemble learning;statistics	DB	-19.47704675432516	53.92271802601042	87145
d19b4b7e1ad9b3325d83b07cc5d75c87d16162b4	pay-as-you-go processing for tracing queries in a p2p record exchange system	article author;query processing;maintenance cost;p2p;pay as you go;p2p networks;peer to peer	In recent years, data provenance or lineage tracing which refers to the process of tracing the sources of data, data movement between databases, and annotations written for data has become an issue of acute importance in database research. Our research concerns the data provenance issue in peer-to-peer (P2P) networks where duplicates and modifications of data occur independently in autonomous peers. To ensure reliability among the data exchanged in P2P networks, we have proposed a reliable record exchange framework with tracing facilities based on database technologies [14, 15]. Tracing operations in the system are executed as distributed recursive queries among cooperating peers in a P2P network. The framework is based on the “pay-as-you-go” approach in which the system maintains a minimum amount of information for tracing with a low maintenance cost and the user pays the cost when he or she issues a tracing query to the system. This paper focuses on the query processing strategies used in the system. We propose two alternative query processing strategies and compare their characteristics on the basis of performance.	autonomous robot;database;hierarchical and recursive queries in sql;lineage (evolution);peer-to-peer;recursion;tracing (software)	Fengrong Li;Takuya Iida;Yoshiharu Ishikawa	2009		10.1007/978-3-642-00887-0_28	computer science;peer-to-peer;data mining;database;world wide web	DB	-25.72167461427965	49.79884816465204	87240
d33705552eb007d40151b035c1e627902f99651e	power capping in high performance computing systems		"""Power consumption is a key factor in modern ICT infrastructure, especially in the expanding world of High Performance Computing, Cloud Computing and Big Data. Such consumption is bound to become an even greater issue as supercomputers are envisioned to enter the Exascale by 2020, granted that they obtain an order of magnitude energy efficiency gain. An important component in many strategies devised to decrease energy usage is """"power capping"""", i.e., the possibility to constrain the system power consumption within certain power budget. In this paper we propose two novel approaches for power capped workload dispatching and we demonstrate them on a real-life high-performance machine: the Eurora supercomputer hosted at CINECA computing center in Bologna. Power capping is a feature not included in the commercial Portable Batch System PBS dispatcher currently in use on Eurora. The first method is based on a heuristic technique while the second one relies on a hybrid strategy which combines a CP and a heuristic approach. Both systems are evaluated and compared on simulated job traces."""	frequency capping	Andrea Borghesi;Francesca Collina;Michele Lombardi;Michela Milano;Luca Benini	2015		10.1007/978-3-319-23219-5_37	embedded system;real-time computing;simulation;computer science;algorithm	HPC	-20.442113446255934	60.35679210625277	87386
be4c6170ee4fd72ff5c8fc92e3d6ba5cba774cf6	parda: proportional allocation of resources for distributed storage access	virtual machine;storage system;issue queue;resource manager;software systems;distributed storage;load sharing;quality of service;flow control	Rapid adoption of virtualization technologies has led to increased utilization of physical resources, which are multiplexed among numerous workloads with varying demands and importance. Virtualization has also accelerated the deployment of shared storage systems, which offer many advantages in such environments. Effective resource management for shared storage systems is challenging, even in research systems with complete end-to-end control over all system components. Commercially-available storage arrays typically offer only limited, proprietary support for controlling service rates, which is insufficient for isolating workloads sharing the same storage volume or LUN. To address these issues, we introduce PARDA, a novel software system that enforces proportional-share fairness among distributed hosts accessing a storage array, without assuming any support from the array itself. PARDA uses latency measurements to detect overload, and adjusts issue queue lengths to provide fairness, similar to aspects of flow control in FAST TCP. We present the design and implementation of PARDA in the context of VMware ESX Server, a hypervisor-based virtualization system, and show how it can be used to provide differential quality of service for unmodified virtual machines while maintaining high efficiency. We evaluate the effectiveness of our implementation using quantitative experiments, demonstrating that this approach is practical.	computer data storage;disk array;end-to-end encryption;experiment;fairness measure;hardware virtualization;hypervisor;logical unit number;multiplexing;quality of service;register renaming;software deployment;software system;virtual machine	Ajay Gulati;Irfan Ahmad;Carl A. Waldspurger	2009			real-time computing;converged storage;quality of service;distributed data store;computer science;virtual machine;resource management;operating system;flow control;database;distributed computing;software system	OS	-22.29881708859658	54.41687476203283	87511
63998142eeaeedfb27d5a2714a77dfedb749239f	comprehensive, open-source resource usage measurement and analysis for hpc systems	hpc resource management;tacc_stats;supremm;xsede;xdmod;performance analysis;system management;usage analysis	The important role high-performance computing HPC resources play in science and engineering research, coupled with its high cost capital, power and manpower, short life and oversubscription, requires us to optimize its usage - an outcome that is only possible if adequate analytical data are collected and used to drive systems management at different granularities - job, application, user and system. This paper presents a method for comprehensive job, application and system-level resource use measurement, and analysis and its implementation. The steps in the method are system-wide collection of comprehensive resource use and performance statistics at the job and node levels in a uniform format across all resources, mapping and storage of the resultant job-wise data to a relational database, which enables further implementation and transformation of the data to the formats required by specific statistical and analytical algorithms. Analyses can be carried out at different levels of granularity: job, user, application or system-wide. Measurements are based on a new lightweight job-centric measurement tool 'TACC_Stats', which gathers a comprehensive set of resource use metrics on all compute nodes and data logged by the system scheduler. The data mapping and analysis tools are an extension of the XDMoD project. The method is illustrated with analyses of resource use for the Texas Advanced Computing Center's Lonestar4, Ranger and Stampede supercomputers and the HPC cluster at the Center for Computational Research. The illustrations are focused on resource use at the system, job and application levels and reveal many interesting insights into system usage patterns and also anomalous behavior due to failure/misuse. The method can be applied to any system that runs the TACC_Stats measurement tool and a tool to extract job execution environment data from the system scheduler. Copyright © 2014 John Wiley & Sons, Ltd.	open-source software	James C. Browne;Robert L. DeLeon;Abani K. Patra;William L. Barth;John Hammond;Matthew D. Jones;Thomas R. Furlani;Barry L. Schneider;Steven M. Gallo;Amin Ghadersohi;Ryan J. Gentner;Jeffrey T. Palmer;Nikolay Simakov;Martins Innus;Andrew E. Bruno;Joseph P. White;Cynthia D. Cornelius;Thomas Yearke	2014	Concurrency and Computation: Practice and Experience	10.1002/cpe.3245	parallel computing;systems management;simulation;computer science;operating system;database;distributed computing;operations research	HPC	-28.58645853232555	52.42268233161417	88153
2bd37a0cf0e846e7bf478aacef0e72aabc863406	transparent symmetric active/active replication for service-level high availability	parallel computing;computers;high availability;reliability;general and miscellaneous mathematics computing and information science;software reliability client server systems grid computing parallel processing software architecture;distributed grid computing;service level;communications;availability;prototypes;service level replication technique;virtual community;distributed computing;service level high availability;client server systems;parallel and distributed computing;virtual communication layer;service side interceptor;computer architecture;software architecture;transparent symmetric active replication;redundancy;redundancy strategy;scientific computing;service side interceptor transparent symmetric active replication service level high availability service oriented architecture parallel computing distributed grid computing individual service instance reliability redundancy strategy service level replication technique virtual communication layer client side interceptor;client side interceptor;computer science;availability redundancy computer science laboratories service oriented architecture distributed computing grid computing scientific computing prototypes software architecture;individual service instance reliability;service oriented architecture;grid computing;software reliability;parallel processing	As service-oriented architectures become more important in parallel and distributed computing systems, individual service instance reliability as well as appropriate service redundancy becomes an essential necessity in order to increase overall system availability. This paper focuses on providing redundancy strategies using service-level replication techniques. Based on previous research using symmetric active/active replication, this paper proposes a transparent symmetric active/active replication approach that allows for more reuse of code between individual service-level replication implementations by using a virtual communication layer. Service- and client-side interceptors are utilized in order to provide total transparency. Clients and servers are unaware of the replication infrastructure as it provides all necessary mechanisms internally.	client-side;communications protocol;communications security;computer cluster;distributed computing;failover;grid computing;high availability;interceptor pattern;megabit;overhead (computing);replication (computing);service-oriented architecture;service-oriented device architecture;software architecture;supercomputer;throughput;visual component library (vcl)	Christian Engelmann;Stephen L. Scott;Chokchai Leangsuksun;Xubin He	2007	Seventh IEEE International Symposium on Cluster Computing and the Grid (CCGrid '07)	10.1109/CCGRID.2007.116	parallel processing;software architecture;availability;real-time computing;service level;computer science;operating system;service-oriented architecture;reliability;database;distributed computing;prototype;redundancy;high availability;software quality;grid computing;replication	HPC	-32.42756287758506	48.728402097369894	88283
95f2fcd3903ef138ae1eabe46ff76f57add6f426	multi-node multi-agent cloud simulation: approximating synchronisation	agent based;simulation;cloud;traffic engineering computing big data cloud computing multi agent systems road traffic synchronisation;computing;synchronisation;computer architecture;computational modeling;roads;synchronization;synchronization computational modeling load modeling vehicles computer architecture roads mathematical model;traffic;mathematical model;agent based synchronisation cloud computing traffic simulation;vehicles;load modeling;traffic engineering road network infrastructure multinode multiagent cloud simulation big data generation cloud computing synchronisation strategy large scale complex simulation distributed computing synchronisation overhead reduction synchronisation accuracy synchronisation approximation	Traffic engineering is a key in effective utilisation of the road network infrastructure. Simulation assists traffic engineers making informed decisions on how to operate and direct traffic within the road networks. These simulations are complex, generate big data and require high-powered computers, which can process information faster than real time, to ensure the results can be used to affect traffic. Cloud computing, a relatively new technology paradigm, can meet the essential requirements, such as scalability, interoperability, availability and high-end performance. In this paper, a novel approach to a synchronisation strategy of large-scale complex simulations is proposed. This approach builds upon advancements achieved in distributed computing. The new synchronisation strategy is designed to allow different granularities of synchronisation accuracy. Through this strategy, synchronisation overhead is reduced, thus allowing the computing bandwidth to be applied to simulation performance increases as a result of the trade off between synchronisation accuracy and performance.	big data;cloud computing;computer;distributed computing;interoperability;overhead (computing);programming paradigm;requirement;scalability;simulation	Antonio Giardina;Yun Yang;Hai Vu;Rajesh Vasa	2015	2015 IEEE 11th International Conference on e-Science	10.1109/eScience.2015.32	real-time computing;simulation;computer science;distributed computing	HPC	-21.461506846535237	57.94256095492977	88419
895c95a079ce6bbd234030fefe01074896f52dfc	want scalable computing?: speculate!	distributed computing;wireless mesh network;sensor network;large scale;paradigm shift;data grid	Distributed computing is currently undergoing a paradigm shift, towards large-scale dynamic systems, where thousands of nodes collaboratively solve computational tasks. Examples of such emerging systems include autonomous sensor networks, data-grids, wireless mesh network (WMN) infrastructures, and more. We argue that speculative computations will be instrumental to successfully performing meaningful computations in such systems. Moreover, solutions deployed in such platforms will need to be as local as possible.	autonomous robot;computation;distributed computing;dynamical system;mesh networking;programming paradigm;scalability;speculative execution;wireless mesh network	Idit Keidar;Assaf Schuster	2006	SIGACT News	10.1145/1165555.1165569	wireless mesh network;paradigm shift;parallel computing;wireless sensor network;computer science;theoretical computer science;data grid;distributed computing;key distribution in wireless sensor networks	HPC	-33.64569681159994	50.49205941379046	88491
b66e78a784daced9f100fc8fc32b0bab15081cb3	a framework for measurement based performance modeling	performance test;measurement;performance;model performance;performance model;modeling;framework	Techniques for performance modeling are broadly classified into measurement, analytical and simulation based techniques. Measurement based performance modeling is commonly adopted in practice. Measurement based modeling requires the execution of a large number of performance tests to build accurate performance models. These performance tests must be repeated for every release or build of an application. This is a time consuming and error-prone manual process.  In this paper, we present a framework for the systematic and automated building of measurement based performance models. The framework is based on our experience in performance modeling of two large applications: the DVD Store application by Dell and another larger enterprise application. We use the Dell DVD Store application as a running example to demonstrate the various steps in our framework. We present the benefits and shortcomings of our framework. We discuss the expected reduction in effort due to adopting our framework.	cognitive dimensions of notations;computer performance;enterprise software;performance prediction;simulation	Dharmesh Thakkar;Ahmed E. Hassan;Gilbert Hamann;Parminder Flora	2008		10.1145/1383559.1383567	real-time computing;simulation;systems modeling;performance;computer science;engineering;software framework;operating system;programming language;measurement	Metrics	-22.27696843344732	56.51373043866051	88556
705f82d7872a9bad2d6c36154303b6b8d3a1d66b	challenges with applying performance testing methods for systems deployed on shared environments with indeterminate competing workloads: position paper	software performance engineering;cloud performance;performance measurement and testing	There is a tendency to move production environments from corporate-owned data centers to cloud-based services. Users who do not maintain a private production environment might not wish to maintain a private performance test environment either. The application of performance engineering methods to the development and delivery of software systems is complicated when the form and or parameters of the target deployment environment cannot be controlled or determined. The difficulty of diagnosing the causes of performance issues during testing or production may be increased by the presence of highly variable workloads on the target platform that compete with the application of interest for resources in ways that might be hard to determine. In particular, performance tests might be conducted in virtualized environments that introduce factors influencing customer-affecting metrics (such as transaction response time) and observed resource usage. Observed resource usage metrics in virtualized environments can have different meanings from those in a native environment. Virtual machines may suffer delays in execution. We explore factors that exacerbate these complications. We argue that these complexities reinforce the case for rigorously using software performance engineering methods rather than diminishing it. We also explore possible performance testing methods for mitigating the risk associated with these complexities.	cloud computing;data center;deployment environment;indeterminacy in concurrent computation;performance engineering;response time (technology);software performance testing;software system;virtual machine	Andre B. Bondi	2016		10.1145/2859889.2859895	embedded system;real-time computing;simulation;software performance testing;engineering	OS	-22.706474840295368	57.32323199678006	88560
407ec355b2a740a0fa17b3b24bf76f234cc8ad96	predictable management of system resources for linux	soft real time;domain knowledge;large scale;operating system;model fitting	In current operating systems, a process acts both as a protection domain and as a resource principal. This may not be the right model as a user may like to see a set of processes or a sub activity in a process as a resource principal. Another problem is that much of the processing may happen in the interrupt context, and they will not be accounted for properly. Resource Containers[1] have been introduced to solve such problems in the large-scale server systems context by separating out the protection domain from the resource principal by associating and charging all the processing to the correct container. This paper tries to investigate how this model fits into a Linux framework, especially, in thesoft real timecontext. We show that this model allows us to allocate resources in a predictable manner and hence can be used for scheduling soft real-time tasks like multimedia. We also provide a framework in Linux which allows privileged users to have their own schedulers for scheduling a group of activities so that they can make use of the domain knowledge about the applications. We also extend this model to allow multiple scheduling classes.	central processing unit;computation;data structure;fits;i/o scheduling;include directive;init;input/output;interrupt;kernel (operating system);linux;memory management;multiprocessing;operating system;paging;real-time clock;real-time computing;scheduling (computing);server (computing);working set	Mansoor Alicherry;K. Gopinath	2001			real-time computing;simulation;computer science;operating system;domain knowledge	OS	-22.4776835564676	57.874936485705035	88853
d046b97ff711c32b25eff210c417dd3c5916e816	includeos: a minimal, resource efficient unikernel for cloud services	libraries;unikernel;standards;virtual machining;virtio;virtual machine monitors;operating systems libraries cloud computing hardware virtual machining standards virtual machine monitors;dns server includeos resource efficient unikernel cloud services cloud computing elastically scaling services single tasking library operating system c language extremely small disk and memory footprint efficient asynchronous i o os library bootable disk image;resource allocation c language cloud computing input output programs operating system kernels;library os;full virtualization;virtio unikernel library os full virtualization;cloud computing;operating systems;hardware	The emergence of cloud computing as a ubiquitous platform for elastically scaling services has generated need and opportunity for new types of operating systems. A service that needs to be both elastic and resource efficient needs A) highly specialized components, and B) to run with minimal resource overhead. Classical general purpose operating systems designed for extensive hardware support are by design far from meeting these requirements. In this paper we present IncludeOS, a single tasking library operating system for cloud services, written from scratch in C++. Key features include: extremely small disk-and memory footprint, efficient asynchronous I/O, OS-library where only what your service needs gets included, and only one device driver by default (virtio). As a test case a bootable disk image consisting of a simple DNS server with OS included is shown to require only 158 kb of disk space and to require 5-20% less CPU-time, depending on hardware, compared to the same binary running on Linux.	asynchronous i/o;booting;c++;central processing unit;cloud computing;device driver;disk image;disk space;emergence;image scaling;input/output;linux;memory footprint;operating system;overhead (computing);requirement;server (computing);test case;unikernel	Alfred Bratterud;Alf-Andre Walla;Hårek Haugerud;Paal E. Engelstad;Kyrre M. Begnum	2015	2015 IEEE 7th International Conference on Cloud Computing Technology and Science (CloudCom)	10.1109/CloudCom.2015.89	embedded system;full virtualization;real-time computing;cloud computing;computer science;operating system	Embedded	-28.792286446601654	55.55266823022597	88909
a705b0cd87e857848819a959dd5c109a8eff210a	service-oriented architecture (soa) concepts and implementations	aws server;available esb solution;soa infrastructure;reliable system;enterprise service bus;esb endpoint;service-oriented architecture;ada application;mule esb;ada web server;web service	This tutorial explains how to implement a Service-Oriented Architecture (SOA) for reliable systems using an Enterprise Service Bus (ESB) and the Ada Web Server (AWS). The first part of the tutorial describes terms of Service-Oriented Architectures (SOA) including service, service registry, service provider, service consumer, Service Oriented Architecture Protocol (SOAP), and Web Service Description Language (WSDL). This tutorial also presents principles of SOA including loose coupling, encapsulation, composability of web services, and statelessness of web services. The tutorial also covers the benefits of SOA and organizations that are supporting SOA infrastructure. The second part of the tutorial covers the Enterprise Service Bus (ESB) including definitions, capabilities, benefits and drawbacks. The tutorial discusses the difference between SOA and an ESB, as well as some of the commercially available ESB solutions on the market. The Mule ESB is explored in more detail and several examples are given. In the third part, the tutorial covers the Ada Web Server (AWS) built using the Ada programming language. The tutorial covers the capabilities of AWS and explains how to build and install AWS. The tutorial explains how to build an AWS server and include the server in an Ada application. The tutorial demonstrates how to build a call back function in AWS and build a response to a SOAP message. Finally, the tutorial explains how to connect an AWS server to an ESB endpoint. AWS is a key component to building a SOA for a reliable system. This capability allows the developer to expose services in a high-integrity system using the Ada and SPARK programming languages.	ada;amazon web services;communication endpoint;composability;data mule;encapsulation (networking);enterprise service bus;loose coupling;programming language;soap;spark;server (computing);service-oriented architecture;service-oriented device architecture;terms of service;web services description language;web server;web service	Ricky E. Sward;Jeff Boleng	2012		10.1145/2402676.2402683		Web+IR	-33.46446504771441	51.1108361912623	89434
63279bee461f310f3a07152a2c9a5135ead23771	qos-based network service broker for high-performance grid applications	grid applications;service orientation;information infrastructure;service oriented grid architecture qos based network service broker quality of service grid computing computer networks grid applications networking systems system architecture;service oriented grid architecture;computer networks;computer network;next generation;quality of service computer networks grid computing;quality of service;network services;system architecture;networked systems;quality of service application software grid computing computer networks computer network management distributed computing service oriented architecture computer architecture computer science next generation networking;grid computing;high performance;networking systems;qos based network service broker	Grid computing is an emerging computing paradigm that have a significant impact on the next generation information infrastructure. Computer networks and the Grid should be integrated together for supporting high-performance applications and a QoS-based network service broker is the key to this integration. In this paper, we propose a new approach for describing QoS capabilities of network services and develop a QoS-based broker technology for selecting network services that meet QoS requirements of grid applications. Our approach for network service description and selection is highly flexible and applicable to networking systems with various implementations. We also propose a system architecture for realizing the QoS-based network service broker within the service-oriented grid architecture.	embedded system;grid computing;grid network;network calculus;performance;programming paradigm;provisioning;quality of service;reachability;requirement;service discovery;service-oriented architecture;service-oriented device architecture;systems architecture	Qiang Duan;Michelle Talley;Neeta Seetha	2007	21st International Conference on Advanced Information Networking and Applications (AINA '07)	10.1109/AINA.2007.117	information infrastructure;broker pattern;element management system;quality of service;computer science;distributed computing;world wide web;grid computing;computer network;systems architecture	HPC	-33.663463978113974	47.50709300294127	89461
5fea4223b952df7318e732f42b9eb9c1b05957c5	supporting adaptable applications in grid resource management systems	dynamic collection;grid computing;grid resource management system;computational resource;initial modification;grid resource manager;additional modification;adaptable application;inside grid;autonomous dynamic adaptability;unmodified infrastructure;dynamic grid environment;satisfiability;resource allocation;resource manager;cost efficiency;user requirements	Grid computing promises to bring the resources to satisfy the increasing requirements of scientific applications. As grids result from several organizations that pool their computational resources, resource availability varies frequently inside grids. Relying on autonomous dynamic adaptability and managing dynamic collections of resources, technologies have been proposed in order to handle those variations at the level of applications. However, despite applications have evolved in order to fit better dynamic grid environments, grid resource managers still restrict to rigid jobs, thus inhibiting application adaptability and malleability. This paper discusses 3 options to overcome that restriction. Malleable job management can be built on top of existing unmodified infrastructures. It can also be implemented as a modification of the infrastructure. At last, we propose an intermediate approach that fosters the cooperation between the infrastructure and its users. Requiring an initial modification of the infrastructure, the latter design combines cost efficiency with possibility to further extend the job model without any additional modification of the infrastructure. In the discussion, qualitative arguments arc supported by some experimental results.	autonomous robot;computational resource;cost efficiency;grid computing;job stream;requirement	Jérémy Buisson;Françoise André;Jean-Louis Pazat	2007	2007 8th IEEE/ACM International Conference on Grid Computing		real-time computing;resource allocation;computer science;knowledge management;resource management;user requirements document;distributed computing;management;grid computing;cost efficiency;satisfiability	HPC	-31.570634506306156	53.708938959156875	89553
156070559d47d5d59d6c594d5cdd70ce262cabf5	partitioning of distributed virtual environments based on objects' attributes	virtual environment avatars partitioning algorithms distributed computing informatics network servers computational modeling computer simulation real time systems application software;avatar behavior;virtual reality;distributed virtual environments;objects attributes;avatar behavior distributed virtual environments objects attributes virtual scene partitioning problem;partitioning problem;distributed virtual environment;virtual scene;virtual environment;virtual space	The partitioning constitutes one of the basic problems that need to be handled in Distributed Virtual Environments for maintaining consistency among users' view of the virtual scene and for optimizing the performance of the system. This paper presents an approach for handling the partitioning problem, which is based on objects' attributes. In particular, the approach proposes a technique, which takes into account the degree of interaction that the objects of the virtual environment carry for predicting future avatar behavior. This prediction of avatars' behavior based on objects' attributes defines the initial partitioning of the virtual space.	partition problem;virtual reality	Pedro Northon Nobile;Roberto R. F. Lopes;Célio Estevan Morón;Luís Carlos Trevelin	2007	11th IEEE International Symposium on Distributed Simulation and Real-Time Applications (DS-RT'07)	10.1109/DS-RT.2007.34	simulation;human–computer interaction;computer science;instructional simulation;kernel virtual address space;multimedia;virtual finite-state machine	Visualization	-27.711691793668695	49.48600341659542	89958
9795af92fdccf440a9f1a9e8649aac625706688c	a hybrid gpu cluster and volunteer computing platform for scalable deep learning	cluster computing;volunteer computing;deep learning	Deep learning is a very computing-intensive and time-consuming task. It needs an amount of computing resource much greater than a single machine can afford to train a sophisticated model within a reasonable time. Normally, GPU clusters are required to reduce the training time of a deep learning model from days to hours. However, building large dedicated GPU clusters is not always feasible or even ineffective for most organizations due to the cost of purchasing, operation and maintenance while such systems are not fully utilized all the time. In this regard, volunteer computing can address this problem as it provides additional computing resources at less or no cost. This work presents the hybrid cluster and volunteer computing platform that scales out GPU clusters into volunteer computing for distributed deep learning. The owners of the machines contribute unused computing resources on their computers to extend the capability of the GPU cluster. The challenge is to seamlessly align the differences between GPU cluster and volunteer computing systems so as to ensure the scalability transparency, whereas performance is also another major concern. We validate the proposed work with two well-known sample cases. The results show an efficient use of our hybrid platform at sub-linear speedup.	align (company);computer;deep learning;gpu cluster;graphics processing unit;purchasing;scalability;speedup;volunteer computing	Ekasit Kijsipongse;Apivadee Piyatumrong;Suriya U.-ruekolan	2018	The Journal of Supercomputing	10.1007/s11227-018-2375-9	computer science;gpu cluster;parallel computing;distributed computing;deep learning;speedup;cluster (physics);scalability;transparency (graphic);computer cluster;artificial intelligence;purchasing	HPC	-25.107604103633285	59.45039042540391	90203
3e42d60afaf4f204e4ecc9764543294a2530da48	scheduler support for video-oriented multimedia on client-side virtualization	cpu scheduling;virtual machine;virtualization;multimedia;xen;scheduling;client side virtualization;multimedia services	Virtualization has recently been adopted for client devices to provide strong isolation between services and efficient manageability. Even though multimedia service is not rare for the devices, the virtual machine hosting this service is not guaranteed to receive proper scheduling support from the underlying hypervisor. The quality of multimedia service is often compromised when several virtual machines compete for computing power. This paper presents a new scheduling scheme for the hypervisor to transparently identify if the workload handles multimedia and to provide proper scheduling supports. An implementation of our scheme has shown that the virtual machine hosting a video-oriented application receives propoer CPU scheduling even when other virtual machines host CPU intensive workloads.	central processing unit;client-side;hypervisor;interaction;multi-core processor;scheduling (computing);virtual machine	Hwanju Kim;Jinkyu Jeong;Jeaho Hwang;Joonwon Lee;Seung Ryoul Maeng	2012		10.1145/2155555.2155566	fair-share scheduling;embedded system;full virtualization;real-time computing;virtualization;temporal isolation among virtual machines;computer science;virtual machine;operating system;hardware virtualization;two-level scheduling	OS	-21.902379431305874	60.03772808868641	90347
0c34e00dcd7f15126110b9d430306157a0aae769	basil: automated io load balancing across storage devices	load balance	Live migration of virtual hard disks between storage arrays has long been possible. However, there is a dearth of online tools to perform automated virtual disk placement and IO load balancing across multiple storage arrays. This problem is quite challenging because the performance of IO workloads depends heavily on their own characteristics and that of the underlying storage device. Moreover, many device-specific details are hidden behind the interface exposed by storage arrays. In this paper, we introduce BASIL, a novel software system that automatically manages virtual disk placement and performs load balancing across devices without assuming any support from the storage arrays. BASIL uses IO latency as a primary metric for modeling. Our technique involves separate online modeling of workloads and storage devices. BASIL uses these models to recommend migrations between devices to balance load and improve overall performance. We present the design and implementation of BASIL in the context of VMware ESX, a hypervisor-based virtualization system, and demonstrate that the modeling works well for a wide range of workloads and devices. We evaluate the placements recommended by BASIL, and show that they lead to improvements of at least 25% in both latency and throughput for 80 percent of the hundreds of microbenchmark configurations we ran. When tested with enterprise applications, BASIL performed favorably versus human experts, improving latency by 18-27%.	benchmark (computing);disk image;disk storage;enterprise software;hard disk drive;hypervisor;load balancing (computing);software system;throughput	Ajay Gulati;Chethan Kumar;Irfan Ahmad;Karan Kumar	2010			embedded system;real-time computing;computer hardware;computer science;load balancing;operating system;database	OS	-22.799021358133995	56.48374972908967	90530
730734f3e3f5641a3f7f5a341d4ce75aacd4d85c	cloudtops: latency aware placement of virtual desktops in distributed cloud infrastructures			interrupt latency;virtual desktop	Nishio Yamada;Toshiyuki Moritsu;Kaustubh R. Joshi;Matti A. Hiltunen;Richard D. Schlichting	2013			latency (engineering);cloud computing;virtual desktop;computer science;distributed computing	HPC	-20.458861323126733	59.76206643180118	90969
ecf91364d8875f48a173cf8a45c63aca7899b4d8	access control in a heterogeneous distributed database management system.	distributed database;management system;access control			Ching-Yi Wang;David L. Spooner	1987			database server;database tuning;computer science;access control;management system;database;distributed system security architecture;distributed database;database testing;replication;distributed concurrency control	DB	-27.982626514778573	46.651241173011314	91079
1a511e91c711401c8fe4251513e5f38009eff9c8	a sledgehammer approach to reuse of legacy device drivers	virtual machine;operating system;device driver	Device drivers account for the majority of an operating system's code base, and reuse of the existing driver infrastructure is a pragmatic requirement of any new OS project. New operating systems should benefit from the existing device driver code base without demanding legacy support from the kernel.Instead of trying to directly integrate existing device drivers we propose a more radical approach. We run the unmodified device driver, with its complete original OS, isolated in a virtual machine. Our flexible approach, requiring only minimal support infrastructure, allows us to run any existing device driver, independently of the OS or driver vendor.	device driver;legacy system;operating system;virtual machine	Joshua LeVasseur;Volkmar Uhlig	2004		10.1145/1133572.1133617	embedded system;real-time computing;engineering;operating system	OS	-26.500593204002126	50.718130198060564	91389
bc93b0782abf480ae98c50d62cac3283adbc12ba	bounds on the effects of replication on availability	distributed databases;availability;mutually exclusive access;performance measures;replica consistency model;replication	I n this p a p e r we discuss some theoretical limitattons on the potential benefits of replication. I n particular, we investigate two fundamental questions: (1) does placing copies of data around a network increase the probability that the data will be available, and (2) does such a technaque decrease the mean duration of unavaalability of the data. Given that many applications require mutually exclusive access to the data, we show that the potential benefits of replication are rather low with respect to both of these metrics. Although these results are not necessarily surprising, it is interesting that the proofs are protocol independent and, in the case of the availability measure, topology independent. They are useful, therefore, in focusing attention on the replica consistency model and performance measures and away from any particular protocol or network co nfig u rat a o n .	consistency model	Larry Raab	1992			computer science;data mining;database;distributed computing	Theory	-21.572360260373845	46.596684007322146	91525
1e32492f456bcf58d07b1658825733dbfb9d816c	clock-si: snapshot isolation for partitioned data stores using loosely synchronized clocks	silicon;protocols;transaction processing benchmark testing clocks distributed processing probability protocols software performance evaluation synchronisation;snapshot isolation;probability;clocks;clock si read only transactions partitioned key value store application level benchmark microbenchmark transaction delay probabilities transaction throughput transaction latency snapshot timestamp centralized timestamp authority distributed protocol loosely synchronized clocks partitioned data stores snapshot isolation;distributed processing;distributed transactions;software performance evaluation;loosely synchronized clocks;synchronisation;synchronization;clocks silicon synchronization protocols delays distributed databases;distributed databases;loosely synchronized clocks snapshot isolation distributed transactions partitioned data;partitioned data;transaction processing;benchmark testing;delays	Clock-SI is a fully distributed protocol that implements snapshot isolation (SI) for partitioned data stores. It derives snapshot and commit timestamps from loosely synchronized clocks, rather than from a centralized timestamp authority as used in current systems. A transaction obtains its snapshot timestamp by reading the clock at its originating partition and Clock-SI provides the corresponding consistent snapshot across all the partitions. In contrast to using a centralized timestamp authority, Clock-SI has availability and performance benefits: It avoids a single point of failure and a potential performance bottleneck, and improves transaction latency and throughput. We develop an analytical model to study the trade-offs introduced by Clock-SI among snapshot age, delay probabilities of transactions, and abort rates of update transactions. We verify the model predictions using a system implementation. Furthermore, we demonstrate the performance benefits of Clock-SI experimentally using a micro-benchmark and an application-level benchmark on a partitioned key-value store. For short read-only transactions, Clock-SI improves latency and throughput by 50% by avoiding communications with a centralized timestamp authority. With a geographically partitioned data store, Clock-SI reduces transaction latency by more than 100 milliseconds. Moreover, the performance benefits of Clock-SI come with higher availability.	benchmark (computing);centralized computing;data store;experiment;key-value database;read-only memory;reliability engineering;single point of failure;snapshot (computer storage);snapshot isolation;throughput;timestamp-based concurrency control	Jiaqing Du;Sameh Elnikety;Willy Zwaenepoel	2013	2013 IEEE 32nd International Symposium on Reliable Distributed Systems	10.1109/SRDS.2013.26	synchronization;parallel computing;real-time computing;computer science;operating system;database;distributed computing;distributed database;computer network	OS	-21.522575519902155	49.4634104803788	91605
06682230b5bdffc22b6a1f81aadaebffd13d3dc3	hypervisors and virtual machines: implementation insights on the x86 architecture				Don Revelle	2011	;login:			OS	-28.068895596420017	56.34053385858291	91627
043f4249c68eb5803f5c7f2c3efa591bb98a18f9	building and evaluating p2p systems using the kompics component framework	p2p system;deployment;protocols;high level languages;local execution;real time;simulation;protocols distributed object management high level languages peer to peer computing;single machine execution;computer and information science;p2p;dynamic system;peer to peer system;cyclon protocol;network distribution;design evaluation;system evaluation;servers;single machine;monitoring;component framework;reproducible simulation;distributed object management;component oriented design;deployment peer to peer evaluation component framework design simulation experimentation;web based interfaces;component model;design;evaluation;peer to peer computing;component architecture;data och informationsvetenskap;cyclon protocol peer to peer system simulation local execution distributed deployment kompics component framework reproducible simulation single machine execution network distribution component oriented design web based interfaces chord protocol;experimentation;peer to peer;chord protocol;buildings;wide area network;component architectures;distributed deployment;real time systems;kompics component framework;java protocols component architectures real time systems monitoring stochastic processes computer science silicon carbide buildings computational modeling;dynamic behavior	We present a framework for building and evaluating P2P systems in simulation, local execution, and distributed deployment. Such uniform system evaluations increase confidence in the obtained results. We briefly introduce the Kompics component model and its P2P framework. We describe the component architecture of a Kompics P2P system and show how to define experiment scenarios for large dynamic systems. The same experiments are conducted in reproducible simulation, in real-time execution on a single machine, and distributed over a local cluster or a wide area network. This demonstration shows the component oriented design and the evaluation of two P2P systems implemented in Kompics: Chord and Cyclon. We simulate the systems and then we execute them in real time. During real-time execution we monitor the dynamic behavior of the systems and interact with them through their web-based interfaces. We demonstrate how component-oriented design enables seamless switching between alternative protocols.	component-based software engineering;dynamical system;experiment;peer-to-peer;real-time clock;real-time computing;seamless3d;simulation;software deployment;web application	Cosmin Arad;Jim Dowling;Seif Haridi	2009	2009 IEEE Ninth International Conference on Peer-to-Peer Computing	10.1109/P2P.2009.5284499	design;real-time computing;computer science;theoretical computer science;evaluation;operating system;database;distributed computing;world wide web;computer network	Embedded	-32.95503864181429	46.89698408088412	91628
93de7148d6d3a9447c5e09c74602aad0a796e471	approximate consistency in transactional memory		In Transactional Memory each shared object can be accessed by concurrent transactions which may cause conflicts and aborts. Opacity is a precise consistency property which maps a concurrent execution to a legal sequential execution that preserves the real time order of events. However, having precise consistency in large scale network and database applications may result in a large rate of aborts, especially in systems that have frequent memory updates. Actually, high rate of aborts causes huge negative performance impact. In real applications, there are systems that do not require precise consistency especially when the data is not sensitive. Thus, we introduce here the notion of approximate consistency in transactional memory. We define K-opacity as a relaxed consistency property where transactions are allowed to read one of the K most recent written values to the objects (and not just the latest value only). This increases the throughput and reduces the abort rate by reducing the chance of conflicts. In multi-version transactional memory, the relaxed consistency allows to save a new object version once every K object updates, and hence, reduces space requirements by a factor of K. In fact, we apply the concept of K-opacity to regular read/write, count, and queue objects, which are common objects used in typical concurrent programs. We use the technique of writer lists to keep track of the transactions and the data being written to the system, in order to control the error rate and to prevent error propagation. We illustrate with an experimental analysis the positive impact of our approach on performance, where higher opacity relaxation (higher values of K) increases the throughput and decreases the aborts rate significantly.	approximation algorithm;commitment scheme;composability;correctness (computer science);library (computing);linear programming relaxation;map;propagation of uncertainty;requirement;software propagation;throughput;transactional memory	Basem Assiri;Costas Busch	2018	IJNC		abort;throughput;real-time computing;word error rate;propagation of uncertainty;queue;opacity;transactional memory;computer science	PL	-22.88408341903981	48.20015358586558	91701
6d0b830690e5598ad5ed01382b92cbb4cebcebaf	supplying high availability with a standard network file system	high availability;file servers;protocols;network file service;highly available files;fault tolerant;file systems file servers network servers broadcasting access protocols design methodology voting sun fault tolerance counting circuits;heterogeneous workstations;network file system;counting circuits;network servers;fault tolerant computing;voting;protocols fault tolerant computing file servers local area networks;file system;fault tolerance;sun nfs protocol;sun;access protocols;fail stop failures;technical report;computer science;broadcasting;local area networks;client server model network file service fail stop failures fault tolerance heterogeneous workstations highly available files sun nfs protocol;file systems;client server model;design methodology	This paper describes the design of a network le service that is tolerant to fail-stop failures and can be run on top of a standard network le service. The fault-tolerance is completely transparent , so the resulting le system supports the same set of heterogeneous workstations and applications as the chosen standard. To demonstrate that our design can provide the beneet of highly available les at a reasonable cost to the user, we implemented a prototype based on the Sun NFS protocol. Our approach is not limited to being used with NFS, however. And, the methodology used should apply to any network le service built along the client-server model.	client-side;client–server model;expect;fail-stop;fault tolerance;finite-state machine;high availability;isis;image scaling;password;prototype;self-replicating machine;server (computing);workstation	Keith Marzullo;Frank B. Schmuck	1988		10.1109/DCS.1988.12547	fork;self-certifying file system;file server;fault tolerance;real-time computing;torrent file;device file;computer file;network file system;computer science;stub file;versioning file system;operating system;unix file types;ssh file transfer protocol;journaling file system;database;distributed computing;open;distributed file system;file system fragmentation;global namespace;computer network;file control block;virtual file system	Networks	-22.330672620586224	50.712539866347065	91750
1e91c96cc639c612f4f0bb567e176dd9598abd62	a distributed and parallel component architecture for stream-oriented applications	distributed application;american sign language;proof of concept;single machine;execution environment;component architecture;reusable component;gesture recognition	This paper introduces ThreadMill a distributed and parallel component architecture for applications that process large volumes of streamed (time-sequenced) data, such as is the case e.g. in speech and gesture recognition applications. Many stream-oriented applications offer ample opportunity for enhanced performance via concurrent execution, exploring a wide variety of parallel paradigms, such as task, data and pipeline parallelism. ThreadMill addresses the challenges of development and evolution of parallel and distributed applications in this domain by offering a modeling formalism, a programming framework and a runtime infrastructure. Component development and reuse, and application evolution are facilitated by the isolation of communication, concurrency, and synchronization concerns promoted by ThreadMill: 1) communication between components is mediated, so that components are oblivious to who their peers in an application are, allowing them to be composed in unanticipated ways, in different contexts; 2) concurrency is exogenous to components and can be controlled via an integrated staging mechanism that affords detailed control of distribution across multiple address spaces and/or concurrency within each address space; 3) synchronization is effected via an extensible set of orchestration operators, that embed recurrent coordination patterns that are matched to the requirements of stream-oriented applications. A direct consequence of the novel mechanisms introduced by ThreadMill is that applications composed of reusable components can be re-targeted, unchanged, and made to run efficiently on a variety of execution environments. These environments can range e.g. from a single machine with a single processor, to a cluster of heterogeneous computational nodes, to certain classes of supercomputers. Experimental results show an eightfold speedup when using ten nodes of an AlphaServer DS20 cluster running a proof-ofconcept 2D video-based tracker for hands and face of American Sign Language signers.	alphaserver;assignment (computer science);attachments;component-based software engineering;computation;concurrency (computer science);data structure;distributed computing;gesture recognition;iterative method;load balancing (computing);mathematical optimization;neptune;parallel computing;pipeline (computing);quality of service;reflection (computer programming);run-time infrastructure (simulation);semantics (computer science);speedup;streaming media;supercomputer;synchronization (computer science)	Paulo Barthelmess;Clarence A. Ellis	2004		10.1007/978-3-540-30469-2_38	real-time computing;common component architecture;computer science;applications architecture;gesture recognition;component;distributed computing;programming language;proof of concept	HPC	-23.886665487629394	53.1049407014608	91836
728777a3a62484332fbd770034f7e82079b8a657	a redundancy protocol for service-oriented architectures	high availability;service provider;service orientation;computer system architecture;service oriented architecture	Achieving high-availability in service-oriented systems is a challenge due to the distributed nature of the architecture. Redundancy, using replicated services, is a common software strategy for improving the availability of services. However, traditional replication strategies are not appropriate for service-oriented systems, where diverse services may be grouped together to provide redundancy. In this paper we describe the requirements for a redundancy protocol and propose a set of processes to manage redundant service providers.	high availability;requirement;service-oriented architecture;service-oriented device architecture;service-oriented software engineering	Nicholas R. May	2008		10.1007/978-3-642-01247-1_22	service provider;active redundancy;real-time computing;differentiated service;computer science;service delivery framework;service-oriented architecture;service;distributed computing;high availability;law	SE	-32.98353883464288	47.55769265405184	91964
0c713fd981169d29a1b9bb3a36b2fdc27cce81a2	augmented inverted indexes to track causality in eventually consistent data stores	solr;riak;information retrieval	We propose a novel algorithm for providing highly-available and fault-tolerant distributed search queries in a Dynamo-inspired distributed data store. It leverages the use of advanced causality tracking mechanisms to allow quorum reads of an inverted index, while maintaining the ability to reason about the correct response. This algorithm improves on existing distributed search mechanisms by providing higher availability in the event of failures, given the use of multiple replicas.	algorithm;causality;data store;distributed web crawling;eventual consistency;fault tolerance;inverted index;web search query	Christopher Meiklejohn	2014		10.1145/2678508.2678512	computer science;data mining;database;distributed computing	Web+IR	-21.62432099626028	49.10688700305597	92279
a93192fc5af1286e17ec6fb0fba1521c0c68fe3a	applying the fully-informed checkpointing protocol to the lazy indexing strategy	dom- ino effect;fault tolerant computing;checkpointing protocols;rollback-recovery;distributed systems;fault tolerant;simulation experiment;domino effect;indexation;distributed system;distributed computing	Communication-induced checkpointing (CIC) protocols can be used to prevent domino effect. The fully-informed (FI) protocol proposed in the literature is known as the best CIC protocol so far. Such a protocol is originally designed according to the classical indexing strategy. In this paper, we show how to apply the FI protocol to another improved indexing strategy, called lazy indexing strategy in the context. The lazy-indexing version of such a protocol can reduce number of forced checkpoints for a distributed computation with the lazy indexing strategy. Particularly, the management of a boolean array in FI’s control information carried on a message must be modified. Finally, we present a simulation experiment to analyze the impact of such a modification.	application checkpointing;bit array;communications protocol;computation;distributed computing;lazy evaluation;mathematical optimization;simulation;transaction processing system	Jichiang Tsai	2007	J. Inf. Sci. Eng.			DB	-21.281114893195394	48.373070345002574	92488
304b959f708400d779e6412d93609f3eafcc080c	a fair locking protocol for multilevel secure databases	phase locking;protocols;protocols data security concurrency control transaction databases delay computer security concurrent computing timing information systems computer science;protocols distributed databases concurrency control security of data transaction processing;workloads fair locking protocol multilevel secure databases concurrency control algorithms kernelized architecture covert channels security levels high security transaction data conflict lower security transaction concurrency control mechanisms secure databases secure version two phase locking starvation high security transactions simulation study fairness performance;covert channel;concurrency control;distributed databases;transaction processing;security of data;multilevel security	Most concurrency control algorithms for multilevel secure databases based on kernelized architecture prevent covert channels between transactions at different security levels by preempting the high security transaction in the event of a data conflict with a lower security transaction. In environments with moderate to high levels of contention between low and high security transactions, this can lead to poor performance and even starvation of high security transactions. In this paper, we examine this problem of unfairness in concurrency control mechanisms for secure databases. Based on an analysis of the performance of a secure version of twophase locking, we propose three different modifications to the protocol that address the problem of starvation of high security transactions. Through a detailed simulation study, we examine the fairness and performance of these approaches for a variety of workloads.	algorithm;concurrency (computer science);concurrency control;control system;covert channel;database;denial-of-service attack;fairness measure;kernel method;lattice-based access control;lock (computer science);multilevel security;run time (program lifecycle phase);simulation;two-phase commit protocol;two-phase locking	Sushil Jajodia;Luigi V. Mancini;Sanjeev Setia	1998		10.1109/CSFW.1998.683167	computer security model;timestamp-based concurrency control;communications protocol;optimistic concurrency control;isolation;transaction processing;covert channel;distributed transaction;computer science;two-phase locking;concurrency control;database;security service;distributed computing;non-lock concurrency control;serializability;distributed database;computer security;acid;distributed concurrency control	DB	-23.332614241558424	48.03795859261563	92490
176ea05d10794c855812472ee9db1ff732cd0e81	a new availability concept for (n, k)-way cluster systems regarding waiting time	high availability;satisfiability;performance metric;waiting time;cluster system;quantitative evaluation	It is necessary to have the precise definition of available performance of high availability systems that can represent the availability and performability of the systems altogether. However, the difference between numeric scales of availability and performance metrics such as waiting time makes quantitative evaluation difficult. A number of previous studies on availability do not include a performance metric in their availability modeling. In this paper, we propose a new availability model for (n,k)-way cluster systems which compose of n primary servers and k backup servers and compute the redundancy levels for the systems to satisfy both the requirements of availability and waiting time performance.	backup;clustered file system;fault detection and isolation;high availability;load balancing (computing);loose coupling;requirement;steady state;trusted computer system evaluation criteria	Kiejin Park;Sungsoo Kim;Jyh-Charn Liu	2003		10.1007/3-540-44839-X_105	real-time computing;computer science;operating system;distributed computing;high availability;satisfiability	Metrics	-22.2580227568181	52.86697795835079	92950
1c621ac7a8ea0a96b9a76876e451822a92791644	c2cfs: a collective caching architecture for distributed file access	scientific application;cache storage;data sharing;protocols;kernel;cooperative caching;information retrieval;software architecture cache storage client server systems information retrieval;data serving role;nfs;consistency management role;client server systems;decentralized data access;spectrum;data mining;software architecture;servers;time factors;client server;direct client to client transfers;distributed file access;decentralized data access distributed file access decentralized collective caching architecture client server model consistency management role data serving role direct client to client transfers protocol;consistency management;data access;linux;network servers file servers access protocols delay computer architecture wide area networks high performance computing data engineering reliability engineering design engineering;decentralized collective caching architecture;high performance;nfs distributed file access cooperative caching;wide area network;wide area networks;client server model;protocol	In this paper we present C2Cfs - a decentralized collective caching architecture for distributed filesystems. C2Cfs diverges from the traditional client-server model and advocates decoupling the consistency management role of the central server from the data serving role. Our design enables multiple client-side caches to share data and efficiently propagate updates via direct client-to-client transfers, while maintaining the standard consistency semantics. We present an NFSv4-based implementation of our architecture, which works with unmodified NFS servers and requires no changes to the protocol. Finally, we evaluate the implementation and demonstrate the performance benefits of decentralized data access enabled by our approach.	cache (computing);client-side;client–server model;coupling (computer programming);data access;direct client-to-client;server (computing)	Andrey Ermolinskiy;Renu Tewari	2009	2009 11th IEEE International Conference on High Performance Computing and Communications	10.1109/HPCC.2009.90	computer science;operating system;database;distributed computing;world wide web;client–server model;computer network	HPC	-21.83854892123048	52.33000024514119	93215
0c0b97de89ed5d8585fbafce88ff505409e7e700	ocp: a distributed real time commit protocol	phase locking;cohort;workdone message;database system;real time;system performance;commit;data access;workstarted message;distributed real time database systems	Most of the existing commit protocols try to improve the system performance by allowing a committing cohort to lend its data to an executing cohort, thus reducing data inaccessibility. However, these protocols block the borrower when it tries to send WORKDONE/PREPARED message (Qin & Liu 2003, Haritsa, Ramamritham & Gupta 2000, Gupta, Haritsa, Ramamritham & Seshadri 1996, Gupta, Haritsa, & Ramamritham 1997), thus increasing the transactions commit time. This paper first analyzes all kind of dependencies that may arise due to data access conflicts in executing-committing transaction when a committing cohort is allowed to lend its data to an executing cohort, and then proposes a static two phase locking based optimistic commit protocol i.e. OCP. In OCP, the execution phase of a cohort is divided into two parts locking phase and processing phase and then, in place of WORKDONE message, WORKSTARTED message is sent just before the start of processing phase of the cohort. Again, in case of dependency, borrower with only commit dependency is allowed to send WORKSTARTED message instead of being blocked. This reduces the time needed for commit processing and is free from cascaded aborts. To ensure non-violation of ACID properties, checking of completion of processing and removal of dependency of cohort are required before sending the Yes-Vote message. The performance of the OCP is also analyzed for partial read-only optimization. .	acid;cascaded integrator–comb filter;data access;granular computing;lock (computer science);mathematical optimization;open core protocol;read-only memory;two-phase commit protocol;two-phase locking	Udai Shanker;Manoj Misra;Anil Kumar Sarje	2006		10.1145/1151736.1151757	three-phase commit protocol;commit;data access;real-time computing;atomic commit;computer science;cohort;database;distributed computing;computer performance;computer security	DB	-22.954184105333045	47.905179537047566	93420
f19308a3120690e65c8f25e3874961cbe8ebe66f	the decentralized file system igor-fs as an application for overlay-networks			overlay network	Kendy Kutzner	2008			distributed file system;file system;computer network;global namespace;overlay network;self-certifying file system;computer science	HPC	-19.908098896593923	52.221480326818366	93579
9323c144bd423b7d9354b9c4dd0a445241c5880a	an innovative science gateway for the cherenkov telescope array	cta;science gateways;guse;ws pgrade;acid;workflows;cloud services;creative technologies	The Cherenkov Telescope Array (CTA) is currently building the next generation, ground-based, very high-energy gamma-ray instrumentation. CTA is expected to collect very large datasets (in the order of petabytes) which will have to be stored, managed and processed. This paper presents a graphical user interface built inside a science gateway aiming at providing CTA-users with a common working framework. The gateway is WS-PGRADE/gUSE workflow-oriented and is equipped with a flexible SSO (based on SAML) to control user access for authentication and authorization. An interactive desktop environment is provided, called Astronomical & Physics Cloud Interactive Desktop (ACID). Users are able to exploit the graphical interface as provided natively by the tools included in ACID. A cloud data service shares and synchronizes data files and output results between the user desktop and the science gateway. Our solution is a first attempt towards an ecosystem of new technologies with a high level of flexibility to suit present and future requirements of the CTA community.	acid;authentication;authorization;central processing unit;cloud computing;desktop computer;ecosystem;graphical user interface;high-level programming language;java portlet specification;petabyte;random-access memory;requirement;saml 2.0;security assertion markup language;shibboleth;software project management;user requirements document;workflow engine	Alessandro Costa;Piero Massimino;Marilena Bandieramonte;Ugo Becciani;Mel Krokos;Costantino Pistagna;Simone Riggi;Eva Sciacca;Fabio Vitello	2015	Journal of Grid Computing	10.1007/s10723-015-9330-2	workflow;simulation;cloud computing;computer science;operating system;database;distributed computing;world wide web;computer security;acid	HPC	-32.01691126470287	52.49411199969448	93590
2b2f302b3c4ea773fef0dba8f829c66c4a4c549e	distributed processing of time-constrained queries in case-db	memory management;algorithm analysis;distributed processing;association rules;data mining;background;optimization;scalability	We develop and experimentally evaluate a real-time distributed relational prototype database system that permits the specification of time constraints for relational algebra queries and evaluates queries in parallel over distributed sites in a completely replicated data environment. We develop, implement and evaluate on multiple server sites iterative distributed query evaluation techniques. The risk of overspending the time constraint at each iteration is controlled using a probabilistic risk cent rol technique. The query execution plan is designed to restructure a query into an equivalent, esay-to-parallelize form, and distributed execution is achieved by multi-threaded processes and RPC calls.	algorithm;database;experiment;iteration;prototype;query plan;real-time clock;relational algebra;remote procedure call;server (computing);thread (computing)	Sungkil Lee;Gultekin Özsoyoglu	1996		10.1145/238355.238557	distributed algorithm;scalability;association rule learning;computer science;theoretical computer science;data mining;database;memory management	DB	-22.67891372363188	48.311035634441645	93733
e4e46e4c8cafa82e024f7f0f7dc27b96288dc1f6	gsaf grid storage access framework	internet authorisation grid computing;access grid;grid storage access framework;authorisation;software utility;software development grid storage access framework web application software utility grid user communities;internet;web application;software development;application software internet testing embedded software software tools computer architecture grid computing information retrieval nuclear physics instruments;grid user communities;grid computing	This work has been focused on the porting activities of classical web application from the standard internet infrastructure to the newest Grid Infrastructure. GSAF framework, here presented and discussed, provides both a software utility to allow application to manage data on Grid and a friendly Web based instrument to access Grid data remotely. This product is dedicated to the Grid user communities, software developers and last but not least commercial structures interested on developing new business.	access grid;software developer;web application	Salvatore Scifo	2007	16th IEEE International Workshops on Enabling Technologies: Infrastructure for Collaborative Enterprises (WETICE 2007)	10.1109/WETICE.2007.147	web application;the internet;semantic grid;computer science;software framework;software development;operating system;software construction;data grid;database;authorization;programming language;world wide web;drmaa;grid computing	HPC	-32.19031249251679	50.749585499702505	94042
7450286e9560267ae449d6adcaf28faa8b5e583c	visual methods for parallel and distributed programming	distributed programs		distributed computing	Guido Wirtz;Kang Zhang	2001	J. Vis. Lang. Comput.	10.1006/jvlc.2000.0190	distributed algorithm;computer science;distributed computing;distributed design patterns;distributed concurrency control	HPC	-29.308576670313446	46.56728323746038	94359
e78a85fb2e12f73d37cfbd87441d0fc1d2946f8c	setting cloud standards in a new world	technological innovation;standards;cloud;cloud computing technological innovation standards development;standards development;compliance;compliance standards cloud;cloud computing	"""The ongoing process of defining, developing, and recognizing standards is intrinsically a community activity. As standards are developed and implemented in the emerging cloud computing landscape, the """"Standards and Compliance"""" area will serve as a forum for describing not only the standards that are seeing use, but also the process by which they're developed to the point that they can see the light of day."""	cloud computing	Alan Sill	2014	IEEE Cloud Computing	10.1109/MCC.2014.21	simulation;cloud computing;computer science;operating system	Visualization	-32.42689384150924	55.042332943732376	94397
7129aeadd7844d2ca4fb83f9eaf9a057c263126a	virtual organization clusters	scientific application;kernel based virtual machine;virtual machine;virtual networks;cluster computing;cluster;end user computing;performance test;system configuration;magnetic heads;hardware head organizations magnetic heads fabrics virtual machining virtual machine monitors;real time;virtual machining;cluster computing system;grid virtual machine virtual organization cluster;network performance;multiprogramming systems;latency tolerance;virtual machines grid computing;grid;virtual machine monitors;software requirements;selective availability;operating system;virtual machines;kernel based virtual machine virtual organization clusters traditional clusters sharing multiprogramming systems cluster computing system operating system software selection physical fabric provider high throughput computing;virtual organization;high performance computer;fabrics;head;organizations;cluster model;traditional clusters sharing;physical fabric provider;grid computing;software selection;virtual organization clusters;high throughput computing;hardware	Sharing traditional clusters based on multiprogramming systems among different Virtual Organizations (VOs) can lead to complex situations resulting from the differing software requirements of each VO. This complexity could be eliminated if each cluster computing system supported only a single VO, thereby permitting the VO to customize the operating system and software selection available on its private cluster. While dedicating entire physical clusters on the Grid to single VOs is not practical in terms of cost and scale, an equivalent separation of VOs may be accomplished by deploying clusters of Virtual Machines (VMs) in a manner that gives each VO its own virtual cluster. Such Virtual Organization Clusters (VOCs) can have numerous bene?ts, including isolation of VOs from one another, independence of each VOC from the underlying hardware, allocation of physical resources on a per-VO basis, and clear separation of administrative responsibilities between the physical fabric provider and the VO itself.Initial results of implementing a complete system utilizing the proposed Virtual Organization Cluster Model con?rm the administrative simplicity of isolating VO software from the physical system. End-user computational jobs submitted through the Grid are executed only on the virtual cluster supporting the respective VO, and each VO has substantial administrative ?exibility in terms of software choice and system con?guration. Performance tests using the Kernel-based Virtual Machine (KVM) hypervisor indicated a virtualization overhead of under 10% for latency-tolerant scienti?c applications, such as those that would be submitted to a standard or vanilla Condor universe. Latency-sensitive applications, such as MPI, experience substantial performance degradation with virtualization overheads on the order of 60%. These results suggest that VOCs are suitable for High-Throughput Computing (HTC) applications, where real-time network performance is not critical. VOCs might also be useful for High-Performance Computing (HPC) applications if virtual network performance can be sufficiently improved.	computer cluster;computer multitasking;elegant degradation;high-throughput computing;hypervisor;job stream;message passing interface;network performance;operating system;overhead (computing);real-time clock;real-time computing;requirement;software requirements;systems architecture;throughput;virtual machine;virtual organization (grid computing)	Michael A. Murphy;Michael Fenn;Sebastien Goasguen	2009	2009 17th Euromicro International Conference on Parallel, Distributed and Network-based Processing	10.1109/PDP.2009.23	embedded system;parallel computing;real-time computing;computer science;virtual machine;operating system;distributed computing;computer network	HPC	-23.12367997648415	54.62034961161494	94588
d8384dde63c2137cd7443192a5ab135d4ce96cf6	reeact: a customizable virtual execution manager for multicore platforms	application management;resource manager;resource management;chip multiprocessor;virtual execution environment;operating system;execution environment;adaptive applications;power consumption;dynamic adaptation;run time adaptation;management policy	With the shift to many-core chip multiprocessors (CMPs), a critical issue is how to effectively coordinate and manage the execution of applications and hardware resources to overcome performance, power consumption, and reliability challenges stemming from hardware and application variations inherent in this new computing environment. Effective resource and application management on CMPs requires consideration of user/application/hardware-specific requirements and dynamic adaption of management decisions based on the actual run-time environment. However, designing an algorithm to manage resources and applications that can dynamically adapt based on the run-time environment is difficult because most resource and application management and monitoring facilities are only available at the operating system level. This paper presents REEact, an infrastructure that provides the capability to specify user-level management policies with dynamic adaptation. REEact is a virtual execution environment that provides a framework and core services to quickly enable the design of custom management policies for dynamically managing resources and applications. To demonstrate the capabilities and usefulness of REEact, this paper describes three case studies--each illustrating the use of REEact to apply a specific dynamic management policy on a real CMP. Through these case studies, we demonstrate that REEact can effectively and efficiently implement policies to dynamically manage resources and adapt application execution.	algorithm;application lifecycle management;manycore processor;multi-core processor;operating system;requirement;runtime system;stemming;user space;utility	Wei Wang;Tanima Dey;Ryan W. Moore;Mahmut Aktasoglu;Bruce R. Childers;Jack W. Davidson;Mary Jane Irwin;Mahmut T. Kandemir;Mary Lou Soffa	2012		10.1145/2151024.2151031	parallel computing;real-time computing;resource management;operating system;application lifecycle management	OS	-22.00627436465766	57.99176917117594	94835
6217c76a1f65d2703a4748061a2f24a3ca5b9a89	clustering the cloud - a model for (self-)tuning of cloud data management systems		Popularity and complexity of cloud data management systems are increasing rapidly. Thus providing sophisticated features becomes more important. The focus of this paper is on (self-)tuning where we contribute the following: (1) we illustrate why (self-)tuning for cloud data management is necessary but yet a much more complex task than for traditional data management, and (2) propose an model to solve some of the outlined problems by clustering nodes in zones across data management layers for applications with similar require-	cloud database;cloud storage;cluster analysis;high-availability cluster;management system;mathematical optimization	Siba Mohammad;Eike Schallehn;Sebastian Breß	2013			cloud computing;computer science;distributed computing;cluster analysis;popularity;data management;self-tuning	DB	-20.68964011861263	59.34119828406253	94853
8cf45ef6ad72520c5b42c709b5df85717ba64416	using a policy language to control tuple-space synchronization in a mobile environment	disconnected operation;formal specification;data integrity;data synchronization;synchronisation data integrity formal specification middleware mobile communication mobile computing;distributed platform;tuple space;middleware mobile communication file systems network servers intelligent networks computer networks distributed computing base stations ad hoc networks data mining;qa 76 software;ad hoc network;policy language;information sharing;synchronisation;computer programming;mobile environment;application requirements;base station;synchronization;mobile communication;data consistency policy language tuple space synchronization control mobile environment information sharing distributed platform network access mobile communication data synchronization application requirements policy based pervasive middleware;middleware;policy based pervasive middleware;tuple space synchronization control;software requirements and specifications;mobile computing;data consistency;network access	Any sharing of information using a distributed platform carries the risk of disconnection because of loss of network access. This is particularly the case when considering mobile communication, either using base stations or by forming ad-hoc networks. Replication of shared data is one way to increase data availability in such an environment, but leads to the problem of inconsistency between copies of data, and so requires some means of data synchronization. This paper investigates how policies can be used to resolve data conflict in a way that can be tailored to meet the needs of different types of application in different situations. It discusses a range of application requirements, and describes a policy-based pervasive middleware to support the sharing of data using a tuple-space paradigm. Policies maintained within the middleware are used to trigger a wide range of synchronization options to restore the consistency of the data after periods of disconnected operation	access network;compiler;data synchronization;hoc (programming language);middleware;programming paradigm;real life;requirement;run time (program lifecycle phase);test automation;tuple space	Vorapol Jittamas;Peter F. Linington	2006	Seventh IEEE International Workshop on Policies for Distributed Systems and Networks (POLICY'06)	10.1109/POLICY.2006.38	real-time computing;computer science;database;distributed computing;data synchronization;synchronization	Embedded	-25.42574181447526	48.29021115273387	94885
a319088c9fa11cb6f48c16f3ece563ffd7835b17	on the potential of optimized data exchange in distributed embedded simulation	virtualization;resource provisioning;servers;computational modeling;synchronization;optimisation discrete event simulation distributed processing electronic data interchange embedded systems;distributed databases;multi tenant data centers;discrete event simulation distributed simulation embedded simulation;embedded simulation;atmospheric modeling;data models distributed databases hardware atmospheric modeling computational modeling servers synchronization;quality of service;distributed simulation;des data exchange optimization distributed embedded simulation discrete event simulation;cloud computing;data models;hardware;discrete event simulation	Simulation is an important and widely used method for the analysis of the behavior of large systems, many applications exist. Special branches of research are the simulation of very large models using distributed simulation, and embedded simulation, i.e. The coupling of virtual models with physical hardware. In our work, we approach the combination of both challenges, thus we use distributed simulation on the simulation side of an embedded simulation. Our work concentrates on Discrete Event Simulation (DES) and presents an approach that optimizes the data exchange in such scenario of distributed embedded DES based on a simulation of communication network as our case study. The evaluation shows that the optimization approach reduces the traffic in the distributed simulation system and increases the accuracy of the simulation. We conclude that in the special scenario of distributed embedded simulation, our approach improves the performance to a certain extent, however many open topics for further investigation exist that are pointed out in the conclusions.	embedded software;embedded system;mathematical optimization;simulation;telecommunications network	Desheng Fu;Matthias Becker;Helena Szczerbicka	2015	2015 IEEE/ACM 19th International Symposium on Distributed Simulation and Real Time Applications (DS-RT)	10.1109/DS-RT.2015.21	real-time computing;computer science;theoretical computer science;systems simulation;distributed computing;simulation language;hardware-in-the-loop simulation;network traffic simulation	Embedded	-28.553219522642852	50.51206543430208	94892
6724e00f67e6c6010b2667a21da55992383ed06f	a european perspective on supercomputing	multicore architectures;fault tolerant;service orientation;exascale computing;supercomputing grids;holistic approach;chip;programming model;resource sharing;high performance computer;supercomputing infrastructure;exaflop;programming models;load balance;system management;cloud computing	Massive computing systems will be needed to maintain competitiveness in all areas of science, engineering and business, to provide both management efficiency and computing capability. From a systems management perspective, massive installations offer an efficient platform for resource sharing and service-oriented cloud computing; from a capability perspective, they allow unprecedented performance for supercomputing applications. With top supercomputing systems reaching the PetaFlop barrier, the next challenge is to devise technology to reach, and applications to take advantage of, ExaFlop performance. Multicore chips are already here but will grow in the next decade to several hundred cores. Although these chips will be used for general-purpose computing, they will be the tera-device components of future exascale systems.  Europe is aware of the importance of having a well-structured supercomputing infrastructure, as well as the need to exchange experiences and know-how across the Union. Infrastructure projects such as the current DEISA (Distributed European Infrastructure for Supercomputing Applications) or the future PRACE (Partnership for Advanced Computing in Europe) aim at setting up such coordinated resources. PRACE, in particular, will create a world-class pan-European high performance computing service and infrastructure, managed as a single European entity. The service will include five superior supercomputing centers, strengthened by regional and national centers, working in collaboration through grid technologies. The BSC-CNS is the Spanish representative, one of five principal partners in the project (the others being Germany, France, UK and the Netherlands). The principal partner countries have agreed to contribute more to the PRACE budget, and to host the tier-0 machines which will form part of the distributed infrastructure.  Having hit the power wall, the computing market is now undergoing a shaky era of dispersion, where many kinds of multicore alternatives are being proposed as the way to develop chips with ever increasing performance capabilities. In this time of confusion we have built a tight Gordian knot, where excitement at the potential performance shown by different hardware platforms is counteracted by the fear of wasted effort in targeting applications to each and every one of those platforms. We believe that the programming model is the key component that should break the knot. With increased scale, hierarchical levels of granularity should be considered, providing the programmer with an abstract model that will be mapped to the different target platforms by their specific runtimes. Asynchrony, decoupling between logical and physical resources of all types (cores, memory, etc.), load balancing, fault tolerance, and actual understanding of the behavior of our systems are issues that will have to be addressed and supported when targeting exascale performance.  Holistic approaches with a global vision for the design of such systems should coordinate experience and techniques at all levels, from application to programming model design, runtime implementation, and architecture, both at the node and interconnect level.  The talk will first describe how the BSC and the Spanish distributed infrastructure of computers (RES) around it was set up, and discuss experiences with its operation. Then some of the European activities will be described, finishing with the BSC's vision on the major issues in designing and using the upcoming systems of the ExaFlop era.	acm/ieee supercomputing conference;asynchrony (computer programming);binary symmetric channel;cns;cloud computing;computer;coupling (computer programming);flops;fault tolerance;grid computing;holism;load balancing (computing);multi-core processor;processor register;programmer;programming model;runtime system;semiconductor intellectual property core;service-oriented device architecture;supercomputer;systems management	Mateo Valero	2009		10.1145/1542275.1542277	parallel computing;real-time computing;simulation;computer science;operating system;distributed computing;programming paradigm	HPC	-20.772420240587916	57.64081706341275	94895
cb502174c81ffb3f1b6840f192572cbf03dbacec	a proactive top-down approach to dynamic allocation of resources in data centers	william griswold seracini filippo;computer engineering engineering computer science a proactive top down approach to dynamic allocation of resources in data centers university of california san diego ingolf krueger	Over the last couple of decades, data centers have seen a substantial increase in number, size, and use. This massive growth started with the emergence of the World Wide Web and accelerated with the advent of social media (e.g. Facebook, Twitter), contentsharing platforms (e.g. Instagram, Flickr), and cloud technologies (e.g., Amazon EC2, Microsoft Azure and OneDrive). Nowadays, large enterprises, service providers, and public cloud providers utilize anywhere from few thousands to hundred of thousands of servers in their data centers, which consume incredible amounts of electricity. However, studies have shown that the average utilization of these data centers is extremely low, resulting in a waste of computing resources and energy. This is due to the fact that resources are typically allocated in such a way that, if a spike of requests occurs in a data center, the Internet applications that run inside it do not violate service level agreements (SLAs). As a result, resources are over- allocated, and the overall utilization of the data center remains very low. This dissertation improves upon the state of the art of resource allocation techniques by taking an integrated, holistic, top-down approach, that proactively adapts the amount of resources allocated, based on incoming future workloads. This approach can be applied to workloads where different requests are correlated, such as service-oriented Internet applications. To enable proactive adaptation, this research introduces two contributions : 1) an approach to predict future workloads by leveraging correlations between requests sent to an application and 2) a technique to estimate, with acute accuracy, the impact that those workloads have on the performance of the currently allocated infrastructure. Thanks to these contributions, the work presented here can predict an incoming workload, calculate the required amount of computing resources needed to run it under a given SLA, and proactively adapt the infrastructure whenever needed. The contributions presented in this dissertation are validated using a well- known benchmark and an implementation of a web service based on a public API; results are compared against existing solutions from scientific literature. Results show double-digit improvements for both energy savings and reduction of the amount of resources allocated, with no additional SLA violations	proactive parallel suite	Filippo Seracini	2014			simulation;engineering;operations management;world wide web	Metrics	-23.36158348435346	60.343607098268485	94996
1a19bab56d8ae4a325b650de71cd1d908c7bd715	towards optimizing hadoop provisioning in the cloud	data-intensive analytics job;processing data;provision mapreduce application;mapreduce-based analytics;mapreduce programming paradigm;process data;data analytics;application area;resource consumption;mapreduce job	Data analytics is becoming increasingly prominent in a vari ety of application areas ranging from extracting business i ntelligence to processing data from scientific studies. MapRedu c programming paradigm lends itself well to these data-inten sive analytics jobs, given its ability to scale-out and leverage several machines to parallely process data. In this work we argu e that such MapReduce-based analytics are particularly syne rgistic with the pay-as-you-go model of a cloud platform. However, a key challenge facing end-users in this environment i s the ability to provision MapReduce applications to minimiz e the incurred cost, while obtaining the best performance. Th is paper first motivates the importance of optimally provision ing a MapReduce job, and demonstrates that existing approaches c an result in far from optimal provisioning. We then present a pr eliminary approach that improves MapReduce provisioning by analyzing and comparing resource consumption of the applic ation at hand with a database of similar resource consumption signatures of other applications.	antivirus software;apache hadoop;cloud computing;mapreduce;optimizing compiler;programming paradigm;provisioning;scalability	Karthik Kambatla;Abhinav Pathak;Himabindu Pucha	2009			computer science;operating system;data mining;database;world wide web	DB	-23.449301086209744	59.648676370994146	95106
1d51f3bf4018d632114a14147ed03e579ae94539	autonomous tools for grid management, monitoring and optimization	cluster computing;computational grid;structure and function;high energy physics	We outline design and lines of development of autonomous tools for the computing Grid management, monitoring and optimization. The management is proposed to be based on the notion of utility. Grid optimization is considered to be application-oriented. A generic Grid simulator is proposed as an optimization tool for Grid structure and functionality. 1. An autonomous tool for Grid monitoring and	autonomous robot;mathematical optimization;utility	Wojciech Wislicki	2007	CoRR		simulation;computer cluster;computer science;theoretical computer science;operating system;grid computing	HPC	-30.97136939054888	47.97443482583627	95252
ca90180f82b9604f889a0255a234ee2c468b6615	fault tolerant computing on the grid: what are my options?	wide area networks fault tolerant computing;wide area replication distributed computing wide area networks fault tolerance parallel applications single program multiple data checkpoint recovery;fault tolerant;mean time to failure;fault tolerant computing;fault tolerance grid computing chromium checkpointing cost function distributed computing testing computer science large scale systems file servers;wide area networks	High-performance distributed computing across wide-area networks has become an active topic of research [1][3][4][11]. Metasystem and grid software infrastructure projects, most notably, Legion [4] and Globus [3], have emerged to support this new computational paradigm. Achieving large-scale distributed computing in a seamless manner introduces a number of difficult problems. This paper examines one of the most critical problems, fault tolerance. A large wide-area system that contains hundreds to thousands of machines and multiple networks has a small mean time to failure. The most common failure modes include machine faults in which hosts go down and get rebooted, and network faults where links go down. A single monolithic solution for fault tolerance that is acceptable to all user applications is unlikely. For example, some applications may require continuous availability, or may require protection from byzantine failures, or require light-weight, low overhead fault tolerance. The most appropriate method for fault tolerance clearly may be application-specific. This follows the current trend in distributed systems and operating systems in which generic functions once performed within the “system” are now being moved to user-space for increased flexibility and performance. Because general purpose systems often impose a high cost on applications that do not fit their assumptions, the maxim “pay for what you need” has been proposed as a guiding principle for application-centric policy decisions in metacomputing systems such as Legion. We believe that the relative performance of fault tolerance methods is a key piece of information needed to enable users to make these decisions on behalf of their applications. This is particularly true for high-performance applications. To this end, we have examined fault tolerance options for a common class of high-performance parallel applications, single-program-multiple-data (SPMD). Performance models for two fault tolerance methods, checkpoint-recovery and wide-area replication, have been developed. These models enable quantitative comparisons of the two methods as applied to SPMD applications. While these	byzantine fault tolerance;computation;continuous availability;distributed computing;generic function;legion (software);mean time between failures;meta-system;metacomputing;operating system;overhead (computing);programming paradigm;spmd;seamless3d;transaction processing system;user space	Jon B. Weissman	1999		10.1109/HPDC.1999.805323	fault tolerance;parallel computing;real-time computing;mean time between failures;computer science;distributed computing;software fault tolerance	HPC	-24.295135967471506	53.849390131947885	95278
36343f9c69371ae221107831c422279c8b74ea30	implementation of the open source virtualization technologies in cloud computing		The “Virtualization and Cloud Computing” is a recent buzzword in the digital world. Behind this fancy poetic phrase there lies a true picture of future computing for both in technical and social perspective. Though the “Virtualization and Cloud Computing are recent but the idea of centralizing computation and storage in distributed data centres maintained by any third party companies is not new but it came in way back in 1990s along with distributed computing approaches like grid computing, Clustering and Network load Balancing. Cloud computing provide IT as a service to the users on-demand basis. This service has greater flexibility, availability, reliability and scalability with utility computing model. This new concept of computing has an immense potential in it to be used in the field of e-governance and in the overall IT development perspective in developing countries like Bangladesh.	centralisation;cloud computing;computation;distributed computing;e-governance;grid computing;load balancing (computing);network load balancing;open-source software;scalability;utility computing	Mohammad Mamun Or Rashid;M. Masud Rana;Jugal Krishna Das	2016	CoRR		cloud computing security;parallel computing;cloud computing;computer science;data virtualization;theoretical computer science;operating system;end-user computing;cloud testing;distributed computing;utility computing;world wide web;grid computing	HPC	-28.777585989545763	57.64730008479937	95358
6a62d3cbb6f97bf2c9a1643e0e3a0dd72958ee2a	grid computing gets small	protocols;interfaces;us enlightened computing project;grid computing protocols cities and towns medical services intelligent networks technological innovation corporate acquisitions bandwidth testing middleware;dynamic on demand bandwidth provisioning;computer networks;computer network;large scale;ncsu;open systems grid computing middleware;protocols grid computing dynamic on demand bandwidth provisioning automated interoperability japanese g lambda project us enlightened computing project middleware interfaces;japanese g lambda project;automated interoperability;middleware;open systems;grid computing;high performance;computer networks grid computing	The US and Japan have successfully demonstrated one of grid computing's long-standing holy grails - dynamic, on-demand provisioning of bandwidth and interoperability between high-performance resources in two national research testbeds. The automated interoperability between Japan's G-lambda project and the US's Enlightened Computing project was demonstrated 11 September at the annual Global LambdaGrid Workshop (http://news.ncsu.edu/releases/2006/sept/documents/global lowbargrid.pdf) in Tokyo. The demonstration featured some of the most advanced research facilities in both nations, highlighting new middleware capable of reliably coordinating both network and computational resources as well as other protocol and interface technologies. Advances in grid computing technology have tended to focus on large-scale research deployments like this one, but smaller deployments are beginning to get headlines as well. This shift could change the way we view this field-as long as grid architects are willing to expand their vision of a grid beyond raw network speed and CPU aggregation	cpu power dissipation;central processing unit;computational resource;grails;grid computing;interoperability;middleware;provisioning	Greg Goth	2006	IEEE Distributed Systems Online	10.1109/MDSO.2006.66	communications protocol;computer science;operating system;interface;middleware;database;distributed computing;open system;world wide web;computer security;grid computing;computer network	HPC	-32.14185796685582	54.871677886461306	95524
02fa22a203a697fa313e99fb955ee4cf89002208	plan 9 - an integrated approach to grid computing	integrated approach;resource discovery;high performance computing;network operating systems;authentication;data management;distributed computing;resource discovery grid computing distributed operating system de facto standard middleware toolkit plan 9 operating system authentication data management;grid computing operating systems distributed computing computer architecture laboratories middleware high performance computing supercomputers authentication data security;distributed operating system;computer architecture;operating system;plan 9 operating system;de facto standard middleware toolkit;middleware grid computing network operating systems;middleware;grid computing;supercomputers;operating systems;data security	"""Summary form only given. This paper describes the use of the """"Plan 9 from Bell Labs """" distributed operating system as a grid computing infrastructure. In particular it compares solutions using the de facto standard middleware toolkit for grids, Globus, to an environment constructed using computers running the Plan 9 operating system. These environments are compared based on the features they offer in the context of grid computing: authentication, security, data management, and resource discovery."""	9p (protocol);administrative domain;authentication;computer;distributed computing;distributed operating system;grid computing;lightweight protocol;middleware;open-source license;open-source software;plan 9 from bell labs;unix;usability	Andrey Mirtchovski;Rob Simmonds;Ronald Minnich	2004	18th International Parallel and Distributed Processing Symposium, 2004. Proceedings.	10.1109/IPDPS.2004.1303349	semantic grid;data management;computer science;operating system;middleware;authentication;database;distributed computing;data security;grid computing	HPC	-31.812894714739258	51.75429453182701	95661
cbf32fbfcbf507cf24a573a1bd52051399f7b089	review: volunteer computing: requirements, challenges, and solutions	volunteer computing;performance;classification;voting;scheduling;task retrieval	Volunteer computing is a form of network based distributed computing, which allows public participants to share their idle computing resources, and helps run computationally expensive projects. Many existing volunteer computing platforms consist of millions of users, providing huge amount of memory and processing. Since the rapid growth in the volunteer computing projects, more researchers have been attracted to study and improve the existing volunteer computing system. However, the progress of concurrently running projects has slowed down due to the increasing competition of volunteers. Moreover, because of high computational needs and low participation rate of volunteers, attracting more volunteers and using their resources more efficiently have become extremely important, if volunteer computing is to remain a feasible method. In order to competently use the huge number of volunteered resources, workers' analysis and efficient task retrieval policies are important. The purpose of this paper is to assess the strengths and requirements of current volunteer computing platforms. The paper analyses different issues relating to volunteer computing such as analysis of workers, the effectiveness of workers, how their communication and computation can be modeled and how the effectiveness of task distribution and results verification policies are analyzed. At the end, some research directions in the form of partial results, and their intermediate verification have been shown, which may improve the performance of the overall system. Moreover, this survey will enable the research community to study the available schemes used in volunteer computing and help them fill gaps in existing research. & 2013 Elsevier Ltd. All rights reserved.	analysis of algorithms;computation;distributed computing;requirement;volunteer computing	Nouman M. Durrani;Jawwad Shamsi	2014	J. Network and Computer Applications	10.1016/j.jnca.2013.07.006	simulation;voting;performance;biological classification;computer science;operating system;end-user computing;distributed computing;management science;utility computing;law;scheduling;computer security	HPC	-20.707287716566263	58.46486821040482	95829
5c8baedff46e79bcebb14d2f411e91a66b25a86b	design of a new cloud computing simulation platform	validation;scalability;flexibility;cloud computing	"""Cloud computing is a paradigm which allows the use of outsourced infrastructures in a """"pay-as-you-go"""" basis, thanks to which scalable and customizable infrastructures can be built on demand. The ability to infer the number and type of the Virtual Machines (VM) needed determines the final budget, thus it represents a key in order to efficiently manage a cloud infrastructure. In order to develop new proposals aimed at different topics related to cloud computing (for example, datacenter management, or provision of resources), a lot of work and money is required to set up an adequately sized testbed including different datacenters from different organizations and public cloud providers. Therefore, it is easier to use simulation as a tool for studying complex scenarios. With this in mind, this paper introduces iCanCloud, a novel simulator of cloud infrastructures with remarkable features such as usability, flexibility, performance and scalability. This tool is specially aimed at simulating instance types provided by Amazon, so models of these are included in the simulation framework. Accuracy experiments conducted by means of comparing results obtained using iCanCloud and a validated mathematical model of Amazon in the context of a given application are also presented. These illustrate the efficiency of iCanCloud at reproducing the behavior of Amazon instance types."""		Alberto Nuñez;José Luis Vázquez-Poletti;Agustín C. Caminero;Jesús Carretero;Ignacio Martín Llorente	2011		10.1007/978-3-642-21931-3_45	scalability;simulation;cloud computing;computer science;operating system;data mining;database;distributed computing;computer security	Theory	-22.881092306570594	58.69644539366384	96217
a6dd2b58aabbcf50a889cd47d9efc2906b8c147e	providing network performance isolation in vde-based cloud computing systems	virtualisation cloud computing local area networks operating systems computers virtual machines;virtual machine;virtual distributed ethernet vde;resource allocation;proportionally fair resource allocation;network virtualization software module network performance isolation cloud computing system virtual machine performance isolation enabled virtual distributed ethernet pie vde;network performance;proportionally fair resource allocation network performance isolation cloud computing virtual distributed ethernet vde;large scale;virtual machines;network performance isolation;bandwidth switches cloud computing schedules protocols resource management;bandwidth sharing;operating systems computers;local area networks;virtualisation;cloud computing	In a cloud computing system, virtual machines owned by different clients are co-hosted on a single physical machine. It is vital to isolate network performance between the clients for ensuring fair usage of the constrained and shared network resources of the physical machine. Unfortunately, the existing network performance isolation techniques are not effective for cloud computing systems because they are difficult to be adopted in a large scale and require non-trivial modification to the network stack of a guest OS. In this paper, we propose a performance isolation-enabled virtual distributed Ethernet (PIE-VDE) to overcome such difficulties. It is a network virtualization software module running on a host OS. It intends to (1) allocate fair share of outgoing link bandwidth to the co-hosted clients and (2) divide a client's share to the virtual machines owned by it in a fair way. Our approach supports full virtualization of a guest OS, ease in wide scale adoption, limited modification to the existing system, low run-time overhead and work-conserving servicing. Experimental results show the effectiveness of the proposed mechanism. Every client received at least 99.5% of its bandwidth share as specified by its weight.	client (computing);cloud computing;full virtualization;network performance;operating system;overhead (computing);protocol stack;virtual distributed ethernet;virtual machine	Vijeta Rathore;Jonghun Yoo;Jaesoo Lee;Seongsoo Hong	2011	2011 IEEE International Conference on High Performance Computing and Communications	10.1109/HPCC.2011.102	parallel computing;real-time computing;cloud computing;computer science;virtual machine;operating system;distributed computing;computer security;computer network	HPC	-22.025638144091985	60.079804238979136	96300
034ed7b5b2e4357ec1c7266e64b509033eb64b17	meghaos: a framework for scalable, interoperable cloud based operating system	internet;operating system;web browser;cross platform;cloud computing	"""Cloud computing is becoming relevant due to increase in speed of Internet and reduction in its access cost. Desktop computing demands expensive hardware and software suits which become obsolete too often. Ownership of Personal Computers has always remained low in developing countries mainly due to its prohibitive cost. Alternate models of Computer usage like Public-Access Kiosks and low cost Laptops have been tried with limited success. Issues like security and privacy are major concerns in pubic computing. Laptops and other mobile devices lack required hardware support to run computing intensive applications. Increased penetration of Internet and mobile phones are providing new opportunities to bring computing closer to people. Cloud based Operating Systems are an effort in this direction. The present system, named MeghaOS, provides a framework for Desktop-like Operating System (OS) on a Web Browser. The Cloud becomes a metaphor for Operating System services accessed though Internet. Unlike traditional Operating System, MeghaOS can be accessed on any device having just a Web Browser. Since applications developed using this framework will be cached in Client’s machine, network utility will be low. Since data and applications are hosted remotely users can use them without transferring data into local device. MeghaOS provides scalable, multi-device compatible, browser accessible framework for Cloud based Operating System demonstrating next generation computing paradigm. DOI: 10.4018/ijcac.2012010104 54 International Journal of Cloud Applications and Computing, 2(1), 53-70, January-March 2012 Copyright © 2012, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited. Web browser. The services consist of data, computation and other resources that can be located anywhere in the world. The services and applications require no installation and make the deployment of applications on the Cloud server exceptionally simple and rapid. Ideally, applications should also support user interaction and collaboration, by allowing multiple client devices to interact and share the same data and application set over the Internet (Pianese, Bosch, Duminuco, Janssens, Stathopoulos, & Steiner, 2010). There is another parallel transition currently occurring where mobile devices are becoming an important application platform and a gateway to the Web. While the Web has conventionally been accessed from a personal computer, the increasing network bandwidth, processor speed, memory capacity and network service plans are rapidly increasing the mobile web usage. The two transitions, towards Cloudbased software usage and Web enabled mobile devices are transforming present era in many important ways. It is thought that in the long run the popularity of the Cloud will make it the well used application platform all over the world. It is also predicted that with the growing popularity of mobile devices and users, a common application platform will emerge for the unified usage with different computing gadgets from desktop to mobile devices. The structure of this paper is as follows. Section 2 briefs the background on mobile web applications and few existing solutions in the domain along with the comparison studies. Section 3 gives the overview of proposed framework along with mobile browser optimization in Cloud computing platform. Section 4 briefs the architecture of the proposed framework. Sections 5, describes the various optimization approaches such as dynamic content caching and offloading of computations. Section 6 discusses the experimental results showing the power of Cloud computing, compatibility of system for various network capabilities and the effect of caching. Finally, Section 7 concludes the paper along with future directions. 2. RELATED WORK With the rapid development of new forms of Cloud and mobile computing paradigms, it is necessary that a generic operating system must evolve with a virtual OS framework and a common set of applications to cater the needs of all the internet enabled computing gadgets (Steinke, 2008). As a result, the Internet or the Cloud could potentially support and manage a virtual OS that can be accessed by all the devices connected by a network. The various contemporary systems with similar goal of a Web OS are described below. Google Chrome OS: It is a popular open source operating system developed by Google and runs typically on netbooks. It is widely known as Chromium OS. The user interface is austere and resembles the Google Chrome web browser. On being an open source project, Chromium OS is prone to changes by budding developers while the Chrome OS is specific and runs only on the supporting hardware (Barth, Jackson, & Reis, 2009). The architecture has three significant components of which the firmware forms an integral part. The firmware helps to boot the OS in a jiffy. The verified booting process takes care of security. System recovery option helps in reinstallation. The services are differentiated for system level and user level. The user land services managed by Upstart assist in running only the critical services. The user interaction with multiple client windows is successfully managed by the Window Manager which is similar to X window managers. The X-Composite Extension is used to handle windows and pixmaps to draw final and composite images. In addition to this, it shelters the computer against the numerous forms of viruses and malwares. But this entire system is web centric and thus requires a very reliable internet connection to support data centric applications and multitasking. Despite supporting the development and 16 more pages are available in the full version of this document, which may be purchased using the """"Add to Cart"""" button on the product's webpage: www.igi-global.com/article/meghaos-framework-scalableinteroperable-cloud/64635?camid=4v1 This title is available in InfoSci-Journals, InfoSci-Journal Disciplines Computer Science, Security, and Information Technology. Recommend this product to your librarian: www.igi-global.com/e-resources/libraryrecommendation/?id=2"""	booting;cache (computing);care-of address;chrome os;chromium os;clock rate;cloud computing;computation;computer multitasking;computer science;computer virus;desktop computer;dynamic web page;firmware;floor and ceiling functions;google chrome;internet access;interoperability;jackson;jiffy (time);laptop;librarian;malware;mathematical optimization;microsoft windows;mobile computing;mobile device;mobile phone;netbook;network utility;open-source software;operating system;personal computer;programming paradigm;scalability;server (computing);software deployment;steiner tree problem;upstart;user interface;web application;webos;whole earth 'lectronic link;world wide web;x window manager	K. G. Srinivasa;S. RaddiC. HarishRaddiC.;H. KrishnaS. MohanKrishnaS.;Nidhi Venkatesh	2012	IJCAC	10.4018/ijcac.2012010104	embedded system;cloud computing;computer science;operating system;utility computing;world wide web	Web+IR	-28.97885796738704	55.57069945863454	96463
08e9e68c80183cc83607e52a90f574f38c035bd1	data management with load balancing in distributed computing	distributed system;design and development;data management;distributed computing;system performance;data communication;distributed computing system;middleware;load balance;high level architecture	  This paper reviews existing data management schemes and presents a design and development of a data management scheme with  load balancing in a distributed computing. This scheme defines a variety of degree of load balancing, maps each degree to  each data management configuration, and reduces data traffic among distributed components. The scheme allows to share geographically  dispersed data assets collaboratively and execute a complex large-scale distributed computing system including cooperating  and distributed components with reasonable computation and communication resources. In addition, this paper introduces a HLA  (High Level Architecture) bridge middleware environment for data communication among multiple federations. We analyze system  performance and scalability with a variety of degree of load balancing configurations. The empirical result on a heterogeneous  OS distributed system apparently presents advantages of the data management scheme with load balancing in terms of system  performance and scalability.    	distributed computing;load balancing (computing)	Jong Sik Lee	2004		10.1007/978-3-540-24767-8_65	distributed algorithm;dce/rpc;middleware;parallel computing;real-time computing;data management;computer science;load balancing;csiv2;data-intensive computing;middleware;distributed computing;utility computing;distributed system security architecture;distributed design patterns;grid computing;replication;autonomic computing;distributed concurrency control	HPC	-29.835786126608777	48.74166650887536	97161
676ecf51199dd67101ef6e8b07073596d1e8b4bf	agility in virtualized utility computing	virtual machine;virtualization;resource management system virtualized utility computing single administrative domain application level clusters resource reassignment mechanisms ghost virtual machines;resource allocation;distributed processing;virtual machining;resource manager;resource management;virtualized utility computing;virtual machines distributed processing resource allocation;resumes;ghost virtual machines;servers;internet;virtual machines;application level clusters;resource reassignment mechanisms;utility computing;single administrative domain;switches;agility;resource management system;virtualization agility utility computing;virtual machining voice mail platform virtualization resource management virtual manufacturing computer applications physics computing mechanical factors resource virtualization testing	Virtual machines have emerged as an attractive approach for utility computing platforms because applications running on VMs are fault- and security- isolated from each other, yet can share physical machines. An important property of a virtualized utility computing platform is how quickly it can react to changing demand. We refer to the capability of a utility computing platform to quickly reassign resources as the agility of the platform. We are targeting hosting utility provider environments where the entire platform is under the control of a single administrative domain and application instances often form application-level clusters. In this work, we examine resource reassignment mechanisms in these environments from the agility perspective and outline a new mechanism that exploits properties of a virtualized utility computing platform.  This new mechanism employs ghost virtual machines (VMs), which participate in application clusters, but do not handle client requests until activated by the resource management system. We evaluate this, as well as other, mechanisms on a utility computing testbed. The results show that this ghost VM approach is superior to other approaches in its agility, and allows a new VM to be added to an existing application cluster in a few seconds with negligible overhead. This is a promising result as we develop resource management algorithms for a globally distributed utility computing platform.	administrative domain;algorithm;overhead (computing);resource management (computing);testbed;utility computing;virtual machine	Hangwei Qian;Elliot Miller;Wei Zhang;Michael Rabinovich;Craig E. Wills	2007	Proceedings of the 2nd International Workshop on Virtualization Technology in Distributed Computing (VTDC '07)	10.1145/1408654.1408663	real-time computing;computer science;operating system;distributed computing;utility computing	HPC	-24.3489965665899	54.318410052785666	97343
0332fccafab1171c649ec4ba9be4ab9a469a6b13	shark: scaling file servers via cooperative caching	cooperative caching	Network file systems offer a powerful, transparent interface for accessing remote data. Unfortunately, in current network file systems like NFS, clients fetch data from a central file server, inherently limiting the system's ability to scale to many clients. While recent distributed (peer-to-peer) systems have managed to eliminate this scalability bottleneck, they are often exceedingly complex and provide non-standard models for administration and accountability. We present Shark, a novel system that retains the best of both worlds--the scalability of distributed systems with the simplicity of central servers.  Shark is a distributed file system designed for large-scale, wide-area deployment, while also providing a drop-in replacement for local-area file systems. Shark introduces a novel cooperative-caching mechanism, in which mutually-distrustful clients can exploit each others' file caches to reduce load on an origin file server. Using a distributed index, Shark clients find nearby copies of data, even when files originate from different servers. Performance results show that Shark can greatly reduce server load and improve client latency for read-heavy workloads both in the wide and local areas, while still remaining competitive for single clients in the local area. Thus, Shark enables modestly-provisioned file servers to scale to hundreds of read-mostly clients while retaining traditional usability, consistency, security, and accountability.	cache (computing);clustered file system;distributed computing;drop-in replacement;file server;peer-to-peer;shark;scalability;server (computing);software deployment;usability	Siddhartha Annapureddy;Michael J. Freedman;David Mazières	2005			self-certifying file system;file server;real-time computing;torrent file;computer science;stub file;operating system;ssh file transfer protocol;journaling file system;database;bittorrent tracker;open;distributed file system;file system fragmentation;world wide web;server;computer network	Networks	-21.922298820497947	51.79222738189573	97365
f043177376bf93aab536030f9c7d445e2c4cb0c9	replica consistency in lazy master replicated databases	data mart;design of algorithms;distributed database;refreshment algorithm;replicated data;replica consistency;correctness criteria;data warehouse;lazy replication	In a lazy master replicated database, a transaction can commit after updating one replica copy (primary copy) at some master node. After the transaction commits, the updates are propagated towards the other replicas (secondary copies), which are updated in separate refresh transactions. A central problem is the design of algorithms that maintain replica's consistency while at the same time minimizing the performance degradation due to the synchronization of refresh transactions. In this paper, we propose a simple and general refreshment algorithm that solves this problem and we prove its correctness. The principle of the algorithm is to let refresh transactions wait for a certain “deliver time” before being executed at a node having secondary copies. We then present two main optimizations to this algorithm. One is based on specific properties of the topology of replica distribution across nodes. In particular, we characterize the nodes for which the deliver time can be null. The other improves the refreshment algorithm by using an immediate update propagation strategy.	algorithm;correctness (computer science);elegant degradation;lazy evaluation;replication (computing);software propagation	Esther Pacitti;Pascale Minet;Eric Simon	2001	Distributed and Parallel Databases	10.1023/A:1019217729633	real-time computing;computer science;operating system;data warehouse;database;distributed computing;distributed database	DB	-22.614115258737456	48.106078264008104	97451
4386cd9025d4a2ba95756dad0915054241823efe	policy-based multi-datacenter resource management	policy based resource management policy based multidata center resource management resource sharing;information centres resource allocation;fault tolerant;resource allocation;resource manager;data center;information centres;resource management fault tolerance computer architecture internet concurrent computing yarn delay conferences	Large enterprises operate two or more data centers for scaling and fault tolerance. Each data center maintains a certain degree of autonomy while sharing resources with other data centers. In this paper we describe how policy-based resource management can be used in existing commercial products such that each data center can control when, what, and how to share resources with other data centers.	autonomy;data center;fault tolerance;image scaling	Murthy V. Devarakonda;Vijay K. Naik;Nithya Rajamani	2005	Sixth IEEE International Workshop on Policies for Distributed Systems and Networks (POLICY'05)	10.1109/POLICY.2005.27	resource allocation;knowledge management;resource breakdown structure;resource management;database;distributed computing;business;human resource management system	Networks	-26.921279715308685	56.70722733873303	97856
8e26c8cc0b0898164e4d38c8cf933861aba19871	a new architecture for secure carrier-class clusters	secure carrier class clusters architecture;carrier class application servers secure carrier class clusters architecture carrier class requirements cluster security distributed security architecture;high availability;carrier class application servers;security hardware application software availability computer architecture packaging telecommunications communication industry computer industry scalability;carrier class requirements;application software;availability;network operating systems;application server;computer industry;packaging;cluster security;communication industry;computer architecture;cluster system;telecommunication security;security architecture;network operating systems telecommunication security security of data workstation clusters;cost effectiveness;scalability;workstation clusters;distributed security architecture;security;security of data;telecommunications;hardware	Traditionally, the telecom industry has used clusters to meet its carrier-class requirements of high availability, reliability, and scalability, while relying on cost-effective hardware and software. Efficient cluster security is now an essential requirement and has not yet been addressed in a coherent fashion on clustered systems. This paper presents an approach for distributed security architecture that supports advanced security mechanisms for current and future security needs, targeted for carrier-class application servers running on clustered systems.	application server;coherence (physics);computer cluster;computer security;high availability;requirement;scalability	Makan Pourzandi;Ibrahim Haddad;Charles Levert;Miroslaw Zakrzewski;Michel Dagenais	2002		10.1109/CLUSTR.2002.1137790	software security assurance;computer security model;cloud computing security;availability;packaging and labeling;application software;parallel computing;scalability;cost-effectiveness analysis;computer science;information security;operating system;security service;distributed system security architecture;high availability;computer security;application server;enterprise information security architecture;computer network	OS	-32.72238426663384	48.55358574213354	98221
4d3466ac04dac9453d5c1dc71fdb939552e18621	fault tolerant shared-object management system with dynamic replication control strategy	management system;fault tolerant;availability;meta level implementation shared object management dynamic replication control fault tolerance;dynamic replication;dynamic environment;fault tolerant computing;shared memory systems;shared object;fault tolerant systems control systems costs communication system control software algorithms access protocols condition monitoring resumes software tools hardware;fault tolerance;distributed object management;communication cost;meta level architecture;shared memory systems fault tolerant computing distributed object management;article;control strategy	This paper is based on a dynamic replication control strategy for minimizing communications costs. In dynamic environments where the access pattern to share resources can not be predicted statically, it is required to monitor such parameter during the whole lifetime of the system so as to adapt it to new requirements. The shared-object management system is implemented in a centralized manner in which a master processor deals with the serialization of invocations. On one hand, we attempt to provide fault tolerance as a way to adjust the system parameters to work only with a set of correct processors so as to enhance system functionality. On the other hand, we attempt to furnish availability by masking the failure of the master processor. A new master processor is elected that resumes the master processor processing. Our shared-object management system modularity is realized through a meta level implemen-	central processing unit;centralized computing;control theory;distributed computing;fault tolerance;library (computing);overhead (computing);requirement;serialization	Juan Carlos Leonardo;Takaichi Yoshida;Shuji Narazaki	2000		10.1109/PADSW.2000.884677	embedded system;fault tolerance;parallel computing;real-time computing;computer science;operating system;database;distributed computing	DB	-26.606487407073505	48.186057130506164	98294
541aeab04fcf08b400eede667cb84ca9582f6bb2	control-as-a-service from the cloud: a case study for using virtualized plcs	control as a service;office systems;host computing resources;virtualized plc;virtualization;enterprise systems;cloud computing automation hardware security virtualization real time systems virtual machine monitors;industrial automation systems;virtual machine monitors;service on demand;security;cloud computing;hardware;real time systems;automation	Cloud computing has recently emerged as a new computing paradigm in many application areas comprising office and enterprise systems. It offers various solutions to provide a dynamic and flexible infrastructure to host computing resources and deliver them as a service on-demand. Since industrial automation systems of the future have to be adaptable and agile, cloud computing can be considered as a promising solution for this area. However, the requirements of industrial automation systems differ significantly from the office and enterprise world. In this paper we describe a case study that implements a concept of PLC as a service within a cloud based infrastructure and provides a performance evaluation with respect to legacy PLCs.	agile software development;automation;cloud computing;enterprise system;experiment;hypervisor;microsoft outlook for mac;performance evaluation;power-line communication;programming paradigm;real-time clock;real-time computing;real-time transcription;requirement;software deployment	Omid Givehchi;Jahanzaib Imtiaz;Henning Trsek;Jürgen Jasperneite	2014	2014 10th IEEE Workshop on Factory Communication Systems (WFCS 2014)	10.1109/WFCS.2014.6837587	cloud computing security;embedded system;real-time computing;cloud computing;engineering;operating system;cloud testing;utility computing	Embedded	-30.695167730118367	56.65378760356306	98353
658200ed0c41f4c222a7d048f00058c13037fe3c	architecture of the internet archive	flash memory;write amplification;production system;solid state drives;solid state storage systems	The Internet Archive is a live production system supporting close to a petabyte of data and delivering an average of 2.3Gb/sec of data to Internet users. We describe the architecture of this system with an emphasis on its robustness and how it is managed by a very small team of systems personnel. Notably, the current system does not employ a cache. We analyze the reasons for this decision and show that an effective cache could not be built until now. However, new solid state disk technology may offer promising new cache implementations.	archive;cpu cache;petabyte;production system (computer science);solid-state drive	Elliot Jaffe;Scott Kirkpatrick	2009		10.1145/1534530.1534545	embedded system;real-time computing;computer hardware;cache;engineering;cache algorithms	Networks	-26.944894310216718	54.329811751041994	98363
93b2039e78e2b4eebc820a72d8072684554fecb7	claret: using data types for highly concurrent distributed transactions	adaptive replication;accessibility;large scale database replication;geo replication	Out of the many NoSQL databases in use today, some that provide simple data structures for records, such as Redis and MongoDB, are now becoming popular. Building applications out of these complex data types provides a way to communicate intent to the database system without sacrificing flexibility or committing to a fixed schema. Currently this capability is leveraged in limited ways, such as to ensure related values are co-located, or for atomic updates. There are many ways data types can be used to make databases more efficient that are not yet being exploited.  We explore several ways of leveraging abstract data type (ADT) semantics in databases, focusing primarily on commutativity. Using a Twitter clone as a case study, we show that using commutativity can reduce transaction abort rates for high-contention, update-heavy workloads that arise in real social networks. We conclude that ADTs are a good abstraction for database records, providing a safe and expressive programming model with ample opportunities for optimization, making databases more safe and scalable.	abstract data type;data structure;database;distributed transaction;mathematical optimization;mongodb;nosql;programming model;redis;scalability;social network	Brandon Alexander Holt;Irene Zhang;Dan R. K. Ports;Mark Oskin;Luis Ceze	2015		10.1145/2745947.2745951	computer science;data mining;database;world wide web	DB	-21.630373259189504	49.9540494351006	98474
fee662d3095b774b45fc7e9bc9e4ab030f22cd6b	analysis of data access methods in a distributed computing system	data access		data access;distributed computing	Elias Drakopoulos	1994			distributed system security architecture;distributed concurrency control;grid computing;autonomic computing;distributed algorithm;csiv2;dce/rpc;distributed design patterns;computer science;distributed computing	HPC	-29.286722139128216	46.94356848681211	98493
00a69102e2be4953011a1bfb762dceb38954d12b	a distributed algorithm for graphic objects replication in real-time group editors	graphics editing;real time;distributed computing;collaborative editing;object replication;concurrency control;consistency maintenance;distributed algorithm;object identification	Real-time collaborative editing systems are groupware systems that allow multiple users to edit the same document at the same time from multiple sites. A specific type of collaborative editing system is the object-based collaborative graphics editing system. One of the major challenge in building such systems is to solve the concurrency control problems. This paper addresses the concurrency control problem of how to preserve the intentions of concurrently generated operations whose effects are conflicting. An object replication strategy is proposed to preserve the intentions of all operations. The effects of conflicting operations are applied to different replicas of the same object, while non-conflicting operations are applied to the same object. An object identification scheme is proposed to uniquely and consistently identify non-replicated and replicated objects. Lastly, an object replication algorithm is proposed to produce consistent replication effects at all sites.	collaborative real-time editor;collaborative software;concurrency (computer science);concurrency control;distributed algorithm;graphics;identification scheme;multi-user;object-based language;real-time transcription;replication (computing)	David Chen;Chengzheng Sun	1999		10.1145/320297.320310	distributed algorithm;method;computer science;theoretical computer science;operating system;concurrency control;database;distributed computing;distributed object;world wide web	DB	-25.376485534848772	47.53395678799891	98611
11678bd8cbd61ae62565a4d8207f61cea257b982	applying similarity in concurrency control for real-time database application	phase locking;database system;real time;satisfiability;high priority;concurrency control;time constraint	Most of the proposed concurrency control protocols for real-time database systems (RTDBS) are based on serializability theorem. Owing to the unique characteristics of realtime database applications and the importance of satisfying the timing constraints of the transactions, serializable concurrency control protocols are not suitable for RTDBS for most cases. In this paper, similarity, which is a less restrictive correctness criterion, is used for concurrency control in RTDBS, for instance, a stock trading database system. By studying the correctness requirements of the system, similarity is defined and incorporated into a real-time two phase locking protocol, High Priority Two Phase Locking (H2PL). With the use of similarity, although serializability is not ensured, the amount of inconsistency in the database is tolerable and is within the system requirements. On the other hand, the performance of the system can be much improved.	concurrency (computer science);concurrency control;correctness (computer science);database;global serializability;lock (computer science);real-time clock;real-time operating system;real-time transcription;requirement;system requirements;two-phase locking	Kam-yiu Lam;Wai-cheong Yau;Victor Chung Sing Lee	1996		10.1007/BFb0034676	timestamp-based concurrency control;database theory;optimistic concurrency control;real-time computing;isolation;database tuning;computer science;concurrency control;database;distributed computing;multiversion concurrency control;non-lock concurrency control;serializability;acid;satisfiability;distributed concurrency control	DB	-23.668124404850253	48.041684019329615	98620
2789e2a7b270c1791c07cfb2868c30f77ef3a134	flex5gware: flexible and efficient hardware and software platforms for 5g network elements and devices		This paper presents the Flex5Gware project, whose goal is to deliver highly reconfigurable hardware (HW) platforms and HW-agnostic software (SW) platforms for 5G network elements and terminal devices. Flex5Gware will enable 5G HW/SW platforms to meet the requirements imposed by the growth in mobile traffic and the diversity of applications by increasing their capacity, reducing their energy footprint and enhancing their scalability and modularity. To put in place this vision in this early stage of 5G PPP research, Flex5Gware is designing and prototyping key building blocks of 5G HW/SW platforms. In particular, the developed technologies will be evaluated and demonstrated with proofs-of-concept by the end of the project. Copyright © 2016 John Wiley & Sons, Ltd.	computer hardware;field-programmable gate array;john d. wiley;requirement;scalability;shattered world	Miquel Payaró;Michael Färber;Panagiotis Vlacheas;Nikolaos G. Bartzoudis;Fredrik Tillman;Dieter Ferling;Vincent Berg;Tapio Rautio;Pablo Serrano;Dario Sabella	2016	Trans. Emerging Telecommunications Technologies	10.1002/ett.3070	embedded system;real-time computing;telecommunications control software;computer science;software engineering	EDA	-29.574860797635953	57.1656116950847	98795
475a94f0c5e560d1a2fe72d241766b020d7cb439	virtual resources for the internet of things	performance evaluation cloud computing sensors programming actuators yttrium topology;software architecture cloud computing energy consumption internet of things power aware computing;control loop latencies virtual resources internet of things software architecture resource limited sensors restful interfaces full fledged cloud hosted applications application logic performance issues excessive energy consumption intermediate lot devices cloud hosted applications coap prototype resource constrained devices cycle accurate emulation cloud centric architectures restful interaction pattern energy consumption representative scenarios;computer and information science;data och informationsvetenskap	We present Virtual Resources: a software architecture to resolve the tension between effective development and efficient operation of Internet of Things (IoT) applications. Emerging IoT architectures exhibit recurring traits: resource-limited sensors and actuators with RESTful interfaces at one end; full-fledged Cloud-hosted applications at the opposite end. The application logic resides entirely at the latter, creating performance issues such as excessive energy consumption and high latencies. To ameliorate these, Virtual Resources allows developers to push a slice of the application logic to intermediate IoT devices, creating a continuum between physical resources and Cloud-hosted applications. With Virtual Resources, for example, developers can push processing of sensed data to IoT devices close to the physical sensors, reducing the data to transmit and thus saving energy. We describe the key concepts of Virtual Resources and their realization in a CoAP prototype atop resource-constrained devices. Experimental results from cycle-accurate emulation indicate that Virtual Resources enable better performance than Cloud-centric architectures, while retaining the RESTful interaction pattern. For example, energy consumption in representative scenarios improves up to 40% and control loop latencies reduce up to 60%.	as-interface;business logic;cloud computing;constrained application protocol;control system;emulator;interaction design pattern;internet of things;level of detail;logistics;prototype;representational state transfer;ssi ceb;sensor;separation of concerns;software architecture;triune continuum paradigm	Andrea Azzara;Luca Mottola	2015	2015 IEEE 2nd World Forum on Internet of Things (WF-IoT)	10.1109/WF-IoT.2015.7389060	embedded system;real-time computing;simulation;operating system;computer security;information and computer science;computer network	Mobile	-29.91911337882539	56.22580362592625	99231
5d2aae92a6a912c8edd1f56b5014966d929ee656	cluster infrastructure for biological and health related research	biology computing;high availability;school of no longer in use;electronics and computer science;cluster;fault tolerant;fault tolerance grid computing workstation clusters biology computing;health related research health industries data clusters fail over configuration grid technology job scheduling time critical applications cluster infrastructure biological research;data clustering;biological research;fault tolerance;parallel;biology computing grid computing;distributed;workstation clusters;grid computing;job scheduling	Researchers in the biological and health industries need powerful and stable systems for their work. These systems must be dependable, fault-tolerant, highly available and easy to use. To cope with these demands we propose the use of computational and data clusters in a fail-over configuration combined with the grid technology and job scheduling. Our infrastructure has been deployed successfully for running time-critical applications in commercial environments. We also present experimental results from this pilot implementation that demonstrate the viability of our approach.	application server;behavioral pattern;bottleneck (software);c++;computation;dependability;distributed computing;error detection and correction;experiment;failover;fault tolerance;formal verification;high availability;high-availability cluster;image processing;job scheduler;load balancing (computing);perl;quality of service;real-time clock;requirement;scheduling (computing);server (computing);shell script;time complexity;timeline;unix shell;uptime;window of opportunity	Sophia Corsava;Vladimir Getov	2003		10.1109/CCGRID.2003.1199416	fault tolerance;parallel computing;real-time computing;computer science;operating system;database;distributed computing	HPC	-29.79471568448968	49.669448984741074	99496
b21c5480d21c5e6ee8a37ae30cc00ba1f844376e	supporting the dynamic grid service lifecycle	open systems grid computing;collaborative work;fluctuations;application software;availability;prototypes;dynamic grid service lifecycle;adaptive grid;dynamic virtual organization dynamic grid service lifecycle ogsa based grid service architecture gt3;gt3;virtual organization;grid service;scientific computing;cities and towns;middleware;availability grid computing middleware prototypes application software cities and towns fluctuations costs scientific computing collaborative work;resource availability;open systems;grid computing;ogsa based grid service architecture;dynamic virtual organization	This paper presents an architecture and implementation for a dynamic OGSA-based grid service architecture that extends GT3 to support dynamic service hosting - where to host and re-host a service within the grid in response to service demand and resource fluctuation. Our model goes beyond current OGSI implementations in which the service is presumed to be pre-installed at all sites (and only service instantiation is dynamic). In dynamic virtual organizations (VOs), we believe dynamic service hosting provides an important flexibility. Our model also defines several new adaptive grid service classes that support adaptation at multiple levels. Dynamic service deployment allows new services to be added or replaced without taking down a site for reconfiguration and allows a VO to respond effectively to dynamic resource availability and demand. The preliminary results suggest that the cost of dynamic installation, deployment, and invocation, is tolerable.	adaptive mesh refinement;open grid services architecture;open grid services infrastructure;pre-installed software;quantum fluctuation;software deployment;universal instantiation;virtual organization (grid computing)	Jon B. Weissman;Seonho Kim;Darin England	2005	CCGrid 2005. IEEE International Symposium on Cluster Computing and the Grid, 2005.	10.1109/CCGRID.2005.1558645	service level requirement;availability;application software;differentiated service;computer science;service delivery framework;operating system;middleware;database;distributed computing;prototype;open system;world wide web;grid computing	HPC	-31.517551388703097	53.257535160823636	99778
3c3aae07e0b071bce9159c8f2a75bb773696d6e4	reliable broadcast for fault-tolerance on local computer networks	protocols;self checking components;fault tolerant;telecommunication network reliability;clocks;computer network reliability broadcasting fault tolerance telecommunication network reliability local area networks protocols clocks computer architecture displays synchronization;protocols fault tolerant computing local area networks;generic reliable communication architecture;synchronism properties reliable broadcast fault tolerance local computer networks generic reliable communication architecture local area network self checking components protocol high performance real time applications project delta 4 ced esprit ii consortium distributed architecture;computer architecture;fault tolerant computing;distributed environment;high performance real time applications;synchronization;displays;fault tolerance;project delta 4;ced esprit ii consortium;local computation;error processing;local computer networks;broadcasting;synchronism properties;local area networks;local area network;reliable broadcast;distributed architecture;computer network reliability;protocol	The importance of faulttolerance mechanisms in applicationindependent systems, has led to the increased use of techniques based in “macroscopic” replication of components and software oriented error processing. In distributed environments, management of the replication of components throughout different sites, may benefit from the availability of reliable broadcast or multicast protocols. This paper discusses the definition and design of a generic reliable communication architecture, on a widely used host independent platform, such as a local area network. Two aspects of relevance are the use of non-rep6icated LANs and of self-checking components. The protocol itself is innovative, in the sense that, although clock-less, and running on a non-replicated network, it displays bounded execution times. The architecture is in consequence capable of reliably addressing real-time.	accessibility;causality;correctness (computer science);datagram;emoticon;fault tolerance;multicast;quality of service;real-time clock;real-time computing;real-time transcription;recursion;relevance;requirement;run time (program lifecycle phase);token ring;two-phase commit protocol	Paulo Veríssimo;José Alves Marques	1990		10.1109/RELDIS.1990.93951	local area network;embedded system;fault tolerance;real-time computing;computer science;operating system;database;distributed computing;computer security;computer network	Networks	-32.14147982810353	47.56852500939776	99789
f45fda5f62283d02d7a3c10e14d585fbab47ad5e	designing a flexible and modular architecture for a private cloud: a case study	private cloud;virtualization;service provider;web service;soft real time;design and implementation;settore ing inf 05 sistemi di elaborazione delle informazioni;utility computing;modular architecture;storage;cloud computing;free software	Cloud computing is an emerging paradigm used by an increasingly number of enterprises to support their business and promises to make the utility computing model fully realized by exploiting virtualization technologies. Free software is now mature not only to offer well-known server-side applications, but also to land on desktop computers. However, administering in a decentralized way a large amount of desktop computers represents a demanding issue: system updates, backups, access policies, etc. are hard tasks to be managed separately on each computer. This paper presents a general purpose architecture for building a reliable, scalable, flexible, and modular private cloud that exploits virtualization technologies at different levels. The architecture can be used to offer a variety of services that span from web applications and web services to soft real-time applications.  To show the features of the proposed architecture, we also present the design and implementation over it of a Linux Terminal Server Project (LTSP) cluster that benefits from the underlying IaaS services offered by the private cloud. The cloud infrastructure, as well as the LTSP, have been implemented exclusively using free software and are now in a production state, being used by approximately 200 users for their everyday work. We hope that our description and design decisions can provide some guidance about designing an architecture for a cloud service provider.	apache axis;backup;central processing unit;cloud computing;computation;desktop computer;disk storage;fault tolerance;gentoo linux;high availability;ltsp;lvm;principal component analysis;programming paradigm;real-time clock;real-time computing;scalability;server (computing);server-side;systems architecture;terminal server;throughput;utility computing;web application;web service	Valeria Cardellini;Stefano Iannucci	2012		10.1145/2287056.2287067	cloud computing security;real-time computing;simulation;cloud computing;engineering;operating system;service-oriented modeling;cloud testing;utility computing;service virtualization	OS	-29.83798524760068	54.89595255405081	99808
4bf06308ec1bd44a0628c1c6e23a81f05406bd4a	haven: holistic load balancing and auto scaling in the cloud	software defined networking cloud computing power aware computing resource allocation;resource management;logic gates resource management;hardware load balancer haven holistic load balancing software appliances hardware appliances multitenant cloud environment resource utilization autoscaling algorithms software defined networking load balancing algorithm network controller software data plane software load balancer;logic gates	Load balancing and auto scaling are important services in the cloud. Traditionally, load balancing is achieved through either hardware or software appliances. Hardware appliances perform well but have several drawbacks. They are fairly expensive and are typically bought for managing peaks even if average volumes are 10% of peak. Further, they lack flexibility in terms of adding custom load balancing algorithms. They also lack multi-tenancy support. To address these concerns, most public clouds have adopted software load balancers that typically also comprise an auto scaling service. However, software load balancers do not match the performance of hardware load balancers. In order to avoid a single point of failure, they also require complex clustering solutions which further drives their cost higher. In this context, we present HAVEN - a system for holistic load balancing and auto scaling in a multi-tenant cloud environment that is naturally distributed, and hence scalable. It supports multi-tenancy and takes into account the utilization levels of different resources as part of its load balancing and auto scaling algorithms. HAVEN leverages software-defined networking to ensure that while the load balancing algorithm (control plane) executes on a server running network controller software, the packets to be load balanced never leave the data plane. For this reason, HAVEN is able to provide performance at par with a hardware load balancer while still providing the flexibility and customizability of a software load balancer. We validate HAVEN on a hardware setup and our experiments confirm that it achieves high performance without any significant overheads.	algorithm;autoscaling;bespoke;cloud computing;cluster analysis;control plane;data center;data haven;experiment;forwarding plane;holism;image scaling;load balancing (computing);middlebox;multitenancy;network interface controller;openflow;reliability engineering;scalability;server (computing);single point of failure;software appliance;software deployment;software-defined networking;testbed	Rishabh Poddar;Anilkumar Vishnoi;Vijay Mann	2015	2015 7th International Conference on Communication Systems and Networks (COMSNETS)	10.1109/COMSNETS.2015.7098681	round-robin dns;network load balancing services;parallel computing;real-time computing;logic gate;computer science;load balancing;resource management;operating system;cloud testing;distributed computing;computer network	HPC	-22.430184308659594	60.35249812429367	99835
db327950171a53ac53be19bce993b42eb8272aa6	a study on the optimal heartbeat interval for high available systems	high availability	Abstrat A high-available cluster system provides the service without interruptions when a failure occurs in any node consisting of a cluster. Each node sends heartbeat signal periodically to other nodes in the system to indicate that it is still alive. Checkpoint and rollback schemes are used to reduce a loss of computation in the presence of failures. This paper analyzes the expected task execution cost depending on a checkpoint interval and a heartbeat interval, and compares the performance. From this analysis, we can choose the optimal heartbeat interval as well as checkpoint interval to minimize cost for task execution.	application checkpointing;computation;failure rate;overhead (computing);transaction processing system	Jooyong Park;Jai-Hoon Kim	2002			real-time computing;computer science;operating system;high availability	HPC	-22.533665427217635	50.77728843573182	99877
7730125ba227162ef9871fe6b790ea36183e7e77	performance tuning policies for application level fault tolerance in distributed object systems	software replication;dependable systems;fault tolerance	In distributed object systems, application level fault tolerance is often attained by appropriate object replication policies. These policies aim at increasing the exhibited service availability by masking potential faults that do not recur after recovery. Existing middleware support infrastructures allow customizing object replication properties. However, since fault tolerance has a significant impact in the perceived service performance, there is a need for a suitable quantitative design technique, which allows comparing different replication policies by trading off the caused overhead cost against the achieved fault-tolerance effectiveness. We are also interested in taking into account different concerns in a combined manner (e.g. fault tolerance combined with load balancing and multithreading). This paper presents experimental evidence for the most important performance tradeoffs revealed in a simulation-based study. We considered different cases of object request loss behavior for the faulty objects, as well as, a number of request-retry strategies. The experiments took place in two different application workload levels for varied fault detection settings. We provide results for the combined effects of the studied replication policies with two specific load-balancing strategies. The presented results constitute a valuable experience report for performance tuning object replication policies for application level fault tolerance.	distributed computing;distributed object;experiment;fault detection and isolation;fault tolerance;load balancing (computing);middleware;multithreading (computer architecture);overhead (computing);perceived performance;performance tuning;retry;simulation;thread (computing)	Theodoros Soldatos;Nantia Iakovidou	2006	J. Comput. Meth. in Science and Engineering		fault tolerance;real-time computing;computer science;operating system;database;distributed computing;software fault tolerance	HPC	-22.588556884294935	57.29173281688117	100084
10e9f52b396fbea91db3873e86a7259b79093bd9	effects of virtualization on network and processor performance using open vswitch and xen server	data center open vswitch xenserver network virtualization cloud computing;software;virtualization;network virtualization;virtualization throughput servers hardware switches security software;data center;open vswitch;servers;virtualisation cloud computing computer centres public domain software resource allocation virtual machines;xenserver;cloud computing virtual resources network virtualization hardware cost energy consumption resource management processor virtualization open source tools open vswitch xenserver tcp udp round trip time rtt data centers;switches;security;cloud computing;throughput;hardware	Cloud computing is based on virtualization, where a single physical resource is virtualized into multiple virtual resources. Processor and network virtualization offer many advantages like saving in hardware cost, energy consumption, human effort and management of resources. In this paper we have evaluated the effect of network and processor virtualization using popular open source tools, Open vSwitch and Xen Server. Based on a number of experiments in which we compared virtualized with non virtualized scenarios, we found that the virtual network using Open vSwitch is secure. Moreover TCP and UDP throughput is not much effected but there is an increase in average Round Trip Time (RTT). Similarly, processor virtualization on Xen Server does not affect much the average schedule time in comparison with a non-virtualized machine. We thus conclude that in general a slight decrease in the performance in case of virtualization is not significant as compared with the advantages we get from virtualization when using Open vSwitch with XenServer. This work motivates for the application of virtualization using Open vSwitch and Xen Server instead of using non-virtualized environments for setting up data centers.	cloud computing;data center;experiment;open vswitch;open-source software;server (computing);throughput	Adnan Noor Mian;Ali Mamoon;Raees Khan;Ashiq Anjum	2014	2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing	10.1109/UCC.2014.124	embedded system;data center;throughput;full virtualization;virtualization;thin provisioning;application virtualization;cloud computing;network switch;computer science;virtual machine;information security;operating system;hardware virtualization;storage virtualization;server;computer network	HPC	-21.820007720335123	59.69829251262722	100159
184120361be28cd0e59727b7037b75a12d6a2d12	a market-based model for resource advanced reservation in grid computing system	advanced reservation;grid computing		grid computing	Dandan Yu;Yanxiang He;Guoqing Tu;Maofu Liu	2005			grid computing;parallel computing;drmaa;computer science;reservation;distributed computing	HPC	-29.631337668077418	47.989825980357026	100225
295c81ab853789e048dada25f0aa46eeb7a00486	concurrency control with java and relational databases	concurrency control java relational databases guidelines information systems safety control systems computer languages system testing performance analysis;information resources;information systems;perforation;web based information system;information systems java internet information resources concurrency control relational databases;relational database;internet;concurrency control;relational databases;redundant concurrency control java relational databases world wide web based information systems system implementation system testing;system safety;control strategy;java	As web–based information systems usually run in concurrent environment, the complexity for implementing and testing those systems is significantly high. Therefore it is useful to have guidelines to introduce concurrency control, avoiding ad hoc control strategies, which may have a negative impact in efficiency and may not guarantee system safety. This paper defines guidelines for concurrency control in web–based information systems implemented in Java with relational databases. In particular, we show where Java and relational database concurrency control mechanisms should be used in order to implement our concurrency control strategy. Additionally, we analyze the performance of different concurrency controls approaches. The main point of the guidelines is to guarantee system correctness without redundant concurrency control, both increasing performance and guaranteeing safety.	concurrency control;control system;control theory;correctness (computer science);hoc (programming language);information system;java;relational database;system safety	Sérgio Soares;Paulo Borba	2002		10.1109/CMPSAC.2002.1045112	timestamp-based concurrency control;optimistic concurrency control;real-time computing;isolation;java concurrency;concurrent computing;relational database;computer science;concurrency control;database;distributed computing;multiversion concurrency control;non-lock concurrency control;serializability;concurrent object-oriented programming;distributed concurrency control	DB	-24.649800941176636	47.7438775332079	100415
672894c21667474f8dde350da0f2748202c11b79	tolerating stop failures in distributed maple		In previous work we have introduced some fault tolerance mechanisms to the parallel computer algebra system Distributed Maple such that a session may tolerate the failure of computing nodes and of connections between nodes without overall failure. In this paper, we extend this fault tolerance by some advanced mechanisms. The first one is the reconnection of a node after a connection failure such that a session does not deadlock. The second mechanism is the restarting of a node after a failure such that the session does not fail. The third mechanism is the change of the root node such that a session may tolerate also the failure of the root without overall failure.	computer algebra system;deadlock;fault tolerance;maple;parallel computing;tree (data structure)	Károly Bósa;Wolfgang Schreiner	2005	Scalable Computing: Practice and Experience		parallel computing;real-time computing;computer science;distributed computing	HPC	-19.5590431145654	47.86705847794758	100760
bdfb42e3f185502785d068a2d534746c06684b9b	modeling i/o performance variability using conditional variational autoencoders		Storage system performance modeling is crucial for efficient use of heterogeneous shared resources on leadership-class computers. Variability in application performance, particularly variability arising from concurrent applications sharing I/O resources, is a major hurdle in the development of accurate performance models. We adopt a deep learning approach based on conditional variational auto encoders (CVAE) for I/O performance modeling, and use it to quantify performance variability. We illustrate our approach using the data collected on Edison, a production supercomputing system at the National Energy Research Scientific Computing Center (NERSC). The CVAE approach is investigated by comparing it to a previously proposed sensitivity-based Gaussian process (GP) model. We find that the CVAE model performs slightly better than the GP model in cases where training and testing data come from different applications, since CVAE can inherently leverage the whole data from multiple applications whereas GP partitions the data and builds separate models for each partition. Hence, the CVAE offers an alternative modeling approach that does not need pre-processing; it has enough flexibility to handle data from a wide variety of applications without changing the inference approach.	autoencoder;computer;deep learning;encoder;gaussian process;generative modelling language;heart rate variability;iw engine;input/output;intel edison;performance prediction;performance tuning;preprocessor;spatial variability;supercomputer;system monitoring;variational principle	Sandeep Madireddy;Prasanna Balaprakash;Philip H. Carns;Robert Latham;Robert B. Ross;Shane Snyder;Stefan M. Wild	2018	2018 IEEE International Conference on Cluster Computing (CLUSTER)	10.1109/CLUSTER.2018.00022	computer science;deep learning;decoding methods;input/output;machine learning;distributed computing;gaussian process;inference;data modeling;test data;artificial intelligence;supercomputer	HPC	-22.047137051322647	56.266059536430085	100872
d8fc6a795ee2a734df405325b10a801babb55961	an architecture of thin client-edge computing collaboration for data distribution and resource allocation in cloud			edge computing;thin client	Aymen Abdullah Alsaffar;Pham Phuoc Hung;Eui-nam Huh	2017	Int. Arab J. Inf. Technol.		machine learning;architecture;thin client;artificial intelligence;computer science;cloud computing;distributed computing;edge computing;resource allocation	HPC	-29.114834367554614	56.69763212149319	100903
9de8002f1210dcb84290bf9283d94572fdac5241	predictable and configurable component-based scheduling in the composite os	component based operating systems;scheduling	This article presents the design of user-level scheduling hierarchies in the Composite component-based system. The motivation for this is centered around the design of a system that is both dependable and predictable, and which is configurable to the needs of specific applications. Untrusted application developers can safely develop services and policies, that are isolated in protection domains outside the kernel. To ensure predictability, Composite enforces timing control over user-space services. Moreover, it must provide a means by which asynchronous events, such as interrupts, are handled in a timely manner without jeopardizing the system. Towards this end, we describe the features of Composite that allow user-defined scheduling policies to be composed for the purposes of combined interrupt and task management. A significant challenge arises from the need to synchronize access to shared data structures (e.g., scheduling queues), without allowing untrusted code to disable interrupts. Additionally, efficient upcall mechanisms are needed to deliver asynchronous event notifications in accordance with policy-specific priorities, without undue recourse to schedulers. We show how these issues are addressed in Composite, by comparing several hierarchies of scheduling polices, to manage both tasks and the interrupts on which they depend. Studies show how it is possible to implement guaranteed differentiated services as part of the handling of I/O requests from a network device while diminishing livelock. Microbenchmarks indicate that the costs of implementing and invoking user-level schedulers in Composite are on par with, or less than, those in other systems, with thread switches more than twice as fast as in Linux.	component-based software engineering;data structure;deadlock;dependability;differentiated services;interrupt;linux;network switch;networking hardware;operating system;scheduling (computing);user space	Gabriel Parmer;Richard West	2013	ACM Trans. Embedded Comput. Syst.	10.1145/2536747.2536754	embedded system;real-time computing;computer science;operating system;distributed computing;scheduling	Embedded	-26.00756943256942	50.46161114279578	101339
2fc8b27aceeabeb4ff0638edbec1e0f6b49ba742	the pointcast network	broadcast news;wall street journal;data center;client server;managing data;indexation;self tuning database;geographic distribution	PointCast Inc, the inventor and leader in broadcast news via the Internet and corporate intranets was founded in 1992 to deliver news as it happens from leading sources such as CNN, the New York Times, Wall Street Journal Interactive Edition and more, directly to a viewer's computer screen. The PointCast Network is an integrated client/server system. The system give users control of selecting kinds of information the client retrieves and, within limits, the frequency of those retrievals. The system is divided into client and server segments, referred to as “PointCast Client” and “the DataCenter” respectively. PointCast client is a program that runs on the user's Internet-connected computer, and performs a number of functions in addition to retrieving information from the DataCenter. The server side of the system, known as the PointCast DataCenter, supports the client by providing compelling content in a timely fashion. The Data Center is composed of multiple sites geographically distributed all over US, each carrying a number of industrial strength web servers called “PointServers”. The PointServers are highly customized and “infinitely” scalable to serve close to 200 million requests handled by the PointCast network in a day. The PointCast network receives content from over 100 different sources via satellite links or over the internet. A cluster of servers in the Data Center run customized processes round the clock which assimilate data from various sources to index, format and store it in multiple content databases. This presentation will describe the basic plumbing of the PointCast Network and how some of the challenges of establishing one of the busiest data centers in the world were addressed and implemented. It will focus on following issues:fault tolerance load balancing achieving scalability through pre-caching on servers packaging information to optimize internet bandwidth usage minimizing data latency. 	client–server model;computer monitor;data center;database;fault tolerance;internet;intranet;load balancing (computing);pointcast (dotcom);scalability;server (computing);server-side;the new york times;the wall street journal;web server	Satish Ramakrishnan;Vibha Dayal	1998		10.1145/276304.276361	data center;computer science;database;fat client;internet privacy;world wide web;client–server model;server;inter-process communication	Networks	-25.719593486213252	55.77528983264386	101435
9c56013f737ef1bb42f8c9f57520459bb6e56467	load balancing at the edge of chaos: how self-organized criticality can lead to energy-efficient computing	energy efficiency;chaos;nonlinear dynamical systems;distributed computing;automata;scheduling algorithms energy efficiency nonlinear dynamical systems distributed computing;scheduling algorithms;energy consumption;scheduling;complex systems;chaos load modeling automata quality of service energy consumption complex systems scheduling;quality of service;load modeling	This paper investigates a self-organized critical approach for dynamically load-balancing computational workloads. The proposed model is based on the Bak-Tang-Wiesenfeld sandpile: a cellular automaton that works in a critical regime at the edge of chaos. In analogy to grains of sand, tasks arrive and pile up on the different processing elements or sites of the system. When a pile exceeds a certain threshold, it collapses and initiates an avalanche of migrating tasks, i.e., producing load-balancing. We show that the frequency of such avalanches is in power-law relation with their sizes, a scale-invariant fingerprint of self-organized criticality that emerges without any tuning of parameters. Such an emergent pattern has organic properties such as the self-organization of tasks into resources or the self-optimization of the computing performance. The conducted experimentation also reveals that the system has a critical attractor in the point in which the arrival rate of tasks equals the processing power of the system. Taking advantage of this fact, we hypothesize that the processing elements can be turned on and off depending on the state of the workload as to maximize the utilization of resources. An interesting side effect is that the overall energy consumption of the system is minimized without compromising the quality of service.	abelian sandpile model;adaptive behavior;apache axis;cellular automaton;cloud computing;computation;converge;edge of chaos;emergence;fingerprint;interconnection;load balancing (computing);mathematical optimization;quality of service;queueing theory;run time (program lifecycle phase);scheduling (computing);self-organization;self-organized criticality;semiconductor consolidation;signaling protocol;simulation;virtual machine;work stealing	Juan Luis Jim&#x00E9;nez Laredo;Frédéric Guinand;Damien Olivier;Pascal Bouvry	2017	IEEE Transactions on Parallel and Distributed Systems	10.1109/TPDS.2016.2582160	complex systems;parallel computing;real-time computing;simulation;computer science;operating system;distributed computing;scheduling;algorithm	HPC	-19.467634315174603	59.32841151635447	101689
55dfb33ce0c30b6fc8468205843abb2d81605384	scientific workflow interoperability framework	notification messages;workflow design;publish subscribe messaging;workflow management;scientific workflow;scientific workflows;qa75 electronic computers computer science;workflow interoperability;triana;web services;kepler;asynchronous processing;ws eventing;taverna	This paper presents a scientific workflow interoperability framework (SWIF), based on a publish/subscribe asynchronous messaging system, aiming to achieve workflow interoperability. Using the SWIF approach, interoperability can be achieved among workflow management systems that operate remotely. SWIF realises asynchronous processing which reduces dependencies between processes in a workflow management system and presents a decoupled approach, highly suitable to a coarse-grained distributed environment. The design of SWIF is based on a set of web services that follow WS-Eventing specifications, enabling the scientists using the workflow management system to make processes on a workflow engine available to other workflows, so users can register their interests in this event. When this event occurred, all registered workflows receive a notification message. A scientific application example, using Triana, Taverna and Kepler workflow management systems, is demonstrated to verify the approach and evaluate system performance.	.net framework;inter-process communication;interoperability;kepler;message-oriented middleware;publish–subscribe pattern;web service;workflow engine;taverna	Ahmed Alqaoud;Ian J. Taylor;Andrew Jones	2010	IJBPIM	10.1504/IJBPIM.2010.033177	web service;workflow;xpdl;computer science;knowledge management;workflow management coalition;database;windows workflow foundation;law;world wide web;workflow management system;workflow engine;kepler;workflow technology	DB	-30.180695338247617	52.588162597777156	101696
3d9c59f7cd0747876618a323ecb939b25ee85e76	skew handling techniques in sort-merge join	edge caching;indexation;proxy based caching;dynamic content	Joins are among the most frequently executed operations. Several fast join algorithms have been developed and extensively studied; these can be categorized as sort-merge, hash-based, and index-based algorithms. While all three types of algorithms exhibit excellent performance over most data, ameliorating the performance degradation in the presence of skew has been investigated only for hash-based algorithms. However, for sort-merge join, even a small amount of skew present in realistic data can result in a significant performance hit on a commercial DBMS. This paper examines the negative ramifications of skew in sort-merge join and proposes several refinements that deal effectively with data skew. Experiments show that some of these algorithms also impose virtually no penalty in the absence of data skew and are thus suitable for replacing existing sort-merge implementations. We also show how sort-merge band join performance is significantly enhanced with these refinements.	algorithm;categorization;elegant degradation;hash function;join (sql);sort-merge join	Wei Li;Dengfeng Gao;Richard T. Snodgrass	2002		10.1145/564691.564711	real-time computing;computer science;dynamic web page;database;distributed computing	DB	-21.40372485517661	48.94741266052914	101819
479c762c2161402c8bf47a71a28216069e3ce3b9	distributed simulation in manufacturing: epochs: integrated commercial off-the-shelf software for agent-based electric power and communication simulation	commercial off the shelf software;agent based;computer network;control system;commercial off the shelf;complex system;power system;power grid;communication protocol;electric power;distributed simulation;simulation tool	This paper reports on the development of the Electric Power and Communication Synchronizing Simulator (EPOCHS), a distributed simulation environment. Existing electric power simulation tools accurately model power systems of the past, which were controlled as large regional power pools without significant communication elements. However, as power systems increasingly turn to protection and control systems that make use of computer networks, these simulators are less and less capable of predicting the likely behavior of the resulting power grids. Similarly, the tools used to evaluate new communication protocols and systems have been developed without attention to the roles they might play in power scenarios. EPOCHS utilizes multiple research and commercial off-the-shelf (COTS) systems to bridge the gap. EPOCHS is also notable for allowing users to transparently encapsulate complex system behavior that bridges multiple domains through the use of a simple agent-based framework.	agent-based model;bridging (networking);complex system;control system;ibm power systems;open-source software;power supply;simulation	Kenneth M. Hopkinson;Kenneth P. Birman;Renan Giovanini;Denis V. Coury;Xiaoru Wang;James S. Thorp	2003		10.1145/1030818.1030972	embedded system;communications protocol;real-time computing;simulation;electric power;computer science;engineering;control system;electric power system	Mobile	-33.1776597337514	60.414135291533505	102134
5cc507cc55c4b1c2558f6dc5760fe75aa928e1bf	a concurrency control protocol for continuously monitoring moving objects	databases;moving object;continuously monitoring moving objects;protocols;link based strategies concurrency control protocol continuously monitoring moving objects location aware devices gps rfid moving object management continuous query processing b tree based framework lock coupling strategies;query processing concurrency control mobile computing;lock coupling strategies;query processing;location aware devices;continuous query;concurrency control protocols monitoring vehicles query processing conference management engineering management computer science databases mobile computing;conference management;multi user;indexes;real world application;gps;monitoring;link based strategies;indexing;engineering management;rfid;b tree based framework;moving object management;concurrency control;continuous query processing;search problems;location awareness;vehicles;computer science;mobile computing;data consistency;concurrency control protocol	The increasing usage of location-aware devices, such as GPS and RFID, has made moving object management an important task. Especially, being demanded in real-world applications, continuous query processing on moving objects has attracted significant research efforts. However, little attention has been given to the design of concurrent continuous query processing for multi-user environments. In this paper, we propose a concurrency control protocol to efficiently process continuous queries over moving objects on a B-tree-based framework. The proposed protocol integrates link-based and lock-coupling strategies, and is proven to assure serializable isolation, data consistency, and deadlock-free for continuous query processing. Concurrent operations including continuous query, object movement, and query movement are protected under this protocol. Experimental results on benchmark data sets demonstrated the scalability and efficiency of the proposed concurrent framework.	b-tree;benchmark (computing);bx-tree;concurrency (computer science);concurrency control;correctness (computer science);database;deadlock;experiment;global positioning system;location awareness;lock (computer science);multi-user;r-tree;radio-frequency identification;scalability;serializability;space-filling curve;temporal database	Jing Dai;Chang-Tien Lu;Lien Fu Lai	2009	2009 Tenth International Conference on Mobile Data Management: Systems, Services and Middleware	10.1109/MDM.2009.24	radio-frequency identification;database index;communications protocol;search engine indexing;query optimization;real-time computing;global positioning system;computer science;concurrency control;database;distributed computing;data consistency;mobile computing	DB	-24.48386147679196	49.22043865767334	102195
3bad48fa79d15208d82382b7b50cbc81eced36b0	special issue: distributed simulation and real-time applications	distributed simulation;real time application		real-time transcription;simulation	Stephen John Turner	2004	Concurrency - Practice and Experience	10.1002/cpe.935	computer science;distributed computing	Embedded	-29.87183375067879	46.435608692264225	102206
de597ca4b0967139c9937680dfb2890a50537f05	environment-sensitive performance tuning for distributed service orchestration		Modern distributed systems are designed to tolerate unreliable environments, i.e., they aim to provide services even when some failures happen in the underlying hardware or network. However, the impact of unreliable environments can be significant on the performance of the distributed systems, which should be considered when deploying the services. In this paper, we present an approach to optimize the performance of distributed systems under unreliable deployed environments, through searching for optimal configuration parameters. To simulate an unreliable environment, we inject several failures in the environment of a service application, such as node crash in the cluster, network failures between nodes, resource contention in nodes, etc. Then, we use a search algorithm to find the optimal parameters automatically in the user-selected parameter space, under the unreliable environment we created. We have implemented our approach in a testing-based framework and applied it to several well-known distributed service systems.	distributed computing;orchestration (computing);performance tuning;resource contention;search algorithm;simulation	Yu Lin;Franjo Ivancic;Pallavi Joshi;Gogul Balakrishnan;Malay K. Ganai;Aarti Gupta	2014		10.1007/978-3-319-17353-5_18	performance tuning;orchestration (computing);parameter space;computer science;fold (higher-order function);distributed computing;search algorithm;crash	HPC	-24.536250647179767	57.03869057433478	102208
323cc9e8e4d380d648b364c638e61c9fd11369e3	editorial: applications of distributed computing environments			distributed computing	Mark Baker	1999	Concurrency - Practice and Experience	10.1002/(SICI)1096-9128(19990410)11:4%3C167::AID-CPE437%3E3.0.CO;2-9	computer science;end-user computing;distributed computing;distributed design patterns;grid computing;distributed computing environment;autonomic computing	HPC	-30.076718420625284	47.042540494838775	102474
84c7b2e0b273a34de379f7f3fa9321e16e5ff291	a comprehensive review of the data replication techniques in the cloud environments: major trends and future directions	replication;static;big data;dynamic;cloud computing	Nowadays, in various scientific domains, large data sets are becoming an important part of shared resources. Such huge mass of data is usually stored in cloud data centers. Therefore, data replication which is generally used to manage large volumes of data in a distributed manner speeds up data access, reduces access latency and increases data availability. However, despite the importance of the data replication techniques and mechanisms in cloud environments, there has not been a comprehensive study about reviewing and analyzing its important techniques systematically. Therefore, in this paper, the comprehensive and detailed study and survey of the state of art techniques andmechanisms in this field are provided. Also, we discuss the data replicationmechanisms in the cloud systems and categorize them into two main groups including static and dynamic mechanisms. Static mechanisms of data replication determine the location of replication nodes during the design phase while dynamic ones select replication nodes at the run time. Furthermore, the taxonomy and comparison of the reviewedmechanisms are presented and their main features are highlighted. Finally, the related open issues and some hints to solve the challenges are mapped out. The review indicates that some dynamic approaches allow their associated replication strategies to be adjusted at run time according to changes in user behavior and network topology. Also, they are applicable for a service-oriented environment where the number and location of the users who intend to access data often have to be determined in a highly dynamic fashion. & 2016 Elsevier Ltd. All rights reserved.	access time;categorization;cloud computing;consistency model;data access;data center;denial-of-service attack;network switch;network topology;replication (computing);responsiveness;run time (program lifecycle phase);server (computing);service-orientation;service-oriented device architecture;service-oriented software engineering;telecommunications link	Bahareh Alami Milani;Nima Jafari Navimipour	2016	J. Network and Computer Applications	10.1016/j.jnca.2016.02.005	replication;real-time computing;big data;cloud computing;computer science;operating system;distributed computing;world wide web;computer security;computer network	HPC	-20.755215222566054	59.61002917619851	102589
90ab5f52c2ce5e2388a7b21d6c9b63c07c9240d4	a concurrency control protocol for read-only transactions in real-time secure database systems	read only transactions;erbium;database system;transaction scheduling;protocols;concurrent computing;performance evaluation;query processing;real time secure database systems;database management systems;performance evaluations;real time;freezing method;database management systems concurrency control access protocols software performance evaluation security of data query processing transaction processing real time systems;software performance evaluation;interference;satisfiability;deadline;serializability guarantee;concurrency control protocols real time systems database systems concurrent computing data security erbium transaction databases timing interference;transaction databases;rot freeze;security requirements;database systems;concurrency control;access protocols;data conflict resolution;real time requirements;transaction processing;read only transaction;serializability guarantee concurrency control protocol read only transactions real time secure database systems queries update transactions transaction scheduling deadline data conflict resolution rot freeze freezing method real time requirements security requirements performance evaluations legacy protocols;security of data;update transactions;queries;concurrency control protocol;legacy protocols;real time systems;data security;timing	A read-only transaction (ROT) or a query is a transaction that only reads data items, without modifying them. When we use a protocol that takes care of ROTs distinctively from update transactions, the number of conflicts between ROTs and update transactions can be reduced. As a result, a database system can schedule many more transactions within a given deadline, and it improves the degree of concurrency by reducing the number of data conflicts. In this paper, we propose a new concurrency control protocol for ROTs, called ROT-FREEZE, in real-time secure database systems. ROT-FREEZE improves on the freezing method that has been suggested by C. Park et al. (1998) to resolve conflicts between real-time requirements and security requirements. In ROT-FREEZE, ROTs are never aborted due to update transactions. By our performance evaluations, ROT-FREEZE is proved to be better than other legacy protocols for real-time secure database systems. We also prove that the proposed protocol guarantees serializability and satisfies both real-time and security requirements.	concurrency (computer science);concurrency control;database;read-only memory;real-time transcription	Heejun Han;Seog Park;Chanjung Park	2000		10.1109/RTCSA.2000.896426	communications protocol;optimistic concurrency control;real-time computing;erbium;concurrent computing;database transaction;transaction processing;distributed transaction;computer science;concurrency control;database;distributed computing;interference;data security;serializability;acid;satisfiability;schedule	DB	-23.516591992905585	48.03725493680839	102770
e92baf2b2cc4d5a7814092f6f323a1e63f32d8a0	approaches to improve the resources management in the simulator cloudsim	broker;message of availability;reservation;cloudsim;cloud computing	In Cloud Computing, service availability and performance are two significant aspects to be dealt with. These two aspects can deteriorate or even stopping the services of Cloud Computing, if they are not taken into account. Users see that cloud computing delivers elastic computing services to users on the basis of their needs. This paper aims at improving operation of service of the Cloud Computing environment. Cloud service must be available some is the situations and powerful being by a response time reduced at a user’s request. To meet this aim, we propose, in this paper, two approaches which aim at returning a better availability of Datacenters without deteriorating the performances for the answers of the users. The first uses the principle of the messages of availability and the second uses the principle of reservation in advance.	cloud computing;cloudsim;cloudlet;data center;elasticity (cloud computing);multi-agent system;performance;response time (technology);separation of mechanism and policy	Ghalem Belalem;Fatima Zohra Tayeb;Wieme Zaoui	2010		10.1007/978-3-642-16167-4_25	cloud computing security;cloud computing;computer science;end-user computing;cloud testing;distributed computing;utility computing;world wide web;computer security	HPC	-26.61128803033687	59.23211374387806	102821
75efd1443aa4cc870d09ce04f1355d6d920e07c5	cloudbus toolkit for market-oriented cloud computing	virtual machine;virtualization;energy efficient;resource allocation;resource manager;risk management;service management;software systems;software development kit;market orientation;computer software;content delivery network;utility computing;modelling and simulation;cloudbus;conference proceeding;cloud computing	This keynote paper: (1) presents the 21st century vision of computing and identifies various IT paradigms promising to deliver computing as a utility; (2) defines the architecture for creating market-oriented Clouds and computing atmosphere by leveraging technologies such as virtual machines; (3) provides thoughts on market-based resource management strategies that encompass both customer-driven service management and computational risk management to sustain SLA-oriented resource allocation; (4) presents the work carried out as part of our new Cloud Computing initiative, called Cloudbus: (i) Aneka, a Platform as a Service software system containing SDK (Software Development Kit) for construction of Cloud applications and deployment on private or public Clouds, in addition to supporting market-oriented resource management; (ii) internetworking of Clouds for dynamic creation of federated computing environments for scaling of elastic applications; (iii) creation of 3 party Cloud brokering services for building content delivery networks and e-Science applications and their deployment on capabilities of IaaS providers such as Amazon along with Grid mashups; (iv) CloudSim supporting modelling and simulation of Clouds for performance studies; (v) Energy Efficient Resource Allocation Mechanisms and Techniques for creation and management of Green Clouds; and (vi) pathways for future research.	cloud computing;cloudsim;content delivery network;digital distribution;e-science;emoticon;image scaling;internetworking;platform as a service;risk management;service-level agreement;simulation;software deployment;software development kit;software system;virtual machine	Rajkumar Buyya;Suraj Pandey;Christian Vecchiola	2009		10.1007/978-3-642-10665-1_4	virtualization;simulation;cloud computing;risk management;resource allocation;service management;computer science;virtual machine;resource management;operating system;distributed computing;utility computing;efficient energy use;world wide web;software system	HPC	-31.32796749171122	54.83391303517667	102962
778b6c9dd8aa51fed9b7132e20cdf1a2acba928c	classification of weak correctness criteria for real-time database applications	database system;transaction databases real time systems database systems timing robots sparks throughput delay scheduling algorithm job shop scheduling;real time;satisfiability;concurrency control;schedules weak correctness criteria classification real time database applications transaction processing logical consistency constraints timing constraints conflict serializability concurrent execution semantics statewise serializability relaxed serializability controlled inconsistent read operation algorithm;transaction processing;concurrency control transaction processing timing real time systems;real time systems;timing;time constraint	For real-time database systems, transaction processing must satisfji not only logical consistency constraints but also timing constraints. The conflict serializability is too restrictive to achieve the acceptable throughput and predictable response time. Moreover, serializability may not be necessary for concurrent execution and different correctness criteria may be applied to different applications depending on the semantics and the requirements of transactions. In this paper, we classify the consistency into six forms of consistency and propose a relaxed serializability, called statewise serializability, as the weakest form of consistency in our classification. The statewise serializability alleviates the strictness of serializability by allowing for controlled inconsistent read operation. It can be properly used as a correctness criterion in real-time database applications. We also present the algorithm that determines whether the schedules are statewise serializable, und compare it to other correctness criteria.	algorithm;correctness (computer science);database;real-time clock;real-time transcription;requirement;response time (technology);serializability;throughput;transaction processing	Kyu-Woong Lee;Seog Park	1996		10.1109/CMPSAC.1996.544163	global serializability;real-time computing;commitment ordering;transaction processing;computer science;two-phase locking;concurrency control;database;distributed computing;serializability;snapshot isolation;satisfiability;schedule	DB	-23.554982813252977	48.10909296251712	103035
471edd9e816ac1195cdcc292fd8df4e2e947c5ac	middleware design issues for application management in heterogeneous networks	middleware intelligent networks resource management computer network management application software quality of service protocols communication system control computer architecture control systems;internet distributed object management application program interfaces quality of service open systems computer network management;dynamic reconfiguration;application management;dynamic management middleware design application management heterogeneous networks internet distributed computing cross domain resource management quality of service expected requirements application service level agreement distributed networked resources resource management system interface resource type interoperability dynamic service discovery management functions monitoring controlling reporting dynamic reconfiguration;distributed networks;resource manager;distributed computing;satisfiability;internet;dynamic service discovery;application program interfaces;computer network management;distributed object management;middleware;service level agreement;quality of service;open systems;resource management system;heterogeneous network	With the increase of the heterogeneity of Internet and the complexity of distributed computing, application management is facing more and more sophisticated requirements. In a dynamic, synergistic, and geographically broad environment, cross-domain resource management is becoming an important factor for application Quality of	application lifecycle management;distributed computing;internet;middleware;requirement;synergy	Ye Tian;S. Frank;Vassilis Tsaoussidis;Hussein G. Badr	2000		10.1109/ICON.2000.875779	middleware;real-time computing;the internet;heterogeneous network;quality of service;computer science;resource management;operating system;middleware;database;distributed computing;open system;application lifecycle management;human resource management system;computer network;satisfiability	HPC	-31.963703111735292	49.7585246897539	103084
83324c6f4c800fc420513a4002577c1b10cb83b7	commitment in a partitioned distributed database	distributed system;distributed database	Network partition is among the hardest failure types in a distributed system even if all processors and links are of fail-stop type. We address the transaction commitment problem in a partitioned distributed database. It is assumed that partitions are detectable. The approach taken is conservative - that is, the same transaction cannot be committed by one site and aborted by another. A new and very general formal model of protocols operating in a partitioned system is introduced and protocols more efficient than the existing ones are constructed.	central processing unit;distributed computing;distributed database;fail-stop;mathematical model;network partition	K. V. S. Ramarao	1988		10.1145/50202.50247	real-time computing;distributed transaction;computer science;database;distributed computing;distributed database;distributed concurrency control	DB	-23.598951664489665	46.97603156278912	103330
59a2b81923048d426c865210e9fdffd5ad88d981	using extended hierarchical quorum consensus to control replicated data: from traditional voting to logical structures	hierarchy;protocols;distributed systems;distributed database systems;hierarchies;high availability;software reliability;distributed databases	In large distributed computing systems, where copies of the same logical data are stored at many diierent sites, the replica control protocol must reduce communication costs when forming the quorums required at each access to the replicated data, in order to improve the system response time. An interesting way to achieve this reduction is to organize the copies into some logical structure, like a grid or a tree, and then to use this structure to form smaller quorums. Another example of a structure used to form smaller quorums is a generic tree in which only the leaves correspond to copies of the replicated data. This paper presents a new replica control protocol that logically organizes the copies as leaves of a generic tree, but introduces the blind-write as another operation (besides the traditional read and write) deened over the replicated data. With this third operation , the proposed protocol turns out to be a generalization of the traditional voting scheme and others existing protocols that use a symmetrical logical structure (i.e., a structure in which the responsibility for controlling the replicated data is equally shared by all copies) to form the access quorums. The proposed protocol also makes possible to achieve better relations between quorums size and the total number of copies, even under high availability requirements.	blind write;consensus (computer science);distributed computing;high availability;requirement;response time (technology)	Nabor das Chagas Mendonça;Ricardo de Oliveira Anido	1994			computer science;theoretical computer science;operating system;database;distributed computing;management;distributed database;hierarchy	DB	-22.790728018425355	48.02482538445765	103365
2fa85e34fd785e2d223961f55c9230237e2840ee	prediction of system critical event in virtualized medical applications	event recognition;virtualizations event recognition event prediction;event prediction;virtualizations;nonintrusive system critical event prediction framework virtualized medical application computing environment remote medical assistant internet medical services cloud computing platform vrtualization techniques reliability system critical event patterns virtual machines;virtual machining medical services reliability cloud computing biomedical equipment indexes prediction algorithms;virtualisation cloud computing medical computing safety critical software virtual machines	Medical applications need reliable and secure computing environment. Traditional medical applications only serve local requests and they are deployed locally. Along with the development of remote medical assistants, the number of requests also increases. Medical applications have be moved to the Internet and become medical services that receive many types of requests. The cloud computing platform provides reliable computing environment for medical applications. Virtualization techniques also increase the availability of medical applications. However, the nature of cloud computing allows co-resident of multiple applications, the influence of other virtual machine will affects the reliability of medical applications. System critical event patterns reflect behaviors of systems. In order to avoid system critical events that affect reliability, resources, applications, and services can be scheduled around predicted failure and limit the impact. Virtual machines, which medical services are running on, produce hundreds of system events. These system events include normal level events and critical events. In order to increase the reliability of medical services, we propose a non-intrusive system critical event prediction framework that increases the reliability of medical service in cloud computing.	cloud computing;dspace;internet;pattern recognition;real-time clock;virtual machine	Yuanyao Liu;Zhengping Wu	2014	2014 28th International Conference on Advanced Information Networking and Applications Workshops	10.1109/WAINA.2014.20	real-time computing;simulation;cloud computing;computer science;operating system;data mining;database;computer security	HPC	-32.09748851184112	56.62103560871667	103390
e2539bce33160552cafe13e7a577e2b8bfdf7397	introducing weirs: an abstraction for next generation streaming workflows		In HPC applications, it is widely understood that in situ systems will play a significant role in next generation systems. The rate that next-generation leadership machines will be able to generate data will exceed the bandwidths of the planned I/O systems, leading to a need for in situ processing of the resulting data to reduce it. There have been a number of techniques proposed for in situ workflow systems; we are concerned here specifically with the needs of publish/subscribe or streaming-based approaches, which allow the in situ system to be composed of completely separate executables running concurrently on the hardware. In this paper we introduce an abstract known as a Weir. A weir is a useful mechanism to control state in a point to point streaming workflow in the absence of a third party publish subscribe system.	bandwidth (signal processing);executable;input/output;next-generation network;publish–subscribe pattern	Erich Lohrmann;Greg Eisenhauer;Matthew Wolf	2017	2017 IEEE International Conference on Cluster Computing (CLUSTER)	10.1109/CLUSTER.2017.107	distributed computing;point-to-point;computer science;parallel computing;real-time computing;abstraction;workflow;next-generation network;in situ;weir;executable;synchronization	HPC	-24.12692381943211	53.457485020726374	103451
3e176ec637fd55d85455a789c4bcc7d3fe2f9490	towards energy-proportional clouds partially powered by renewable energy		With the emergence of the Future Internet and the dawning of new IT models such as cloud computing, the usage of data centers (DC), and consequently their power consumption, increase dramatically. Besides the ecological impact, the energy consumption is a predominant criterion for DC providers since it determines the daily cost of their infrastructure. As a consequence, power management becomes one of the main challenges for DC infrastructures and more generally for large-scale-distributed systems. In this paper, we present the EpoCloud prototype, from hardware to middleware layers. This prototype aims at optimizing the energy consumption of mono-site Cloud DCs connected to the regular electrical grid and to renewable-energy sources.	algorithm;cloud computing;data center;dawning information industry;distributed computing;epoc (operating system);emergence;first-class function;future internet;interconnection;middleware;ops5;power management;prototype;requirement;scheduling (computing);server (computing);service-level agreement;solar cell;terabyte;traffic exchange	Nicolas Beldiceanu;Barbara Dumas Feris;Philippe Gravey;Md Sabbir Hasan;Claude Jard;Thomas Ledoux;Yunbo Li;Didier Lime;Gilles Madi-Wamba;Jean-Marc Menaud;Pascal Morel;Michel Morvan;Marie-Laure Moulinard;Anne-Cécile Orgerie;Jean-Louis Pazat;Olivier H. Roux;Ammar Sharaiha	2016	Computing	10.1007/s00607-016-0503-z	embedded system;real-time computing;simulation	HPC	-26.100707619510345	60.16540049649228	103546
6857cb89b78a63c08d26535902b8a344f7614d81	invalidation-based distributed shared memory integrated into a distributed operating system	distributed shared memory		distributed operating system;distributed shared memory	Jackie Silcock;Andrzej M. Goscinski	1997			distributed concurrency control;data diffusion machine;distributed system security architecture;replication (computing);shared disk architecture;distributed shared memory;distributed memory;computer science;shared memory;distributed computing	HPC	-28.437354778267984	46.46208014483132	104073
ff6c7a83bcbb9650ec632d0e6db9210a31ac8ef5	recovery in multidatabase systems		A multidatabase consists of a collection of autonomous local databases. Systems used to manage multidatabases are called multidatabase systems (MDBSs). In such a system, global transactions are executed under the control of the MDBS. Independently, local transactions are submitted directly to a local DBS (LDBS) by local applications. An MDBS should provide a mechanism to globally manage transactions. However, global transactions are long-living and involve operations on multiple and autonomous local databases. Moreover, MDBSs do not have any information about the existence and execution order of local transactions. Thus, conventional approaches to manage transactions are unsuitable for MDBSs. In this paper, we address the reliability problem in MDBSs. For this purpose, we propose two types of protocols for making MDBSs resilient to failures. One type of protocol should enforce that, when a given global transaction completes its execution, it has the same state (committed or aborted) at every site it has run. The other type determines the actions to be triggered after failures in a multidatabase environment. These protocols can reduce the frequency of global transaction undo after the occurrence of failures, and make the MDBS able to deal with failures which may occur in a multidatabase environment.	autonomous robot;categorization;database transaction;direct-broadcast satellite;distributed transaction;pegasus;transaction processing;undo	Angelo Brayner;Theo Härder	1999			database;undo;computer science;distributed computing;database transaction	DB	-24.442803098887108	46.54481275819892	104080
721add372dc756683851093a14d46121e0cd5388	trace - an adaptive organizational policy for mas	adaptive organizational policy;protocols;multi agent system;multi agent system trace adaptive organizational policy mas task and resource allocation in a computational economy resource allocation time constraints load variations problem solving organizations task allocation protocol subtask allocation lost requests price directed resource allocation protocol rap;resource allocation;price directed resource allocation protocol;mas;trace;load variations;rap;multi agent systems;economics multi agent systems resource allocation problem solving protocols;resource management protocols time factors load management information science problem solving computational modeling cloning mobile agents proposals;task allocation protocol;task and resource allocation in a computational economy;adaptive computing;subtask allocation;economics;high performance;fair allocation;time constraints;problem solving organizations;problem solving;task allocation;lost requests;time constraint	This paper proposes an adaptive organizational policy, TRACE (Task and Resource Allocation in a Computational Economy), incorporating task and resource allocation for MAS that operate under time constraints and load variations. The MAS is comprised of several problem-solving organizations. The task allocation protocol (TAP) takes requests and plans and allocates subtasks to agents within an organization. As requests arrive arbitrarily, at any instant, some organizations could have surplus resources while others could become overloaded. In order to minimize the number of lost requests caused by an overload, the allocation of resources to organizations is changed dynamically by the price-directed resource allocation protocol (RAP). Simulation results show that TRACE exhibits high performance despite unanticipated changes in the environment.	computation;function overloading;problem solving;simulation	S. Shaheen Fatima	2000		10.1109/DEXA.2000.875105	communications protocol;real-time computing;simulation;resource allocation;computer science;artificial intelligence;trace;multi-agent system	AI	-26.71296116952171	48.58680225708817	104231
a10851bec4f0886e61a458e764c939bea2b3bfaa	fair-share scheduling for performance-asymmetric multicore architecture via scaled virtual runtime	multicore processing runtime processor scheduling scheduling program processors kernel;fairness;performance asymmetry;task scheduling fairness performance asymmetry multicore linux;scheduling embedded systems linux operating system kernels;multicore;parsec benchmark fair share scheduling performance asymmetric multicore architecture scaled virtual runtime user experience asymmetric multicore processors embedded systems kernel service kernel dynamic resource control mechanisms dynamic performance asymmetry arm little architecture linaro scheduling framework completely fair scheduler cfs linux kernel task balancing versatile express tc2 board spec cpu2006 benchmark;linux;task scheduling	As users begin to demand applications with superior user experience and high service quality, asymmetric multicore processors are increasingly adopted in embedded systems due to their architectural benefits in improved performance and power savings. While fair-share scheduling is a crucial kernel service for such applications, it is still in an early stage when it comes to performance-asymmetric multicore architecture. In this paper, we propose a new fair-share scheduler by adopting the notion of scaled CPU time which reflects performance asymmetry between different types of cores. Our scheduler can work with kernel's dynamic resource control mechanisms since it makes use of a varying performance ratio between cores and thus captures dynamic performance asymmetry such as a core's changing operating frequency. We develop our approach on top of ARM's big. LITTLE architecture which runs Linaro's scheduling framework. Since Linaro's relies on the completely fair scheduler(CFS) of the Linux kernel and CFS is virtual runtime based, we revise the notion of virtual runtime using the scaled CPU time and incorporate it into the proposed approach. As a result, our approach achieves fair-share scheduling by simply balancing tasks' virtual runtimes. To demonstrate its effectiveness, we have implemented the proposed scheduler and performed a series of experiments on ARM's Versatile Express TC2 board. We ran the SPEC CPU2006 and PARSEC benchmarks for three minutes and measured tasks' virtual runtimes. We observed that the maximum virtual runtime difference was only 0.69 seconds in our approach while the original CFS yielded the maximum difference of 8.35 seconds.	benchmark (computing);central processing unit;climate forecast system;clock rate;control system;embedded system;experiment;fair-share scheduling;filesystem-level encryption;kernel (operating system);linux;maxdiff;multi-core processor;parsec;scheduling (computing);spec#;user experience	Myungsun Kim;Soonhyun Noh;Sungju Huh;Seongsoo Hong	2015	2015 IEEE 21st International Conference on Embedded and Real-Time Computing Systems and Applications	10.1109/RTCSA.2015.10	multi-core processor;fair-share scheduling;fixed-priority pre-emptive scheduling;embedded system;parallel computing;real-time computing;dynamic priority scheduling;computer science;operating system;scheduling;linux kernel	Embedded	-21.675275949742964	60.2057238876723	104446
d38a6382b62f40887692e02147140f81deeddc5a	edfs: a semi-centralized efficient distributed file system	distributed file system	In this paper we present (1) EDFS, a simple and semi-centralized efficient distributed file system (for cloud computing environments) and (2) a load balancing and resource sharing scheme for EDFS. EDFS uses a central front-end server only to distribute keys (hashed filenames or indices) to Namenodes. Each Namenode is then responsible for holding the corresponding file (directory) information such as the i-node (metadata) and pointers to where each block of the file is stored in the cloud. Each node is also responsible for every operation (open, close, delete, modify) of the files (directories) whose keys it has. This will enable EDFS to distribute the cloud computing to all nodes in the cloud unlike the current Hadoop File System (HDFS) and Google File System (GFS) which load the Namenode with almost every operation of every data block, potentially making the Namenode a resource bottleneck.	centralized computing;clustered file system;semiconductor industry	Debessay Fesehaye;Rahul Malik;Klara Nahrstedt	2009			self-certifying file system;computer file;computer science;operating system;unix file types;database;distributed computing;open;file control block	HPC	-20.263263616324018	52.38848077085229	104691
9f93de740b4e172bafdb5d68e8d53cc8f29be89f	concurrency control mechanism for an available distributed data base system	distributed data;concurrency control	That paper presents algorithms used in an available database system to process and control operations. The structure of the system and the algorithms are designed to allow transaction processing to proceed in case of one site failure. We shall focus on the required network functionalities, the concurrency control and transaction atomicity problems. The protocols used on each site will be formally described using evaluation nets.	concurrency control;distributed database	Jean-Michel Feuvre	1985		10.1007/978-3-642-70285-3_11	timestamp-based concurrency control;optimistic concurrency control;isolation;concurrent computing;computer science;concurrency control;multiversion concurrency control;non-lock concurrency control;serializability;acid;distributed concurrency control	DB	-23.268795358162624	47.56108205845767	104786
4c715f024c86af4021c813b414bd6b55d95bfc15	complex data-intensive systems and semantic grid: applications in satellite missions	informatica;data sharing;instruments;satellite ground stations;computer networks;satellite broadcasting;computer architecture;data analysis;satellite broadcasting instruments satellite ground stations cameras grid computing geoscience computer architecture artificial satellites computer networks monitoring;geoscience;complex data;monitoring;artificial satellites;aeronautica;grid computing;semantic grid;cameras;heterogeneous network	The use of a Semantic Grid architecture can ease the deployment of complex applications, in which several organizations are involved and where resources of diverse nature (data and computing elements) are shared. This is the situation in the Space domain, with a strong demand of computational resources inscribed in an extensive and heterogeneous network of facilities and institutions. This paper presents the S-OGSA architecture, defined in the Ontogrid project, as applied into a scenario for the overall monitoring and data analysis in a Satellite Mission currently in nominal operations. Flexibility, scalability, interoperability and use of a common framework for data sharing are the main advantages of a Semantic Grid implementation in Complex and dataintensive systems.	computation;computational resource;interoperability;open grid services architecture;scalability;semantic grid;software deployment	Manuel Sánchez-Gestido;L. Blanco-Abruña;María S. Pérez-Hernández;Rafael González-Cabero;Asunción Gómez-Pérez;Óscar Corcho	2006	2006 Second IEEE International Conference on e-Science and Grid Computing (e-Science'06)	10.1109/E-SCIENCE.2006.41	semantic interoperability;embedded system;semantic computing;simulation;heterogeneous network;semantic grid;computer science;database;distributed computing;data analysis;satellite;grid computing;computer network;complex data type	HPC	-30.855311468902983	50.50262318462306	104836
7a14a077f3553c1dc2438e944b8b74a7045ba18f	monitoring multiple concurrent service level parameters with multidimensional trees	service level;simple network management protocol;search trees;video server	The introduction of new computing paradigms in the Internet as well as the increasing size and complexity of services and resources demand the development of new approaches for defining and monitoring service levels. It is often necessary to keep track of multiple concurrent service level requirements. In this paper we present a service level monitoring strategy that allows both online and offline tracking the performance of multiple concurrent resources. Data is collected with SNMP (Simple Network Management Protocol). The strategy is based on building multidimensional search trees. k-d (k-dimensional) trees are employed for online continuous monitoring, and k-d-B trees are employed for offline monitoring, based on logs of monitored data. Searching with the proposed strategy has cost O(logN) where N is the number of samplings or log size. The strategy allows clients and providers to confirm whether contract specifications were hold or not, and for how long. Experimental results are presented, including a comparison of the proposed approach with a traditional database. A practical tool was implemented and results are shown for a set of monitored Web and Video servers, as well as for monitoring data obtained from a real Telecom billing system.	b+ tree;b-tree;cloud computing;data structure;database;distributed computing;electronic billing;internet;mysql;online and offline;range searching;requirement;simple network management protocol;world wide web	Andreas Kiefer;Elias Procópio Duarte;Cristina D. Murta	2009		10.1007/978-3-642-04989-7_3	real-time computing;service level;computer science;operating system;data mining;database;distributed computing;simple network management protocol;computer security;computer network	DB	-25.238636888427386	55.46781451366088	104847
7bbedcb82e72dcf6dba970d205ef06b4be2a9fa6	i/o scheduling schemes for better i/o proportionality on flash-based ssds	computers;virtualization;interference;servers;bandwidth;linux;throughput	In cloud computing, multiple servers are consolidated into a physical machine in order to reduce the cost of deploying the servers. Guaranteeing the service level objective (SLO) of each server is one of the most important factors in a virtualization system. Particularly, isolating the I/O resources among VMs competing for a shared storage system is challenging. Recently, use of flash based Solid State Drives (SSDs) is being extended to enterprise systems. However, there are few studies for guaranteeing SLOs on such systems. In this paper, we empirically analyze the I/O behavior of a shared SSD. We show that performance SLOs of storage systems employing SSDs being shared by VMs or tasks are not satisfactory. We analyze and show that components of SSDs such as channels, DRAM buffer, and Native Command Queuing (NCQ) are the reasons behind this problem. Based on these analysis and observations, we propose two SSD-aware host level I/O schedulers that we call A+CFQ and H+BFQ, which are extensions of state-of-the-art I/O schedulers CFQ and BFQ, respectively. Through implementation and experiments on Linux, we show that the proposed I/O schedulers improve proportionality without sacrifice to performance.	cfq;cloud computing;computer data storage;dynamic random-access memory;enterprise system;experiment;flash memory;hard disk drive;i/o scheduling;input/output;linux;scheduling (computing);server (computing);solid-state drive	Jaeho Kim;Eunjae Lee;Sam H. Noh	2016	2016 IEEE 24th International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems (MASCOTS)	10.1109/MASCOTS.2016.53	throughput;parallel computing;real-time computing;virtualization;computer science;operating system;interference;linux kernel;bandwidth;server;computer network	OS	-22.082400637630887	60.037418048308524	105221
4251f21d0b24dba97d9a66ef5e81d6c22182a9b8	accomplish the application area in cloud computing		In the cloud computing application area of accomplish, we find the fact that cloud computing covers a lot of areas are its main asset. At a top level, it is an approach to IT where many users, some even from different companies get access to shared IT resources such as servers, routers and various file extensions, instead of each having their own dedicated servers. This offers many advantages like lower costs and higher efficiency. Unfortunately there have been some high profile incidents where some of the largest cloud providers have had outages and even lost data, and this underscores that it is important to have backup, security and disaster recovery capabilities. In education field, it gives better choice and flexibility to IT departments than others. The platform and applications you use can be onpremises, off-premises, or a combination of both, depending on your academic organization’s needs. With cloud computing in education, you get powerful software and massive computing resources where and when you need them. Use cloud services to best combine: *On-demand computing and storage. *A familiar development experience with ondemand scalability. *Online services for anywhere, anytime access to powerful web-based tools.	anytime algorithm;backup;cloud computing;dedicated hosting service;disaster recovery;router (computing);scalability;server (computing);web application	Nidhi Bansal;Amit Awasthi	2012	CoRR		cloud computing security;simulation;cloud computing;operating system;utility computing;world wide web;computer security;computer network	HPC	-28.48309463589697	58.60780545313356	105375
e7d48c05a3daeaf2cd2942c313cfc9bf6b880b5f	on the use of resource reservation for web services load balancing	high performance computing;dependability;web services;load balancing;cloud computing	A key issue for good performance of worldwide distributed web services is the efficiency of the load balancing mechanism used to distribute client requests among the replicated servers. The load balancing solution allows providers to make better use of their resources, soften the need for over-provision, and help tolerate abrupt load peaks. In this paper, we propose a new load balancing solution that reduces service response times by applying a protocol for the reservation of remote resources. This protocol prevents the overload of remote servers by limiting the amount of load each server can redirect to the others. We describe a middleware that supports this protocol by managing the trade of resources among a set of servers, allowing them to share their spare capacity in a transparent way. We also present the results of a large set of simulations that encompass an exhaustive set of workload scenarios based on realistic internet traffic models.	autonomous robot;experiment;function overloading;load balancing (computing);memory management;middleware;overhead (computing);rapid refresh;redirection (computing);server (computing);simulation;software deployment;testbed;web server;web service;whole earth 'lectronic link	Alan Massaru Nakai;Edmundo Roberto Mauro Madeira;Luiz Eduardo Buzato	2014	Journal of Network and Systems Management	10.1007/s10922-014-9303-y	web service;round-robin dns;network load balancing services;supercomputer;real-time computing;cloud computing;computer science;load balancing;dependability;distributed computing;computer network	HPC	-26.2539799802933	58.876533065451135	105410
d16bc928f76db3204d0d00f45754af834cfed23a	researches on scalable architecture for security information distribution service	distributed system;infrastructure information;congestion trafic;reseau pair;systeme reparti;service information;information security;congestion trafico;information infrastructure;distributed computing;service web;web service;securite donnee;igual a igual p2p;sistema repartido;internet;traffic congestion;distribution temporelle;denial of service;servicio informacion;calculo repartido;enseignement;information service;peer to peer technology;educacion;peer to peer;calcul reparti;security of data;distribucion temporal;infraestructura informacion;servicio web;teaching;denegacion de servicio;flash crowds;deni service;time distribution;ensenanza	The GEODE project is developing user-oriented Grid-based services, accessible via a portal, for social scientists who require and use 'occupational information' within their research. There are many complexities associated with social scientists’ use of data on individual occupations. These arise for example from the availability of numerous alternative occupational classifications, and the use of different occupational definitions across countries. This paper describes how the GEODE project is developing an online service which acts as a facility supporting access to numerous occupational information resources. This is achieved through an integrated Grid service which uses a Globus Toolkit 4 infrastructure and OGSADAI (Database Access and Integration) middleware to provide the necessary data indexing and matching services, accessed through a user-oriented front-end portal (using GridSphere). The paper discusses issues in the implementation and organization of these services.	geode (processor);middleware;online service provider	Haitao Chen;Chuanfu Xu;Zunguo Huang;Zhenghu Gong;Huaping Hu	2004		10.1007/978-3-540-30207-0_56	information infrastructure;web service;the internet;security information and event management;computer science;information security;security service;distributed computing;law;world wide web;computer security;denial-of-service attack;computer network	HPC	-33.28018290404804	52.93821626908236	105455
a1a70fa8f1c3e2b2b56e211c6be1bda1898466db	combining static and dynamic storage management for data intensive scientific workflows		Workflow management systems are widely used to express and execute highly parallel applications. For data-intensive workflows, storage can be the constraining resource: The number of tasks running at once must be artificially limited to not overflow the space available in the filesystem. It is all too easy for a user to dispatch a workflow which consumes all available storage and disrupts all system users. To address these issues, we present a three-tiered approach to workflow storage management: (1) A static analysis algorithm which analyzes the storage needs of a workflow before execution, giving a realistic prediction of success or failure. (2) An online storage management algorithm which accounts for the storage needed by future tasks to avoid deadlock at runtime. (3) A task containment system which limits storage consumption of individual tasks, enabling the strong guarantees of the static analysis and dynamic management algorithms. We demonstrate the application of these techniques on three complex workflows.	algorithm;data-intensive computing;deadlock;dynamic dispatch;memory management;run time (program lifecycle phase);static program analysis	Nicholas L. Hazekamp;Nathaniel Kremer-Herman;Benjamín Tovar;Haiyan Meng;Olivia Choudhury;Scott J. Emrich;Douglas Thain	2018	IEEE Transactions on Parallel and Distributed Systems	10.1109/TPDS.2017.2764897	workflow management system;real-time computing;algorithm design;deadlock;computer science;distributed computing;static analysis;workflow	HPC	-20.55896050940769	55.95849256349992	105482
921bd5c85d98d3605bfbae3ca3699ceb353ce6ce	scale-out architecture for service order processing systems	scale out service order processing operation system;queueing theory;load management load modeling servers synchronization process control computer architecture system performance;queueing theory next generation networks;scale out architecture next generation network ngn web application servers ops network operations systems queuing model device expansion so systems competing control service order processing systems;next generation networks	We propose an architecture that optimizes system performance by adding resources cost-effectively and in a timely manner to reduce operation costs. This paper presents two methods for competing control, which is one of the requirements of service order processing (SO) systems. We compared the two methods based on ease of implementing device expansion, and a queuing model. It's hoped that this technology is appropriate for network operations systems (OpS) such as SO systems, rather than general applications to web/application servers.	application server;bottleneck (software);emoticon;provisioning;queueing theory;requirement;scalability;systems architecture	Masafumi Shimizu;Hikotoshi Nakazato;Hikaru Seshake	2013	2013 IFIP/IEEE International Symposium on Integrated Network Management (IM 2013)		real-time computing;next-generation network;computer science;operating system;distributed computing;queueing theory;statistics;computer network;systems design	Arch	-27.80714056807402	57.58424956957161	105499
8a8d54a8296980bbfc09d4bd84f14e12c8d6e74f	synchronous replication of remote storage	timur mirzoev 同步性 远程存储 网络存储 虚拟 synchronous replication of remote storage	Storage replication is one of the essential requirements for network environments. While many forms of Network Attached Storage (NAS), Storage Area Networks (SAN) and other forms of network storage exist, there is a need for a reliable synchronous storage replication technique between distant sites (> 1 mile). Such technology allows setting new standards for network failover and failback systems for virtual servers; specifically, addressing the growing need for effective disaster recovery (DR) planning. The purpose of this manuscript is to identify newest technologies such as SAN/iQ and Storage VMotion that allow for remote storage synchronous replication for virtual servers. This study provides an analysis and a comparison of various SANs that create solutions for enterprise’s needs. Additionally, the interoperability of these technologies with the industry’s leading product VMware ESX Server will be discussed.	disaster recovery;failover;interoperability;network-attached storage;replication (computing);requirement;sans institute;storage area network	Timur Mirzoev	2008	CoRR		embedded system;real-time computing;storage area network;converged storage;telecommunications;computer science;operating system;information repository;computer security;computer network	HPC	-27.400363146061633	51.51380595504444	105550
5aca03dd53af0d655de153b5985f7752fd4d3766	enhancing real-time dbms performance with multiversion data and priority based disk scheduling	database system;serialization order;disk scheduling algorithms;concurrent computing;disk scheduling;application software;job shop scheduling;database management systems;processor scheduling;real time;processor scheduling concurrency control scheduling algorithm transaction databases real time systems concurrent computing job shop scheduling database systems application software runtime;i o requests;real time disk resident database system model;runtime;input output;real time disk scheduling algorithms;transaction processing concurrency control database management systems real time systems scheduling;scheduling algorithm;miss ratio;transaction databases;scheduling;database systems;concurrency control;multiversion concurrency control algorithms;miss ratio serialization order disk scheduling algorithms input output i o requests real time disk resident database system model multiversion concurrency control algorithms real time disk scheduling algorithms multiversion concurrency control;multiversion concurrency control;transaction processing;real time systems	Databases are increasingly being used in real-time applications where timeliness of result is part of the correctness criterion. Time constrained CPU scheduling is one of the critical issues for real-time systems. For a real-time database system (RTDBS) this idea must be extended to concurrency control and disk scheduling. This paper proposes real-time multiversion concurrency control algorithms to fulfil following goals: i ) increase concurrency, ii) adjust the serialization order dynamically, iii) work without a priori estimate of a transaction's runtime. We also propose new disk scheduling algorithms which consider not only the transactions which request I 1 0 but also those affected by 110. Realtime multiversion concurrency control and disk scheduling algorithms are shown to decrease miss ratio signifi;cantly.	algorithm;central processing unit;concurrency (computer science);correctness (computer science);database;i/o scheduling;multiversion concurrency control;real-time clock;real-time computing;real-time operating system;real-time transcription;scheduling (computing);serialization	W. Kim;Jaideep Srivastava	1991		10.1109/REAL.1991.160377	job shop scheduling;optimistic concurrency control;real-time computing;isolation;concurrent computing;computer science;operating system;database;distributed computing;multiversion concurrency control;scheduling;acid;i/o scheduling;snapshot isolation	Embedded	-23.593147060778062	48.173989441776335	105573
914ad80b989589e752edf95f123f0631dc28a231	implementation of large catalogs for price enforcement in b2b e-commerce	electronic commerce;client server systems;business transaction catalogs price enforcement b2b electronic commerce client server model unbalanced computing power buyer suppliers computational algorithms limited computer resources purchase order;client server;algorithm theory electronic commerce client server systems order processing;algorithm theory;order processing;electronic commerce computer architecture costs delay pricing application software electronic catalog algorithm design and analysis marketing and sales books;b2b e commerce	This paper presents an efficient architecture to implement a catalog for its application in B2B electronic commerce. The architecture is designed based on a client-server model under the constraint of the unbalanced computing power between the buyer and suppliers. Computational algorithms are designed according to this assumption of limited computer resources at the client side and ample resources at the server side. The application is to provide a complete and workable catalog solution for price enforcement when fulfilling a purchase order to eliminate costs associated with having to make amendments to a past purchase order and having to delay a business transaction.		Trung T. Pham;Simone Keller Füchter	2004	Proceedings. IEEE International Conference on e-Commerce Technology, 2004. CEC 2004.	10.1109/ICECT.2004.1319720	e-commerce;computer science;world wide web;order processing;client–server model	Robotics	-32.168116004521764	59.71168923967718	105702
c2b76777931620c4ac58b119a4022563b21f1fd2	a peer-to-peer framework for resource discovery in large-scale grids	grid computing;resource discovery;distributed hash tables.;peer-to-peer	As Grids enlarge their boundaries and users, some of their functions should be decentralized to avoid bottlenecks and guarantee  scalability. A way to provide Grid scalability is to adopt Peer-to-Peer (P2P) models to implement non hierarchical decentralized Grid services and systems. A core Grid functionality that can be effectively  redesigned using the P2P approach is resource discovery. This paper proposes a P2P resource discovery architecture aiming to manage various Grid resources and complex queries. Its  goal is two-fold: to address discovery of multiple resources, and to support discovery of dynamic resources and arbitrary  queries in Grids. The architecture includes a scalable technique for locating dynamic resources in large-scale Grids. Simulation  results are provided to demonstrate the efficiency of the proposed technique.  	peer-to-peer	Domenico Talia;Paolo Trunfio;Jingdi Zeng;Mikael Högqvist	2006		10.1007/978-0-387-72812-4_10	grid;scalability;architecture;grid computing;peer-to-peer;distributed computing;computer science	HPC	-25.481349777163654	53.8045389647695	105832
4b3f45a3a897112aaded9c8be0bd8ec9fd10fd61	supporting iot multi-tenancy on edge devices	social computing;internet of things;payloads;conferences;green computing;service computing	IoT systems that support multi-tenancy tend to use cloud-hosted device/thing abstraction or virtualization. By abstracting the IoT edge components e.g. as data streams or by virtualizing the sensors/actuators, it becomes possible to avoid or resolve access conflicts. However, by using hosting the abstractions/virtualizations away from the edge components in the cloud, significant latency is introduced. This paper presents the idea of light-weight virtual resources that can be hosted on edge devices and thus offer the same abstraction/virtualization without latency.	cloud computing;multitenancy;sensor	Mayra Samaniego;Ralph Deters	2016	2016 IEEE International Conference on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData)	10.1109/iThings-GreenCom-CPSCom-SmartData.2016.38	green computing;embedded system;payload;real-time computing;human–computer interaction;computer science;operating system;distributed computing;services computing;world wide web;computer security;internet of things;social computing;computer network	Visualization	-31.246043391462972	57.90915485176255	105973
2f4e1095f65440e9311f401df81f075db9f33805	load balancing in the presence of random node failure and recovery	chaos;availability;uncertainty;resource allocation;resource management;distributed computing;delay effects;indexing terms;node recovery;computer networks;computer fault tolerance;probabilistic model;statistical distributions;resource allocation computer networks fault tolerant computing;fault tolerant computing;computational modeling;distributed computing system;performance analysis;load management;load balancing;random node failure;robustness;load management distributed computing delay effects chaos availability uncertainty statistical distributions robustness performance analysis computational modeling;load balance;node recovery load balancing random node failure distributed computing;failure recovery;regenerative process;article	In many distributed computing systems that are prone to either induced or spontaneous node failures, the number of available computing resources is dynamically changing in a random fashion. A load-balancing (LB) policy for such systems should therefore be robust, in terms of workload re-allocation and effectiveness in task completion, with respect to the random absence and re-emergence of nodes as well as random delays in the transfer of workloads among nodes. In this paper two LB policies for such computing environments are presented: The first policy takes an initial LB action to preemptively counteract the consequences of random failure and recovery of nodes. The second policy compensates for the occurrence of node failure dynamically by transferring loads only at the actual failure instants. A probabilistic model, based on the concept of regenerative processes, is presented to assess the overall performance of the system under these policies. Optimal performance of both policies is evaluated using analytical, experimental and simulation-based results. The interplay between node-failure/recovery rates and the mean load-transfer delay are highlighted	distributed computing;emergence;lattice boltzmann methods;load balancing (computing);simulation;spontaneous order;statistical model	Sagar Dhakal;Majeed M. Hayat;Jorge E. Pezoa;Chaouki T. Abdallah;J. Douglas Birdwell;John N. Chiasson	2006	Proceedings 20th IEEE International Parallel & Distributed Processing Symposium	10.1109/IPDPS.2006.1639293	parallel computing;real-time computing;computer science;load balancing;resource management;theoretical computer science;distributed computing;statistics;computer network	HPC	-22.452622859492667	53.11266470724298	106014
dac25692dad13ab8921fd614cce2945c26c65d54	real-time on-demand motion video change detection in the sensor web environment	service-oriented wps architecture;real-time processing;sensor web environment;video change detection case;motion-based video change;change detection;different motion video change;real-time on-demand motion video;motion video;motion video data;wps interface layer	Detecting motion-based video change, such as different types of motion video or applications using different change detection algorithms in a Web system, is difficult. This paper designs and implements architecture for a real-time or near real-time on-demand motion video change detection system using the Sensor Observation Service (SOS) and the Web Processing Service (WPS) in the Sensor Web environment. Real-time or near real-time includes sensors that obtain motion video data, SOS provides motion video data to WPS and WPS processes this data. Three solution methods are introduced: the GetObservation operation of SOS by transaction, dynamical interaction between SOS and WPS, and WPS real-time or near real-time processing. On-demand means that a developer can choose different motion video change detection algorithms under different applications or different conditions. For this purpose, a flexible, standards-based and service-oriented WPS architecture is designed, which consists of three layers: the WPS interface layer, the field interface layer and the implementation layer. To test the proposed approach, a video change detection case of monitoring a road situation is shown, which was a demonstration for Open Geospatial Consortium Web Service phase 7. The results demonstrate that the proposed approach is feasible.	algorithm;consortium;extensibility;information system;interaction;microsoft outlook for mac;real-time clock;real-time computing;real-time locating system;real-time transcription;real-time web;secure two-party computation;sensor observation service;sensor web;service-oriented device architecture;the computer journal;video;web processing service;web service;world wide web;dialog	Zeqiang Chen;Liping Di;Genong Yu;Nengcheng Chen	2011	Comput. J.	10.1093/comjnl/bxr066	computer vision;web processing service;simulation;computer science;operating system;database;world wide web	Embedded	-31.810763567225067	46.76386587513419	106181
b50249475825c3d25a0a195cc25694e7086f518f	simultaneous logging and replay for recording evidences of system failures	virtual machine;embedded system;operating system	As embedded systems take more important roles at many places, it is more important for them to be able to show the evidences of system failures. Providing such evidences makes it easier to investigate the root causes of the failures and to prove the responsible parties. This paper proposes simultaneous logging and replaying of a system that enables recording evidences of system failures. The proposed system employs two virtual machines, one for the primary execution and the other for the backup execution. The backup virtual machine maintains the past state of the primary virtual machine along with the log to make the backup the same state as the primary. When a system failure occurs on the primary virtual machine, the VMM saves the backup state and the log. The saved backup state and the log can be used as an evidence. By replaying the backup virtual machine from the saved state following the saved log, the execution path to the failure can be completely analyzed. We developed such a logging and replaying feature in a VMM. It can log and replay the execution of the Linux operating system. The experiment results show the overhead of the primary execution is only fractional.	backup;booting;embedded system;experiment;failure;hypervisor;ibm pc compatible;linux;operating system;overhead (computing);symmetric multiprocessing;tracing (software);virtual machine manager;virtual machine	Shuichi Oikawa;Jin Kawasaki	2010		10.1007/978-3-642-16256-5_15	embedded system;real-time computing;incremental backup;computer science;virtual machine;operating system	OS	-23.229813731002828	51.19930866548355	106208
3f993cae174eaa84a50f6682fea6b479af142969	bridging the high performance computing gap: the ourgrid experience	computational grid;high performance computing;grid middleware;null;bag of tasks;high performance computer;system development;middleware;high performance computing grid computing peer to peer computing biology computing middleware application software production power system management computational modeling computer simulation;peer to peer computing grid computing middleware;peer to peer grid middleware;peer to peer computing;ourgrid experience;peer to peer;grid computing;parallel applications;public free to join grid;public free to join grid high performance computing ourgrid experience peer to peer grid middleware	High performance computing is currently not affordable for those users that cannot rely on having a highly qualified computing support team. To cater for these users' needs we have proposed, implemented and deployed OurGrid. OurGrid is a peer-to-peer grid middleware that supports the automatic creation of large computational grids for the execution of embarrassingly parallel applications. It has been used to support the OurGrid Community - a public free-to-join grid that is in production since December 2004. In this paper we show how the OurGrid Community has been used to support the execution of a number of applications. Further we discuss the main benefits brought up by the system and the difficulties that have been faced by the system developers and the users and managers of the OurGrid Community.	bridging (networking);embarrassingly parallel;middleware;ourgrid;peer-to-peer;supercomputer	Francisco Vilar Brasileiro;Eliane Araújo;William Voorsluys;Milena Oliveira;Flavio de Figueiredo	2007	Seventh IEEE International Symposium on Cluster Computing and the Grid (CCGrid '07)	10.1109/CCGRID.2007.28	supercomputer;computer science;operating system;middleware;database;distributed computing;world wide web;grid computing	HPC	-29.86486410448094	51.40961105935873	106429
701eeb3d5669d2bc124c3c21c164c2fbea062eb1	a linux-based tool for hardware bring up, linux development, and manufacturing	chip;operating system;file system;fault detection	T. Venton M. Miller R. Kalla A. Blanchard In this paper we describe Bare Metal Linux (BML), a cut-down version of Linuxt 2.6 that requires no firmware, has an in-memory root file system, and runs without a virtualization layer. We designed and implemented BML in order to accelerate the bring up of POWER5e-based systems. The use of BML allows testing and validation of the POWER5-based system to be conducted in parallel with the standard path, which involves the bring up of a hypervisor, the partition firmware, and the operating system. BML, which has fast boot times and can be modified quickly, is used in fault detection during chip manufacturing, POWER5 chip verification, system-board verification, and benchmarking for performance. BML is also used to reproduce and resolve problems in Linux.	bare machine;battle management language;fault detection and isolation;firmware;hypervisor;in-memory database;linux;operating system;power5	T. Venton;M. Miller;R. Kalla;A. Blanchard	2005	IBM Systems Journal	10.1147/sj.442.0319	chip;embedded system;computer hardware;computer science;operating system;database;fault detection and isolation	OS	-26.863228999957656	51.61339324547544	106451
702cf475d6410718f24f56035a8577d928f386c0	maintaining coherent views over dynamic distributed data	distributed data;design of algorithms;data coherency;web pages;views;internet scale algorithms;web;dynamic data;dynamic data dissemination;data dissemination	"""Data delivered today over the web reflects rapid and unpredictable changes in the world around us. We are increasingly relying on content that provides dynamic, interactive, personalized experiences. To achieve this, the content of most web pages is created dynamically, by executing queries dynamically, using data that changes dynamically from distributed sources identified dynamically. A typical web page presents a """"view"""" of the world constructed from multiple sources. In this paper, we examine the nature of dynamics of distributed data and discuss fresh approaches to maintain the coherency of the views seen by the users. Achieving such coherency while developing scalable low-overhead solutions poses challenges in terms of delivering data with the required fidelity in spite of data dynamics as well as failures in the infrastructure. How these challenges can be met by the judicious design of algorithms for data dissemination, caching, and cooperation forms the crux of our work."""	algorithm;coherent;overhead (computing);personalization;scalability;web page	Krithi Ramamritham	2010		10.1007/978-3-642-11659-9_2	dynamic data;data web;computer science;web page;database;distributed computing;view;world wide web;dissemination	DB	-26.157725874561105	49.400188643479105	106624
273b362746e28a214761f1757e3fc63a14596669	an infrastructure for mechanised grading	mechanised grading;grading experimentation	Mechanised grading is now mature. In this paper, we propose elements for the next step: building a commodity infrastructure for grading. We propose an architecture for a grading infrastructure able to be used from various learning environments, a set of extensible Internet-based protocols to interact with the components of this infrastructure, and a selfcontained file format to thoroughly define an exercise and its entire life-cycle in order to ease deployment. The infrastructure is designed to be scalable, robust and secure. First experiments with a partial implementation of this infrastructure show its versatility and its neutrality. It does not impose any IDE and it respects the tenets of the embedding learning environment or course-based system thus favouring the fostering of an eco-system around mechanised grading.	ecosystem;experiment;integrated development environment;internet;programming language;robustness (computer science);scalability;software deployment	Christian Queinnec	2010			software deployment;software engineering;multimedia;architecture;computer engineering;grading (education);the internet;scalability;computer science;learning environment;extensibility;file format	Web+IR	-33.50907128992156	52.09828585382495	106730
b7dad3b4e6a8e27fc1b49e4cc84420a3d44d2d0b	automatic network services aligned with grid application requirements in carriocas project	level of service;optical network;transportation networks;grid applications;perforation;collaborative application;large scale;collaborative environment;network services;high performance	Automatic Service framework named Scheduling, Reconfiguration and Virtualization (SRV) is developed in CARRIOCAS project to enhance existing Telecom network infrastructures for supporting grid applications sharing IT resources interconnected with ultra-high performance optical networks. From the requirements of Grid applications a classification is proposed to specify the network services and their attributes. In large-scale collaborative environments, SRV solution is described to enable automatic network service operations according to high-performance computing service access. The resources hosted at datacenters are virtualized to be attached to transport network infrastructure offering uniform interfaces towards external customers. New level of service bindings is defined with network services during executions of Grid applications’ workflows. On-demand intensive computing and visualization services scenario is described in Telecom environment.	requirement;schedule (project management);supercomputer	Dominique Verchère;Olivier Audouin;Bela Berde;Agostino Chiosi;Richard Douville;Hélia Pouyllau;Pascale Vicat-Blanc Primet;Marcelo Pasin;Sebastien Soudan;T. Marcot;Veronique Piperaud;Remi Theillaud;D. Hong;Dominique Barth;Christian Cadéré;V. Reinhart;Joanna Tomasik	2008		10.1007/978-3-642-02080-3_20	intelligent computer network;database;distributed computing;level of service;world wide web	HPC	-30.226446428872205	53.072290369440495	106743
6cd06d2ebd6f05a4a0b1a579306f5384e076b1b0	requirements and design for replication services for a time series management system	replication service design;financial data processing;investments;formal specification;management system;financial management;database management systems;integrated directory services financial researchers time series management system replication service design replication service requirements database management systems publish and subscribe mechanism scheduling facility dynamic replication;economic forecasting;filters;portfolios;time series;financial management economic forecasting time series analysis database systems dynamic scheduling portfolios investments statistical analysis quality management filters;dynamic replication;publish and subscribe mechanism;statistical analysis;financial researchers;time series analysis;scheduling;integrated directory services;database systems;distributed databases;time series management system;replication service requirements;database management system;scheduling facility;formal specification time series financial data processing replicated databases distributed databases scheduling;replicated databases;quality management;dynamic scheduling	This paper analyzes how financial researchers manage large numbers of time series and how they work with these data. We show that replication services are a central facility of a time series management system and we define the requirements for such replication services. An evaluation of current time series management systems shows that they do not support replication. Neither replication systems of other database management systems nor other kinds of currently available replication systems cover our requirements. Thus, we present our design of replication services that are adapted to the needs of time series management. It is based on a publish-andsubscribe mechanism, a sophisticated scheduling facility, dynamic replication, and integrated directory ser-	database;directory (computing);general-purpose markup language;management system;requirement;scheduling (computing);time series	Werner Dreyer;Duri Schmidt;Angelika Kotz Dittrich;Manuel Bleichenbacher	1996		10.1109/SSDM.1996.506063	quality management;real-time computing;computer science;economic forecasting;time series;data mining;database;replication	DB	-32.77870048799497	50.38821990377903	106747
248bd03290955d67bb46b34bc6a2039607b6e503	job-site level fault tolerance for cluster and grid environments	distributed system;high availability;grid computing checkpointing fault tolerant computing;fault tolerant;smart failover fault tolerance cluster environment grid computing distributed system checkpoint recovery job replication job site recovery beowulf cluster globus ha oscar;checkpointing;job replication;job site recovery;performance improvement;fault tolerant computing;critical system;checkpoint recovery;fault tolerance;fault tolerance grid computing availability fault tolerant systems collaborative work distributed computing scheduling contracts mission critical systems aggregates;ha oscar;beowulf cluster;globus;grid computing;high performance;cluster environment;smart failover	"""In order to adopt high performance clusters and grid computing for mission critical applications, fault tolerance is a necessity. Common fault tolerance techniques in distributed systems are normally achieved with checkpoint-recovery and job replication on alternative resources, in cases of a system outage. The first approach depends on the system's MTTR while the latter approach depends on the availability of alternative sites to run replicas. There is a need for complementing these approaches by proactively handling failures at a job-site level, ensuring the system high availability with no loss of user submitted jobs. This paper discusses a novel fault tolerance technique that enables the job-site recovery in Beowulf cluster-based grid environments, whereas existing techniques give up a failed system by seeking alternative resources. Our results suggest sizable aggregate performance improvement during an implementation of our method in Globus-enabled HA-OSCAR. The technique called ''smart failover"""" provides a transparent and graceful recovery mechanism that saves job states in a local job-manager queue and transfers those states to the backup server periodically, and in critical system events. Thus whenever a failover occurs, the backup server is able to restart the jobs from their last saved state"""	aggregate data;backup;beowulf cluster;critical system;distributed computing;downtime;failover;fault tolerance;grid computing;high availability;job scheduler;job stream;mean time to repair;mission critical;server (computing);supercomputer;transaction processing system	Kshitij Limaye;Chokchai Leangsuksun;Zeno Greenwood;Stephen L. Scott;Christian Engelmann;Richard Libby;Kasidit Chanchio	2005	2005 IEEE International Conference on Cluster Computing	10.1109/CLUSTR.2005.347043	fault tolerance;parallel computing;real-time computing;computer science;operating system;distributed computing;computer network	HPC	-22.27994232168359	50.75812401691672	106834
81fedeec58a282d12dfcb69a2df921404a4c250e	bpcs: a block-based service process caching strategy to accelerate the execution of service processes	semantic cache;service process;web service	Composing a set of Web services as a service process is becoming a common practice, but it involves multiple service invocations over the network, which incurs a huge time cost. To accelerate its execution, we propose an engine-side block-based service process caching strategy (BPCS). It is based on, and derives its advantages from, three key ideas. First, the invocation of Web service embodies semantics which enables the application of semantic-based caching. Second, cachable blocks are identified from a service process and each block is equiped with a separate cache so that the time overhead of service invocation and caching can be minimized. Third, a replacement strategy is introduced taking into account time and space factors to manage the space allocation for a process with multiple caches. The algorithms and methods used in BPCS are introduced in detail. Finally, BPCS is validated with a detailed performance study on real service processes and Web services via comparison experiments, which shows considerable improvements of BPCS over other strategies.	algorithm;business planning and control system;cpu cache;cache (computing);experiment;overhead (computing);run time (program lifecycle phase);shortest path problem;web service	Tingjie Jia;Jian Cao;Yan Yao;Zitai Ma	2016	2016 IEEE International Conference on Web Services (ICWS)	10.1109/ICWS.2016.28	web service;real-time computing;computer science;operating system;database;law;world wide web	HPC	-19.397408649865095	54.43130507908514	106935
87ef0b7df7a70cfe03c12fa0261323486e0f7306	shared execution of recurring workloads in mapreduce		With the increasing complexity of data-intensive MapReduce workloads, Hadoop must often accommodate hundreds or even thousands of recurring analytics queries that periodically execute over frequently updated datasets, e.g., latest stock transactions, new log files, or recent news feeds. For many applications, such recurring queries come with user-specified service-level agreements (SLAs), commonly expressed as the maximum allowed latency for producing results before their merits decay. The recurring nature of these emerging workloads combined with their SLA constraints make it challenging to share and optimize their execution. While some recent efforts on multi-job optimization in MapReduce have emerged, they focus on only sharing work among ad-hoc jobs on static datasets. Unfortunately, these sharing techniques neither take the recurring nature of the queries into account nor guarantee the satisfaction of the SLA requirements. In this work, we propose the first scalable multi-query sharing engine tailored for recurring workloads in the MapReduce infrastructure, called “Helix”. Helix deploys new sliced window-alignment techniques to create sharing opportunities among recurring queries without introducing additional I/O overheads or unnecessary data scans. And then, Helix introduces a cost/benefit model for creating a sharing plan among the recurring queries, and a scheduling strategy for executing them to maximize the SLA satisfaction. Our experimental results over real-world datasets confirm that Helix significantly outperforms the state-of-art techniques by an order of magnitude.	apache hadoop;data logger;data-intensive computing;hoc (programming language);input/output;mapreduce;mathematical optimization;optimization problem;query optimization;query plan;requirement;scalability;scheduling (computing);service-level agreement	Chuan Lei;Zhongfang Zhuang;Elke A. Rundensteiner;Mohamed Y. Eltabakh	2015	PVLDB	10.14778/2752939.2752941	real-time computing;computer science;data mining;database	DB	-19.567632809356596	55.45677056120115	106959
14e62fa79d9357641c47f06589fbd3ec9039c23b	self-stabilizing distributed file system	file system;distributed file system;spanning tree;distributed algorithm;reading and writing	A self-stabilizing distributed file systems is presented. The system constructs and maintains a spanning tree for each file volume. The spanning tree consists of the servers that have volume replicas and caches for the specific file volume. The spanning trees are constructed and maintained by self-stabilizing distributed algorithms. A self-stabilizing synchronizer enforces file system semantics and consistent replications. File system updates use the tree to implement file read and write operations. A prototype of the system based on the Linux operating system has been implemented.	clustered file system;self-stabilization	Shlomi Dolev;Ronen I. Kat	2005	J. High Speed Networks		fork;self-certifying file system;distributed algorithm;parallel computing;torrent file;indexed file;device file;computer file;andrew file system;spanning tree;computer science;stub file;versioning file system;operating system;fstab;unix file types;ssh file transfer protocol;journaling file system;distributed computing;open;distributed file system;file system fragmentation;file area network;file control block;virtual file system	HPC	-20.489741006563595	49.04966704314879	107101
7d6dbe3fbbae5a084908d6c25b192a64aed8c3ab	consistency oracles: towards an interactive and flexible consistency model specification		Many modern distributed storage systems emphasize availability and partition tolerance over consistency, leading to many systems that provide weak data consistency. However, weak data consistency is difficult for both system designers and users to reason about. Formal specifications offer precise descriptions of consistency behavior, but they require expertise and specialized tools to apply to real software systems. In this paper, we propose and describe consistency oracles, an alternative way of specifying the consistency model of a system that provides interactive answers, making them easier and more flexible to use in a variety of ways. A consistency oracle mimics the interface of a distributed storage system, but returns all possible values that may be returned under a given consistency model. This allows consistency oracles to be directly applied in the testing and verification of both distributed storage systems and the client software that uses those systems.	client (computing);clustered file system;computer data storage;consistency model;formal specification;network partition;processor consistency;software system;tier 2 network;universal instantiation;weak consistency;xojo	Beom Heyn Kim;Sukwon Oh;David Lie	2017		10.1145/3102980.3102994	causal consistency;theoretical computer science;strong consistency;consistency model;data consistency;weak consistency;data mining;sequential consistency;computer science;release consistency;eventual consistency	OS	-25.076741877668233	51.51458174711573	107233
603f11da592ac53fbed8c791019adebe8b957fb9	the design of the dents dns server	domain name;clean design;good karma;corba-based control facility;future direction;server implementation;dents dns server;free software;dents main feature;modular driver architecture;server design	Dents is a server implementation of the Internet’s Domain name System. Dents main features are a modular driver architecture, a CORBA-based control facility, a replaceable tree system, a clean design and good karma. Dents is free software, licensed under version 2 of the GPL. In this paper, I describe the design of Dents, concentrating on the innovations and evolutions it embodies, and including the future directions in which we hope to take the server. I describe some of the problems we’ve had. Finally, I summarize some lessons about server design which Dents reflects.	common object request broker architecture;internet;server (computing)	Todd Lewis	1999			world wide web;computer security	OS	-32.8103319315983	51.60361165212402	107455
0ed1cc44b317c1213ddb6257c9306f75e07b499e	cloud storage and bioinformatics in a private cloud deployment: lessons for data intensive research		This paper describes service portability for a private cloud deployment, including a detailed case study about Cloud Storage and bioinformatics services developed as part of the Cloud Computing Adoption Framework (CCAF). Our Cloud Storage design and deployment is based on Storage Area Network (SAN) technologies, details of which include functionalities, technical implementation, architecture and user support. Experiments for data services (backup automation, data recovery and data migration) are performed and results confirm backup automation is completed swiftly and is reliable for data-intensive research. The data recovery result confirms that execution time is in proportion to quantity of recovered data, but the failure rate increases in an exponential manner. The data migration result confirms execution time is in proportion to disk volume of migrated data, but again the failure rate increases in an exponential manner. In addition, benefits of CCAF are illustrated using several bioinformatics examples such as tumour modelling, brain imaging, insulin molecules and simulations for medical training. Our Cloud Storage solution described here offers cost reduction, timesaving and user friendliness.	archive;backup;bioinformatics;cloud computing;cloud storage;data recovery;data-intensive computing;failure rate;run time (program lifecycle phase);simulation;software deployment;speedup;storage area network;terabyte;the times;time complexity;usability;virtual machine	Victor I. Chang;Robert John Walters;Gary B. Wills	2012		10.1007/978-3-319-04519-1_16	simulation;computer science;operating system;data mining;world wide web	HPC	-25.236285571211532	58.082343147956216	107748
4b003fd7a61e7249ade0d69aeb647a3829088e67	a problem-specific fault-tolerance mechanism for asynchronous distributed systems	distributed algorithms;cluster computing;fault tolerant;tree searching fault tolerant computing distributed algorithms;fault tolerance fault tolerance mechanism distributed systems idle computers computational resource branch and bound problems reliability decentralized algorithm membership protocol;simulation framework;fault tolerant systems fault tolerance algorithm design and analysis internet computer science computer networks scalability protocols middleware laboratories;fault tolerant computing;asynchronous distributed system;resource availability;tree searching;branch and bound;wide area network	The idle computers on a local area, campus area, or even wide area network represent a significant computational resource—one that is, however, also unreliable, het erogeneous, and opportunistic. We describe an algorithm that allows branch-and-boundproblems to be solved in such environments. In designing this algorithm, we faced two challenges: (1) scalability, to effectively exploit the va riably sized pools of resources available, and (2) fault tolerance , to ensure the reliability of services. We achieve scalability through a fully decentralized algorithm, in which the dy namically available resources are managed through a membership protocol. We guarantee fault tolerance in the sense that the loss of up to all but one resource will not affect the quality of the solution. For propagating information reliably, we use epidemic communication for both the membership protocol and the fault-tolerance mechanism. We have developed a simulation framework that allows us to evaluate design alternatives. Results obtained in this framewor k suggest that our techniques can execute scalably and reli-	algorithm;branch and bound;byzantine fault tolerance;central processing unit;computation;computational resource;computer;distributed computing;hall-effect thruster;mathematical optimization;problem domain;run time (program lifecycle phase);scalability;simulation;terminate (software);tree structure	Adriana Iamnitchi;Ian T. Foster	2000		10.1109/ICPP.2000.876065	distributed algorithm;fault tolerance;parallel computing;real-time computing;computer cluster;computer science;theoretical computer science;operating system;distributed computing;branch and bound	HPC	-24.89341820314008	53.254405125792935	107878
e9e81d88b8533df22cd6b77ad3c4b204951d1383	caching highly compute-intensive cloud applications: an approach to balancing cost with performance	databases;cache storage;cloud computing cache storage;distributed caching architecture compute intensive cloud applications cost balancing cloud computing compute intensive application scaling business logic replication cloud caches system responsiveness tco;cache economies;cache measurement cloud caches scaling cache economies cache performance;frequency domain analysis;catalogs;cloud caches;scaling;time factors;business;cache performance;time factors catalogs cloud computing performance gain business frequency domain analysis databases;performance gain;time factor;cache measurement;cloud computing	"""With Cloud Computing, scaling compute-intensive applications out to thousands of nodes has become a matter of minutes. If with an increasing number of users the system throughput reaches inacceptable values, additional computational power can be purchased on demand and """"as you go"""". Thereby, the newly added instance nodes channel user requests to yet more servers and this way distribute the entire system load across an increased number of physical machines. Even though the standard cloud approach aims at replicating business logic to scale out, we believe that leveraging cloud caches can result into even better and yet cheaper scaling. While only one cache instance can often replace multiple service instances, the charged prices for either instance are the same. Throughout this paper, we are going to examine how by using cloud caches system responsiveness can be greatly improved. Furthermore, we are going to show how an at least similar level of throughput can be achieved at a significantly reduced system TCO, by introducing a distributed caching architecture to an existing cloud application."""	business logic;cpu cache;cache (computing);cloud computing;image scaling;load (computing);marginal model;mathematical optimization;responsiveness;scalability;software as a service;tf–idf;throughput;total cost of ownership	Robert Neumann;Eric Goltzer;Reiner R. Dumke;Andreas Schmietendorf	2011	2011 Joint Conference of the 21st International Workshop on Software Measurement and the 6th International Conference on Software Process and Product Measurement	10.1109/IWSM-MENSURA.2011.18	parallel computing;real-time computing;computer science;distributed computing	Arch	-22.712556628718687	59.82666831575222	108165
0967a175d1fa8bf0543e34efa3561937684c8304	metabolic ring: tape based renewable system	i2c renewable computing metabolic computing model metabolic ring;renewable computing;mesh connected norma metabolic ring tape based renewable system metaboloid tape metabolic computing model;metabolic computing model;metabolic ring;i2c;computational modeling recycling computer architecture protocols fault tolerance fault tolerant systems maintenance engineering;power aware computing;sustainable development green computing power aware computing recycling;recycling;sustainable development;green computing	In previous papers, we proposed a metabolic computing model to realize sustainable information systems. We believe that this model has high fault tolerance and sustainability. We also proposed an architecture for the metabolic computing model based on a mesh connected NORMA, organized as several metaboloids. However, since this requires a complex delivery and recycling system, it is hard to implement. In this paper, we propose a metabolic ring, which is based on a metaboloid tape. We also discuss a possible architecture for the metabolic ring, which has a high probability of being realized.	communications protocol;fault tolerance;information system	Minoru Uehara;Hideki Mori	2013	2013 Eighth International Conference on Broadband and Wireless Computing, Communication and Applications	10.1109/BWCCA.2013.79	green computing;embedded system;real-time computing;computer science;distributed computing;sustainable development;recycling	HPC	-32.42819155384117	47.04836231013103	108341
6d2604c4ccf9f4ec486d02b0aa08f82642725bae	clientvisor: leverage cots os functionalities for power management in virtualized desktop environment	virtual machine;client virtualization;commercial off the shelf;operating system;user experience;power management;power consumption	As an emerging trend, virtualization is more and more widely used in today's computing world. But, the introduc-tion of virtual machines bring trouble for the power man-agement (PM for short), since the operating system can not directly access and control the hardware as before. Solu-tions were proposed to manage the power in the server con-solidation case. However, such solutions are VMM-centric: the VMM gathers the PM decisions of the guests as hints, and makes the final decision to manipulate the hardware. These solutions do not fit well for the virtualized desktop environment, which is highly interactive with the users.  In this paper, we propose a novel solution, called Cli-entVisor, to manage the power in the virtualized desktop environment. The key idea of our scheme is to leverage the functionalities of the Commercial-Off-The-Shelf (COTS) operating system, which actually interacts with the user, to manage the power of the processor and the peripheral de-vices in all possible cases. VMM coordinates the PM deci-sions of the guests only at the key points. By prototype implementation and experiments, we find our scheme re-sults in 22% lower power consumption in the static power usage scenario, and about 8% lower in the dynamic sce-nario than the corresponding cases of Xen. Moreover, the experimental data shows that the deployment of our scheme will not deteriorate the user experience.	desktop computer;experiment;hypervisor;naruto shippuden: clash of ninja revolution 3;operating system;peripheral;power management;prototype;server (computing);software deployment;user experience;virtual machine manager	Huacai Chen;Hai Jin;Zhiyuan Shao;Ke Yu;Kun Tian	2009		10.1145/1508293.1508312	embedded system;user experience design;real-time computing;computer science;virtual machine;operating system	OS	-26.55808018511276	56.01253800585387	108358
83e4b48e7507e96b8ecee3c69cb9b076e0b0fa2b	using operational standards to enhance system performance	software tool;service level;system performance;data center;graphical method;user behavior	There are only three reasons for a data center to vary from service level objectives, i.e., volume, mix, and efficiency. The first two are aspects of user behavior, while only the third is under the full control of the data center. With the proliferation of online systems, user behavior is affecting the data center in realtime. Management processes, technology and software tools exist to provide the basis for scientific management of the data center. Key to this endeavor is the ability to model the system and present in graphic form the relationship between the system and user behavior characteristics. This paper points out the source of data, existing software tools, and graphic methods and includes data and the results from a study and simulation of a data center.	data center;service-level agreement;simulation	David R. Vincent	1981		10.1145/1500774.1500780	simulation;data center services;human–computer interaction;power usage effectiveness;computer science;systems engineering	ML	-31.598545018671764	56.70389973963647	108433
f4072ae4749293984fe4d9ac1fdb0ea23b915ff1	practical aspects of ip take-over mechanisms	ip take over mechanisms;ip take over scheme;fail over scheme;transparent replication;protocols;ip based replication;load shedding;fault tolerant;computer crashes;servers ip networks lead computer crashes protocols fault tolerance conferences;resource allocation;resource management;service replication;client server systems;computer fault tolerance;servers;fault tolerant computing;internet;lead;fault tolerance;load balancing;atomic broadcast;ip networks;conferences	Transparent replication has been viewed as the holy grail of fault-tolerant computing. We discuss issues arising when using atomic broadcast to replicate services that are accessed via the Internet. We show how some of these issues can be addressed by the use of IP based replication schemes. In particular, we argue for the replication of services using a simple IP take-over scheme. Also, we show how a simple fail-over scheme can be combined with a load shedding and balancing mechanism.	atomic broadcast;failover;fault tolerance;internet;load shedding;load balancing (computing);self-replication;server (computing);server-side;stateless protocol	Christof Fetzer;Neeraj Suri	2003	2003 The Ninth IEEE International Workshop on Object-Oriented Real-Time Dependable Systems	10.1109/WORDS.2003.1267533	real-time computing;computer science;distributed computing;computer network	Networks	-26.289844250491377	57.153814215813554	108436
64d79acdee2c729a735d5e5d18b3f6af10f3e12e	adaptive protocols for survivability of transactions operating on replicated objects	transaction management;critical threshold;network operating systems;real time;distributed operating systems;adaptive protocol;real time operating system;adaptive transaction management;distributed operating system;adaptive concurrency control;safety critical software;distributed object management;concurrency control;safety critical system;failure rate;transaction processing;transaction criticality adaptive protocols transaction survivability replicated objects transaction model distributed safety critical real time applications transaction dependent criticality threshold adaptive techniques distributed concurrency control safety critical real time requirements standard concurrency control protocols concurrency control algorithm deadline failure rates survivability melody model reactive features reactive protocol adaptive transaction management adaptive version o2pl adt adaptive measures hard deadlines distributed real time operating system melody;real time application;protocols programmable control adaptive control concurrency control real time systems operating systems adaptive systems power system management power generation collaboration;distributed object management safety critical software transaction processing network operating systems concurrency control real time systems;safety critical systems;real time systems	We present a novel transaction model for distribut ed safety-critical real-time applications in which transactions become increasingly cri tical under failures of subsequent incarnations. When a transaction-dependent criticality thr es old has been reached the transaction is considered essentially critical or hard, i.e. the deadline has to be met lest the system is doomed to be in a disastrous state. The corresponding adaptive m easures (criticality, sensitivity) to provide that potentially all hard deadlines are met, i.e. that t he system survives, are based on the adaptive services of the distributed real-time operating sys tem MELODY for safety-critical applications which will be briefly summarized. The stepwise defi nition of transaction criticality and sensitivity utilizes the nested transaction structure as mu ch as the fact that these measures are defined in, and supported by, MELODY. As the major contribution of this paper we propose adaptive techniques for distributed concurrency control under sa fety-critical real-time requirements such as evolving from the modification and implementation o f standard concurrency control protocols. In particular, we refrain from traditional features like roll-back, or restart after preemption, because they are costly and have an unpredictable eff ect on meeting hard deadlines. Through extensive distributed experiments we compare the perf ormance of a recent concurrency control algorithm (O2PL) to its adaptive version O2PL.ADT, re sulting in a clear advantage of the latter one, both in terms of deadline failure rates and su rvivability. Since the MELODY model is currently extended by adding reactive features (object and task similarity) we confirm our findings in reporting on early experiments with the reactive protocol versions O2PL.SIM and O2PL.ADT.SIM.	algorithm;concurrency (computer science);distributed concurrency control;experiment;nested transaction;perf (linux);preemption (computing);real-time clock;real-time transcription;reflow soldering;replication (computing);requirement;self-organized criticality;stepwise regression	Horst F. Wedde;Sabine Böhm;Wolfgang Freund	2001		10.1109/WORDS.2001.945128	real-time computing;distributed transaction;computer science;database;distributed computing;serializability	DB	-23.726124978871	49.21466810048121	108543
dbcdb4c402756b2b5ac910b9eb17ddb412290d16	pslo: enforcing the xth percentile latency and throughput slos for consolidated vm storage	achievable rate;covert channel;capacity bound;transmission scheme;thermal	It is desirable but challenging to simultaneously support latency SLO at a pre-defined percentile, i.e., the Xth percentile latency SLO, and throughput SLO for consolidated VM storage. Ensuring the Xth percentile latency contributes to accurately differentiating service levels in the metric of the application-level latency SLO compliance, especially for the application built on multiple VMs. However, the Xth percentile latency SLO and throughput SLO enforcement are the opposite sides of the same coin due to the conflicting requirements for the level of IO concurrency. To address this challenge, this paper proposes PSLO, a framework supporting the Xth percentile latency and throughput SLOs under consolidated VM environment by precisely coordinating the level of IO concurrency and arrival rate for each VM issue queue. It is noted that PSLO can take full advantage of the available IO capacity allowed by SLO constraints to improve throughput or reduce latency with the best effort. We design and implement a PSLO prototype in the real VM consolidation environment created by Xen. Our extensive trace-driven prototype evaluation shows that our system is able to optimize the Xth percentile latency and throughput for consolidated VMs under SLO constraints.	best-effort delivery;concurrency (computer science);fairness measure;hypervisor;mathematical optimization;memory timings;prototype;queueing theory;register renaming;requirement;semiconductor consolidation;throughput	Ning Li;Hong Jiang;Dan Feng;Zhan Shi	2016		10.1145/2901318.2901330	thermal;embedded system;real-time computing;covert channel;computer science;operating system	OS	-21.828882408345656	60.3088350848978	108637
0ec63a596c56efb75cb823d3f0cc6c6988c300df	opensaf and vmware from the perspective of high availability	opensaf;high availability;availability virtualization redundancy silicon measurement middleware hardware;virtualization;vmware;formal specification;vmware high availability virtualization opensaf saforum;object oriented programming;virtual machines;virtual machines cloud computing formal specification object oriented programming;virtualized environments opensaf cloud services computational services cloud infrastructure virtual machines software components availability solutions virtualization vendors vmware ha vmware ft saforum specifications;saforum;cloud computing	Cloud services are becoming one of the most popular means of delivering computational services to users who demand services with higher availability. Virtualization is one of the key enablers of the cloud infrastructure. Availability of the virtual machines along with the availability of the hosted software components are the fundamental ingredients for achieving highly available services in the cloud. There are some availability solutions introduced by virtualization vendors like VMware HA and VMware FT. At the same time the SAForum specifications and OpenSAF as a compliant implementation offer a standard based open solution for service high availability. In this paper, we investigate these solutions for availability through experiments, compare them according to metrics and based on the results propose architectures that combine them to provide highly available applications in virtualized environments.	bare machine;baseline (configuration management);cloud computing;component-based software engineering;computation;experiment;hardware virtualization;high availability;hypervisor;open-source software;virtual machine	Ali Nikzad;Ferhat Khendek;Maria Toeroe	2013	Proceedings of the 9th International Conference on Network and Service Management (CNSM 2013)	10.1109/CNSM.2013.6727853	full virtualization;real-time computing;virtualization;cloud computing;computer science;virtual machine;operating system;hardware virtualization;formal specification;distributed computing;service virtualization;high availability;object-oriented programming	HPC	-29.93211410739661	55.35246191640627	108656
076e1729089f2501991437ef4c7a4e19f7631ff9	dynamic clusters available under clusterix grid	linux cluster;computer network;dynamic clustering	The increase of computer networks speed paired with the ubiquity of inexpensive, yet fast and generously equipped hardware offers many organizations an affordable way to increase the available processing power. Clusters, hyperclusters and even Grids, not so long ago seen only in huge datacenters, can now be found helping many small organizations in solving their computational needs. CLUSTERIX is a truly distributed national computing infrastructure with 12 sites (local Linux clusters) located across Poland. The computing power of the CLUSTERIX can be increased dramatically by connecting additional clusters. These clusters are called dynamic because it is assumed that they will be connected to the core infrastructure in a dynamic manner, using an automated procedure. In the paper we present the design foundation of the Cumulus hypercluster deployed at Wroclaw University of Technology together with the method for its integration as a dynamic cluster in the CLUSTERIX	cumulus;linux	Jan Kwiatkowski;Marcin Pawlik;Gerard Frankowski;Kazimierz Balos;Roman Wyrzykowski;Konrad Karczewski	2006		10.1007/978-3-540-75755-9_99	parallel computing;computer cluster;computer science;operating system;distributed computing;world wide web	HPC	-27.80601509555604	53.487544080241406	108952
f1affa3241efd23836cc2ea861ad9286ae694bcc	shanghaigrid and intelligent urban traffic applications	service composition;service orientation;optimal parallel block size;pvp;qr;distributed computing;traffic management;lu;data center;data aggregation;risc;internet application;grid computing;is research;scalapack	Grid computing is becoming the preferred basis for large-scale distributed computing and Internet applications. ShanghaiGrid launched in Shanghai, China is a grid infrastructure that aggregates several heterogeneous supercomputers, data centers and other applications scattered in different organizations in Shanghai. One of the major focuses in the ShanghaiGrid project is research and development of an intelligent urban traffic management system based on the ShanghaiGrid system software. The system software provides a platform in a service-oriented way for resource encapsulation and management, service scheduling and accounting, data aggregation and adaptive transmission, as well as service composition and reliability support. This paper presents the ShanghaiGrid system software, with the focus on key technologies for advanced ShanghaiGrid applications, and the implementation of intelligent urban traffic management based on realtime traffic data.	data aggregation;data center;distributed computing;encapsulation (networking);grid computing;internet;scheduling (computing);service composability principle;service-oriented device architecture;supercomputer	Feilong Tang;Minglu Li;Minyi Guo	2007		10.1145/1375783.1375811	data aggregator;embedded system;data center;reduced instruction set computing;active traffic management;parallel computing;computer science;operating system;distributed computing;computer security;grid computing;computer network	HPC	-29.6464321496903	48.42064118682591	109062
f9b034352f3268cb5592cbf7736d1df18f3c2742	tengu: an experimentation platform for big data applications	cloud computing big data;iminds research institute fault tolerance database technologies big data management solutions resource cost configuration time geni compatible test beds fed4fire compatible test beds us federation of test beds eu federation of test beds data processing storage technologies cloud technologies hadoop storm openstack tengu as a service approach virtual wall large scale emulab testbed;big data computer architecture storms software distributed databases monitoring servers	Big data applications have stringent service requirements for scalability and fault-tolerance and involve high volumes of data, high processing speeds and large varieties of database technologies. In order to test big data management solutions, large experimentation facilities are needed, which are expensive in terms of both resource cost and configuration time. This paper presents Tengu, an experimentation platform for big data applications that can automatically be instantiated on GENI (US federation of test beds) and Fed FIRE (EU federation of test beds)compatible test beds. Tengu allows for automatic deployments of several data processing, storage and cloud technologies, including Hadoop, Storm and Open Stack. The paper discusses the Tengu architecture, the Tengu-as-a-service approach and a demonstration of an automated instantiation of the Tengu experimentation suite on the Virtual Wall, a large-scale Emulab testbed at the Minds research institute in Europe.	apache hadoop;application programming interface;big data;data store;fault tolerance;nosql;open-source software;representational state transfer;requirement;response time (technology);sql;scalability;software deployment;testbed;universal instantiation;wiki	Thomas Vanhove;Gregory van Seghbroeck;Tim Wauters;Filip De Turck;Brecht Vermeulen;Piet Demeester	2015	2015 IEEE 35th International Conference on Distributed Computing Systems Workshops	10.1109/ICDCSW.2015.19	simulation;operating system;database;distributed computing;world wide web;computer security	DB	-29.89339214425097	54.5755759001043	109293
459d2484b616243f40454586b4e186a79c2c9bec	umbrella: a portable environment creator for reproducible computing on clusters, clouds, and grids	virtualization;reproducible computing;execution environment;containers	Environment configuration is a significant challenge in large scale computing. An application that runs correctly on one carefully-prepared machine may fail completely on another machine, creating wasted effort and serious concerns about long-term reproducibility. Virtual machines and system containers provide a partial solution to this problem, in that they allow for the accurate reconstruction of an entire computing environment. However, when used directly, they have the dual problems of significant overhead and a lack of portability. To avoid this problem, we present Umbrella, a tool for specifying and materializing comprehensive execution environments from the hardware all the way up to software and data. A user simply invokes Umbrella with the desired task, and Umbrella determines the minimum mechanism necessary to run the task - direct execution, a system container, a local virtual machine, or submission to a cloud or grid environment. We present the overall design of Umbrella and demonstrate its use to precisely execute a high energy physics application across many platforms using combinations of chroot, Docker, Parrot, Condor, and Amazon EC2.	amazon elastic compute cloud (ec2);chroot;docker;overhead (computing);parrot virtual machine;software portability	Haiyan Meng;Douglas Thain	2015		10.1145/2755979.2755982	real-time computing;simulation;computer hardware;computer science	HPC	-23.07253273049537	55.36801558327235	109673
255ddb640c97ae4cc3c6bb0c6467a69f112dcaaa	distributed data mining on virtual clusters	economic forecasting;computational intelligence;information technology;resource management;null;satisfiability;data mining;drives;computer architecture;geoscience;data mining resource management councils geoscience computational intelligence economic forecasting identity management systems computer architecture drives information technology;councils;identity management systems;distributed data mining;knowledge discovery	For complex processes investigated in scientific fields such as medicine and earth sciences, knowledge discovery that exposes the underlying structure of the processes is crucial for detecting changes of state and constructing forecasting procedures. A model discovery approach has been recently developed, which uses computational intelligence techniques to deal with the heterogeneity, incompleteness and imprecision of the data describing complex proceeses. While the approach offers a tractable and effective means for model discovery, it is still computationally expensive, routinely requiring tens of thousands of hours of CPU time. To satisfy the needs of such applications, it is more costeffective to employ shared resources located in different departments of an organization, than to purchase large and expensive compute clusters. We present a method for aggregating resources under multiple administrative domains into a virtual resource that can satisfy efficiently the needs of data-mining based model discovery. The proposed resource aggregation and job management approach provides an end-to-end solution to distributed data mining across organization-wide resources.	analysis of algorithms;application lifecycle management;central processing unit;cobham's thesis;computational intelligence;computer cluster;data mining;disk staging;end-to-end encryption;end-to-end principle;loose coupling;scheduling (computing);sensor;software deployment;throughput	Gabriel Mateescu;Julio J. Valdés	2006	20th International Symposium on High-Performance Computing in an Advanced Collaborative Environment (HPCS'06)	10.1109/HPCS.2006.20	computer science;data science;resource management;operating system;computational intelligence;economic forecasting;data mining;database;distributed computing;information technology;satisfiability	HPC	-29.823494090939477	51.319259563353214	109772
ff24b8bdff9c8708151cfdd0aa71c3ad7a5767a0	beyond beowulf clusters	goddard space flight center;distributed computing;message passing interface;operating system;network of workstation;scientific computing;distributed parallel computing;beowulf cluster;parallel virtual machine;open source	In the early ’90s, the Berkeley NOW (Network of Workstations) Project under David Culler posited that groups of less capable machines (running SunOS) could be used to solve scientific and other computing problems at a fraction of the cost of larger computers. In 1994, Donald Becker and Thomas Sterling worked to drive the costs even lower by adopting the then-fledgling Linux operating system to build Beowulf clusters at NASA’s Goddard Space Flight Center. By tying desktop machines together with open source tools such as PVM (Parallel Virtual Machine), MPI (Message Passing Interface), and PBS (Portable Batch System), early clusters—which were often PC towers stacked on metal shelves with a nest of wires interconnecting them—fundamentally altered the balance of scientific computing. Before these first clusters appeared, distributed/parallel computing was prevalent at only a few computing centers, national laboratories, and a very few university departments. Since the introduction of clusters, distributed computing is now, literally, everywhere.	beowulf cluster;computational science;computer cluster;desktop computer;distributed computing;donald becker;linux;message passing interface;open-source software;operating system;parallel virtual machine;parallel computing;portable batch system;sunos;workstation	Philip M. Papadopoulos;Greg Bruno;Mason J. Katz	2007	ACM Queue	10.1145/1242489.1242501	parallel computing;computer cluster;computer science;message passing interface;operating system;software engineering;data-intensive computing;database;distributed computing	HPC	-27.362640827072113	52.13318372822547	109836
7198355532a925926673b8725723268fa4bfdc18	resource brokering using a multi-site resource allocation strategy for computational grids	globus toolkit;ganglia;computational grid;resource allocation;network information model;multi site resource allocation;resource broker;nws;grid computing	Grid computing employs heterogeneous resources which may be installed on different platforms, hardware/software, computer architectures, and perhaps using different computer languages to solve large-scale computational problems. As many more Grids are being developed worldwide, the number of multi-institutional collaborations is growing rapidly. However, to realize Grid computing’s full potential, it is expected that Grid participants must be able to share one another’s resources. This paper presents a resource broker that employs the multi-site resource allocation (MSRA) strategy and the dynamic domain-based network information model that we propose to allocate Grid resources to submitted jobs, where the Grid resources may be dispersed at different sites, and owned and governed by different organizations or institutes. The jobs and resources may also belong to different clusters/sites. Resource statuses collected by the Ganglia, and network bandwidths gathered by the Network Weather Service, are both considered in the proposed scheduling approach. A dynamic domain-based model for network information measurement is also proposed to choose the most appropriate resources that meet the jobs’ execution requirements. Experimental results show that MSRA outperformed the other tested strategies. Copyright 2010 John Wiley & Sons, Ltd.	central processing unit;computational problem;computer architecture;computer language;concurrency control;distributed computing;ganglia;grid computing;heterogeneous computing;information model;job scheduler;job stream;john d. wiley;list of toolkits;load balancing (computing);national supercomputer centre in sweden;queuing delay;requirement;resource description framework;run time (program lifecycle phase);scheduling (computing);way to go	Chao-Tung Yang;Fang-Yie Leu;Sung-Yi Chen	2011	Concurrency and Computation: Practice and Experience	10.1002/cpe.1667	parallel computing;semantic grid;resource allocation;computer science;operating system;database;distributed computing;resource allocation;world wide web;drmaa;grid computing	HPC	-30.132666233334383	52.11496700308471	109839
ca50d1a7dba0af4d7494df70fa617081b73f934b	an architectural style for liquid web services	inductors web services scalability multicore processing hardware runtime;liquid web services;restful web service interface;performance;patterns scalability performance web services rest architectural styles;rest;web service;web services cloud computing service oriented architecture shared memory systems;runtime;liquid service oriented architectures;shared memory architectures;restful actor;virtualized cloud computing environments;design constraints;architectural styles;shared memory systems;shared memory architectures architectural style liquid web services liquid service oriented architectures design constraints restful actor restful web service interface virtualized cloud computing environments multicore processors;multicore processing;web services;inductors;multicore processors;patterns;scalability;service oriented architecture;cloud computing;hardware;architectural style	Just as liquids adapt their shape to the one of their container, liquid architectures feature a high degree of adaptability so that they can provide scalability to applications as they are executed on a wide variety of heterogeneous deployment environments. In this paper we enumerate the properties to be guaranteed by so-called liquid service-oriented architectures and define a set of design constraints that make up a novel architectural style for liquid architectures. These constraints drive the careful construction of a pattern, the Restful Actor (Reactor), which enables to deliver the required scalability by means of replication of its constituent parts. Reactors feature a Restful Web service interface and a composable architecture which is capable of delivering scalability and high performance in a way that is independent from the chosen deployment infrastructure. We discuss how the Reactor can be deployed to run on distributed (shared-nothing) execution environments typical of virtualized Cloud computing environments as well as on modern multicore processors with shared memory architectures.	architectural pattern;central processing unit;cloud computing;composability;computer architecture;enumerated type;multi-core processor;programming model;reactor (software);reactor pattern;representational state transfer;run-time infrastructure (simulation);scalability;service-oriented architecture;service-oriented device architecture;service-oriented programming;shared memory;shared nothing architecture;software deployment;web service	Daniele Bonetta;Cesare Pautasso	2011	2011 Ninth Working IEEE/IFIP Conference on Software Architecture	10.1109/WICSA.2011.38	multi-core processor;web service;embedded system;computer architecture;real-time computing;computer science;operating system	HPC	-29.717606036515825	56.096051010891486	109935
2893c34c11ed52b9db30c74253de232240702f9d	a comprehensive investigation of distribution in the context of workflow management	workflow management;distributed processing;workflow management software hardware distributed computing computer network management computer science environmental management taxonomy computer architecture computer networks large scale systems;distributed processing workflow management software;distributed environment;workflow management software;workflow management system;distributed workflow management system distribution workflow management distributed environments	Distribution is ofren discussed under different issues of interest in the context of workfrow management. This paper contributes a general framework for this discussion, i.e. it presents a general framework for workflow management in distributed environments. Technical and organiza tional issues are analyzed that have an impact on the design of a distributed workflow management system. Within the core part of this paper we present a taxonomy enabling to compare implementation concepts of different workfrow management systems especially with respect to distribution.	distributed computing;taxonomy (general)	Stefan Jablonski;Ralf Schamburger;Christian Hahn;Stefan Horn;Rainer Lay;Jens Neeb;Michael Schlundt	2001		10.1109/ICPADS.2001.934818	workflow;xpdl;systems management;computer science;knowledge management;business process management;workflow management coalition;document management system;database;distributed computing;enterprise data management;windows workflow foundation;workflow management system;workflow engine;distributed computing environment;workflow technology	DB	-31.243662875668733	50.43490105360563	109936
d3eebb3e68f87cfb4e9a26bb2d6a58167af927ad	understanding scheduling implications for scientific applications in clouds	scientific application;virtualization;schedulers;scheduling;clouds;cloud computing	This paper explores some of the effects that the paradigm of Cloud Computing has on schedulers when executing scientific applications. We present premises regarding to provisioning and architectural aspects of a Cloud infrastructure, that are not present in other environments, and which implications they may have on scheduling decisions in presence of relevant policies like improving performance. We also argue that using virtualization as a mechanism for workload consolidation in a multi-core environment has important performance consequences for e-science. We propose and test a preliminary workload classification, based on usage modes, that may improve early scheduling decisions as we research towards automatic deployment of scientific applications.	cloud computing;e-science;multi-core processor;programming paradigm;provisioning;scheduling (computing);semiconductor consolidation;software deployment	Giacomo V. Mc Evoy;Bruno Schulze	2011		10.1145/2089002.2089005	parallel computing;real-time computing;computer science;distributed computing	HPC	-22.02481628016571	58.93827990235983	110050
37b25bb3467fb415db378170700697dd5d1ce8b7	innovative internet computing systems		We extend the Jinni Agent Programming Infrastructure with a multicast network transport layer. We show that the resulting protocol emulates client/server exchanges while providing high performance multicasting of remote calls to multiple sites. To ensure that our agent infrastructure runs efficiently independently of router-level multicast support, we also describe a blackboard based algorithm for locating a randomly roaming agent for message delivery. Jinni’s orthogonal design, separating blackboards from networking and multi-threading, turns out to be easy to adapt to support a generalization of Jinni’s Linda based coordination model. The resulting system is particularly well suited for building large scale, agent based, IP transparent, fault tolerant, tele-teaching and shared virtual reality applications.	agent-based model;algorithm;client–server model;emulator;fault tolerance;linda (coordination language);multicast;randomness;router (computing);server (computing);television;thread (computing);virtual reality	Jan van Leeuwen	2001		10.1007/3-540-48206-7	internet architecture board;the internet;end-user computing	Networks	-32.70803930677823	46.61230459008697	110222
c047456a3da63f9e98c162ba1e58d52e8d9e9d84	overview of tpc benchmark e: the next generation of oltp benchmarks	oltp;tpc e;benchmark;software engineering;system performance;database management;tpc c;operating system;next generation;information system;database performance	Set to replace the aging TPC-C, the TPC Benchmark E is the next generation OLTP benchmark, which more accurately models client database usage. TPC-E addresses the shortcomings of TPC-C. It has a much more complex workload, requires the use of RAID-protected storage, generates much less I/O, and is much cheaper and easier to set up, run, and audit. After a period of overlap, it is expected that TPC-E will become the de facto OLTP benchmark.		Trish Hogan	2009		10.1007/978-3-642-10424-4_7	real-time computing;computer science;operating system;database	Vision	-21.292070214009904	56.128666225967656	110225
b4bf13704e8a26c5a9925cbb7e581a78a6c436f3	survey of architectures of parallel database systems	virtual machine;multiprocessor systems;data exchange;parallel database system;spectrum;database management;structure function;parallel databases;community networks;parallel systems;parallel computer	The paper is devoted to the classification, design, and analysis of architectures of parallel database systems. A formalization of the notion “parallel database system” is suggested, which relies on a concept of a virtual machine. Based on this formalization, a new approach to the classification of architectures of parallel database systems is suggested. Requirements to parallel database systems are formulated, which serve as criteria for comparing various architectures. Various classes of architectures of parallel database systems are considered and compared.	algorithm;content delivery network;database machine;emulator;experiment;extension (semantics);multiprocessing;omega;online analytical processing;online transaction processing;parallel computing;parallel database;principle of abstraction;prototype;qualitative comparative analysis;refinement (computing);requirement;systems architecture;virtual machine	Leonid B. Sokolinsky	2004	Programming and Computer Software	10.1023/B:PACS.0000049511.71586.e0	data exchange;kolmogorov structure function;spectrum;database theory;database tuning;computer science;virtual machine;theoretical computer science;database;distributed computing;database schema;distributed database;database testing;database design	DB	-28.579780492109535	46.38883828202101	110269
daa3542722e8e54f4288f50c452e999662b0100d	euforia integrated visualization	databases;libraries;software;time scale;data visualization physics open source software plasma simulation plasma applications plasma devices medical simulation tokamaks extraterrestrial phenomena monitoring;physics modelling;sustainable energy source;tokamak devices data visualisation physics computing public domain software;tokamak;physics codes coupling;euforia;physics computing;euforia integrated visualization;public domain software;data visualisation;visualization;data visualization;physical modelling;xml;next generation;visualization euforia;fusion devices;tokamak devices;open source visualization tools;europe;energy source;iter;open source visualization tools euforia integrated visualization iter fusion devices sustainable energy source physics modelling physics codes coupling tokamak;data models;open source	ITER is the next generation of fusion devices and is intended to demonstrate the scientific and technical feasibility of fusion as a sustainable energy source for the future. To exploit the full potential of the device and to guarantee optimal operation for the device, a high degree of physics modelling and simulation is needed. The European project Euforia contributes to the development of a platform which enables the coupling of different physics codes. The aim of this platform is to simulate the different coupled phenomena which take place in a tokamak at different space and time scales. In particular, the visualization is an issue when the data are generated by codes developed at different places by different people. This paper presents a set of unified and open source visualization tools that have been integrated into this framework. Some of these tools are used for the post-processing purpose, others are directly integrated into the workflow to allow visualization and monitoring of the results during its execution.	code;next-generation network;open-source software;simulation;video post-processing	Matthieu Haefele;Leon Kos;Pierre Navaro;Eric Sonnendrücker	2010	2010 18th Euromicro Conference on Parallel, Distributed and Network-based Processing	10.1109/PDP.2010.76	data modeling;xml;simulation;visualization;computer science;theoretical computer science;database;tokamak;public domain software;data visualization	HPC	-30.020057314974387	50.43079609606152	110343
4e39d6b0712a657be7e231a54c7e704d24e1e2bb	trac: toward recency and consistency reporting in a database with distributed data sources	distributed data;computational grid;monitoring system;distributed computing environment;legacy system	"""Distributed computing environments, including workflows in computational grids, present challenges for monitoring, as the state of the system may be captured only in logs distributed throughout the system. One approach to monitoring such systems is to """"sniff"""" these distributed logs and to store their transformed content in a DBMS. This centralizes the state and exposes it for querying; unfortunately, it also creates uncertainty with respect to the recency and consistency of the data. Previous related work has focused on allowing queries to express currency and consistency constraints, which are then enforced by """"pulling"""" data from the distributed sources on demand, or by requiring synchronous updates of a centralized data store. In some instances this is impossible due to legacy system issues or inefficient as the system scales to large numbers of processors. Accordingly, we propose that instead of enforcing consistency and recency, such monitoring systems should report these properties along with query results, with the hope that this will allow the data to be appropriately interpreted. We present techniques for reporting consistency and recency for queries and evaluate them with respect to efficiency and precision. Finally, we describe our prototype implementation and present experimental results of our techniques."""	autonomous robot;central processing unit;centralized computing;consistency model;data store;distributed computing;legacy system;norm (social);overhead (computing);packet analyzer;postgresql;prototype;relevance;scheduling (computing);trac;eric	Jiansheng Huang;Jeffrey F. Naughton;Miron Livny	2006			real-time computing;computer science;consistency model;data mining;database;legacy system;distributed computing environment	DB	-26.432141913438645	49.67461039430164	110605
c989428a2fbfc56944dac532653dacb132c9402d	ncluster: using multiple active name nodes to achieve high availability for hdfs	availability;namenode hdfs availability;servers;synchronization;ip networks;availability synchronization servers hardware ip networks file systems;file systems;hardware	Hadoop HDFS is an open source project from Apache Software Foundation for scalable, distributed computing and data storage. HDFS has become a critical component in today's cloud computing environment and a wide range of applications built on top of it. However, the initial design of HDFS has introduced a single-point-of-failure, HDFS contains only one active name node, if this name node experiences software or hardware failures, the whole HDFS cluster is unusable until the recovery of name node is finished, this is the reason why people are reluctant to deploy HDFS for an application whose requirement is high availability. In this paper, we present a solution to enable the high availability for HDFS's name node through efficient metadata replication. Our solution has two major advantages than existing ones: we utilize multiple active name nodes, instead of one, to build a cluster to serve request of metadata simultaneously. We implements a pub/sub system to handle the metadata replication process across these active namonodes efficiently. Based on the solution we implement a prototype called NCluster and integrate it with HDFS. We also evaluate NCluster to exhibit its feasibility and effectiveness. The experimental results show that our solution performs well with low replication cost, good throughput and scalability.	apache hadoop;cloud computing;computer data storage;distributed computing;high availability;hot spare;load balancing (computing);open-source software;prototype;publish–subscribe pattern;scalability;single point of failure;throughput;uptime;usability	Zhanye Wang;Dongsheng Wang	2013	2013 IEEE 10th International Conference on High Performance Computing and Communications & 2013 IEEE International Conference on Embedded and Ubiquitous Computing	10.1109/HPCC.and.EUC.2013.329	synchronization;availability;parallel computing;real-time computing;computer science;operating system;distributed computing;world wide web;computer security;server;computer network	HPC	-24.974885423184716	54.57855416987003	110735
aaeba634eabab5cf6d1cadf192374a0949345bb0	fine-grained profiling for data-intensive workflows	workflow management software input output programs;file systems statistical analysis grid computing application software workflow management software databases statistical distributions cloud computing data engineering computer science;generators;profiling;fuses;processor scheduling;input output programs;workflow profiling;data processing;runtime;workflow management systems fine grained profiling data intensive workflows dynamic analysis approach paratrac i o characteristics i o subsystems data processes interactions montage workflow;file system;distributed databases;workflow management software;workflow;workflow management system;scalability;gallium nitride;dynamic analysis	Profiling is an effective dynamic analysis approach to investigate complex applications. ParaTrac is a user-level profiler using file system and process tracing techniques for data-intensive workflow applications. In two respects ParaTrac helps users refine the orchestration of workflows. First, the profiles of I/O characteristics enable users to quickly identify bottlenecks of underlying I/O subsystems. Second, ParaTrac can exploit fine-grained data-processes interactions in workflow execution to help users understand, characterize, and manage realistic data-intensive workflows. Experiments on thoroughly profiling Montage workflow demonstrate that ParaTrac is scalable to tracing events of thousands of processes and effective in guiding fine-grained workflow scheduling or workflow management systems improvements.	data-intensive computing;input/output;interaction;montagejs;profiling (computer programming);scalability;scheduling (computing);user space	Nan Dun;Kenjiro Taura;Akinori Yonezawa	2010	2010 10th IEEE/ACM International Conference on Cluster, Cloud and Grid Computing	10.1109/CCGRID.2010.29	fuse;workflow;real-time computing;scalability;data processing;computer science;database;dynamic program analysis;profiling;world wide web;workflow management system;workflow engine;workflow technology	HPC	-23.379482051815174	56.589744760137755	110913
735b05b95bab3d0cd4b306c121064dfacb02860c	developing a web caching architecture with configurable consistency: a proposal	time consistency;consistency model;web caching	In recent years, Web Caching has been considered one of the key areas to improve web usage efficiency. However, caching web objects proposes many considerations about the validity of the cache. Ideally, it would be valuable to have a consistent cache, where no invalid relationships among objects are held. Several alternatives have been offered to keep consistency in the web cache, each one being better in different situations and for diverse requirements. Usually, web cachers implement just one strategy for maintaining consistency, sometimes giving bad results if circumstances are not appropriate for such strategy. Given that, a web cacher where this policy can be adapted to different situations, will offer good results in an execution with changing conditions. A web caching architecture is proposed as a testbed for consistency models, allowing both timing andordering issues to be considered.	agile software development;cpu cache;cache (computing);cache coherence;consistency model;requirement;testbed;web 2.0;web application;web cache	Francisco J. Torres-Rojas;Esteban Meneses;Alexander Carballo	2005			cache coherence;real-time computing;computer science;consistency model;database;data consistency;world wide web;time consistency	Web+IR	-23.265830687040054	53.71375284261667	111018
c5d82e762eea27e6fb18e2e579e31e713f1dc674	benchmarking hadoop performance in the cloud - an in depth study of resource management and energy consumption	virtualization;resources consumption;hadoop;green consumption;cloud computing;docker container	Virtual technologies have proven their capabilities to ensure good performance in the context of high performance computing (HPC). During the last decade, the big data tools have been emerging, they have their own needs in performance and infrastructure. Having a wide breadth of experience in the HPC domain, the experts can evaluate the infrastructures used to run big data tools easily. The outcome of this paper is the evaluation of two technologies of virtualization in the context of big data tools. We compare the performance and the energy consumption of two technologies of virtualization (Docker containers and VMware) and benchmark the software Hadoop (JoshBaer, 2015) using these environments. Firstly, the aim is the reduction of the Hadoop deployment cost using the cloud. Secondly, we discuss and analyze the assumptions learned from the HPC experiments and their applicability in the big data context. Thirdly, the Hadoop community finds an in-depth study of the resource consumption depending on the deployment environment. We come to the point that the use of the Docker container gives better performance in most experiments. Besides, the energy consumption varies according to the executed workload.	apache hadoop;benchmark (computing);big data;deployment environment;docker;experiment;software deployment;supercomputer	Aymen Jlassi;Patrick Martineau	2016		10.5220/0005861701920201	real-time computing;virtualization;cloud computing;computer science;operating system;world wide web	HPC	-22.54296137570834	58.95202553408596	111146
31a091c9d36e40bda6e7b425aa71e7bd6c08e998	managing long-running queries	executive control;etl;data warehousing;business intelligence;cost estimation;quick response;data integration;large data;management policy	Business Intelligence query workloads that run against very large data warehouses contain queries whose execution times range, sometimes unpredictably, from seconds to hours. The presence of even a handful of long-running queries can significantly slow down a workload consisting of thousands of queries, creating havoc for queries that require a quick response. Long-running queries are a known problem in all commercial database products. However, we have not seen a thorough classification of long-running queries nor a systematic study of the most effective corrective actions.  We present here a systematic study of workload management policies, including many implemented by commercial database vendors. Our goal is to enable a system to: (1) recognize long-running queries and categorize them in terms of their impact on performance and (2) determine and take (automatically!) the most effective control actions to remedy the situation.  To this end, we identify common workload management scenarios involving long-running queries, and create a taxonomy of long-running queries. We carry out an extensive set of experiments to evaluate different management policies and the relative and absolute thresholds that they may use. We find that in some scenarios, the right combination of policies can reduce the runtime of a workload by a factor of two, but that in other scenarios, any action taken increases runtime. One surprising result was that relative thresholds for execution control can compensate for inaccurate cost estimates, so that Kill&Requeue actions perform as well as Suspend&Resume.	categorization;control flow;experiment;job shop scheduling;makespan;run time (program lifecycle phase);scheduling (computing);taxonomy (general)	Stefan Krompass;Harumi A. Kuno;Janet L. Wiener;Kevin Wilkinson;Umeshwar Dayal;Alfons Kemper	2009		10.1145/1516360.1516377	real-time computing;computer science;data integration;data warehouse;data mining;database;business intelligence;cost estimate	DB	-20.810594293468597	57.11689589792018	111215
09bac56e46edb0282e863d9da58f751225841cb5	toward a high availability cloud: techniques and challenges	virtual machine;multicore processing virtual machine monitors availability cloud computing hardware checkpointing generators;generators;availability;storage management;checkpointing;virtual machine monitors;multicore;multicore processing;commodity systems cloud computing multicore processing virtualization distributed storage systems overarching management framework;availability cloud computing multicore virtual machine;multiprocessing systems;virtualisation cloud computing multiprocessing systems storage management;virtualisation;cloud computing;hardware	Cloud computing in its many forms has become the key computing-infrastructure that supports business and more recently governmental computing across the globe. With its geographical spread and value proposition comes the need to provide guaranteed level of availability in the infrastructure and in its services. Multicore processing, virtualization, distributed storage systems and an overarching management framework that enable a Cloud, offer a plethora of possibilities to provide high availability using commodity systems. Herein lie the opportunities and challenges discussed in this paper.	cloud computing;clustered file system;computer data storage;high availability;intel core (microarchitecture);multi-core processor;virtual machine	Cuong Manh Pham;Phuong Cao;Zbigniew T. Kalbarczyk;Ravishankar K. Iyer	2012	IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN 2012)	10.1109/DSNW.2012.6264687	multi-core processor;parallel computing;real-time computing;computer science;operating system;distributed computing	HPC	-28.42862851901355	57.400152079970965	111420
632a0e35ab5ed4ff47310b01744091e1d279c6c0	a semantic matchmaker service on the grid	resource selection;globus toolkit;resource allocation;grid services;semantic web technology;grid service;semantic web;networking and distributed web applications	A fundamental task on the Grid is to decide what jobs to run on what computing resources based on job or application requirements. Our previous work on ontology-based matchmaking discusses a resource matchmaking mechanism using Semantic Web technologies. We extend our previous work to provide dynamic access to such matchmaking capability by building a persistent online matchmaking service. Our implementation uses the Globus Toolkit for the Grid service development, and exploits the monitoring and discovery service in the Grid infrastructure to dynamically discover and update resource information. We describe the architecture of our semantic matchmaker service in the poster.	consistency model;disk space;job stream;requirement;semantic web	Andreas Harth;Stefan Decker;Yu He;Hongsuda Tangmunarunkit;Carl Kesselman	2004		10.1145/1013367.1013458	semantic grid;resource allocation;computer science;knowledge management;semantic web;social semantic web;semantic web stack;database;world wide web;grid computing	HPC	-31.04367834031849	51.44455366139233	111741
f0d5040d86188567339e5eaad7e84adbe4dfba07	a distributed repository for immutable persistent objects	system modeling;search algorithm	Jasmine is an object-oriented system for programming-in-the-large. Jasmine describes software using <italic>system model objects</italic>. These objects are persistent (they have lifetimes of days or decades) and immutable (since system models act as historical records). This paper describes JStore, a distributed, replicated repository for system model objects. JStore provides <italic>robust, transactional, write-once</italic> storage. Designs are presented for the serialization, location, and replication of objects. <italic>Description procedures</italic> serialize objects for network transmission and permanent storage. An <italic>expanding ring multicast</italic> search algorithm locates saved objects. JStore replicates objects using a <italic>lazy replication</italic> algorithm built on top of the location mechanism. <italic>Decision procedures</italic> determine the replication policy used at each storage site.	data transfer object;immutable object;jasmine;lazy evaluation;mega man network transmission;persistent data structure;programming in the large and programming in the small;replication (computing);search algorithm;serialization	Douglas Wiebe	1986		10.1145/28697.28744	real-time computing;systems modeling;computer science;object-oriented design;database;distributed computing;search algorithm	PL	-27.62829670960903	46.53620317224939	112074
ad897b9261a39cdae6e8b0fdcd755e6001e004bc	wafl iron: repairing live enterprise file systems		Consistent and timely access to an arbitrarily damaged file system is an important requirement of enterpriseclass systems. Repairing file system inconsistencies is accomplished most simply when file system access is limited to the repair tool. Checking and repairing a file system while it is open for general access present unique challenges. In this paper, we explore these challenges, present our online repair tool for the NetApp® WAFL® file system, and show how it achieves the same results as offline repair even while client access is enabled. We present some implementation details and evaluate its performance. To the best of our knowledge, this publication is the first to describe a fully functional online repair tool.	netapp filer;online and offline	Ram Kesavan;Harendra Kumar;Sushrut Bhowmik	2018			real-time computing;database;computer science;file system	OS	-23.259999564841703	50.047544692531034	112259
b41d2d41c693895d739e1388ca5f6304f04ef08e	transpacific live migration with wide area distributed storage	virtual machine;electronic mail;virtual machining;computer architecture;servers;live migration distributed cluster storage virtual machine;nation wide live migration transpacific live migration wide area distributed storage remote data backup business processes disaster recovery distcloud project computer resource clustering shared single posix file system ldlm long distance live migration vms virtual machines technical improvement;servers virtual machining computer architecture distributed databases delays electronic mail educational institutions;distributed databases;distributed cluster storage;virtual machines back up procedures business continuity business data processing cloud computing pattern clustering storage management unix;delays;live migration	In recent years, much attention has been paid to wide area distributed storages to backup data remotely and ensure that business processes can continue in terms of disaster recovery. In the 'distcloud' project, authors have been involved in the research of wide area distributed storage by clustering many computer resources located in geographically distributed areas, where the number of sites is more than 2 (N > 2). The storage supports a shared single POSIX file system so that LDLM (Long Distance Live Migration) of VMs (Virtual Machines) works well between multiple sites. We introduce the concept and basic architecture of the wide area distributed storage and its technical improvement for LDLM. We describe the result of our experiment, that is, 1) Nation Wide Live Migration (about 500 Km) in Japan, and 2) Transpacific Live Migration (over 24,000 Km). We show the technical benefit of the current implementation and discuss suitable applications and remaining issues for further research topics.	backup;business process;cluster analysis;clustered file system;disaster recovery;posix	Ikuo Nakagawa;Kohei Ichikawa;Tohru Kondo;Yoshiaki Kitaguchi;Hiroki Kashiwazaki;Shinji Shimojo	2014	2014 IEEE 38th Annual Computer Software and Applications Conference	10.1109/COMPSAC.2014.71	computer science;virtual machine;operating system;software engineering;database;distributed computing;world wide web;distributed database;computer security;server	HPC	-26.462270887320518	51.57888719759693	112303
7f9a69454145a269a968b8310fb923f31d4a61c6	artificial immune system framework for pattern extraction in distributed environment	artificial immune system;distributed environment	Information systems today are dynamic, heterogeneous environments and places where a lot of critical data is stored and processed. Such an envrionment is usually build over many virtualization layers on top of backbone which is hardware and network. The key problem within this environment is to find, in realtime, valuable information among large sets of data. In this article a framework for a pattern extraction system based on artificial immune system is presented and discussed. As an example a system for anomalous pattern extraction for intrusion detection in a computer network is presented.	artificial immune system;heterogeneous computing;information system;internet backbone;intrusion detection system;pattern recognition;real-time computing	Rafal Pokrywka	2010			computer science;artificial immune system;distributed computing environment	DB	-26.470491633684578	54.68885390769211	112479
51f917c0ae4b33dce3c8e96361a39a643123b82a	implementing an exat-based distributed monitoring system prototype	resource monitoring;exat;intelligent agents;erlang;grid computing	Monitoring resource utilization in distributed systems remains of importance. This is especially the case in LAN-based distributed systems (and, in particular, in global Grid systems), where individual nodes can be (may need to be) added to and/or removed from the system at “random” moments. The aim of this paper is to report initial results of the project that aims at using Erlang-based software agents as a robust and flexible resource monitoring infrastructure. The implemented prototype is capable not only of collecting performance data, but can also detect certain network problems. Furthermore, an assessment of the eXAT agent platform, based on experiences gathered during prototype implementation, is included.	distributed computing;erlang (programming language);prototype;software agent	Gleb Peregud;Julian Zubek;Maria Ganzha;Marcin Paprzycki	2012	Comput. Sci. Inf. Syst.	10.2298/CSIS120108028P	erlang;embedded system;real-time computing;computer science;operating system;database;distributed computing;programming language;computer security;intelligent agent;grid computing	HPC	-33.18231435972285	49.492495484048746	112503
31dfe6e3b9d2c04cc152aeafb5dee8b38209ebfb	grid security for fault tolerant grid applications	fault tolerant			Nicole Hallama;André Luckow;Bettina Schnor	2006			grid;fault tolerance;distributed computing;computer science	HPC	-29.321023324122695	46.68220012775084	112558
92099f7fce74847156349df4daee87d057bdc710	virtual machines: versatile platforms for systems and processes (the morgan kaufmann series in computer architecture and design)	virtual machines;versatile platforms;computer architecture;morgan kaufmann series	Virtualization technology from VMware, IBM, Xen, Microsoft, and others is an indication of the importance of this technology, especially in this era where almost everything is in the cloud. Virtualization became the IT world's hottest trend in recent years. Have you ever wondered how you can run multiple operating systems on a single physical system and share the underlying hardware resources? How does your operating system see a hardware that differs from the actual hardware, and why we need to do so? What is a virtual server? Virtual machine is the answer to all that and much more. In this course we will study the design and implementation of virtual machines, virtual machine monitors (aka hypervisors), as well other recent trends in virtualization. We will study virtual machines across the disciplines that use them: operating systems, programming languages, and computer architecture.	cloud computing;computer architecture;hypervisor;operating system;programming language;server (computing);virtual machine;virtual private server	James E. Smith;Ravi Nair	2005			computer science;artificial intelligence;theoretical computer science	Arch	-28.402706302417215	56.17981188796096	112614
046f82ba1675c56ef48ca0fbf97b3b45a56c3853	highly available long running transactions and activities for j2ee applications	cadcam;high availability;computer crashes;availability;prototypes;application server;web service;companies;transaction databases;computer aided manufacturing;web services;councils;middleware;service level agreement;availability computer crashes middleware transaction databases councils prototypes computer aided manufacturing cadcam companies web services	Today’s business applications are typically built on top of middleware platforms such as J2EE and use transactions that have evolved into long running activities able to adapt to different circumstances. Specifications, such as the J2EE Activity Service, have arised for applications requiring that support. These applications also demand high availability to prevent financial losses and/or service level agreements (SLAs) violations due to service unavailability or crashes. Replication is a means to attain high availability but current middleware does not provide highly available transactions. In the advent of crashes, running transactions abort and the application is forced to re-execute them, what results in a loss of availability and transparency. Most approaches using J2EE consider the replication of either the application server or the database. This results in poor availability when the non-replicated tier crashes. This paper presents a novel J2EE replication support for both, application server and database layers providing highly available transactions and long running activities. Failure masking is transparent to client applications. A prototype has been implemented and evaluated.	application server;crash (computing);high availability;interaction;java platform, enterprise edition;middleware;multitier architecture;online analytical processing;overhead (computing);prototype;reliability engineering;server (computing);service-level agreement;single point of failure;unavailability	Francisco Perez-Sorrosal;Marta Patiño-Martínez;Ricardo Jiménez-Peris;Jaksa Vuckovic	2006	26th IEEE International Conference on Distributed Computing Systems (ICDCS'06)	10.1109/ICDCS.2006.47	web service;real-time computing;computer science;operating system;database;distributed computing;computer security;computer network;computer-aided manufacturing	Visualization	-32.53141613150749	48.67110136847617	112740
0b6adc0dbc55076dc9c9a8931f4a4df58fd291b6	differentiated storage services	o classifier;file and storage systems;storage system;hybrid storage system;differentiated storage service;storage system provision;o class;classifying data;quality-of-service qos;classification;policybased management;different class;solid-state storage;tiered storage;o request;small file;caching;operating system;out of band;service architecture;quality of service;semantic gap;low latency;proof of concept	We propose an I/O classification architecture to close the widening semantic gap between computer systems and storage systems. By classifying I/O, a computer system can request that different classes of data be handled with different storage system policies. Specifically, when a storage system is first initialized, we assign performance policies to predefined classes, such as the filesystem journal. Then, online, we include a classifier with each I/O command (e.g., SCSI), thereby allowing the storage system to enforce the associated policy for each I/O that it receives.  Our immediate application is caching. We present filesystem prototypes and a database proof-of-concept that classify all disk I/O --- with very little modification to the filesystem, database, and operating system. We associate caching policies with various classes (e.g., large files shall be evicted before metadata and small files), and we show that end-to-end file system performance can be improved by over a factor of two, relative to conventional caches like LRU. And caching is simply one of many possible applications. As part of our ongoing work, we are exploring other classes, policies and storage system mechanisms that can be used to improve end-to-end performance, reliability and security.	cache (computing);computer data storage;end-to-end encryption;end-to-end principle;input/output;operating system;scsi	Michael P. Mesnier;Feng Chen;Tian Luo;Jason B. Akers	2011		10.1145/2043556.2043563	embedded system;design;quality of service;performance;computer science;specfs;operating system;reliability;database;record-oriented filesystem;metadata;proof of concept;computer security;service quality;bandwidth;semantic gap;low latency	OS	-24.00057881329636	58.24909823271535	112749
c3acca87dca24df115676368c8458a021bfda4ce	introduction to special issue on cloud and service computing	swinburne	Over the years, the service-oriented paradigm of assembling software systems and applications using Internet-accessible autonomous and platform-independent software components in a loosely coupled manner has been well received by the public for its unprecedented flexibility and interoperability [2–4]. The research area of service delivery and provisioning has gradually matured into the so-called cloud computing by combining the achievements in both utility computing and grid computing. Cloud computing has the potential to reshape the landscape of the IT industry. It brings a brand-new model of IT provisioning and delivery by offering benefits such as zero upfront investment, on-demand services, flexible and elastic capacity, and rapid service deployment [1]. Popular cloud services such as Amazon EC2, Google Compute Engine, and Apple iCloud are now hosted on large-scale data centers and delivered to users through a variety of devices. However, realizing the full potential of cloud services raises a number of significant challenges, which have not been fully recognized or addressed in the community. One important issue is that large-scale data centers must offer reli-	amazon elastic compute cloud (ec2);autonomous robot;cloud computing;component-based software engineering;data center;google compute engine;grid computing;itil;interoperability;loose coupling;programming paradigm;provisioning;service-oriented modeling;services computing;software deployment;software system;utility computing;icloud	Jian Yu;Quan Z. Sheng;Yanbo Han	2013	Service Oriented Computing and Applications	10.1007/s11761-013-0132-8	computer science	HPC	-31.6639020896298	54.82373986911957	113008
8b7e801e0000c03ad408acbf7afabfb237a855df	building global and scalable systems with atomic multicast	scalability;message passing;consensus;multicore	The rise of worldwide Internet-scale services demands large distributed systems. Indeed, when handling several millions of users, it is common to operate thousands of servers spread across the globe. Here, replication plays a central role, as it contributes to improve the user experience by hiding failures and by providing acceptable latency. In this paper, we claim that atomic multicast, with strong and well-defined properties, is the appropriate abstraction to efficiently design and implement globally scalable distributed systems. We substantiate our claim with the design of two modern online services atop atomic multicast, a strongly consistent key-value store and a distributed log. In addition to presenting the design of these services, we experimentally assess their performance in a geographically distributed deployment.	multicast;scalability	Samuel Benz;Parisa Jalili Marandi;Fernando Pedone;Benoît Garbinato	2014		10.1145/2663165.2663323	multi-core processor;parallel computing;message passing;real-time computing;scalability;consensus;computer science;operating system;distributed computing;source-specific multicast;computer network	HPC	-24.508425756045202	54.053300951775284	113009
4fb73c162092dab4491d9e5c62eadf01ef34b9c7	filewall: a firewall for network file systems		Access control in network file systems relies on primitive mechanisms like access control lists and permission bits, which are not enough when operating in a hostile network environment. Network middleboxes, e.g., firewalls, completely ignore file system semantics when defining policies. Therefore, implementing simple context-aware access policies requires modifications to file servers and/or clients, which is impractical. We present FileWall, a network middlebox that allows administrators to define context-aware access policies for file systems using both the network context and the file system context. FileWall interposes on the client-server network path and implements administrator defined policies through message transformation without modifying either clients or servers. In this paper, we present the design and implementation of FileWall for the NFS protocol. Our evaluation demonstrates that FileWall imposes minimal overheads for common file system operations, even under heavy loads.	access control list;client–server model;clustered file system;firewall (computing);middlebox;server (computing)	Stephen Smaldone;Aniruddha Bohra;Liviu Iftode	2007	Third IEEE International Symposium on Dependable, Autonomic and Secure Computing (DASC 2007)	10.1109/DASC.2007.27	shared resource;self-certifying file system;device file;resolv.conf;network file system;computer science;stub file;versioning file system;operating system;unix file types;ssh file transfer protocol;journaling file system;database;open;everything is a file;file system fragmentation;global namespace;ubiquitous computing;computer network;virtual file system	OS	-22.330788751676035	51.54345330604368	113097
79cc67a4bffb204302e9f282213e97feea090824	beyond the cloud: cyberphysical systems	graph theory;information technology;cyber physical systems;cyber physical systems cloud computing information technology graph theory cyberphysical systems;graph theory cyberphysical system fellow cloud competitor cloud cloud computing;cyberphysical systems;graph theory cloud computing;cloud computing	C louds offer a novel way to revitalize the seemingly archaic concept of time-sharing. Rather than relying on fixed mainframes, clouds thrive in the Internet. Clouds offer impressive efficiencies, unprecedented collaboration opportunities, and economies of scale for all manner of networked users. Yet cloud server farms have enormously costly power consumption footprints and require massive data pipelines to transport transactions and data to and from the servers. Such connectivity means that clouds rarely stand alone. They currently rely on other equally relevant leading-edge technologies— the Internet of Things (IOT), mobile apps, and big data. Relying on the Internet, clouds both influence how the Internet is used and are influenced by the Internet’s use. When connected to banks of remote autonomous sensors, clouds become cyberphysical clouds—that is, specific cloud-based instances of cyberphysical systems (CPSs). Thus, such clouds are citizens of the IOT.1 Clouds can also offer personal value through mobile apps—for example, reinforcing the idea of Bring Your Own Device. Such apps will continue growing in number and perceived use so long as they can rely on clouds to share data across devices and connect with users. For example, we prepared this article using Dropbox, calendar, and reminder apps working across three different devices. Interestingly, the majority of the cloud-borne transactions for transferring the data to multiple devices were automatic after initial data entry. Furthermore, clouds harbor big data, be it in small quasirelated datasets or massively large data collections. Other clouds, often relying on Apache’s Hadoop open source framework for distributed data-intensive applications, broker big-data transactions and expedite novel views drawing from seemingly disparate datasets. Although few clouds freely interact with fellow or competitor clouds today, this is likely to change,2 leading to larger networks of collaborative, autonomously operating clouds, each optimized for specific services. As clouds continue to expand and eventually aggregate, a framework must emerge for understanding the resulting dynamic, nonlinear space embodied in various cloudrelated networks.	aggregate data;apache hadoop;autonomous robot;big data;bring your own device;cloud computing;data-intensive computing;dropbox;internet of things;mainframe computer;mobile app;nonlinear system;open-source software;pipeline (computing);sensor;server (computing);server farm;tag cloud;time-sharing;virtual private server	George F. Hurlburt;Jeffrey M. Voas	2013	IT Professional	10.1109/MITP.2013.24	simulation;cloud computing;computer science;graph theory;theoretical computer science;operating system;distributed computing;cyber-physical system;law;information technology	Mobile	-24.773894537487948	54.75705552971605	113170
9971395ed3dc578cf8b4910b332c2f14b0b5043f	cloudtracker: using execution provenance to optimize the cost of cloud use		In this work, we investigate tools that enable dollar cost optimization of scientific simulations using commercial clouds. We present a framework, called CLOUDTRACKER, that transparently records information from a simulation that is executed in a commercial cloud so that it may be “replayed” exactly to reproduce its results. Using the automated CLOUDTRACKER provenance and replay facilities, scientists can choose either to store the results of a simulation or to reproduce it on-demand – whichever is more cost efficient in terms of the dollar cost charged for storage and computing by the commercial cloud provider. We present a prototype implementation of CLOUDTRACKER for the Amazon AWS commercial cloud and the StochSS stochastic simulation system. Using this prototype, we analyze the storage-versus-compute cost tradeoffs for different classes of StochSS simulations when deployed and executed in AWS.	amazon web services;cloud computing;cost efficiency;mathematical optimization;prototype;simulation	Geoffrey Douglas;Brian Drawert;Chandra Krintz;Richard Wolski	2014		10.1007/978-3-319-14609-6_7	real-time computing;operating system;database	HPC	-25.073646588722852	58.87658380732472	113183
b8969c22db8aeb71882fa19bdc4eb33937412ba5	jetstream: early operations performance, adoption, and impacts		Jetstream, built with OpenStack, is the first production cloud funded by the NSF for conducting general-purpose science and engineering research as well as an easy-to-use platform for education activities. Unlike many high-performance computing systems, Jetstream uses the interactive Atmosphere graphical user interface developed as part of the iPlant (now CyVerse) project and focuses on interactive use on uniprocessor or multiprocessor. This interface provides for a lower barrier of entry for use by educators, students, practicing scientists, and engineers. A key part of Jetstream's mission is to extend the reach of the NSF's eXtreme Digital (XD) program to a community of users who have not previously utilized NSF XD program resources, including those communities and institutions that traditionally lack significant cyberinfrastructure resources. OpenStack deployments all have the same five basic services: identity, images, block storage, networking, and compute. There are additional services offered; however, by and large, they are underutilized. The use of these services will be discussed as well as highlights from the first year of production operations, and future plans for the project.	block (data storage);browser speed test;cyberinfrastructure;general-purpose markup language;graphical user interface;ibm notes;image;multiprocessing;nx bit;supercomputer;uniprocessor system	David Hancock	2017		10.1145/3147213.3155104	architecture;computer engineering;cloud computing;outreach;cyberinfrastructure;engineering research;multiprocessing;uniprocessor system;computer science;graphical user interface	OS	-27.458721095943172	52.87065245528681	113467
202645bf69a5a6b2947bbcac86986d8a9762cf80	pivot tracing: dynamic causal monitoring for distributed systems	control theory;dynamic systems;adaptive software	Monitoring and troubleshooting distributed systems is notoriously difficult; potential problems are complex, varied, and unpredictable. The monitoring and diagnosis tools commonly used today -- logs, counters, and metrics -- have two important limitations: what gets recorded is defined a priori, and the information is recorded in a component- or machine-centric way, making it extremely hard to correlate events that cross these boundaries. This paper presents Pivot Tracing, a monitoring framework for distributed systems that addresses both limitations by combining dynamic instrumentation with a novel relational operator: the happened-before join. Pivot Tracing gives users, at runtime, the ability to define arbitrary metrics at one point of the system, while being able to select, filter, and group by events meaningful at other parts of the system, even when crossing component or machine boundaries. We have implemented a prototype of Pivot Tracing for Java-based systems and evaluate it on a heterogeneous Hadoop cluster comprising HDFS, HBase, MapReduce, and YARN. We show that Pivot Tracing can effectively identify a diverse range of root causes such as software bugs, misconfiguration, and limping hardware. We show that Pivot Tracing is dynamic, extensible, and enables cross-tier analysis between inter-operating applications, with low execution overhead.	apache hbase;apache hadoop;causal filter;deductive lambda calculus;distributed computing;happened-before;java;mapreduce;multitier architecture;overhead (computing);prototype;relational operator;run time (program lifecycle phase);sql;software bug	Jonathan Mace;Ryan Roelke;Rodrigo Fonseca	2015	;login:	10.1145/2815400.2815415	embedded system;real-time computing;computer science;operating system;dynamical system;distributed computing	OS	-23.948817694381393	55.553664062791874	113705
7a6c67c34f90b486d66cc99be906b29a118533b6	active proxy-g: optimizing the query execution process in the grid	real applications gain;new incoming query;collaborative setting;data repository;query result;query execution process;raw datasets;process data;collaborative work;active proxy-g service;grid environment;grid computing;middleware;technical report;vliw;application server;computer science;data analysis;data engineering;blade server;cluster;biomedical informatics	The Grid environment facilitates collaborative work and allows many users to query and process data over geographically dispersed data repositories. Over the past several years, there has been a growing interest in developing applications that interactively analyze datasets, potentially in a collaborative setting. We describe the Active Proxy-G service that is able to cache query results, use those results for answering new incoming queries, generate subqueries for the parts of a query that cannot be produced from the cache, and submit the subqueries for final processing at application servers that store the raw datasets. We present an experimental evaluation to illustrate the effects of various design tradeoffs. We also show the benefits that two real applications gain from using the middleware.	application server;interactivity;middleware;optimizing compiler;sql	Henrique Andrade;Tahsin M. Kurç;Alan Sussman;Joel H. Saltz	2002	ACM/IEEE SC 2002 Conference (SC'02)		health informatics;query optimization;parallel computing;information engineering;computer science;very long instruction word;technical report;operating system;middleware;data mining;database;data analysis;programming language;blade server;world wide web;application server;grid computing;computer network;cluster	HPC	-21.003515528590615	53.07028807882807	113739
ed478a1686152f3243d19f4de518ff5b50a9ad97	decentralized architecture for fault tolerant multi agent system	agent platform;management system;multi agent system;fault tolerant;application software;information technology;distributed processing;software fault tolerance;virtual agent cluster paradigm decentralized architecture fault tolerant multiagent system agent management system distributed architecture;software agents;software architecture;multi agent systems;fault tolerant systems;software architecture multi agent systems distributed processing software fault tolerance;execution environment;fault tolerance;load management;artificial intelligence;load balance;scalability;point of view;quality of service;fault recovery;supply chain management;fault tolerant systems fault tolerance scalability load management application software supply chain management quality of service information technology artificial intelligence software agents;virtual agent;distributed architecture	Multi agent systems (MAS) are expected to be involved in futuristic technologies. Agents require some execution environment in which they can publish their service interfaces and can provide services to other agent. Such execution environment is called agent platform (AP). From a technical point of view any abnormal behavior of platform can distress agents residing on that platform. That's why it is necessary to provide a suitable architecture for the AP which should not only provide fault tolerance but also scalability features. There also exist some management components within the platform, which provide services to application agents. All the agents within MAS are managed by agent management system (AMS) which is the mandatory supervisory authority of any AP. To be more scalable, a single agent platform can be distributed over several machines which not only provides load balancing but also fault tolerance depending upon the distributed architecture of the AP. In existing systems, AMS is centralized i.e. it exists on one machine. With centralized AMS, this infrastructure lacks fault tolerance, which is a key feature of high assurance. Absence of fault tolerance is the main reason for the small number of deployments of MAS. Failure of AMS leads towards abnormal behavior in the distributed platform. This paper proposes virtual agent cluster (VAC) paradigm which strongly supports decentralized AMS to achieve fault tolerance in distributed AP. VAC provides fault tolerance by using separate communication layers among different machines. Experiments show that it improves performance, brings autonomy and supports fault recovery along with load balancing in distributed AP.	autonomy;centralized computing;distress (novel);distributed computing;experiment;fault tolerance;load balancing (computing);multi-agent system;programming paradigm;scalability;vhdl-ams;vacuum cleaner;verilog-ams	Zaheer Abbas Khan;Salman Shahid;Hafiz Farooq Ahmad;Arshad Ali;Hiroki Suguri	2005	Proceedings Autonomous Decentralized Systems, 2005. ISADS 2005.	10.1109/ISADS.2005.1452043	embedded system;fault tolerance;real-time computing;computer science;artificial intelligence;operating system;multi-agent system;distributed computing	Robotics	-31.620047026242478	48.04549560036253	114079
9d2b089a9d764f40dcd11f975f09f32770e022f6	performance analysis of cloud applications		Many popular cloud applications are large-scale distributed systems with each request involving tens to thousands of RPCs and large code bases. Because of their scale, performance optimizations without actionable supporting data are likely to be ineffective: they will add complexity to an already complex system often without chance of a benefit. This paper describes the challenges in collecting actionable data for Gmail, a service with more than 1 billion active accounts. Using production data from Gmail we show that both the load and the nature of the load changes continuously. This makes Gmail performance difficult to model with a synthetic test and difficult to analyze in production. We describe two techniques for collecting actionable data from a production system. First, coordinated bursty tracing allows us to capture bursts of events across all layers of our stack simultaneously. Second, vertical context injection enables us combine high-level events with low-level events in a holistic trace without requiring us to explicitly propagate this information across our software stack.	cloud computing;complex system;computer;debugging;distributed computing;email;experiment;gmail;high- and low-level;high-level programming language;holism;long tail;production system (computer science);profiling (computer programming);remote procedure call;software as a service;synthetic intelligence;tracing (software)	Dan Ardelean;Amer Diwan;Chandra Erdman	2018			cloud computing;computer science;distributed computing	HPC	-23.79080183650371	56.742377499951736	114081
5649aaaaf0adfc72eb4b5988c46c8089228c0551	a hadoop use case for engineering data	commodity nodes;visualization;hpc;hive;hbase;hadoop	This paper presents the VELaSSCo project (Visualization for Extremely LArge-Scale Scientific Computing). It aims to develop a platform to manipulate scientific data used by FEM (Finite Element Method) and DEM (Discrete Element Method) simulations. The project focuses on the development of a distributed, heterogeneous and high-performance platform, enabling the scientific communities to store, process and visualize huge amounts of data. The platform is compatible with current hardware capabilities, as well as future hardware.	apache hadoop;discrete element method;finite element method;simulation	Benoit Lange;Toàn Nguyên	2015		10.1007/978-3-319-24132-6_16	supercomputer;parallel computing;visualization;computer science;operating system;database;world wide web	HPC	-28.450900265596083	52.73431356340882	114267
3303d1cbea2d9d66e45bf65620bac413adeb67f4	a consistent backup mechanism for disaster recovery that using container based virtualization	disaster recovery;containers production systems operating systems servers writing;virtualization;virtualization backup disaster recovery;virtualisation back up procedures business continuity knowledge based systems;back up procedures;servers;business continuity;writing;production systems;nonvolatile local buffer consistent backup mechanism disaster recovery container based virtualization technology system backup consistent checkpoint concept memory image disk image incremental backup method two step aggressive backup process production system data backup disk backup method memory checkpoint function;backup;knowledge based systems;containers;virtualisation;operating systems	Today's businesses have relied much on system backup and disaster recovery, of which there are more and more products that based on various virtualization platforms, as the virtualization technologies kept developing. Container based virtualization, which is a kind of high efficiency virtualization technology, has great potential to support building more flexible backup and disaster recovery system on its platform. In this paper, we introduce a consistent backup mechanism for disaster recovery that using container based virtualization. First, we proposed the concept of consistent checkpoint which contains both memory and disk image at the backup point in time. Then, for backing up disk image, we use incremental backup method and a two-step aggressive backup process, in which production system and data backup can run at the same time, to deal with it. At last, we combine our disk backup method and the memory checkpoint function of virtualization platform together to accomplish the whole consistent checkpoint's backup. A prototype of our system, in which a non-volatile local buffer is introduced for both speed and reliability purpose, is implemented. And the experimental testing result shows that our system's running overhead to production system is sensible, especially the overhead can drop to only 0.5% when backup is done. Also by using our system, backup frequency of production system has the potential to reach over fifteen times per minute. All these can prove that our consistent backup mechanism is suitable for disaster recovery purpose in container based virtualization platform.	application checkpointing;disaster recovery;disk image;incremental backup;lazy evaluation;non-volatile memory;overhead (computing);production system (computer science);prototype;remote backup service;snapshot (computer storage);systems architecture;transaction processing system;x86 virtualization	Yida Xu;Hongliang Yu;Weimin Zheng	2012	2012 Seventh ChinaGrid Annual Conference	10.1109/ChinaGrid.2012.10	backup software;continuous data protection;embedded system;full virtualization;real-time computing;virtualization;data loss;incremental backup;engineering;ndmp;computer security	OS	-23.95094644098614	51.02627544100262	114682
0c8d9e21a2e7ef0a9ec2b4c88899d15cdc083f80	abcgrid: application for bioinformatics computing grid	computational grid;heterogeneous computing;source code;high performance	UNLABELLED We have developed a package named Application for Bioinformatics Computing Grid (ABCGrid). ABCGrid was designed for biology laboratories to use heterogeneous computing resources and access bioinformatics applications from one master node. ABCGrid is very easy to install and maintain at the premise of robustness and high performance. We implement a mechanism to install and update all applications and databases in worker nodes automatically to reduce the workload of manual maintenance. We use a backup task method and self-adaptive job dispatch approach to improve performance. Currently, ABCGrid integrates NCBI_BLAST, Hmmpfam and CE, running on a number of computing platforms including UNIX/Linux, Windows and Mac OS X.   AVAILABILITY The source code, executables and documents can be downloaded from http://abcgrid.cbi.pku.edu.cn	backup;bioinformatics;computation (action);dynamic dispatch;executable;genetic heterogeneity;grid computing;heterogeneous computing;laboratory;linux;microsoft windows;name;node (computer science);node - plant part;operating system;published database;source code;unix	Ying Sun;Shuqi Zhao;Huashan Yu;Ge Gao;Jingchu Luo	2007	Bioinformatics	10.1093/bioinformatics/btm086	computer science;bioinformatics;operating system;database;distributed computing;symmetric multiprocessor system;grid computing;source code	HPC	-28.564093060855484	51.07407519488336	114759
e7b505c7a37c4d4fcea381d924a1e97a4de1ba96	preemptive cloud resource allocation modeling of processing jobs	big data processing;markov chain;cloud computing;queuing theory	Cloud computing allows execution and deployment of different types of applications such as interactive databases or web-based services which require distinctive types of resources. These applications lease cloud resources for a considerably long period and usually occupy various resources to maintain a high quality of service (QoS) factor. On the other hand, general big data batch processing workloads are less QoS-sensitive and require massively parallel cloud resources for short period. Despite the elasticity feature of cloud computing, fine-scale characteristics of cloud-based applications may cause temporal low resource utilization in the cloud computing systems, while process-intensive highly utilized workload suffers from performance issues. Therefore, ability of utilization efficient scheduling of heterogeneous workload is one challenging issue for cloud owners. In this paper, addressing the heterogeneity issue impact on low utilization of cloud computing system, conjunct resource allocation scheme of cloud applications and processing jobs is presented to enhance the cloud utilization. The main idea behind this paper is to apply processing jobs and cloud applications jointly in a preemptive way. However, utilization efficient resource allocation requires exact modeling of workloads. So, first, a novel methodology to model the processing jobs and other cloud applications is proposed. Such jobs are modeled as a collection of parallel and sequential tasks in a Markovian process. This enables us to analyze and calculate the efficient resources required to serve the tasks. The next step makes use of the proposed model to develop a preemptive scheduling algorithm for the processing jobs in order to improve resource utilization and its associated costs in the cloud computing system. Accordingly, a preemption-based resource allocation architecture is proposed to effectively and efficiently utilize the idle reserved resources for the processing jobs in the cloud paradigms. Then, performance metrics such as service time for the processing jobs are investigated. The accuracy of the proposed analytical model and scheduling analysis is verified through simulations and experimental results. The simulation and experimental results also shed light on the achievable QoS level for the preemptively allocated processing jobs.	"""algorithm;batch processing;big data;binary prefix;blocking (computing);cloud computing;computer simulation;data center;database;display resolution;elasticity (cloud computing);erlang (unit);job stream;p (complexity);pgf/tikz (""""pgf and tikz"""");pr (complexity);preemption (computing);quality of service;scheduling (computing);semiconductor industry;software deployment;stream processing;testbed;web application"""	Shahin Vakilinia;Mohamed Cheriet	2017	The Journal of Supercomputing	10.1007/s11227-017-2226-0	massively parallel;quality of service;architecture;distributed computing;computer science;workload;cloud computing;batch processing;scheduling (computing);resource allocation	HPC	-21.47454895350801	59.06259742722027	114892
d013d420b13dfafc266151f6b705088a86fdf0d0	scale-out vs. scale-up techniques for cloud performance and productivity	elastic resources;measurement;cloud performance modeling;cloud productivity;scalability;productivity;quality of service;benchmark testing;cloud computing;throughput	An elastic cloud provisions machine instances upon user demand. Auto-scaling, scale-out, scale-up, or any mixture techniques are used to reconfigure the user cluster as workload changes. We evaluate three scaling strategies to upgrade the performance, efficiency and productivity of elastic clouds like EC2, Rack space, etc. We developed new performance models and run the Hi Bench benchmark to test Hadoop performance on various EC2 configurations. The strengths and shortcomings of three scaling strategies are revealed in our Hi Bench experiments: (1). Scale-out overhead is shown lower than that experienced in scale-up or mixed scaling clouds. Scale-out to a larger cluster of small nodes demonstrated high scalability. (2). Scaling up and mixed scaling have high performance in using smaller clusters with a few powerful machine instances. (3). With a mixed scaling mode, the cloud productivity is shown upgradable with higher flexibility in applications with performance/cost tradeoffs.	amazon elastic compute cloud (ec2);apache hadoop;autoscaling;benchmark (computing);experiment;overhead (computing);scalability;test bench	Kai Hwang;Yue Shi;Xiaoying Bai	2014	2014 IEEE 6th International Conference on Cloud Computing Technology and Science	10.1109/CloudCom.2014.66	benchmark;throughput;productivity;parallel computing;real-time computing;scalability;simulation;quality of service;cloud computing;computer science;operating system;measurement	HPC	-22.08407997676293	60.13388905028268	114941
ae7f39ffbd45a1c58e4f148070baa8ee717865ce	high-throughput state-machine replication using software transactional memory	state-machine replication;software transactional memory;ordered strong strict two-phase locking;multiversion concurrency control;one-copy serializability	State-machine replication is a common way of constructing general purpose fault tolerance systems. To ensure replica consistency, requests must be executed sequentially according to some total order at all non-faulty replicas. Unfortunately, this could severely limit the system throughput. This issue has been partially addressed by identifying non-conflicting requests based on application semantics and executing these requests concurrently. However, identifying and tracking non-conflicting requests require intimate knowledge of application design and implementation, and a custom fault tolerance solution developed for one application cannot be easily adopted by other applications. Software transactional memory offers a new way of constructing concurrent programs. In this article, we present the mechanisms needed to retrofit existing concurrency control algorithms designed for software transactional memory for state-machine replication. The main benefit for using software transactional memory in state-machine replication is that general purpose concurrency control mechanisms can be designed without deep knowledge of application semantics. As such, new fault tolerance systems based on state-machine replications with excellent throughput can be easily designed and maintained. In this article, we introduce three different concurrency control mechanisms for state-machine replication using software transactional memory, namely, ordered strong strict two-phase locking, conventional timestamp-based multiversion concurrency control, and speculative timestamp-based multiversion concurrency control. Our experiments show that speculative timestamp-based multiversion concurrency control mechanism has the best performance in all types of workload, the conventional timestamp-based multiversion concurrency control offers the worst performance due to high abort rate in the presence of even moderate contention between transactions. The ordered strong strict two-phase locking mechanism offers the simplest solution with excellent performance in low contention workload, and fairly good performance in high contention workload.	algorithm;concurrency (computer science);conflict (psychology);control system;correctness (computer science);dna replication;entity name part qualifier - adopted;experiment;fault tolerance;finite-state machine;libraries;library (computing);lock (computer science);multiversion concurrency control;operational semantics;order (action);serialization;software transactional memory;speculative execution;state machine replication;throughput;two-phase locking;executing - querystatuscode	Wenbing Zhao;William Yang;Honglei Zhang;Jack Yang;Xiong Luo;Yueqin Zhu;Mary Yang;Chaomin Luo	2016	The Journal of Supercomputing	10.1007/s11227-016-1747-2	timestamp-based concurrency control;optimistic concurrency control;transactional memory;parallel computing;real-time computing;isolation;computer science;operating system;software transactional memory;distributed computing;multiversion concurrency control;non-lock concurrency control;serializability	OS	-21.505902969990544	48.193543447741085	114945
224078cf5b82b29ff161c2422a9746cfafa81a1b	evaluating the average reproducibility cost of the scientific workflows		Applying scientific workflow to perform in-silico experiment is a more and more prevalent solution among the scientist's communities. Because of the data and compute intensive behavior of the scientific workflows parallel and distributed system (grids, clusters, clouds and supercomputers) are required to execute them. After all the complexity of these infrastructures and the continuously changing environment significantly encumber or even prevent the repeatability or the reproducibility which is often needed for results sharing or for judging scientific claims. The necessary data and parameters of the re-execution can be originated from different sources (infrastructural, third party, or related to the binaries), which may change or become unavailable during the years. Our ultimate goal is to compensate the lack of the original parameters by replacing, evaluating or simulating the value of the parameters in dispute. In order to create these methods we determined the levels of the re-execution and we defined a descriptor-space to collect all the parameters needed to the reproducibility. However these procedures take some extra cost the average reproducibility cost can be computed or even evaluated. In this paper we give a method to evaluate the average cost of making a workflow reproducible if the exact computation is not possible.	binary file;cloud computing;computation;computer cluster;distributed computing;grid computing;loss function;repeatability;simulation;software portability;supercomputer	Anna Bánáti;Péter Kárász;Péter Kacsuk;Miklós Kozlovszky	2016	2016 IEEE 14th International Symposium on Intelligent Systems and Informatics (SISY)	10.1109/SISY.2016.7601475	computer science;data science;data mining;management science	HPC	-23.03580678343848	55.035214758450614	115079
6f54a7933235ced5684e3bff18f7e5dc40510018	the missing piece in complex analytics: low latency, scalable model management and serving with velox		To enable complex data-intensive applications such as personalized recommendations, targeted advertising, and intelligent services, the data management community has focused heavily on the design of systems to train complex models on large datasets. Unfortunately, the design of these systems largely ignores a critical component of the overall analytics process: the serving and management of models at scale. In this work, we present Velox, a new component of the Berkeley Data Analytics Stack. Velox is a data management system for facilitating the next steps in real-world, large-scale analytics pipelines: online model management, maintenance, and serving. Velox provides end-user applications and services with a low-latency, intuitive interface to models, transforming the raw statistical models currently trained using existing offline large-scale compute frameworks into full-blown, end-to-end data products capable of targeting advertisements, recommending products, and personalizing web content. To provide up-to-date results for these complex models, Velox also facilitates lightweight online model maintenance and selection (i.e., dynamic weighting). In this paper, we describe the challenges and architectural considerations required to achieve this functionality, including the abilities to span online and offline systems, to adaptively adjust model materialization strategies, and to exploit inherent statistical properties such as model error tolerance, all while operating at “Big Data” scale.	big data;data hub;data-intensive computing;end-to-end encryption;error-tolerant design;interrupt latency;online and offline;personalization;pipeline (computing);statistical model;web content	Daniel Crankshaw;Peter Bailis;Joseph Gonzalez;Haoyuan Li;Zhao Zhang;Michael J. Franklin;Ali Ghodsi;Michael I. Jordan	2015	CoRR		analytics;computer science;data science;data mining;database;world wide web	OS	-19.666918836282388	55.47052505103922	115199
518590121fdc58708ce39c25a7193d0d31eb6728	a model of concurrent database transactions	databases;history;transaction databases history polynomials sufficient conditions airplanes merging encoding writing;sufficient conditions;polynomials;skeleton;transaction databases;airplanes;games;transforms;polynomial time;merging;schedules;writing;transaction processing;encoding;solids;reading and writing;timing	"""When several transactions (processes) read and write items in a database, the question of consistency of the database arises. Consistency is maintained if transactions are serial: all actions of a transaction execute completely before the actions of the next transaction begin. A particular history of interleaved actions belonging to several transactions is correct if it is equivalent to a serial history. We provide a natural framework for studying serializability that encompasses models that have been considered in the literature. A history is represented by a dag in which there is a vertex for each instantaneous value taken by an item in the database. The main tool for determining serializability are """"exclusion rules"""" which determine implied orderings between vertices. For example, a vertex that uses a value must be serialized before the value is overwritten. An exclusion closure -- the result of applying the rules as long as possible -- can be constructed in time cubic in the number of vertices. Since determining serializability is NP-complete, exclusion closures cannot always decide serializability, but useful sufficient conditions can be proven. Exclusion closures allow the largest known classes of polynomially serializable histories to be properly extended. When studying serializability it is not necessary to work solely with the dag containing instantaneous values of all items. Three abstractions of the dag are presented which correspond to models of transactions and to """"version graphs"""" in the literature. Since serializability of histories is known to be NP-complete, subclasses of serializable histories have been studied. One such class consists of histories serializable in a strict sense: transactions that are already in serial in a history must remain in the same relative order. When there are no useless actions in a history, strict serializability can be determined in polynomial time. If useless actions are permitted then strict serializability becomes NP-complete. The above results apply to two step transactions in which there is a read step followed by a write step, Each step involves some subset of the items in the database. With multistep transactions strict serializability is NP-complete even if there are no useless actions."""	cubic function;database transaction;directed acyclic graph;mutual exclusion;np-completeness;serializability;serialization;time complexity;vertex (geometry);vertex (graph theory)	Ravi Sethi	1981	22nd Annual Symposium on Foundations of Computer Science (sfcs 1981)	10.1109/SFCS.1981.8	global serializability;time complexity;games;combinatorics;real-time computing;commitment ordering;transaction processing;schedule;computer science;two-phase locking;solid;database;serializability;writing;skeleton;algorithm;encoding;polynomial;schedule	DB	-23.10898069921751	47.755642139233174	115201
2836049c47e162bb8a4f83dae35bb84bfca47dfc	financial application on an openstack based private cloud.		We build a private Cloud using off-the-shelf servers and do extensive experiments for application and performance testing. We studied a real-world application of financial option pricing by implementing two algorithms (Monte-Carlo simulation and binomial lattice for option pricing) on this private Cloud and used this application for the purpose of accuracy testing in comparison to a well established closed-form solution available as Black-Scholes-Merton formula. Also, using these algorithms we analyze performance of Cloud VMs. We compare the performance with standalone servers and found that the performance of Cloud VM is better to standalone servers when (a) the number of vCPUs are limited to a single node and (b) load balancing issues are not considered.		Molly G Sekar;Alice Rudolph;Ruppa K. Thulasiram;Parimala Thulasiraman	2018		10.1007/978-3-030-02738-4_10	cloud computing;load balancing (computing);valuation of options;finance;server;binomial;computer science	EDA	-24.914170308151864	58.73898491361023	115260
3487808382a91991f8c98fdb73e81cb5584d50c0	agent-oriented and constraint technologies for distributed transaction management	distributed transactions		distributed transaction	Viviana Mascardi;Emanuela Merelli	1999			distributed algorithm;extreme transaction processing;real-time computing;database transaction;transaction processing;distributed transaction;computer science;database;distributed computing;online transaction processing;compensating transaction;serializability;acid;transaction processing system	DB	-28.999450451909272	46.73292630982677	115508
4b14389e3ed8bdf2c470e69cc0eff3be6fdbe254	fast checkpoint recovery algorithms for frequently consistent applications	application development;database recovery;massively multiplayer online game;main memory databases;checkpointing;durability;experimental evaluation;transaction processing;logical logging	Advances in hardware have enabled many long-running applications to execute entirely in main memory. As a result, these applications have increasingly turned to database techniques to ensure durability in the event of a crash. However, many of these applications, such as massively multiplayer online games and main-memory OLTP systems, must sustain extremely high update rates - often hundreds of thousands of updates per second. Providing durability for these applications without introducing excessive overhead or latency spikes remains a challenge for application developers.  In this paper, we take advantage of frequent points of consistency in many of these applications to develop novel checkpoint recovery algorithms that trade additional space in main memory for significantly lower overhead and latency. Compared to previous work, our new algorithms do not require any locking or bulk copies of the application state. Our experimental evaluation shows that one of our new algorithms attains nearly constant latency and reduces overhead by more than an order of magnitude for low to medium update rates. Additionally, in a heavily loaded main-memory transaction processing system, it still reduces overhead by more than a factor of two.	algorithm;computer data storage;copy-on-write;crash (computing);dspace;durability (database systems);granular computing;interrupt latency;lock (computer science);massively multiplayer online role-playing game;online transaction processing;overhead (computing);snapshot isolation;software transactional memory;state (computer science);transaction processing system	Tuan Cao;Marcos Antonio Vaz Salles;Ben Sowell;Yao Yue;Alan J. Demers;Johannes Gehrke;Walker M. White	2011		10.1145/1989323.1989352	parallel computing;real-time computing;transaction processing;computer science;durability;database;distributed computing;rapid application development	DB	-20.12950595985454	50.16796405295014	115692
185f0ae22bf31ad9a909bb54a92c74b3e7aef6b1	distributed agent-based online auction system	distributed system;multi agent middleware;online auction	This paper concerns the design and development of a distributed agentbased online system for English auctions. The proposed system is composed of two parts: an Agent-based Auction Server and a Web-based Graphical User Interface. The first part of our work brought about the advantages introduced by the multiagent systems technology to the high-level of abstraction, modularity and performance of the server architecture and its implementation. On the server side, bids submitted by auction participants are handled by a hierarchical organization of agents that can be efficiently distributed on a computer network. This approach avoids the bottlenecks of bid processing that might occur during periods of heavy bidding, like for example snipping. We present experimental results that show a significant improvement of the server throughput compared with the architecture where a single auction manager agent is used for coordinating the participants for each active auction that is registered with the server. The second part of our work involved analysis of external functionalities, implementation and usability of a prototype online auction system that incorporates the Agent-based Auction Server. Our solution is outlined in terms of information flow management and its relation to the functionalities of the system. The main outcome of this part of the work is a clean specification of the information exchanges between the agent and non-agent software components of the system. Special attention is also given to the interoperability, understood here as successful integration of the different data communication protocols and software technologies that we employed for the implementation of the system.	agent architecture;agent-based model;component-based software engineering;computer cluster;distributed computing;e-commerce;experiment;graphical user interface;high- and low-level;interoperability;multi-agent system;prototype;requirement;scalability;server (computing);server-side;snipping tool;supercomputer;systems architecture;throughput;usability;web service;world wide web;on-line system	Costin Badica;Sorin Ilie;Alex Muscar;Amelia Badica;Liviu Sandu;Raluca Sbora;Maria Ganzha;Marcin Paprzycki	2014	Computing and Informatics		simulation;computer science;operating system;database;distributed computing;world wide web	Web+IR	-28.865125933405665	49.3296054030582	115742
95feb37145715eff3239f951773ea15c3103b0d9	autonomy requirements in heterogeneous distributed database systems.	multidatabase system;distributed database system	In the context of multidatabase systems and heterogeneous distributed database systems , it has been observed that autonomy of the component databases has to be violated in order to maintain traditional database and transaction properties. However, very little work exists that systematically analyzes (a) the semantics of autonomy and (b) the implications of autonomy vis a vis correctness speciications and database protocols. Hence, this paper is aimed at characterizing the diierent types of autonomy by focusing on transaction management and showing the relationships between autonomy requirements and database protocols. As a case-study, we investigate the autonomy implications of the two-phase commit protocol and its multidatabase variants. Our analysis shows that these protocols involve tradeoos between the autonomy of the transactions, with respect to accessing the data objects, and the autonomy of the transaction management system, with respect to responding to the transaction management primitives. As a result, this paper brings out the practical considerations involved in selecting between alternative protocols.	autonomy;correctness (computer science);distributed database;emoticon;requirement;transaction processing;two-phase commit protocol	Panos K. Chrysanthis;Krithi Ramamritham	1994			computer science;distributed database;distributed concurrency control	DB	-24.035978504749465	46.92238869719536	115785
1b98b4c6119214c197cf0246a79bbdb361abbe1f	measuring the cost of scalability and reliability for internet-based, server-centered applications	application development;geographically separated replicated servers;multicast communication;degradation;reliability;passive replication;availability;collaboration;costs scalability internet web server delay collaboration degradation availability ip networks network servers;client server systems;synchronous networks;corba;reliable multicast;dependable internet based application development;geographically dispersed clients;corba scalability cost reliability internet based server centred applications geographically dispersed clients dependable internet based application development decentralized approach logical structuring collaborating subsystems geographically separated replicated servers internet auction centralized approach distributed servers multicast groups passive replication reliable multicasting membership service synchronous networks asynchronous networks;network servers;centralized approach;asynchronous networks;internet;internet auction;distributed object management;distributed servers;ip networks;scalability;web server;network servers internet reliability client server systems replicated databases distributed object management multicast communication;decentralized approach;logical structuring;membership service;reliable multicasting;collaborating subsystems;scalability cost;replicated databases;multicast groups;internet based server centred applications	With large numbers of geographically dispersed clients, a centralized approach to Internet-based application development is not scalable and also not dependable. This paper presents a decentralized approach to dependable Internet based application development, consisting of a logical structuring of collaborating subsystems of geographically-apart replicated servers. Two implementations of an Internet auction, one using a centralized approach and the other using our decentralized approach, are described. To evaluate the scalability of the two approaches, a number of experiments are performed on these implementations and the results presented here.	centralized computing;dependability;experiment;internet;scalability	Paul D. Ezhilchelvan;Mohammad-Reza Khayyambashi;Graham Morgan;Doug Palmer	2001		10.1109/WORDS.2001.945114	computer science;database;distributed computing;computer network	SE	-24.637670297101252	52.10509683035439	115829
ce750c615a1876b8257cdfd4fe9306c8e2df1a68	high-speed distributed data handling for on-line instrumentation systems		The advent (and promise) of shared, widely available, high-speed networks provides the potential for new approaches to the collection, organization, storage, and analysis of high-speed and high-volume data streams from high data-rate, on-line instruments. We have worked in this area for several years, have identified and addressed a variety of problems associated with this scenario, and have evolved an architecture, implementations, and a monitoring methodology that have been successful in addressing several different application areas.In this paper we describe a distributed, wide area network-based architecture that deals with data streams that originate from on-line instruments. Such instruments and imaging systems are a staple of modern scientific, health care, and intelligence environments. Our work provides an approach for reliable, distributed real-time analysis, cataloguing, and archiving of the data streams through the integration and distributed management of a high-speed distributed cache, distributed high-performance applications, and tertiary storage systems.		William E. Johnston;William Greiman;Gary Hoo;Jason Lee;Brian Tierney;Craig E. Tull;Doug Olson	1997		10.1109/SC.1997.10025	shared memory;parallel computing;real-time computing;distributed memory;computer science;operating system;group method of data handling;database;data analysis;health care;computer network	HPC	-20.263706080067657	53.135068603549065	115982
3f6eb56461bb589604a0aeefc355ce7ea3345280	datamill: rigorous performance evaluation made easy	performance;datamill;robustness;experimentation;repeatability;infrastructure;reproducibility	Empirical systems research is facing a dilemma. Minor aspects of an experimental setup can have a significant impact on its associated performance measurements and potentially invalidate conclusions drawn from them. Examples of such influences, often called hidden factors, include binary link order, process environment size, compiler generated randomized symbol names, or group scheduler assignments. The growth in complexity and size of modern systems will further aggravate this dilemma, especially with the given time pressure of producing results. So how can one trust any reported empirical analysis of a new idea or concept in computer science?  This paper introduces DataMill, a community-based easy-to-use services-oriented open benchmarking infrastructure for performance evaluation. DataMill facilitates producing robust, reliable, and reproducible results. The infrastructure incorporates the latest results on hidden factors and automates the variation of these factors. Multiple research groups already participate in DataMill.  DataMill is also of interest for research on performance evaluation. The infrastructure supports quantifying the effect of hidden factors, disseminating the research results beyond mere reporting. It provides a platform for investigating interactions and composition of hidden factors.	compiler;computer science;interaction;performance evaluation;randomized algorithm;scheduling (computing);systems theory	Augusto Born de Oliveira;Jean-Christophe Petkovich;Thomas Reidemeister;Sebastian Fischmeister	2013		10.1145/2479871.2479892	repeatability;simulation;performance;computer science;engineering;electrical engineering;reproducibility;data mining;management science;statistics;robustness	Metrics	-23.60706443030283	53.42335255311756	116194
db32d9adc8ea582b390bdc643cffb81a7b6383c1	towards chemical coordination for grids	pervasive computing;run time system;rfid tags;software component;tuple based coordination;runtime system;quality of service	"""In [6], Ian Foster and Karl Kesselman explain that grids need """"a rethinking of existing programming models and, most likely, new thinking about novel models"""". In this work, we investigate a """"novel programming model"""" for grids based on the chemical metaphor."""	programming model	Jean-Pierre Banâtre;Pascal Fradet;Yann Radenac	2006		10.1145/1141277.1141381	radio-frequency identification;real-time computing;quality of service;computer science;theoretical computer science;component-based software engineering;operating system;software engineering;database;distributed computing;programming language;world wide web;computer security;ubiquitous computing	HPC	-30.20950525783085	47.89799295974401	116336
b84c3ef6905faec0589ff319b23a3b5e236d19d5	distributed transactions for semantic web workflows - overcoming the cap limitations on virtual organizations	availability;distributed processing;distributed transactions;semantic web workflows;ontologies artificial intelligence;worst case infinite delay;servers;distributed decision making;clouds;virtual organization;mobile communication;semantic web distributed processing middleware ontologies artificial intelligence;semantic web;cooperative engineering;middleware;ontologies;virtual organizations;organizations;cap theorem;virtual synchrony cap theorem transactions cooperative engineering;ontologies organizations servers availability clouds educational institutions mobile communication;transactions;virtual synchrony;infinite delay;cooperative work;worst case infinite delay distributed transactions semantic web workflows ontologies virtual organizations cap theorem distributed decision making	Short term and long term transactions will become very important for both personal and cooperative work involving workflows that use ontologies of the Semantic Web. We describe a way to get around the CAP theorem (the impossibility of simultaneous consistency, availability, and the possibility of network partitioning) and the worst-case infinite delay for any distributed decision-making.	acknowledgement (data networks);authorization;backup;best, worst and average case;cap theorem;distributed transaction;hypertext transfer protocol;middleware;multicast;network partition;ontology (information science);process group;programmer;representational state transfer;semantic web;server (computing);stateless protocol;transaction processing system;virtual synchrony;web server	Daniel J. Buehrer;Chun-Yao Wang	2010	International Symposium on Parallel and Distributed Processing with Applications	10.1109/ISPA.2010.17	cap theorem;availability;mobile telephony;distributed transaction;computer science;organization;ontology;operating system;semantic web;middleware;database;distributed computing;virtual synchrony;world wide web;server	OS	-24.7485060460517	48.29864577805081	116371
621423aa9ca3d75658fddc68be260aa3643ff165	latency-aware placement of data stream analytics on edge computing		The interest in processing data events under stringent time constraints as they arrive has led to the emergence of architecture and engines for data stream processing. Edge computing, initially designed to minimize the latency of content delivered to mobile devices, can be used for executing certain stream processing operations. Moving operators from cloud to edge, however, is challenging as operator-placement decisions must consider the application requirements and the network capabilities. In this work, we introduce strategies to create placement configurations for data stream processing applications whose operator topologies follow series parallel graphs. We consider the operator characteristics and requirements to improve the response time of such applications. Results show that our strategies can improve the response time in up to 50% for application graphs comprising multiple forks and joins while transferring less data and better using the resources.	edge computing	Alexandre Da Silva Veith;Marcos Dias de Assunção;Laurent Lefèvre	2018		10.1007/978-3-030-03596-9_14	data stream;architecture;real-time computing;operator (computer programming);latency (engineering);stream processing;cloud computing;computer science;analytics;edge computing	DB	-19.130871486809173	56.51755702156934	116412
05f213d51dc7ea3fadbb4794256ec24d774b5533	locking policies: safety and freedom from deadlock	databases;concurrent computing;history;bioreactors;gain;tellurium;contracts;strontium;protection;artificial neural networks;computer aided software engineering;system recovery;transaction databases;safety system recovery transaction databases concurrency control bioreactors tin protection concurrent computing tellurium contracts;coils;safety;concurrency control;schedules;organizations;switches;tin;integrated circuits;titanium	A database consists of ellliflc.\' yvhich reLlte to each other in certain ways, i,e., they satisfy cerltlin cOllsistency constraints. Many tinles, when a user updates the database, he nlay have to update tcnlporarily these constraints in orde r tC) eventuaII y t I'an s1'0 I' 111 the database in to a new, consis ten t stat C . For this I'eas 0 n, at 0 nl ic act ion s by the sa nlC user arc grou ped toget her into un its of consistency called transactiolls. In practice. a transaction nlay be either an interactive session, or the execution of a user update progranl. When, however, nlan y' transactions access and update the SanlC database cOI1curTently, there rnust he sonle kind of coordination to ensure that the resulting sequence of interleaved atonlic actions (or schedule) is correct. This TlleanS that all transactions have a consistent view of the data. and furthernlore the database is Icft at the end in sonle consistent state, This required coordination is achic\cd via the COIlcurrency cOlltrol,nechalllsfn ()f the database. ('onsiderahle research effort has heen devoted recently to the theoretical aspects or the design of such a systenl !ECiLTl. SLR, SK, KS, Pa, PBR, KPI. The theory of databasc concurrency control bears a superficial silllilarity to the () pe ra ting systenl Sinspi I'ed con cLI rrency 1he 0 I'Y [K [vI, (' [) 1. The difference is lhtl{ in operating systeIllS \\le have cooperating, Ill0nitoring. dnd 1110n itored. processes, and the goal is to prevent had cooperation or Tllanagenlent (e.g. indetcrnlinacy. deadlocks) In databases, we have a population of' users that arc una\\'are of each other's pres-	concurrency (computer science);concurrency control;database;deadlock;digital single-lens reflex camera;naruto shippuden: clash of ninja revolution 3;pbr theorem;ski combinator calculus;the superficial	Mihalis Yannakakis;Christos H. Papadimitriou;H. T. Kung	1979	20th Annual Symposium on Foundations of Computer Science (sfcs 1979)	10.1109/SFCS.1979.22	titanium;real-time computing;strontium;concurrent computing;network switch;schedule;gain;tin;computer science;organization;concurrency control;database;tellurium;computer-aided software engineering;bioreactor	Theory	-24.065045281337525	47.61719130015595	116465
56576e69d50c091787cdb954a3fb029a8052c611	towards energy-proportional anomaly detection in the smart grid		Phasor Measurement Unit (PMU) deployment is increasing throughout national power grids in an effort to improve operator situational awareness of rapid oscillations and other fluctuations that could indicate a future disruption of service. However, the quantity of data produced by PMU deployment makes real-time analysis extremely challenging, causing grid designers to invest in large centralized analysis systems that consume significant amounts of energy. In this paper, we argue for a more energy-proportional approach to anomaly detection, and advocate for a decentralized, heterogeneous architecture to keep computational load at acceptable levels for lower-energy chipsets. Our results demonstrate how anomalies can be detected at real-time speeds using single board computers for on-line analysis, and in minutes when running off-line historical analysis using a multicore server running Apache Spark.		Spencer Drakontaidis;Michael Stanchi;Gabriel Glazer;Jason Hussey;Aaron St. Leger;Suzanne J. Matthews	2018	2018 IEEE High Performance extreme Computing Conference (HPEC)	10.1109/HPEC.2018.8547695	anomaly detection;software deployment;grid;real-time computing;architecture;smart grid;multi-core processor;phasor measurement unit;computer science;server	HPC	-24.239986453441674	59.07131363255342	116820
442789b6ad35156e318e3fad86f7180eb06be1d9	program and file allocation algorithm for large scale distributed systems	large scale distributed systems			Raouf Boutaba;Bertil Folliot	1994			distributed algorithm;parallel computing;database;distributed computing;distributed design patterns;replication	HPC	-28.778380963796586	46.96328405721397	116834
6fc963dfe8649b33144341d58aab23824e94e6b3	niobe: a practical replication protocol	distributed system;replication;storage system;first principle;specification language;large scale;replicated data;enterprise storage;internet services;primary backup	The task of consistently and reliably replicating data is fundamental in distributed systems, and numerous existing protocols are able to achieve such replication efficiently. When called on to build a large-scale enterprise storage system with built-in replication, we were therefore surprised to discover that no existing protocols met our requirements. As a result, we designed and deployed a new replication protocol called Niobe. Niobe is in the primary-backup family of protocols, and shares many similarities with other protocols in this family. But we believe Niobe is significantly more practical for large-scale enterprise storage than previously published protocols. In particular, Niobe is simple, flexible, has rigorously proven yet simply stated consistency guarantees, and exhibits excellent performance. Niobe has been deployed as the backend for a commercial Internet service; its consistency properties have been proved formally from first principles, and further verified using the TLA + specification language. We describe the protocol itself, the system built to deploy it, and some of our experiences in doing so.	backup;canonical account;computer data storage;distributed computing;experience;requirement;specification language	John MacCormick;Chandramohan A. Thekkath;Marcus Jager;Kristof Roomp;Lidong Zhou;Ryan R. Peterson	2008	TOS	10.1145/1326542.1326543	replication;real-time computing;specification language;first principle;computer science;operating system;database;distributed computing;enterprise storage;programming language	OS	-24.64356545072262	53.839086217355195	116838
274c5ceafdcf7ae70f8ebdaf2f66710ab9337157	limosense - live monitoring in dynamic sensor networks		We present LiMoSense, a fault-tolerant live monitoring algorithm for dynamic sensor networks. This is the first asynchronous robust average aggregation algorithm that performs live monitoring, i.e., it constantly obtains a timely and accurate picture of dynamically changing data. LiMoSense uses gossip to dynamically track and aggregate a large collection of ever-changing sensor reads. It overcomes message loss, node failures and recoveries, and dynamic network topology changes. We formally prove the correctness of LiMoSense; we use simulations to illustrate its ability to quickly react to changes of both the network topology and the sensor reads, and to provide accurate information.		Ittay Eyal;Idit Keidar;Raphael Rom	2011		10.1007/978-3-642-28209-6_7	real-time computing;computer science;distributed computing;computer network	Mobile	-22.99685038337267	50.8576893846488	116955
2a329bfb7906e722a23e593a30a116584ff83ea9	containerization and the paas cloud	kernel;cluster;virtualization;virtualisation cloud computing distributed processing virtual machines;distributed multicloud platform containerization platform as a service paas cloud lightweight virtualization solution virtual machine application packaging mechanism;cloud;packaging;computer architecture;multicloud;paas;docker;linux;kubernetes;virtualization cloud cloud computing cluster container docker kubernetes multicloud paas;containerization;containerization virtualization file systems linux computer architecture packaging;file systems;container;containers;cloud computing	Containerization is widely discussed as a lightweight virtualization solution. Apart from exhibiting benefits over traditional virtual machines in the cloud, containers are especially relevant for platform-as-a-service (PaaS) clouds to manage and orchestrate applications through containers as an application packaging mechanism. This article discusses the requirements that arise from having to facilitate applications through distributed multicloud platforms.	cloud computing;distributed shared memory;multicloud;platform as a service;requirement;virtual machine	Claus Pahl	2015	IEEE Cloud Computing	10.1109/MCC.2015.51	embedded system;cloud computing;computer science;operating system;distributed computing	HPC	-29.57681933650836	55.86046050862442	117359
93288e01ad20158c40e017dd840ffdda50dbee59	research of integrating military storage resources based on storage virtualization	storage virtualization;resource virtualization secure storage resource management network servers electronic mail telecommunication network reliability usability application virtualization middleware data security;storage system;information warfare;military storage resource integration;virtual memories;information storage;military storage systems;network storage system military storage resource integration storage virtualization information warfare information storage military storage systems;network storage system;virtual storage information storage military computing;virtual storage;military computing	In information warfare (IW), information storage is one of important processes to obtaining information dominance. Existing military storage systems are mostly independent, so they can't provide reliable and effective storage service for users. How to integrate existing military storage systems into a network storage system which have stronger compatibility, higher usability and better cooperation? It's the problem urgently need us to solve. The article makes a detailed introduction to technologies of storage virtualization (SV). As to its different realization methods, we analyze both their advantages and disadvantages. In the paper, the conception of integrating military storage resources based on SV is proposed. Moreover, some questions in processes of integration are researched	computer data storage;iw engine;storage virtualization;systemverilog;usability	Qichao Xia;Lianxing Jia;Qicong Shen;Dan Feng	2006	2006 International Workshop on Networking, Architecture, and Storages (IWNAS'06)	10.1109/IWNAS.2006.39	storage efficiency;persistence;embedded system;virtualization;converged storage;object storage;computer science;operating system;emc invista;database;information repository;computer security;storage virtualization	HPC	-32.7461035640349	48.158183667105014	117450
7d4f367c838253c887935185f5a351e87d378451	an efficient strategy for beneficial semijoins				Ye-In Chang;Bor-miin Liu;Cheng-Huang Liao	1998	Informatica (Slovenia)		computer science;distributed computing	AI	-27.829103598986563	47.30392789676907	117802
eac94597db7f8c23357712e5c7c91d44a7cb63d4	distributing efficiently the block-max wand algorithm	wand;inverted files;efficient distributed query evaluation	Large search engines are complex systems composed by several services. Each service is composed by a set of distributed processing nodes dedicated to execute a single operation required to user queries. One of these services is in charge of computing the top-k document results for queries by means of a document ranking operation. This ranking service is a major bottleneck in efficient query processing as billions of documents has to be processed each day. To answer user queries within a fraction of a second, techniques such as the Block-Max WAND algorithm are used to avoid fully processing all documents related to a query. In this work, we propose to efficiently distributing the Block-Max WAND computation among the ranking service processing nodes. Our proposal is devised to reduce memory usage and computation cost by assuming that each one of the P ranking processing nodes provide top-K/P + α documents results, where α is an estimation parameter which is dynamically set for each query. The experimental results show that the proposed approach significantly reduces execution time compared against current approaches used in search engines. c © 2013 The Authors. Published by Elsevier B.V. Selection and/or peer-review under responsibility of the organizers of the 2013 International Conference on Computational Science.	algorithm;complex systems;computation;computational science;database;distributed computing;ranking (information retrieval);run time (program lifecycle phase);time complexity;web search engine	Oscar Rojas;Veronica Gil Costa;Mauricio Marín	2013		10.1016/j.procs.2013.05.175	ranking;computer science;operating system;data mining;database;world wide web	DB	-20.533740189124618	54.310488710022234	117899
8284330eb2c13d9f1fd7a98e883bfc2c732412ef	peer-to-peer based grid workflow runtime environment of swindew-g	large scale resource sharing;decentralised workflow management system peer to peer based grid workflow runtime environment swindew g swinburne decentralised workflow for grid large scale resource sharing system integration e science workflow systems performance bottlenecks;peer to peer computing grid computing;swinburne;p2p;indexing terms;swindew g;e science workflow systems;large scale;decentralised workflow management system;performance bottlenecks;workflow system;peer to peer computing runtime environment quality of service grid computing resource management costs communications technology scalability centralized control java;system design;system integration;resource sharing;grid service;workflow management system;swinburne decentralised workflow for grid;peer to peer computing;peer to peer;grid computing;peer to peer based grid workflow runtime environment	Nowadays, grid and peer-to-peer (p2p) technologies have become popular solutions for large- scale resource sharing and system integration. For e- science workflow systems, grid is a convenient way of constructing new services by composing existing services, while p2p is an effective approach to eliminate the performance bottlenecks and enhance the scalability of the systems. However, existing workflow systems focus either on p2p or grid environments and therefore cannot take advantage of both technologies. It is desirable to incorporate the two technologies in workflow systems. SwinDeW-G (Swinburne Decentralised Workflow for Grid) is a novel hybrid decentralised workflow management system facilitating both grid and p2p technologies. It is derived from the former p2p based SwinDeW system but redeveloped as grid services with communications between peers conducted in a p2p fashion. This paper describes the system design and functions of the runtime environment of SwinDeW-G.	algorithm;e-science;grid computing;load balancing (computing);peer-to-peer;runtime system;scalability;scheduling (computing);system integration;systems design;universal instantiation	Yun Yang;Ke Liu;Jinjun Chen;Joel Lignier;Hai Jin	2007	Third IEEE International Conference on e-Science and Grid Computing (e-Science 2007)	10.1109/E-SCIENCE.2007.56	shared resource;index term;computer science;knowledge management;operating system;peer-to-peer;database;distributed computing;workflow management system;workflow engine;grid computing;system integration;systems design;workflow technology	HPC	-31.27073116369526	50.09916974906615	117965
fe79e46b54d394ef74a8a4cae46fee64357d63f0	xaas multi-cloud marketplace architecture enacting the industry 4.0 concepts		Cloud computing in conjunction with recent advances in Cyber-Physical Systems (CPSs) unravels new opportunities for the European manufacturing industry for high value-added products that can quickly reach the market for sale. The Internet of Things joins the Internet of Services to enact the fourth industrial revolution that digitalises the manufacturing techniques and logistics while pushing forward the development of improved factories with machine-to-machine communication deliv‐ ering massively customised products tailored to the individualised needs of the customer. Interconnected CPSs using internal and cross-organizational services coop‐ erate in real time increase the business agility and flexibility of manufacturing compa‐ nies. Using CPS with cloud computing architectures leverage data and services stored and run in cloud environment from different vendors that usually offer different service interfaces to share their services and data. However, when using such archi‐ tectures data silos appear and different vendors having different service interfaces can easily result in vendor lock-in issues. This paper proposes a multi-cloud marketplace architecture leveraging the existing myriad of different cloud environments at different abstraction levels including the Infrastructure-, Platform-, and Software-asa-Service cloud models—that is, Everything as a Service or XaaS—delivering serv‐ ices and with different properties that can control the computation happening in multiple cloud environments sharing resources with each other.	cloud computing;computation;cyber-physical system;industry 4.0;information silo;internet of things;internet protocol suite;logistics;machine to machine;vendor lock-in	Adrian Juan Verdejo;Bholanathsingh Surajbali	2016		10.1007/978-3-319-31165-4_2	embedded system;simulation;engineering;operating system	HPC	-30.83078276161601	59.6514581278339	118067
8f391c43c9c4637957976f428043546578fb3db4	constructing a performance database for large-scale quantum chemistry packages	performance tool;development process;large scale;robust method;performance database;computational quality of service;common component architecture;quality of service;similarity function;quantum chemistry	When several large-scale quantum chemistry packages interoperate through components and some components provide similar functionality, we are faced with many challenges such as efficiently selecting the component with the best efficiency, finding compromises between efficiency and accuracy, or constructing new computations from available components with minimum overhead. These challenges are core questions in Computational Quality of Service (CQoS) research, and exploring robust methods for these questions requires a performance database as the foundation for referencing historical performance data. However, these large-scale packages have a long history of development, provide many complex computations, and involve a large number of chemists in the development process. Building a database for these packages is thus not as straightforward as simply selecting a database engine and uploading data. In this paper, we present our efforts in fast prototyping a system to construct a performance database for quantum chemistry packages. We discuss the requirements for such a system, delineate the tasks in each building stage, evaluate how current tool technologies can facilitate the building process, and discuss the support required from the performance tools development community for future CQoS research.	computation;database engine;download;interoperability;overhead (computing);quality of service;requirement;upload	Meng-Shiou Wu;Hirotoshi Mori;Jonathan L. Bentz;Theresa L. Windus;Heather Netzloff;Masha Sosonkina;Mark S. Gordon	2008			simulation;quality of service;common component architecture;computer science;systems engineering;engineering;artificial intelligence;theoretical computer science;operating system;data mining;quantum chemistry;software development process;computer network	DB	-32.12867356728262	53.41395854918991	118311
9295ff38bfbc294406dd80007bc2c2510828411c	powering statistical genetics with the grid: using gridway to automate r-based workflows	job flow management;globus toolkit;workflow management;shared memory;heterogeneous systems;portal;profiling;genetics;good practice;statistical analysis;meta scheduling;grid enablement;high performance computer;infrastructure investment;scientific applications;resource availability;division of labor;modeling;grid computing;geographic distribution;wrf;permutation test	"""Many computationally intense workflows are composed of the same algorithm applied to many data sets. For example, it is good practice in statistical genetics to assess the validity of a method by simulating thousands of datasets of known properties. Further each simulation may involve using permutation tests that necessitate repeating analyses thousands of times per data set. Improvements in the overall throughput of this workflow can be achieved with a straightforward increase in the number of computations that can take place simultaneously. High performance compute clusters have significantly improved the ability to run many such computations simultaneously and have shown the adaptability of these workflows to ever-increasing processing capacity.  While clusters of increasing size can be constructed to improve throughput, additional hardware acquisitions impose increasing financial and operating environment burdens. Leveraging multiple clusters in distributed operating centers can dramatically increase capacity while alleviating infrastructure growth burdens, but adds significant complexity in managing workflows across heterogeneous systems and administrative domains.  Grid computing can offer immediate benefits in this area of multi-cluster workflows by offering a consistent, full-featured, programmatic interface and uniform identity infrastructure across cluster and administrative boundaries. The Globus Toolkit in combination with the GridWay meta-scheduler more realistically improves the ability to leverage large, geographically distributed, multi-cluster collections to orchestrate workflows for significant gains in throughput.  Researchers in the field of biostatistics at UAB are heavy users of the R statistical and graphical software environment, available on multiple clusters on campus. Their R-based statistical analysis workflow falls in the above category of applications. Large numbers of R job runs are currently being managed by manually dividing the workload across clusters based on resources availability. This is clearly cumbersome for the end user to manage and unlikely to result in the optimal division of labor amongst available resources.  The focus of our efforts to grid-enable R uses the GridWay meta-scheduler to access existing resources through the Globus-based, campus grid platform, and improve the workflow management across this set of compute resources, optimizing throughput. Because GridWay schedules jobs on multiple clusters using the uniform interface of the Globus Toolkit, this solution promises to transparently increase the throughput for the R workflow with the simple inclusion of additional compute resources. Our plans include adding a large shared-memory compute resource from the state supercomputing center and other clusters through collaborations with partners in SURAgrid, a regional grid infrastructure.  There are many dimensions to """"grid-enabling"""" applications, including complex re-engineering of algorithms. It is advantageous, however, to take a step-wise approach to grid adoption that first maximizes workflow throughput by leveraging the most broadly available, commodity infrastructures. By concentrating on throughput gains first, existing infrastructure investments can be maximized and time-consuming algorithmic redesigns can be delayed until more capable infrastructures exist to address coordination and latency considerations. This presentation will focus on this first step of grid-enabling R and detail experiences and performance gains."""	algorithm;biostatistics;computation;computer cluster;graphical user interface;grid computing;meta-scheduling;operating environment;r language;resampling (statistics);scheduling (computing);shared memory;simulation;supercomputer;throughput	John-Paul Robinson;Purushotham Bangalore;Jelai Wang;Tapan Mehta	2008		10.1145/1341811.1341856	computer science;data mining;database;distributed computing	HPC	-28.831819230958992	51.222047992422134	118948
2d4de256efb1077bd09ba3b7a6995eb3723ad7b2	census: location-aware membership management for large-scale distributed systems	article	We present Census, a platform for building large-scale distributed applications. Census provides a membership service and a multicast mechanism. The membership service provides every node with a consistent view of the system membership, which may be global or partitioned into location-based regions. Census distributes membership updates with low overhead, propagates changes promptly, and is resilient to both crashes and Byzantine failures. We believe that Census is the first system to provide a consistent membership abstraction at very large scale, greatly simplifying the design of applications built atop large deployments such as multi-site data centers. Census builds on a novel multicast mechanism that is closely integrated with the membership service. It organizes nodes into a reliable overlay composed of multiple distribution trees, using network coordinates to minimize latency. Unlike other multicast systems, it avoids the cost of using distributed algorithms to construct and maintain trees. Instead, each node independently produces the same trees from the consistent membership view. Census uses this multicast mechanism to distribute membership updates, along with application-provided messages. We evaluate the platform under simulation and on a real-world deployment on PlanetLab. We find that it imposes minimal bandwidth overhead, is able to react quickly to node failures and changes in the system membership, and can scale to substantial size.	byzantine fault tolerance;data center;distributed algorithm;distributed computing;multicast;overhead (computing);planetlab;simulation;software deployment	James A. Cowling;Dan R. K. Ports;Barbara Liskov;Raluca A. Popa;Abhijeet Gaikwad	2009			data mining;database;management science	OS	-24.617530882371003	52.91588978297971	118974
a1e2ad51719f6869536d1ab1fa240597a9502323	a quorum-based intelligent replicas management in data grids to improve performances			performance	Farouk Bouharaoua;Ghalem Belalem	2017	Multiagent and Grid Systems	10.3233/MGS-170265	distributed computing;computer science	Robotics	-29.44404548452194	46.968463593319896	119162
7d1ef4988ea27c9e9f157d595d11eb9d0aa21a29	scalable distributed two-layer data structures (sd2ds)	databases;two layer design;multicomputers;scalable distributed data structures;distributed ram;scalability;computer science	Scalability and fault tolerance are important features of modern applications designed for the distributed, loosely-coupled computer systems. In the paper, two-layer scalable structures for storing data in a distributed RAM of a multicomputer (SD2DS) are introduced. A data unit of SD2DS (a component) is split into a header and a body. The header identifies the body and contains its address in a network. The headers are stored in the first layer of SD2DS, called the component file, while the bodies are stored in the second layer, called the component storage. Both layers are managed independently. Details of the management algorithms are given, along with SD2DS variant suitable for storing plain records of data. The SD2DS is compared to similar distributed structures and frameworks. Comparison considerations together with test results are also given. The results proved superiority of SD2DS over similar structures.	algorithm;fault tolerance;parallel computing;random-access memory;scalability	Krzysztof Sapiecha;Grzegorz Lukawski	2013	IJDST	10.4018/jdst.2013040102	parallel computing;scalability;computer science;operating system;database;distributed computing	HPC	-20.69775917704861	51.28055285746322	119230
7a5409c9bd32ac2f08d37c1c438f1f2acf5d0d86	an empirical performance metrics measurement and analysis of software platforms for implementation of web services	software metrics;software;measurement;generic model;software platform;network operating systems;distributed computing;web service;j2ee;software measurement performance analysis software performance web services application software simple object access protocol information science information analysis electronic mail distributed computing;performance metric;subspace constraints;net;network traffic performance metrics measurement software platform web service distributed computing grid computing net j2ee;servers;time factors;web services grid computing java network operating systems software metrics;network traffic;web services;performance metrics web services;performance metrics measurement;simple object access protocol;grid computing;performance metrics;java	Web services play a vital role in the paradigms of distributed computing and grid computing. Web services can be implemented in different platforms, but the most commonly used platforms are .NET and J2EE as they provide a wide variety of tools for creation and integration of web services to any existing business application. The selection of appropriate platform for implementation is purely based on performance offered by the particular platform. This paper focuses on analyzing the performance of a generic model composed of three tiers for implementing and consuming web services. This generic model is implemented with four different combinations of .NET and J2EE in different tiers. The analysis of performance is carried out with a novel set of four performance metrics proposed and the metrics are based on time spent for actual response and network traffic involved between the different tiers. The performance metrics measurement is observed on the model to study performance of platforms deployed for implementation of web services.	.net framework;business software;distributed computing;grid computing;java platform, enterprise edition;network traffic control;software metric;web service	K. Velmurugan;M. A. Maluk Mohamed	2009	2009 Eighth IEEE/ACIS International Conference on Computer and Information Science	10.1109/ICIS.2009.194	web service;web modeling;computer science;ws-policy;database;distributed computing;services computing;world wide web	HPC	-33.41577432131436	51.67516971521559	119234
5b2dbd078efa8d65e3c031a3c08f18c88fdc1b8a	a self-adaptive spike detection algorithm with application in performance and capacity management		This paper presents a simple, yet novel, approach to address the problem of spike detection. The definition of a spike varies with the domain under study. This results in domain-specific solutions that are often rendered unusable in other domains. Moreover, the spike detection algorithms proposed in the past are effective only when suitably tuned. This often limits their usability in real-life scenarios. In this paper, we present a generic approach to address the problem. We propose various spike definitions and present a computationally light-weight algorithm that can cater to all these definitions. The algorithm can automatically adapt itself to changing data characteristics. We demonstrate the soundness of our approach by experimental evaluation. We also compare the proposed algorithm with the existing implementations. We then present an application of spike detection in the domain of performance and capacity management in enterprise systems by presenting real-life case-studies.	algorithm;anomaly detection;baseline (configuration management);bifurcation theory;enterprise system;foremost;modal logic;preprocessor;real life;sensor;smoothing;the spike (1997);time series;usability	Shivam Sahai;Maitreya Natu;Manoj Jain	2009			simulation;computer science;theoretical computer science;algorithm	ML	-28.445086334429543	60.083471363554835	119343
bf3011be477d07b0377fec155e29178b8789ed65	orchestrating complex application architectures in heterogeneous clouds	cloud-computing;heterogeneous-cloud;multi-cloud;open-source;tosca	Private cloud infrastructures are now widely deployed and adopted across technology industries and research institutions. Although cloud computing has emerged as a reality, it is now known that a single cloud provider cannot fully satisfy complex user requirements. This has resulted in a growing interest in developing hybrid cloud solutions that bind together distinct and heterogeneous cloud infrastructures. In this paper we describe the orchestration approach for heterogeneous clouds that has been implemented and used within the INDIGO-DataCloud project. This orchestration model uses existing open-source software like OpenStack and leverages the OASIS Topology and Specification for Cloud Applications (TOSCA) open standard as the modeling language. Our approach uses virtual machines and Docker containers in an homogeneous and transparent way providing consistent application deployment for the users. This approach is illustrated by means of two different use cases in different scientific communities, implemented using the INDIGO-DataCloud solutions.		Miguel Caballer;Sahdev Zala;Álvaro López García;Germán Moltó;Pablo Orviz Fernández;Mathieu Velten	2017	Journal of Grid Computing	10.1007/s10723-017-9418-y	software deployment;distributed computing;modeling language;orchestration (computing);cloud computing;computer science;software;virtual machine;use case;user requirements document	HPC	-31.55716266069122	52.809933754491816	119629
7599e451bf42f341b9c1c2d5604a4cab023e07f6	breaking apart the vfs for managing file systems		File system management applications, such as data scrubbers, defragmentation tools, resizing tools, and partition editors, are essential for maintaining, optimizing, and administering storage systems. These applications require fine-grained control over file-system metadata and data, such as the ability to migrate a data block to another physical location. Such control is not available with the VFS API, and so these applications bypass the VFS and access and modify file-system metadata directly. As a result, these applications do not work across file systems, and must be developed from scratch for each file system, which involves significant engineering effort and impedes adoption of new file systems. Our goal is to design an interface that allows these management applications to be written once and be usable for all file systems that support the interface. Our key insight is that these applications operate on common file system abstractions, such as file system objects (e.g., blocks, inodes, and directory entries), and the mappings from logical blocks of a file to their physical locations. We propose the Extended Virtual File System (eVFS) interface that provides fine-grained access to these abstractions, allowing the development of generic file system management applications. We demonstrate the benefits of our approach by building a file-system agnostic conversion tool that performs in-place conversion of a source file system to a completely different destination file system, showing that arbitrary modifications to the file system format can be handled by the interface.	directory (computing);high- and low-level;in-place algorithm;list of disk partitioning software;systems management	Kuei Sun;Matthew Lakier;Angela Demke Brown;Ashvin Goel	2018				OS	-25.64891023164494	50.72030933167052	119875
f9d7e0e53c244ee4cc53668c8ee2d8e8985fbc5f	a quantitative analysis of redundancy schemes for peer-to- peer storage systems	storage system;erasure code;p2p;statistical properties;quantitative analysis;peer to peer;loss probability	Fully decentralized peer-to-peer (P2P) storage systems lack the reliability guarantees that centralized systems can give. They need to rely on the system's statistical properties, only. Nevertheless, such probabilistic guarantees can lead to highly reliable systems. Moreover, their statistical nature makes P2P storage systems an ideal supplement to centralized storage systems, because they fail in entirely different circumstances than centralized systems.#R##N##R##N#In this paper, we investigate the behavior of different replication and erasure code schemes as peers fail. We calculate the data loss probability and the repairing delay, which is caused by the peers' limited bandwidth. Using a Weibull model to describe peer behavior, we show that there are four different loss processes that affect the availability and durability of the data: initial loss, diurnal loss, early loss, and longterm loss. They need to be treated differently to obtain optimal results. Based on this insight we give general recommendations for the design of redundancy schemes in P2P storage systems.		Yaser Houri;Bernhard Amann;Thomas Fuhrmann	2010		10.1007/978-3-642-16023-3_40	real-time computing;computer science;distributed computing;computer security	HPC	-22.605271205597116	53.15254472761775	119948
ed0e23d4b009b4f85412796929deed7cfa4d367a	cache management in corba distributed object systems	cache storage;caching;corba;data structures distributed object management cache storage;clients;design and implementation;hash table;data structures;distributed object system;distributed object management;performance gains cache management distributed object systems corba based systems removal algorithm double linked structure hash table eviction;algorithms;distributed systems;cache management;design optimization frequency microprocessors algorithm design and analysis performance gain intelligent networks bandwidth degradation network servers databases	For many distributed data intensive applications, the default remote invocation of CORBA objects by clients is not acceptable because of performance degradation. Caching enables clients to invoke operations locally on distributed objects instead of fetching them from remote servers. This paper addresses the design and implementation of a specific caching approach for CORBA-based systems. We propose a new removal algorithm which uses a double linked structure and a hash table for eviction. We also present a new variation of optimistic two phase locking for consistency control, which does not require a lock at the client side by using a perprocess caching design. With the experiments we have performed, we demonstrate that the proposed caching approach provides an important performance gain: the caching with half buffer saves up to 45% of access time and the caching with full buffer saves up to 50% of access time. The Common Object Request Broker Architecture (CORBA) provides several advantages over existing communication protocols (like remote procedure call or sockets) such as location, operating system and programming language transparencies. However the core of CORBA, the Object Request Broker (ORB), which is responsible for providing such transparency, has problems like messaging overhead and over-use of networking bandwidth. By default, a CORBA client application performs a remote invocation for every request. For many data intensive distributed applications, this default remote invocation of CORBA objects by clients is not acceptable as it would result in performance degradation. Caching enables clients to invoke operations locally on distributed objects instead of fetching them from remote servers. Although caching has been substantially addressed in several areas (e.g. microprocessors, database systems [3, 4, 8], network file server [2, 7], web systems [9]), very little work has been done in CORBA environments. CORBA systems have specific requirements for object caching, including functional requirements related to architecture, model considerations (distributed objects are semantic entities containing data and operations), and non-functional requirements (scalability, availability, and reliability). Two issues need to be considered when designing a caching approach: (i) the design of an eviction technique (when the objects to be removed whenever the cache is full), and (ii) the design of a consistency technique (which will keep objects in the cache consistent with the server objects). This paper provides solutions to these two issues in the context of CORBA environments. An extension of the LRU algorithm, called enhanced LRU (LRU+), is proposed and is aimed at minimising the overall overhead by distribution of sorting process. This paper also presents a variation of optimistic two phase locking (O2PL) for consistency control, which does not require a lock at the client side by applying a per-process caching design. With the experiments we have performed in Orbix, we demonstrate that the proposed CORBA caching approach provides an important performance gain: caching with half buffer saves up to 45% of access time and the caching with full buffer saves up to 50% of access time. The paper is organised as follows. The next section describes some of the existing approaches that dealing with the issues of cache replacement and data consistency and applies our approach in context. Section 2 discusses the proposed caching architecture, and Section 3 describes a suitable caching approach for CORBA environ-	access time;algorithm;cache (computing);client (computing);client-side;common object request broker architecture;consistency model;data-intensive computing;database;distributed computing;doubly linked list;elegant degradation;entity;experiment;file server;functional requirement;hash table;hoare logic;lock (computer science);microprocessor;non-functional requirement;operating system;orbix (software);overhead (computing);programming language;remote procedure call;scalability;server (computing);sorting;subroutine;tao;transparency (projection);two-phase locking	Zahir Tari;Herry Hamidjaja;Qitang Lin	2000	IEEE Concurrency	10.1109/4434.865893	client;hash table;cache-oblivious algorithm;parallel computing;data structure;cache;computer science;cache invalidation;common object request broker architecture;database;distributed computing;distributed object;smart cache;programming language;cache algorithms	OS	-21.607964462579	49.502451360599004	120354
5828cb8f31fcfb2f94b5cd5f95c2a351ec57cb36	planning and implementing a smart city in taiwan	distributed systems smart city internet of things cloud computing big data information management;smart cities mobile communication streaming media broadband communication real time systems cloud computing;smart cities;internet of things;smart city cloud computing open source virtualization solution cht virtuoso cloud virtualization resource management system chunghwa telecom business development government strategy security healthcare education environmental protection disaster management energy savings transportation ict taiwan;big data;streaming media;information management;mobile communication;distributed systems;smart city;broadband communication;virtualisation cloud computing public domain software smart cities town and country planning;cloud computing;real time systems	"""The emerging smart city concept is a means of utilizing ICT to improve quality of life while optimizing city operations. Many issues are involved, including transportation, energy savings, disaster management, environmental protection, education, healthcare, and security. Currently, the government of Taiwan is cooperating with telephone companies and industries to implement the smart city program. Major telecommunication companies have actively responded to the government strategy and made it a principal of their business development. The program proposed by Chunghwa Telecom aims to assist central and local governments in achieving smart cities by the end of 2017. The program includes the """"convenient city,"""" """"happy city,"""" and """"friendly city,"""" initiatives, which comprise three programs and 12 subprograms. Because of smart cities' diverse demands, a powerful and effective platform is necessary. This article describes a cloud virtualization resource management system called CHT Virtuoso. Based on an open source virtualization solution, it is a cost-effective and well-designed system that fully utilizes the characteristics of cloud computing."""	cloud computing;open-source software;smart city;subroutine;x86 virtualization	Chiung-I Chang;Chih-Cheng Lo	2016	IT Professional	10.1109/MITP.2016.67	simulation;big data;mobile telephony;cloud computing;computer science;operating system;information management;world wide web;computer security;internet of things	HCI	-32.40662662066017	57.11623243364396	120365
06266f7791594ebeca0b85620ea8914e0b7a6c17	maintaining object ordering in a shared p2p storage environment	high availability;cryptographic technology;network design;object ordering;storage system;system recovery cryptography peer to peer computing storage management;memory management;persistency;storage management;versioning peer to peer storage dht persistency continuous update;p2p;denial of service attack;system recovery;cryptography;active decentralized object replication;peer to peer computing availability read only memory sun large scale systems object detection cryptography information retrieval file systems maintenance;peer to peer storage;file sharing;peer to peer computing;versioning;continuous update system;peer to peer;continuous update;dht;shared peer to peer storage system;system recovery shared peer to peer storage system data persistence characteristics continuous update system active decentralized object replication cryptographic technology object ordering dht;communication service;data persistence characteristics	To be considered a viable storage solution, modern peer-to-peer (P2P) storage systems must exhibit high availability and data persistence characteristics. In an attempt to provide these, most systems assume a continuously connected and available underlying communication infrastructure. This however is not warranted in any real large-scale distributed system, and thus needs to be addressed. Continuous update systems that allow updating data by multiple writers have harder problems to overcome since the ordering of updates needs to be maintained independently of connectivity conditions. In this paper we propose a solution for maintaining a global view of the ordering even when severe connectivity disruptions take place, allowing the system to continue functioning while connectivity is disrupted and to recover from the disruption smoothly when connectivity is restored. To this end, we introduce and discuss three new concepts to the realm of P2P storage systems: 1) the maintenance of additional state information to detect and trace object updates during partitioning, 2) the usage of active decentralized object replication through shadow roots, and 3) the deployment of cryptographic technologies to allow for the recovery of private state information	cryptography;denial-of-service attack;distributed computing;high availability;peer-to-peer;persistence (computer science);persistent data structure;smoothing;software deployment	Germano Caronni;Raphael Rom;Glenn Scott	2005	Third IEEE International Security in Storage Workshop (SISW'05)	10.1109/SISW.2005.8	real-time computing;computer science;database;distributed computing	HPC	-23.380697970244757	51.28548762588153	120453
55762a82285ee2b135cf8dbf9f5bd898bcc62a83	campus cloud for data storage and sharing	data sharing;portals;extensible data access service;storage management client server systems educational technology;campus network cloud storage data sharing;storage management;client server systems;storage resources;system performance;data storage;extensible data access service campus cloud personal data storage communities data sharing campus network cloud storage storage resources easy to use featured client tools;communities data sharing;personal data storage;campus cloud;data access;distributed databases;easy to use featured client tools;driver circuits;communities;educational technology;cloud storage;meteorology;campus network;memory cloud computing laboratories employment resource management portals computer networks grid computing computer science information science;memory;throughput	We address the challenge of personal data storage and communities data sharing in campus network with cloud storage consisting of a federation of storage resources. In this paper we present the design, architecture, functionality and deployment of our first campus cloud prototype. Easy-to-use featured client tools and extensible data access service make our system both user-friendly and powerful. The practical results show that our system performs well and brings students a lot of benefits.	cloud computing;cloud storage;computer data storage;data access;data storage tag;personally identifiable information;prototype;scheduling (computing);software deployment;usability	Pengzhi Xu;Xiaomeng Huang;Yongwei Wu;Likun Liu;Weimin Zheng	2009	2009 Eighth International Conference on Grid and Cooperative Computing	10.1109/GCC.2009.18	data access;throughput;educational technology;computer science;operating system;computer data storage;database;internet privacy;memory;information repository;world wide web;computer security;computer network	HPC	-33.159275851849614	54.87974721988307	120555
d3a734393f6e10e9c0786c55bd0569d7ccd109b1	a ws-agreement-based qos auditor negotiation mechanism for grids	grid computing ws agreement based qos auditor negotiation mechanism peer to peer system quality of service service level agreements auditor negotiation process;standards;computer architecture;monitoring;auditor negotiation;quality of service grid computing peer to peer computing;web services;service level agreement auditor negotiation grid computing;service level agreement;peer to peer computing;quality of service;grid computing;quality of service grid computing monitoring computer architecture web services standards	High performance platforms composed of commodity computing resources, such as grids and peer-to-peer systems, have greatly evolved and assumed an important role in the last decade. Nevertheless, their wide commercial use still depends on the establishment of an effective quality of service (QoS) infrastructure in those environments. For this reason, a variety of proposals have recently emerged in which consumer and provider monitor and control grid resources in order to guarantee previously established service level agreements. However, in many cases there is lack of trust between provider and consumer in relation to monitoring those agreements. In such cases, it becomes necessary to introduce a third entity - an impartial and trustworthy QoS auditor - in order to solve conflicts of interest. Though, as there may be several auditors trusted by provider and consumer, we claim that the QoS auditor needs to be negotiated and established just as the service level agreement is negotiated by the parties. In order to support this issue, the present paper proposes and evaluates a negotiation mechanism for QoS auditors in computational grids. Some of the proposed mechanism's characteristics are low intrusiveness and use of open standards, such as the WS-Agreement. Experimental analysis on a prototype of the proposed negotiation mechanism have shown that the auditor negotiation process took less than a minute to finish, which is far less than the service execution time in most grid computing use cases.	cloud computing;commodity computing;data flow diagram;downtime;grid computing;peer-to-peer;prototype;quality of service;run time (program lifecycle phase);service-level agreement;software deployment;web service	Alisson Andrade;Alba Cristina Magalhaes Alves de Melo	2011	2011 IEEE/ACM 12th International Conference on Grid Computing	10.1109/Grid.2011.10	web service;service level requirement;service level objective;quality of service;computer science;database;utility computing;world wide web;computer security;grid computing	HPC	-26.174725797636626	58.90380030594691	120844
9dd5efde2d82b4e798aba76f7b00ccdb68590aa7	protocols of accessing overlapping sets of resources				Józef Winkowski	1981	Inf. Process. Lett.	10.1016/0020-0190(81)90023-5	deadlock;theoretical computer science;deadlock prevention algorithms;computer science;distributed computing	DB	-28.25100202489843	46.99932239939628	120941
0fac428b6b0b5aba2a98d94f417261320ba749aa	an adaptable workflow system architecture on the internet for electronic commerce applications	high availability;hypermedia markup languages;electronic commerce;electronic marketplace adaptable workflow system architecture internet electronic commerce applications business process workflow technology workflow engine multiple servers dynamic modification componentwise architecture corba 2 xml network transportable applets java end user workflow components workflow domain manager availability the component server repository;internet electronic commerce search engines web server xml java availability network servers buildings consumer electronics;client server systems;adaptive workflow;electronic marketplace;workflow system;java workflow management software electronic commerce internet distributed object management hypermedia markup languages client server systems;internet;distributed object management;workflow management software;system architecture;java	An electronic commerce (EC) process is a business process and defining it as a workflow provides all the advantages that come with this technology. Yet electronic commerce processes place certain demands on the workflow technology like the distribution of the load of the workflow engine to multiple servers, dynamic modification of workflows for adaptability, openness and availability. In this paper we propose a workflow system architecture to address these issues. The componentwise architecture of the system makes it possible to incorporate the functionality and thus the complexity only when it is actually needed. The infrastructure of the system is based on CORBA 2.0 where methods are invoked through XML. The clients of the system are coded as network transportable applets written in Java so that the end user can activate workflow components through the workflow domain manager over the network. The system provides high availability by replicating the component server repository and the workflow domain manager. We also discuss how this architecture can be used in building an electronic marketplace.	applet;business process;common object request broker architecture;e-commerce;high availability;internet;java;openness;requirement;scalability;self-replicating machine;server (computing);systems architecture;workflow engine;xml	Ibrahim Cingil;Asuman Dogac;Nesime Tatbul;Sena Nural Arpinar	1999		10.1109/DOA.1999.794038	workflow;xpdl;computer science;operating system;workflow management coalition;database;windows workflow foundation;world wide web;workflow management system;workflow engine;workflow technology	DB	-33.178552621821886	48.74998633153498	120988
7e8657c5228dd6c312221f0085cc6b8bc77c0aac	monitoring large-scale location-based information systems	filtering;massive multiplayer online games;large scale location based information systems multiplayer games virtual worlds monitoring setup location changes state changes map objects geographic map nonintrusive monitoring middleware human observer large scale systems distributed information distributed virtual world;monitoring games servers computer architecture bandwidth real time systems observers;virtual environments;load balancing monitoring virtual worlds virtual environments mmo mmog massive multiplayer online games distributed systems scalability data aggregation filtering;monitoring;virtual reality computer games middleware;data aggregation;mmo;load balancing;scalability;distributed systems;mmog;virtual worlds	Monitoring the state of a distributed virtual world is challenging for several reasons: 1) the distributed information must be gathered in real-time without affecting the performance of the information system, 2) in large-scale systems it is impossible for a single node to collect and process all the data, 3) the vast information must be filtered and aggregated according to what the human observer wants to focus on, and 4) the point of interest of the observer can change frequently. In this paper we present and evaluate a non-intrusive monitoring middleware that addresses these challenges by dynamically partitioning the geographic map (e.g., of the virtual world or the game) in terms of map objects and (expected) state changes. We assign a different collector node to each of these partitions to collect and pre-process the data, and forward it to a central monitoring node. Furthermore, we provide mechanisms to efficiently filter and aggregate location changes, the pre-dominant changes in location-based information systems. We describe a specific monitoring setup that takes advantage of the replication model that is common in many virtual worlds and multiplayer games to collect the data. Finally, we present extensive performance results that show the trade-offs between scalability, precision, and real-time performance.	aggregate data;data aggregation;distributed computing;experiment;game engine;information system;load balancing (computing);middleware;point of interest;preprocessor;real-time clock;real-time locating system;replication (computing);requirement;scalability;virtual world	Hammad Khan;Julien Gascon-Samson;Jörg Kienzle;Bettina Kemme	2015	2015 IEEE International Parallel and Distributed Processing Symposium	10.1109/IPDPS.2015.91	filter;data aggregator;real-time computing;scalability;simulation;computer science;load balancing;operating system;distributed computing	Embedded	-25.600279291736538	51.779135147717405	121043
11c54572a97270198c18878bee5163877178e8f3	converged infrastructure: addressing the efficiency challenge	virtualization;storage management;cloud computing converged infrastructure systems efficiency challenge it infrastructure datacenter product packages centralized management tool;storage management business data processing cloud computing computer centres;virtualization converged infrastructure computing storage networking;computer centres;computing storage;business data processing;networking;converged infrastructure;cloud computing	Vendors are combining computing, storage, and networking systems into one integrated IT infrastructure, promising more efficiency and flexibility than traditional silo-based approaches. There are three things that datacenter and cloud-system operators most look for in their technology: efficiency, efficiency, and efficiency. Many companies work with large amounts of bursty data, which places growing demands on their computing, storage, and networking infrastructures, which adds complexity and cost to their operations. Vendors are taking a radical approach by releasing product packages that offer customers a converged infrastructure, with a preconfigured, scalable pool of computing, storage, and networking resources run by a centralized management tool. The rise of cloud computing- which requires a flexible pool of resources-is a major force driving demand for and development of converged-infrastructure systems. However, the new approach may not appeal to all users and must still overcome several obstacles before being widely adopted.	centralized computing;cloud computing;data center;scalability;silo;sysop	Lee Garber	2012	Computer	10.1109/MC.2012.261	virtualization;converged storage;cloud computing;computer science;operating system;distributed computing;utility computing;converged infrastructure;computer security;computer network	HPC	-28.017931947430334	58.16965682943721	121066
0a614caf973151e546846434842aedd69de32def	network latency adaptive tempo in the public sound objects system	music performance;real time;latency tolerance;musical instruments;computer network;long distance;computer music;latency in real time performance;communication delay;dynamic adaptation;article;collaborative remote music performance;network music instruments;interface decoupled electronic musical instruments;behavioral driven interfaces	In recent years Computer Network-Music has increasingly captured the attention of the Computer Music Community. With the advent of Internet communication, geographical displacement amongst the participants of a computer mediated music performance achieved world wide extension. However, when established over long distance networks, this form of musical communication has a fundamental problem: network latency (or net-delay) is an impediment for real-time collaboration. From a recent study, carried out by the authors, a relation between network latency tolerance and Music Tempo was established. This result emerged from an experiment, in which simulated network latency conditions were applied to the performance of different musicians playing jazz standard tunes. The Public Sound Objects (PSOs) project is web-based shared musical space, which has been an experimental framework to implement and test different approaches for on-line music communication. This paper describe features implemented in the latest version of the PSOs system, including the notion of a network-music instrument incorporating latency as a software function, by dynamically adapting its tempo to the communication delay measured in real-time.	displacement mapping;internet;interrupt latency;online and offline;real-time clock;real-time transcription;web application	Álvaro Barbosa;Jorge C. S. Cardoso;Günter Geiger	2005			real-time computing;simulation;acoustics;human–computer interaction;computer science;artificial intelligence;operating system;multimedia;computer music;programming language	ML	-25.911828590929982	52.377153214172885	121320
cab5082f1b5783340437a785b281339d13d04832	overhaul: extending http to combat flash crowds	normal operator;web service	The increasing use of the web for serving http content, for database transactions, etc. can place heavy stress on servers. Flash crowds can occur at a server when there is a burst of a large number of clients attempting to access the service, and an unprepared server site could be overloaded or become unavailable. This paper discusses an extension to the http protocol that allows graceful performance at web servers under flash crowds. We implement our modifications for the Apache web server, and call the new system as Overhaul. In Overhaul mode, a server copes with a stampede by offloading file transfer duties to the clients. Overhaul enables servers to chunk each requested document into small sections and distribute these partial documents to clients. The clients then share the sections amongst themselves to form a complete document. Overhaul enables a web server to remain responsive to further requests from other clients and at the same time helps conserve the amount of bandwidth utilized by a flash crowd. We present detailed experimental results comparing the benefits of using Overhaul under flash crowds and under normal operating situations. Although we restrict our studies to static content, the Overhaul architecture is applicable to improving web services in general.	chunking (computing);database transaction;file transfer;hypertext transfer protocol;server (computing);slashdot effect;web server;web service	Jay A. Patel;Indranil Gupta	2004		10.1007/978-3-540-30471-5_3		OS	-26.302492789448948	53.95464852268814	121514
282f9695d0f9821d2993d34d87cabdf0b0a40637	deviceless edge computing: extending serverless computing to the edge of the network	function as a service;serverless computing;edge computing	The serverless paradigm has been rapidly adopted by developers of cloud-native applications, mainly because it relieves them from the burden of provisioning, scaling and operating the underlying infrastructure. In this paper, we propose a novel computing paradigm - Deviceless Edge Computing that extends the serverless paradigm to the edge of the network, enabling IoT and Edge devices to be seamlessly integrated as application execution infrastructure. We also discuss open challenges to realize Deviceless Edge Computing, based on our experience in prototyping a deviceless platform.	edge computing;image scaling;machine code;peer-to-peer;programming paradigm;provisioning;serverless computing	Alex Glikson;Stefan Nastic;Schahram Dustdar	2017		10.1145/3078468.3078497	distributed computing	HPC	-29.202424670417408	56.83710845244039	121537
91ebdf9a0ce548d531fd09553d8d035abc19ca64	unified service platform for accessing grid resources	grid resources;gt4;web services;wsrf	Web Services Resource Framework (WSRF) redefines Grid Services standards and extends Web Services by adding stateful resources. Using GT4 to develop WSRF Grid Services is a taxing work, and it is difficult to build and deploy these services dynamically. Addressing these issues, this paper proposes a unified service platform which can provide a series of unified service interfaces for accessing kinds of different Grid resources. On the platform, Grid resources are independent of the service interfaces. The platform provides unified service interfaces to access different Grid resources on server, so Grid services developers only pay attentions to realizing the native methods of Grid resources and configuring necessary resource database. The remainder work of composing typical Grid Services such as mapping the resources into the service interfaces would be automatically finished by the platform. It means that Grid Services development becomes native application development. What is more, there are no needs to restart the service container when deploying/undeploying Grid resources, so it does not affect other resources. The platform provides service-users with two types of clients, one is for directly invoking the unified interfaces and the other is a proxy client associating to the specific Grid resource. Finally, the test shows that the service development and deployment is much easier on this platform and the service performs well. Index Terms WSRF, GT4, Grid resources, Web Services	definition;native (computing);proxy server;server (computing);software deployment;state (computer science);web services resource framework;web service	Shaochong Feng;Yuanchang Zhu;Yanqiang Di	2011	JSW		web service;semantic grid;computer science;database;distributed computing;world wide web;grid computing	HPC	-31.551606888779133	52.8509843280042	121899
2b04fd82e1069193780dcb50645316a10da0db9a	adock: a cloud infrastructure experimentation environment based on open stack and docker	server consolidation;openstack;experimentation environments infrastructure as a service openstack server consolidation;infrastructure as a service;experimentation environments;virtual machines cloud computing;server consolidation algorithm adock cloud infrastructure experimentation environment functional cloud installation open stack virtual machine placement algorithm;servers containers measurement computational modeling hardware computer architecture databases	A common barrier to experimenting with cloud infrastructure is the lack of access to a fully functional cloud installation. Although Open Stack can be used to create test beds, it is not uncommon in literature to find works that are plagued by unrealistic setups that use only a handful of servers. Moreover, setting up a test bed is necessary but not sufficient. One must also be able to create repeatable experiments that can be used to compare one's results to baseline or related approaches from the state of the art. We present a Dock, a suite of tools for creating perform ant, sand boxed, and configurable cloud infrastructure experimentation environments based on Open Stack and Docker. Its light-weight approach facilitates the development of Open Stack extensions, allowing one to more easily test them in realistic scenarios. A dock also provides a set of simulation tools that, for now, focus on creating experiments for Virtual Machine Placement and Server Consolidation algorithms. In this paper we also show how a Dock was used to develop a Server Consolidation Service for Nova, and provide an analysis of a Dock's scalability and simulation capabilities.	algorithm;baseline (configuration management);blueprint;cloud computing;docker;experiment;linear algebra;list of code lyoko episodes;sandbox (computer security);scalability;semiconductor consolidation;simulation;testbed;virtual machine	Lorenzo Affetti;Giacomo Bresciani;Sam Guinea	2015	2015 IEEE 8th International Conference on Cloud Computing	10.1109/CLOUD.2015.36	embedded system;cloud computing;computer science;operating system;world wide web	SE	-29.867010588944026	55.03765990686019	121906
68aae419d7f6b35d454dc37ec1dbbf3e1221b6fd	distributed computing paradigms for collaborative processing in sensor networks	groupware;energy efficient;groupware wireless sensor networks client server systems mobile agents;mobile agents;computer model;distributed processing;distributed computing;client server systems;distributed computing collaboration mobile agents computational modeling distributed processing mobile computing energy efficiency delay energy consumption energy measurement;energy efficiency distributed computing paradigms collaborative processing sensor networks multiple sensor nodes client server based paradigm mobile agent based paradigm;sensor network;client server;low latency;energy consumption;sensor nodes;mobile agent;simulation tool;wireless sensor networks	In sensor networks, collaborative processing between multiple sensor nodes is essential in order to complement for each other’s sensing capability, tolerate faults, and provide reliable information. The client/server-based paradigm is typical for distributed processing. However, it is not the most efficient in the context of sensor networks. In this paper, we present a mobileagent-based paradigm to carry out collaborative processing, where instead of each sensor node sending local information to a processing center, as is typical in the client/server-based computing, the processing code is moved to the sensor nodes through mobile agents. This approach has great potential in providing energy-efficient and scalable collaborative processing with low latency. We design two metrics (execution time and energy consumption) and use simulation tools to quantitatively measure the performance of different computing models in collaborative processing. Experimental results show that the mobile agent paradigm performs much better when the number of nodes is large while the client/server paradigm is advantageous when the number of nodes is small. Based on this result, we develop a cluster-based hybrid computing paradigm to combine the advantages of both paradigms. We analyze two different scenarios in hybrid computing and simulation results show that there is always one scenario that performs better than either the client/serveror mobile-agent-based paradigm.	agent-based model;client–server model;cluster analysis;distributed computing;failure rate;information processing;mobile agent;programming paradigm;run time (program lifecycle phase);scalability;sensor node;server (computing);simulation;software agent	Yingyue Xu;Hairong Qi;Phani Teja Kuruganti	2003		10.1109/GLOCOM.2003.1258891	computer simulation;real-time computing;wireless sensor network;computer science;operating system;distributed computing;computer network	Mobile	-25.843714945880443	53.18764830988941	121963
0d76c6076997e6e1c4382e95ce051a15b77f8aab	a new operational transformation framework for real-time group editors	groupware;convergence;control group;collaborative work;real time;consistency control;collaboration;sufficient conditions;computer networks;computer network;multimedia computing concurrency control groupware;multimedia computing;operational transformation consistency control group editors groupware;optimistic consistency control operational transformation framework real time group editor distributed interactive groupware application shared multimedia document computer network;replicated data;concurrency control;group editors;humans;algorithm design and analysis collaborative software collaborative work sufficient conditions humans computer networks convergence optimization methods delay collaboration;operational transformation;algorithm design and analysis;collaborative software;optimization methods	Group editors allow a group of distributed human users to edit a shared multimedia document at the same time over a computer network. Consistency control in this environment must not only guarantee convergence of replicated data, but also attempt to preserve intentions of operations. Operational transformation (OT) is a well-established method for optimistic consistency control in this context and has drawn continuing research attention since 1989. However, counterexamples to previous works have often been identified despite the significant progress made on this topic over the past 15 years. This paper analyzes the root of correctness problems in OT and establishes a novel operational transformation framework for developing OT algorithms and proving their correctness	algorithm;control theory;correctness (computer science);operational transformation;real-time transcription	Rui Li;Du Li	2007	IEEE Transactions on Parallel and Distributed Systems	10.1109/TPDS.2007.35	parallel computing;real-time computing;computer science;theoretical computer science;operating system;database;distributed computing;programming language;computer security;algorithm;collaborative software;computer network	DB	-25.63967069858597	47.42691925778182	122064
7c5aaa2792943f301b164fffa7cafb8838125205	automatic performance diagnosis for changing workloads in dbmss	data mining database management systems resource allocation transaction processing;database management systems;resource allocation;on line transaction processing;generic online transaction processing automatic performance diagnosis workload change database management system dbms resource allocation;data mining;cost of ownership;transaction processing;database management system;performance tuning;resource management tuning databases memory management costs automation application software automatic control monitoring buffer storage	Database performance is directly linked to database management system (DBMS) resource allocation. Complex relationships between DBMS resources make problem diagnosis and performance tuning difficult, time-consuming tasks. Database administrators are currently required to retune the DBMS as databases grow and workloads change. Performance can be increased and cost of ownership reduced by automating the tuning process, starting specifically with the diagnosis of resource allocation problems. In this paper, we overview our automatic diagnosis framework designed to determine resource problems. We present our results demonstrating the ability to correctly identify system bottlenecks for a generic on-line transaction processing workload when new transactions are added to the workload.	bottleneck (software);online and offline;online transaction processing;performance tuning;relational database management system;total cost of ownership	Darcy G. Benoit	2006	20th International Conference on Advanced Information Networking and Applications - Volume 1 (AINA'06)	10.1109/AINA.2006.111	real-time computing;database transaction;transaction processing;database tuning;distributed transaction;resource allocation;computer science;data mining;database;online transaction processing;transaction processing system	DB	-20.62656457463031	56.723906688758106	122246
63b5bef1ccbe3d9aa6d7bfd46f3819b4b6c21b6d	adaptive management of applications across multiple clouds: the seaclouds approach		How to deploy and manage, in an efficient and adaptive way, complex applications across multiple heterogeneous cloud platforms is one of the problems that have emerged with the cloud revolution. In this paper we present context, motivations and objectives of the EU research project SeaClouds, which aims at enabling a seamless adaptive multi-cloud management of complex applications by supporting the distribution, monitoring and migration of application modules over multiple heterogeneous cloud platforms. After positioning SeaClouds with respect to related cloud initiatives, we present the SeaClouds architecture and discuss some of its aspect, such as the use of the OASIS standard TOSCA and the compatibility with the OASIS CAMP initiative.	cloud management;interoperability;load balancing (computing);oasis tosca;open-source software;orchestration (computing);platform as a service;quality of service;requirement;seamless3d;software deployment;throughput	Antonio Brogi;José Carrasco;Javier Cubo;Elisabetta Di Nitto;Francisco Durán;Michela Fazzolari;Ahmad Ibrahim;Ernesto Pimentel;Jacopo Soldani;PengWei Wang;Francesco D'Andria	2015	CLEI Electron. J.		simulation;management science;operations research	HPC	-30.601670710789367	56.6219992941553	122526
001adf32860c651544767f3977b3c641fa1c015c	marco: a middleware architecture for distributed multimedia collaboration	performance measure;groupware;real time;distributed multimedia;data scalability marco middleware architecture distributed multimedia collaboration collaborative visualization system real time rendering dataset management data location transparency heterogeneous node management data replication policy remote visualization collaboration data availability;data replication;4d rendering middleware remote collaboration data grid heterogeneous devices;multimedia computing;data visualisation;real time systems middleware multimedia computing groupware data visualisation;remote visualization;middleware;4d rendering;heterogeneous devices;middleware collaboration collaborative tools data visualization computer architecture real time systems biomedical imaging application software rendering computer graphics navigation;data grid;remote collaboration;real time rendering;real time systems	A distributed, real time, collaborative visualization (DRCV) system for four dimensional datasets like fMRI images, can be a valuable tool to support scientific and medical research. Software applications supporting DRCV are lagging due to complex challenges, such as real time rendering, coordinated, real time navigation, dataset management, data location transparency, heterogeneous node management, data replication policies, and so on. In addition, people using DRCV tools are tightened to computing devices with specific configuration, which lower opportunities for remote visualization collaborations. We present a novel middleware to facilitate development of DRCV applications which guarantee high data availability and scalability in terns of collaborative groups and members, and are adaptable to computing devices with varying display capabilities. We also show our prototype implementation of this middleware with performance measurements.	algorithm;experiment;fault tolerance;middleware;mobile agent;overhead (computing);prototype;real-time clock;replication (computing);responsiveness;scalability	Chia-Yen Shih;Jie Hu;Jinhwan Lee;Raymond Klefstad;Douglas Tolbert	2005	Seventh IEEE International Symposium on Multimedia (ISM'05)	10.1109/ISM.2005.76	middleware;real-time computing;computer science;operating system;middleware;data grid;database;real-time rendering;world wide web;data visualization;replication	HPC	-31.223253870386653	46.92912092344643	122568
bcf0507b68f759122d9578409e6e6548f70b1547	dockerization impacts in database performance benchmarking.		Docker seems to be an attractive solution for cloud database benchmarking as it simplifies the setup process through pre-built images that are portable and simple to maintain. However, the usage of Docker for benchmarking is only valid if there is no effect on measurement results. Existing work has so far only focused on the performance overheads that Docker directly induces for specific applications. In this paper, we have studied indirect effects of dockerization on the results of database benchmarking. Among others, our results clearly show that containerization has a measurable and non-constant influence on measurement results and should, hence, only be used after careful analysis. Dockerization Impacts in Database Performance Benchmarking Martin Grambow, Jonathan Hasenburg, Tobias Pfandzelter, David Bermbach TU Berlin & Einstein Center Digital Future, Mobile Cloud Computing Research Group mg,jh,tpz,db@mcc.tu-berlin.de		Martin Grambow;Jonathan Hasenburg;Tobias Pfandzelter;David Bermbach	2018	CoRR		real-time computing;database tuning;benchmarking;database;computer science;overhead (business);cloud database	Metrics	-24.348892249474158	58.29833580242206	123293
28f82394bdc5deb0ce3c470e49f63b55d3654f0c	system independent and distributed fault management system	telecommunication computing;correlators;event correlation;distributed programming telecommunication network management java correlators telecommunication computing distributed object management;distributed programming;distributed object management;next generation;system architecture;correlators network servers computer network management computer architecture filtering quality management resource management next generation networking technology management monitoring;java distributed fault management engine independent fault management system distributed fault management system network management systems extensible fault management computing framework intrusion detection systems business systems rmi;fault management;intrusion detection system;network management system;telecommunication network management;java	This paper will outline a distributed and dynamic fault management system and practice of it. This work shows that proposed platform-independent, distributed and reusable fault management system architecture can be an integral part of the next generation of network management systems. Another feature of the proposed fault management system is being an extensible fault management computing framework for researchers. By the proposed infrastructure, researcher can carry out their original work on this framework by the help of event, correlator and alarm programming interfaces. Proposed architecture is applicable not only to network management system, but also to intrusion detection systems, business systems and health care information correlation systems. We present this architecture and the use of advanced technologies such as RMI and Java. Finally, we demonstrate how these technological solutions have been implemented in the distributed fault management system called JADFAME—Java Distributed Fault Management Engine.	cross-correlation;floor and ceiling functions;intrusion detection system;java remote method invocation;management system;next-generation network;systems architecture	Ertugrul Akbas	2003		10.1109/ISCC.2003.1214302	intrusion detection system;network management;out-of-band management;embedded system;fcaps;element management system;real-time computing;systems management;network management station;computer science;event correlation;operating system;fault management;distributed computing;network management application;structure of management information;java;computer security;network monitoring;systems architecture	DB	-33.24920307115677	47.524598745440635	123300
e3e7b92e262a64f0807a25555ebf3fd4a78151f2	fully peer-to-peer virtual environments with 3d voronoi diagrams	applied voronoi diagrams;distributed virtual environments;massively multiplayer online games;peer to peer overlay networks	A fully peer-to-peer (P2P) virtual environment (VE) represents a unique challenge in the realm of extreme distributed systems. Resource scalability and system resiliency, hallmark requirements of the P2P principle, are further complicated with the need for secure and responsive game-play amongst the millions of players in a P2P-VE. Inter-peer communications need to be almost real-time in nature, to facilitate a smooth, immersive experience and provide consistency within the virtual world. Players will not tolerate slow connections and players who cheat by modifying or delaying inter-peer updates. 3D Voronoi diagrams are a natural extension to traditional 2D varieties and they serve as the foundation for our proposed approach. We augment our 3D-VD platform with a novel concept: the usual spatial coordinates for the 3rd dimension is substituted with a non-spatial metric. Instead of taking the Z-axis to literally mean above or below someone within the virtual map, we use it to signify a player’s current resource capabilities. This effectuates more fluid self-organisation amongst the millions of peers within the VE. Work-loads, consisting of resource intensive arbitration tasks, are handled dynamically at localised clusters. Thus, we have a strictly decentralised mechanism that addresses responsiveness and security concerns from the bottom up. Simulation results verify the feasibility and performance of our technique, with arbitration failure situations reduced significantly.	apache axis;computer cluster;distributed computing;internationalization and localization;peer-to-peer;real-time transcription;requirement;responsiveness;scalability;self-organization;simulation;top-down and bottom-up design;virtual reality;virtual world;voronoi diagram	Mahathir Almashor;Ibrahim Khalil	2012	Computing	10.1007/s00607-012-0197-9	simulation;computer science;theoretical computer science;distributed computing;algorithm	HPC	-27.334709247439203	49.72674922479629	123472
eae40de1459b9ea831262ffd65aaf47d5d93d20f	policy based self-management in distributed environments	xacml;self managing file storage;management system;self managing file storage policy based self management business goals distributed component management system niche policy manager group centralized policy decision making load balancing policy languages spl xacml;policy manager group;resource allocation;policy languages;prototypes;distributed processing;distributed system management;policy language;business goals;policy based self management;fault tolerant computing;engines;monitoring;distributed environment;xml distributed processing fault tolerant computing resource allocation;policy based management;xml;load balancing;distributed component management system;datavetenskap datalogi;on the fly;robustness;load balance;scalability;engines monitoring prototypes scalability decision making conferences robustness;computer science;niche;system management;spl;conferences;centralized policy decision making;policy management	Currently, increasing costs and escalating complexities are primary issues in the distributed system management. The policy based management is introduced to simplify the management and reduce the overhead, by setting up policies to govern system behaviors. Policies are sets of rules that govern the system behaviors and reflect the business goals or system management objectives. This paper presents a generic policy-based management framework which has been integrated into an existing distributed component management system, called Niche, that enables and supports self-management. In this framework, programmers can set up more than one Policy-Manager-Group to avoid centralized policy decision making which could become a performance bottleneck. Furthermore, the size of a Policy-Manager-Group, i.e. the number of Policy-Managers in the group, depends on their load, i.e. the number of requests per time unit. In order to achieve good load balancing, a policy request is delivered to one of the policy managers in the group randomly chosen on the fly. A prototype of the framework is presented and two generic policy languages (policy engines and corresponding APIs), namely SPL and XACML, are evaluated using a self-managing file storage application as a case study.	business logic;centralized computing;distributed computing;hard coding;high- and low-level;load balancing (computing);niche blogging;on the fly;overhead (computing);programmer;prototype;randomness;run time (program lifecycle phase);self-management (computer science);systems management;xacml	Lin Bao;Ahmad Al-Shishtawy;Vladimir Vlassov	2010	2010 Fourth IEEE International Conference on Self-Adaptive and Self-Organizing Systems Workshop	10.1109/SASOW.2010.72	computer science;knowledge management;load balancing;operating system;database;distributed computing	HPC	-30.000296100336836	52.64494208733641	123485
71e18ff5726b242d444b80f853ea25e852397ae7	connecting the dots: anomaly and discontinuity detection in large-scale systems		Cloud providers and data centers rely heavily on forecasts to accurately predict future workload. This information helps them in appropriate virtualization and cost-effective provisioning of the infrastructure. The accuracy of a forecast greatly depends upon the merit of performance data fed to the underlying algorithms. One of the fundamental problems faced by analysts in preparing data for use in forecasting is the timely identification of data discontinuities. A discontinuity is an abrupt change in a time-series pattern of a performance counter that persists but does not recur. Analysts need to identify discontinuities in performance data so that they can (a) remove the discontinuities from the data before building a forecast model and (b) retrain an existing forecast model on the performance data from the point in time where a discontinuity occurred. There exist several approaches and tools to help analysts identify anomalies in performance data. However, there exists no automated approach to assist data center operators in detecting discontinuities. In this paper, we present and evaluate our proposed approach to help data center analysts and cloud providers automatically detect discontinuities. A case study on the performance data obtained from a large cloud provider and performance tests conducted using an open source benchmark system show that our proposed approach provides on average precision of 84 % and recall 88 %. The approach does not require any domain knowledge to operate.	anomaly detection;reflections of signals on conducting lines	Haroon Malik;Ian J. Davis;Michael W. Godfrey;Douglas Neuse;Serge Mankovskii	2016	J. Ambient Intelligence and Humanized Computing	10.1007/s12652-016-0381-4	real-time computing;simulation;artificial intelligence;data mining	HPC	-24.381271096826694	59.02255229070235	123890
59a2d2aee7903c972de490e6552f5ca84ecdd374	a multi-criteria approach for large-object cloud storage		In the area of storage, various services and products are available from several providers. Each product possesses particular advantages of its own. For example, some systems are offered as cloud services, while others can be installed on premises, some store redundantly to achieve high reliability while others are less reliable but cheaper. In order to benefit from the offerings at a broader scale, e.g., to use specific features in some cases while trying to reduce costs in others, a federation is beneficial to use several storage tools with their individual virtues in parallel in applications. The major task of a federation in this context is to handle the heterogeneity of involved systems. This work focuses on storing large objects, i.e., storage systems for videos, database archives, virtual machine images etc. A metadata-based approach is proposed that uses the metadata associated with objects and containers as a fundamental concept to set up and manage a federation and to control storage locations. The overall goal is to relieve applications from the burden to find appropriate storage systems. Here a multi-criteria approach comes into play. We show how to extend the object storage developed by the VISION Cloud project to support federation of various storage systems in the discussed sense.	archive;associative entity;cloud computing;cloud storage;computer data storage;confidentiality;data store;federation (information technology);floor and ceiling functions;load balancing (computing);object storage;on-board data handling;on-premises software;requirement;shard (database architecture);systems architecture;virtual machine	Uwe Hohenstein;Michael C. Jäger;Spyridon V. Gogouvitis	2017		10.5220/0006432100750086	cloud storage;database;computer science	OS	-29.23457047354512	58.22958305640224	123923
e7f1e595f761ef150a592392fb85602f5baac88c	an openflow implementation for network processors	hardware protocols libraries coordinate measuring machines program processors pipelines;software defined networking multiprocessing systems protocols;multicore cpu openflow implementation network processors software defined networking technologies sdn technologies openflow control plane deployment network platforms fp7 alien project hardware abstraction layer openflow capabilities legacy network elements hal implementation programmable network platforms	OpenFlow is catalyzing the deployment of software defined networking (SDN) technologies around the globe. In practice, however, compatibility issues hinder the deployment of an OpenFlow control plane on a number of network platforms. The FP7 ALIEN project addresses this problem by introducing a Hardware Abstraction Layer (HAL) which enables OpenFlow capabilities on legacy network elements. This paper presents the implementation of a HAL on programmable network platforms with multi-core CPUs and summarizes the implementation experience gained in the process.	abstraction layer;central processing unit;control plane;hal;hardware abstraction;multi-core processor;network processor;openflow;software deployment;software-defined networking	Marc Suñé;Victor Alvarez;Tobias Jungel;Umar Toseef;Kostas Pentikousis	2014	2014 Third European Workshop on Software Defined Networks	10.1109/EWSDN.2014.17	openflow;parallel computing;computer science;operating system;software-defined networking;computer network	Networks	-28.185113016102573	56.677810879996905	124483
e4917603eb810a959d1b95a42d2cd4d27ac75d6c	mapreduce processing on iot clouds	computers;mobile agents;cloud;data processing;runtime;computational capabilities mapreduce processing iot clouds low band networks cloud computing;internet of things;internet of things cloud computing;data processing mobile agents computers runtime cloud computing java mobile communication;mobile code;mobile communication;mapreduce;mobile agent;mobile agent mapreduce data processing cloud mobile code;cloud computing;java	We present a data processing framework for IoTs with clouds. It was inspired by the MapReduce processing but aims at data processing at the edges through low-band networks. It enables data at the edges to be locally processed as much as possible by using the MapReduce processing and to be cooperated with cloud computing if the edges lacks computational capabilities. This paper describes the design and implementation of the framework, basic performance, and an practical application.	apache hadoop;cloud computing;computer;distributed computing;embedded system;mapreduce;mobile agent;scheduling (computing);thinning	Ichiro Satoh	2013	2013 IEEE 5th International Conference on Cloud Computing Technology and Science	10.1109/CloudCom.2013.49	parallel computing;real-time computing;data processing;cloud computing;computer science;operating system;distributed computing	DB	-29.12115460019665	56.7294514947891	124616
3b4844758a224cc593a960507754eb0b6cd7622e	issues in the design and use of a distributed file system	operating system;community networks;data access;distributed file system;data consistency;random access	This paper discusses an independent file facility, one that is not embedded in an operating system. The distributed file system (DFS) is so named because it is implemented on a cooperating set of server computers connected by a communications, network, which together create the illusion of a single, logical system for the creation, deletion, and random accessing of data. Access to the DFS can only be accomplished over the network; a computer (or, more precisely, a program running on one) that uses the DFS is called a client. This paper describes the division of responsibility between servers and clients. The basic tool for maintaining data consistency in these situations is the atomic property of transactions, which protects clients from system malfunctions and from the competing activities of other clients. Several cooperating clients may share a transaction. The DFS provides an unconventional locking mechanism between transactions that supports client caches and eliminates a novel form of deadly embrace. We have implemented and put into service a system based on these concepts.	client (computing);clustered file system;computer;dce distributed file system;deadlock;embedded system;file server;formal system;indivisible;linearizability;lock (computer science);operating system;parallel computing;server (computing)	Howard E. Sturgis;J. Mitchell;J. Israel	1980	Operating Systems Review	10.1145/850697.850705	self-certifying file system;data access;real-time computing;computer science;operating system;database;distributed computing;distributed file system;data consistency;computer security;random access	OS	-26.380793491117394	46.531772681915626	124710
ed435d793399b2c55db438bd30b8d392f4ca2333	drisp: a versatile scheme for distributed fault-tolerant queues	distributed system;high availability;protocols;performance evaluation;fault tolerant;queueing theory;queueing theory distributed processing fault tolerant computing performance evaluation protocols;distributed processing;storage space requirements replicated queue drisp versatile scheme distributed fault tolerant queues replications intelligent sequential probe protocols fifo order dynamic fault tolerance availability uniform load balancing;fault tolerant computing;communication cost;fault tolerance availability costs access protocols computer science fault tolerant systems probes spine heuristic algorithms load management;load balance	A versatile scheme for implementing fault-tolerant queues in a distributed system is proposed. Based on the combination of two simple concepts, by distributing replications of a queue and conducting intelligent-sequential probe (DRISP), the backbone of this work is unsophisticated, yet powerful. A description is presented of the protocols for handling a replicated and distributed queue, which is different from a replicated file in the sense that FIFO order should be maintained in a queue. The replicated distributed queue supported by these algorithms provides dynamic fault tolerance, high availability, and uniform load balancing with small storage space requirements and low communication cost. It is also adaptable to environment changes. Consistency is guaranteed. >	fault tolerance;queue (abstract data type)	Pen-Nan Lee;Yiwei Chen;J. M. Holdman	1991		10.1109/ICDCS.1991.148732	communications protocol;fault tolerance;parallel computing;real-time computing;computer science;load balancing;distributed computing;high availability;queueing theory;computer network	Theory	-21.48801819788019	51.18898872650181	124759
7117500e45f7489396f13e4104b493a568f10950	coterie availability in sites	independent and identically distributed;wide area network	In this paper, we explore new failure models for multi-site systems, which are systems characterized by a collection of sites spread across a wide area network, each site formed by a set of computing nodes running processes. In particular, we introduce two failure models that allow sites to fail, and we use them to derive coteries. We argue that these coteries have better availability than quorums formed by a majority of processes, which are known for having best availability when process failures are independent and identically distributed. To motivate introducing site failures explicitly into a failure model, we present availability data from a production multi-site system, showing that sites are frequently unavailable. We then discuss the implementability of our abstract models, showing possibilities for obtaining these models in practice. Finally, we present evaluation results from running an implementation of the Paxos algorithm on PlanetLab using different quorum constructions. The results show that our constructions have substantially better availability and response time compared to	algorithm;hierarchical database model;markov chain;markov model;paxos (computer science);planetlab;quorum (distributed computing);quorum sensing;response time (technology)	Flavio Paiva Junqueira;Keith Marzullo	2005		10.1007/11561927_3	independent and identically distributed random variables;computer science;statistics	OS	-21.659916825968914	46.60981227886509	124770
307552566c7817aeec0fb1a6b79183cdc3e9b170	ecsim: a simulation tool for performance evaluation of erasure coded storage systems	simulator for erasure coded storage;erasure codes;cloud storage;cloud computing	Simulation environments provide a comprehensive set of advantages to users, like cost effectiveness and capability to understand the shortcomings of the system under design, without physical implementation. Simulation platforms help the industry and academia, to document and publish their research outputs in a timely and cost efficient manner. The ECSim tool presented here, is meant for academic use in the initial stages of research. The platform provides an environment where the performance of erasure coded storage systems can be tested without much effort. The main highlight of the simulator is that it provides a very simple environment which can run on a standalone system. The environment does not require the user to be a programmer, since it provides an interactive command line interface to the user. The ability to simulate data center, clusters, master nodes, storage nodes with computing power, storage devices, bandwidth usage and disk I/O involved are notable features of ECSim.	application programming interface;cloud storage;command-line interface;cost efficiency;data center;deployment environment;erasure code;graphical user interface;image scaling;input/output;performance evaluation;programmer;simulation	Ojus Thomas Lee;S. D. Madhu Kumar;Priya Chandran	2016	2016 International Conference on Advances in Computing, Communications and Informatics (ICACCI)	10.1109/ICACCI.2016.7732471	erasure code;embedded system;real-time computing;simulation;cloud computing;computer science	HPC	-27.87820827356062	53.18914798468855	124783
84822045cebff3a5488a7b1e4b54b1960b00c2d4	an optimal speculative transactional replication protocol	distributed system;protocols;speculative transactional replication protocol;optimistic atomic broadcast service;concurrent computing;history;availability;speculative replication protocol;optimality property;replicated transactional system;transaction processing protocols;correctness property;data access;merging;joining processes;atomic broadcast;view serializability;history protocols data models merging availability concurrent computing joining processes;speculative processing;speculative replication protocol distributed system replicated transactional system speculative processing view serializability;transaction processing;data models;optimality property speculative transactional replication protocol replicated transactional system optimistic atomic broadcast service correctness property	In this paper we investigate the problem of speculative processing in a replicated transactional system layered on top of an optimistic atomic broadcast service. We consider a realistic model in which transactions' read/write sets are not known a-priori, and transactions' data access patterns may vary depending on the observed snapshot. We formalize a set of correctness and optimality properties aimed at ensuring that transactions are not activated on inconsistent snapshots, as well as the minimality and completeness of the set of explored serialization orders. Finally, an optimal speculative transaction replication protocol is presented.	atomic broadcast;computation;correctness (computer science);data access;encode;replication (computing);serialization;snapshot (computer storage);speculative execution;transaction processing	Paolo Romano;Roberto Palmieri;Francesco Quaglia;Nuno Carvalho;Luís E. T. Rodrigues	2010	International Symposium on Parallel and Distributed Processing with Applications	10.1109/ISPA.2010.94	data access;data modeling;communications protocol;availability;parallel computing;atomic broadcast;concurrent computing;transaction processing;computer science;database;distributed computing	Arch	-23.918589272017844	48.00972754493567	124951
9dc898a1c9049f9a8d4260e78aba4e2e5d512d5f	a new collaborative and cloud based simulation as a service platform: towards a multidisciplinary research simulation support	data sharing;multidisciplinary research simulation support;groupware;research publication;computer platform;code sharing;parallel processing cloud computing digital simulation grid computing groupware;distributed platform;collaboration;research dissemination method;computational modeling collaboration solid modeling parallel processing data models adaptation models servers;cloud based simulation as a service platform;simulation software;servers;computational modeling;collaborative simulation;distributed platform simulation platform collaboration cloud computing;nonfunctional performance;solid modeling;simulation script;simulation platform;crowd testing;adaptation models;online testing;grid computing;nonfunctional performance collaborative simulation cloud based simulation as a service platform multidisciplinary research simulation support scientific findings validation computer platform research dissemination method simulation code simulation script simulation software online testing crowd testing runmycode code sharing data sharing research publication parallel processing grid based platform;grid based platform;simulation code;scientific findings validation;parallel processing;digital simulation;runmycode;cloud computing;data models	"""Nowadays, the simulation becomes an essential part of the scientific findings validation and experimentation. It becomes crucial to researchers to launch simulations in order to evaluate and test their models and algorithms on computer platforms. However, research dissemination methods suffer from a major lack as they do not allow publishing simulation code and scripts along with the published papers. Furthermore, conventional simulation software solutions have solely been designed to allow online and crowd testing of simulation codes and scripts. This paper demonstrates our on-going development project of a Simulation as a Service platform called """"RunMyCode"""". RunMyCode enables scientists to openly share the underlying code and data within their research publications. Moreover, relying on the powerful parallel processing feature of the grid-based platform, scientists can launch effortlessly and simultaneously a collection of simulations codes and scripts using the same set of input data and the same - or different - running environments. The outputs of these simulations are then and displayed in the same view enabling the user to compare the functional and non-functional performance of the different selected simulation codes."""	algorithm;backward compatibility;code;configuration management;crowdsourced testing;mathematical optimization;parallel computing;platform as a service;sandbox (computer security);simulation software;software deployment;version control	Layth Sliman;Benoit Charroux;Yvan Stroppa	2013	2013 UKSim 15th International Conference on Computer Modelling and Simulation	10.1109/UKSim.2013.108	simulation;computer science;distributed computing;world wide web	HPC	-28.568503596407066	52.91025365250895	125159
08857be26db0f262a3d510dd01a885176efd12f8	a transaction model for management of replicated data with multiple consistency levels	databases;silicon;protocols;multilevel data consistency model transaction model replicated data management multiple data consistency levels serializable transactions causal snapshot isolation csi with commutative updates csi with asynchronous updates e commerce application;computational modeling;transaction processing data integrity electronic commerce;scalability;adaptation models;data models adaptation models silicon protocols computational modeling scalability databases;data models	We present a transaction model which simultaneously supports different consistency levels, which include serializable transactions for strong consistency, and weaker consistency models such as causal snapshot isolation (CSI), CSI with commutative updates, and CSI with asynchronous updates. This model is useful in managing large-scale replicated data with different consistency guarantees to make suitable trade-offs between data consistency and performance. Data and the associated transactions are organized in a hierarchy which is based on consistency levels. Certain rules are imposed on transactions to constrain information flow across data at different levels in this hierarchy to ensure the required consistency guarantees. The building block for this transaction model is the snapshot isolation model. We present an example of an e-commerce application structured with data items and transactions defined at different consistency levels. We have implemented a testbed system for replicated data management based on the proposed multilevel consistency model. We present here the results of our experiments with this e-commerce application to demonstrate the benefits of this model.	causal filter;consistency model;e-commerce;experiment;processor consistency;serializability;snapshot (computer storage);snapshot isolation;strong consistency;testbed	Anand R. Tripathi;Bhagavathi Dhass Thirunavukarasu	2015	2015 IEEE International Conference on Big Data (Big Data)	10.1109/BigData.2015.7363788	real-time computing;computer science;consistency model;release consistency;data mining;database;causal consistency;eventual consistency;data consistency;consistency;sequential consistency	DB	-24.69615565762417	48.207926087837365	125182
cc977141ecda4914987bb3d91b3d6aee603bef46	the performance of concurrency control algorithms for database management systems	simulation framework;concurrency control;cross section;database management system	This paper describes a study of the performance of centralized concurrency control algorithms. An algorithm-independent simulation framework was developed in order to support comparative studies of various concurrency control algorithms. We describe this framework in detail and present performance results which were obtained for what we believe to be a representative cross-section of the many proposed algorithms. The basic algorithms studied include four locking algorithms. two timestamp algorithms. and one optimistic algorithm. Also. we briefly summarize studies of several multiple version algorithms and several hierarchical algorithms. We show that. in general, locking algorithms provide the best performance.	algorithm;centralized computing;concurrency (computer science);concurrency control;lock (computer science);management system;simulation	Michael J. Carey;Michael Stonebraker	1984			timestamp-based concurrency control;database theory;optimistic concurrency control;real-time computing;isolation;database tuning;computer science;concurrency control;database;cross section;distributed computing;multiversion concurrency control;non-lock concurrency control;serializability;database design;distributed concurrency control	DB	-20.33150484431671	46.60436188932984	125229
592bbb52bf238fd50f6d3df84a60058709d92265	concurrent storage structure conversion: from b+ tree to linear hash file	analytical models;sequential access;database system;file reorganization;database management systems;simulation;database processing;contracts;idms;indexes;database systems computer science transaction databases performance analysis algorithm design and analysis analytical models relational databases indexes contracts;transaction databases;database systems;ingres;performance analysis;direct access;database simulation model concurrent storage structure conversion file reorganization sequential access b tree linear hash file user transaction processing database processing direct access ingres ims idms;relational databases;b tree;computer science;concurrent storage structure conversion;transaction processing;database simulation model;simulation model;ims;simulation database management systems file organisation;algorithm design and analysis;linear hash file;analytical model;user transaction processing;file organisation	Shows that the efficient reorganization of a B+ tree file into a linear hash file can be done concurrently with user transaction processing. This conversion is motivated by a change in database processing, in which efficient sequential and direct access were originally needed, but now only efficient direct access is needed. This is quite reasonable for a database system which accommodates new and changing applications. Several existing database systems, e.g. INGRES, IMS and IDMS allow this type of reorganization, but the reorganization is performed offline. The author devises an algorithm which performs the conversion and presents an analytic model of the conversion process. The author uses a typical database simulation model to evaluate the reorganization scheme. The results from the analytic model are within 3% (on the average) of the observed simulation results. >	b+ tree;linear hashing	Edward Omiecinski	1988		10.1109/ICDE.1988.105507	b-tree;database index;algorithm design;transaction processing;relational database;computer science;simulation modeling;data mining;database;world wide web;ip multimedia subsystem;sequential access	DB	-20.305623151779777	47.49940646871757	125315
e39f2ad0d9f00f869d05d6cd132a0e27aeaf0673	a survey of mobile transactions	databases;transaction management;transaction execution;isolation;mobility;mobile host;mobile environment;atomicity;durability;mobile transactions;mobile data management;consistency	Transaction support is crucial in mobile data management. Specific characteristics of mobile environments (e.g. variable bandwidth, disconnections, limited resources on mobile hosts) make traditional transaction management techniques no longer appropriate. Several models for mobile transactions have been proposed but it is difficult to have an overview of all of them. This paper analyzes and compares several contributions to mobile transactions. The analysis distinguishes two groups of models. The first group includes proposals where transactions are completely or partially executed on mobile hosts. In this group we focus on ACID properties support. The second group considers transactions requested by mobile hosts and executed on the wired network. In this case, ACID properties are not compromised and focus is on supporting mobile host movements during transaction execution. Discussions pointing out limitations, interesting solutions and research perspectives complete this paper.	acid;atomicity (database systems);computer cluster;concurrency (computer science);concurrency control;correctness (computer science);database;durability (database systems);modified huffman coding;multitier architecture;nested transaction;online and offline;pointbase;rational clearcase ucm;serial experiments lain;serialization;transaction processing;web application	Patricia Serrano-Alvarado;Claudia Roncancio;Michel E. Adiba	2004	Distributed and Parallel Databases	10.1023/B:DAPD.0000028552.69032.f9	real-time computing;isolation;database transaction;transaction processing;distributed transaction;mobile database;computer science;operating system;durability;database;distributed computing;online transaction processing;compensating transaction;consistency;mobile computing;atomicity;mobile payment	DB	-23.140230627954022	47.55544791651113	125330
f82b7982e7f93b81704ea48ebf77963af6fac7e6	a system-level simulation framework for evaluating resource management policies for heterogeneous system architectures	computational modeling resource management kernel runtime central processing unit graphics processing units power demand;kernel;resource allocation middleware;resource management;runtime;computational modeling;graphics processing units;middleware designer system level simulation framework heterogeneous system architectures cpus data intensive workloads runtime resource management policies systemc transaction level modeling;power demand;central processing unit	Nowadays, heterogeneous system architectures, integrating CPUs and one or more kinds of accelerators (e.g., GPUs or HW accelerators), are a promising solution to achieve high performance for data-intensive workloads while fulfilling other system-level requirements on the available power/energy budgets. However, heterogeneity comes at the cost of greater design and management complexity leading to an increasing quest for the definition of innovative runtime resource management policies. We propose a system-level simulation framework implemented in SystemC and Transaction Level Modeling for a fast evaluation of resource management policies for such systems to provide a quick feedback to the middleware designer. A set of case studies shows the efficiency of the proposed framework in supporting a fast analysis of the investigated policies.	central processing unit;data-intensive computing;graphical user interface;graphics processing unit;method stub;middleware;requirement;system-level simulation;systemc;transaction-level modeling	Antonio Miele;Gianluca Durelli;Marco D. Santambrogio;Cristiana Bolchini	2015	2015 Euromicro Conference on Digital System Design	10.1109/DSD.2015.99	embedded system;parallel computing;kernel;real-time computing;computer science;resource management;operating system;central processing unit;computational model	EDA	-21.596841356231423	57.293816387654665	125361
337e4c969bc78ae11b24ae84793bb8a4db96f444	towards update relevance checks in a context aware mobile information system		In order to reduce transmission cost mobile information system clients often cache data retrieved in a context aware manner. In the case of updates it may happen that this data becomes outdated. So, the caches must be invalidated. In this paper we discuss how to check the relevancy of updates regarding the server database as well as the client contexts.	algorithm;cache (computing);information system;microsoft outlook for mac;relevance;server (computing)	Hagen Höpfner	2005			thermoplastic;database;nut;thread (computing);mechanical engineering;computer science	DB	-25.28214004123625	49.289930545302575	125370
3b599fbf68db0a9c50c924c588815a6541b3237e	a scalable services architecture	configuration data management scalable services architecture data centers scalable clustered application tcp based chain replication gossip based subsystem;service architecture;software architecture;data center;transaction databases web services availability computer crashes computer architecture protocols computer science service oriented architecture quality of service fluctuations;high performance	"""Data centers constructed as clusters of inexpensive machines have compelling cost-performance benefits, but developing services to run on them can be challenging. This paper reports on a new framework, the scalable services architecture (SSA), which helps developers develop scalable clustered applications. The work is focused on non-transactional high-performance applications; these are poorly supported in existing platforms. A primary goal was to keep the SSA as small and simple as possible. Key elements include a TCP-based """"chain replication"""" mechanism and a gossip-based subsystem for managing configuration data and repairing inconsistencies after faults. Our experimental results confirm the effectiveness of the approach"""	class of service;scalability;service-oriented device architecture;transaction processing;virtual community	Tudor Marian;Kenneth P. Birman;Robbert van Renesse	2006	2006 25th IEEE Symposium on Reliable Distributed Systems (SRDS'06)	10.1109/SRDS.2006.7	enterprise architecture framework;reference architecture;software architecture;data center;space-based architecture;real-time computing;database-centric architecture;computer science;applications architecture;operating system;service-oriented architecture;database;solution architecture;distributed computing;computer security;data architecture;computer network	OS	-25.045801972807407	54.626202953501945	125372
c06df1a462cd0592e004f90808dae25b717ffd31	introducing community single sign-on for edit		The European Distributed Institute of Taxonomy (EDIT) platform, as well as biodiversity providers in general, provides a multitude of web-based taxonomic applications and services. Also, the diversity of service providers reflects the highly distributed, cross-national organisational infrastructure of taxonomic institutions and collections. This results in a problem of identity management. While the provider's system administrators have to register users and maintain individual access control lists for each offered service, users have to remember a variety of login/password combinations to use all these different services. Therefore, EDIT promotes a Community Single Sign-On (CSSO) security infrastructure, which protects and provides access to all EDIT platform components based on a single identity per user. That way, users need to remember only one login/password combination to use EDIT's platform facilities. And, service providers can proceed to protect their resources and services by defining individual access control policies, but at considerably reduced administrative costs. These fundamental enhancements can be achieved through the introduction of a Security Assertion Markup Language (SAML) based (Shibboleth) single sign-on framework, adapted to the requirements of the EDIT platform. Since, information infrastructures within EDIT are quite similar to those in the general biodiversity community, our approach shall motivate other providers to follow. Therefore, this document provides a first-hand report initiating single sign-on for EDIT.	access control list;identity management;login;password;requirement;security assertion markup language;shibboleth;single sign-on;system administrator;web application	Lutz Suhrbier	2009			service provider;world wide web;access control;password;single sign-on;string-to-string correction problem;business;speech recognition;security assertion markup language;identity management;login	Web+IR	-32.92784481964894	52.79449655018017	125395
34f0e16059d9ccafef72e7955215c68f43d6068c	effective integration of scientific instruments in the grid	nuclear magnetic resonance spectroscopy	The paper presents a framework for Virtual Laboratory (VLab). Authors focus their attention on the application structure, which can make it possible to remotely use very expensive devices. It is important that this structure is universal and can be used with every device and every simulation. Therefore the framework is a critical issue in that concept. To prove the concept we plan to put into practice the Virtual Laboratory of Nuclear Magnetic Resonance Spectroscopy (NMR), which bases on this general structure. Another significant issue is to handle remote facilities like other Grid resources. This assumption generates new requirements concerning e.g. resource scheduling and global accounting.	acm/ieee supercomputing conference;atm turbo;e-science;experiment;grid computing;requirement;resonance;scheduling (computing);simulation;strongly correlated material;television	Norbert Meyer;Marcin Lawenda;Marcin Okon;Tomasz Rajtar;Dominik Stoklosa;Maciej Stroinski	2004			computational physics;grid;medical physics;nuclear magnetic resonance spectroscopy;scientific instrument;virtual laboratory;engineering	HPC	-32.1112297189061	51.67677826299776	125400
939e5686e222b559fbfd0519ecf2e6ba2f68de8f	consistency control for synchronous and asynchronous collaboration based on shared objects and activities	synchronous and asynchronous collaboration;replication;activity centric collaboration;control algorithm;asynchronous collaboration;consistency control;object centric sharing;dynamic environment;replicated data;peer to peer;shared workspace;activityexplorer	We describe a new collaborative technology that bridges the gap between ad hoc collaboration in email and more formal collaboration in structured shared workspaces. Our approach is based on the notion of object-centric sharing, where users collaborate in a lightweight manner but aggregate and organize different types of shared artifacts into semi-structured activities with dynamic membership, hierarchical object relationships, as well as real-time and asynchronous collaboration. We present a working prototype that implements object-centric sharing on the basis of a replicated peer-to-peer architecture. In order to keep replicated data consistent in such a dynamic environment with blended synchronous and asynchronous collaboration, we designed appropriate consistency control algorithms, which we describe in detail. The performance of our approach is demonstrated by means of simulation results.	aggregate data;algorithm;consistency model;data model;email;hoc (programming language);library (computing);muller automaton;overhead (computing);peer-to-peer;prototype;real-time clock;real-time locating system;semiconductor industry;simulation;state (computer science);synchronization (computer science);usability;word lists by frequency;workspace	Jürgen Vogel;Werner Geyer;Li-Te Cheng;Michael J. Muller	2004	Computer Supported Cooperative Work (CSCW)	10.1007/s10606-004-5064-6	replication;real-time computing;computer science;distributed computing;world wide web;statistics	DB	-26.04448075868551	47.70998429262757	125422
f567502fc68df73c5514f31034358ef6351dbe06	flexible replica placement for optimized p2p backup on heterogeneous, unreliable machines	availability;distributed storage;unstructured p2p networks;data replication;enterprise backup	P2P architecture is a viable option for enterprise backup. In contrast to dedicated backup servers, nowadays, a standard solution, making backups directly on organization’s workstations should be cheaper as existing hardware is used, more efficient as there is no single bottleneck server, and more reliable as the machines can be geographically dispersed. We present an architecture of a P2P backup system that uses pairwise replication contracts between a data owner and a replicator. In contrast to a standard P2P storage system using directly a distributed hash table (DHT), the contracts allow our system to optimize replicas’ placement depending on a specific optimization strategy and so to take advantage of the heterogeneity of the machines and the network. Such optimization is particularly appealing in the context of backup: replicas can be geographically dispersed, the load sent over the network can be minimized, or the optimization goal can be to minimize the backup/restore time. However, managing the contracts, keeping them consistent and adjusting them in response to dynamically changing environment is challenging. We built a scientific prototype and ran experiments on 150 workstations in our university’s computer laboratories and, separately, on 50 PlanetLab nodes. We found out that the main factor affecting the performance of the system is the availability of the machines. Yet, our main conclusion is that it is possible to build an efficient and reliable backup system on highly unavailable machines, as our computers had just 13% average availability. Copyright © 2015 John Wiley & Sons, Ltd.	backup;computer data storage;computer lab;distributed hash table;experiment;john d. wiley;mathematical optimization;peer-to-peer;planetlab;prototype;server (computing);workstation	Piotr Skowron;Krzysztof Rzadca	2016	Concurrency and Computation: Practice and Experience	10.1002/cpe.3491	backup software;continuous data protection;availability;parallel computing;real-time computing;distributed data store;incremental backup;computer science;operating system;database;distributed computing;replication	OS	-22.03769348898608	52.14365622617541	125615
2b6f1e247aed43a0b0b48bc58a4ef85f2c088bbc	autonomic management architecture for flexible grid services deployment based on policies	grid services;open grid service architecture;user profile;policy based management architecture;policy based management;grid service;quality of service;network computing	This paper describes a dynamic, scalable and flexible Policy-based Management Architecture (PbMA), which is characterized by a reliable and autonomous deployment, activation and management of Grid Services. This architecture follows the implied conditions by the Open Grid Services Architecture (OGSA) standard. Although applicable to any user profiles, our system is essentially intended for non-massive resource owners accessing large amounts of computing, software, memory and storage resources. Unlike similar architectures, it is able to manage service requirements demanded by users, providers and services themselves. This architecture is also able to manage computational resources in order to fulfill Quality of Service (QoS) requirements, based on a balanced scheduling of resources exploitation. Our approach is scalable and flexible by extending itself the management components and policies interpreters needed to control multiple infrastructures regardless network technology, operative platform or administrative domain. The management architecture shows its reliability through a Grid Service deployment example.	autonomic computing;software deployment	Edgar Magaña;Laurent Lefèvre;Joan Serrat	2007		10.1007/978-3-540-71270-1_12	enterprise architecture framework;reference architecture;space-based architecture;quality of service;computer science;applications architecture;enterprise architecture management;database;service;solution architecture;world wide web;grid computing;computer network	HPC	-30.94054656744955	53.55209505177954	125626
717b1e1350b8422dc41f5109b9542614532938a4	efficient characterization of hidden processor memory hierarchies		A processor’s memory hierarchy has a major impact on the performance of running code. However, computing platforms, where the actual hardware characteristics are hidden from both the end user and the tools that mediate execution, such as a compiler, a JIT and a runtime system, are used more and more, for example, performing large scale computation in cloud and cluster. Even worse, in such environments, a single computation may use a collection of processors with dissimilar characteristics. Ignorance of the performance-critical parameters of the underlying system makes it difficult to improve performance by optimizing the code or adjusting runtime-system behaviors; it also makes application performance harder to understand. To address this problem, we have developed a suite of portable tools that can efficiently derive many of the parameters of processor memory hierarchies, such as levels, effective capacity and latency of caches and TLBs, in a matter of seconds. The tools use a series of carefully considered experiments to produce and analyze cache response curves automatically. The tools are inexpensive enough to be used in a variety of contexts that may include install time, compile time or runtime adaption, or performance understanding tools.	central processing unit;compile time;compiler;computation;experiment;just-in-time compilation;memory hierarchy;runtime system	Keith Cooper;Xiaoran Xu	2018		10.1007/978-3-319-93713-7_27	distributed computing;real-time computing;compiler;runtime system;end user;cloud computing;computation;hierarchy;computer science;memory hierarchy	HPC	-23.16469974149255	56.70003007623391	125655
4a0bb4eece00f3e9445d1a0d933422aa408ce8d1	apache hadoop yarn: yet another resource negotiator	fairness;multi resource;data center	The initial design of Apache Hadoop [1] was tightly focused on running massive, MapReduce jobs to process a web crawl. For increasingly diverse companies, Hadoop has become the data and computational agorá---the de facto place where data and computational resources are shared and accessed. This broad adoption and ubiquitous usage has stretched the initial design well beyond its intended target, exposing two key shortcomings: 1) tight coupling of a specific programming model with the resource management infrastructure, forcing developers to abuse the MapReduce programming model, and 2) centralized handling of jobs' control flow, which resulted in endless scalability concerns for the scheduler.  In this paper, we summarize the design, development, and current state of deployment of the next generation of Hadoop's compute platform: YARN. The new architecture we introduced decouples the programming model from the resource management infrastructure, and delegates many scheduling functions (e.g., task fault-tolerance) to per-application components. We provide experimental evidence demonstrating the improvements we made, confirm improved efficiency by reporting the experience of running YARN on production environments (including 100% of Yahoo! grids), and confirm the flexibility claims by discussing the porting of several programming frameworks onto YARN viz. Dryad, Giraph, Hoya, Hadoop MapReduce, REEF, Spark, Storm, Tez.	agora;apache hadoop;centralized computing;component-based software engineering;computational resource;control flow;dryad;fault tolerance;job stream;mapreduce;next-generation network;programming model;spark;scalability;scheduling (computing);software deployment;viz: the computer game;yet another	Vinod Kumar Vavilapalli;Arun C. Murthy;Chris Douglas;Sharad Agarwal;Mahadev Konar;Robert Evans;Thomas Graves;Jason Lowe;Hitesh Shah;Siddharth Seth;Bikas Saha;Carlo Curino;Owen O'Malley;Sanjay Radia;Benjamin Reed;Eric Baldeschwieler	2013		10.1145/2523616.2523633	embedded system;data center;real-time computing;simulation;telecommunications;engineering;operating system;world wide web;computer network	OS	-22.342772026942097	58.749415305718884	125764
6c2aadf0759178a32a90c3ebcc5c7c4ce4d9483b	scalable architecture for anomaly detection and visualization in power generating assets		Power-generating assets (e.g., jet engines, gas turbines) are often instrumented with tens to hundreds of sensors for monitoring physical and performance degradation. Anomaly detection algorithms highlight deviations from predetermined benchmarks with the goal of detecting incipient faults. We are developing an integrated system to address three key challenges within analyzing sensor data from power-generating assets: (1) difficulty in ingesting and analyzing data from large numbers of machines; (2) prevalence of false alarms generated by anomaly detection algorithms resulting in unnecessary downtime and maintenance; and (3) lack of an integrated visualization that helps users understand and explore the flagged anomalies and relevant sensor context in the energy domain. We present preliminary results and our key findings in addressing these challenges. Our system's scalable event ingestion framework, based on OpenTSDB, ingests nearly 400,000 sensor data samples per seconds using a 30 machine cluster. To reduce false alarm rates, we leverage the False Discovery Rate (FDR) algorithm which significantly reduces the number of false alarms. Our visualization tool presents the anomalies and associated content flagged by the FDR algorithm to inform users and practitioners in their decision making process. We believe our integrated platform will help reduce maintenance costs significantly while increasing asset lifespan. We are working to extend our system on multiple fronts, such as scaling to more data and more compute nodes (70 in total).	algorithm;anomaly detection;downtime;elegant degradation;image scaling;scalability;sensor	Paras Jain;Chirag Tailor;Sam Ford;Liexiao Ding;Michael Phillips;Fang Cherry Liu;Nagi Gebraeel;Duen Horng Chau	2017	2017 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)	10.1109/IPDPSW.2017.77	parallel computing;real-time computing;operating system;data mining;distributed computing;computer security	Visualization	-22.47421245574884	55.850003939433165	125937
07743015445cb763be19a3156f2ecf908a04651f	key factors for improving performance of concurrency control algorithms	performance evaluation;simulation;response time;multi user;concurrency control;algorithms;computer systems programming;r trees;access method;computer simulation	Five key factors for improving the performance of access methods with concurrency control features are presented. Three of these factors reflect search and concurrency control methods, and the other two explore the interference behaviors among a set of concurrent operations. These factors can be used as keys to improve the efficiency and robustness of a concurrency control algorithm (CCA) and as basic interpretations to explain why the performance of one CCA would be better (or worse) than another in a multi-user environment. To illustrate the applicability of these factors, two CCAs for R-trees were used as examples to explain how to improve the performance of a CCA using these five key factors.	algorithm;concurrency (computer science);concurrency control	J. K. Chen;Yeh-Hao Chin;Yin-Fu Huang	2001	Inf. Sci.	10.1016/S0020-0255(01)00124-4	computer simulation;r-tree;optimistic concurrency control;real-time computing;computer science;theoretical computer science;concurrency control;distributed computing;access method;response time	DB	-20.240674890398584	46.64238753703457	125948
4b068766c6a1014290be53e4556d851a91bec03a	the cactus worm: experiments with dynamic resource discovery and allocation in a grid environment	resource selection;computers;engineers;cluster computing;general and miscellaneous mathematics computing and information science;resource discovery;resource allocation;performance;computer networks;adaptive applications;runtime system;grid computing	The ability to harness heterogeneous, dynamically available “Grid” resources is attractive to typically resource-starved computational scientists and engineers, as in principle it can increase, by significant factors, the number of cycles that can be delivered to applications. However, new adaptive application structures and dynamic runtime system mechanisms are required if we are to operate effectively in Grid environments. In order to explore some of these issues in a practical setting, we are developing an experimental framework, called Cactus, that incorporates both adaptive application structures for dealing with changing resource characteristics and adaptive resource selection mechanisms that allow applications to change their resource allocations (e.g., via migration) when performance falls outside specified limits. We describe here the adaptive resource selection mechanisms and describe how they are used to achieve automatic application migration to “better” resources following performance degradation. Our results provide insights into the architectural structures required to support adaptive resource selection. In addition, we suggest that this “Cactus Worm” is an interesting challenge problem for Grid computing.	approximation;cactus framework;elegant degradation;grid computing;ibm notes;programmer;runtime system	Gabrielle Allen;Dave Angulo;Ian T. Foster;Gerd Lanfermann;Chuang Liu;Thomas Radke;Edward Seidel;John Shalf	2001	IJHPCA	10.1177/109434200101500402	parallel computing;simulation;performance;computer cluster;resource allocation;computer science;theoretical computer science;operating system;distributed computing;management;grid computing	HPC	-20.229924655958435	58.532091058973606	125989
6398dedb5b09e4efe1fde3eb6ffc32a73dff2d85	locking with prevention of cyclic and infinite restarting in distributed database systems	distributed database system	A new solution to the cyclic restarting and infinite restarting problems for locking schemes in Distributed Database Systems (DDBSE) is presented. The solution proposed is based on the data marking mechanism, which ensures the completion of each transaction in the system. The solution is fully distributed. It only requires information locally accessible on each site of the DDBS, and it intervenes into transaction processing only in the case of real danger of cyclic and/or infinite restarting. Simulation has shown that this solution significantly reduces the number of transaction restarts in DDBSs using locking schemes, and thus considerably improves DDBS performance.		Wojciech Cellary;Tadeusz Morzy	1985			real-time computing;computer science;database;distributed computing	DB	-22.274411987778137	48.16544782155334	126006
beaf7e440c895bf626d8e82d9aa75e57ca919a0c	transparent driver-kernel isolation with vmm intervention		How to satisfy the on-demand environment while providing highly dependable services with minimum cost is a challenging issue. Improvements in the reusability of virtualization technology have enabled operating system's adaptability, which helps users customize their application environments by using various types and versions of operating systems and drivers. However, driver faults in virtual machine are a critical obstacle to achieve reliable user environment, and may even harm the reliability of the entire server. This paper describes Chariot which transparently isolates drivers in a virtual machine without affecting the reusability of the virtualization environment. An isolation loading mechanism links an isolated driver with monitoring wrappers in a virtual machine, which avoids modifying the VM kernel and drivers. According to the monitoring information, Chariot not only instantaneously updates the access control table which records the memory used by the driver, but also sets the write protection of the shadow page table which is corresponding to the whole kernel space of the virtual machine. As a result, the write operations of a driver can be captured and examined in advance. Experimental results show that Chariot can effectively isolate driver faults and improve the reliability of the operating system in a virtual machine. Furthermore, Chariot can be easily extended to isolate new drivers and ported to other versions of operating systems.	access control;chariot;control table;dependability;device driver;hardware virtualization;kernel (operating system);operating system;page table;server (computing);user interface;user space;virtual machine manager;write protection;x86 virtualization	Hao Zheng;Endong Wang;Yinfeng Wang;Xingjun Zhang;Baoke Chen;Weiguo Wu;Xiaoshe Dong	2013		10.1145/2524211.2524219	embedded system;full virtualization;real-time computing;engineering;virtual machine;operating system	OS	-24.332892057663692	51.03854219731662	126066
19155104d0005ce43667cfda8e72cb0ea3b46553	distributed plug-and-play network management model based on mobile agents	distributed programming mobile agents computer network management;plug and play;mobile agents;runtime;technology management;telecommunication traffic;distributed programming;self adapting network management distributed plug and play network management mobile agents legacy network management system operator intensive problem;computer network management;legacy network management system;humans;distributed plug and play network management;network management;self adapting network management;mobile agent;mobile agents environmental management software development management humans runtime laboratories hardware operating systems technology management telecommunication traffic;environmental management;operator intensive problem;software development management;network management system;operating systems;hardware	As network is developing to distribution and heterogeneity, legacy network management systems do not work efficiently on operator-intensive distribution of management tasks. A distributed plug-and-play network management model is presented to implement an automatic and self-adapting network management system. In this model, network components can automatically locate the proper management stations, which are configured in specified function, as added to network and get initial management tasks without human intervention. We adopt mobile agent infrastructure to communicate and fulfill management tasks, thus improve the flexibility and autonomy. Moreover, it makes extension and update of management tasks at run-time, thus increases self-adapting capability of the model.	agent-based model;mobile agent;performance evaluation;plug and play	Nan Guo;Tianhan Gao;Hong Zhao	2004		10.1109/EEE.2004.1287351	network management;out-of-band management;fcaps;element management system;network traffic control;real-time computing;intelligent computer network;network architecture;network management station;systems engineering;engineering;network simulation;distributed computing;network management application;structure of management information	OS	-33.323507363651956	47.58620651316924	126151
c3f3273404eb65dcaf153ef93cc4fa085e943012	a hybrid fault tolerance scheme for easygrid mpi applications	distributed system;computational grid;fault tolerant;application management;autonomic mpi applications;large scale;distributed environment;fault tolerance;computational grids;high performance	Writing applications capable of executing efficiently in distributed systems is extremely difficult and tedious for inexperienced users. The resources may be heterogeneous, non-dedicated, and offered without any performance or availability guarantees. Systems capable of adapting the execution of an application to these characteristics are essential. The EasyGrid Application Management System (AMS) transforms cluster-based MPI applications into autonomic ones capable executing robustly and efficiently in distributed environments. This work describes a strategy to endow these autonomic MPI applications with the property of self-healing and thus be capable of withstanding multiple simultaneous crash faults of processes and/or processors. The extremely low intrusion cost of the proposed hybrid solution might now facilitate acceptance of fault tolerance techniques in large scale high performance applications.	application lifecycle management;autonomic computing;central processing unit;distributed computing;experience;fault tolerance;management system;message passing interface;vhdl-ams	Jacques Alves da Silva;Vinod E. F. Rebello	2011		10.1145/2089002.2089006	parallel computing;real-time computing;computer science;distributed computing	HPC	-24.81853551642812	52.95509146709016	126245
d6af0c70d74a08fee977bf85a6890f6fc40a6cb6	a conceptual design of job pre-processing flow for heterogeneous batch systems in data center	resource utilization;data center;batch system;job processing	Processing a workflow (or a job) created by a user, who can be a researcher from a scientific laboratory or an analysis from a commercial organization, is the main functionality that a data center or a high-performance computing center is generally expected to provide. It can be accomplished with a single core processor and rather small amount of memory if the problem is adequately small while it may require thousands of nodes to solve a complicated problem and peta-bytes of storage for its output. Also specific applications on various platforms are required in general by users for resolving the problems appropriately. In this aspect, a data center should operate non-homogeneous systems for resource management, so-called batch system, in which it results in inefficient resource utilization due to stochastic behavior of user activity. Implementation of virtualization for resource management, e.g. Cloud Computing, is one of promising solutions recently arising, however, it results in the increase of complexity of the system itself as well as the system administration because it naturally implies the intervention of virtualization stack, e.g. hypervisor, between Operating System and applications for resource management. In this paper, we propose a new conceptual design to be implemented as a pre-scheduler capable to insert user submitted jobs dedicated to a specific batch system into available resources managed by other kind of batch systems. The proposed design features transparency in between clients and batch systems, accuracy in terms of monitoring and prediction on the available resources, and scalability for additional batch systems. We suggest the implementation example of the conceptual design based on the scenario established from our experience of operating a data center.	algorithm;batch processing;byte;cloud computing;data center;gene ontology term enrichment;hypervisor;job stream;norm (social);operating system;preprocessor;requirement;scalability;scheduling (computing);supercomputer;system administrator	Sang Un Ahn;Jin Wook Kim	2016	Wireless Personal Communications	10.1007/s11277-016-3224-x	data center;in situ resource utilization;real-time computing;computer science;job scheduler;operating system;portable batch system;job stream;distributed computing;computer network;batch processing	OS	-30.0095072710652	54.216373724893465	126514
fdf8e1c38bc51a34b7953c7d89463ecf553c3fed	is ordering of disk updates required to maintain file-system crash-consistency?		On reboot after a crash, the file system should be consistent: e.g., previously correct files should not now contain garbage. In early file systems, getting to a consistent state involved a full scan after reboot. This was very slow, and impractical for large systems. Modern file systems improve upon this by writing updates to disk in a specific order : e.g, metadata before commit blocks. This allows them to get to a consistent state without a scan. However, ordering updates results in certain problems: 1. The file system write order may not be the most efficient order for writing blocks to disk. This reduces performance. 2. The file system has to be very careful about the order; this increases complexity, potentially leading to more bugs and lower reliability. 3. For disks with write caches, commands such as cache flushes are required to ensure correct ordering. If such commands are not properly implemented, consistency is compromised [1]. 4. In virtualized stacks, even if one of the many layers between the file system and the disk does not enforce ordering, consistency is lost. The question then arises: can crash-consistency be maintained without ordering updates? Recent work introduced the No-Order File System (NoFS) [2], the first file system to provide strong consistency despite not ordering updates. NoFS uses a novel technique called Backpointer-Based Consistency (BBC) that establishes consistency via mutual agreement between file-system objects. NoFS performs as well as a comparable journaling file system (ext3) for most workloads, and increases throughput by 20-70% for metadata-intensive workloads. BODY Not only is it possible to maintain file-system crash-consistency without ordering updates, but doing so may actually increase performance. REFERENCES [1] Rajimwale, A., Chidambaram, V., Ramamurthi, D., Arpaci-Dusseau, A. C., and Arpaci-Dusseau, R. H. Coerced cache eviction and discreet mode journaling: Dealing with misbehaving disks. In Dependable Systems & Networks (DSN), 2011 IEEE/IFIP 41st International Conference on (Hong Kong, China, June 2011), IEEE, pp. 518–529. [2] Vijay Chidambaram, Tushar Sharma, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau. Consistency Without Ordering. In Proceedings of the 10th Conference on File and Storage Technologies (FAST ’12) (San Jose, California, February 2012). Volume 2 of Tiny Transactions on Computer Science This content is released under the Creative Commons Attribution-NonCommercial ShareAlike License. Permission to make digital or hard copies of all or part of this work is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. CC BY-NC-SA 3.0: http://creativecommons.org/licenses/by-nc-sa/3.0/.	booting;cpu cache;complexity;computer science;crash (computing);dependability;digital data;international federation for information processing;software bug;strong consistency;throughput	Vijay Chidambaram	2013	TinyToCS		real-time computing;journaling file system;file system fragmentation	OS	-19.99685016051198	49.81143175352065	126546
83720ba7ae7d9f6f0b9d0fdf34393a5f985fbecb	cloudnetsim++: a gui based framework for modeling and simulation of data centers in omnet++	cloud computing computational modeling data models clouds distributed databases graphical user interfaces servers;virtual machine migration cloud computing data center omnet energy efficiency cloud simulator	State-of-the-art cloud simulators in use today are limited in the number of features they provide, lack real network communication models, and do not provide extensive Graphical User Interface (GUI) to support developers and researchers to extend the behavior of the cloud environment. We propose CloudNetSim++, a comprehensive packet level simulator that enables simulation of cloud environments. CloudNetSim++ can be used to evaluate a wide spectrum of cloud components, such as processing elements, storage, networking, Service Level Agreement (SLA), scheduling algorithms, fine grained energy consumption, and VM consolidation algorithms. CloudNetSim++ offers extendibility, which means that the developers and researchers can easily incorporate own algorithms for scheduling, workload consolidation, VM migration, and SLA agreement. The simulation environment of CloudNetSim++ offers a rich GUI that provides a high level view of distributed data centers connected with various network topologies. The package also includes an energy computation module that provides a fine grained analysis of energy consumed by each component. This paper shows the flexibility and effectiveness of CloudNetSim++ through experimental results demonstrated using real-world data center workloads. Moreover, to demonstrate the correctness of CloudNetSim++, we performed formal modeling, analysis, and verification using High-level Petri Nets, Satisfiability Modulo Theories (SMT), and Z3 solver.	algorithm;cloud computing;cloudsim;computation;correctness (computer science);data center;graphical user interface;high-level programming language;modulo operation;network topology;petri net;requirement;satisfiability modulo theories;scheduling (computing);semiconductor consolidation;service-level agreement;simulation;solver;z3 (computer)	Asad W. Malik;Kashif Bilal;Saif Ur Rehman Malik;Zahid Anwar;Khurram Aziz;Dzmitry Kliazovich;Nasir Ghani;Samee Ullah Khan;Rajkumar Buyya	2017	IEEE Transactions on Services Computing	10.1109/TSC.2015.2496164	scheduling (computing);service-level agreement;real-time computing;computer science;cloud computing;distributed computing;distributed database;data modeling;modeling and simulation;server;solver	SE	-24.15060990528967	57.376284184653976	126831
38834ac501c692804c6f2b9a1b80c4744f86db4f	spanedge: towards unifying stream processing over central and near-the-edge data centers	prototypes;computer and information science;runtime;computer architecture;storms;engines;distributed databases;data och informationsvetenskap;wide area networks	In stream processing, data is streamed as a continuous flow of data items, which are generated from multiple sources and geographical locations. The common approach for stream processing is to transfer raw data streams to a central data center that entails communication over the wide-area network (WAN). However, this approach is inefficient and falls short for two main reasons: (i) the burst in the amount of data generated at the network edge by an increasing number of connected devices, (ii) the emergence of applications with predictable and low latency requirements. In this paper, we propose SpanEdge, a novel approach that unifies stream processing across a geo-distributed infrastructure, including the central and near-the-edge data centers. SpanEdge reduces or eliminates the latency incurred by WAN links by distributing stream processing applications across the central and the near-the-edge data centers. Furthermore, SpanEdge provides a programming environment, which allows programmers to specify parts of their applications that need to be close to the data source. Programmers can develop a stream processing application, regardless of the number of data sources and their geographical distributions. As a proof of concept, we implemented and evaluated a prototype of SpanEdge. Our results show that SpanEdge can optimally deploy the stream processing applications in a geo-distributed infrastructure, which significantly reduces the bandwidth consumption and the response latency.	apache storm;bada;component-based software engineering;data center;dataflow;emergence;emulator;foreign function interface;integrated development environment;klara nahrstedt;programmer;prototype;requirement;scheduling (computing);stream processing;streaming media	Hooman Peiro Sajjad;Ken Danniswara;Ahmad Al-Shishtawy;Vladimir Vlassov	2016	2016 IEEE/ACM Symposium on Edge Computing (SEC)	10.1109/SEC.2016.17	data stream clustering;parallel computing;real-time computing;computer science;operating system;distributed computing;prototype;data stream mining;storm;distributed database;computer security	OS	-20.53140932558088	54.8193733786159	127025
4185164442506bc433d3eb9a5b3cd636263c17be	infrastructure for advanced network management based on mobile code	project management;application software;mobile agents;resource management;object oriented programming;advanced network management;technology management;software agents;network servers;java advanced network management mobile code perpetuum project mobile agents code mobility;perpetuum project;engineering management;computer network management;mobile code;mobile agents technology management resource management java computer network management project management engineering management mobile computing application software network servers;code mobility;network management;mobile agent;mobile computing;java	The research is part of the Perpetuum project that makes use of mobile agents for network management. In this paper, we introduce an infrastructure that is required for a new approach to network management utilizing mobile code. This involves providing a framework for code mobility, access to managed resources and communication between agents. The infrastructure is the fundamental part that provides the base upon which our research of applications of mobile code technology is built. The infrastructure is built on Java. Java addresses several critical issues, such as security, portability, persistent state through serialization, networking, and other features. That's why Java was selected for the development of the infrastructure. We also present some examples of infrastructure application to demonstrate the advantages of the use of mobile agents for network management.	code mobility	Gatot Susilo;Andrzej Bieszczad;Bernard Pagurek	1998		10.1109/NOMS.1998.654432	network management;project management;application software;real-time computing;public land mobile network;computer science;technology management;resource management;software agent;operating system;mobile agent;distributed computing;object-oriented programming;java;mobile computing;code mobility;computer network	Mobile	-33.49144442218114	47.27533809292751	127375
ed5aa5717b9e7ef9191def9b5f71060b05b97ade	on construction of a diskless cluster computing environment in a computer classroom	computer classroom;diskless;hpc challenge hpcc;bootable cluster cd bccd;pc clusters;diskless remote boot in linux drbl	"""This paper's objective is to implement and evaluate a high-performance computing environment by clustering idle PCs personal computers with diskless slave nodes on campuses to obtain the effectiveness of the largest computer potency. Two sets of Cluster platforms, BCCD and DRBL, are used to compare computing performance. It's to prove that DRBL has better performance than BCCD in this experiment. Originally, DRBL was created to facilitate instructions for a Free Software Teaching platform. In order to achieve the purpose, DRBL is applied to the computer classroom with 32 PCs so to enable PCs to be switched manually or automatically among different OS operating systems. The bioinformatics program, mpiBLAST, is executed smoothly in the Cluster architecture as well. From management's view, the state of each Computation Node in Clusters is monitored by """"Ganglia"""", an existing Open Source. The authors gather the relevant information of CPU, Memory, and Network Load for each Computation Node in every network section. Through comparing aspects of performance, including performance of Swap and different network environment, they attempted to find out the best Cluster environment in a computer classroom at the school. Finally, HPL of HPCC is used to demonstrate cluster performance."""		Chao-Tung Yang;Wen-Feng Hsieh	2012	IJGHPC	10.4018/jghpc.2012100105	embedded system;computer hardware;computer science;operating system;computer security	HPC	-28.298977339380404	52.69564739562604	127412
479654549653edad6d29437f6757850770ae41e3	grid services engineering and management	swinburne	The grid paradigm is an useful technique in the astronomical community to enable the information discovery with a very large amount of massive and complex data. Astronomical datasets are essentially structured in catalogs and archives managed by several database management systems. Porting these data in a grid environment has twofold aspects according to put or not the entire management system on grid: this paper presents an approach of a grid application deploy based on web services technology to access an existing astronomical database. The framework hosting the application consists of the grid infrastructure provided by the Italian INFN Institute [1]: this middleware affects both technological choices than current and further develops in order to maintain the compatibility. The document covers the implementation design in this context of all the actors involved in the web service architecture (service requestor, service provider and service broker) and the mapping with their corresponding grid components. This relation gives the interoperability of web services within the existing grid architecture. It is detailed point out the open problem of service discovery in this grid environment: the current implementation of service registry through WS-Inspection documents [2] and/or Universal Description, Discovery and Integration [3] registry could be adopted in a local grid infrastructure, but should be progressively integrated with the main grid information system up to now implemented as Globus Monitoring and Discovery Service (MDS2) [4]. 1 Astronomical Data Resources in a Virtual Organization Environment According to the definition of the grid concept [5], a grid solution is created to allows applications to share data and resources as well to access them across multiple organizations in an efficient way. Each solution can be configured as a physical grid referring to hardware resources shared over a distributed network or as a logical grid referring to software and application sharing. In a research environment such as the astronomical community, one of the key concerns is to find more standard and scalable way to integrate massive and complex data coming from digital survey in a distributed environment. This data has an exponential growth that causes the problem of information discovery with such amount of data. In this context the grid paradigm is a technique able	archive;astronomical catalog;database;grid computing;information discovery;information system;interoperability;middleware;programming paradigm;scalability;service discovery;service-oriented architecture;time complexity;virtual organization (grid computing);web service	Josef Kittler	2004		10.1007/b100715	engineering management;media studies;operations research	HPC	-31.152846210243933	51.026600208461254	127735
3e1a99137a1cf74d8bffaf42520886499158263a	pacer: a progress management system for live virtual machine migration in cloud computing	datacenter live migration cloud computing progress management;cloud computing virtual machine monitors prediction algorithms virtual machining web servers virtualization;software maintenance;software management;virtual machines cloud computing computer centres scheduling software maintenance software management;computer centres;virtual machines;scheduling;amazon ec2 pacer migration progress management system live virtual machine migration cloud computing dependent task scheduling unacceptable application degradation distant cloud datacenters application performance tradeoff migration time tradeoff progress predictions user defined migration objectives;cloud computing	Live migration of virtual machines is a key management function in cloud computing. Unfortunately, no live migration progress management system exists in the state-of-theart, leading to (1) guesswork over how long a migration might take and the inability to schedule dependent tasks accordingly; (2) unacceptable application degradation when application components become split over distant cloud datacenters for an arbitrary period during migration; (3) inability to tradeoff application performance and migration time e.g. to finish migration later for less impact on application performance. Pacer is the first migration progress management system that solves these problems. Pacer's techniques are based on robust and lightweight run-time measurements of system and workload characteristics, efficient and accurate analytic models for progress predictions, and online adaptation to maintain user-defined migration objectives for coordinated and timely migrations. Our experiments on a local testbed and on Amazon EC2 show that Pacer is highly effective under a range of application workloads and network conditions.	amazon elastic compute cloud (ec2);automated planning and scheduling;cloud computing;component-based software engineering;elegant degradation;enterprise software;experiment;input/output;key management;management system;mathematical model;scheduling (computing);system migration;testbed;virtual machine	Jie Zheng;T. S. Eugene Ng;Kunwadee Sripanidkulchai;Zhaolei Liu	2013	IEEE Transactions on Network and Service Management	10.1109/TNSM.2013.111013.130522	real-time computing;simulation;cloud computing;computer science;virtual machine;operating system;distributed computing;software maintenance;scheduling;computer security;computer network	HPC	-23.71471685526807	60.15330850271158	127994
567885f870ef95bfabcaa7cb573c1799080dd492	energy-efficient virtual machine consolidation	energy efficiency;energy conservation;virtualization;storage management;information technology;energy saving energy efficient virtual machine consolidation vm consolidation energy efficient storage migration live vm migration eucalyptus open source clone amazon elastic compute cloud;public domain software;virtual machines cloud computing energy conservation public domain software storage management;information technology energy efficiency cloud computing virtualization;virtual machines;energy storage;cloud computing energy efficiency virtual machines energy storage economics;economics;cloud computing	A novel approach to virtual machine (VM) consolidation, based on energy-efficient storage migration and live VM migration, is implemented using Eucalyptus, an open source clone of the Amazon Elastic Compute Cloud. Several experiments demonstrate the potential energy savings.	experiment;open-source software;semiconductor consolidation;virtual machine	Pablo Graubner;Matthias Schmidt;Bernd Freisleben	2013	IT Professional	10.1109/MITP.2012.48	real-time computing;virtualization;converged storage;energy conservation;cloud computing;computer hardware;computer science;virtual machine;operating system;efficient energy use;information technology;energy storage;public domain software	OS	-20.09046955645242	60.10965390477509	128001
01ef8a66f7285d5cec520a22b8bb26553f8060cb	integrating multiple concurrency control algorithms	concurrency control		algorithm;concurrency control	Michael Ranft	1993			distributed concurrency control;computer science;database;optimistic concurrency control;concurrency control;non-lock concurrency control;isolation (database systems);distributed computing;serializability;multiversion concurrency control;timestamp-based concurrency control	ML	-23.65308521462398	47.51489484870488	128073
b40a0bb5f6b863585dbba6afb4e4195d2b850b2d	the smash impacts to cluster computing	data model profiles;unified command line protocol;protocols;cluster computing;resource discovery;grid administration;heterogeneous computing;environmental management high performance computing grid computing computer architecture hardware costs computer interfaces protocols data models job shop scheduling;performance aware job scheduling cluster computing multiple management frameworks hpc clusters grid administration management interoperability systems management architecture for server hardware management interfaces remote management architecture heterogeneous computing environments unified command line protocol resource discovery resource addressing data model profiles info structure status;multiple management frameworks;total cost of ownership;info structure status;heterogeneous environment;data model;network servers;remote management architecture;scheduling;management interoperability;performance aware job scheduling;high performance computer;resource addressing;workstation clusters grid computing network servers open systems protocols scheduling;systems management architecture for server hardware management interfaces;workstation clusters;open systems;grid computing;system management;job scheduling;hpc clusters;heterogeneous computing environments	Summary form only given. High performance computing clusters scaling out fact indicates manageability will become more important than ever. Over time, a computer center tends facilitate multiple management frameworks from vendors to remote manage generations of heterogeneous HPC clusters to complete one task. The heterogeneous and scaling out computing info structure made HPCC/grid administration even more challenging and time consuming than before. Management interoperability is usually compromised or absent due to the heterogeneous environment. In order to solve this problem for the long run and further reduce the total cost of ownership, industry is defining the systems management architecture for server hardware (SMASH) initiative. The SMASH initiative is a suite of specifications, which standardize management interfaces and remote management architecture for heterogeneous computing environments. The suite of specifications includes unified command line protocol, resource discovery, and resource addressing and data model profiles. SMASH not only addresses complicated administration challenges as well as enables hardware independent remote manageability plus computing info structure status/performance aware job scheduling schemes and as a result, will bring HPC clusters/grid utilization rates to an even higher level. This poster uses figures to illustrate the challenges, corresponding SMASH specifications and point out the potential research directions in supercomputing space over SMASH implementations	command-line interface;computer cluster;data model;hpcc;heterogeneous computing;image scaling;interoperability;job scheduler;scheduling (computing);server (computing);supercomputer;systems management;total cost of ownership	Yung-Chin Fang;Jenwei Hsieh	2005	2005 IEEE International Conference on Cluster Computing	10.1109/CLUSTR.2005.347081	communications protocol;parallel computing;real-time computing;systems management;data model;computer cluster;computer science;job scheduler;operating system;database;distributed computing;open system;scheduling;symmetric multiprocessor system;grid computing;computer network	HPC	-29.78641226204205	53.44152941946103	128381
7891e9e7d4bfa7bd61856a0e3b19b9b07f399627	performance analysis of a periodic data reorganization algorithm for concurrent blink-trees in database systems	database system;b link trees;algorithm analysis;b trees;database;periodic maintenance;concurrent maintenance;performance analysis;deadlocks	Tainan, Taiwan This paper develops a periodic data reorganization algorithm for the B ‘ink-tree data structure in concurrent environments, and identifies conditions under which the data reorganization should be performed in order to minimize the response time per access operation. Index Terms B*-trees, B”“k-tress, performance analysis, database, concurrent maintenance, periodic maintenance. algorithm analysis, deadlocks.	algorithm;analysis of algorithms;data structure;database;deadlock;profiling (computer programming);response time (technology);tree (data structure)	Ing-Ray Chen;Salah Hassan	1995		10.1145/315891.315901	b-tree;computer science;deadlock;data mining;database;distributed computing	DB	-20.03000355264044	47.22549993876062	128391
070c3a8c3ce10277424f23c01a54b377478ee59c	limplock: understanding the impact of limpware on scale-out cloud systems	virtualization;latency;cloud computing	"""We highlight one often-overlooked cause of performance failure: limpware -- """"limping"""" hardware whose performance degrades significantly compared to its specification. We report anecdotes of degraded disks and network components seen in large-scale production. To measure the system-level impact of limpware, we assembled limpbench, a set of benchmarks that combine data-intensive load and limpware injections. We benchmark five cloud systems (Hadoop, HDFS, ZooKeeper, Cassandra, and HBase) and find that limpware can severely impact distributed operations, nodes, and an entire cluster. From this, we introduce the concept of limplock, a situation where a system progresses slowly due to the presence of limpware and is not capable of failing over to healthy components. We show how each cloud system that we analyze can exhibit operation, node, and cluster limplock. We conclude that many cloud systems are not limpware tolerant."""	apache hbase;apache hadoop;benchmark (computing);data-intensive computing;failure;floppy disk;scalability	Thanh Do;Mingzhe Hao;Tanakorn Leesatapornwongsa;Tiratat Patana-anake;Haryadi S. Gunawi	2013		10.1145/2523616.2523627	embedded system;latency;real-time computing;virtualization;simulation;cloud computing;computer science;engineering;operating system;computer network	OS	-21.747145303663277	55.7728694823309	128502
1d686b11fcc19d57ac7c2489a7fc58ad5100c438	cape: continuous query engine with heterogeneous-grained adaptivity	heterogeneous-grained adaptivity;continuous query engine	We present CAPE, our Continuous Adaptive Query Processing Engine, that is designed to efficiently evaluate continuous queries in highly dynamic stream environments with the following characteristics: (1) the input data may stream into the query engine at widelyvarying rates; (2) meta knowledge such as punctuations [10] may dynamically be embedded into data streams; (3) as queries are registered into or removed from the query engine, the computing resources available for processing an individual operator may vary greatly over time; (4) different users may impose different quality of service (QoS) requirements. In view of these uncertainties, no one unique optimization technique can be expected to always succeed. Correspondingly, CAPE employs an optimization framework with heterogeneous-grained adaptivity for effectively coping with such dynamic variations. In our demonstration, we will focus on the following novel features of the CAPE system: 1. Highly reactive query operators capable of exploiting metadata to reduce resource usage and to improve execution efficiency (intra-operator adaptivity). 2. Online query reoptimization and migration between sub-plans to continuously converge to the best possible plan given the current situation (plan-level adaptivity). 3. Adaptive operator scheduling and plan distribution among multiple machines (system-wide adaptivity). Our system differs from prior continuous query systems in several ways. The STREAM system [7], fo-	converge;embedded system;mathematical optimization;quality of service;requirement;scheduling (computing);stream (computing);system 7	Elke A. Rundensteiner;Luping Ding;Timothy M. Sutherland;Yali Zhu;Bradford Pielech;Nishant K. Mehta	2004			sargable;query optimization;query expansion;real-time computing;web query classification;computer science;database;rdf query language;web search query;world wide web	DB	-19.760656790211392	57.16426956592781	128582
08f138e09f3d4a1e5b16e3cfd6e7dc5ce3215176	first-class access for developing-world environments	internet access;developing region;developing regions;developing world;wan acceleration;web caching	"""Improvements in connectivity and the cost of laptops will soon enable widely-available Internet access in parts of the world where access ranged from rare to unavailable. While such steps represent tremendous progress, we believe that the next barrier to adoption will be to convince providers that the gap between """"good enough"""" access and first-class access, while quite large, is possible to bridge. This paper presents our views for the creation of a networking software stack tailored toward developing-world usage. With this stack, we believe that developing-world users will have a participatory Internet experience, similar to that of users in the US and Europe. The stack is also geared toward a low resource footprint, making it deployable in cost-sensitive environments. We have completed some portions of the stack, and believe that the total system represents a three to five year effort."""	internet access;laptop;principle of good enough	Vivek S. Pai;Anirudh Badam;Sunghwan Ihm;KyoungSoo Park	2010		10.1145/1853079.1853089	simulation;engineering;world wide web;computer security	Networks	-29.88510360125821	57.50660821025953	128742
b4f2e0ce8364ee4e36103eceaddf0c3382e661d7	configurable clouds		Hyperscale datacenter providers have struggled to balance the growing need for specialized hardware with the economic benefits of homogeneity. The Configurable Cloud datacenter architecture introduces a layer of reconfigurable logic (FPGAs) between the network switches and servers. This enables line-rate transformation of network packets, acceleration of local applications running on the server, and direct communication among FPGAs, at datacenter scale. This low latency, ubiquitous communication enables deployment of hardware services spanning any number of FPGAs to be used and shared quickly and efficiently by services of any scale throughout the datacenter. The authors deploy this design over a production server bed and show how it can be used to accelerate applications that were explicitly ported to FPGAs and support hardware-first services. It can even accelerate applications without any application-specific FPGA code being written. The Configurable Cloud architecture has been deployed at hyperscale in Microsoft's production datacenters worldwide.	data center;field-programmable gate array;file spanning;hyperscale;network switch;reconfigurable computing;server (computing);software deployment	Adrian M. Caulfield;Eric S. Chung;Andrew Putnam;Hari Angepat;Daniel Firestone;Jeremy Fowers;Michael Haselman;Stephen Heil;Matt Humphrey;Puneet Kaur;Joo-Young Kim;Daniel Lo;Todd Massengill;Kalin Ovtcharov;Michael Papamichael;Lisa Woods;Sitaram Lanka;Derek Chiou;Doug Burger	2017	IEEE Micro	10.1109/MM.2017.51	hyperscale;parallel computing;software deployment;real-time computing;latency (engineering);network switch;architecture;computer science;cloud computing;server;network packet	Networks	-28.323337783351878	57.206689176901904	128762
95843091667cfbb0d85710ee1630c4c5745c6325	class-dependent assignment in cluster-based servers	front end;simulation;empirical evidence;round robin;cluster based server;overall response;power law distribution;assignment policy	A cluster-based server consists of a front-end dispatcher and several back-end servers. The dispatcher receives incoming requests, and then assigns them to back-end servers for processing. Our goal is to devise an assignment policy that has good response time performance, and is practical to implement in that the amount of information used by the dispatcher is relatively small, so that the attendant computation and communication overheads are low. In contrast to extant assignment policies that apply the same assignment policy to all incoming jobs, our approach calls for the dispatcher to classify incoming jobs as long or short, and then use class-dependent assignment policies. Specifically, we propose a policy, called CDA (Class Dependent Assignment), where short jobs are assigned in Round-Robin manner as soon as they arrive, while long jobs are deferred and assigned only when a back-end server becomes idle. Furthermore, when processing a long job, a back-end server is not assigned any other jobs.Our approach is motivated by empirical evidence suggesting that the sizes of files traveling on the Internet follow power-law distributions, where long jobs constituting a small fraction of all incoming jobs actually account for a large fraction of the overall load. To gauge the performance of the proposed policy, we exercised it on empirical data traces measured at Internet sites serving the 1998 World Cup. Since the assignment of long jobs incurs computational overhead as well as extra communication overhead, we studied the performance of CDA as function of the fraction of jobs classified as long. Our study shows that classification of even a small fraction of jobs as long can have a profound impact on overall response time performance. More speciafically, our experimental results show that if less than 3% of the jobs are classified as long, then CDA outperforms traditional policies, such as Round-Robin, by two orders of magnitude. From an implementation viewpoint, these results support our contention that CDA-based assignment is a practical policy combining low overhead and greatly improved performance.	.cda file;augmented assignment;computation;internet;job stream;jumpstart our business startups act;overhead (computing);response time (technology);round-robin scheduling;server (computing);tracing (software)	Victoria Ungureanu;Benjamin Melamed;Phillip G. Bradford;Michael N. Katehakis	2004		10.1145/967900.968185	real-time computing;simulation;empirical evidence;computer science;pareto distribution;front and back ends;operating system;database;world wide web;computer security	Metrics	-23.112597624669537	58.879837337745855	128866
488032d342f766a77e60d6715eb81579eeacc71c	improving server utilization via resource-adaptive batch vms: poster		Public cloud data centers often suffer from low resource utilization [1]. To increase utilization, recent works have proposed running batch workloads next to customer VMs to leverage idle resources [6]. While effective, the key challenge here is interference - the performance degradation of the colocated customer VMs due to resource contention with batch workload VMs at the underlying host server [5].	colocation centre;data center;elegant degradation;interference (communication);openvms;resource contention;server (computing)	Seyyed Ahmad Javadi;Piyush Shyam Banginwar;Vaishali Chanana;Rashmi Narvekar;Mitesh Kumar Savita;Anshul Gandhi	2017		10.1145/3155016.3155025	workload;leverage (finance);business;resource management;cloud computing;real-time computing;idle	Metrics	-22.117786340868832	59.90480550476018	129125
15f70b21b814dddfd0796154544fd69c39ce3c71	distributed garbage collection by timeouts and backward inquiry	distributed objects distributed garbage collection backward inquiry garbage collection mechanism distributed systems distributed cyclic garbage global synchronization backward links local garbage collection remote garbage collection cyclic garbage collection referenceable timestamp propagation backtracking backward references prospero directory service;distributedobjects;storage management;garbage collection;timeouts;distributed object management;distributed garbage collection;backtracking;back tracing;large scale systems national security intersymbol interference object detection software maintenance computer errors error correction web sites web pages network servers;large scale distributed systems;time to live;backtracking storage management distributed object management;cyclic garbage	We present a practical and e cient garbage collection mechanism for large scale distributed systems The mechanism collects all garbage including distributed cyclic garbage without global synchronization or back ward links The primary method used for local and remote garbage collection is timeouts each object has a time to live and clients which have a link to an ob ject must refresh the target object within the time to live to guarantee that the link will remain valid For cyclic garbage collection objects suspected to be garbage are detected by last referenceable timestamp propaga tion and cyclic garbage is reclaimed by backward inquiry back tracing Since without additional overhead the information about backward references can be obtained during the refreshing process and since messages neces sary for cyclic garbage collection are bundled with the messages used for the refreshing communication com putation and storage overhead is minimized This mech anism has been implemented and evaluated on Prospero directory service and the performance results show that it works well for large scale distributed systems	directory service;distributed computing;distributed garbage collection;garbage collection (computer science);overhead (computing);tcp global synchronization;time to live	Sung-Wook Ryu;Eul-Gyu Im;B. Clifford Neuman	2003		10.1109/CMPSAC.2003.1245375	manual memory management;garbage;real-time computing;time to live;computer science;operating system;garbage in, garbage out;database;distributed computing;garbage collection;programming language;backtracking	OS	-22.33729171049557	48.973891128964816	129294
03cdfd51569f79e5c1e8b124d01915dd2d55a1a8	incorporating virtualization awareness in service monitoring systems	irrigation;virtualization awareness;web services virtualisation;monitoring system;servers;monitoring;physical computer;web services;physical computers;monitoring servers irrigation fires;fires;virtualization support virtualization awareness service monitoring systems physical computers server virtualization platforms physical computer monitoring logic;server virtualization platforms;monitoring logic;virtualisation;service monitoring systems;virtualization support	Traditional service monitoring systems (e.g., Nagios and Cacti) have been conceived to monitor services hosted in physical computers. With the recent popularization of server virtualization platforms (e.g., Xen and VMware), monitored services can migrate from a physical computer to another, invalidating the original monitoring logic. In this paper, we investigate which strategies should be used to modify traditional service monitoring systems so that they can still provide accurate status information even for monitored services that are constantly moving on top of a set of servers with virtualization support.	computer;virtual private server	Marcio Barbosa de Carvalho;Lisandro Zambenedetti Granville	2011	12th IFIP/IEEE International Symposium on Integrated Network Management (IM 2011) and Workshops	10.1109/INM.2011.5990704	web service;embedded system;full virtualization;real-time computing;computer science;operating system;hardware virtualization;irrigation;server;computer network	Arch	-31.857720548290338	57.81097639903818	129479
b2f82093a2b8a30d2f0fceb2f89d380561b6489e	testing scalability and performance of decentralized on-line social networks	simulation;load testing;on line social networks;decentralized	During the last years, a lot of research has been conducted in the field of Decentralized Online Social Networks (DOSNs). Due to limited dependency on a dedicated central-based infrastructure, such kind of social networking enhances the privacy of users' personal data and information in case of local attacks, yet on the other hand suffers in terms of scalability. In this paper, we propose a platform that enables the custom creation of a decentralized network configuration, along with all necessary modules for simulating the traffic of real-world online social networks and measuring in parallel the performance of several DOSN architectures across artificial data. The goal of our work is two fold since it i) verifies the benefits of the decentralized paradigm based on several conclusions drawn and ii) provides a platform that enables the provision of custom experimentation for researchers in the related field.	online and offline;personally identifiable information;programming paradigm;scalability;simulation;social network;world online	Konstantinos Goutsias;Georgios Spathoulas;Ioannis Anagnostopoulos	2016		10.1145/3003733.3003774	real-time computing;simulation;load testing;data mining;database;distributed computing;world wide web;decentralization;computer security	HPC	-30.325177849410935	60.17893944926176	129520
594d15ce9e3b13b4fba63aa98bac484f20217b35	an integrated cloud platform for rapid interface generation, job scheduling, monitoring, plotting, and case management of scientific applications	platform as a service paas;rapid application development rad;html cloud computing databases processor scheduling monitoring computational modeling;user interfaces cloud computing scheduling software architecture system monitoring systems analysis;population genetics platform as a service paas rapid application development rad scientific computing cloud computing;geophysics integrated cloud platform rapid interface generation job scheduling case management running job monitoring web interface ixp input execute plot style apps scientific platform for the cloud spc plotting capability system architecture design scientific workload population genetics turbulence physics dna analysis big data;scientific computing;cloud computing;population genetics	The Scientific Platform for the Cloud (SPC) presents a framework to support the rapid design and deployment of scientific applications (apps) in the cloud. It provides common infrastructure for running typical IXP (Input-execute-Plot) style apps, including: a web interface, post-processing and plotting capabilities, job scheduling, real-time monitoring of running jobs, and case manager. In this paper we (1) describe the design of the system architecture, (2) evaluate its applicability to a scientific workload, and (3) present a number of case studies which represent a wide variety of scientific applications including Population Genetics, Geophysics, Turbulence Physics, DNA analysis, and Big Data.	big data;cloud computing;job scheduler;job shop scheduling;job stream;real-time transcription;scheduling (computing);software deployment;systems architecture;turbulence;user interface;video post-processing	Wesley Brewer;William Scott;John Sanford	2015	2015 International Conference on Cloud Computing Research and Innovation (ICCCRI)	10.1109/ICCCRI.2015.24	real-time computing;simulation;cloud computing;computer science;operating system;population genetics	HPC	-28.942006853012842	53.126300054771946	129537
a2452e733c7c32bc5d1cb28da53a0abd37db4435	a component-based middleware for hybrid grid/cloud computing platforms	esb;soa;hpc;middleware for grid and cloud computing	Current solutions for hybrid grid/cloud computing have been developed to hide from heterogeneity, dynamicity, and distributed nature of resources. These solutions are, however, insufficient to support distributed applications with non-trivial communication patterns among processes or that are structured so as to reflect the organization of resources they are deployed onto. In this paper, we present a generic, adaptable, and extensible component-based middleware that seamlessly enables a transition of non-trivial applications from traditional grids to hybrid grid–cloud platforms. This middleware goes beyond the resolution of well-known technical challenges for multi-domain computing, as it offers mechanisms to exploit the hierarchical, heterogeneous, and dynamic natures of platforms. We validate its capabilities and versatility through two use cases: an Internet-wide federation of Distributed Service Buses and a runtime supporting DD high-performance computing in heterogeneous computing environments using programming that is similar to message-passing interface. Performance results show the efficiency and usefulness of our middleware and so contribute to promote research efforts geared toward flexible, on-demand information technology solutions. Copyright © 2012 John Wiley & Sons, Ltd.	amazon web services;ambiguous name resolution;cloud computing;component-based software engineering;concurrency control;dd (unix);distributed computing;event-driven programming;expect;google cloud messaging;heterogeneous computing;john d. wiley;message passing interface;middleware;overhead (computing);proactive parallel suite;routing;supercomputer;testbed	Elton Manias;Françoise Baude	2012	Concurrency and Computation: Practice and Experience	10.1002/cpe.2822	middleware;supercomputer;parallel computing;real-time computing;computer science;message oriented middleware;operating system;service-oriented architecture;middleware;database;distributed computing;utility computing;grid computing	HPC	-31.009836585829255	53.182958408318065	129656
73199e434ef9adca471e686b34f7a6a41c7e959e	on the root causes of cross-application i/o interference in hpc storage systems	parallel file systems;random access memory;degradation;exascale i o;performance evaluation;interference synchronization servers random access memory performance evaluation degradation benchmark testing;interference;cross application contention;storage management input output programs parallel processing;servers;i o stack components cross application i o interference hpc storage systems performance variability grid 5000 testbed application access pattern network components file system configuration backend storage devices;synchronization;interference exascale i o parallel file systems cross application contention;benchmark testing	As we move toward the exascale era, performance variability in HPC systems remains a challenge. I/O interference, a major cause of this variability, is becoming more important every day with the growing number of concurrent applications that share larger machines. Earlier research efforts on mitigating I/O interference focus on a single potential cause of interference (e.g., the network). Yet the root causes of I/O interference can be diverse. In this work, we conduct an extensive experimental campaign to explore the various root causes of I/O interference in HPC storage systems. We use microbenchmarks on the Grid'5000 testbed to evaluate how the applications' access pattern, the network components, the file system's configuration, and the backend storage devices influence I/O interference. Our studies reveal that in many situations interference is a result of bad flow control in the I/O path, rather than being caused by some single bottleneck in one of its components. We further show that interference-free behavior is not necessarily a sign of optimal performance. To the best of our knowledge, our work provides the first deep insight into the role of each of the potential root causes of interference and their interplay. Our findings can help developers and platform owners improve I/O performance and motivate further research addressing the problem across all components of the I/O stack.	experiment;flow control (data);infiniband;input/output;interference (communication);lustre;read-only memory;simulation;spatial variability;testbed	Orcun Yildiz;Matthieu Dorier;Shadi Ibrahim;Robert B. Ross;Gabriel Antoniu	2016	2016 IEEE International Parallel and Distributed Processing Symposium (IPDPS)	10.1109/IPDPS.2016.50	embedded system;synchronization;benchmark;parallel computing;real-time computing;degradation;computer science;operating system;distributed computing;interference;server;computer network	HPC	-21.812874620979073	55.71992385206381	129919
5e91f08ce88ad92fa3ac8187d6b062e1b414ba9f	decentralized collaborative editing platform	replicated data structure p2p distributed communication;overhead avoidance decentralized collaborative editing platform data convergence linear data structure unique identifier scheme indexed communication;collaboration convergence data structures real time systems sun mobile communication concurrent computing;p2p;text editing concurrency computers data structures groupware indexing;distributed communication;replicated data structure	We aim to designe decentralized collaborative editing system. Keeping in mind that such system becoming more complex and care taking, we take in account some important properties like out-of-order execution of concurrent operations and data convergence. In this paper, we introduce a novel linear data structure based on unique identifier scheme for indexed communication. These identifiers are still real numbers with a specific controlled pattern of precision. This noval scheme preserves data convergence, provides good performance and avoids overheads as compared to other available approaches. We test our approach by performing experiments and we implemented our idea successfully.	algorithm;case preservation;data structure;experiment;identifier;list of data structures;out-of-order execution;prototype;unique identifier;unique key	Mumtaz Ahmad;Abdessamad Imine	2015	2015 16th IEEE International Conference on Mobile Data Management	10.1109/MDM.2015.26	real-time computing;computer science;operating system;peer-to-peer;database;distributed computing;computer network	DB	-24.83660166548283	47.33197295727107	130524
54d9e8e4df6c1575131ffdf29de28b74db6a7e52	fault-tolerant process tracking in lego	fault tolerant;design and implementation;process migration	Abstract   In a system supporting process migration, a process can migrate from the host upon which it was created to a second or subsequent host during its lifetime. Should the process' execution-host or its creation-host terminate abnormally, two problems may arise. First, system resources on the creation-host (in the form of residual dependencies) may be wasted if the migrated process terminates without informing its creation-host. Second, interference between processes may result if the creation-host fails and restarts without terminating its migrated processes. This paper describes the design and implementation of a distributed, fault-tolerant algorithm for tracking migrated processes. The algorithm ensures at-most-once semantics and the conservation of system resources.	process (computing)	Larry Hughes;Glenn Stoddart	1995	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/0141-9331(96)82012-6	embedded system;fault tolerance;parallel computing;real-time computing;simulation;process migration;computer science;operating system;distributed computing;algorithm	EDA	-23.03208902245688	50.472753987671005	130705
93a0ed1633e42eee1eca73030a4553692a6379b8	analysis of hierarchical storage management: a model and case study			hierarchical storage management	Bruce McNutt	1997			database;hierarchical storage management;computer science	SE	-19.567220417148757	51.612261412836574	130974
cc1a7ad5539b2bae5cdb934ff4ab33909caabe3f	a framework for the autonomic management of multi-layer p2p overlays	organizational requirements autonomic management multilayer p2p overlays pervasive environments future internet networking infrastructures mobility users networking requirements diversity network virtualization techniques network infrastructures management mechanisms static solutions autonomic framework context information high level policy;internet;ubiquitous computing;peer to peer computing;context logic gates monitoring xml network topology topology mesh networks;virtualisation internet peer to peer computing ubiquitous computing;virtualisation	The advent of pervasive environments and the future Internet has brought on heterogeneity, large scale and dynamicity in current networking infrastructures, which nonetheless also promote flexibility, availability, adaptability and support for mobility. With the increasing diversity in the users' networking requirements, solutions based on network virtualization techniques such as P2P overlays emerge as promising approaches to create network infrastructures with desirable characteristics. Successful deployment of such P2P overlays necessitates however efficient management mechanisms. Taking into account that static solutions are not geared towards handling the dynamics of the considered environments, as well as the diversity of users' and applications' requirements, adaptive solutions become prominent. We present here the design of an autonomic framework to concurrently manage multiple P2P overlays built on top of pervasive environments, by utilising context information and high-level policies to guide their reconfiguration. Relevant organizational requirements and design issues are also highlighted.	algorithm;autonomic computing;functional requirement;future internet;high- and low-level;image resolution;layer (electronics);mesh networking;overhead (computing);peer-to-peer;pervasive informatics;policy-based design;requirement;scalability;simulation;software deployment	Apostolos Malatras;Fei Peng;Béat Hirsbrunner	2012	2012 Global Information Infrastructure and Networking Symposium (GIIS)	10.1109/GIIS.2012.6466659	computer science;distributed computing;world wide web;computer network	Networks	-30.10791049639186	57.51401299935677	131004
ad45fcc902527fca5e7d09f061e72e8d91cc5de7	a self-organized load balancing mechanism for cloud computing	virtualization;game theory;data center;load balancing;cloud computing;live migration	The growth in computer and networking technologies over the past decades established cloud computing as a new paradigm in information technology. The cloud computing promises to deliver cost-effective services by running workloads in a large scale data center consisting of thousands of virtualized servers. The main challenge with a cloud platform is its unpredictable performance. A possible solution to this challenge could be load balancing mechanism that aims to distribute the workload across the servers of the data center effectively. In this paper, we present a distributed and scalable load balancing mechanism for cloud computing using game theory. The mechanism is self-organized and depends only on the local information for the load balancing. We proved that our mechanism converges and its inefficiency is bounded. Simulation results show that the generated placement of workload on servers provides an efficient, scalable, and reliable load balancing scheme for the cloud data center. Copyright © 2016 John Wiley & Sons, Ltd.	cloud computing;data center;game theory;john d. wiley;load balancing (computing);programming paradigm;scalability;self-organization;simulation	Hadi Khani;Nasser Yazdani;Siamak Mohammadi	2017	Concurrency and Computation: Practice and Experience	10.1002/cpe.3897	game theory;data center;parallel computing;virtualization;cloud computing;computer science;load balancing;operating system;cloud testing;distributed computing;computer network	HPC	-20.368294273078497	59.21972039784765	131021
248ce2bc48fc9c92cc183cce53cdbd8ca0fb0f94	task assignment in a virtualized gpu enabled cloud		Cloud computing vendors are beginning to offer GPU based high performance computing as a service. One approach uses virtual machines (VM), running in a hypervisor like VMware vSphere, equipped with virtual GPUs like Nvidia's vGPU solution. In this approach, multiple VMs running concurrently can share a single GPU. The number of VMs that share the GPU can be configured by the user/system administrator. Further, VMs can be re-assigned to GPUs, if more than one is available, dynamically. This approach allows tasks/jobs that use GPUs to run in individual VMs guaranteeing isolation whilst sharing resources. In a typical cloud environment with multiple servers each with one or more GPUs, finding an efficient, fast solution to the problem of placing VMs (i.e. VM-placement) on GPUs and moving them around as needed is extremely important to achieve high throughput of tasks while maximizing server utilization and minimizing task wait times. In this paper, we present the simulator we built to compare different solutions to the problem of VM-placement together with some early results.	cloud computing;concurrent computing;graphics processing unit;hypervisor;job stream;server (computing);supercomputer;system administrator;throughput;virtual machine;while;x86 virtualization	Hari Sivaraman;Uday Kurkure;Lan Vu	2018	2018 International Conference on High Performance Computing & Simulation (HPCS)	10.1109/HPCS.2018.00143	operating system;throughput;hypervisor;virtual machine;shared resource;cloud computing;system administrator;computer science;server;supercomputer	HPC	-21.62787191112563	59.50551907104791	131161
32a3b41386fae24060be5ade321ac98a66278217	iebs ticketing protocol as answer to synchronization issue	storage system;real time;distributed computing system	In this paper we present a novel synchronization technology, which is designed for distributed computer systems that use replication as means of redundancy. Our mechanism was primarily created for Intelligent ExaByte Storage, which is a network storage system with multiuser access. The protocol is based on the concept of digital identity certificates and supports real-time update of replicas and annulment of operations that might have produced out-of-date results. It also guarantees that the number of replicas stays the same at all times during the system's performance.		Barbara Palacz;Tomasz Milos;Lukasz Dutka;Jacek Kitowski	2007		10.1007/978-3-540-68111-3_7	embedded system;parallel computing;real-time computing;computer science;operating system;computer data storage;distributed computing	Theory	-23.213125338683643	49.50973787018419	131250
4bd2fec76931c1360d1bb8b9cc5de4599a013bc4	decentralized resource management for a distributed mobile computing environment	mobile computer;resource manager		mobile computing	Jong-Kook Kim;Hwanho Yong	2010			computer science;resource management;distributed computing;mobile agent;mobile computing	HPC	-30.20635891016326	47.09915357417732	131279
154f36b741e8252c7eb8b71f8ceb9b909ab33a12	repeating history beyond aries	transaction management;information technology;concurrency control	In this paper, I describe first the background behind the development of the original ARIES recovery method, and its significant impact on the commercial world and the research community. Next, I provide a brief introduction to the various concurrency control and recovery methods in the ARIES family of algorithms. Subsequently, I discuss some of the recent developments affecting the transaction management area and what these mean for the future. In ARIES, the concept of repeating history turned out to be an important paradigm. As I examine where transaction management is headed in the world of the internet, I observe history repeating itself in the sense of requirements that used to be considered significant in the mainframe world (e.g., performance, availabilit y and reliabilit y) now becoming important requirements of the broader information technology community as well .	algorithm;algorithms for recovery and isolation exploiting semantics;computer data storage;concurrency (computer science);concurrency control;debugging;emergence;fast path;intellect;mainframe computer;mission critical;programming paradigm;requirement;shrink wrap contract;transaction processing;usability;vldb	C. Mohan	1999			optimistic concurrency control;parallel computing;real-time computing;isolation;distributed transaction;computer science;concurrency control;database;information technology	DB	-23.940272360628523	49.105514379227095	131300
5ddacc1148f084853e40509df2d4c399f74f89ed	the software architecture of a virtual distributed computing environment	application design and development;software libraries;application software;distributed processing;vdce runtime system virtual distributed computing environment software architecture grand challenge problems network computing problem solving environment high performance distributed computing wide area networks application editor application development environment application scheduler;distributed computing;application generators;software engineering;vdce;computer networks;high performance distributed computing;vdce runtime system;software architecture;application editor;grand challenge problems;virtual distributed computing environment;application generators wide area networks software engineering distributed processing;application development environment;distributed computing environment;metacomputing;software design;software architecture distributed computing application software computer networks costs distributed processing problem solving wide area networks software libraries software design;problem solving environment;wide area network;network computing;application scheduler;wide area networks;problem solving;task allocation	The requirements of grand challenge problems and the deployment of gigabit networks makes the network computing framework an attractive and cost effective computing environment with which to interconnect geographically distributed processing and storage resources. Our project, Virtual Distributed Computing Environment (VDCE), provides a problem-solving environment for high-performance distributed computing over wide area networks. VDCE delivers well-deened library functions that relieve end-users of tedious task implementations and also support reusability. In this paper we present the conceptual design of VDCE software architecture, which is deened in three modules: a) the Application Editor, a user-friendly application development environment that generates the Application Flow Graph (AFG) of an application; b) the Application Scheduler, which provides an eecient task-to-resource mapping of AFG; and c) the VDCE Runtime System, which is responsible for running and managing application execution and monitoring the VDCE resources.	distributed computing environment;emoticon;gigabit;grand challenges;problem solving environment;requirement;runtime system;software architecture;software deployment;software development kit;usability	Haluk Topcuoglu;Salim Hariri;Wojtek Furmanski;Jon Valente;Ilkyeun Ra;Dongmin Kim;Yoonhee Kim;Xue Bing;Baoqing Ye	1997		10.1109/HPDC.1997.622361	software architecture;application software;parallel computing;real-time computing;computer science;software design;operating system;distributed computing;distributed computing environment	HPC	-30.82748155047766	48.20938669478039	131348
da3245ec6d9ac487336944682611f450347bbf1c	bonfire: the clouds and services testbed	observability;portals;web services cloud computing program testing;testing;cloud testing;computer architecture;ease of use function bonfire multisite testbed clouds testbed services testbed cloud based applications distributed applications cloud offerings media e health electronic health environment manufacturing open access scheme observability function control function advanced cloud features function;program testing;monitoring;cloud computing testing monitoring portals observability computer architecture bandwidth;web services;bandwidth;control;services;experimentation;control cloud testing services experimentation observability;cloud computing	BonFIRE is a multi-site test bed that supports testing of Cloud-based and distributed applications. BonFIRE breaks the mould of commercial Cloud offerings by providing unique functionality in terms of observability, control, advanced Cloud features and ease of use for experimentation. A number of successful use cases have been executed on BonFIRE, involving industrial and academic users and delivering impact in diverse areas, such as media, e-health, environment and manufacturing. The BonFIRE user-base is expanding through its free, Open Access scheme, daily carrying out important research, while the consortium is working to sustain the facility beyond 2014.	analysis of algorithms;cloud computing;cloud testing;distributed computing;ecosystem;experiment;testbed;usability	Konstantinos Kavoussanakis;Alastair C. Hume;Josep Martrat;Carmelo Ragusa;Michael Gienger;Konrad Campowsky;Gregory van Seghbroeck;Constantino Vázquez;Celia Velayos;Frederic Gittler;Philip Inglesant;Giuseppe Carella;Vegard Engen;Michal Giertych;Giada Landi;David Margery	2013	2013 IEEE 5th International Conference on Cloud Computing Technology and Science	10.1109/CloudCom.2013.156	web service;real-time computing;observability;simulation;service;cloud computing;computer science;operating system;cloud testing;distributed computing;software testing;bandwidth;scientific control	Robotics	-32.751770903888676	55.35036119812371	131589
0a513ea95cf451bdbfbfaf05b737b2aae2c89af3	toward real-time, many-task applications on large distributed systems	distributed system;real time;worst case execution time;middleware;task scheduling;short period;task management;parallel applications;state transition	In the age of Grid, Cloud, volunteer computing, massively parallel applications are deployed over tens or hundreds of thousands of resources over short periods of times to complete immense computations. In this work, we consider the problem of deploying such applications with stringent real-time requirements. One major challenge is the server-side management of these tasks, which often number in tens or hundreds of thousands on a centralized server. In this work, we design and implement a real-time task management system for many-task computing, called RT-BOINC. The system gives low O(1) worst-case execution time for task management operations, such as task scheduling, state transitioning, and validation. We implement this system on top of BOINC, a common middleware for volunteer computing. Using micro and macro-benchmarks executed in emulation experiments, we show that RT-BOINC provides significantly lower worst-case execution time, and lessens the gap between the average and the worst-case performance compared with the original BOINC implementation.	boinc;best, worst and average case;centralized computing;cloud computing;computation;dspace;data structure;distributed computing;emulator;experiment;grid computing;interactive computing;many-task computing;memory management;middleware;real-time clock;real-time computing;real-time transcription;requirement;run time (program lifecycle phase);scheduling (computing);server (computing);server-side;shared memory;space–time tradeoff;task computing;volunteer computing;windows rt;worst-case execution time	Sangho Yi;Derrick Kondo;David P. Anderson	2010		10.1007/978-3-642-15277-1_35	parallel computing;real-time computing;computer science;boinc credit system;operating system;middleware;database;distributed computing;worst-case execution time	Embedded	-21.0352589639636	58.286952367315244	131838
ac1ddc1a22c1e2fced698e76360201dc7254d088	a new mechanism for deadlock detection and resolution in nested transactions	nested transaction;deadlock detection			Assmaa A. El-Sayed;Mohamed E. El-Sharkawi;Hossam S. Hassanein	1998			nested transaction;distributed computing;deadlock prevention algorithms;computer science	DB	-23.420953372099017	47.372134682880414	131860
156f39a413ae9d86050b77553bf0cb1d3754a8cf	grid portal solutions: a comparison of gridportlets and ogce	ogce;grid portal;gridsphere;grid computing environment;grid middleware;comparison;gridportlets	This paper will discuss two of the major Grid portal solutions, the Open Grid Computing Environments Collaboratory (OGCE) and GridPortlets, both of which provide basic tools that portal developers can use to interact with Grid middleware when designing their own custom or application specific Grid portals. We investigate and compare what each of these packages provides, discuss their advantages and disadvantages, and identify missing features vital for Grid portal development. The main purpose of this paper is to identify what current toolkits provide, reveal some of their limitations, and provide motivation for the evolution of Grid portal solutions. Application groups should find this paper useful in helping to choose an appropriate Grid portal toolkit for building their Grid portals rapidly in a flexible and modular way.	extensibility;grid computing;list of toolkits;middleware;portals;portlet;requirement;scalability	Chongjie Zhang;Ian Kelley;Gabrielle Allen	2007	Concurrency and Computation: Practice and Experience	10.1002/cpe.1112	human–computer interaction;semantic grid;computer science;database;world wide web;drmaa;grid computing	HPC	-32.336215870626816	52.036820783598145	131962
2c310b8c13cb1fd2b3cf531cf0777d737a47567c	towards the integration of a hpc build system in the cloud ecosystem		Once the Cloud computing matures, and the diversification of resources and levels at which they can be accessed, there is a growing need to identify specialized languages and technologies that can provide a high level of flexibility and transparency in accessing, managing, and utilizing these resources. In this context, the alignment of these capabilities with current developments, especially at topology, orchestration and management level, becomes a necessity. Such an implementation is usually based on a self-* approach, which is also suitable for supporting the migration of selected HPC applications to the cloud. Our research is based on a self-organizing, self-management approach and investigates the option of self-configuration, supported by the easybuild toolchain.		Ioan Dragan;Teodora Selea;Teodor-Florin Fortis	2017		10.1007/978-3-319-61566-0_87	distributed computing;orchestration (computing);toolchain;cloud computing;computer science;transparency (graphic);ecosystem;diversification (marketing strategy)	HPC	-30.031231921899415	57.211541329738196	132015
2d95333f9087c252e5df0dffff61b4076785d5d1	enabling dynamic data centers with a smart bare-metal server platform	provisioning;discovery;cim;manageability standards;wbem;pre boot environment;bare metal;asset inventory;dynamic data center	Ever increasing data center complexity poses a significant burden on IT administrators. This burden can become unbearable without the help of self-managed systems that monitor themselves and automatically modify their state in order to carry out business processes according to high level objectives set by service level agreements (SLA) and policies. Among the key IT management tasks that must be automated and enhanced to realize the idea of an autonomic and highly dynamic data center, are discovery, configuration, and provisioning of new servers. In this direction, this paper describes pre-boot capabilities endowing the bare metal server with the ability to be discovered, queried, configured, and provisioned at time zero using industry standards like Common Information Model (CIM), CIM-XML, and Service Location Protocol (SLP). The capabilities are implemented as a payload of an Intel® Extensible Firmware Interface (EFI)-compliant BIOS, the Intel® Rapid Boot Toolkit (IRBT), allowing a resource manager to discover a new server during pre-boot, possibly in a bare-metal state, and then perform an asset inventory, configure the server including CPU-specific settings, and provision it with the most appropriate image. All these tasks may be carried out based on decisions taken by the resource manager according to server capabilities, application requirements, SLAs, and high-level policies. Additionally, this system uses reliable protocols, thus minimizing error possibilities. Future work is proposed, including the integration of a persistent hypervisor for enhanced management capabilities.	apple–intel architecture;autonomic computing;bios;bare machine;bare-metal server;billy cranston;booting;business process;central processing unit;computer-integrated manufacturing;data center;dynamic data;flash memory;high- and low-level;high-level programming language;hypervisor;information model;megabyte;out of the box (feature);payload (computing);preboot execution environment;provisioning;rapid boot;requirement;semiconductor consolidation;server (computing);service-level agreement;superword level parallelism;unified extensible firmware interface;xml	Arzhan Kinzhalin;Rodolfo Kohn;David Lombard;Ricardo Morin	2010	Cluster Computing	10.1007/s10586-010-0125-8	parallel computing;real-time computing;computer science;operating system;computer security;provisioning;computer network	OS	-30.886044438139166	54.59859559001792	132043
a810e29e27a6617fd150580f85e67c833f12c861	securing virtualized datacenters		Virtualization is a very popular solution to many problems in datacenter management. It offers increased utilization of existing system resources through effective consolidation, negating the need for more servers and additional rack space. Furthermore, it offers essential capabilities in terms of disaster recovery and potential savings on energy and maintenance costs. However, these benefits may be tempered by the increased complexities of securing virtual infrastructure. Do the benefits of virtualization outweigh the risks? In this study, the authors evaluated the functionalities of the basic components of virtual datacenters, identified the major risks to the data infrastructure, and present here several solutions for overcoming potential threats to virtual infrastructure.	data center;data infrastructure;disaster recovery;semiconductor consolidation	Timur Mirzoev;Baijian Yang	2014	CoRR		real-time computing;simulation;computer security	OS	-28.27599238470876	58.67423308825238	132275
4cea446d8edc8c0b434c5334b4df9f3b89bcdfcd	distributed geographical model service based on wsrf	geospatial information technology;cyberspace;service framework;wsrf;resource allocation;geographical model;resource management;distributed heterogeneous geographical model sharing service;geographic information systems distributed computing problem solving resource management embedded computing grid computing joining processes web services computer science education information technology;cyberspace distributed heterogeneous geographical model sharing service wsrf grid computer technology geospatial information technology resource sharing;service framework geographical model wsrf resource sharing;web services geographic information systems grid computing resource allocation;computational modeling;geographic information systems;resource sharing;web services;subscriptions;grid computer technology;grid computing;object oriented modeling;problem solving;data models	Numerous geographical models distributed in different geographical locations or cyberspaces lead to difficulties for sophisticated geographical problems solving due to the heterogeneity and decentralization. Grid computer technology provides a new way to solve problems in geographical sharing and service based on geospatial information technology. The method for understanding, describing and representing geographical models and geographical model service based on WSRF are discussed in this paper, and distributed geographical model sharing service framework is proposed. A prototype of distributed heterogeneous geographical models service platform is developed, and some experiments are implemented to examine the proposed methods. The proposed platform can provide a geographical resource sharing environment for geographical problem solving and geographical experimental in cyberspace.	computer;cyberspace;experiment;problem solving;prototype	Hongjun Su;Guonian Lv;Yongning Wen;Yehua Sheng	2009	2009 International Conference on Scalable Computing and Communications; Eighth International Conference on Embedded Computing	10.1109/EmbeddedCom-ScalCom.2009.78	computer science;knowledge management;database;world wide web	HPC	-32.337492778372706	49.86486978480601	132290
8878d0ad6b205952302044528eb8a3eacc828a21	electronic mail in a distributed heterogeneous system	electronic mail;heterogeneous systems		email	Stig Östholm	1992	Future Generation Comp. Syst.	10.1016/0167-739X(92)90047-F	computer science	Arch	-29.836802445049045	47.15595547139652	132423
e2ac386db18d7752323039fd567af7070a1c319a	hard disk management standards in a networked environment	hard disk management standard;networked environment	A campus wide network based on AT&T’s ISN (Information System Network) provides the university community with access to a variety of computing resources from any classroom, lab, office, or residence hall room equipped with a telephone jack. There are about 250 student workstations distributed in various labs around campus, all of which are connected to a LAN and have access to larger processors attached to the ISN. Nearly all faculty members have a workstation on their desktop, and the vast majority of these also have LAN and ISN access.	baton;backup;central processing unit;desktop computer;fastback;fifth generation computer;forge;hard disk drive;information system;international relations and security network;logical disk manager;public-domain software;rouge (metric);technical support;workstation	Paul Reince	1990		10.1145/99186.99257	control engineering;real-time computing;distributed computing	Web+IR	-27.380950620166363	51.382434112324034	132583
7bd4a7affdca9240a52d880b5e52e79f4e4cf57f	self-managing pervasive computing	ubiquitous computing fault tolerant computing;pervasive computing;tutorials software engineering java conferences pervasive computing educational institutions complexity theory;software administration self managing pervasive computing autonomic computing;autonomic computing;pervasive computing autonomic computing	The focus of this tutorial is to provide a synopsis of self-managing computing also known as Autonomic Computing. In doing so, we will introduce the techniques that enable computer systems to manage themselves so as to minimise the need for human input. This will also discuss how self-managing systems can address some of the issues resulting from the ever-increasing complexity of software administration and the growing difficulty encountered by software administrators in performing their job effectively.	autonomic computing;software development;ubiquitous computing;video synopsis	Philippe Lalanda;Julie A. McCann;Ada Diaconescu	2014	2014 IEEE Eighth International Conference on Self-Adaptive and Self-Organizing Systems Workshops	10.1109/SASOW.2014.42	context-aware pervasive systems;cloud computing;computer science;artificial intelligence;end-user computing;distributed computing;utility computing;grid computing;autonomic computing	HPC	-32.14514295140814	56.139107869416875	132642
4d284fde954fe867d2da02d07f9abba6cfda6aa9	cepaas: complex event processing as a service	container management systems;multi cloud;complex event processing;software as a service	Complex Event Processing (CEP) is a technology for performing continuous operations on fast and distributed streams of data. By using CEP, companies can obtain real-time insights, create competitive advantage, and, ultimately unlock the potential of Big Data. Nevertheless, despite this recent surge of interest, the CEP market is still dominated by solutions that are costly and inflexible or too low-level and hard to operate. To overcome these adoption barriers, this research proposes the creation of a CEP as a Service (CEPaaS) system to provide CEP functionalities to users together with the advantages of the Software as a Service (SaaS) model, such as no up-front investment and low maintenance cost. To ensure the success of such a system, however, many complex requirements must be satisfied, such as low latency processing, fault tolerance, and query execution isolation. To satisfy these requirements, this paper also presents an architecture and implementation for this CEPaaS system based on three main pillars: multi-cloud architecture, container management systems, and extensible multi-tenant design. Experimental results demonstrate that the proposed system achieves the goal of offering CEP functionalities as a scalable and fault-tolerant service.	big data;cloud computing;complex event processing;fault tolerance;geographic coordinate system;high- and low-level;internet;memory management;multitenancy;real-time clock;requirement;sim lock;scalability;software as a service	Wilson A. Higashino;Miriam A. M. Capretz;Luiz Fernando Bittencourt	2017	2017 IEEE International Congress on Big Data (BigData Congress)	10.1109/BigDataCongress.2017.31	real-time computing;complex event processing;operating system;software as a service;database	DB	-28.86956423582617	58.56764852316493	132765
f8399d757ddd9824ec2e12c3d5c333bf6b09ffa9	self-adapting resource bounded distributed computations	distributed processor resource delivery;resource allocation;distributed processing;distributed computing;inference mechanisms;resource knowledge based reasoning;cyberorgs model;adaptive algorithm;distributed computing concurrent computing encapsulation computer networks contracts computer science communication system control prototypes open systems availability;concurrency control;cyberorgs resource awareness;distributed concurrent computations;control mechanism self adapting resource bounded distributed computations cyberorgs model distributed concurrent computations cyberorgs resource awareness distributed processor resource delivery resource knowledge based reasoning;self adapting resource bounded distributed computations;resource allocation concurrency control distributed processing inference mechanisms;control mechanism	Self-adaptation is about computations adapting to their environments. The need for adaptation may dynamically arise as a result of evolving computations or the environment. An important part of the environment is the computational resources for which computations compete. The CyberOrgs model encapsulates distributed concurrent computations along with the computational and communication resources they require plus purchasing power for acquiring additional resources. Ownership of resources coupled with an effective control mechanism creates a predictable resource environment for computations to execute in - in a coordinated manner. CyberOrgs create three opportunities for self- adaptation: algorithms may be chosen using resource knowledge, additional resources may be purchased to adapt to evolving needs, and computations may coordinate use of known computational and network resources for optimal results. The CyberOrgs model is presented and a prototype implementation is described. Our experience with using CyberOrgs' resource awareness for hierarchical coordination of distributed processor resource delivery is presented. Experimental results show that resource knowledge based reasoning leads to efficient distributed adaptation.	algorithm;computation;computational resource;distributed computing;prototype;purchasing	Nadeem Jamali;Xinghui Zhao	2007	First International Conference on Self-Adaptive and Self-Organizing Systems (SASO 2007)	10.1109/SASO.2007.49	real-time computing;resource allocation;computer science;theoretical computer science;concurrency control;distributed computing	HPC	-26.76465829093581	48.4617801697072	132788
c4b72245dff3c1400676d1a3fa6c654849070f12	measuring teragrid: workload characterization for a high-performance computing federation	workload;workload characterization;teragrid;metrics;incomplete information;analyses;high performance computer;patterns;grid system	TeraGrid has deployed a significant monitoring and accounting infrastructure in order to understand its operational success. In this paper, we present an analysis of the jobs reported by TeraGrid for 2008. We consider the workload from several perspectives: traditional high-performance computing (HPC) workload characteristics; grid-oriented workload characteristics; and finally userand group-oriented characteristics. We use metrics reported in prior studies of HPC and grid systems in order to understand whether such metrics provide useful information for managing and studying resource federations. This study highlights the importance of distinguishing between analyses of job patterns and work patterns; that small sets of users dominate the workload both in terms of job and work patterns; and that aggregate analyses across even loosely coupled federations, with incomplete information for individual systems, reflect patterns seen in more tightly coupled grids and in single HPC systems.	aggregate data;loose coupling;supercomputer;teragrid	David L. Hart	2011	IJHPCA	10.1177/1094342010394382	real-time computing;computer science;operating system;database;distributed computing;pattern;metrics;complete information	HPC	-22.96144908661196	57.953473530500034	132862
cb6719dd6a380cff3a4f60026e469778803bdd3d	taking omid to the clouds: fast, scalable transactions for real-time cloud analytics		Transactional API over NoSQL key value Client Library + Runtime Service Open source – Apache incubator Implemented over Apache HBase Snapshot Isolation consistency Highly Available Originally optimized for throughput Secondary index creation When a new index is created, two coprocessors are installed: A. Populates new index with history B. Maintains index by augmenting write to table with a write to index	apache hbase;application programming interface;attribute–value pair;cloud analytics;coprocessor;nosql;population;real-time transcription;snapshot isolation;throughput	Ohad Shacham;Yonatan Gottesman;Aran Bergman;Edward Bortnikov;Eshcar Hillel;Idit Keidar	2018	PVLDB	10.14778/3229863.3229868	database;data mining;scalability;computer science;cloud computing;analytics	DB	-30.12784642538789	54.07363723198859	132865
4637fb3ee40177e71c668237cde74d2804c9c004	autonomous orchestration of distributed discrete event simulations in the presence of resource uncertainty	neural networks;checkpointing;fault tolerance;distributed discrete event simulation;prediction	Discrete event simulations model the behavior of complex, real-world systems. Simulating a wide range of events and conditions provides a more nuanced model, but also increases its computational footprint. To manage these processing requirements in a scalable manner, discrete event simulations can be distributed across multiple computing resources. Orchestrating the simulations in a distributed setting involves coping with resource uncertainty. We consider three key aspects of resource uncertainty: resource failures, heterogeneity, and slowdowns. Each of these aspects is managed autonomously, which involves making accurate predictions of future execution times and latencies while also accounting for differences in hardware capabilities and dynamic resource consumption profiles. Further complicating matters, individual tasks within the simulation are stateful and stochastic, requiring inter-task communication and synchronization to produce accurate outcomes. We deal with these challenges through intelligent state collection and migration, active resource monitoring, and empirical evaluation of resource capabilities under changing conditions. To underscore the viability of our solution, we provide benchmarks using a production discrete event simulation that can simultaneously sustain failures, manage resource heterogeneity, and handle slowdowns while being orchestrated by our framework.	autonomous robot;benchmark (computing);computation;computer simulation;inter-process communication;requirement;scalability;state (computer science);stochastic process;world-system	Zhiquan Sui;Matthew Malensek;Neil Harvey;Shrideep Pallickara	2015	TAAS	10.1145/2746345	fault tolerance;real-time computing;simulation;prediction;computer science;operating system;distributed computing;artificial neural network;statistics	HPC	-24.18803651914392	59.614802890105786	132874
4363078843fe55976fed881f90b1fb950b566f1e	dqmp: a decentralized protocol to enforce global quotas in cloud environments	scalable quota-enforcement protocol;clouds free company;paas cloud customer;decentralized protocol;global quota;unused resource quota;costly use;careless use;dynamically scalable;financial loss;involuntary resource usage;cloud environment;cpu cycle	Platform-as-a-Service (PaaS) clouds free companies of building infrastructures dimensioned for peak service demand and allow them to only pay for the resources they actually use. Being a PaaS cloud customer, on the one hand, offers a company the opportunity to provide applications in a dynamically scalable way. On the other hand, this scalability may lead to financial loss due to costly use of vast amounts of resources caused by program errors, attacks, or careless use. To limit the effects of involuntary resource usage, we present DQMP, a decentralized, fault-tolerant, and scalable quota-enforcement protocol. It allows customers to buy a fixed amount of resources (e. g., CPU cycles) that can be used flexibly within the cloud. DQMP utilizes the concept of diffusion to equally balance unused resource quotas over all processes running applications of the same customer. This enables the enforcement of upper bounds while being highly adaptive to all kinds of resourcedemand changes. Our evaluation shows that our protocol outperforms a lease-based centralized implementation in a setting with 1,000 processes.	apollonian network;central processing unit;centralized computing;cloud computing;customer relationship management;data center;distributed computing;fault tolerance;instruction cycle;platform as a service;process (computing);prototype;scalability;virtual machine	Johannes Behl;Tobias Distler;Rüdiger Kapitza	2012		10.1007/978-3-642-33536-5_21	simulation;operations management;business;computer security	OS	-29.377657442869825	59.320464465361276	133073
b023ad8c7538ff21c53809dfc099bf0a32946108	improving data availability for a cluster file system through replication	replication;storage system;performance evaluation;computer crashes;large scale cluster file system;availability;data availability improvement;relaxed consistency model;file systems genetic mutations computer crashes large scale systems availability peer to peer computing delay bandwidth laboratories computer architecture;concurrent conflict mutations;failure free execution data availability improvement large scale cluster file system replication lionfs relaxed consistency model concurrent conflict mutations;computer architecture;lionfs;large scale;file system;bandwidth;genetic mutations;relaxed consistency;very large databases;peer to peer computing;mechanism design;failure free execution;very large databases replicated databases;replicated databases;file systems;large scale systems	Data availability is a challenging issue for large- scale cluster file systems built upon thousands of individual storage devices. Replication is a well-known solution used to improve data availability. However, how to efficiently guarantee replicas consistency under concurrent conflict mutations remains a challenge. Moreover, how to quickly recover replica consistency from a storage server crash or storage device failure is also a tough problem. In this paper, we present a replication-based data availability mechanism designed for a large-scale cluster file system prototype named LionFS. Unlike other replicated storage systems that serialize replica updates, LionFS introduces a relaxed consistency model to enable concurrent updating all replicas for a mutation operation, greatly reducing the latency of operations. LionFS ensures replica consistency if applications use file locks to synchronize the concurrent conflict mutations. Another novelty of this mechanism is its light-weight log, which only records failed mutations and imposes no overhead on failure-free execution and low overhead when some storage devices are unavailable. Furthermore, recovery of replica consistency needs not stop the file system services and running applications. Performance evaluation shows that our solution achieves 50-70% higher write performance than serial replica updates. The logging overhead is shown to be low, and the recovery time is proportional to the amount of data written during the failure.	clustered file system;consistency model;file server;file sharing;lock (computer science);overhead (computing);performance evaluation;prototype;serialization;server (computing)	Jin Xiong;Jianyu Li;Rongfeng Tang;Yiming Hu	2008	2008 IEEE International Symposium on Parallel and Distributed Processing	10.1109/IPDPS.2008.4536154	mechanism design;availability;replication;parallel computing;real-time computing;computer science;operating system;database;distributed computing;bandwidth;statistics;computer network	OS	-21.888176008335215	49.23324408579765	133160
6e4e3e6e5fe6d3c61962169233956bc1a1ac49a9	on replay detection in distributed systems	network operating systems distributed processing fault tolerant computing;distributed system;network operating systems;distributed processing;time window;clocks synchronization nonvolatile memory cryptography educational institutions propagation losses servers communication system security data security degradation;fault tolerant computing;nonvolatile memory;legitimate client requests second chance approach guaranteed continuous service availability service request messages replay detection distributed systems variable size time window mechanism challenge mechanism memory buffer size;clock synchronization	Various approaches to the problem of replay detection in distributed systems are briefly reviewed. A new approach is proposed based on combining a variable-size timewindow mechanism with a challenge mechanism. This a p proach has the properties that: (1) i t does not depend on clock synchronization, (2) i t allows the setting of a minimum server’s memory-buffer size in a way that ensures acceptance of all legitimate client requests, (3) i t is robust without requiring stable (non-volatile) memory for the server buffer needed to save past client requests.	clock synchronization;distributed computing;non-volatile memory;server (computing)	Shyh-Wei Luan;Virgil D. Gligor	1990		10.1109/ICDCS.1990.89271	clock synchronization;embedded system;real-time computing;non-volatile memory;computer science;operating system;database;distributed computing;computer security;computer network	Embedded	-24.757016616756125	50.024923466712	133281
bc709200f2343f04da8e00bd7202e5791ca2dcad	pragmatic development of service based real-time change data capture	data protection;confidentiality	This thesis makes a contribution to the Change Data Capture (CDC) field by providing an empirical evaluation on the performance of CDC architectures in the context of realtime data warehousing. CDC is a mechanism for providing data warehouse architectures with fresh data from Online Transaction Processing (OLTP) databases. There are two types of CDC architectures, pull architectures and push architectures. There is exiguous data on the performance of CDC architectures in a real-time environment. Performance data is required to determine the real-time viability of the two architectures. We propose that push CDC architectures are optimal for real-time CDC. However, push CDC architectures are seldom implemented because they are highly intrusive towards existing systems and arduous to maintain. As part of our contribution, we pragmatically develop a service based push CDC solution, which addresses the issues of intrusiveness and maintainability. Our solution uses Data Access Services (DAS) to decouple CDC logic from the applications. A requirement for the DAS is to place minimal overhead on a transaction in an OLTP environment. We synthesize DAS literature and pragmatically develop DAS that eciently execute transactions in an OLTP environment. Essentially we develop effeicient RESTful DAS, which expose Transactions As A Resource (TAAR). We evaluate the TAAR solution and three pull CDC mechanisms in a real-time environment, using the industry recognised TPC-C benchmark. The optimal CDC mechanism in a real-time environment, will capture change data with minimal latency and will have a negligible affect on the database's transactional throughput. Capture latency is the time it takes a CDC mechanism to capture a data change that has been applied to an OLTP database. A standard definition for capture latency and how to measure it does not exist in the field. We create this definition and extend the TPC-C benchmark to make the capture latency measurement. The results from our evaluation show that pull CDC is capable of real-time CDC at low levels of user concurrency. However, as the level of user concurrency scales upwards, pull CDC has a significant impact on the database's transaction rate, which affirms the theory that pull CDC architectures are not viable in a real-time architecture. TAAR CDC on the other hand is capable of real-time CDC, and places a minimal overhead on the transaction rate, although this performance is at the expense of CPU resources.		Mitchell John Eccles	2013			embedded system;real-time computing;computer science;database	Embedded	-20.88910565278812	54.88679281850333	133295
f246a93e375f670c25eb6f271bc6741eb3a7b9dd	the need to move toward virtualized and more resilient disaster-recovery architectures	disasters mission critical systems computer security finance virtualization resilience	Due to growing concerns around natural disasters, information technology (IT) complexity, increasing cyber-attacks, and the sensitivity of financial systems such that corporations may lose millions of dollars per minute if key business processes are not available, corporations are finding the need to develop more resilient disaster-recovery (DR) architectures. For many years, corporations have critical business functions that have relied on tape methods for DR. However, due to pressures from regulatory groups such as the FFIEC (Federal Financial Insurance Examination Council), there is a growing requirement to recover business functions faster than offered by tape solutions. As a result, application owners are challenged with more aggressive recovery-time objectives that necessitate the development of recovery solutions that offer a faster, near-continuous recovery of critical business function. However, moving mission-critical workloads from tape to a near-continuous method can be very expensive. This is true when dealing with legacy, multisite, heterogeneous workloads that have a business process and governance model that prevents workloads from moving to a cloud-computing model. Nevertheless, to offset DR costs, emerging technologies such as cloud computing and virtualization can be used, along with existing underutilized server capacity, to form effective and affordable DR solutions that can accommodate heterogeneous and legacy workloads.	architectural pattern;asynchronous i/o;business process;cloud computing;complexity;criticality matrix;disaster recovery;load balancing (computing);mission critical;puresystems;recovery time objective;requirement;self-organized criticality;server (computing);software deployment	E. Bartholomy;G. Greenlee;M. Sylvia	2013	IBM Journal of Research and Development	10.1147/JRD.2013.2258759	simulation;telecommunications;computer science;engineering;operating system;computer security	DB	-28.335751725642204	58.195703952326745	133403
72ff8fee735c6c914dc82fe2a9e59fe52d72e32e	sharing a conceptual model of grid resources and services	cluster computing;storage management;common information model;conceptual model;grid middleware;conceptual schema;design guideline;high energy physics;resource sharing;information service;domain specificity;wide area network;problem solving	Grid technologies aim at enabling a coordinated resource-sharing and problem-solving capabilities over local and wide area networks and span locations, organizations, machine architectures and software boundaries. The heterogeneity of involved resources and the need for interoperability among different grid middlewares require the sharing of a common information model. Abstractions of different flavors of resources and services and conceptual schemas of domain specific entities require a collaboration effort in order to enable a coherent information services cooperation. With this paper, we present the result of our experience in grid resources and services modelling carried out within the Grid Laboratory Uniform Environment (GLUE) effort, a joint US and EU High Energy Physics projects collaboration towards grid interoperability. The first implementation-neutral agreement on services such as batch computing and storage manager, resources such as the hierarchy cluster, sub-cluster, host and the storage library are presented. Design guidelines and operational results are depicted together with open issues and future evolutions.	batch processing;coherence (physics);coherent information;entity;information model;interoperability;problem solving;refinement (computing);requirement	Sergio Andreozzi;Massimo Sgaravatto;Maria Cristina Vistoli	2003	CoRR		shared resource;parallel computing;computer cluster;computer science;conceptual schema;conceptual model;operating system;data mining;database;distributed computing;world wide web;grid computing	HPC	-30.560489637167965	50.40880782513953	133635
234d05c7522379a91f594b94c1cd0c32f3922413	remote profiling of resource constraints of web servers using mini-flash crowds	high availability;resource constraint;resource manager;flash crowds	Unexpected surges in Web request traffic can exercise server-side resources (e.g., access bandwidth, processing, storage etc.) in undesirable ways. Administrators today do not have requisite tools to understand the impact of such “flash crowds” on the their servers. Most Web servers either rely on over-provisioning and admission control, or use potentially expensive solutions like CDNs, to ensure high availability in the face of flash crowds. A more fine-grained understanding of the performance of individual server resources under emulated but realistic and controlled flash crowd-like conditions can aid administrators to make more efficient resource management decisions. In this paper, we present miniflash crowds (MFC) – a light-weight profiling service that reveals resource bottlenecks in a Web server infrastructure. MFC uses a set of controlled probes where an increasing number of distributed clients make synchronized requests that exercise specific resources or portions of a remote Web server. We carried out controlled labbased tests and experiments in collaboration with operators of production servers. We show that our approach can faithfully track the impact of request loads on different server resources and provide useful insights to server operators on the constraints of different components of their infrastructure. We also present results from a measurement study of the provisioning of several hundred popular Web servers, a few hundred Web servers of startup companies, and about hundred phishing servers.	emulator;experiment;high availability;microsoft foundation class library;phishing;provisioning;server (computing);server-side;slashdot effect;the wisdom of crowds;web server	Pratap Ramamurthy;Vyas Sekar;Aditya Akella;Balachander Krishnamurthy;Anees Shaikh	2008			real-time computing;computer science;resource management;operating system;internet privacy;high availability;world wide web;client–server model;server;server farm	OS	-24.688527705942757	57.37583531133015	133637
e567d51ee78d207e17d2a66bdde07bd4ae2761ba	efficient distributed cq processing using peers.			conjunctive query	Wee Siong Ng;Beng Chin Ooi;Yanfeng Shu;Kian-Lee Tan;Wee Hyong Tok	2003			parallel computing;database;distributed computing	HPC	-27.953856387678083	47.19007645236637	133666
f684c4096e87d91b799a1e81cf495014935cdae2	an agent based platform for task distribution in virtual environments	context aware;multi agent system;mobile device;distributed agents;agent based;resource allocation;task distribution and execution;context aware middleware;mobile environment;middleware;source code;load balance;utility computing;virtual environment;mobile agent;point of view	This paper focuses on automatic mechanisms for task distribution and execution in virtual and mobile environments. In particular, the goal is the implementation of Utility Computing services that enable users to submit their source code and to have their applications executed without concerning about resource allocation, task distribution, and load-balancing. The proposed solution consists in a distributed agent-based software infrastructure that grants a high level of transparency from the user point of view. As a matter of fact, accordingly with the Utility Computing model, the user has just to submit its tasks and their input parameters; after that, the software infrastructure takes care of (1) encapsulating tasks in mobile agents; (2) distributing them in the virtual environment; (3) launching and controlling execution; (4) picking up results; and (5) handling computing stations. Finally, it is important to note that the infrastructure is able to integrate both fixed and mobile hardware resources to build a community of computing stations for task executions. As a consequence, such an infrastructure can get useful computing power even from stations (mobile devices) that have always been neglected by classic task execution platforms.	agent-based model;virtual reality	Antonio Coronato;Giuseppe De Pietro;Luigi Gallo	2008	Journal of Systems Architecture - Embedded Systems Design	10.1016/j.sysarc.2008.01.011	embedded system;real-time computing;resource allocation;computer science;virtual machine;load balancing;operating system;multi-agent system;middleware;mobile agent;mobile device;distributed computing;utility computing;source code	Embedded	-33.10870537837751	49.91482950894979	133670
669e1bc851c3cbf78eb52cea7a6ff28b4a6aa23b	crowdware: a framework for gpu-based public-resource computing with energy-aware incentive mechanism	volunteer computing;public resource computing;gpu crowd computing public resource computing volunteer computing energy aware incentive mechanisms;distributed processing;gpu;power aware computing cloud computing graphics processing units;computer applications;cloud computing online banking servers graphics processing units scalability computer applications distributed processing;servers;online banking;crowd computing;graphics processing units;scalability;sustainability crowdware energy aware incentive mechanism crowdsourced resource desktop computer laptop computer mobile computing device tablets smart phone cloud data center crowd sourced resources public resource computing project volunteer computing project seti home milkyway home crowdware sustainable gpu based public resource computing financial incentive mechanism auction based resource allocation algorithm profit based resource participation algorithm electricity cost profitability resource provider resource consumer md5 password recovery job amazon ec2 gpu thin server design;energy aware incentive mechanisms;cloud computing	The power of the crowd, more precisely crowdsourced resources, is in its ubiquity. Accounting for traditional desktop/laptop computers and recent mobile computing devices including tablets and smart phones far surpasses the number of servers in cloud data centers. Besides, the capacity and capability of these resources owned by the crowd (crowd-sourced resources) has increased dramatically with GPUs in particular. Although a myriad of public-resource (or volunteer) computing projects, including SETI@home and Milkyway@home, have attracted the participation of crowd-sourced resources at very large scale, the sustainability of such participation is in doubt due primarily to ever-increasing energy costs. In this paper, we present Crowdware, a framework for enabling sustainable GPU-based public-resource computing with a realistic financial incentive mechanism. To this end, we design an auction-based resource allocation algorithm and a profit-based resource participation algorithm, explicitly considering the electricity cost of participating resources. Our results show that Crowdware greatly promotes profitability and cost efficiency for resource providers and resource consumers, respectively. Specifically, Crowdware has enabled the execution of MD5 password recovery jobs, in our testbed, with only 2.2% of the cost of using Amazon EC2 GPU instances while the participation of crowd-sourced resources is profitable with an average profit rate of 9.2%. Crowdware also shows great scalability with its fat-client and thin-server design. Together, Crowdware significantly improves the sustainability of public-resource computing.	algorithm;amazon elastic compute cloud (ec2);cloud computing;cost efficiency;crowdsourcing;data center;data-intensive computing;desktop computer;fat client;graphics processing unit;laptop;md5;middleware;milkyway@home;mobile computing;password cracking;programming language;seti@home;scalability;server (computing);smartphone;software deployment;testbed;volunteer computing;web application	Zhongli Dong;Young Choon Lee;Albert Y. Zomaya	2015	2015 IEEE 7th International Conference on Cloud Computing Technology and Science (CloudCom)	10.1109/CloudCom.2015.73	scalability;simulation;cloud computing;computer science;operating system;distributed computing;utility computing;computer applications;world wide web;server	HPC	-25.263716299515586	59.810138727608674	133680
8810eddbebc2a23eee244d427d8a7cf7937e7f37	channel dependability of the atm communication network based on the multilevel distributed cloud technology		Air Traffic Management (ATM) systems represent essential infrastructure that is critical for flight safety. Voice communication system (VCS) on the base of embedded cloud technology is a vital part of modern air traffic control operations. In the paper the dependability of VCS based on the multilevel distributed cloud technology is discussed. Mathematical model of the channel reliability for this system is developed. Different strategies for operation of VCS hardware within the embedded clouds and boundary value of reliability parameters for automatics are analysed.	atm turbo;cloud computing;dependability	Igor Kabashkin	2017		10.1007/978-3-319-67642-5_49	telecommunications network;computer network;communications system;air traffic management;cloud computing;channel reliability;air traffic control;communication channel;computer science;dependability	HPC	-32.311642448081074	57.811793057652565	133716
4dcb113dea372648bbeed418845b237b31228964	architecture, workflows, and prototype for stateful data usage control in cloud	concurrent computing;security of data cloud computing;ucon;resource management distributed databases concurrent computing authorization data models process control;resource management;cloud computing stateful data usage control data objects distributed systems distributed enforcement access rule usage rule;usage control;attributes;concurrency control;process control;distributed databases;cloud system;authorization;concurrency control usage control ucon cloud system attributes;data models	This paper deals with the problem of continuous usage control of multiple copies of data objects in distributed systems. This work defines an architecture, a set of workflows, a set of policies and an implementation for the distributed enforcement. The policies, besides including access and usage rules, also specify the parties that will be involved in the decision process. Indeed, the enforcement requires collaboration of several entities because the access decision might be evaluated on one site, enforced on another, and the attributes needed for the policy evaluation might be stored in many distributed locations.	concurrency (computer science);distributed computing;entity;multiversion concurrency control;overhead (computing);prototype;reference architecture;serialization;state (computer science);stateful firewall;testbed	Aliaksandr Lazouski;Gaetano Mancini;Fabio Martinelli;Paolo Mori	2014	2014 IEEE Security and Privacy Workshops	10.1109/SPW.2014.13	data modeling;real-time computing;concurrent computing;computer science;resource management;concurrency control;process control;database;distributed computing;authorization;distributed concurrency control	Security	-25.166629467743835	48.335168001022765	133927
dd535d9c5530f69b99935c1a0719b84f2c4e1eb0	detecting performance anomalies in scientific workflows using hierarchical temporal memory		Technological advances and the emergence of the Internet of Things have lead to the collection of vast amounts of scientific data from increasingly powerful scientific instruments and a growing number of distributed sensors. This has not only exacerbated the significance of the analyses performed by scientific applications but has also increased their complexity and scale. Hence, emerging extreme-scale scientific workflows are becoming widespread and so is the need to efficiently automate their deployment on a variety of platforms such as high performance computers, dedicated clusters, and cloud environments. Performance anomalies can considerably affect the execution of these applications. Theymaybe caused by different factors including failures and resource contention and theymay lead to undesired circumstances such as lengthy delays in the workflow runtime or unnecessary costs in cloud environments. As a result, it is essential for modern workflow management systems to enable the early detection of this type of anomalies, to identify their cause, and to formulate and execute actions to mitigate their effects. In this work, we propose the use of Hierarchical Temporal Memory (HTM) to detect performance anomalies on real-time infrastructure metrics collected by continuously monitoring the resource consumption of executingworkflow tasks. The framework is capable of processing a stream ofmeasurements in an online and unsupervisedmanner and is successful in adapting to changes in the underlying statistics of the data. This allows it to be easily deployed on a variety of infrastructure platformswithout the need of previously collecting data and training a model. We evaluate our approach by using two real scientific workflows deployed inMicrosoft Azure’s cloud infrastructure. Our experiment results demonstrate the ability of our model to accurately capture performance anomalies on different resource consumption metrics caused by a variety of competing workloads introduced into the system. A performance comparison of HTM to other online anomaly detection algorithms is also presented, demonstrating the suitability of the chosen algorithm for the problem presented in this work. © 2018 Elsevier B.V. All rights reserved.	algorithm;anomaly detection;bioinformatics;cloud computing;computer;critical path method;distributed computing;emergence;html;hierarchical temporal memory;ibm websphere extreme scale;internet of things;job shop scheduling;makespan;microsoft azure;pegasus;provisioning;real-time clock;resource contention;scheduling (computing);sensor;software deployment;supercomputer;unsupervised learning	Maria Alejandra Rodriguez;Ramamohanarao Kotagiri;Rajkumar Buyya	2018	Future Generation Comp. Syst.	10.1016/j.future.2018.05.014	workflow management system;software deployment;real-time computing;cloud computing;anomaly detection;distributed computing;workflow;hierarchical temporal memory;computer science;scientific instrument;internet of things	HPC	-24.053474968609077	59.71558041801543	133954
b154433081be7713768565ab0ee9a988e04400ca	cloud computing: programming model and information exchange mechanism	cloud exchange;qos;programming model;social network;information exchange;internet application;task scheduling;cloud computing	Acting as an entirely new Internet application model, Cloud computing will be the leading way to access services and information in the near future. The thesis focuses on the definition of Cloud computing and its application occasions; puts forward six technical advantages on the basis of the beginning of its technical features; analyses the programming and task scheduling model according to the present-used cloud computing system. Examples are applied to explain the process of programming and its modifying directions, as well as the process within which services and resources exchange. For the explanation of Cloud computing, how social network may increase the Qos through changing the service load will be discussed. The development of Cloud computing is estimated in the end of the thesis.	cloud computing;information exchange;programming model;scheduling (computing);social network	Haoming Liang;Wenbo Chen;Kefu Shi	2011		10.1145/2071639.2071642	cloud computing security;real-time computing;quality of service;information exchange;cloud computing;computer science;theoretical computer science;operating system;cloud testing;distributed computing;utility computing;programming paradigm;social network	HPC	-28.804971206998225	58.2318325095282	133965
e5fd8f8f7d2a9a68862f04ef88c8d90a12dc03be	resilient cloud in dynamic resource environments		Traditional cloud stacks are designed to tolerate random, small-scale failures, and can successfully deliver highly-available cloud services and interactive services to end users. However, they fail to survive large-scale disruptions that are caused by major power outage, cyber-attack, or region/zone failures. Such changes trigger cascading failures and significant service outages. We propose to understand the reasons for these failures, and create reliable data services that can efficiently and robustly tolerate such large-scale resource changes.  We believe cloud services will need to survive frequent, large dynamic resource changes in the future to be highly available.  (1) Significant new challenges to cloud reliability are emerging, including cyber-attacks, power/network outages, and so on. For example, human error disrupted Amazon S3 service on 02/28/17 [2]. Recently hackers are even attacking electric utilities, which may lead to more outages [3, 6].  (2) Increased attention on resource cost optimization will increase usage dynamism, such as Amazon Spot Instances [1].  (3) Availability focused cloud applications will increasingly practice continuous testing to ensure they have no hidden source of catastrophic failure. For example, Netflix Simian Army can simulate the outages of individual servers, and even an entire AWS region [4].  (4) Cloud applications with dynamic flexibility will reap numerous benefits, such as flexible deployments, managing cost arbitrage and reliability arbitrage across cloud provides and datacenters, etc.  Using Apache Cassandra [5] as the model system, we characterize its failure behavior under dynamic datacenter-scale resource changes. Each datacenter is volatile and randomly shut down with a given duty factor.  We simulate read-only workload on a quorum-based system deployed across multiple datacenters, varying (1) system scale, (2) the fraction of volatile datacenters, and (3) the duty factor of volatile datacenters. We explore the space of various configurations, including replication factors and consistency levels, and measure the service availability (% of succeeded requests) and replication overhead (number of total replicas).  Our results show that, in a volatile resource environment, the current replication and quorum protocols in Cassandra-like systems cannot high availability and consistency with low replication overhead.  Our contributions include:  (1) Detailed characterization of failures under dynamic datacenter-scale resource changes, showing that the exiting protocols in quorum-based systems cannot achieve high availability and consistency with low replication cost.  (2) Study of the best achieve-able availability of data service in dynamic datacenter-scale resource environment.	amazon elastic compute cloud (ec2);amazon simple storage service;amazon web services;apache cassandra;cloud computing;consistency model;continuous testing;dvd region code;data center;denial-of-service attack;downtime;duty cycle;high availability;human error;mathematical optimization;overhead (computing);randomness;read-only memory;simulation;volatile memory	Fan Yang;Andrew A. Chien;Haryadi S. Gunawi	2017		10.1145/3127479.3132571	human error;cascading failure;catastrophic failure;computer science;real-time computing;cloud computing;data as a service;continuous testing;computer network;server;high availability	Networks	-23.889482046469375	59.930862603623105	133971
95c7d768850a4a4fc0b041b89e714310e3e14df8	drems-os: an operating system for managed distributed real-time embedded systems		Distributed real-time and embedded (DRE) systems executing mixed criticality task sets are increasingly being deployed in mobile and embedded cloud computing platforms, including space applications. These DRE systems must not only operate over a range of temporal and spatial scales, but also require stringent assurances for secure interactions between the system’s tasks without violating their individual timing constraints. To address these challenges, this paper describes a novel distributed operating system focusing on the scheduler design to support the mixed criticality task sets. Empirical results from experiments involving a case study of a cluster of satellites emulated in a laboratory testbed validate our claims.	central processing unit;cloud computing;criticality matrix;distributed computing;distributed operating system;embedded system;emulator;experiment;interaction;mixed criticality;mobile computing;open-source software;posix;process isolation;real-time clock;real-time operating system;real-time transcription;scheduling (computing);self-organized criticality;spatial scale;testbed	Abhishek Dubey;Gabor Karsai;Aniruddha S. Gokhale;William Emfinger;Pranav Srinivas Kumar	2017	2017 6th International Conference on Space Mission Challenges for Information Technology (SMC-IT)		real-time computing;distributed operating system;mixed criticality;cloud computing;embedded operating system;schedule;testbed;computer science	Embedded	-31.51631752466148	58.849774515855614	134427
8597387325d0603498791269c11307b05f6a2520	average message overhead of replica control protocols	failure resiliency;system recovery database theory distributed databases fault tolerant computing protocols;control systems;replicated data management;protocols;average message overhead;availability;system availability;communication link failures;binary trees;replica management protocols;fault tolerant computing;system recovery;replica management protocols average message overhead replica control protocols replicated data management node failures communication link failures failure resiliency performance penalty communication overhead system availability;permission;communication overhead;voting;replicated data;replica control protocols;link failure;performance analysis;distributed databases;protocols voting permission communication system control control systems educational institutions performance analysis computer science availability binary trees;computer science;communication system control;database theory;node failures;performance penalty	Management of replicated data has received considerable attention in the last few years. Several replica control schemes have been proposed which work in the presence of both node and communication link failures. However, this resiliency to failure inflicts performance penalty in terms of communication overhead incurred. Though the issue of performance of these schemes from the stand point of availability of the system has been well addressed, the issue of message overhead has been limited to the analysis of worst case and best case message bounds. In this paper we compare several well known replica management protocols in terms of their average case message overhead.	algorithm;best, worst and average case;limbo;overhead (computing)	Debanjan Saha;Sampath Rangarajan;Satish K. Tripathi	1993		10.1109/ICDCS.1993.287677	communications protocol;availability;database theory;real-time computing;voting;binary tree;computer science;control system;database;distributed computing;distributed database;computer security;computer network	DB	-22.269026214772918	49.035415224583495	134443
c14be4a4a589b22279510e890b3c53760c80dfce	elastic database systems		Distributed on-line transaction processing (OLTP) database management systems (DBMSs) are a critical part of the operation of large enterprises. These systems often serve time-varying workloads due to daily, weekly or seasonal fluctuations in load, or because of rapid growth in demand due to a company’s business success. In addition, many OLTP workloads are heavily skewed to “hot” tuples or ranges of tuples. For example, the majority of NYSE volume involves only 40 stocks. To manage such fluctuations, many companies currently provision database servers for peak demand. This approach is wasteful and not resilient to extreme skew or large workload spikes. To be both efficient and resilient, a distributed OLTP DBMS must be elastic; that is, it must be able to expand and contract its cluster of servers as demand fluctuates, and dynamically balance load as hot tuples vary over time. This thesis presents two elastic OLTP DBMSs, called E-Store and P-Store, which demonstrate the benefits of elasticity for distributed OLTP DBMSs on different types of workloads. E-Store automatically scales the database cluster in response to demand spikes, periodic events, and gradual changes in an application’s workload, but it is particularly well-suited for managing hot spots. In contrast to traditional single-tier hash and range partitioning strategies, E-Store manages hot spots through a two-tier data placement strategy: cold data is distributed in large chunks, while smaller ranges of hot tuples are assigned explicitly to individual nodes. P-Store is an elastic OLTP DBMS that is designed for a subset of OLTP applications in which load varies predictably. For these applications, P-Store performs better than reactive systems like E-Store, because P-Store uses predictive modeling to reconfigure the system in advance of predicted load changes. The experimental evaluation shows the efficacy of the two systems under variations in load across a cluster of machines. Compared to single-tier approaches, E-Store improves throughput by up to 130% while reducing latency by 80%. On a predictable workload, P-Store outperforms a purely reactive system by causing 72% fewer latency violations, and achieves performance comparable to static allocation for peak demand while using 50% fewer servers. Thesis Supervisor: Michael R. Stonebraker Title: Adjunct Professor of Computer Science	computer science;database server;elasticity (data store);high-availability cluster;multitier architecture;online and offline;online transaction processing;partition (database);predictive modelling;throughput	Rebecca Taft	2017			database;computer science	DB	-22.72748402335023	58.858523622567155	134464
0ce812cb4250cbe3ba697df93d891311697b9624	forge toolkit: leveraging distributed systems in elearning platforms	electronic learning;protocols;ibcn;electronic learning fires computer science protocols laboratories hardware;computer science;fires;hardware	While more and more services become virtualized and always accessible in our society, laboratories supporting computer science (CS) lectures have mainly remained offline and class-based. This apparent abnormality is due to several limiting factors, discussed in the literature, such as the high cost of deploying and maintaining computer network testbeds and the lack of standardization for the presentation of eLearning platforms. In this paper, we present the FORGE toolkit, which leverages experimentation facilities currently deployed in international initiatives for the development of e-learning materials. Thus, we solve the institutional challenge mentioned in the ACM/IEEE 2013 CS curricula concerning the access and maintenance of specialized and heterogeneous hardware thanks to a seamless integration with the networking test-bed community. Moreover, this project builds an ecosystem, where teaching and educational materials, tools, and experiments are available under open scheme and policies. We demonstrate how it already meets most of the requirements from the network and communication component of CS 2013 and some of the labs of the Cisco academy. Finally, we present experience reports illustrating the potential benefits of this framework based on the first deployments in four post-graduate courses in prestigious institutions around the world.	academy;accessibility;authentication;book;computer science;distributed computing;e-book;ecosystem;experiment;heterogeneous computing;online and offline;requirement;seamless3d;software deployment;testbed;user interface	Guillaume Jourjon;Johann M. Márquez-Barja;Thierry Rakotoarivelo;Alexander Mikroyannidis;Kostas Lampropoulos;Spyros G. Denazis;Christos Tranoris;Daan Pareit;John Domingue;Luiz A. DaSilva;Maximilian Ott	2017	IEEE Transactions on Emerging Topics in Computing	10.1109/TETC.2015.2511454	communications protocol;simulation;telecommunications;computer science;operating system;multimedia;computer security	DB	-32.417301878491955	55.6282400569062	134478
e252bb39041a87e7c1f5059cfedaa02450fb3f9d	selective reservation strategies for backfill job scheduling	supercomputer;paralelismo masivo;supercomputador;reserve selection;scheduling;ordonamiento;job scheduling;parallelisme massif;massive parallelism;ordonnancement;superordinateur	Although there is wide agreement that backfilling produces significant benefits in scheduling of parallel jobs, there is no clear consensus on which backfilling strategy is preferable should conservative backfilling be used or the more aggressive EASY backfilling scheme. Using tracebased simulation, we show that if performance is viewed within various job categories based on their width (processor request size) and length (job duration), some consistent trends may be observed. Using insights gleaned by the characterization, we develop a selective reservation strategy for backfill scheduling. We demonstrate that the new scheme is better than both conservative and aggressive backfilling. We also consider the issue of fairness in job scheduling and develop a new quantitative approach to its characterization. We show that the newly proposed schemes are also comparable or better than aggressive backfilling with respect to the fairness criterion.	fairness measure;job scheduler;job shop scheduling;job stream;scheduling (computing);simulation	Srividya Srinivasan;Rajkumar Kettimuthu;Vijay Subramani;P. Sadayappan	2002		10.1007/3-540-36180-4_4	fair-share scheduling;supercomputer;parallel computing;real-time computing;computer science;rate-monotonic scheduling;job scheduler;operating system;distributed computing;scheduling	HPC	-19.686224309279396	46.635473009484585	134542
02d71ed0ca049f2c716641a22b61fe742b271e21	ires: intelligent, multi-engine resource scheduler for big data analytics workflows	profiling;analytics workflows;cost modelling;big data;multi engine optimization	"""Big data analytics tools are steadily gaining ground at becoming indispensable to businesses worldwide. The complexity of the tasks they execute is ever increasing due to the surge in data and task heterogeneity. Current analytics platforms, while successful in harnessing multiple aspects of this ``data deluge"""", bind their efficacy to a single data and compute model and often depend on proprietary systems. However, no single execution engine is suitable for all types of computation and no single data store is suitable for all types of data. To this end, we demonstrate IReS, the Intelligent Resource Scheduler for complex analytics workflows executed over multi-engine environments. Our system models the cost and performance of the required tasks over the available platforms. IReS is then able to match distinct workflow parts to the execution and/or storage engine among the available ones in order to optimize with respect to a user-defined policy. During the demo, the attendees will be able to execute workflows that match real use cases and parametrize the input datasets and optimization policy. The underlying platform supports multiple compute and data engines, allowing the user to choose any subset of them. Through the inspection of the produced plan, its execution and the collection and presentation of numerous cost and performance metrics, the audience will experience first-hand how IReS takes advantage of heterogeneous runtimes and data stores and effectively models operator cost and performance for actual and diverse workflows."""	big data;computation;data store;database engine;information explosion;mathematical optimization;runtime system;scheduling (computing)	Katerina Doka;Nikolaos Papailiou;Dimitrios Tsoumakos;Christos Mantas;Nectarios Koziris	2015		10.1145/2723372.2735377	analytics;real-time computing;big data;computer science;data mining;database;profiling;world wide web	DB	-24.54077997663568	59.29674984256258	134587
dd78aaab63c8be986290eb432c418538251a5310	high-performance rendering on clusters of workstations	high performance rendering;solid object model;fault tolerant;image resolution;computer graphics;workstation clusters data visualisation distributed shared memory systems fault tolerant computing image resolution java message passing network servers rendering computer graphics resource allocation solid modelling;resource allocation;large data sets;cluster of workstations;multiserver system;null;heterogeneous distributed environment;computer graphic;data visualisation;network servers;fault tolerant computing;distributed environment;distributed shared memory systems;scanline algorithm;workstations network servers distributed computing data visualization solid modeling clustering algorithms java fault tolerant systems hardware costs;network of workstation;data visualization;growth rate;message passing;workstation clusters;rendering computer graphics;high performance;java technology;image resolution high performance rendering workstation cluster computer graphics data visualization solid object model heterogeneous distributed environment multiserver system scanline algorithm java technology fault tolerant computing;solid modelling;java;workstation cluster	The computer-graphics aspects of the visualization of large data sets, in particular, digital models of real or planned solid objects in a heterogeneous distributed environment are investigated. It is demonstrated that binary-swap compositing does not scale well on networks of workstations. A multi-server system, based on scanline algorithms and using Java technology, is proposed. The proposed system is efficient as servers only need to solve a problem of growth rate of n log n, it is fault tolerant as both lost messages and server failures are tolerated, and it has negligible hardware costs as it runs on existing networks of workstations. The system is also scalable, as data sets are sent to all servers in the same packets, regardless of the number of servers, and the amount of data sent back by servers only depends on the resolution of the final image	algorithm;cache-oblivious algorithm;client–server model;commodity computing;component-based software engineering;compositing;computation;computational geometry;computer graphics;distributed computing;fault tolerance;geometric modeling;grid computing;java;multiprocessing;paging;scalability;scan line;scanline rendering;server (computing);shared memory;software architecture;visibility (geometry);workstation	Sourav Dalal;Frank Dévai;Mizanur Rahman	2006	Geometric Modeling and Imaging--New Trends (GMAI'06)	10.1109/GMAI.2006.27	fault tolerance;parallel computing;message passing;real-time computing;image resolution;resource allocation;computer science;distributed computing;programming language;computer graphics;java;scanline rendering;data visualization;server;distributed computing environment	HPC	-21.14433702631528	52.04947713583488	134826
745579e60542543ee6fcc175f6f3e4f110b1238f	evolving a model of transaction management with embedded concurrency control for mobile database systems	distributed algorithms;database system;transaction management;computacion informatica;parallel algorithm;management system;real time;mobile database systems;mobile host;grupo de excelencia;non blocking protocols;mobile database;ciencias basicas y experimentales;concurrency control;serializability;distributed algorithm;database management system;parallel algorithms	Transactions within a mobile database management system face many restrictions. These cannot afford unlimited delays or participate in multiple retry attempts for execution. The proposed embedded concurrency control (ECC) techniques provide support on three counts, namely—to enhance concurrency, to overcome problems due to heterogeneity, and to allocate priority to transactions that originate from mobile hosts. These proposed ECC techniques can be used to enhance the server capabilities within a mobile database management system. Adoption of the techniques can be beneficial in general, and for other special cases of transaction management in distributed real-time database management systems. The proposed model can be applied to other similar problems related to synchronization, such as the generation of a backup copy of an operational database system. q 2003 Elsevier Science B.V. All rights reserved.	backup;concurrency (computer science);concurrency control;database;embedded system;real-time locating system;retry;server (computing);transaction processing	Subhash Bhalla	2003	Information & Software Technology	10.1016/S0950-5849(03)00045-4	distributed algorithm;optimistic concurrency control;real-time computing;isolation;database transaction;rollback;database tuning;distributed transaction;computer science;database;distributed computing;parallel algorithm;multiversion concurrency control;serializability;acid;distributed concurrency control	DB	-23.679030176106018	47.15677772562885	134872
41ada28b1860fdc04db559b3a33eda62e4d92714	oracle timesten scaleout: a new scale-out in-memory database architecture for extreme oltp		Oracle TimesTen Scaleout is a shared-nothing scale-out inmemory database designed for extreme OLTP workloads, such as IoT, real-time fraud detection, telecommunications etc. TimesTen Scaleout features rich SQL including complex queries with joins, aggregations and analytic functions, transparent distributed execution, full ACID multi-statement transactions, global secondary indexes and sequences. The design features built-in high availability using a k-safe data duplication mechanism, and transparent failure handling in order to minimize application down time. All management functions such as installation, configuration, and monitoring are provided via a centralized management repository.  We describe some of the challenges in developing such a scale-out in-memory database architecture and show extreme performance results exceeding 100 million transactions per second and 1 billion SQL selects per second.	acid;aggregate function;built-in self-test;centralized computing;distributed transaction;downtime;high availability;online transaction processing;oracle database;oracle timesten in-memory database;real-time clock;sql;scalability;shared nothing architecture;throughput;transactions per second	Yu-Han Chou;Ananth Raghavan;Tirthankar Lahiri	2018		10.1145/3242153.3271881	in-memory database;database;architecture;online transaction processing;data architecture;oracle;sql;scalability;transactions per second;computer science	DB	-20.17432177696765	52.61799040797475	134881
b4520f4254516768ce4c62a735b2ffd389084358	elastic stream processing for distributed environments	elasticity;internet of things big data;routing;processes;internet web technologies;big data elastic stream processing distributed environments internet of things iot;runtime;smart cities;internet;monitoring;distributed databases;internet of people;smart cities distributed databases monitoring runtime;cloud computing internet web technologies internet of people processes and things;cloud computing;and things	The current development towards the Internet of Things introduces the need for more flexibility in stream processing. To counter these challenges, the authors propose elastic stream processing for distributed environments, building on top of cloud computing and allowing a scalable and more flexible solution compared to traditional approaches.	cloud computing;internet of things;scalability;stream processing	Christoph Hochreiner;Stefan Schulte;Schahram Dustdar;Freddy Lécué	2015	IEEE Internet Computing	10.1109/MIC.2015.118	routing;the internet;cloud computing;computer science;operating system;distributed computing;internet privacy;elasticity;law;world wide web;distributed database;computer network	HPC	-31.370080614299393	57.88183590549456	135027
f1b0cd2947aea780c05b601ec8ca05f1c01be93d	improving data locality in p2p-based fog computing platforms		Abstract Fog computing extends the Cloud Computing paradigm to the edge of the network, relying on computing services intelligently distributed to best meet the applications needs such as low communication latency, data caching or confidentiality reinforcement. While P2P is especially prone to implement Fog computing platforms, it usually lacks important elements such as controlling where the data is stored and who will handle the computing tasks. In this paper we propose both a mapping approach for data-locality and a location-aware scheduling for P2P-based middlewares, improving the data management performance on fog environments. Experimental results comparing the data access performances demonstrate the interest of such techniques.		Luiz Angelo Steffenel;Manuele Kirsch-Pinheiro	2018		10.1016/j.procs.2018.10.151	data mining;locality;scheduling (computing);latency (engineering);data management;cloud computing;data access;computer science;distributed computing	HPC	-20.368086920029203	59.61062044383971	135248
362b83c4131311fbf27546400466ae1f056909c0	autonomic optimization of an oil reservoir using decentralized services	instruments;application software;prototypes;hydrocarbon reservoirs;distributed computing;petroleum hydrocarbon reservoirs prototypes laboratories application software distributed computing access protocols instruments production systems computational modeling;petroleum;computational modeling;grid service;access protocols;production systems;peer to peer	The Grid community is actively working on defining, deploying and standardizing protocols, mechanisms and infrastructure to support decentralized, seamless, and secure interactions across distributed resources. Such an infrastructure will enable a new generation of autonomic applications where the application components, Grid services, resources and data interact as peers. In this paper we describe the development and operation of a prototype application that uses such peer-to-peer interactions between services on the Grid to enable the autonomic optimization of an oil reservoir.	autonomic computing;autonomic networking;component-based software engineering;interaction;mathematical optimization;peer-to-peer;prototype;seamless3d	Vincent Matossian;Manish Parashar	2003		10.1109/CLADE.2003.1209993	application software;real-time computing;computer science;operating system;database;distributed computing;prototype;production system;computational model;petroleum;computer security;grid computing	HPC	-33.088783633816305	47.45987550433588	135366
217d60e3b7620363f76ffc446bbc8ba7c4c744bf	supporting web-based database application development	application development;cache storage;information resources;web based database application development;jdbc protocol web based database application development pure java database proxy server jdbc compatible driver internet database application development intranet database application development shared database connection strategy flexible caching facilities scalability problem web clients access privilege shared physical connection connection cost cache requirements;web clients;scalability problem;flexible caching facilities;intranets;application software;web and internet services;information retrieval;intranet database application development;cache requirements;pure java database proxy server;client server systems;web and internet services application software database systems information retrieval distributed databases java web server costs delay information management;object oriented programming;jdbc protocol;object oriented programming information resources intranets client server systems network servers distributed databases java cache storage;jdbc compatible driver;network servers;shared database connection strategy;shared physical connection;information management;database systems;access privilege;distributed databases;connection cost;web server;proxy server;java;internet database application development	This paper discusses our experiences of designing and implementing a pure JAVA database proxy server with a JDBC compatible driver for Intranet/Internet database application development. In particular, we present a shared database connection strategy and flexible caching faciliti es to address the scalabilit y problem. Web clients with the same access privilege can maintain their logical connections with shared physical connections to the database. Thus, not only a large number of users can be entertained by limited physical resource, the connection cost is also not indulged for each individual client. Web clients can also express their cache requirements explicitly in the JDBC protocol so that a large number of clients can be served with improved response time. The effectiveness of such strategies is demonstrated through a set of experiments.	cache (computing);cache invalidation;client-side;communications protocol;database server;encryption;experiment;hybrid cryptosystem;internet;intranet;jdbc;jdbc driver;java;java applet;performance evaluation;proxy server;requirement;response time (technology);seamless3d;secure communication;server (computing);server-side;transport layer security;warez	Quan Xia;Ling Feng;Hongjun Lu	1999		10.1109/DASFAA.1999.765732	application software;computer science;database;distributed computing;information management;programming language;object-oriented programming;java;rapid application development;world wide web;distributed database;web server	DB	-33.53240495429145	48.8131161181544	135407
ea6e6f39fc6caa8e5dcc1c09312169f3ed2eab12	pastrygrid: decentralisation of the execution of distributed applications in desktop grid	distributed application;pastrygrid;system configuration;distributed applications;rdv;desktop grid	This paper proposes a decentralised system for managing Desktop Grid (DG). The idea is to bypass the main drawback of existing systems putting all the control on a single master that can fails. Here, each node can play alternatively the role of client or server. Our main contribution is to design the PastryGrid protocol (based on Pastry) for DG in order to decentralise the execution of a distributed application with precedence between tasks. Comparing to a centralised system, we evaluate our approach over 205 machines executing 2500 tasks. The results show that our decentralised system runs better than the same system configured as a master/slave because it gives less overhead.	centralisation;client (computing);decentralised system;desktop computer;discontinuous galerkin method;distributed computing;master/slave (technology);overhead (computing);server (computing)	Heithem Abbes;Christophe Cérin;Mohamed Jemni	2008		10.1145/1462704.1462708	embedded system;real-time computing;computer science;operating system;database;distributed computing	OS	-29.752618899779375	53.19340914019584	135565
da5d4ac0e09a8cb3fdbc871cb992716c6aa127b3	dynamic pricing in cloud markets: evaluation of procurement auctions		One of the fundamental principles which cloud computing paradigm builds upon is that resources in the cloud may be accessed “on-demand”, i.e., when they are required and for just the time they are required. This intrinsic technologic feature encouraged the cloud commercial providers to adopt the pay-per-use pricing mechanism as it turned to be the most convenient and the easiest to implement. Though pay-per-use ensures significant incomes to providers, still providers experience an underutilization of their computing capacity. It is a matter of fact that unemployed resources represent both a missed income and a cost to providers. In this paper a procurement auction market is proposed as an alternative sell mechanism to maximize the utilization rate of providers’ datacenters. Benefits for the providers are achieved through the use of an adaptive strategy that can be easily tuned to cater for the provider’s own business needs. Also, in the paper the resort to resource overbooking within the provider’s strategy has been analyzed. The proposal’s viability was finally proved through simulation tests conducted on the Cloudsim simulator.		Paolo Bonacquisto;Giuseppe Di Modica;Giuseppe Petralia;Orazio Tomarchio	2014		10.1007/978-3-319-25414-2_3	variable pricing	ECom	-24.949972540342685	60.09224736461144	135666
abd9dce69e3cf97f7f1427af81054552f5821748	a distributed environment approach for the gps radio occultation analysis	software;distributed system;earth atmosphere distributed system architecture gps radio occultation analysis data processing grid computing system system reliability remote sensing;system reliability;remote sensing atmospheric techniques geophysics computing global positioning system grid computing occultations;generators;grid applications;gps radio occultation analysis;gps radio occultation;statistical test;data processing;occultations;global positioning system computer architecture grid computing low earth orbit satellites data processing performance evaluation receivers satellite broadcasting educational institutions remote monitoring;data mining;grid computing system;receivers;computer architecture;orbits;geophysics computing;distributed environment;global positioning system;remote sensing;scheduler;distributed system architecture;radio occultation;atmospheric techniques;grid application;distributed architecture grid computing scheduler grid application gps radio occultation scheduler agent;weed management;grid computing;scheduler agent;distributed architecture;earth atmosphere	In this paper we describe this new scenario for the Radio Occultation data processing, which is based to the use of distributed system architecture. In particular, we describe our approach based on using a Grid computing system in order to decrease the calculation of RO time and to increase the flexibility and reliability of the system. We want to focus the attention of the reader on how we managed the parallelization of the entire jobs chain for the complete processing of Radio Occultation. We executed some statistical tests in order to evaluate the gain in performances and time obtained using the distributed architecture instead of a more standard non distributed architecture.	complex event processing;distributed computing;global positioning system;grid computing;numerical analysis;numerical weather prediction;parallel computing;performance;radio occultation;requirement;run time (program lifecycle phase);scheduling (computing);systems architecture	Olivier Terzo;Lorenzo Mossucca;Pietro Ruiu;Maurizio Molinaro;Riccardo Notarpietro;Manuela Cucca;S. Stigliano	2009	2009 Fifth International Conference on Networking and Services	10.1109/ICNS.2009.10	meteorology;embedded system;statistical hypothesis testing;global positioning system;data processing;computer science;weed control;atmosphere of earth;grid computing;distributed computing environment	HPC	-30.177765537198493	48.337624981050865	135764
5adeade0ca86115150994a1895376c7d96883ff3	modeling and simulation of distributed systems and networks	distributed system;modeling and simulation		distributed computing;simulation	Helen D. Karatza	2004	Simulation Modelling Practice and Theory	10.1016/j.simpat.2004.04.001	dynamic simulation;simulation;computer science;modeling and simulation;distributed design patterns	Logic	-30.01022300454246	46.47311103852492	135815
db03f675570e2f230037fb17398150a354f87c18	layered architectural approach for distributed simulation systems: the simarch case			simulation	Daniele Gianni	2014		10.1201/b17902-11	architectural geometry;computer science;distributed computing	Logic	-30.11318725656893	46.445635523808484	135972
0e5058b6c53ba41e00c4d3b0b8ed43920fcd53d4	extending mpi to better support multi-application interaction	current mpi implementation;application boundary;independent mpi application;collective call;mpi standard;separate application;extending mpi;current scientific workflows;operating mode;mpi intercommunicators;better support multi-application interaction;separate processing phase	Current scientific workflows consist of generally several components either integrated in situ or as completely independent, asynchronous components using centralized storage as an interface. Neither of these approaches are likely to scale well into Exascale. Instead, separate applications and services will be launched using online communication to link these components of the scientific discovery process. Our experiences with coupling multiple, independent MPI applications, each with separate processing phases, exposes limitations preventing use of some of the optimized mechanisms within the MPI standard. In this regard, we have identified two shortcomings with current MPI implementations. First, MPI intercommunicators offer a mechanism to communicate across application boundaries, but do not address the impact this operating mode has on possible programming models for each separate application. Second, MPI Probe offers a way to interleave both local messaging and remote messages, but has limitations as MPI Bcast and other collective calls are not supported by MPI Probe thus limiting use of optimize collective calls in this operating mode.	centralized computing;computer-mediated communication;message passing interface	Jay F. Lofstead;Jai Dayal	2012		10.1007/978-3-642-33518-1_32	real-time computing;computer science;theoretical computer science;distributed computing	HPC	-22.69641512934881	52.37259176862547	135986
124f6e768d4be6994b1ed9505dc1cebc7bd4ec15	a load balanced distributed computing system			distributed computing	M. Bozyigit;J. Al-Ghamdi;M. Ghouseuddin;Hassan R. Barada	1999	Concurrency - Practice and Experience	10.1002/(SICI)1096-9128(199910)11:12%3C753::AID-CPE454%3E3.0.CO;2-A	computer science;load balancing;distributed computing	HPC	-29.11178046932213	47.012107424529354	136125
c924617a9feabc742bbd5f66bf2273b413e7c15c	infrastructure sharing and shared operations for mobile network operators: from a deployment and operations view	network management infrastructure sharing network sharing site sharing ran sharing outsourcing managed services;network resources;outsourcing;multiplication operator;telecommunications equipment;principal agent problem;site sharing;radio access networks context modeling outsourcing technology management resource management costs profitability network topology civil engineering subcontracting;resource management;multivendor landscape;ran sharing;technology management;critical success factors;radio access network;network topology;business model;key assets;network infrastructure;physical network assets;network management infrastructure sharing shared operations mobile network operators business model physical network assets critical success factors key assets telecommunications equipment network infrastructure radio access networks multivendor landscape principal agent problem network resources network sharing managed services site sharing ran sharing;civil engineering;mobile radio;telecommunication network management mobile radio;shared operations;profitability;network management;network sharing;subcontracting;managed services;mobile network operator;context modeling;infrastructure sharing;mobile network operators;telecommunication network management;radio access networks	"""The traditional and still prevailing mobile network operator (MNO) business model is based on the carrier's full ownership of the physical network assets. However, rapid and complex technology migration, regulatory requirements, and increasing capital expenditures on one side and competitive environments, saturated markets, and pressure on margins on the other side advocate a new paradigm: the focus on """"critical success factors"""" and """"key assets"""". Simultaneously, telecommunications equipment is commoditized. These trends are paving the way for the sharing of network infrastructure in the core and radio access networks among multiple operators. Challenges arise with regard to technical solutions to enable such business models in a multi-vendor landscape, but also in the context of the principal-agent-problem accompanying the re-allocation of assets and operational duties. This paper investigates the current technological, regulatory, and business landscape from the perspective of sharing network resources, and proposes several different approaches and technical solutions for network sharing. We introduce a model for estimating savings on capital and operating expenses, and present the results of our simulations for the various scenarios. Finally, we assess the benefits of """"managed services"""" for the shared network case, a potentially highly attractive model to overcome some of the challenges posed by infrastructure sharing."""	access network;cardiovascular technologist;confidentiality;outsourcing;programming paradigm;provisioning;requirement;simulation;software deployment;synergy;turnkey	Thomas Frisanco;Paul Tafertshofer;Pierre Lurin;Rachel Ang	2008	2008 IEEE International Conference on Communications	10.1109/NOMS.2008.4575126	network management;business model;radio access network;multiplication operator;telecommunications;computer science;telecommunications equipment;technology management;resource management;resource;context model;critical success factor;computer security;network topology;profitability index;outsourcing;computer network	Mobile	-29.821228282413887	59.578837669857485	136563
22225e8cb65fe72717ecbb6775968eced31adb19	two problems in wide area network programming	programming paradigm;distributed networks;application server;distributed programs;operating system;internet application;mobile agent;peer to peer;wide area network	Motivations Highly distributed networks have now become a common platform for large scale distributed programming. Internet applications distinguish themselves from traditional applications on scalability (huge number of users and nodes), connectivity (both availability and bandwidth), heterogeneity (operating systems and application software) and autonomy (of administration domains having strong control of their resources). Hence, new programming paradigms (thin client and application servers, collaborative “peer-to-peer”, code-on-demand, mobile agents) seem more appropriate for applications over internet.	regular expression	Ugo Montanari	2000		10.1007/3-540-44929-9_45	real-time computing;computer science;mobile agent;distributed computing;programming paradigm;programming language;application server;computer network	Theory	-33.364280174761326	47.818042661817564	136586
21d6c7a9131b10a8e3d6ff6db7d3968bfe61ecb5	resource and job aware coordination in multi agent filtering framework.				Sahin Albayrak;Dragan Milosevic	2005			real-time computing;knowledge management;distributed computing	AI	-29.88809057180245	47.2890762048988	136689
d66ee584d3301f4931923c5f03f28eb8d74d884f	an occi compliant interface for iaas provisioning and monitoring		In the Cloud scenario provisioning and monitoring play an important role giving the possibility to maintain always the best resources configuration that satisfies the application requirements. Dynamic Cloud provisioning and monitoring allow for the possibility of getting resources in a way that is suited especially well to the business model of IT companies, which can adapt their costs to the current needs continuously and easily. A solution for Cloud resource provisioning and monitoring should also be vendor independent, platform neutral to choose the best proposal among a collection of business offers the widest it is possible. One of the first proposals of standard in Cloud is represented by OCCI (Open Cloud Computing Interface) that is a protocol and API for all kinds of management tasks. In this paper we describe a proposal of extension of OCCI to support provisioning, monitoring and reconfiguration. Furthermore we introduce Cloud Agency, a software platform that complements the common IAAS management facilities with a set of advanced services for dynamic provisioning and monitoring of Cloud resources.	application programming interface;autonomic computing;collegehumor;open cloud computing interface;oracle c++ call interface;programming paradigm;provisioning;requirement;software portability;vendor lock-in	Salvatore Venticinque;Alba Amato;Beniamino Di Martino	2012			embedded system;operating system;computer network	DB	-31.08793218259661	54.33468109380793	136693
7e7b39faf260ddb7849fd0455bfcd6259321a23f	a p2p-based virtual cluster computing using piax	cluster computing;resource allocation;cloud;p2p;satisfiability;large scale;relays portals virtual machining ip networks middleware throughput organizations;overlay network;middleware;virtualisation cloud computing grid computing middleware peer to peer computing resource allocation virtual private networks;utility computing;peer to peer computing;grid computing;scientific research;piax virtual cluster p2p cloud;piax;grid middleware technology p2p virtual cluster computing piax large scale stable robust computing environment grid computing computational resource utilization multiple organization user requirement satisfaction virtual computational resources virtualization technology overlay network xen based virtual computational resources cloud middleware technology;virtualisation;cloud computing;virtual private networks;virtual cluster	Providing large scale, stable and robust computing environments is indispensable to various fields including scientific researches as well as business. Grid computing has gathered considerable attention as a means to efficiently and effectively utilize computational resources among multiple organizations. However, the current Grid technologies do not always provide the computational environment enough to satisfy users' requirements because of the heterogeneity and disconnect among computational resources on the Grid. For the reason, we have been attempting to develop a new middleware that leverages newly emerging Cloud technologies and allows for a virtual cluster composed of virtual computational resources by Xen as a virtualization technology on top of an overlay network realized by PIAX as an overlay networking technology. In this paper, we propose a system that allows Xen-based virtual computational resources composing a virtual cluster to communicate with each other toward the future development of Grid and Cloud middleware technology. The evaluation shown in the paper indicates that such a virtual cluster is feasible and could work effectively for computationally intensive applications.	computation;computational resource;computer cluster;grid computing;middleware;overlay network;relay;requirement;technical support;throughput;x86 virtualization	Kohei Ichikawa;Susumu Date;Yasuyuki Kusumoto;Shinji Shimojo	2011	2011 IEEE International Conference on Granular Computing	10.1109/GRC.2011.6122611	cloud computing;semantic grid;computer science;operating system;distributed computing;utility computing;world wide web;grid computing	HPC	-26.41590815038104	56.71654837045623	136788
09c7abfb6ddd0f329eae4ce6e244305297d805b3	a model for information and action flows connecting science gateways to distributed computing infrastructures		To support scientists of different disciplines, different fields of Computer Science have developed tools and infrastructures with the aim of giving them access to vast computational resources in the easiest possible way. Such extremely complex structures have evolved naturally in the last decades both in depth and breath and, in addition to scientists, a plethora of heterogeneous actors (system administrators, developers, etc.) cooperate and interact with them. This complex and unstructured flow of actions and information poses difficulties in the development and usage of Science Gateways because information can be missing or hard to isolate at the right layer. In this paper, we aim to start a discussion on how to best manage these information flows to help the design and implementation of more flexible and userfriendly Science Gateways and workflow management systems in the future.	computation;computational resource;computer science;distributed computing;hubzero;refinement (computing);system administrator	Gabriele Pierantoni;Dermot Frost;Sandra Gesing;Sílvia Delgado Olabarriaga;Mohammad Mahdi Jaghoori;Gábor Terstyánszky;Junaid Arshad	2016			science, technology and society;catenet;computer science;distributed computing	HPC	-33.61795956027841	50.50422101391896	136889
7bc810c730e3c36a19133f84008e52c0e703ef3c	the iceprod framework: distributed data processing for the icecube neutrino observatory	swedarp;data management;distributed computing;physical sciences;websearch;physics and astronomy;hep;monitoring;icecube;datavetenskap datalogi;computer science;grid computing;fysik	IceCube is a one-gigaton instrument located at the geograph ic South Pole, designed to detect cosmic neutrinos, identify the particle nature of dark matter, and study high-ener gy neutrinos themselves. Simulation of the IceCube detecto r and processing of data require a significant amount of comput ational resources. IceProd is a distributed management system based on Python, XML-RPC and GridFTP. It is driven by a central database in order to coordinate and administer production of simulations and processing of data prod uce by the IceCube detector. IceProd runs as a separate layer on top of other middleware and can take advantage of a va riety of computing resources, including grids and batch systems such as CREAM, Condor, and PBS. This is accompl ished by a set of dedicated daemons that process job submission in a coordinated fashion through the use of mi ddleware plugins that serve to abstract the details of job submission and job management from the framework.	cosmic;daemon (computing);dark matter;distributed computing;gridftp;middleware;plug-in (computing);python;simulation;xml-rpc	Mark G. Aartsen;Rasha U. Abbasi;Markus Ackermann;Jenni Adams;Juan Antonio Aguilar;Markus Ahlers;David Altmann;Carlos A. Argüelles Delgado;Jan Auffenberg;Xinhua Bai;Michael F. Baker;Steven W. Barwick;Volker Baum;Ryan Bay;James J. Beatty;Julia K. Becker Tjus;Karl-Heinz Becker;Segev BenZvi	2015	J. Parallel Distrib. Comput.	10.1016/j.jpdc.2014.08.001	real-time computing;simulation;data management;computer science;physical science;distributed computing;grid computing	HPC	-29.79435516096952	51.85782781752194	137022
25ce36c56bfb73f7f5201ffb90d959e4c8ca1159	mjölnir: the magical web application hammer	virtualization;virtual machine monitoring;content based page sharing	Conventional wisdom suggests that rich, large-scale web applications are difficult to build and maintain. An implicit assumption behind this intuition is that a large web application requires massive numbers of servers, and complicated, one-off back-end architectures. We provide empirical evidence to disprove this intuition. We then propose new programming abstractions and a new deployment model that reduce the overhead of building and running web services.	overhead (computing);software deployment;web application;web service	Jelle van den Hooff;David Lazar;James Mickens	2015		10.1145/2797022.2797025	web service;web development;web modeling;computer science;operating system;database;web 2.0;world wide web;mashup	Web+IR	-23.420563057438017	52.5263256291805	137030
40c9f75ac95850925fb71ed96cf3f2ebb7e9d1b0	towards a performance-as-a-service cloud	data analytics;cloud storage	Motivation While the pay-as-you-go model of Infrastructure-as-a-Service (IaaS) clouds is more flexible than an in-house IT infrastructure, it still has a resource-based interface towards users, who can rent virtual computing resources over relatively long time scales. There is a fundamental mismatch between this resource-based interface and what users really care about: performance.	cloud computing	Davide B. Bartolini;Filippo Sironi;Martina Maggio;Gianluca Durelli;Donatella Sciuto;Marco D. Santambrogio	2013		10.1145/2523616.2525933	simulation;computer science;internet privacy;data analysis;world wide web	HPC	-26.59181024805601	59.723039303039855	137065
3979ed3d6ce457f6598acd68d43477e94d5bebf1	impact of cloud computing virtualization strategies on workloads' performance	virtual machine;virtual networks;memory management;service provider;physical layer;heating;servers;virtualisation cloud computing task analysis virtual machines;virtual machines;task analysis;memory allocation;program processors;virtualisation;cloud computing;throughput;physical machines cloud computing virtualization strategies workloads performance critical task virtual machine migration virtual cpu vcpu;cloud computing memory management program processors heating servers throughput physical layer	Cloud computing brings significant benefits for service providers and users because of its characteristics: \emph{e.g.}, on demand, pay for use, scalable computing. Virtualization management is a critical task to accomplish effective sharing of physical resources and scalability. Existing research focuses on live Virtual Machine (VM) migration as a workload consolidation strategy. However, the impact of other virtual network configuration strategies, such as optimizing total number of VMs for a given workload, the number of virtual CPUs (vCPUs) per VM, and the memory size of each VM has been less studied. This paper presents specific performance patterns on different workloads for various virtual network configuration strategies. For loosely coupled CPU-intensive workloads, on an 8-CPU machine, with memory size varying from 512MB to 4096MB and vCPUs ranging from 1 to 16 per VM, 1, 2, 4, 8 and 16VMs configurations have similar running time. The prerequisite of this conclusion is that all 8 physical processors are occupied by vCPUs. For tightly coupled CPU-intensive workloads, the total number of VMs, vCPUs per VM, and memory allocated per VM, become critical for performance. We obtained the best performance when the ratio of the total number of vCPUs to processors is 2. Doubling the memory size on each VM, for example from 1024MB to 2048MB, gave us at most 15% improvement of performance when the ratio of total vCPUs to physical processors is 2. This research will help private cloud administrators decide how to configure virtual resources for given workloads to optimize performance. It will also help public cloud providers know where to place VMs and when to consolidate workloads to be able to turn on/off Physical Machines (PMs), thereby saving energy and associated cost. Finally it helps cloud service users decide what kind of and how many VM instances to allocate for a given workload and a given budget.	autonomous robot;central processing unit;cloud computing;computer data storage;data-intensive computing;experiment;lisp machine;loose coupling;middleware;openvms;overhead (computing);period-doubling bifurcation;scalability;semiconductor consolidation;system administrator;testbed;time complexity;virtual machine;virtual reality	Qingling Wang;Carlos A. Varela	2011	2011 Fourth IEEE International Conference on Utility and Cloud Computing	10.1109/UCC.2011.27	parallel computing;real-time computing;computer science;virtual machine;operating system;memory management	HPC	-21.65615267152914	60.09176362869632	137086
35f7a771db093369fdf750eae8ef3cbd0b52dc01	fine-grained preemption analysis for latency investigation across virtual machines	information systems applications incl internet;software engineering programming and operating systems;computer communication networks;special purpose and application based systems;computer system implementation;computer systems organization and communication networks	This paper studies the preemption between programs running in different virtual machines on the same computer. One of the current monitoring methods consist of updating the average steal time through collaboration with the hypervisor. However, the average is insufficient to diagnose abnormal latencies in time-sensitive applications. Moreover, the added latency is not directly visible from the virtual machine point of view. The main challenge is to recover the cause of preemption of a task running in a virtual machine, whether it is a task on the host computer or in another virtual machine. We propose a new method to study thread preemption crossing virtual machines boundaries using kernel tracing. The host computer and each monitored virtual machine are traced simultaneously. We developed an efficient and portable trace synchronization method, which is required to account for time offset and drift that occur within each virtual machine. We then devised an algorithm to recover the root cause of preemption between threads at every level. The algorithm successfully detected interactions between multiple competing threads in distinct virtual machines on a multi-core machine.	algorithm;data synchronization;host (network);hypervisor;interaction;multi-core processor;point of view (computer hardware company);preemption (computing);virtual machine	Mohamad Gebai;Francis Giraldeau;Michel Dagenais	2014	Journal of Cloud Computing	10.1186/s13677-014-0023-3	real-time computing;computer science;virtual machine;operating system;distributed computing;virtual finite-state machine	PL	-22.074198674017875	55.03990395780959	137266
18bb9a0158ca224b343017bf282fe301e40a305b	toward automated testing of geo-distributed replica selection algorithms	software testing and debugging;communication systems;symbolic execution;kommunikationssystem;replica selection algorithms;wide area networks	Many geo-distributed systems rely on a replica selection algorithms to communicate with the closest set of replicas. Unfortunately, the bursty nature of the Internet traffic and ever changing network conditions present a problem in identifying the best choices of replicas. Suboptimal replica choices result in increased response latency and reduced system performance. In this work we present GeoPerf, a tool that tries to automate testing of geo-distributed replica selection algorithms. We used GeoPerf to test Cassandra and MongoDB, two popular data stores, and found bugs in each of these systems.	data store;distributed computing;internet;mongodb;selection algorithm;software bug;test automation	Kirill Bogdanov;Miguel Peón Quirós;Gerald Q. Maguire;Dejan Kostic	2015		10.1145/2785956.2790013	real-time computing;computer science;theoretical computer science;operating system;distributed computing;communications system	Networks	-22.98169972420337	54.35947358249075	137463
023e68f904dd1d4b460d660bb45b3a399ead911a	the p-grade grid portal	qa75 electronic computers computer science szamitastechnika;grid portal;szamitogeptudomany;life cycle;graphical interface;industrial application;parallel applications	Providing Grid users with a widely accessible, homogeneous and easy-to-use graphical interface is the foremost aim of Grid-portal development. These portals if designed and implemented in a proper and user-friendly way, might fuel the dissemination of Grid-technologies, hereby promoting the shift of Grid-usage from research into real life, industrial application, which is to happen in the foreseeable future, hopefully. This paper highlights the key issues in Grid-portal development and introduces P-GRADE Portal being developed at MTA SZTAKI. The portal allows users to manage the whole life-cycle of executing a parallel application in the Grid: editing workflows, submitting jobs relying on Grid-credentials and analyzing the monitored trace-data by means of visualization.	credential;foremost;geographic coordinate system;graphical user interface;p-grade portal;portals;real life;usability	Csaba Németh;Gábor Dózsa;Róbert Lovas;Péter Kacsuk	2004		10.1007/978-3-540-24709-8_2	computational science;biological life cycle;human–computer interaction;computer science;theoretical computer science;operating system;graphical user interface;grid computing	HPC	-32.406332578352426	51.93521804666599	137584
ce21872478fa59ded5644a08f87e250247f382ea	poster: an innovative storage stack addressing extreme scale platforms and big data applications	storage management big data;prototypes;arrays containers laboratories prototypes buffer storage hardware;buffer storage;arrays;doe data management nexus storage stack big data applications us department of energy office of advanced simulation and computing advanced scientific computing research io stack extreme scale environment fast forward storage and io project ffsio project lawrence livermore national laboratory;containers;hardware	Current production HPC IO stack design is unlikely to offer sufficient features and performance to adequately serve extreme scale science platform requirements as well as Big Data problems. A joint effort between the US Department of Energy's Office of Advanced Simulation and Computing and Advanced Scientific Computing Research commissioned a project to develop a design and prototype for an IO stack suitable for the extreme scale environment. It will be referred to as the Fast Forward Storage and IO (FFSIO) project. This is a joint effort led by Lawrence Livermore National Laboratory, with the DOE Data Management Nexus leads Rob Ross and Gary Grider as coordinators and contract lead Mark Gary.	big data;fast forward;ibm websphere extreme scale;prototype;requirement;simulation	Jay F. Lofstead;Ivo Jimenez;Carlos Maltzahn;Quincey Koziol;John Bent;Eric Barton	2014	2014 IEEE International Conference on Cluster Computing (CLUSTER)	10.1109/CLUSTER.2014.6968760	parallel computing;computer hardware;computer science;operating system;database;prototype	HPC	-26.357632889373576	51.66046012943984	137668
2dcc5f7ccb86f953851551495482ffa227311cd4	a management of resource ontology for cloud computing		Cloud computing is a new trend in web based service. It could be use in service based application such as Infrastructure as a Service(IaaS) or Service as a Service(SaaS). However cloud computing has more wide meaning than service or application. In cloud computing environment, we don’t have to have any platform to use computing resources. All the computing-source and resources is in the web and can be manage them for user. Therefore the efficient method that is to control and manage the resources on the web is necessary. In this paper, we proposed management method to control and interface cloud computing resources. For this purpose, we made ontology for cloud computing resources and agent model to interface between the resources by the ontology.	cloud computing;firmware;linux;software as a service	Hwa-Young Jeong;Bong-Hwa Hong	2011		10.1007/978-3-642-27201-1_8	cloud computing;knowledge management;end-user computing;database;utility computing	HPC	-31.299192471250873	55.36422464276514	137878
43cca34ad4714c641fad4ae7c4d9263c43c9fdc5	an overview of the distributed system classification and integration framework decif	distributed system		distributed computing	Felix Cornelius	1999	Bulletin of the EATCS		theoretical computer science;mathematics;distributed computing;distributed design patterns	ML	-29.375739991863735	46.61808311548344	137990
31517ef0652452916bec97ff3a8444a902b357e8	intelligent network storage on the big data	cluster system intelligent network storage big data;storage management big data disc storage;process control artificial intelligence hardware sockets software computers intelligent networks;memory bottleneck intelligent network storage disk big data knowledge economy cluster storage system	In the era of knowledge economy, big data storage requirements greatly promote the rapid development of storage technology. In this paper, we put forward the intelligent network storage disks, they are cluster storage system. It is a novel storage way on the big data, cluster storage system resolve the problems such as the big data storage, access, protection and management. They have developed the cheap and accurate channel for overcoming the memory bottleneck. Through experiment simulations, satisfactory results are obtained, intelligent network storage can resolve effectively big data storage in the networking.	algorithm;automatic control;big data;centralized computing;computer data storage;concurrency (computer science);emergence;experiment;intelligent network;internationalized domain name;mass storage;reliability engineering;requirement;simulation;single point of failure;storage efficiency;von neumann architecture	Haixia Li;Chuiwei Lu;Sheng Sun	2014	2014 International Conference on Multisensor Fusion and Information Integration for Intelligent Systems (MFI)	10.1109/MFI.2014.6997697	embedded system;storage area network;intelligent computer network;converged storage;computer hardware;computer science;emc invista;database;information repository	Robotics	-27.063459369147438	55.37148825608934	138134
f0c73ba8bfe3a3a98b3fa78c701284c3d465a082	scotgrid: a prototype tier 2 centre	particle physics;cluster computing	ScotGrid [1] is a prototype regional computing centre formed as a collaboration between the universities of Durham, Edinburgh and Glasgow as part of the UK’s national particle physics grid, GridPP [2]. We outline the resources available at the three core sites and our optimisation efforts for our user communities. We discuss the work which has been conducted in extending the centre to embrace new projects both from particle physics and new user communities and explain our methodology for doing this.	epcc;experiment;gridpp;large hadron collider;mathematical optimization;multitier architecture;prototype;system administrator;tier 2 network;worldwide lhc computing grid	A. Earl;P. Clark;S. Thorn	2004	CoRR		simulation;human–computer interaction;computer science;architecture;world wide web	HPC	-29.62503868757536	51.93281440204897	138149
8d5e5343e8b346fc12c24085ab788178c01971ae	an experimental distributed processing facility - a vehicle for survivability.	distributed processing			Gus H. Smith;W. Shojinaga	1983			computer science;distributed computing;computer security;computer network	DB	-29.37449420457313	47.09140630216895	138316
71eb92df9f4d362fc9cd64e9ffe2c0c7501edbe4	log management approach in three-dimensional spatial data management system	developer productivity;log files;management system;spatial data;oracle database tables;data auditing;geospatial analysis;visual databases security of data;spatial data management security;3d spatial data management system;three dimensional;geology;file;registers;developer productivity log management approach 3d spatial data management system data auditing data monitoring oracle database tables spatial data management security;spatial databases;three dimensional spatial data;three dimensional spatial data log management oracle file;log management;data monitoring;log management approach;security of data;oracle;file systems;concrete;visual databases;spatial databases geospatial analysis file systems registers concrete geology	Log management is essential to register the three-dimensional spatial data. It plays the role of auditing and monitoring in the system. The traditional method of log management is to set up a management module which records the users' manipulations respectively for each function. Though it's easy to apply such a management system, it consumes more time and energy. Another method is only to use the triggers to record the users' operations. But if this method is adopted, the information would be lost before the triggers were created manually. Thus it's necessary to design an efficient system to improve this situation. This log management designed in this paper introduces the stored procedures and triggers to design a log system to store and manage the three-dimensional spatial data. The stored procedure is used to create triggers and Oracle database tables which are applied for recording information. Triggers would be fired when users do anything to the records stored in the Oracle database tables. It also sets the switches to disable and enable triggers timely to release resources. All the tables and triggers are created dynamically by procedures. Once users created a user account on the Oracle database, the system would create all tables and triggers by calling the procedure which written in the system package. It can save a lot of time for the user and record all the operations in case users damage the data before the tables were created. Oracle database records the concrete operations, including operate-times, operate-types, Ips, usernames, etc. And the log files of this system are designed especially for recording each detail of the operations. The administrator is able to do the recovery operation according to the log management system. So it is essential to use the log management system to insure the safety of the three-dimensional spatial data. This system cuts down the programmers' workload for writing duplicate codes. It is not only an improvement to the Oracle database application performance for the three-dimensional spatial data management and the developer productivity, but also a consolidation to the three-dimensional spatial data management security.	code;content management system;data logger;database trigger;log management;network switch;oracle database;piaget's theory of cognitive development;programmer;semiconductor consolidation;spatial analysis;stored procedure;system administrator;table (database);user (computing)	Jing Li;Xing Li;Gang Liu;Zhenwen He	2010	2010 18th International Conference on Geoinformatics	10.1109/GEOINFORMATICS.2010.5568028	computer science;data mining;database;world wide web	DB	-25.508703141194186	50.905114214465385	138494
833da74cbc229854d4bca326d5397491fd67c686	a scalable architecture for distributed osgi in the cloud		Elasticity is one of the essential characteristics for cloud computing. The presented use case is a Software as a Service for Ambient Assisted Living that is configurable and extensible by the user. By adding or deleting functionality to the application, the environment has to support the increase or decrease of computational demand by scaling. This is achieved by customizing the auto scaling components of a PaaS management platform and introducing new components to scale a distributed OSGi environment across virtual machines. We present different scaling and load balancing scenarios to show the mechanics of the involved components.	autoscaling;cloud computing;computation;elasticity (data store);failover;image scaling;load balancing (computing);osgi;platform as a service;service discovery;software as a service;software deployment;virtual machine	Hendrik Kuijs;Christoph Reich;Martin Knahl;Nathan L. Clarke	2016		10.5220/0005810301090117	embedded system;operating system;computer network	HPC	-31.451120269771806	53.57663643218292	138598
8f0e62cdc18e8cf594317212437a3d89535bfcd3	critical perspectives on large-scale distributed applications and production grids	distributed application;computers;software;distributed infrastructure;large scale distributed application;qa75 electronic computers computer science;computational modeling;lead;production grid infrastructure;data visualization;cyberinfrastructure;production;communities;production grid infrastructure large scale distributed application cyberinfrastructure distributed infrastructure;grid computing;large scale systems production application software distributed computing computer science grid computing computer networks computer vision embedded computing investments	It is generally accepted that the ability to develop large-scale distributed applications that are extensible and independent of infrastructure details has lagged seriously behind other developments in cyberinfrastructure. As the sophistication and scale of distributed infrastructure increases, the complexity of successfully developing and deploying distributed applications increases both quantitatively and in qualitatively newer ways. In this paper we trace the evolution of a representative set of “state-of-the-art” distributed applications and production infrastructure; in doing so we aim to provide insight into the evolving sophistication of distributed applications — from simple generalizations of legacy static high-performance to applications composed of multiple loosely-coupled and dynamic components. The ultimate aim of this work is to highlight that even accounting for the fact that developing applications for distributed infrastructure is a difficult undertaking, there are suspiciously few novel and interesting distributed applications that utilize production Grid infrastructure. Along the way, we aim to provide an appreciation for the fact that developing distributed applications and the theory and practise of production Grid infrastructure have often not progressed in phase. Progress in the next phase and generation of distributed applications will require stronger coupling between the design and implementation of production infrastructure and the theory of distributed applications, including but not limited to explicit support for distributed application usage modes and advances that enable distributed applications to scale-out.	anytime algorithm;cyberinfrastructure;distributed computing;e-science;egi;extensibility;in-phase and quadrature components;interoperability;nx bit;open science grid consortium;parallel computing;programming paradigm;requirement;ruth teitelbaum;scalability;software deployment;theme (computing);theory	Shantenu Jha;Daniel S. Katz;Manish Parashar;Omer F. Rana;Jon B. Weissman	2009	2009 10th IEEE/ACM International Conference on Grid Computing	10.1109/GRID.2009.5353064	lead;computer science;theoretical computer science;operating system;data mining;database;distributed computing;computational model;data visualization;grid computing	HPC	-29.577955033308612	51.16973742860289	138819
e363389d4c6cefa95b83f6433fffa8c0a5771a48	the uncracked pieces in database cracking		Database cracking has been an area of active research in recent years. The core idea of database cracking is to create indexes adaptively and incrementally as a side-product of query processing. Several works have proposed different cracking techniques for different aspects including updates, tuple-reconstruction, convergence, concurrency-control, and robustness. However, there is a lack of any comparative study of these different methods by an independent group. In this paper, we conduct an experimental study on database cracking. Our goal is to critically review several aspects, identify the potential, and propose promising directions in database cracking. With this study, we hope to expand the scope of database cracking and possibly leverage cracking in database engines other than MonetDB. We repeat several prior database cracking works including the core cracking algorithms as well as three other works on convergence (hybrid cracking), tuple-reconstruction (sideways cracking), and robustness (stochastic cracking) respectively. We evaluate these works and show possible directions to do even better. We further test cracking under a variety of experimental settings, including high selectivity queries, low selectivity queries, and multiple query access patterns. Finally, we compare cracking against different sorting algorithms as well as against different main-memory optimised indexes, including the recently proposed Adaptive Radix Tree (ART). Our results show that: (i) the previously proposed cracking algorithms are repeatable, (ii) there is still enough room to significantly improve the previously proposed cracking algorithms, (iii) cracking depends heavily on query selectivity, (iv) cracking needs to catch up with modern indexing trends, and (v) different indexing algorithms have different indexing signatures.	computer data storage;concurrency (computer science);concurrency control;database;emoticon;experiment;monetdb;password cracking;radix tree;robustness (computer science);selectivity (electronic);sorting algorithm;type signature;vergence	Felix Schuhknecht;Alekh Jindal;Jens Dittrich	2013	PVLDB	10.14778/2732228.2732229	simulation;data mining;database	DB	-20.105372309536357	49.38064518641558	138837
04da63ad31d462a5da593053eefbd10c6b4ff773	a hybrid cloud infrastructure for big data applications	big data hybrid cloud computing map reduce;computational modeling;hybrid cloud computing;big data;yttrium;cloud computing integrated circuits yttrium big data bandwidth computational modeling;map reduce;bandwidth;integrated circuits;cloud computing	The trending evolution towards the Internet of things and the general increase in broadband are constantly creating large volumes of data that need to be processed to extract further knowledge. Recently, the cloud computing model has seen the evolution from the initial scenario of a public cloud offering its resources to customers through virtualization and Internet, toward the concept of hybrid cloud, where the classic scenario is enriched with a private (company owned) cloud e.g., for the management of sensible data. In this work, we propose a software layer for the deployment and dynamic scaling of virtual clusters on a hybrid cloud. This system can be used for cloud bursting in the context of big data applications. Our work shows that although the execution is significantly influenced by the inter-cloud bandwidth, a dynamic off-premise provisioning mechanism could allow the user to significantly increase the application performance.	analysis of algorithms;apache hadoop;autonomic computing;benchmark (computing);big data;cloud computing;component-based software engineering;computer cluster;data-intensive computing;image scaling;internet of things;mapreduce;on-premises software;provisioning;run time (program lifecycle phase);software deployment	Daniela Loreti;Anna Ciampolini	2015	2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems	10.1109/HPCC-CSS-ICESS.2015.140	simulation;big data;single-chip cloud computer;cloud computing;computer science;operating system;yttrium;cloud testing;distributed computing;computational model;world wide web;bandwidth	Embedded	-27.849676446588337	57.706265004175165	139097
dbeee36f24e339c93ad942af12b1fa62a36eba6a	an approach to predict desktop uptime for job allocation in ad-hoc cloud		In an academic institution, in a public library or in several other organizations some of the computing devices like PCs and workstations lie unused or underused. An ad hoc cloud can be hosted on these devices. Jobs can be allocated to these devices. A major challenge in this scheme is that the devices are sporadically available and non-dedicated. The machine will probably not remain power on for 24x7. A machine may become unavailable after job allocation before the job is completed. Such unavailability can occur for various reasons. The most common reason being that the user may just switch the machine off. In this paper we propose a method to predict the future availability of a machine based on past data. We have collected data from multiple machines in various laboratories in our University. We have built a regression model and a classification model on this data set. We used three different training algorithms- Random forest regressor, Support vector regressor and Neural Network based regressor. We found three of them to perform sufficiently well.	artificial neural network;hoc (programming language);job scheduler;personal computer;public library;random forest;the machine;unavailability;uptime;workstation	Atreya Bandyopadhyay;Nandini Mukherjee	2018		10.1145/3229710.3229743	regression analysis;support vector machine;distributed computing;artificial neural network;scheduling (computing);unavailability;cloud computing;computer science;random forest;workstation	Web+IR	-25.20269540990987	59.14090511869315	139337
2da75ca675edf6eb1012e3d7873146bb6667ac01	automatically building service evaluation metadata in a grid environment	dynamic evaluation metadata;service evaluation metadata;application software;grid applications;grid computing application;metaservice;automatic metadata building service;resource management;metaservice service evaluation metadata grid computing group testing;testing;web service;meta data grid computing internet;round robin;internet;monitoring;dynamic evaluation metadata service evaluation metadata grid service environment metaservice automatic metadata building service web service group testing grid computing application;building simulation;web services;grid service;building services web services international collaboration testing round robin service oriented architecture resource management monitoring application software conferences;meta data;building services;service oriented architecture;grid computing;grid service environment;web service group testing;group testing;conferences	This paper presents a mechanism that automatically build evaluation metadata for grid services by using the information from the execution of different applications. A new MetaService AMDBS, automatic metadata building service, is designed to support this mechanism. The typical feature of this mechanism is that it can get not only the absolute evaluation information, but also the relative global ranking result through the collaboration of different AMDBSs. Web service group testing is used as an example of a grid application to accelerate the process of evaluation metadata building. Simulation experiment results show that the HP (heavy-powerful) algorithm which is designed based on the dynamic evaluation metadata can improve the application performance significantly, compared with the RR (round-robin) algorithm which does not use the evaluation metadata. The effect of using evaluation metadata in different task and resource conditions is also demonstrated by experiment results	algorithm;build automation;experiment;round-robin scheduling;simulation;web service	Zhihui Du;Yinong Chen;Lei Wu;Suihui Zhu	2006	The Fourth IEEE Workshop on Software Technologies for Future Embedded and Ubiquitous Systems, and the Second International Workshop on Collaborative Computing, Integration, and Assurance (SEUS-WCCIA'06)	10.1109/SEUS-WCCIA.2006.18	web service;metadata modeling;semantic grid;computer science;resource management;data mining;database;law;world wide web;data element;meta data services;metadata repository	HPC	-26.12448095814786	58.25015616198039	139437
43497b0d5db0edac01a5bc32f4a4c0d187258f0e	preventive multi-master replication in a cluster of autonomous databases	replication;base donnee repartie;distributed database;equilibrio de carga;equilibrage charge;heterogeneous databases;base repartida dato;data replication;replicacion;scaling up;strong consistency;load balancing;load balance;peer to peer;application service provider	We consider the use of a cluster of PC servers for Application Service Providers where applications and databases must remain autonomous. We use data replication to improve data availability and query load balancing (and thus performance). However, replicating databases at several nodes can create consistency problems, which need to be managed through special protocols. In this paper, we present a lazy preventive data replication solution that assures strong consistency without the constraints of eager replication. We first present a peer-to peer cluster architecture in which we identify the replication manager. Cluster nodes can support autonomous, heterogeneous databases that are considered as black boxes. Then we present the multi-master refresher algorithm and show all system components necessary for implementation. Next we describe our prototype on a cluster of 8 nodes and experimental results that show that our algorithm scales-up and introduces a negligible loss of data freshness (almost equal to mutual consistency).	algorithm;approximation;autonomous robot;autonomy;black box;commitment ordering;database;experiment;exploit (computer security);lazy evaluation;load balancing (computing);multi-master replication;mutual exclusion;peer-to-peer;prototype;queue (abstract data type);replay attack;replication (computing);scheduling (computing);strong consistency;systems architecture	Esther Pacitti;M. Tamer Özsu;Cédric Coulon	2003		10.1007/978-3-540-45209-6_48	real-time computing;computer science;load balancing;operating system;database;distributed computing;distributed database;computer security;replication	DB	-28.34155959345577	47.377804879474006	139483
94c7533a36f99ed4ccb474bd0e2de34cb2f71322	on the challenges of building a bft scada		"""In the last decade, Industrial Control Systems have been a frequent target of cyber attacks. As the current defenses sometimes fail to prevent more sophisticated threats, it is necessary to add advanced protection mechanisms to guarantee that correct operation is (always) maintained. In this work, we describe a Supervisory Control and Data Acquisition (SCADA) system enhanced with Byzantine fault-tolerant (BFT) techniques. We document the challenges of building such system from a """"""""traditional"""""""" non-BFT solution. This effort resulted in a prototype that integrates the Eclipse NeoSCADA and the BFT-SMaRt open-source projects. We also present an evaluation comparing Eclipse NeoSCADA with our BFT solution. Although the results show a decrease in performance, our solution is still more than enough to accommodate realistic workloads."""	byzantine fault tolerance;control system;data acquisition;eclipse;open-source software;protection mechanism;prototype	André Nogueira;Miguel Garcia;Alysson Neves Bessani;Nuno Neves	2018	2018 48th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)	10.1109/DSN.2018.00028	real-time computing;industrial control system;eclipse;computer science;scada;distributed computing;fault tolerance;server	DB	-23.558727928006725	52.05605798758864	139525
e8ba243c590de8def556096e7b3b9bb19f8a1586	large-scale system monitoring experiences and recommendations		Monitoring of High Performance Computing (HPC) platforms is critical to successful operations, can provide insights into performance-impacting conditions, and can inform methodologies for improving science throughput. However, monitoring systems are not generally considered core capabilities in system requirements specifications nor in vendor development strategies. In this paper we present work performed at a number of large-scale HPC sites towards developing monitoring capabilities that fill current gaps in ease of problem identification and root cause discovery. We also present our collective views, based on the experiences presented, on needs and requirements for enabling development by vendors or users of effective sharable end-to-end monitoring capabilities.	end-to-end principle;recommender system;requirement;system monitoring;system requirements;throughput	Ville Ahlgren;Stefan Andersson;Jim M. Brandt;Nicholas P. Cardo;Sudheer Chunduri;Jeremy Enos;Parks Fields;Ann C. Gentile;Richard Gerber;Michael Gienger;Joe Greenseid;Annette Greiner;Bilel Hadri;Yun He;Dennis Hoppe;Urpo Kaila;K. Lance Kelly;Mark Klein;A M Kristiansen;Steve Leak;Mike Mason	2018	2018 IEEE International Conference on Cluster Computing (CLUSTER)	10.1109/CLUSTER.2018.00069	throughput;computer engineering;computer science;real-time computing;root cause;system monitoring;vendor;supercomputer;system requirements	HPC	-24.52853772319675	55.786555748831056	139726
6114477ddb472336ca79ddf934ee1b806fec0e20	private cloud collaboration framework for e-learning environment for disaster recovery using smartphone alert notification	disaster recovery;private cloud collaboration;smartphone application;disaster alert notification	In this research, we have built a framework of disaster recovery such as against earthquake and tsunami disaster for e-Learning environment. We build a prototype system based on IaaS architecture, and this prototype system is constructed by several private cloud computing fabrics. These private cloud fabrics are constructed to operate one large private cloud fabric under the VPN connection. The distributed storage system builds on each private cloud fabric; that is handled almost like same block device such as one file system. For LMS (Learning Management System) to work, we need to boot virtual machines. The virtual machines are booted from the virtual disk images that are stored into the distributed storage system. The distributed storage system will be able to keep running as one large file system when some private cloud fabric does not work by any disasters. The disaster alert such as Earthquake Early Warning can be caught by usual smartphone. And we control virtual machines status and virtual machines positioning on the private cloud fabrics by caught disaster alert notifications. We think that our private cloud collaboration framework can continue working for e-Learning environment under the post-disaster situation. In this paper, we show our private cloud collaboration framework and the experimental results on the prototype configuration.	cloud collaboration;disaster recovery;smartphone	Satoshi Togawa;Kazuhide Kanenishi	2014		10.1007/978-3-319-07863-2_13	simulation;engineering;world wide web;computer security	HCI	-31.699778628383932	57.970469188518464	139802
37dcf4cac169e8457664778c1efcfe87a5fc1f6d	data stream partitioning re-optimization based on runtime dependency mining	dependency mining data stream processing dsms distributed processing partitioning optimization temporal approximate dependencies;distributed processing;query processing data handling distributed processing optimisation;data stream processing;linear road benchmark data stream partitioning reoptimization runtime dependency mining distributed data stream processing multiple queries optimization methods query syntax analysis compile time methods temporal approximate dependencies tad;dsms;temporal approximate dependencies;dependency mining;accuracy optimization runtime data mining monitoring roads routing;partitioning optimization	In distributed data stream processing, a program made of multiple queries can be parallelized by partitioning input streams according to the values of specific attributes, or partitioning keys. Applying different partitioning keys to different queries requires re-partitioning intermediary streams, causing extra communication and reduced throughput. Re-partitionings can be avoided by detecting dependencies between the partitioning keys applicable to each query. Existing partitioning optimization methods analyze query syntax at compile-time to detect inter-key dependencies and avoid re-partitionings. This paper extends those compile-time methods by adding a runtime re-optimization step based on the mining of temporal approximate dependencies (TADs) between partitioning keys. A TAD is defined in this paper as a type of dependency that can be approximately valid over a moving time window. Our evaluation, based on a simulation of the Linear Road Benchmark, showed a 94.5% reduction of the extra communication cost.	approximation algorithm;benchmark (computing);compile time;compiler;mathematical optimization;parallel computing;sensor;simulation;stream processing;throughput	Emeric Viel;Haruyasu Ueda	2014	2014 IEEE 30th International Conference on Data Engineering Workshops	10.1109/ICDEW.2014.6818327	real-time computing;computer science;data mining;database;data stream mining	DB	-19.348392952758353	55.08914963155826	140001
45840c22fe99205ad2d2ec63260f47b270582eae	quality assured ad hoc grids	quality assurance;formal model;availability;resource management;collaboration;component framework;collaboration java quality of service environmental management distributed control centralized control availability electronic switching systems grid computing resource management;electronic switching systems;centralized control;quality of service;environmental management;grid computing;distributed control;java	This paper presents an integrated architecture for ad hoc Grids developed within the Java CoG Kit project. It provides an overview of the key component frameworks that collectively build the ad hoc Grid architecture. Further, it outlines a formal model that can be formally evaluated. The paper also presents an enhancement to the Java CoG Kit to address requirements posed by ad hoc Grids. It integrates into the Java CoG Kit commodity technologies such as Jxta, and ClassAds.	authentication;authorization;autonomous robot;emergence;floor and ceiling functions;grid computing;hoc (programming language);information management;jxta;java;mathematical model;requirement	Kaizar Amin;Gregor von Laszewski;Armin R. Mikler	2005	Joint International Conference on Autonomic and Autonomous Systems and International Conference on Networking and Services - (icas-isns'05)	10.1109/ICAS-ICNS.2005.82	quality assurance;availability;quality of service;computer science;resource management;operating system;database;distributed computing;programming language;java;management;computer security;grid computing;computer network;collaboration	SE	-33.401450863707645	47.39352319114585	140006
796642cbdd1b27e847ff93994522939fa3f5c639	hint-based execution of workloads in clouds with nefeli	random access memory;availability;virtual machining;virtual machine scheduling;random access memory hardware middleware availability layout virtual machining logic gates;layout;public domain software;power aware computing;logic gates;virtual machines;virtual machines cloud computing power aware computing public domain software scheduling;scheduling;virtual machine scheduling distributed systems cloud computing iaas cloud;middleware;distributed systems;iaas cloud;power aware policies workload hint based execution infrastructure as a service clouds nefeli distributed processing cloud interface abstractions usage patterns virtual machine scheduling virtual infrastructure gateway physical hosting nodes vm collocation vm anticollocation cloud internal physical characteristics consumer provided hints high level placement policies cloud administration placement policies placement hints constraint satisfaction problem vm to host placement energy consumption reduction;cloud computing;hardware	Infrastructure-as-a-Service clouds offer entire virtual infrastructures for distributed processing while concealing all physical underlying machinery. Current cloud interface abstractions restrict users from providing information regarding usage patterns of their requested virtual machines (VMs). In this paper, we propose Nefeli, a virtual infrastructure gateway that lifts this restriction. Through Nefeli, cloud consumers provide deployment hints on the possible mapping of VMs to physical nodes. Such hints include the collocation and anticollocation of VMs, the existence of potential performance bottlenecks, the presence of underlying hardware features (e.g., high availability), the proximity of certain VMs to data repositories, or any other information that would contribute in a more effective placement of VMs to physical hosting nodes. Consumers designate only properties of their virtual infrastructure and remain at all times agnostic to the cloud internal physical characteristics. The set of consumer-provided hints is augmented with high-level placement policies specified by the cloud administration. Placement policies and hints form a constraint satisfaction problem that when solved, yields the final VM-to-host placement. As workloads executed by the cloud may change over time, VM-to-host mappings must follow suit. To this end, Nefeli captures such events, changes VM deployment, helps avoid bottlenecks, and ultimately, improves the quality of the rendered services. Using our prototype, we examine overheads involved and show significant improvements in terms of time needed to execute scientific and real application workloads. We also demonstrate how power-aware policies may reduce the energy consumption of the physical installation. Finally, we compare Nefeli's placement choices with those attained by the open-source cloud middleware, OpenNebula.	bottleneck (software);cloud computing;collocation;constraint satisfaction problem;distributed computing;emoticon;high availability;high- and low-level;load balancing (computing);middleware;open-source software;prototype;scalability;scheduling (computing);separation of concerns;software deployment;virtual machine;z/vm	Konstantinos Tsakalozos;Mema Roussopoulos;Alex Delis	2013	IEEE Transactions on Parallel and Distributed Systems	10.1109/TPDS.2012.220	layout;availability;parallel computing;real-time computing;logic gate;cloud computing;computer science;virtual machine;operating system;middleware;database;distributed computing;public domain software;scheduling;computer security;computer network	HPC	-29.933455068079148	56.794420383756794	140097
3f933394f8f66646cdc4335b99eb80a18cfc98a4	serializability problems of interleaved database transactions	serializability problems;interleaved database transactions	The interleaved execution of database transactions produces correctness problems. It is called correct — or serializable —, if it is equivalent to a serial execution of the same transactions. An execution of a set of transactions is described by the sequence of the read/write actions — called schedule —, a reads-from rolation ϕ and an overwrite relation ω.	database transaction;serializability	Georg Lausen	1981		10.1007/3-540-10885-8_46	global serializability;parallel computing;database transaction;commitment ordering;two-phase locking;database;distributed computing;serializability;schedule	DB	-22.89300018157096	47.95508396157489	140139
13d52a2184f40e3a2fe7770ead1b34cd4798b226	a collaborative citizen science platform for real-time volunteer computing and games		Volunteer computing (VC) or distributed computing projects are common in the citizen cyberscience (CCS) community and present extensive opportunities for scientists to make use of computing power donated by volunteers to undertake large-scale scientific computing tasks. VC is generally a noninteractive process for those contributing computing resources to a project, whereas volunteer thinking (VT) or distributed thinking allows volunteers to participate interactively in CCS projects to solve human computation tasks. In this paper, we describe the integration of three tools, the Virtual Atom Smasher (VAS) game developed by CERN, LiveQ, a job distribution middleware, and CitizenGrid, an online platform for hosting and providing computation to CCS projects. This integration demonstrates the combining of VC and VT to help address the scientific and educational goals of games like VAS. This paper introduces the three tools and provides details of the integration process along with further potential usage scenarios for the resulting platform.		Poonam Yadav;Ioannis Charalampidis;Jeremy Cohen;John Darlington;Francois Grey	2018	IEEE Transactions on Computational Social Systems	10.1109/TCSS.2017.2771479	computer science;artificial intelligence;machine learning;multimedia;human computation;computation;citizen science;computer applications;middleware	HPC	-29.320047469722333	51.93782208167588	140496
5ca3fc57de9d60f05f762cfe46783db6de626f3c	distributed database/file systems (session overview): introduction	resolution;distributed database;boyer moore technique;database management;theorem proving;distributed database system;file system;program sysnthesis;local computation;information system;local area network;word processing;central processing unit	During the last fifteen years database management has grown so quickly that today it is found on machines of every size from the largest mainframes to the smallest micro computers. Almost without exception, the production information systems which use database technology store the data at single site, usually local to the main processor. The reasons for this fact are:The most expensive component of a computing system has traditionally been the central processing unit. Duplicating central processors at multiple sites, until recently, was cost prohibitive. Relatively slow transmission speed and resulting poor performance made distribution infeasible. Algorithms and other techniques for solving the problems in 2 above have been slow in coming. Commercially available software has been slow in reaching the marketplace because of 1, 2 and 3 above.  With the relatively recent advancements in price performance of new machines offered by every vendor, distribution of data to multiple sites is rapidly becoming commonplace. No longer is the cost of the central processing unit an obstacle to distributing the processing associated with distributed databases. The processing power needed for distributed database systems is already in place in most big corporations. Micro computers are being used in almost every department for word processing, spreadsheets and other local computational needs. Local area networks connecting these micro computers allow rapid access to data stored at different sites.	algorithm;central processing unit;computation;distributed database;information system;mainframe computer;microcomputer;spreadsheet	John C. Peck	1985		10.1145/320599.320714	local area network;resolution;computer science;artificial intelligence;theoretical computer science;software engineering;central processing unit;database;distributed computing;automated theorem proving;programming language;distributed database;information system	DB	-27.45625258215397	53.69116863003563	140848
1d73a6231adb1a48c15dc8b5fc3db63b6fcf0e11	low-overhead virtualization of mobile platforms	virtual machine;virtualization;mobile device;virtual reality mobile computing operating systems computers security of data;virtual reality;mobile communication security virtual machine monitors servers reliability engineering operating systems;operating system;virtual machines;processor consolidation virtual machines hypervisors virtualization security;processor consolidation;hypervisors;mobile computing;security;high performance;security of data;operating systems computers;high performance hypervisor low overhead visualization mobile platform software stack complexity operating system server desktop world server world mobile device virtualization smartphone hardware resource management cost issue security issue	Mobile platforms are becoming as powerful as PCs were not too long ago. The complexity of their software stacks also starts rivalling those of PCs, and increasingly they run operating systems which originated in the desktop world. It should therefore not be too surprising that another phenomenon familiar from the server and desktop world, virtualization, is taking a foothold in mobile platforms. The talk will outline the motivation for using virtualization on mobile devices, especially smartphones. These mostly relate to efficient use and management of hardware resources, cost and security issues. We will also discuss the overheads imposed by a high-performance hypervisor.	desktop computer;hypervisor;mobile device;overhead (computing);personal computer;server (computing);smartphone	Gernot Heiser	2011	2011 Proceedings of the 14th International Conference on Compilers, Architectures and Synthesis for Embedded Systems (CASES)	10.1145/2038698.2038702	embedded system;full virtualization;parallel computing;real-time computing;mobile search;virtualization;computer science;virtual machine;operating system;virtual reality;mobile computing	EDA	-28.682692809542722	56.34281371622276	140975
c4103767b6afe43bcaf26df3d02b618a67ab547b	building near real-time p-2-p applications with jxta	protocols;application software;near real time p2p applications;next generation peer to peer applications;distributed computing;testing;xml grid computing protocols meta data;internet;jxta platform v2 1;intelligent agent;xml;next generation;message passing;next generation peer to peer applications near real time p2p applications jxta platform v2 1;meta data;file sharing;near real time;peer to peer computing;peer to peer;grid computing;next generation networking;peer to peer computing real time systems delay internet application software testing distributed computing next generation networking intelligent agent message passing;real time systems	The development of peer-to-peer applications has reached a point where they will have to surpass the limitations of tradition file sharing applications like those pioneered by Napster and Gnutella. In this paper we examine the JXTA platform v2.1 and the challenges it faces in meeting the near real-time characteristics required by next generation peer-to-peer applications.	file sharing;gnutella;jxta;napster;next-generation network;peer-to-peer;real-time clock;real-time computing	D. C. Parker;S. A. Collins;D. C. Cleary	2004	IEEE International Symposium on Cluster Computing and the Grid, 2004. CCGrid 2004.	10.1109/CCGrid.2004.1336586	communications protocol;application software;message passing;the internet;xml;computer science;operating system;database;distributed computing;software testing;metadata;world wide web;file sharing;intelligent agent;grid computing	Embedded	-33.358830917734544	47.780510937323406	141049
ad5c695c248a8b0cb4bf43cbf5b83f0152bd88f2	seal: managing accesses and data in peer-to-peer sharing networks	p2p sharing network;novel data;p2p data;verification layer;key problem;selfish behavior;verification subsystem;accounting subsystem;data resource;monitoring subsystem;data access;high availability;p2p;computer network;resource manager;database management systems;file sharing;distributed application	We present SeAl, a novel data/resource and data-access management infrastructure designed for the purpose of addressing a key problem in P2P data sharing networks, namely the problem of wide-scale selfish peer behavior. Selfish behavior has been manifested and well documented and it is widely accepted that unless this is dealt with, the scalability, efficiency, and the usefulness of P2P sharing networks will be diminished. SeAl essentially consists of a monitoring/accounting subsystem, an auditing/verification subsystem, and incentive mechanisms. The monitoring subsystem facilitates the classification of peers into selfish/altruistic. The auditing/verification layer provides a shield against perjurer/slandering and colluding peers that may try to cheat the monitoring subsystem. The incentives mechanisms effectively utilize these layers so to increase the computational/networking and data resources that are available to the community. Our extensive performance results show that SeAl performs its tasks swiftly, while the overhead introduced by our accounting and auditing mechanisms in terms of response time, network, and storage overheads are very small.	cryptography;malware;mathematical optimization;offset binary;overhead (computing);peer-to-peer;response time (technology);svl;scalability;software performance testing	Nikos Ntarmos;Peter Triantafillou	2004	Proceedings. Fourth International Conference on Peer-to-Peer Computing, 2004. Proceedings.	10.1109/PTP.2004.1334938	data access;computer science;resource management;operating system;peer-to-peer;database;distributed computing;high availability;world wide web;computer security;file sharing;computer network	HPC	-24.95667357590232	57.19393292451091	141498
930ec44cca33d1548a5a44937bf31a94757b884f	the lotus notes storage system	storage system;client server;heterogeneous network;document management	Lotus Notes is a commercial product that empowers individuals and organizations to collaborate and share information [1].Notes enables the easy development of applications such as messaging, document management, workflow, and asynchronous conferencing. Notes applications can be deployed globally, across independent organizations, among a heterogeneous network of loosely coupled computers that range in size from small notebooks to large multi-processor systems.The third major release of Lotus Notes occurred in May 1993. Notes is a client-server product, with clients available on Windows, OS/2, Macintosh, SCO UNIX, HP-UX, AIX, and Solaris. The server is available on Windows, OS/2, Windows NT (for Intel processors), NetWare, SCO UNIX, HP-UX, AIX, and Solaris.	a/ux;aix;central processing unit;client–server model;computer;ibm notes;loose coupling;lotus 1-2-3;microsoft windows;multiprocessing;netware;os/2;server (computing);unix;windows nt	Kenneth Moore	1995		10.1145/223784.223859	embedded system;heterogeneous network;computer science;operating system;document management system;database;programming language;windows server;client–server model	OS	-27.47002154303461	51.48631993518233	141702
62c70231b08915e806aac5e8a4a768cd3d3bc88f	resource selection for autonomic database tuning	resource selection;database systems relational databases tuning system performance costs computer buffers performance analysis displays data analysis concurrency control;database system;system performance;data analysis;tuning;displays;database systems;concurrency control;performance analysis;relational databases;computer buffers	Database administrators should be aware of resource usages to maintain database system performance. As database applications become more complex and diverse, managing database systems becomes too costly and prone to error. Autonomic database tuning becomes more important than ever. This paper starts with an analysis on how resource usages respond by changing resource sizes in database systems. Then, we present a simple method that automatically selects resources that affect the system performance. An experiment using the TPC-C and TPC-W workloads has been carried out with a commercial database system. A preliminary analysis shows that our method works.	autonomic computing;database tuning;ibm tivoli storage productivity center;relational database management system;tpc-w	Jeong Seok Oh;Sang Ho Lee	2005	21st International Conference on Data Engineering Workshops (ICDEW'05)	10.1109/ICDE.2005.274	database theory;database server;intelligent database;database transaction;database tuning;relational database;computer science;data administration;database model;concurrency control;data mining;database;computer performance;data analysis;view;world wide web;database schema;distributed database;physical data model;alias;database testing;database design	DB	-20.62451860088048	56.75527952589568	141720
aa839da836396616e309b7433ee7949a5ea386d9	xen virtualization and multi-host management using mln	operating system;virtual machine;resource manager	Xen virtualization is a powerful tool for encapsulating services and providing seamless migration of tasks on hardware failure. This tutorial shows how to set up multiple Xen instances in a network using the MLN management tool.One of the main challenges in virtual machine administration on a large scale is the specification of complex and repeatable virtualized scenarios. Creating a singe new virtual machine, boot it and install an operating system is straight forward with many of todays tools. But what if you need to deploy 50 identical virtual machines across 25 servers and manage them as an atomic unit? How do you at a later point make consistent design adjustments such as migrating a subset of the virtual machines to a new server or adjusting memory levels? These issues are at the heart of this tutorial.MLN is a virtual machine management tool supporting both User-Mode Linux and Xen. It has been developed at the Oslo University College in conjunction with its research on system administration and resource management. MLN and its research has previously been presented at the Norwegian Unix User Group (NUUG) and the 20th USENIX Large Installation System Administration conference LISA. This tutorial is interesting for all who want to look beyond the typical one-vm-on-my-desktop scenario. Teachers interested in virtual student labs should also attend.We start with a short introduction to the Xen virtual machine technology and then proceed to MLN and its own configuration language. In the second part of the tutorial, we will talk about installation and configuration of MLN into a virtual infrastructure across several servers.		Kyrre M. Begnum	2007			embedded system;real-time computing;computer science;virtual machine;resource management;operating system;database;computer security	ML	-28.175229625846434	51.49590464846079	142038
e8ea2f9c7aac76014e9103728f67334bd68010f9	incremental mature garbage collection using the train algorithm	management system;programming language;garbage collection;object oriented;garbage collector;training algorithm	We present an implementation of the Train Algorithm, an incremental collection scheme for reclamation of mature garbage in generation-based memory management systems. To the best of our knowledge, this is the rst Train Algorithm implementation ever. Using the algorithm, the traditional mark-sweep garbage collector employed by the Mjjlner run-time system for the object-oriented BETA programming language was replaced by a non-disruptive one, with only negligible time and storage overheads.	algorithm;beta;garbage collection (computer science);memory management;programming language;requirement;runtime system	Jacob Seligmann;Steffen Grarup	1995		10.1007/3-540-49538-X_12	manual memory management;garbage;parallel computing;real-time computing;computer science;garbage collection;programming language;mark-compact algorithm	PL	-20.069162643546672	48.2644490560379	142072
a14d69d557283fb332d0fae4e654cf13c0645f69	modelling distributed service systems with resources using uml	uml;resources;distributed systems	Web services managing distributed resources is a technology that allows the user to have an easier and cheaper mechanism to access and manage distributed resources. Then, the formal modeling of web services with distributed resources becomes very important in order to understand accurately their behavior. In this paper, we present a framework to model these systems by using a widely adopted standard (UML 2.0), enriched with the resource management capabilities, on the basis on WSRF. c © 2012 The Authors. Published by Elsevier B.V. Selection and/or peer-review under responsibility of the organizers of the 2013 International Conference on Computational Science.	computation;computational science;unified modeling language;web service	María-Emilia Cambronero;Valentín Valero	2013		10.1016/j.procs.2013.05.177	computer science;knowledge management;applications of uml;database	HPC	-32.24003545244649	53.60125224017867	142114
1f41aa3b32e62ba85c8581d8da78a946886a5b87	what we talk about when we talk about cloud network performance	performance guarantees;cloud networks	"""Infrastructure-as-a-Service (""""Cloud"""") data-centers intrinsically depend on high-performance networks to connect servers within the data-center and to the rest of the world. Cloud providers typically offer different service levels, and associated prices, for different sizes of virtual machine, memory, and disk storage. However, while all cloud providers provide network connectivity to tenant VMs, they seldom make any promises about network performance, and so cloud tenants suffer from highly-variable, unpredictable network performance.  Many cloud customers do want to be able to rely on network performance guarantees, and many cloud providers would like to offer (and charge for) these guarantees. But nobody really agrees on how to define these guarantees, and it turns out to be challenging to define """"network performance"""" in a way that is useful to both customers and providers. We attempt to bring some clarity to this question."""	cloud computing;data center;disk storage;network performance;openvms;virtual machine	Jeffrey C. Mogul;Lucian Popa	2012	Computer Communication Review	10.1145/2378956.2378964	computer science;internet privacy;computer security;computer network	Networks	-27.72353385463494	59.16417588704857	142371
f9e856eed3f42d88269f8e3b3af29e4e7aea6cec	agile computing: bridging the gap between grid computing and ad-hoc peer-to-peer resource sharing	fault tolerant;resource allocation;resource management;ad hoc peer to peer resource sharing;ad hoc network;grid computing peer to peer computing resource management military computing marine vehicles fault tolerance computer networks logistics fires workstations;computer networks;agile computing;performance metric;fault tolerance agile computing grid computing ad hoc peer to peer resource sharing user demand;fault tolerant computing;logistics;marine vehicles;workstations;fault tolerance;resource sharing;user demand;ad hoc networks;fault tolerant computing grid computing ad hoc networks resource allocation;peer to peer computing;short period;fires;peer to peer;grid computing;military computing	Agile computing may be defined as opportunistically (or on user demand) discovering and taking advantage of available resources in order to improve capability, performance, efficiency, fault tolerance, and survivability. The term agile is used to highlight both the need to quickly react to changes in the environment as well as the need to exploit transient resources only available for short periods of time. Agile computing builds on current research in grid computing, ad-hoc networking, and peer-to-peer resource sharing. This paper describes both the general notion of agile computing as well as one particular approach that exploits mobility of code, data, and computation. Some performance metrics are also suggested to measure the effectiveness of any approach to agile computing.	agile software development;bridging (networking);cloud computing;computation;fault tolerance;grid computing;hoc (programming language);peer-to-peer	Niranjan Suri;Jeffrey M. Bradshaw;Marco M. Carvalho;Thomas B. Cowin;Maggie R. Breedy;Paul T. Groth;Raul Saavedra	2003		10.1109/CCGRID.2003.1199423	wireless ad hoc network;fault tolerance;real-time computing;agile unified process;computer science;resource management;operating system;distributed computing;utility computing;grid computing;computer network	HPC	-30.932711197210118	48.13843291330559	142400
1e562ae9f4d1424d31c9ae07c7587b68a7d0ba05	power consumption modeling and prediction in a hybrid cpu-gpu-mic supercomputer	paper;high performance computing;job power modeling;energy efficient computing;theoretical computer science;support vector regression;computer science all;job power prediction;python;package;hybrid system;computer science;task scheduling;intel xeon phi	Power consumption is a major obstacle for High Performance Computing (HPC) systems in their quest towards the holy grail of ExaFLOP performance. Significant advances in power efficiency have to be made before this goal can be attained and accurate modeling is an essential step towards power efficiency by optimizing system operating parameters to match dynamic energy needs. In this paper we present a study of power consumption by jobs in Eurora, a hybrid CPU-GPUMIC system installed at the largest Italian data center. Using data from a dedicated monitoring framework, we build a data-driven model of power consumption for each user in the system and use it to predict the power requirements of future jobs. We are able to achieve good prediction results for over 80 % of the users in the system. For the remaining users, we identify possible reasons why prediction performance is not as good. Possible applications for our predictive modeling results include scheduling optimization, power-aware billing and system-scale power modeling. All the scripts used for the study have been made available on GitHub.	cpu socket;central processing unit;data center;electronic billing;flops;graphics processing unit;job stream;mathematical optimization;performance per watt;predictive modelling;requirement;scheduling (computing);supercomputer	Alina Sîrbu;Özalp Babaoglu	2016		10.1007/978-3-319-43659-3_9	supercomputer;parallel computing;real-time computing;simulation;python;computer science;artificial intelligence;operating system;machine learning;database;distributed computing;programming language;package;computer security;algorithm;hybrid system	HPC	-19.327541686328683	58.488875148700785	142432
c123fc08da39228b4c8015bc728a49e51763dceb	providing support for data replication protocols with multiple isolation levels	data replication	Concurrent transaction execution with different isolation levels is an issue solved a long time ago in centralised databases. This allows an application to execute critical transactions with a high isolation level and non-critical with a weak one. In replicated databases this is still an open line, nearly virgin, with a lot of open fronts which conform the main objectives of this work. What we intend to do is a methodology to construct replication protocols supporting concurrent transaction execution with different isolation levels. As example and case of study, in this document we define the new Generalised Loose Read Committed isolation level, near to Read Committed but easier to provide in replication protocols, to study how can we apply our methodology to support GLRC and Serialisable in the weak voting replication protocol schema.	isolation (database systems);replication (computing)	Josep M. Bernabé-Gisbert	2007		10.1007/978-3-540-76888-3_48	real-time computing;computer science;database;distributed computing	Arch	-22.626518027965027	48.309489397713044	142468
3bcee8259f266b70718b786ff396185198bf05c2	efficient resource management for data centers: the acticloud approach		Despite their proliferation as a dominant computing paradigm, cloud computing systems lack effective mechanisms to manage their vast resources efficiently. Resources are stranded and fragmented, limiting cloud applicability only to classes of applications that pose moderate resource demands. In addition, the need for reduced cost through consolidation introduces performance interference, as multiple VMs are co-located on the same nodes. To avoid such issues, current providers follow a rather conservative approach regarding resource management that leads to significant underutilization. ACTiCLOUD is a three-year Horizon 2020 project that aims at creating a novel cloud architecture that breaks existing scale-up and share-nothing barriers and enables the holistic management of physical resources, at both local and distributed cloud site levels. This extended abstract provides a brief overview of the resource management part of ACTiCLOUD, focusing on the design principles and the components.		Vasileios Karakostas;Georgios I. Goumas;Ewnetu Bayuh Lakew;Erik Elmroth;Stefanos Gerangelos;Simon Kolberg;Konstantinos Nikas;Stratos Psomadakis;Dimitrios Siakavaras;Petter Svärd;Nectarios Koziris	2018		10.1145/3229631.3236095	resource efficiency;cloud computing;reduced cost;risk analysis (engineering);resource management;architecture;holistic management;limiting;design elements and principles;computer science	HPC	-28.958246227803844	60.15320571566271	142529
038f4ea90d5d223ab16b085dd88b2bb108ad1f49	a mobile-agent-based approach to software coordination in the hoope system	distributed application;software coordination;mobile agent software coordination internet computing;software coordination internet computing mobile agent;software agent;large scale;internet computing;object oriented;mobile agent;dynamic adaptation;parallel applications;geographic distribution	Software coordination is central to the construction of large-scale high-performance distributed applications with software services scattered over the decentralized Internet. In this paper, a new mobile-agent-based architecture is proposed for the utilization and coordination of geographically distributed computing resources. Under this architecture, a user application is built with a set of software agents that can travel across the network autonomously. These agents utilize the distributed resources and coordinate with each other to complete their task. This approach’s advantages include the natural expression and flexible deployment of the coordination logic, the dynamic adaptation to the network environment and the potential of better application performance. This coordination architecture, together with an object-oriented hierarchical parallel application framework and a graphical application construction tool, is implemented in the HOOPE environment, which provides a systematic support for the development and execution of Internet-based distributed and parallel applications in the petroleum exploration industry.	agent-based model;algorithm;application framework;autonomous robot;client–server model;code mobility;dataflow;directory service;distributed computing;fault tolerance;graphic art software;graphical user interface;internet;java;machine translation;mobile agent;persistence (computer science);soap;server (computing);software agent;software deployment;www;web services description language;web service;xml	Xiaoxing Ma;Jian Lü;XianPing Tao;Yingjun Li;Hao Hu	2002	Science in China Series F: Information Sciences	10.1360/02yf9019	embedded system;real-time computing;computer science;software framework;component-based software engineering;software development;software agent;middleware;mobile agent;distributed computing;distributed design patterns;object-oriented programming;resource-oriented architecture;software deployment;software system	HPC	-32.179370966600956	48.50594818662999	142986
6936928d4ee9b0d05f5957cdfa3b540f9d5cf171	hipers batch.net - a high performance, extensible, reliable and scalable batch framework using microsoft.net	data parallelism;scalable;task parallelism;microsoft net;batch processing;high performance batch;hipers batch	"""Batch jobs are the back bone of IT functionality in all industries. Typically these jobs are executed overnight, after """"End of Day/Week/Month/Year"""" and perform business critical processing of huge amounts of data, to bring software systems up-to-date for the next day's business. Due to the huge processing requirements, batch systems have been traditionally implemented in legacy Mainframe environments. These systems are costly to develop and maintain, and require manpower skilled in what is fast becoming an obsolete platform. In this paper, I propose HiPERS Batch- a simple multi-server batch framework which: (i) is High in Performance -- can match the processing power of legacy batch systems, (ii) is Extensible - accommodates new batch business functionalities without code change, (iii) is Reliable -- recovers from failures and resumes processing from the correct point, and (iv) is Scalable -- easily handles growing volumes of business data by accommodating increasing number of servers in its configuration without performance degradation. HiPERS Batch exploits the data and task level parallelism inherent in batch functionality. A salient feature of HiPERS multi server batch framework is that the communication between the servers takes place solely through the database -- this does not impact performance as is commonly believed. As part of work done for a large Insurer, HiPERS Batch was implemented using Microsoft""""s.NET framework 2.0 (HiPERS Batch.NET) with virtual servers of modest specifications (no more powerful than a modern day laptop) to replace a powerful legacy mainframe batch system; the results of this implementation, as shown in this paper, prove that HiPERS Batch is a viable framework for a modern batch system."""	.net framework version history;batch processing;elegant degradation;laptop;mainframe computer;parallel computing;requirement;scalability;server (computing);software system;task parallelism	Gayatri Lakshmanan	2012		10.1145/2459118.2459137	parallel computing;real-time computing;scalability;computer science;job scheduler;operating system;portable batch system;data mining;job stream;database;distributed computing;data parallelism;programming language;computer security;task parallelism;batch processing	OS	-26.535669581106415	53.102909408571236	142989
2f9f206a8b1569739ddb6c47a1da7e72b24a1e39	an overview of virtual and cloud computing	file servers;project management;digital library;digital libraries;project manager;computer applications;resource use;service design;computer application;system management;cloud computing	Purpose – The purpose of this paper is to define and describe virtualization of servers and cloud computing. Understanding the differences and similarities between the two technology models will help digital library managers make better decisions related to hosting of application services.Design/methodology/approach – The paper is a general overview of the principles and techniques of virtualization and cloud computing.Findings – Both virtualization and cloud computing can be effective methods of optimizing hardware resources used to run digital library applications; however, system managers should not overlook the potential for security problems and management problems given the outsourced nature of the computing resources.Originality/value – The paper fills a gap in the digital library project management literature by providing an overview of the server virtualization and cloud computing models which could be applied to digital library projects.	cloud computing	H. Frank Cervone	2010	OCLC Systems & Services	10.1108/10650751011073607	cloud computing security;file server;full virtualization;digital library;virtualization;systems management;cloud computing;computer science;service design;cloud testing;database;utility computing;computer applications;world wide web	HPC	-31.619841719730577	56.52404075947852	143193
3d2fca1fe368696d2a433786d927a710ab2812a4	cloud computing: issues and challenges	file servers;nist;servcice oriented computing;cloud interoperability;biological system modeling;distributed computing;web service;cloud computing grid computing computer networks ecosystems intelligent networks australia physics computing distributed computing nist file servers;distributed comptuing;physics computing;ict industry;computer networks;software architecture;cloud interoperability cloud computing ict industry service oriented computing grid computing;computational modeling;internet;ecosystems;service oriented computing;web services;web services cloud computing servcice oriented computing distributed comptuing;intelligent networks;organizations;open systems;security;grid computing;software architecture grid computing internet open systems;meteorology;australia;cloud computing	Many believe that Cloud will reshape the entire ICT industry as a revolution. In this paper, we aim to pinpoint the challenges and issues of Cloud computing. We first discuss two related computing paradigms - Service-Oriented Computing and Grid computing, and their relationships with Cloud computing We then identify several challenges from the Cloud computing adoption perspective. Last, we will highlight the Cloud interoperability issue that deserves substantial further research and development.	cloud computing issues;grid computing;interoperability;service-oriented device architecture	Tharam S. Dillon;Chen Wu;Elizabeth Chang	2010	2010 24th IEEE International Conference on Advanced Information Networking and Applications	10.1109/AINA.2010.187	web service;cloud computing security;computer science;information security;operating system;end-user computing;database;distributed computing;utility computing;law;world wide web	HPC	-32.33662656042657	55.083303913284546	143396
9072c624116418f47040e3042b803210c001356b	resource provisioning of web applications in heterogeneous clouds	poor performance;heterogeneous cloud;identical virtual machine instance;individual performance profile;new machine instance;performance profile;web application;different machine instance;performance characteristic;different performance;better performance;individual virtual machine instance	Cloud computing platforms provide very little guarantees regarding the performance of seemingly identical virtual machine instances. Such instances have been shown to exhibit significantly different performance from each other. This heterogeneity creates two challenges when hosting multi-tier Web applications in the Cloud. First, different machine instances have different processing capacity so balancing equal amounts of load to different instances leads to poor performance. Second, when an application must be reprovisioned, depending on the performance characteristics of the new machine instance it may be more beneficial to add the instance to one tier or another. This paper shows how we can efficiently benchmark the individual performance profile of each individual virtual machine instance when we obtain it from the Cloud. These performance profiles allow us to balance the request load more efficiently than standard load balancers, leading to better performance at lower costs. The performance profiles also allow us to predict the performance that the overall application would have if the new machine instance would be added to any of the application tiers, and therefore to decide how to make best use of newly acquired machine instances. We demonstrate the effectiveness of our techniques by provisioning the TPC-W e-commerce benchmark in the Amazon EC2 platform.	amazon elastic compute cloud (ec2);benchmark (computing);cloud computing;e-commerce;load balancing (computing);multitier architecture;provisioning;tpc-w;virtual machine;web application	Dejun Jiang;Guillaume Pierre;Chi-Hung Chi	2011			real-time computing;simulation;computer science;distributed computing	OS	-22.582261022261356	60.17009157446625	143639
57f1da01f73e42e99362ed959ec65fcdc31d7cde	using rules and data dependencies for the recovery of concurrent processes in a service-oriented environment	databases;schedules interference databases semantics context history web services;service composition;log files;history;relaxed isolation;service orientation;rule based;semantics;web service;interference;system recovery;system recovery data handling service oriented architecture;data dependence;execution environment;web services;grid service;schedules;data handling;transaction processing service composition relaxed isolation concurrent process recovery;transaction processing;service oriented architecture;delta enabled grid services data dependencies concurrent processes recovery service oriented environment recovery algorithm service execution failure concurrent process execution database transaction log files;concurrent process recovery;concurrent process;context;reading and writing	This paper presents a recovery algorithm for service execution failure in the context of concurrent process execution. The recovery algorithm was specifically designed to support a rule-based approach to user-defined correctness in execution environments that support a relaxed form of isolation for service execution. Data dependencies are analyzed from data changes that are extracted from database transaction log files and generated as a stream of deltas from Delta-Enabled Grid Services. The deltas are merged by time stamp to create a global schedule of data changes that, together with the process execution context, are used to identify processes that are read and write dependent on failed processes. Process interference rules are used to express semantic conditions that determine if a process that is dependent on a failed process should recover or continue execution. The recovery algorithm integrates a service composition model that supports nested processes, compensation, contingency, and rollback procedures with the data dependency analysis process and rule execution procedure to provide a new approach for addressing consistency among concurrent processes that access shared data. We present the recovery algorithm and also discuss our results with simulation and evaluation of the concurrent process recovery algorithm.	algorithm;basic stamp;correctness (computer science);data dependency;data logger;database transaction;delta encoding;dependence analysis;interference (communication);logic programming;next-generation network;parallel computing;peer-to-peer;sensor;service composability principle;service-oriented device architecture;simulation;streaming media;transaction log	Yang Xiao;Susan Darling Urban	2012	IEEE Transactions on Services Computing	10.1109/TSC.2011.25	web service;real-time computing;computer science;operating system;foreground-background;database;distributed computing;semantics;programming language;law	DB	-24.84765952354165	48.11033445196765	143726
4a848fda2f42f0dbb8506de06524b0d4de7f0a75	service level agreement aware resource management		Next Generation Grids aim at attracting commercial users to employ Grid environments for their business critical compute jobs. These customers demand for contractually fixed service quality levels, ensuring the availability of results in time In this context, a Service Level Agreement (SLA) is a powerful instrument for defining a comprehensive requirement profile. Numerous research projects worldwide already focus on integrating SLA technology in Grid middleware components like broker services. However, solely focusing on Grid middleware services is not sufficient. Services at Grid middleware may accept compute jobs from customers, but they have to realize them by means of local resource management systems (RMS). Current RMS offer best-effort service only, thus they are also limiting the service quality level the Grid middleware service is able to provide. In this thesis the architecture and operation of an SLA-aware resource management system is described, which allows Grid middleware components to negotiate on SLAs. The system uses its internal mechanisms of applicationtransparent fault tolerance to ensure the terms of these SLAs even in case of resource outages. The main parts of this work focus on scheduling aspects and strategies for ensuring SLA compliance, respectively design aspects on implementation. Scheduling strategies significantly determine the level of fault tolerance that the system is able to provide. After presenting requirements of Grid middleware components on service qualities and a description of operation phases of an SLA-aware resource management system, intra-cluster scheduling strategies are described. Here, the system solely uses its own resources and mechanisms for coping with resource outages. For further increasing the level of fault tolerance, strategies for cross-border migration are presented. Beside a migration to other cluster systems in the same administrative domain, the system uses also Grid resources as migration targets. For ensuring the successful restart, mechanisms for describing the compatibility profile of a checkpointed job are presented. The concept of the SLA-aware resource management system has been implemented in the scope of the EC-funded project HPC4U. We will describe design aspects of this realization and show results from system deployments at use-case customers.	administrative domain;best-effort delivery;clustered file system;fault tolerance;grid computing;job stream;middleware;next-generation network;requirement;scheduling (computing);service-level agreement	Matthias Hovestadt	2006			service level objective;resource management;service-level agreement;reliability engineering;service level requirement;business	HPC	-30.268207536953163	55.3123952785993	143877
5b737e31faaf79fca0a62d481330bb9a114a88ea	an automated parallel approach for rapid deployment of composite application servers	parallel processing cloud computing operating systems computers;provisioning;standard vanilla server automated parallel approach composite application server deployment approach infrastructure as a service iaas os application installation scripts multiserver dependency automatic server installation framework soft layer public cloud;infrastructure as a service;servers synchronization checkpointing cloud computing ip networks protocols prototypes;composite application deployment;automated deployment;infrastructure as a code;composite application deployment infrastructure as a service automated deployment provisioning infrastructure as a code	Infrastructure as a Service (IaaS) generally provides a standard vanilla server that contains an OS and basic functions, and each user has to manually install the required applications for the proper server deployments. We are working on a composite application deployment approach to automatically install selected applications in a flexible manner, based on a set of application installation scripts that are invoked on the vanilla server. Some applications have installation dependencies involving multiple servers. Previous research projects on installing applications with multi-server dependencies have deployed the servers sequentially. This means the total deployment time grows linearly with the number of servers. Our automated parallel approach makes the composite application deployment run in parallel when there are installation dependencies across multiple servers. We implemented a prototype system on Chef, a widely used automatic server installation framework, and evaluated the performance of our composite application deployment on a Soft Layer public cloud using two composite application server cases. The deployment times were reduced by roughly 40% in our trials.	application checkpointing;application server;cloud computing;composite application;data dependency;operating system;prototype;scripting language;server (computing);software deployment;virtual appliance	Yasuharu Katsuno;Hitomi Takahashi	2015	2015 IEEE International Conference on Cloud Engineering	10.1109/IC2E.2015.16	deployment diagram;computer science;operating system;database;distributed computing;application server;server;server farm	SE	-30.55242814709866	53.879703227784546	143893
621a8d2b9259757f49e91262940f3d57343810e1	analysis of locking policies in database management systems	performance analysis;hierarchical analytical modelling;transaction behaviour;system performance;global performance;analytical study;modelling level;o resource;transactionnal system;database management system;global performance model;probabilistic model;decentralized control;quantitative analysis	Quantitative analysis of locking mechanisms and of their impact on the performance of transactionnal systems have yet received relatively little attention. Although numerous concurrency mechanisms have been proposed and implemented, there is an obvious lack of experimental as well as analytical studies of their behaviour and their influence on system performance. We present in this paper an analytical framework for the performance analysis of locking mechanisms in transactionnal systems based on hierarchical analytical modelling. Three levels of modelling are considered: at level 1, the different stages (lock request, execution, blocking) transactions of through during their life-time are described; the organization and operations of the CPU and I/O resources are analysed at level 2; transaction's behaviour during their lock request phase is analysed at modelling level 3. This hierarchical approach is applied to the analysis of a physical locking scheme involving a static lock acquisition policy. A simple probabilistic model of the transaction behaviour is used to derived the probability that a new transaction is granted the locks it requests given the number of transactions already active as a function of the granularity of the database. On the other hand, the multiprogramming effect due to the sharing of CPU and I/O resources by transactions is analysed using the standard queueing network approaches and the solution package QNAP. In a final step, the results on the blocking probabilities and the multiprogrammin effect are used as input of a global performance model of the transactionnal system. Markovian analysis is used to solve this model and to obtain the throughput of the system as a function of the data base granularity and other parameters. The results obtained provide a clear understanding of the various factors which determine the global performance, of their role and improtance. They also raise many new issues which can only be solved by further extensive experimental and analytical studies and show that two particular topics deserve special attention: the modelling of transaction behaviour and the modelling of locking overheads.	database;lock (computer science)	Dominique Potier;Ph. Leblanc	1980	SIGMETRICS Performance Evaluation Review	10.1145/1009375.806170	real-time computing;concurrency;database tuning;computer science;quantitative analysis;operating system;database;distributed computing;computer performance;consistency;database testing;hierarchical database model	DB	-20.16783664403121	46.75409990816014	143982
c3e96fc885a39ecedd3607697cb099786c482e03	full-stack performance model evaluation using probabilistic garbage collection simulation		Performance models can represent the performance relevant aspects of an enterprise application. Corresponding simulation engines use such models for simulating performance metrics (e.g., response times, resource utilization, throughput) and allow for performance evaluations without load testing the actual system. Creating such models manually often outweighs their benefits. Therefore, recent research created performance model generators, which can generate such models out of Application Performance Management software. However, a full-stack evaluation containing all relevant resources of an enterprise application (Central Processing Unit, memory, network and Hard Disk Drive) has not been conducted to the best of our knowledge. This work closes this gap using a pre-release version of the next generation industry benchmark SPECjEnterpriseNEXT of the Standard Performance Evaluation Corporation as example enterprise application, the Palladio Component Model as performance model and the performance model generator of the RETIT Capacity Manager. Furthermore, this work extends the generated model with a probabilistic garbage collection model to simulate memory allocation and releases more accurately.	benchmark (computing);central processing unit;enterprise software;garbage collection (computer science);hard disk drive;load testing;performance evaluation;simulation;throughput	Felix Willnecker;Andreas Brunnert;Bernhard Koch-Kemper;Helmut Krcmar	2015	Softwaretechnik-Trends		systems engineering;computer science;probabilistic logic;theoretical computer science;garbage collection	OS	-21.91633168380436	56.46474499538578	143984
5f6d5608cf1b3071c938a2271637fc555bf53231	hyracks: a flexible and extensible foundation for data-intensive computing	parallel programming data handling;data connectors;data intensive application;dag;open source hadoop platform;computer model;parallel programming;data operators;computer architecture;computational modeling computer architecture;computational modeling;hyracks end user model;next generation;open source hadoop platform data intensive computing partitioned parallel software platform dag data operators data connectors hyracks end user model;data handling;data intensive computing;use case;partitioned parallel software platform;user model;open source	Hyracks is a new partitioned-parallel software platform designed to run data-intensive computations on large shared-nothing clusters of computers. Hyracks allows users to express a computation as a DAG of data operators and connectors. Operators operate on partitions of input data and produce partitions of output data, while connectors repartition operators' outputs to make the newly produced partitions available at the consuming operators. We describe the Hyracks end user model, for authors of dataflow jobs, and the extension model for users who wish to augment Hyracks' built-in library with new operator and/or connector types. We also describe our initial Hyracks implementation. Since Hyracks is in roughly the same space as the open source Hadoop platform, we compare Hyracks with Hadoop experimentally for several different kinds of use cases. The initial results demonstrate that Hyracks has significant promise as a next-generation platform for data-intensive applications.	apache hadoop;computation;computer;data-intensive computing;dataflow;directed acyclic graph;experiment;fault tolerance;ibm notes;internet information services;job stream;mapreduce;open-source software;programming model;scheduling (computing);shared nothing architecture;uc browser	Vinayak R. Borkar;Michael J. Carey;Raman Grover;Nicola Onose;Rares Vernica	2011	2011 IEEE 27th International Conference on Data Engineering	10.1109/ICDE.2011.5767921	computer simulation;use case;real-time computing;user modeling;computer science;operating system;group method of data handling;data-intensive computing;data mining;database;programming language;computational model	DB	-29.111113855614214	52.72129936486599	144091
0f69b05020b1a5a71effa7fe39ea227b21d16e9f	storage systems for national information assets	memory architecture;storage management;national storage laboratory;climatic models;digital imaging;distributed storage hierarchies;distributed storage systems;gigabyte class files;integrated testbed system;magnetic fusion energy models;national information assets;network-attached storage;storage system management;storage system services;storage systems	An industry-led collaborative project, called the National Storage Laboratory, has been organized to investigate technology for storage systems that will be the future repositories for our national information assets. Lawrence Livermore National Laboratory through its ● National Energy Research Supercomputer Center (NERSC) is the operational site and the provider of applications. It is anticipated that the integrated testbed system will represent a significant advance in the 9 technology for distributed storage systems capable of handling gigabyte class files at gigabit-per-second data	clustered file system;gigabit;gigabyte;supercomputer;testbed	Robert A. Coyne;Harry Hulen;Richard W. Watson	1992			parallel computing;simulation;converged storage;data processing;performance;object storage;computer science;theoretical computer science;operating system;digital imaging;database;memory;information repository;system testing;information system;computer network;collaboration	HPC	-26.351279685972628	52.099878054647974	144534
7df281c167cafc07a47e1eb362f26bcb83a0c357	clustering versus shared nothing: a case study	databases;software;application software;shared nothing;application server;databases middleware system testing computer architecture web server traffic control application software containers computer applications computer networks;traffic control;client server systems;data mining;computer applications;computer networks;computer architecture;load balance shared nothing clustering;software architecture;servers;round robin;internet;clustering;server clustering;shared nothing design;software architecture client server systems internet;scalable network architecture;system testing;middleware;network architecture;load balance;web server;web application system server clustering shared nothing design internet scalable network architecture software architecture;containers;web application system	The massive growth of the Internet paired with the rise in dynamic website content has increased the need for scalable network architectures. While these various network architectures should be transparent to the client, the speed, reliability, and maintainability of the system depends on the particular architecture that is implemented. This paper will discuss the findings from a case study that tests the speed of two software architectures that are often implemented to build scalable web application systems. The first architecture is server clustering with shared resources. Server clustering can be defined as a group of servers that directly share resources and actively partitions work based on work loads. Thus client traffic to the cluster can be distributed across several physical machines each running an instance of the application server. The other architecture is a shared nothing design, where application servers do not share resources, except for a dispatcher (load balancer). This paper addresses the question, what is the performance overhead of adding application servers into a tiered system.	application server;cluster analysis;computer cluster;dark age of camelot;jdbc;load balancing (computing);overhead (computing);persistence (computer science);scalability;server (computing);shared nothing architecture;software architecture;web application	Jonathan Lifflancer;Adam McDonald;Orest Pilskalns	2009	2009 33rd Annual IEEE International Computer Software and Applications Conference	10.1109/COMPSAC.2009.124	software architecture;application software;the internet;network architecture;computer cluster;computer science;load balancing;operating system;middleware;database;cluster analysis;computer applications;system testing;world wide web;shared nothing architecture;web server;application server;client–server model;server	HPC	-26.111122640509198	55.31029650685956	144551
bcd0e4e8a6525e59393b94d5c8950dc3705b4d47	supporting scalable and distributed data subsetting and aggregation in large-scale seismic data analysis	distributed data;large dataset;data processing;data analysis;large scale;distributed environment;image reconstruction;mass storage system;middleware;seismic data analysis;parallel implementation;high end computing;data driven applications	The ability to query and process very large, terabytescale datasets has become a key step in many scientific and engineering applications. In this paper, we describe the application of two middleware frameworks in an integrated fashion to provide a scalable and efficient system for execution of seismic data analysis on large datasets in a distributed environment. We investigate different strategies for efficient querying of large datasets and parallel implementations of a seismic image reconstruction algorithm. Our results on a state-of-the-art mass storage system coupled with a high-end compute cluster show that our implementation is scalable and can achieve about 2.9 Gigabytes per second data processing rate – about 70% of the maximum 4.2GB/s application-level raw I/O bandwidth of the storage platform.	algorithm;computer data storage;gigabyte;input/output;iterative reconstruction;mass storage;middleware;scalability	Xi Zhang;Benjamin Rutt;Ümit V. Çatalyürek;Tahsin M. Kurç;Paul L. Stoffa;Mrinal K. Sen;Joel H. Saltz	2006	IJHPCA	10.1177/1094342006067471	iterative reconstruction;data processing;computer science;data science;operating system;middleware;data mining;database;data analysis;distributed computing environment	HPC	-19.886465034516785	53.049026898381	144631
b74d17106b805e01d55dbfdd56de430ae59765dc	case study on the recovery of a virtual large-scale disk	storage system;fault tolerant;large scale;fault tolerance;virtual large storage;mean time to repair	With the recent flood of data, one of the major issues is the storage thereof. Although commodity HDDs are now very cheap, appliance storage systems are still relatively expensive. As a result we developed the VLSD (Virtual Large-Scale Disk) toolkit in order to construct large-scale storage using only cheap commodity hardware and software. We also developed a prototype of the large-scale storage system by using the VLSD to collect free disk space on PCs. However, the reliability of this storage depends on the MTTR (Mean Time to Repair). In this paper, we evaluate the MTTR of our prototype and then discuss its efficiency.		Erianto Chai;Minoru Uehara;Hideki Mori	2008		10.1007/978-3-540-85693-1_17	embedded system;fault tolerance;real-time computing;converged storage;computer hardware;computer science;operating system;database	DB	-19.662562309149575	51.937182176054996	144735
be5130ae146cb1e53bd1aa246bc00e724f27aebe	multiagent-based distributed backup system for individuals	distributed system;multi agent;agent framework;computer crashes computers security engines writing databases;agent framework multi agent distributed system;storage management distributed processing multi agent systems redundancy;multiagent based distributed backup system user distribution backup catastrophe geographically distributed backup large scale disaster	The demand for backup in preparation for data disappearance caused by a large-scale disaster, etc. is increasing greatly. For robust backup, geographically distributed backup to save data is indispensable. If the backup is distributed, then total loss of data after a catastrophe is unlikely. For this study, we assume that a user is a college student, and assume distributed backup among plural universities. Although such a user distribution backup is applicable at low cost, it entails some difficulties. We propose to resolve these problems using agent technology.	agent-based model;backup;catastrophe theory;total loss	Takahiro Uchiya;Motohiro Shibakawa;Ichi Takumi;Tetsuo Kinoshita	2015	2015 IEEE/ACIS 14th International Conference on Computer and Information Science (ICIS)	10.1109/ICIS.2015.7166620	backup software;continuous data protection;real-time computing;computer science;database;distributed computing;computer security	DB	-22.638896841579562	51.634816202227746	144760
52d82fd5da8ed77bb72fe9492873d1e2868f9fc3	optimized processing of multiple aggregate continuous queries	experimental analysis;data stream;continuous query;query optimization;data streams;aggregate continuous queries;data stream management system	Data Streams Management Systems are designed to support monitoring applications, which require the processing of hundreds of Aggregate Continuous Queries (ACQs). These ACQs typically have different time granularities, with possibly different selection predicates and group-by attributes. In order to achieve scalability in the presence of heavy workloads, in this paper, we introduce the concept of 'Weaveability' as an indicator of the potential gains of sharing the processing of ACQs. We then propose Weave Share, a cost-based optimizer that exploits weaveability to optimize the shared processing of ACQs. Our experimental analysis shows that Weave Share outperforms the alternative sharing schemes generating up to four orders of magnitude better quality plans. Finally, we describe a practical implementation of the Weave Share optimizer.	aggregate data;aggregate function;continuous integration;experiment;management system;mathematical optimization;query optimization;scalability	Shenoda Guirguis;Mohamed A. Sharaf;Panos K. Chrysanthis;Alexandros Labrinidis	2011		10.1145/2063576.2063793	query optimization;real-time computing;computer science;data mining;database;data stream mining;world wide web;experimental analysis of behavior	DB	-19.51929824830144	55.4520207223291	144935
f664ec9fbbdbe33150bff643f1908796485344b0	dependability in grids	distributed system;optical network units;application software;grid applications;optical fiber networks;grid computing dependability;voting;dependability;middleware;optical sensors;intelligent networks;grid computing;next generation networking;wireless sensor networks;middleware application software voting costs intelligent networks next generation networking wireless sensor networks optical network units optical fiber networks optical sensors	This description seems like it could apply to any traditional distributed system. However, a key difference is that many Grids will provide services that are orchestrations of multiple services, hosted on different sites and across organizational and administrative boundaries, thus raising a lot of new issues. The Globus Toolkit currently at version 4 is the accepted reference implementation for Grid-computing middleware, while Global Grid Forum members work on Grid-computing standards. But, our concern in this article is the last point in Foster's checklist. For all the work being done to provide usable Grid middleware, the subject of dependability has been strangely neglected. The community hasn't launched a large-scale effort to provide tools to help develop dependable Grid applications.	dependability;distributed computing;grid computing;middleware;reference implementation	Paul Townend;Jie Xu	2005	IEEE Distributed Systems Online	10.1109/MDSO.2005.63	embedded system;intelligent network;application software;wireless sensor network;voting;computer science;operating system;middleware;dependability;distributed computing;grid computing;computer network	HPC	-33.46672115393525	47.65005751383335	145001
71b4ade33457348522543bd737eef2a241399b67	proposal of virtual network configuration acquisition function for data center operations and management system	data center operation;xml-based management information model;system utilization;configuration management process;virtual-network management system;data center;vlan-supported switch;operation time;virtual network configuration acquisition;proposed system;virtual network;virtual-network configuration	Virtualization technologies have been widely deployed in data centers to improve the system utilization. However, they cause increased workload for operators to clarify the structure of virtual networks in data centers. To reduce the operation time, this paper provides the virtual-network management system which automates the integration of the configurations of the virtual networks. The proposed system collects the configurations from server virtualization platforms and VLAN-supported switches, and integrates these configurations according to the newly developed XML-based management information model for virtual-network configurations. The preliminary evaluations show that the proposed system helps to reduce the time to collect and update the configurations by about 40 percent. This result implies that the proposed system is effective for improving the configuration management process for virtual networks in data centers.	data center;management system	Hideki Okita;Masahiro Yoshizawa;Keitaro Uehara;Kazuhiko Mizuno;Toshiaki Tarui;Ken Naono	2010		10.1007/978-3-642-21878-1_77	computer science;operating system;database;distributed computing	Networks	-30.664069251619164	54.7720540472059	145021
ddbaa9316e2e81d49e5f2c8a35d8723ecc348efc	two case studies of the application of dynamic modeling techniques in performance assessment and prediction of complex shared storage architectures	application development;difference operator;heterogeneous systems;high performance computing;dynamic model;predictive models supercomputers storage area networks file systems computer architecture surges operating systems network servers high performance computing steady state;surges;storage area networks;computer architecture;system evaluation;network servers;operating system;file system;initial condition;high performance computer;predictive models;high performance;supercomputers;wide area network;file systems;performance assessment;operating systems;steady state	In evaluating alternatives in large supercomputer complexes, the tradeoffs between when NFS-type distribution of data and the more recent 'cluster' or 'Storage Area Network' approaches are optimal can require careful analysis. Further, many HPC (High Performance Computing) workloads are not limited to using only large files accessed with large block I/O requests. Some loads can be very mixed. File systems can fill up rapidly, resulting in performance which deviates substantially from that observed in relatively empty file systems, when most accesses are on the outer tracks and metadata accesses are most efficient. Establishing the optimal file system parameters for a group of homogeneous supercomputers attempting to exchange data is challenging enough. When heterogeneity is added to the HPC complex, as - for example - when using one of the commercially-available heterogeneous SAN software products, the task of defining a configuration to ensure the desired behavior, both under a steady-state load and during surge conditions, can become daunting. In the work we describe in this paper, we investigated two different systems and storage architecture situations. The first was a relatively homogenous system, more than 10 supercomputers of the same vendor, same operating system level, similar or identical capabilities, which needed to share primarily large file, large block data at several different sustained rates but with very predictable performance for certain transactions under even the heaviest surges. There are, however, some smaller files in this workload. The goal was to evaluate whether a SAN, or NFS using high bandwidth local links, or some combination of both, could best provide the desired system behavior. An additional goal was to identify the various tuning options which could be used by the system administrators and application developers to ensure the performance required was preserved as the system grew or various initial conditions changed substantially. The second system evaluated was a heterogeneous system, consisting of servers and supercomputers of different capabilities and different product generations, as well as running different operating systems on hardware from different vendors. Connections between these systems ranged from high-performance local links to WANs (wide area networks). Some of the issues considered in this system related to the proximity of the dataset to the intended compute server, since the goal was optimal workload distribution as well as deadline assurance.	heterogeneous computing;initial condition;input/output;operating system;server (computing);steady state;storage area network;supercomputer;system administrator	Marti Bancroft;Phillip L. Snyder;Mark Woodyard	2001	2001 Eighteenth IEEE Symposium on Mass Storage Systems and Technologies	10.1109/MSS.2001.10024	parallel computing;real-time computing;computer science;operating system	HPC	-23.61467972816867	54.92569273560463	145108
9051676a125df018aa40f1dc621be158f3d2c4d2	an efficient pointer swizzling method for navigation intensive applications	efficient pointer swizzling method;navigation intensive applications	In this paper we introduce the notion of reservation and residency in the context of object faulting and describe the pointer swizzling method employed in our implementation of a persistent C language P8L. Although the method does not assume any special hardware support, our experiments indicate that the reservation checking method is efficient enough that the addition of reservation checks does not severely compromise the performance in navigation intensive applications. Navigation performance is maintained by reducing the frequency of reservation checks and by replacing each persistent reference with a surrogate upon object fault. The replacement condenses a long persistent identifier down to the size of a virtual memory pointer. The virtual memory requirement of our scheme is modest compared to pointer swizzling at page fault tirne[l]. In one of the test cases, our software based scheme outperformed the hardware based scheme. Compared to pointer swizzling upon discovery as is implemented in [2], our implementation runs about 4 to 10 times faster in terms of pure in-memory navigation.	pointer swizzling	Shinji Suzuki;Masaru Kitsuregawa;Mikio Takagi	1994			real-time computing;computer science;database;distributed computing;pointer swizzling	Robotics	-19.712688883383144	48.41166469136352	145284
d0f80a2596e6544967f16eba21b74aba03c98045	a remote procedure call approach for extreme-scale services		When working at exascale, the various constraints imposed by the extreme scale of the system bring new challenges for application users and software/middleware developers. In that context, and to provide best performance, resiliency and energy efficiency, software may be provided as a service oriented approach, adjusting resource utilization to best meet facility and user requirements. These services, which can offer various capabilities, may be used on demand by a broad range of applications.	ibm websphere extreme scale;middleware;remote procedure call;requirement;user requirements document	Jérome Soumagne;Philip H. Carns;Dries Kimpe;Quincey Koziol;Robert B. Ross	2015	CoRR		dce/rpc;parallel computing;real-time computing;computer science;operating system;database;distributed computing;remote procedure call;world wide web	SE	-27.104280684196855	56.44356568972512	145356
6253af9e36055c62b782a7099e0f5f27426a7da9	primary-backup object replications in java	distributed system;high availability;fault tolerant;parallel programming;service replication;parallel languages object oriented languages object oriented programming software reliability parallel programming;object oriented programming;remote method invocation;object replication;fault tolerance;distributed object;java protocols packaging checkpointing distributed computing high performance computing mathematics australia availability fault tolerant systems;performance studies primary backup object replications java service replication availability fault tolerance distributed systems replication schemes passive replication active replication implementation approaches passive primary backup scheme remote method invocation approach replica proxy approach java rmi java network packages primary backup protocol failed server general naming service dynamic memberships replica groups;primary backup;software reliability;parallel languages;object oriented languages	Service replication is a key to providing high availability, fault tolerance, and good performance in distributed systems. Various replication schemes have been proposed, they are based on two streams of techniques, namely passive replication and active replication. This paper focuses on two implementation approaches of the passive primary-backup scheme, remote method invocation approach and replica-proxy approach, using Java RMI and Java network packages respectively. Issues addressed in this paper also include: the primary-backup protocol; restarting a failed server at any site; and a general naming service for the maintenance of dynamic memberships of replica groups. Finally, performance studies based on two implementation approaches are given.	backup;distributed computing;fault tolerance;high availability;java remote method invocation;replication (computing);server (computing);subroutine	Li Wang;Wanlei Zhou	1998		10.1109/TOOLS.1998.713589	parallel computing;real-time computing;computer science;distributed computing	HPC	-28.237767579528875	46.97385618300387	145425
53e4ba6573780c8c133c2da996e057b9b29cb897	a concurrency control model for multiparty business processes	concurrency control;business process	Although the issue of atomicity of multiparty business processes is well understood and widely studied, the concurrency control issues of multiparty business processes is not studied nor well understood. In this paper, we restrict ourselves on this issue. First we motivate the need of concurrency control in this context. Then, we present a liberal correctness criterion, called set-serializability and a scheduler based on timestamp ordering rule that produces set-serializable executions. Technically the scheduler is very simple and it can be easily integrated with the protocol that ensures the atomicity of the multiparty business processes. In implementing the atomicity protocol and the scheduler we utilize the WS-Coordination, which is a general and extensible framework for defining protocols for coordinating activities that are part of business	atomicity (database systems);business process;concurrency (computer science);correctness (computer science);database transaction;lock (computer science);requirement;scheduling (computing);serializability;throughput;timestamp-based concurrency control;ws-coordination	Juha Puustjärvi	2008			optimistic concurrency control;real-time computing;isolation;computer science;concurrency control;database;distributed computing;multiversion concurrency control;business process;non-lock concurrency control;concurrent object-oriented programming;distributed concurrency control	Theory	-23.72539316565779	47.57999594969611	145600
374b111e95855be498f140d6303feeeed219d3fb	fpga-based system for the acceleration of cloud microservices		Scalability, distributivity, interoperability and modularity introduced in cloud computing have deeply changed the legacy data center's architecture, implementation and processing capabilities. The atomic network services offered by cloud architectures are called microservices. Unlike virtual machines, microservices can be implemented in the form of low resources footprint applications as containers (Docker, LXC etc.) or even smaller as unikernels (IncludeOS, ClickOS, Rumprun, HermitOS etc.). The need to efficiently offload the processing of computation-intensive applications has motivated the introduction of Field Programmable Gate Arrays (FPGA) boards in servers. FPGAs can nowadays be considered as cloud-standard processing resources. However, in today's cloud data centers, FPGAs cannot be accessed to run concurrent microservices. This severely limits the efficient deployment of microservices. This paper aims at introducing an FPGA-based system for the concurrent acceleration of cloud-native microservices onto FPGAs.	application programming interface;cloud computing;computation;data center;docker;field-programmable gate array;interoperability;lxc;microservices;run time (program lifecycle phase);scalability;scheduling (computing);server (computing);server-side;service virtualization;software deployment;virtual machine	Julien Lallet;Andrea Enrici;Anfel Saffar	2018	2018 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB)	10.1109/BMSB.2018.8436912	real-time computing;software deployment;parallel computing;computer science;cloud computing;microservices;architecture;scalability;data center;virtual machine;server	Arch	-28.895716475995503	56.95872865438864	145609
0db689e29108123d9f6696fb472d2ad0f8cde821	parallel computing in 2010: opportunities and challenges in a networked world (abstract).	parallel computer	High-performance computing is periodically transformed due to exponential changes in underlying technologies; the emergence of commodity clusters is just the latest instance of this phenomenon. I examine two technology trends that seem likely to have such a transforming effect in the next ten years, namely optical networking and web services. I argue that these two trends will both make parallel computing increasingly important and introduce new challenges due to the need for parallel programs to operate in increasingly dynamic and heterogeneous environments. I describe technologies being developed to address these challenges in three projects in which I am involved, namely Globus, GrADS, and Cactus.	emergence;parallel computing;time complexity;web service	Ian T. Foster	2001		10.1145/379539.379546	parallel computing;computer science;theoretical computer science;distributed computing	HPC	-27.894538200875257	55.76886821830883	145689
7a6b7f1cc047739bdeb015b3ec0a766168b35ae6	goal-oriented dynamic buffer pool management for data base systems	database system;optimisation;goal oriented dynamic buffer pool management;goal orientation;goal satisfaction algorithm;semiconductor memory;database management systems;random access response time goal;storage management;multiple buffer pools;resource management;buffer storage;contracts;delay technology management buffer storage database systems computer science laboratories contracts resource management relational databases semiconductor memory;trace driven simulations;technology management;complex constrained optimization problem;storage management relational databases buffer storage optimisation transaction processing;dynamic tuning;database systems;pre defined response time goals;database workload;relational databases;computer science;dynamic buffer pool size adjustment;transaction processing;database management system;virtual storage;trace driven simulations goal oriented dynamic buffer pool management database management systems dynamic buffer pool size adjustment multiple buffer pools complex constrained optimization problem random access response time goal database workload pre defined response time goals goal satisfaction algorithm virtual storage dynamic tuning	This paper presents a technique for performing dynamic goal oriented buuer pool management for database management systems. To dynamically adjust the buuer pool sizes for the multiple buuer pools provided by database management systems is a complex constrained optimization problem. In the goal oriented approach, the user speciies each buuer pool's random access response time goal and the total available number of buuers for all buuer pools. The problem is to dynamically expand or contract the buuer pool sizes based on the database workload to achieve these pre-deened response time goals for each buuer pool while maintaining the same total number of buuers in the database system. Our goal satisfaction algorithm monitors goal satisfaction of each buuer pool and periodically changes buuer pool sizes to improve goal satisfaction. The expanding and contracting process does not allocate new or free up existing virtual storage. We demonstrate that dynamic tuning can greatly improve buuer pool goal satisfaction through trace driven simulations.	algorithm;constrained optimization;constraint (mathematics);mathematical optimization;optimization problem;oracle database;random access;response time (technology);simulation	Jen-Yao Chwng;Donald F. Ferguson;George Wang;Christos Nikolaou;Jim Teng	1995		10.1109/ICECCS.1995.479328	semiconductor memory;real-time computing;transaction processing;relational database;computer science;technology management;resource management;goal orientation;database;distributed computing	DB	-24.69533229032693	48.99545875079023	145898
982ac76b5146dd312f7515bbd1fc531e3db7563f	a timestamp based multi-version stm protocol that satisfies opacity and multi-version permissiveness		Software Transactional Memory Systems (STM) are a promising alternative to lock based systems for concurrency control in shared memory systems. In multiversion STM systems, each write on a transaction object produces a new version of that object. The advantage obtained by storing multiple versions is that one can ensure that read operations do not fail. Opacity is a commonly used correctness criterion for STM systems. Multi-Version permissive STM system never aborts a read-only transaction. Although many multi-version STM systems have been proposed, to the best of our knowledge none of them have been formally proved to satisfy opacity. In this paper we present a time-stamp based multiversion STM system that satisfies opacity and mv-permissiveness. We formally prove the correctness of the proposed STM system. We also present garbage collection procedure which deletes unwanted versions of the transaction objects and formally prove it correctness.	algorithm;basic stamp;benchmark (computing);concurrency (computer science);concurrency control;correctness (computer science);garbage collection (computer science);mv-algebra;read-only memory;shared memory;software transactional memory	Priyanka Kumar;Sathya Peri	2013	CoRR		parallel computing;real-time computing;computer science;distributed computing	DB	-22.261341048803537	47.70468374754938	145948
a0a1a34f11baddd77b249852c44f4a676c0f47d8	an efficient causal logging scheme for recoverable distributed shared memory systems	distributed system;fault tolerant;lazy release consistency;checkpointing;fault tolerant system;efficient implementation;fault tolerant systems;distributed shared memory systems;rollback recovery;distributed shared memory;data structure;message logging;memory model	This paper presents a causal logging scheme for the lazy rele ase consistent distributed shared memory systems. Causal logging is a very attractive approach to provide the fault tolerance for the distributed systems, since it eliminates the need of stable logging. How ever, since inter-process dependency must causally be transferred with the normal messages, the exces sive message overhead has been a drawback of this approach. In order to achieve an efficient implementa tion of causal logging for the distributed shared memory system, data structures and operations suppo rted by the lazy release consistency memory model are utilized. As a result, the causal logging for th e lazy release consistent distributed shared memory system can be implemented by adding the minimum infor mation for the dependency tracking. To evaluate the performance of the proposed scheme, the prop osed logging scheme has been implemented on top of the CVM distributed shared memory system. Th e experimental results show that the logging operation requires only 1% 4.4% increases in the ex ecution time. Keywords–Checkpointing, Distributed shared memory systems, Fault t olerant systems, Message logging, Lazy release consistency, Rollback-recovery. ————————————– An earlier version of this work has appeared in the Proceedin gs of the ACM Symposium on Applied Computing, 2000.	application checkpointing;causal filter;data structure;distributed computing;distributed shared memory;fault tolerance;lazy evaluation;overhead (computing);release consistency;schedule (computer science);symposium on applied computing	Taesoon Park;Inseon Lee;Heon Young Yeom	2002	Parallel Computing	10.1016/S0167-8191(02)00165-5	distributed shared memory;shared memory;fault tolerance;parallel computing;real-time computing;distributed memory;data structure;computer science;consistency model;distributed computing;programming language	OS	-21.279441159537562	47.839684733245285	146160
0006ac88e62f91b91368f595c826c11b7a1db9ee	optimizing the quality of service for a publish/subscribe system	reliability;measurement;computer crashes;routing;timeliness;pub sub systems;peer to peer computing;quality of service;cloud computing	Publish/Subscribe (Pub/Sub) systems are the middleware which provides the distributed event detection. The increasing requirements from applications have driven Pub/Sub systems to provide diversified quality of service (QoS). Therefore, this paper builds a Pub/Sub system Phoenix on the cloud which can meet the different needs of system reliability and user timeliness. In brief, Phoenix can deal with link failure, broker failure and cluster churning to guarantee that the system can run normally. Meanwhile, it provides the timeliness guarantee mechanism to manage to satisfy the timeliness specified by the users with its best effort. The experimental results show that Phoenix is fairly reliable and accommodates the users' personalized timeliness requirements well.	best-effort delivery;cloud computing;code refactoring;middleware;optimizing compiler;personalization;publish–subscribe pattern;quality of service;requirement	Fusang Zhang;Yuwei Yang;Yuyao Yang;Beihong Jin	2015	2015 IEEE 12th Intl Conf on Ubiquitous Intelligence and Computing and 2015 IEEE 12th Intl Conf on Autonomic and Trusted Computing and 2015 IEEE 15th Intl Conf on Scalable Computing and Communications and Its Associated Workshops (UIC-ATC-ScalCom)	10.1109/UIC-ATC-ScalCom-CBDCom-IoP.2015.92	routing;real-time computing;quality of service;cloud computing;computer science;operating system;reliability;database;world wide web;computer security;measurement;computer network	Embedded	-25.437930693937293	54.86408201477187	146263
dc1017bc0ff52ba33eb8ffb27409ada439505861	porting the pip proto-kernel's model to multi-core environments		In order to bring integrity and confidentiality through memory isolation in embedded devices, we developed Pip, a proto-kernel ensuring only memory isolation and control flow in the Internet of Things's computing environments. Its API is verified, ensuring the correctness of the memory isolation properties. Nevertheless, today, like Cloud Computing environments, IoT environments are becoming more and more multi-core and parallelized. Those environments introduce many new issues of consistency and concurrency, and require specific software architectures. In this paper, we discuss those new aspects, and the evolution of Pip's model into a multi-core model designed for those environments.	application programming interface;cloud computing;concurrency (computer science);confidentiality;control flow;correctness (computer science);embedded system;internet of things;kernel (operating system);memory protection;multi-core processor;parallel computing;software architecture	Valdinar de A. Rocha;Julien Iguchi-Cartigny;Gilles Grimaud	2018	2018 IEEE 16th Intl Conf on Dependable, Autonomic and Secure Computing, 16th Intl Conf on Pervasive Intelligence and Computing, 4th Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress(DASC/PiCom/DataCom/CyberSciTech)	10.1109/DASC/PiCom/DataCom/CyberSciTec.2018.00108	porting;control flow;correctness;software;multi-core processor;concurrency;cloud computing;distributed computing;computer science;multiplexing	DB	-28.17095167211758	56.24247573793619	146270
43a6d5bcfada9bccc966d2f883230228539bda40	an adaptive distributed simulator for cloud and mapreduce algorithms and architectures	computers;computational modeling adaptation models load modeling distributed databases monitoring computers cloud computing;mapreduce simulations;simulations cloud simulations mapreduce simulations adaptive systems in memory data grids;cloud simulations;parallel processing cloud computing digital simulation grid computing java middleware;computational modeling;monitoring;adaptive systems;general purpose auto scaler middleware adaptive distributed simulator mapreduce algorithms scalability cloud algorithms data grid platforms distributed cloud fully parallel cloud mapreduce simulator java in memory data grid platforms cloud 2 sim distributed cloud simulator hazelcast in memory key value store infinispan multitenanted deployment;distributed databases;in memory data grids;adaptation models;load modeling;cloud computing	Scalability and performance are crucial for simulations as much as accuracy is. Due to the limited availability and access to the variety of resources, cloud and MapReduce solutions are often evaluated on simulator platforms. As the complexity of the architectures and algorithms keep increasing, simulations themselves become large and resource-hungry. Simulators can be designed to be adaptive, exploiting the clusters and data-grid platforms. This paper describes the research for the design, development, and evaluation of a complete fully parallel and distributed cloud and MapReduce simulator (Cloud2Sim), leveraging the Java in-memory data grid platforms. Cloud2Sim provides a concurrent and distributed cloud simulator, by extending Cloud Sim cloud simulator, using Hazel cast in-memory key-value store. It also provides an assessment of the MapReduce implementations of Hazel cast and Infinispan, with means of simulating MapReduce executions. Cloud2Sim scales out the cloud and MapReduce simulations to multiple nodes running Hazel cast and Infinispan, based on load. The distributed execution model and adaptive scaling solution could further be leveraged as a general purpose auto-scaler middleware for a multi-tenanted deployment.	algorithm;attribute–value pair;cloudsim;computer cluster;image scaling;in-memory database;infinispan;java;key-value database;limited availability;mapreduce;middleware;scalability;simulation;software deployment;usb flash drive	Pradeeban Kathiravelu;Luís Veiga	2014	2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing	10.1109/UCC.2014.16	parallel computing;real-time computing;cloud computing;computer science;adaptive system;operating system;cloud testing;distributed computing;computational model;distributed database	HPC	-20.846324203455378	53.823423373735096	146372
7c7216ec4179948fa6dccec3b0fd58f975edaa2b	performance prediction upon toolchain migration in model-based software	prediction model based development migration automated code generation estimation;scade;complexity theory;thesis or dissertation;measurement;computational modeling measurement predictive models unified modeling language complexity theory atmospheric modeling mathematical model;migration;automated code generation;computational modeling;estimation;unified modeling language;mathematical model;model based development;predictive models;atmospheric modeling;prediction	Changing the development environment can have severe impacts on the system behavior such as the execution-time performance. Since it can be costly to migrate a software application, engineers would like to predict the performance parameters of the application under the new environment with as little effort as possible. In this paper, we concentrate on model-driven development and provide a methodology to estimate the execution-time performance of application models under different toolchains. Our approach has low cost compared to the migration effort of an entire application. As part of the approach, we provide methods for characterizing model-driven applications, an algorithm for generating application-specific microbenchmarks, and results on using different methods for estimating the performance. In the work, we focus on SCADE as the development toolchain and use a Cruise Control and a Water Level application as case studies to confirm the technical feasibility and viability of our technique.	algorithm;benchmark (computing);model-driven architecture;model-driven engineering;modeling language;performance prediction;randomness;software metric;solver;toolchain	Aymen Ketata;Carlos Moreno;Sebastian Fischmeister;Jia Hui Liang;Krzysztof Czarnecki	2015	2015 ACM/IEEE 18th International Conference on Model Driven Engineering Languages and Systems (MODELS)	10.1109/MODELS.2015.7338261	unified modeling language;atmospheric model;estimation;simulation;prediction;computer science;human migration;theoretical computer science;mathematical model;predictive modelling;computational model;model-based design;measurement	SE	-24.862717225800722	56.47677599821296	146416
1de1e505b5870444866ec145cecc4c2eb9601f98	a scientific workflow supported environment over hybrid infrastructure for aerodynamics design	aerodynamics design;portals;cost reduction scientific workflow supported environment hybrid infrastructure aerodynamics design computing technology e science research environments supercomputers on demand resources scalable resources scientific domains cloud architecture synergy effect grid computing private cloud computing elastic resources;aerodynamics;e science;scientific workflow;cost reduction;science cloud;workflow management software aerodynamics cloud computing computational fluid dynamics cost reduction grid computing scientific information systems;cloud computing computational modeling computer architecture portals aerodynamics monitoring computational fluid dynamics;computational fluid dynamics;aerodynamics design hybrid infrastructure science cloud scientific workflow e science;computer architecture;computational modeling;monitoring;workflow management software;grid computing;scientific information systems;hybrid infrastructure;cloud computing	As computing technologies develop, various studies for e-Science have been actively developed for many years. The advent of Cloud computing, for example, enables scientists to expand their research environments over supercomputers to on-demand and scalable resources. However, several performance drawbacks on the cloud computing can cause considerable obstacles in scientific domains, despite its strong merits. A hybrid infrastructure which consists of the existing and cloud architecture thus can produce a synergy effect for utilizing resources efficiently. In this paper, we proposed hybrid infrastructure-supported a scientific workflow environment for Aerodynamics design and demonstrated its superiority. Hybrid infrastructure covers grid and private cloud computing in this paper. Especially, we focused on improving performance by supporting hybrid infrastructure and efficient usages of physical resources. Since it offers diverse types of computing infrastructures including cloud computing, it can serve elastic resources regardless of the number of tasks for experiments or limitations of space and can reduce costs of time as well as budget during simulations.	amazon elastic compute cloud (ec2);cloud computing;computational science;e-science;experiment;performance evaluation;prototype;scalability;scheduling (computing);simulation;supercomputer;synergy;verification and validation	Seoyoung Kim;Hyejeong Kang;Yoonhee Kim	2012	2012 14th Asia-Pacific Network Operations and Management Symposium (APNOMS)	10.1109/APNOMS.2012.6356101	computational science;simulation;cloud computing;aerodynamics;computational fluid dynamics;computer science;theoretical computer science;operating system;utility computing;computational model;converged infrastructure;grid computing	HPC	-21.594565379401352	58.628111942377515	146593
ca03f256a19e59e9a4ed32320a943bee8e8ba618	an auditing language for preventing correlated failures in the cloud		Todayu0027s cloud services extensively rely on replication techniques to ensure availability and reliability. In complex datacenter network architectures, however, seemingly independent replica servers may inadvertently share deep dependencies (e.g., aggregation switches). Such unexpected common dependencies may potentially result in correlated failures across the entire replication deployments, invalidating the efforts. Although existing cloud management and diagnosis tools have been able to offer post-failure forensics, they, nevertheless, typically lead to quite prolonged failure recovery time. In this paper, we propose a novel language framework, named RepAudit, that manages to prevent correlated failure risks before service outages occur, by allowing cloud administrators to proactively audit the replication deployments of interest. In particular, RepAudit consists of three new components: 1) a declarative domain-specific language, RAL, for cloud administrators to write auditing programs expressing diverse auditing tasks; 2) a high-performance RAL auditing engine that generates the auditing results by accurately and efficiently analyzing the underlying structures of the target replication deployments; and 3) an RAL-code generator that can automatically produce complex RAL programs based on easily written specifications. Our evaluation result shows that RepAudit can determine the top-20 critical correlated failure root causes in a replication system containing 30,528 devices within 1 minute, which is 400x more efficient in auditing time than state-of-the-art efforts. To the best of our knowledge, RepAudit is the first effort capable of simultaneously offering expressive, accurate and efficient correlated failure auditing to the cloud-scale replication systems.	cloud management;code generation (compiler);data center;domain-specific language;network architecture;network switch;source lines of code;system administrator	Ennan Zhai;Ruzica Piskac;Ronghui Gu;Xun Lao;Xi Wang	2017	PACMPL	10.1145/3133921	cloud management;theoretical computer science;real-time computing;computer science;network architecture;cloud computing;server;audit	PL	-23.957954815497004	55.577061555924956	146926
5482869ec24e0724135c72860e6151305846be65	ravel, a support system for the development of distributed, multi-user ve applications	sub task processes;level of service;arbitrary environment attribute information;groupware;software libraries;causally consistent event ordering;distributed processing;remote attribute virtual environment library;virtual reality;multi user;consistent attribute updates;distributed multi user virtual environment application development;support system;low latency communication;low latency;multi access systems;groupware virtual reality software libraries distributed processing development systems software tools interactive systems multi access systems;inter task communication;interactive immersive applications;multi user virtual environment;development systems;software tools;virtual environment;interactive immersive applications ravel remote attribute virtual environment library support system distributed multi user virtual environment application development low latency communication sub task processes arbitrary environment attribute information inter task communication consistent attribute updates causally consistent event ordering service efficiency;interactive systems;ravel;application software distributed computing voltage control information science educational institutions runtime system testing communication system control tracking computer networks;service efficiency	Presents the Remote Attribute Virtual Environment Library (RAVEL), a support system for the development and implementation of distributed, multi-user virtual environment (VE) applications. The system extends earlier work in low-latency communication between sub-task processes of an application to provide communication of arbitrary environment attribute information between tasks, independent of the location of the tasks involved or which task produces or consumes attribute data. In addition, the RAVEL system provides for consistent updates to a particular attribute, and provides for causally consistent ordering of events that occur in the environment, while providing the level of service efficiency required for immersive and interactive VE applications.	distributed computing;information flow (information theory);multi-user;requirement;virtual reality;virtual world	G. Drew Kessler;Larry F. Hodges;Mustaque Ahamad	1998	Proceedings. IEEE 1998 Virtual Reality Annual International Symposium (Cat. No.98CB36180)	10.1109/VRAIS.1998.658504	real-time computing;computer science;virtual machine;operating system;database;distributed computing;virtual reality;level of service;low latency	Visualization	-25.954055636671214	47.224861653718236	146938
35aa2a9cbe57a7d015b13fd5437960122c88e45f	design and implementation of a scalable and qos-aware stream processing framework: the quasit prototype	resource allocation;prototypes;runtime;omg dds scalable stream processing framework design qos aware stream processing framework design optimal trade offs resource consumption quasit model stream processing applications advanced qos based configuration quasit prototype architecture quasit prototype development omg dds quasit framework implementation horizontal scalability level dynamically available processing resource exploitation middleware platforms actor based threading model qos enabled inter process communication;computer architecture;quality of service runtime computer architecture data models java prototypes ports computers;software architecture;software architecture data handling middleware parallel processing quality of service resource allocation;middleware platforms stream processing scalability quality of service;middleware;ports computers;scalability;data handling;quality of service;stream processing;parallel processing;middleware platforms;data models;java	Today's stream processing scenarios are characterized by large volumes of data, e.g., generated by cyber-physical systems in a smart city, on which continuous analysis tasks need to be performed, often with very different optimal trade-offs between achieved QoS and associated resource consumption. Here we present the novel Quasit model and framework offering runtime support to stream processing applications. Differently from existing literature, Quasit originally allows advanced QoS-based configuration, which can be used to finely tune the framework to fit highly different real-world situations. The paper describes the architecture and development of the Quasit prototype by offering interesting insights and lessons learned about the most important design/implementation choices made, such as the actor-based threading model, or the QoS enabled inter-process communication based on OMG DDS. The reported experimental results, measured over simple real test beds, show that our Quasit framework implementation can provide a good level of horizontal scalability with limited overhead and good exploitation of dynamically available processing resources.	cyber-physical system;graphical user interface;inter-process communication;overhead (computing);prototype;quality of results;quality of service;scalability;scheduling (computing);smart city;software deployment;stream processing;threaded code;usability;video processing	Paolo Bellavista;Antonio Corradi;Andrea Reale	2012	2012 IEEE International Conference on Green Computing and Communications	10.1109/GreenCom.2012.54	parallel computing;real-time computing;computer science;operating system	Embedded	-21.69537845297818	57.27944620461239	146942
b4276c91ceafff912832dcc0a4be8976dc5660f3	fitch: supporting adaptive replicated services in the cloud		Despite the fact that cloud computing offers a high degree of dynamism on resource provisioning, there is a general lack of support for managing dynamic adaptations of replicated services in the cloud, and, even when such support exists, it is focused mainly on elasticity by means of horizontal scalability. We analyse the benefits a replicated service may obtain from dynamic adaptations in the cloud and the requirements on the replication system. For example, adaptation can be done to increase and decrease the capacity of a service, move service replicas closer to their clients, obtain diversity in the replication (for resilience), recover compromised replicas, or rejuvenate ageing replicas. We introduce FITCH, a novel infrastructure to support dynamic adaptation of replicated services in cloud environments. Two prototype services validate this architecture: a crash fault-tolerant Web service and a Byzantine fault-tolerant key-value store based on state machine replication.	attribute–value pair;byzantine fault tolerance;cloud computing;controller (control theory);data center;dependability;elasticity (data store);finite-state machine;international symposium on fundamentals of computation theory;key-value database;prototype;provisioning;requirement;scart;scalability;state machine replication;web service	Vinicius V. Cogo;André Nogueira;João Sousa;Marcelo Pasin;Hans P. Reiser;Alysson Neves Bessani	2013		10.1007/978-3-642-38541-4_2	real-time computing;computer science;operating system;database;distributed computing;world wide web;computer network	OS	-26.316680748463572	57.23865566723051	147012
c3bc09cf35b7b8bea3bda3ff924d646cd747a90b	a provenance-based adaptive scheduling heuristic for parallel scientific workflows in clouds	scientific experiment;scientific workflow;provenance;cloud computing	In the last years, scientific workflows have emerged as a fundamental abstraction for structuring and executing scientific experiments in computational environments. Scientific workflows are becoming increasingly complex and more demanding in terms of computational resources, thus requiring the usage of parallel techniques and high performance computing (HPC) environments. Meanwhile, clouds have emerged as a new paradigm where resources are virtualized and provided on demand. By using clouds, scientists have expanded beyond single parallel computers to hundreds or even thousands of virtual machines. Although the initial focus of clouds was to provide high throughput computing, clouds are already being used to provide an HPC environment where elastic resources can be instantiated on demand during the course of a scientific workflow. However, this model also raises many open, yet important, challenges such as scheduling workflow activities. Scheduling parallel scientific workflows in the cloud is a very complex task since we have to take into account many different criteria and to explore the elasticity characteristic for optimizing workflow execution. In this paper, we introduce an adaptive scheduling heuristic for parallel execution of scientific workflows in the cloud that is based on three criteria: total execution time (makespan), reliability and financial cost. Besides scheduling workflow activities based on a 3-objective cost model, this approach also scales resources up and down according to the restrictions imposed by scientists before workflow execution. This tuning is based on provenance data captured and queried at runtime. We conducted a thorough validation of our approach using a real bioinformatics workflow. The experiments were performed in SciCumulus, a cloud workflow engine for managing scientific workflow execution.	adaptive grammar;algorithm;analysis of algorithms;apache hadoop;automated planning and scheduling;baseline (configuration management);bioinformatics;cloud computing;computation;computational resource;computational science;computer;elasticity (cloud computing);elasticity (data store);elegant degradation;experiment;heuristic (computer science);high-throughput computing;makespan;operating-system-level virtualization;parallel computing;performance evaluation;performance prediction;programming paradigm;run time (program lifecycle phase);scheduling (computing);speedup;supercomputer;throughput;virtual machine;workflow engine	Daniel de Oliveira;Kary A. C. S. Ocaña;Fernanda Araujo Baião;Marta Mattoso	2012	Journal of Grid Computing	10.1007/s10723-012-9227-2	real-time computing;cloud computing;computer science;operating system;database;distributed computing;workflow management system;workflow engine;workflow technology	HPC	-21.43129976436548	58.678042323924835	147298
7176f3cee2280527ef2bd53caba9d52bf69ddc18	xos-ssh: a lightweight user-centric tool to support remote execution in virtual organizations	single sign on;security model;large scale;virtual organization;nas parallel benchmarks	Large-scale virtual organizations (VOs) often comprise resource providers from different administrative domains, each probably with a specific security model. Grids try to solve this problem by providing a new security infrastructure featured with single-sign on (SSO). However, the usability of Grids is often impaired by the complexity of configuring and maintaining the new security infrastructure as well as adapting to new interfaces of security enabled services. The co-existing of different Grid platforms and SSO solutions among resource providers makes this situation even worse. In this paper, we present XOS-SSH, a lightweight user-centric tool to support remote execution of jobs among heterogeneous nodes of VOs. XOS-SSH is a modified version of the widely used OpenSSH tool based on several OS-level VO support mechanisms developed in XtreemOS project [23]. XOS-SSH adopts a pluggable framework that is capable of supporting different authentication schemes and making them transparent to shell users. The performance evaluation of XOS-SSH around NAS Parallel Benchmarks (NPB) shows that our current implementation incurs trivial overhead comparing to the unmodified one.	authentication;benchmark (computing);byte;communications protocol;emoticon;experiment;flops;megabit;nas parallel benchmarks;openssh;operating system;overhead (computing);parallel computing;performance evaluation;prototype;scalability;seamless3d;single sign-on;smoothing;usability;virtual organization (grid computing)	An Qin;Haiyan Yu;Chengchun Shu;Bing Xu	2008			computer science;distributed computing;world wide web;computer security	HPC	-30.24467065336805	53.94581159579713	147354
d22666f8d5a79bb9cfea6e44046b66826c43ba9a	pbad: perception-based anomaly detection system for cloud datacenters	virtual machine;anomaly detection;response time;time factors servers support vector machines delays cloud computing computational modeling;virtual machine cloud datacenter anomaly detection response time cloud computing;cloud datacenter;pbad production data center environment system failure virtual machine cloud application cloud complexity component behavior analysis service response time response time monitoring multitier application user perception system state assessment threshold scheme anomaly detection mechanism i o utilization network utilization memory utilization cpu utilization utilization metrics system model complex system interactions virtualized platform technology heterogeneity cloud operational behavior large cloud infrastructure cloud datacenters perception based anomaly detection system;virtualisation cloud computing computer centres security of data system monitoring system recovery virtual machines;cloud computing	Detection of anomalies in large Cloud infrastructure is challenging. Understanding operational behavior of Cloud is extremely difficult due to the heterogeneity of different technologies, virtualized platforms and complex interactions among the systems. Many of existing system models for Cloud are based on utilization metrics such as CPU, memory, network and I/O. Such system models are quite complex and their anomaly detection mechanisms are mostly based on threshold scheme. Utilization metrics exceeding a certain threshold would trigger an alarm. In fact, it is impossible to determine proper threshold for all anomalies. These system models fail to assess the state of the system accurately. We propose a novel anomaly detection system based on user perception rather than complex system models. In our Perception-Based Anomaly Detection system (PBAD), each component within multi-tier applications monitors response time and determines whether overall service response time is adequate. PBAD also locates the anomaly by analyzing component behaviors. PBAD masks the complexity of Cloud and addresses what matters, how user perceives the service provided by the Cloud applications. The key advantages of the proposed algorithm are simplicity and scalability. We implement and deploy PBAD in our production data center environment. The experimental results show that PBAD detects numerous types of anomalies as well as the combination of anomalies where existing systems fail.	algorithm;anomaly detection;asynchronous i/o;central processing unit;cloud computing;complex system;data center;interaction;multitier architecture;response time (technology);scalability;secret sharing	Jiyeon Kim;Hyong S. Kim	2015	2015 IEEE 8th International Conference on Cloud Computing	10.1109/CLOUD.2015.95	anomaly detection;real-time computing;simulation;cloud computing;computer science;virtual machine;operating system;cloud testing;response time;computer security	Metrics	-23.81412854940868	58.05737405527202	147482
893bb3c8c78948955209b68d7a1c036cbdff9d07	grid scheduling based on collaborative random early detection strategies	grid scheduling;resource scheduling;scheduling grid computing groupware;groupware;scheduler to scheduler communication;resource discovery;grid environments;oscillators;decentralized scheduling;resource management;collaboration;collaboration resource management processor scheduling scalability large scale systems proposals collaborative work costs pathology humans;large scale;scheduler to resource communication grid scheduling collaborative random early detection strategies resource discovery resource scheduling decentralized scheduling scheduler to scheduler communication;ip network techniques;scheduling;collaborative random early detection strategies;communication cost;ip networks;scalability;peer to peer computing;grid computing;proposals;ip network techniques grid environments scheduling;random early detection;scheduler to resource communication	A fundamental problem in large scale Grids is the need for efficient and scalable techniques for resource discovery and scheduling. In traditional resource scheduling systems a single scheduler handles information about all computing resources and schedules jobs. This centralized approach has a serious scalability problem, since it introduces a bottleneck, as well as a single point of failure. Some decentralized scheduling systems have been proposed to improve scalability. However, the main contributions in this area are generally carried out under the assumption of several coordinated schedulers. Nevertheless this approach leads to high communication costs. Such costs are mainly caused by the strong dependency on negotiation among scheduler-to-scheduler and scheduler-to-resource communication. Current approaches to decentralized resource management - in particularly approaches based on Random Early Detection (RED) - are non-coordinated since these schedulers make scheduling related decisions in an independent way. This paper introduces a collaborative model of decentralized scheduling that improves resource scheduling based on RED strategies via gossiping. With this approach, schedulers can receive information from other schedulers without creating a high communication overhead and continue scheduling jobs in an independent way. The simulation results shows that our proposal is scalable and it handles large resources efficiently on large scale Grids.	algorithm;bottleneck (engineering);centralized computing;experiment;gossip protocol;grid computing;job stream;overhead (computing);random early detection;reliability engineering;scalability;scheduling (computing);simulation;single point of failure;throughput	Manuel Brugnoli;Elisa Heymann;Miquel A. Senar	2010	2010 18th Euromicro Conference on Parallel, Distributed and Network-based Processing	10.1109/PDP.2010.57	fair-share scheduling;fixed-priority pre-emptive scheduling;real-time computing;earliest deadline first scheduling;scalability;random early detection;dynamic priority scheduling;computer science;rate-monotonic scheduling;resource management;operating system;two-level scheduling;distributed computing;scheduling;round-robin scheduling;oscillation;scheduling;grid computing;computer network;collaboration	HPC	-25.461601612041505	56.618093980346835	147555
3230faaa24762d34ab372acc5dc0ec487341d575	towards elastic stream processing: patterns and infrastructure		Distributed, highly-parallel processing frameworks as Hadoop are deemed to be state-of-the-art for handling big data today. But they burden application developers with the task to manually implement program logic using lowlevel batch processing APIs. Thus, a movement can be observed that high-level languages are developed which allow to declaratively model dataflows that are automatically optimized and mapped to the batch-processing backends. However, most of these systems are based on programming models as MapReduce that provide elasticity and fault-tolerance in a natural manner since intermediate results are materialized and, therefore, processes can simply be restarted and scaled with partitioning input datasets. For continuous query processing on data streams, these concepts cannot be applied directly since it must be guaranteed that no data is lost when nodes fail. Usually, these long running queries contain operators that maintain state information which depends on the data that has already been processed and hence they cannot be restarted without information loss. This also is an issue when streaming tasks should be scaled. Therefore, integrating elasticity and fault-tolerance in this context is a challenging task which is subject of this paper. We show how common patterns from parallel and distributed algorithms can be applied to tackle these problems and how they are mapped to the Mesos cluster management system.	apache hadoop;batch processing;big data;cluster manager;database;distributed algorithm;elasticity (data store);fault tolerance;high- and low-level;mapreduce;parallel computing;stream processing	Kai-Uwe Sattler;Felix Beier	2013			real-time computing;database;batch processing;operator (computer programming);distributed algorithm;stream processing;big data;management system;data stream mining;computer science;programming paradigm	DB	-20.03156452419512	54.80883999611588	147627
f5c9ec07ac2155a2865e57328e3621acb08ebe77	back propagation grouping: load balancing at global scale when sources are skewed		Load balancing is the salient problem in data stream processing systems and also in complex event processing systems. And the imbalance in operational workers becomes extremely apparent as the scale is up with more workers and skewed datasets. In this paper, we find out that the upstream skewed sources can also exacerbate the load imbalance in the downstream workers and this bottleneck cannot be handled well by existing schemes. Thus, we propose a novel stream partitioning solution called BACK PROPAGATION GROUPING (BPG), and its core components are key splitting, back propagation and calibration signal. We verify BPG in theory and test it on both real-world and synthetic data streams. The results show that the imbalance is 10–100x less with BPG than with the previous state-of-the-art. And this metric translates into an improvement of up to 34% in throughput when deployed on Apache Storm cluster. In conclusion, BPG mitigates the load imbalance dramatically on highly skewed datasets and especially when the sources are also skewed.	apache storm;approximation algorithm;backpropagation;better portable graphics;complex event processing;downstream (software development);hash function;load balancing (computing);power of two;secret sharing;software propagation;storm botnet;stream processing;synthetic data;throughput	Xin Zhang;Haopeng Chen;Fei Hu	2017	2017 IEEE International Conference on Services Computing (SCC)	10.1109/SCC.2017.61	data stream;streams;throughput;real-time computing;complex event processing;computer science;load management;bottleneck;synthetic data;load balancing (computing)	DB	-23.059777312596857	58.297281243118896	147887
7f29fa0b00932a319284a256142a24322badd31e	a bulk-synchronous parallel process algebra	data parallel;security properties;parallel algorithm;formal model;calculus of communicating systems;distributed programs;parallel programming;programming model;bulk synchronous parallel;performance model;parallel computer;scheduling problem;path algebras;bsp model;internet security;parallel programs;process algebra;grid system;geographic distribution;process algebras	The calculus of communicating systems (CCS) process algebra is a well-known formal model of synchronization and communication. It is used for the analysis of safety and liveness in protocols or distributed programs. In more recent work, it is used for the analysis of security properties. Bulk-synchronous parallelism (BSP) is an algorithm and programming model of data-parallel computation. It is useful for the design, analysis and programming of scalable parallel algorithms. Many current evolutions require the integration of distributed and parallel programming: grid systems for sharing resources across the Internet, secure and reliable global access to parallel computer systems, geographic distribution of confidential data on randomly accessible systems, etc. Such software services must provide guarantees of safety, liveness, security together with scalable and reliable performance. Formal models are therefore needed to combine parallel performance and concurrent behavior. With this goal in mind, we propose here an integration of BSP with CCS semantics, generalize its cost (performance) model and sketch its application to scheduling problems in meta-computing.	bulk synchronous parallel;process calculus	Armelle Merlin;Gaétan Hains	2007	Computer Languages, Systems & Structures	10.1016/j.cl.2006.11.001	process calculus;parallel computing;computer science;theoretical computer science;internet security;distributed computing;parallel algorithm;programming paradigm;programming language;calculus of communicating systems;bulk synchronous parallel;liveness	Logic	-28.571547381463198	48.10086117072116	148131
0453151b66f15f9f708c9a36ce9e730de105f2c8	distributed accounting on the grid	internets;computers;utilities;distributed processing;low cost;data storage;resources management;computation	"""The Internet has been engineered over the last thirty years to interconnect devices across the globe in an adaptable and fault-tolerant manner. Along with the development of the Intemet, a suite of distributed applications ranging from electronic mail to the World Wide Web that rely upon the global Internet have grown in use and scope in parallel with the universal deployment and use of the lnternet. By the late 1990's, the lntemet was adequately equipped to move vast amounts of data between HPC systems, and efforts were initiated to link together the national infrastructure of high performance computational and data storage resources together into a general computational utility """"grid"""", analogous to the national electrical power grid infrastructure. The purpose of the computational grid is to provide dependable, consistent, pervasive, and inexpensive access to computational resources For the computing community in the form of a computing utility [1]. This paper presents a fully distributed view of Grid usage accounting and a methodology for allocating Grid computational resources for use on a Grid computing system. The Grid will contain a large number of unconnected sites, and these sites will need to exchange accounting and bid/quote information. Specifically, three issues are being addressed by the Global Grid Forum Accounting Working Group: mapping resource usage to GRID users; defining a usage economy or methods for resource exchange, and describing implementation standards that minimize and compartmentalize the tasks required for a site to participate in GRID accounting. For an accounting system to be functional in a GRID environment, it needs to be decentralized, scalable and flexible. It must have a minimum impact on local accounting and should not make any limiting assumptions about whether accounting is done by user, group, project, or site. The requirements on the remote site will be to track the resources used by the requesting job and then pass this information back to the requesting site in some standardized format. At the requesting site, the information can then be accrued as needed for local requirements. A distributed allocation and accounting approach, using a consumer/supplier or client/server structure will work across multiple sites and satisfy the needs of the participating administrative and policy domains. Mapping Usage to Users"""	client–server model;computational resource;computer data storage;distributed computing;email;fault tolerance;grid computing;internet;requirement;scalability;server (computing);software deployment;utility computing;world wide web	William Thigpen;Thomas J. Hacker;Brian D. Athey;Laura F. McGinnis	2002			computer science;theoretical computer science;database;distributed computing	HPC	-31.49786106159601	50.63551575908461	148201
012e3f5b47c282ffb6cab176d23cb0b2173b7103	design and implementation tradeoffs for wide-area resource discovery	planetlab;topology;workstation clusters wide area networks client server systems resource allocation;resource discovery;design engineering;scalable resource discovery service;resource allocation;interconnected groups;distributed computing;emulation;client server systems;peer to peer computing distributed computing computer science topology extraterrestrial measurements computer networks grid computing design engineering computer architecture emulation;intragroup characteristics;intergroup characteristics;decentralized dht based resource discovery;computer networks;wide area resource discovery;wide area distributed system;computer architecture;sword;design and implementation;geographically distributed resources;network characteristics;self healing property;self healing property wide area resource discovery sword scalable resource discovery service wide area distributed systems topology interconnected groups intragroup characteristics intergroup characteristics per node characteristics geographically distributed resources network characteristics modelnet evaluation planetlab 4 node server cluster sites network peering decentralized dht based resource discovery;computer science;workstation clusters;peer to peer computing;modelnet evaluation;extraterrestrial measurements;wide area distributed systems;grid computing;network peering;4 node server cluster sites;per node characteristics;wide area networks	This paper describes the design and implementation of SWORD, a scalable resource discovery service for wide-area distributed systems. In contrast to previous systems, SWORD allows users to describe desired resources as a topology of interconnected groups with required intragroup, intergroup, and per-node characteristics, along with the utility that the application derives from various ranges of values of those characteristics. This design gives users the flexibility to find geographically distributed resources for applications that are sensitive to both node and network characteristics, and allows the system to rank acceptable configurations based on their quality for that application. We explore a variety of architectures to deliver SWORD's functionality in a scalable and highly-available manner. A 1000-node ModelNet evaluation using a workload of measurements collected from PlanetLab shows that an architecture based on 4-node server cluster sites at network peering facilities outperforms a decentralized DHT-based resource discovery infrastructure for all but the smallest number of sites. While such a centralized architecture shows significant promise, we find that our decentralized implementation, both in emulation and running continuously on over 200 PlanetLab nodes, performs well while benefiting from the DHT's self-healing properties.		David L. Oppenheimer;Jeannie R. Albrecht;David A. Patterson;Amin Vahdat	2005		10.1109/HPDC.2005.1520946	emulation;parallel computing;resource allocation;computer science;operating system;distributed computing;world wide web;grid computing;computer network	HPC	-26.215249985112468	56.657094975768324	148265
fd117bb541e43c3be799c8c97124d39b00cc0920	component-based grid programming using the hoc-service architecture	service architecture	Grids are a novel class of distributed systems that allow to seamlessly combine multiple resources of heterogeneous servers on the Internet for demanding applications. We deal with the challenging task of programming grid applications which have to meet the requirements of abstraction and interoperability. We present our concept of customizable Higher-Order Components (HOCs) whose parameters may be not only data but also mobile code units. We demonstrate how applications are built by customizing HOCs using code parameters and describe the HOC-SA --a service-oriented architecture for developing grid applications by composing HOCs. We show where HOC-SA ranges in the variety of currently available software component architectures. In presenting our implementation, we explain how HOC-SA exploits modern grid middleware such as the WSRF implementation in the latest Globus Toolkit while hiding its complexity from the user.		Jan Dünnweber;Sergei Gorlatch	2005			real-time computing;computer science;operating system;service-oriented architecture;distributed computing	HPC	-31.421351535130754	52.208811294640626	148380
d0bb10c84bdda500066dc9840b6bcb84b7172997	firm real-time optimistic concurrency control algorithms for replicated database systems	database system;protocols;replicated database systems;data accesses objective transactions firm real time optimistic concurrency control algorithms replicated database systems;processor scheduling;real time;delay effects;data engineering;process design;firm real time optimistic concurrency control algorithms;scheduling algorithm;transaction databases;database systems;concurrency control;data access;optimistic concurrency control;data accesses objective transactions;real time systems replicated databases concurrency control;real time systems concurrency control database systems protocols transaction databases delay effects data engineering scheduling algorithm processor scheduling process design;replicated databases;real time systems	In this paper, we focus on $rm real-time optimistic concurrency control algorithms that schedule the data accesses of active transactions in order to meet their deadlines, especially in replicated databases. The proposed optimistic concurrency control algorithms incorporate either non-blocking or blocking mechanisms in the validation stage. Our experiments demonstrate that the blocking-based algorithm is superior to non-blockingbased algorithm.	blocking (computing);concurrency (computer science);database;experiment;non-blocking algorithm;optimistic concurrency control;real-time computing;real-time transcription;replication (computing)	Jinhwan Kim	1999		10.1109/RTCSA.1999.811291	data access;process design;timestamp-based concurrency control;communications protocol;optimistic concurrency control;real-time computing;isolation;information engineering;computer science;concurrency control;database;distributed computing;multiversion concurrency control;non-lock concurrency control;serializability;scheduling;distributed concurrency control	DB	-23.627712477360642	48.01039966704388	148883
cb0c4f5c155be2e0aa35912bc94b2237e6803f72	side: isolated and efficient execution of unmodified device drivers	device driver isolation fault tolerance;kernel;performance evaluation;security of data device drivers fault tolerant computing local area networks operating system kernels performance evaluation;virtual machine monitors;kernel performance evaluation switches hardware context virtual machine monitors linux;fault tolerant computing;device drivers;fault tolerance;linux;device driver isolation;side performance optimization protection domain throughput penalty latency gigabit ethernet nic virtual memory hardware kernel code unmodified device drivers streamlined isolated driver execution isolated device driver execution system performance overhead host operating system buggy device drivers;operating system kernels;switches;security of data;context;local area networks;hardware	Buggy device drivers are a major threat to the reliability of their host operating system. There have been myriad attempts to protect the kernel, but most of them either required driver modifications or incur substantial performance overhead. This paper describes an isolated device driver execution system called SIDE (Streamlined Isolated Driver Execution), which focuses specifically on unmodified device drivers and strives to avoid changing the existing kernel code as much as possible. SIDE exploits virtual memory hardware to set up a device driver execution environment that is compatible with existing device drivers and yet is fully isolated from the kernel. SIDE is able to run an unmodified device driver for a Gigabit Ethernet NIC and the latency and throughput penalty is kept under 1% when augmented with a set of performance optimizations designed to reduce the number of protection domain crossings between an isolated device driver and the kernel.	address space;authorization;central processing unit;device driver;downtime;gigabit;hypervisor;interrupt;kernel (operating system);modern operating systems;network interface controller;networking hardware;operating system;overhead (computing);page table;privilege (computing);prototype;requirement;throughput;wrapper function	Yifeng Sun;Tzi-cker Chiueh	2013	2013 43rd Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)	10.1109/DSN.2013.6575348	sysfs;local area network;embedded system;fault tolerance;kernel;real-time computing;network switch;computer science;operating system;kernel preemption;computer security;linux kernel	Arch	-24.517400762088165	51.05374851739416	148941
4b297ac385df2b68c965f0b6ae277a107539fd49	an accounting model for dynamic virtual organizations	resource virtualization;service composition;information systems;environmental economics costs grid computing resource virtualization performance analysis consumer electronics information systems laboratories informatics computer networks;virtual enterprises;service consumptions;consumer electronics;grid accounting dynamic virtual organizations electronic service provisioning service compositions service consumptions;computer networks;electronic services;service model;multi domain;virtual organization;grid accounting;environmental economics;performance analysis;electronic service provisioning;dynamic virtual organizations;informatics;grid computing;service provision;service compositions;virtual enterprises grid computing	The provisioning of remote and composed services in support of various application areas has dramatically increased over recent times. Thus, the concept of Grids has evolved, in the sense of a common platform for electronic service provisioning in multi-domain environments. While, traditionally, Grids have seen a quite static existence, many new service compositions have to take place on-demand and for certain periods of time only. To tackle those issues the concept of Virtual Organizations (VO) delivers a highly suitable representation of such dynamic Grids. However, one important open problem at this stage is the lack of applicable, distributed, and efficient accounting schemes for commercial resource and service consumptions. Even for simple management purposes, e.g., sampling or archiving, this functionality is essential. Therefore, a comprehensive model for Grid accounting has been developed and suitable accountable units have been defined, in which an underlying activity- and resource-based accounting model covers economic cost theory. Furthermore, this work is based on a service model proposed for service provisioning in dynamic VOs, overcoming the typically static nature of traditional Grids.	archive;common platform;d-grid;e-science;provisioning;requirement;sampling (signal processing);service virtualization;virtual organization (grid computing)	Matthias Göhner;Martin Waldburger;Fabian Gubler;Gabi Dreo Rodosek;Burkhard Stiller	2007	Seventh IEEE International Symposium on Cluster Computing and the Grid (CCGrid '07)	10.1109/CCGRID.2007.18	computer science;knowledge management;operating system;service-oriented modeling;database;distributed computing;informatics;world wide web;information system;grid computing	HPC	-32.42816931264307	54.252554868367625	148959
63f08186cb7f8994c4b7e1736fa6fa1ef3e23b96	multimethod communication for high-performance metacomputing applications	data transmission;virtual library;instruments;climates;application software;high speed networks;distributed computing;informing science;numerical computing;computer networks;transport protocols;computer architecture;software architecture;metacomputing application software computer architecture high speed networks supercomputers instruments displays distributed computing transport protocols access protocols;client server;environmental sciences;54 environmental sciences;computer calculations;displays;fault tolerance;networking;mass storage system;load balancing;access protocols;design;99 mathematics computers information science management law miscellaneous;metacomputing;task scheduling;high performance;selection criteria;supercomputers;parallel applications;parallel processing;geographic distribution;heterogeneity;heterogeneous network;mathematics computers information science management law miscellaneous	Metacomputing systems use high-speed networks to connect supercomputers, mass storage systems, scientific instruments, and display devices with the objective of enabling parallel applications to utilize geographically distributed computing resources. However, experience shows that high performance can often be achieved only if applications can integrate diverse communication substrates, transport mechanisms, and protocols, chosen according to where communication is directed, what is communicated, or when communication is performed. In this paper, we describe a software architecture that addresses this requirement. This architecture allows multiple communication methods to be supported transparently in a single application, with either automatic or user-specified selection criteria guiding the methods used for each communication. We describe an implementation of this architecture, based on the Nexus communication library, and use this implementation to evaluate performance issues. This implementation was used to support a wide variety of applications in the I-WAY metacomputing experiment at Supercomputing~95; we use one of these applications to provide a quantitative demonstration of the advantages of multimethod communication in a heterogeneous networked environment.	communications protocol;distributed computing;mass storage;metacomputing;multiple dispatch;software architecture;supercomputer	Ian T. Foster;Jonathan Geisler;Carl Kesselman;Steven Tuecke	1996	Proceedings of the 1996 ACM/IEEE Conference on Supercomputing	10.1145/369028.369113	parallel processing;software architecture;design;fault tolerance;application software;parallel computing;heterogeneous network;computer science;load balancing;theoretical computer science;heterogeneity;operating system;distributed computing;computer network;data transmission	HPC	-30.839958035491087	49.49708036738907	149249
0555eaeba6bd085ee1d127e478fa1eb76f0ff3ca	exploring inter-cloud load balancing by utilizing historical service submission records	cloud scheduling;inter cloud;historical records on job delegations;cloud load balancing;cloud computing	"""Cloud computing offers significant advantages to Internet users by deploying hosted services via bespoke service-provisioning environments. In advance, the emergence of Inter-Cloud increases the competences and opportunities of clients for a wider resource provision selection. This extends current capabilities by decoupling users from cloud providers while at the same time cloud providers offer an augmented service delivery mean. In practice, cloud users make use of their brokering component for selecting the best available resource, in terms of computational power and software licensing of a datacenter based on service level agreements for service execution. However, from the cloud perspective, the overall choice for balancing the different workloads within the Inter-Cloud is a complex decision. This article explores the performance of an Inter-Cloud to measure the utilization levels among their sub-clouds for various job submissions. With this in mind, the solution is modeled for achieving load balancing based on historical records from past service execution experiences. The record files are composed in the form of log files that keep related information about the size of the Inter-Cloud, basic specifications, and job submission parameters. Finally, the solution is integrated in a simulated setting for exploring the performance of the approach for various heavy workload submissions. DOI: 10.4018/jdst.2012070106 International Journal of Distributed Systems and Technologies, 3(3), 72-81, July-September 2012 73 Copyright © 2012, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited. chestrate the cloud service distribution, rather than aim to the management and deployment of the underlying infrastructure. In such environments, massive computing capacity resides at a remote space and could be delivered in the form of software and/or hardware (Carolan & Gaede, 2011). These offered services are identical to job submissions that have been encapsulated in application execution requests that have been posed by the end-users. Although cloud computing is still in its infancy due to the facility-orientation (physical space), it needs to be evolved to a more distributed infrastructure with a broader propagation of services. This could be achieved by utilizing available resources (clusters, high performance computing and grids) relying at the lower level (infrastructure). By transforming the cloud infrastructure to go beyond its premises we could facilitate a wider set of deployed services and applications. The study aims to the InterCloud load balancing which represents an interconnected global cloud of clouds (Sotiriadis et al., 2012a). The generic idea as presented by Bessis et al. (in press-b) and Buyya et al. (2010) is to decouple resource consumers from providers and allowing providers to offer resources on demand and on an ad hoc basis. For achieving this model, a new structure should be established to contain the required conceptions of resource decoupling. These are protocols to control trust standards, discovering, systems for naming, scheduling of services, portability and workload exchange. We focus majorly on the service management concept, and eventually the load balancing mechanism during the service submissions. The target is to effectively achieve an enhanced quality of service by methodologically assign services in the form of job tasks (processes) to resources. These services (jobs) encapsulate various capabilities of the cloud environment (e.g., provisioning of software and/or hardware). Thus, the challenge is to identify the rationality behind the decisions of the cloud provider to manage the Inter-Cloud service execution for efficient load balancing. Specifically, the users submit their requests in a broker with the latter communicating and monitoring the whole service exchanging procedure. This component is responsible for autonomous decisions by selecting a datacenter for forwarding the request. Then, each request is sandboxed in a virtual machine (VM) that satisfies these requirements. Various criteria are implemented in this level that includes the user-defined quality of service levels e.g., pricing, homogeneity in terms of hardware and software, and generic specification of the cloud VM. These are enclosed in service level agreements (SLAs) that formally define the level of agreed terms between provider and client. Usually, this is related with the required computational power (performance) and time constraints. To this extend, we have presented a metabrokering solution in Sotiriadis et al. (2012b) as a novel component that is placed on the top of each broker. The aim was to achieve the decentralization of the setting in which meta-brokers collaborate with each other for SLAs trading. By using cloud meta-brokers an InterCloud is formed into an autonomously management setting of interconnected subclouds. Current efforts in this direction organize (meta-) centralized topologies of brokers, so various drawbacks derived from this narrow view. Herein, the work is inspired from the meta-computing concept and based upon the model of a decentralized meta-broker. Specifically, we measure the utilization level of the Inter-Cloud and we save results in log files named as historical records. Then, we explore the performance of the Inter-Cloud for various sub-cloud numbers and job variations. With this in mind, the following section presents the motivation of our work, which is related, with the area of large-scale systems. The rest of the paper is organized as follows; we present the InterCloud load balancing solution that is followed by the presentation of the experimental analysis and results of the InterCloud utilization levels and the load-balancing mechanism. At last we conclude our work by 8 more pages are available in the full version of this document, which may be purchased using the """"Add to Cart"""" button on the product's webpage: www.igi-global.com/article/exploring-inter-cloud-loadbalancing/67559?camid=4v1 This title is available in InfoSci-Journals, InfoSci-Journal Disciplines Computer Science, Security, and Information Technology, InfoSci-Select, InfoSci-Computer Systems and Software Engineering eJournal Collection, InfoSci-Journal Disciplines Engineering, Natural, and Physical Science, InfoSci-Select. Recommend this product to your librarian: www.igi-global.com/e-resources/libraryrecommendation/?id=2"""	autonomous robot;bespoke;centralized computing;cloud computing;cloud load balancing;computation;computer cluster;computer performance;coupling (computer programming);data center;data logger;distributed computing;emergence;emoticon;encapsulation (networking);hoc (programming language);itil;intercloud;job stream;load balancing (computing);mind;provisioning;quality of service;rationality;requirement;sandbox (computer security);scheduling (computing);service-level agreement;simulation;software deployment;software license;software portability;software propagation;supercomputer;virtual machine	Stelios Sotiriadis;Nik Bessis;Nick Antonopoulos	2012	IJDST	10.4018/jdst.2012070106	real-time computing;simulation;cloud computing;computer science;operating system;cloud testing;data mining;database;distributed computing;world wide web;computer security;computer network	HPC	-30.986081545198537	52.566038762291356	149294
dd43ee858d7bf7be2d0749b41be709ffd44636ce	distributed optimistic concurrency control methods for high-performance transaction processing	phase locking;simulation ordinateur;optimisation;base donnee repartie;distributed database;optimization methods concurrency control throughput computational modeling computer simulation concurrent computing delay protocols system recovery distributed computing;optimizacion;implementation;buffer management;base repartida dato;distributed database systems;commit protocols;access protocol;data partitioning;algorithme;algorithm;ejecucion;object oriented;distributed databases transaction processing concurrency control;analyse performance;deadlocks transaction concurrency two phase locking concurrency control reliable commit protocol internode communication optimistic cc;concurrency control;performance analysis;distributed databases;optimistic concurrency control;deadlock;interbloqueo;oriente objet;simulation study;access invariance;systeme transactionnel;optimization;difference set;controle concurrence;simulacion computadora;control concurrencia;systeme gestion base donnee;interblocage;protocole acces;transaction processing;system performance modeling;sistema gestion base datos;orientado objeto;high performance;database management system;computer simulation;acceso protocolo;transaction system;algoritmo;analisis eficacia	There is an ever-increasing demand for more complex transactions and higher throughputs in transaction processing systems leading to higher degrees of transaction concurrency and, hence, higher data contention. The conventional two-phase locking (2PL) Concurrency Control (CC) method may, therefore, restrict system throughput to levels inconsistent with the available processing capacity. This is especially a concern in shared-nothing or data-partitioned systems due to the extra latencies for internode communication and a reliable commit protocol. The optimistic CC (OCC) is a possible solution, but currently proposed methods have the disadvantage of repeated transaction restarts. We present a distributed OCC method followed by locking, such that locking is an integral part of distributed validation and two-phase commit. This method ensures at most one re-execution, if the validation for the optimistic phase fails. Deadlocks, which are possible with 2PL, are prevented by preclaiming locks for the second execution phase. This is done in the same order at all nodes. We outline implementation details and compare the performance of the new OCC method with distributed 2PL through a detailed simulation that incorporates queueing effects at the devices of the computer systems, buffer management, concurrency control, and commit processing. It is shown that for higher data contention levels, the hybrid OCC method allows a much higher maximum transaction throughput than distributed 2PL in systems with high processing capacities. In addition to the comparison of CC methods, the simulation study is used to study the effect of varying the number of computer systems with a fixed total processing capacity and the effect of locality of access in each case. We also describe several interesting variants of the proposed OCC method, including methods for handling access variance, i.e., when rerunning a transaction results in accesses to a different set of objects. Index Terms —Distributed database systems, transaction processing, optimistic concurrency control, access invariance, commit protocols, system performance modeling. —————————— ✦ ——————————	baseline (configuration management);central processing unit;computer data storage;concurrency (computer science);database;deadlock;distributed computing;floor and ceiling functions;input/output;interrupt;locality of reference;lock (computer science);maximum throughput scheduling;nonlocal lagrangian;optimistic concurrency control;overhead (computing);performance prediction;read-only memory;shared nothing architecture;simulation;transaction processing system;two-phase commit protocol;two-phase locking	Alexander Thomasian	1998	IEEE Trans. Knowl. Data Eng.	10.1109/69.667102	three-phase commit protocol;optimistic concurrency control;real-time computing;two-phase commit protocol;transaction processing;distributed transaction;computer science;two-phase locking;deadlock;concurrency control;x/open xa;database;distributed computing;online transaction processing;object-oriented programming;serializability;implementation;distributed database;acid;difference set;distributed concurrency control	DB	-19.69065521911244	46.426677946027276	149309
06eccd9e0f595343bf548ace6885f87410355541	towards configurable real-time hybrid structural testing: a cyber-physical system approach	instruments;cyber physical systems approach;hybrid structural testing cyber physical systems real time middleware;sensors;computer model;real time;distributed computing;actuators;testing;structural engineering computing computerised instrumentation mechanical testing middleware;cyber physical systems;physics computing;proof of concept;real time middleware configurable real time hybrid structural testing cyber physical systems approach civil structures physical structural components computational models civil infrastructures;computer architecture;structural testing;computational models;computational modeling;civil engineering;structural engineering computing;real time systems system testing physics computing computational modeling computer architecture actuators delay instruments distributed computing civil engineering;mechanical testing;hybrid structural testing;configurable real time hybrid structural testing;system testing;communication delay;middleware;computerised instrumentation;civil structures;physical structural components;simulation model;real time middleware;civil infrastructures;real time computing;hardware;real time systems	Real-time hybrid testing of civil structures represents agrand challenge in the emerging area of cyber-physical systems. Hybrid testing improves significantly on either purely numerical or purely empirical approaches by integrating physical structural components and computational models. Actuator dynamics, complex interactions among computers and physical components, and computation and communication delays all hamper the ability to conduct accurate tests. To address these challenges, this paper presents initial work towards a Cyber-physical Instrument for Real-time hybrid Structural Testing (CIRST). CIRST aims to provide two salient features: a highly configurable architecture for integrating computers and physical components; and system support for real-time operations in distributed hybrid testing. This paper presents the motivation of the CIRST architectureand preliminary test results from a proof-of-concept implementation that integrates a simple structural element and simulation model. CIRST will have broad impacts on thefields of both civil engineering and real-time computing.It will enable high-fidelity real-time hybrid testing of awide range of civil infrastructures, and will also providea high-impact cyber-physical application for the study andevaluation of real-time middleware.	computation;computational model;computer;cyber-physical system;grand challenges;hybrid testing;interaction;middleware;numerical analysis;real-time clock;real-time computing;real-time transcription;real-time web;simulation;structural element;white-box testing	Terry Tidwell;Xiuyu Gao;Huang-Ming Huang;Chenyang Lu;Shirley Dyke;Christopher D. Gill	2009	2009 IEEE International Symposium on Object/Component/Service-Oriented Real-Time Distributed Computing	10.1109/ISORC.2009.41	computer simulation;embedded system;real-time computing;simulation;computer science;operating system;software engineering;distributed computing;computational model	Embedded	-31.895184486334045	47.74177071162398	149445
7d6eea0bb48987fa937738e4859703c1aafa5698	a reconfigurable run-time system for filter-stream applications	distributed system;reconfigurable computing;dynamic reconfiguration;distributed processing data mining;distributed processing;k means;anthill;distributed computing;reconfigurable run time system;data migration;a priori algorithm;data mining;programming model;run time system;filters application software programming profession computer science data mining high performance computing runtime computer architecture biomedical informatics communication channels;filter stream applications;data mining algorithm;k means algorithm reconfigurable run time system filter stream applications distributed systems anthill filter stream programming model data migration data mining algorithms a priori algorithm;data mining algorithms;k means algorithm;filter stream model;filter stream programming model;distributed systems;high performance;distributed computing filter stream model reconfigurable computing	The development of high level abstractions for programming distributed systems is becoming a crucial effort in computer science. Several frameworks have been proposed, which expose simplified programming abstractions that are useful for a broad class of applications and can be implemented efficiently on distributed systems. One such system is Anthill, based on the filter-stream programming model, in which applications are decomposed into sets of independent filters that communicate via streams. Anthill achieves high performance by allowing filters to be transparently replicated across several compute nodes.In this paper we present a global state manager for Anthill, which exports a simple abstraction to manipulate state variables for application filters. The state is distributed transparently among the instances of that filter, and our manager is designed to allow data migration from one filter instance to another, enabling Anthill to dynamically reconfigure applications at execution time.To evaluate our system, we used two well known data mining algorithms: a priori and k-means. Our results show that the framework incurs low overhead, 1.8% on average, and that the resulting system can effectively make use of new resources as they are made available, with execution times 3.57% slower, on average, than the minimum expected time for the reconfiguration scenario.	algorithm;ant colony;average-case complexity;computer science;data mining;distributed computing;high-level programming language;k-means clustering;overhead (computing);programming model;replication (computing);run time (program lifecycle phase);stream processing	Daniel Fireman;George Teodoro;André Cardoso;Renato A. C. Ferreira	2008	2008 20th International Symposium on Computer Architecture and High Performance Computing	10.1109/SBAC-PAD.2008.28	parallel computing;real-time computing;computer science;theoretical computer science;operating system;distributed computing;programming language;k-means clustering	Arch	-20.058011419427658	54.029031063103844	149447
23bb98e00496471377e7ea286809982c87ad2954	moderated group authoring among weakly connected workgroups	collaboration;data mining;conflict resolution;wireless networks;middleware;fuses;mobile computing;availability;groupware;system performance;wireless communication;distributed computing;collaborative software	Our goal is to develop a practical groupware system that allows any group member to modify a shared file. Each user can modify any part of the document; the documents cannot be easily decomposed into sections that can be independently modified. Updates from different users will conflict and the goal is to resolve these conflicts and eventually produce a consistent document. Consider a group of users: Alice, Bob and Tom who are modifying a single document (illustrated in Figure 1). Each user creates updates on a shared document: Alice creates updates a1, a2, and a3, Bob creates b1 and b2 and Tom creates updates t1, t2 and t3. Traditionally, groupware systems ordered these updates using their causality relationships [1] in order to achieve consistency. The updates are then applied in this particular order. However, the system performance and the duration for all the updates to be applied to produce a consistent version depends on the availability patterns of the group members.	alice and bob;causality;collaborative software;connectivity (graph theory);document;tom	Surendar Chandra;Nathan Regola	2009	2009 6th Annual International Mobile and Ubiquitous Systems: Networking & Services, MobiQuitous		embedded system;human–computer interaction;computer science;operating system;conflict resolution;database;distributed computing;mobile computing;world wide web;computer security;collaborative software;computer network	DB	-26.059699446493745	47.12614351236067	149590
9298f7b953a498318ba19b57e9d7a39cc9756eec	the cms remote analysis builder (crab)	distributed data;general and miscellaneous mathematics computing and information science;design and development;implementation;performance;data management;safety reports computing;grid middleware;physics;builders;production;design;middleware;management;geographic distribution	The CMS experiment will produce several Pbytes of data every year, to be distributed over many computing centers geographically distributed in different countries. Analysis of this data will be also performed in a distributed way, using grid infrastructure. CRAB (CMS Remote Analysis Builder) is a specific tool, designed and developed by the CMS collaboration, that allows a transparent access to distributed data to end physicist. Very limited knowledge of underlying technicalities are required to the user. CRAB interacts with the local user environment, the CMS Data Management services and with the Grid middleware. It is able to use WLCG, gLite and OSG middleware. CRAB has been in production and in routine use by end-users since Spring 2004. It has been extensively used in studies to prepare the Physics Technical Design Report (PTDR) and in the analysis of reconstructed event samples generated during the Computing Software and Analysis Challenge (CSA06). This involved generating thousands of jobs per day at peak rates. In this paper we discuss the current implementation of CRAB, the experience with using it in production and the plans to improve it in the immediate future.	middleware;open science grid consortium;user interface;worldwide lhc computing grid;glite	Daniele Spiga;Stefano Lacaprara;W. Bacchi;Mattia Cinquilli;Giuseppe Codispoti;Marco Corvo;A. Dorigo;Alessandra Fanfani;Federica Fanzago;Fabio Farina;M. Merlo;Oliver Gutsche;L. Servoli;Carlos Kavka	2007		10.1007/978-3-540-77220-0_52	design;parallel computing;performance;data management;computer science;operating system;middleware;data mining;database;distributed computing;implementation;world wide web;grid computing	HPC	-29.730010807253787	51.9179251780619	149733
ca86136ee5c7ec551cf59054f8a318294323bb1d	evaluation of a multiple version cheme for concurrency control	parallelisme;synchronisation;parallelism;paralelismo;data base management system;synchronization;concurrency control;deadlock;interbloqueo;controle concurrence;sincronizacion;control concurrencia;systeme gestion base donnee;interblocage;sistema gestion base datos	This paper presents a thorough investigation of all relevant properties of a multiple version scheme for concurrency control. It offers conflict-free scheduling for reader transactions thereby generally reducing resource contention. By using a trace-driven simulation model we explored the effective parallelism achievable in multi-user database systems and the number of occurring deadlocks as a complementary measure. The workload was represented by six reai-life object reference strings of different applications running on databases which vary in size between 60 MB and 2.9 GB. To compare and valuate the outcome of our experiments we used known results for RX-, MCand optimistic s~~hron~~t~on protocols derived under the same conditions. Furthermore, version-dependent criteria and their influence on general database performance were considered. The results obtained underline the value of the multiple version concept for concurrency control; many criteria vote for its use in future database systems with highly concurrent transactions.	concurrency (computer science);concurrency control;database;deadlock;experiment;multi-user;parallel computing;resource contention;scheduling (computing);simulation	Theo Härder;Erwin Petry	1987	Inf. Syst.	10.1016/0306-4379(87)90020-2	timestamp-based concurrency control;synchronization;optimistic concurrency control;real-time computing;isolation;computer science;operating system;database;distributed computing;multiversion concurrency control;non-lock concurrency control;computer security;distributed concurrency control	DB	-20.094391205582806	46.59064014735651	149949
696fe936bac664e9aaeec2afa01670b3a8de7055	anpa - a two-phase commit protocol for distributed databases	distributed database		distributed database;two-phase commit protocol	Zhili Zhang;William Perrizo;Victor T.-S. Shi	1999			distributed computing;parallel computing;computer science;distributed database;two-phase commit protocol	DB	-27.926510111570316	47.06367153659992	150115
92089ab0c79b52523282611812e991136fc8d8c2	atlas data challenges in grid environment on cyfronet cluster	job management;fiabilidad;reliability;cluster;metodo monte carlo;amas;computer model;methode monte carlo;physique haute energie;grid;monte carlo test;fisica alta energia;high energy physics;rejilla;fiabilite;monte carlo method;utilisabilite;grille;gestion trabajos;monton;usabilidad;monte carlo simulation;usability;gestion travaux	The LHC ATLAS experiment at CERN will produce 1.6 PB of data per year. The High Energy Physics analysis techniques require that corresponding samples of at least 2 PB of Monte Carlo simulated data are also required. Currently the Monte Carlo test production is performed, in steps called Data Challenges. Such production and analysis can be performed in distributed sites. The computing model should allow for central brokering of jobs and management of huge amounts of data. The Grid environment is a possible solution. Data Challenges have to prove reliability and usability of the Grid. Main effort is to use Grid as 'yet another job submission system'. Some tentative solutions are presented and some weaknesses of existing software are pointed out. Additionally, perspectives of further development and improvements are indicated.	atlas	Tomasz Bold;Anna Kaczmarska;Tadeusz Szymocha	2003		10.1007/978-3-540-24689-3_13	computer simulation;simulation;computer science;operations research;statistics;monte carlo method	HPC	-25.355959650829778	58.009645809716815	150222
80a59d5f4a427c76a23da542a99471187c754463	polardbms: towards a cost-effective and policy-based data management in the cloud	database management systems cloud computing;availability concrete servers business protocols scalability monitoring;resource usage polardbms policy based data management cloud computing cloud providers data management pay as you go cost model user action	The proliferation of Cloud computing has attracted a large variety of applications which are completely deployed on resources of Cloud providers. As data management is an essential part of these applications, Cloud providers have to deal with many different requirements for data management, depending on the characteristics and guarantees these applications are supposed to have. The objective of a Cloud provider is to support these diverse requirements with a basic set of customizable modules and protocols that can be (dynamically) combined. With the pay-as-you-go cost model of the Cloud, literally each user action and resource usage has a price tag attached to it. Thus, for the application providers, it is essential that the needs of their applications are provided in a cost-optimized manner. In this paper, we present the work in progress PolarDBMS, a flexible and dynamically adaptable system for managing data in the Cloud. PolarDBMS derives policies from application and service objectives. Based on these policies, it will automatically deploy the most efficient and cost-optimized set of modules and protocols and monitor their compliance. If necessary, the modules and/or their customization is changed dynamically at run-time. Several modules and protocols that have already been developed are presented. Additionally, we discuss the challenges that have to be met to fully implement PolarDBMS.	analysis of algorithms;cloud computing;requirement;run time (program lifecycle phase)	Ilir Fetai;Filip-Martin Brinkmann;Heiko Schuldt	2014	2014 IEEE 30th International Conference on Data Engineering Workshops	10.1109/ICDEW.2014.6818323	cloud computing security;cloud computing;computer science;cloud testing;data mining;database;world wide web;computer security	DB	-31.062748696058286	55.76268840048676	150228
3d0bafe8f24990996a9405f325ad231e9cbbf053	the cat theorem and performance of transactional distributed systems	contention;cap;distributed transaction;throughput	We argue that transactional distributed database/storage systems need to view the impossibility theorem in terms of the contention, abort rate, and throughput, rather than via the traditional CAP theorem. Motivated by Jim Gray, we state a new impossibility theorem, which we call the CAT theorem (Contention-Abort-Throughput). We present experimental results from the performance of several transactional systems w.r.t. the CAT impossibility spectrum.	cap theorem;distributed computing;distributed database;throughput	Shegufta Bakht Ahsan;Indranil Gupta	2016		10.1145/2955193.2955205	throughput;parallel computing;distributed transaction;computer science;theoretical computer science;distributed computing	DB	-22.14359811833711	46.505393699371886	150234
34ce1990e57e9079710ba59b34330b3104cb4c4a	iominer: large-scale analytics framework for gaining knowledge from i/o logs		Modern HPC systems are collecting large amounts of I/O performance data. The massive volume and heterogeneity of this data, however, have made timely performance of in-depth integrated analysis difficult. To overcome this difficulty and to allow users to identify the root causes of poor application I/O performance, we present IOMiner, an I/O log analytics framework. IOMiner provides an easy-to-use interface for analyzing instrumentation data, a unified storage schema that hides the heterogeneity of the raw instrumentation data, and a sweep-line-based algorithm for root cause analysis of poor application I/O performance. IOMiner is implemented atop Spark to facilitate efficient, interactive, parallel analysis. We demonstrate the capabilities of IOMiner by using it to analyze logs collected on a large-scale production HPC system. Our analysis techniques not only uncover the root cause of poor I/O performance in key application case studies but also provide new insight into HPC I/O workload characterization.	holism;ibm gpfs;input/output;job scheduler;logistic model tree;parallel computing;scheduling (computing);sweep line algorithm;synergy	Teng Wang;Shane Snyder;Glenn K. Lockwood;Philip H. Carns;Nicholas J. Wright;Suren Byna	2018	2018 IEEE International Conference on Cluster Computing (CLUSTER)	10.1109/CLUSTER.2018.00062	workload;task analysis;computer science;root cause;distributed computing;data mining;input/output;root cause analysis;analytics	HPC	-23.388505956165545	56.17744878980773	150270
fe18907ebd4ee514329db1a9e0e103e3ec50adb6	multi-level per node combiner (mlpnc) to minimize mapreduce job latency on virtualized environment		"""Big data drove businesses and researches more data driven. Hadoop MapReduce is one of the cost-effective ways for processing huge amount of data and also offered as a service from cloud on cluster of Virtual Machines (VM). In Cloud Data Center (CDC), Hadoop VMs are co-located with other general purpose VMs across racks. Such a multi-tenancy leads to varying local network bandwidth availability for Hadoop VMs, which directly impacts MapReduce job latency. Because, shuffle phase in MapReduce execution sequence itself contributes 26%-70% of overall job latency due to large number of intermediate records. Therefore, Hadoop virtual cluster requires to ensure a maximum bandwidth to minimize job latency, but, it also increases the bandwidth usage cost. In this paper, we propose """"Multi-Level Per Node Combiner"""" (MLPNC) that curtails the number of intermediate records in shuffle phase resulting to reduction in overall job latency. It also minimizes bandwidth usage cost as well. We evaluate MLPNC results on wordcount job against default combiner, and Per Node Combiner (PNC). We also discuss the results based on number of shuffled records, shuffle latency, average merge latency, average reduce latency, average reduce task start time, and overall job latency. Finally, we argue in favor of MLPNC as it achieves up to 33% reduction in number of intermediate records and up to 32% reduction in average job latency than PNC."""	apache hadoop;big data;cloud computing;data center;diplexer;high availability;job stream;mapreduce;multitenancy;openvms;power dividers and directional couplers;throughput;virtual machine	Rathinaraja Jeyaraj;S. V. AnanthanarayanaV.	2018		10.1145/3167132.3167149	local area network;real-time computing;big data;cloud computing;latency (engineering);virtual machine;data center;in shuffle;bandwidth (signal processing);computer science	HPC	-21.472710158767004	59.367255182182404	150616
b667e2bff64585e7cf50f7e504c9e8eb303e3252	functional architecture of the nods fault tolerance framework	fault tolerant		fault tolerance	Phuong-Quynh Duong;Elizabeth Pérez Cortés;Christine Collet	2002			fault tolerance;computer science;distributed computing	Robotics	-29.180808719828832	46.53913491699251	150868
347f49b443ee69396a7b75fffe2165338ae2968f	cloud-aware middleware	software performance evaluation;servers cloud computing context time factors observers monitoring;cloud characteristics cloud aware middleware cloud computing cost reduction camid;service oriented computing;middleware;service oriented computing middleware cloud computing;software performance evaluation cloud computing middleware;cloud computing	The exponential growth of cloud computing popularity in the last years brings up new possibilities to reduce costs and places new challenges for application and middleware developers. As usually adopted, applications are developed atop middleware systems whose role is to hide the complexity of underlying cloud technologies and distribution mechanisms. However, traditional distribution transparencies and services provided by off-cloud middleware need to be redesigned in order to leverage cloud computing facilities and capabilities such as elasticity, infinite resource illusion, virtualization, and so on. In this context, this paper proposes a middleware, namely CaMid, which takes advantages of the inherent cloud characteristics and made available them to applications in a transparent way, i.e., applications developers are kept away from IaaS and distribution mechanism complexity. In order to evaluate the CaMid, we carry out an experimental assessment to measure its impact on the performance of distributed applications.	application lifecycle management;client–server model;cloud computing;distributed computing;elasticity (cloud computing);interoperability;load balancing (computing);middleware;overhead (computing);prototype;server (computing);service layer;time complexity;transparency (projection);wrapper library	Tercio de Morais;Diego Liberalquino;Nelson Souto Rosa	2013	2013 IEEE 27th International Conference on Advanced Information Networking and Applications (AINA)	10.1109/AINA.2013.43	cloud computing security;middleware;real-time computing;cloud computing;computer science;message oriented middleware;operating system;service-oriented architecture;middleware;cloud testing;distributed computing;utility computing;world wide web;grid computing	HPC	-29.630931457228332	55.77139443915851	150951
3757b776973039d36afe6524d10f24aa671d602e	configuring large high-performance clusters at lightspeed: a case study	linear algebra;gnu c compiler;cluster;linpack;perforation;linux cluster;hpc;linux;high performance;top500;administration	Over a decade ago, the TOP500 list was started as a way to measure supercomputers by their sustained performance on a particular linear algebra benchmark. Once reserved for the exotic machines and extremely well-funded centers and laboratories, commodity clusters now make it possible for smaller groups to deploy and use highperformance machines in their own laboratories. This paper describes a weekend activity where two existing 128node commodity clusters were fused into a single 256node cluster for the specific purpose of running the benchmark used to rank the machines in the TOP500 supercomputer list. The resulting metacluster sits on the November 2002 list at position 233. A key differentiator for this cluster is that it was assembled, in terms of its software, from the NPACI Rocks open-source cluster toolkit as downloaded from the public website. The toolkit allows non-cluster experts to deploy and run supercomputerclass machines in a matter of hours instead of weeks or months. With the exception of recompiling the University of Tennessee’s Automatically Tuned Linear Algebra Subroutines (ATLAS) library with a recommended version of the GNU C compiler, this metacluster ran a “stock” Rocks distribution. Successful first-time deployment of the fused cluster was completed in a scant 6 hours. Partitioning of the metacluster and restoration of the two 128-node clusters to their original configuration was completed in just over 40 minutes. This paper describes early (pre-weekend) benchmark activities to empirically determine reasonably good parameters for the High-Performance Linpack (HPL) code on both Ethernet and Myrinet interconnects. It fully describes the physical layout of the machine, the description-based installation methods used in Rocks to re-deploy two independent clusters as a single cluster, and gives the benchmark results that were gathered over the 40-hour period allotted for the complete experiment. In addition, we describe some of the online monitoring and measurement techniques that were employed during the experiment. Finally, we point out the issues uncovered with a commodity cluster of this size. The techniques presented in this paper truly bring supercomputers into the hands of the masses of computational scientists.	atlas;benchmark (computing);beowulf cluster;circuit restoration;computation;computer cluster;differentiator;electrical connection;gnu compiler collection;integrated circuit layout;linear algebra;lunpack;open-source software;pet rock;software deployment;subroutine;supercomputer;top500	Philip M. Papadopoulos;Caroline A. Papadopoulos;Mason J. Katz;William J. Link;Greg Bruno	2004	IJHPCA	10.1177/1094342004046056	supercomputer;parallel computing;computer hardware;computer cluster;computer science;linear algebra;operating system;top500;programming language;linux kernel;algorithm;cluster	HPC	-27.033112162011946	52.410176716844646	151372
c282cb70cd6cdf3d5e08bd39c016f54b006b6555	the arbiter: an active system component for implementing synchronizing primitives	critical section	"""An active system component, the arbiter, is proposed as a system struc-turing concept for the implementation of synchronizing primitives. The concept is illustrated by showing a new and very simple implementation of the critical section with a busy form of waiting. This implementation will be reened in such a way that it does not need the commonly stated postulate of atomicity, according to which assignments to and inspections of common store locations are indivisible, non interfering, atomic actions. Thus we refute the well-known and widely believed \paradox of the critical region"""". The use of the arbiter is further illustrated by showing an implementation of the semaphore operations. These operations are implemented \on top of"""" the proposed implementation for the critical section, i.e. using the same busy form of waiting to ensure the mutual exclusion of concurrently issued individual semaphore operations. Nevertheless, the proposed implementation explicitly allows a nonbusy form of waiting to result from one of the semaphore operations by means of an interrupt mechanism."""	arbiter (electronics);atomicity (database systems);critical section;indivisible;interrupt;linearizability;mutual exclusion;semaphore (programming);whole earth 'lectronic link	H. J. M. Goeman	1981	Fundam. Inform.		critical section;synchronizing;mathematics;arbiter;distributed computing	Embedded	-22.39050395274144	46.769765303436806	151556
fff14f28e9481459ce86fa817eb5f170da3b1188	the escrow transactional method	database system;human interaction;base donnee;long period;mise a jour;nested transaction;gollete estrangulamiento;user interface;distributed transactions;database;punto caliente;information access;transaction;simultaneite;hot spot;interfaz usuario;goulot etranglement;simultaneidad;simultaneity;concurrency control;point chaud;acces information;interface utilisateur;acceso informacion;puesta al dia;controle concurrence;base datos;control concurrencia;bottleneck;updating	A method is presented for permitting record updates by long-lived transactions without forbidding simultaneous access by other users to records modified. Earlier methods presented separately by Gawlick and Reuter are comparable but concentrate on “hot-spot” situations, where even short transactions cannot lock frequently accessed fields without causing bottlenecks. The Escrow Method offered here is designed to support nonblocking record updates by transactions that are “long lived” and thus require long periods to complete. Recoverability of intermediate results prior to commit thus becomes a design goal, so that updates as of a given time can be guaranteed against memory or media failure while still retaining the prerogative to abort. This guarantee basically completes phase one of a two-phase commit, and several advantages result: (1) As with Gawlick's and Reuter's methods, high-concurrency items in the database will not act as a bottleneck; (2) transaction commit of different updates can be performed asynchronously, allowing natural distributed transactions; indeed, distributed transactions in the presence of delayed messages or occasional line disconnection become feasible in a way that we argue will tie up minimal resources for the purpose intended; and (3) it becomes natural to allow for human interaction in the middle of a transaction without loss of concurrent access or any special difficulty for the application programmer. The Escrow Method, like Gawlick's Fast Path and Reuter's Method, requires the database system to be an “expert” about the type of transactional updates performed, most commonly updates involving incremental changes to aggregate quantities. However, the Escrow Method is extendable to other types of updates.	aggregate data;concurrency (computer science);concurrency control;database;distributed transaction;extensibility;fast path;hot spare;long-lived transaction;programmer;serializability;two-phase commit protocol	Patrick E. O'Neil	1985	ACM Trans. Database Syst.	10.1145/7239.7265	interpersonal relationship;real-time computing;simultaneity;distributed transaction;computer science;concurrency control;database;compensating transaction;user interface;computer security;hot spot;nested transaction	DB	-22.636618891185538	47.10649888045861	151653
325144297af74a403805195b363774ff1ed1b447	survivable systems based on an adaptive nmr algorithm	byzantine failures;data partitioning technique;protocols;reliability;performance evaluation;application software;web and internet services;conventional replication protocols;performance evaluation system recovery protocols replicated databases reliability;computer crime;nuclear magnetic resonance;data partitioning;computer networks;conventional nmr schemes;system recovery;critical system;multicast protocols;immune system;survivable system;nuclear magnetic resonance application software computer science multicast protocols partitioning algorithms immune system computer networks ip networks web and internet services computer crime;ip networks;byzantine failure;critical systems;computer science;adaptive nmr algorithm;conventional nmr schemes survivable system adaptive nmr algorithm critical systems byzantine failure conventional replication protocols data partitioning technique;replicated databases;partitioning algorithms	Summary form only given. Due to the extensive use of computers and networks in critical systems, survivability is no longer a luxury but an essential requirement. We consider survivability assurance for critical applications where the system may incur accidental failures as well as intentional attacks. A successful attack can penetrate and compromise a pan of the system and, consequently, make the system behave like incurring a malicious failure. We model successful attacks as Byzantine failures and use the replication scheme to uniformly cope with failures and attacks. Instead of using conventional replication protocols that generally have a high overhead, we develop a novel coordination protocol, namely, adaptive NMR (ANMR) algorithm, to provide efficient coordination among replicated sites. Also, a special data partitioning technique is used with ANMR to assure confidentiality of the system even if it is partially compromised. Due to the adaptive nature, our ANMR algorithm reduces the communication overhead incurred in conventional NMR schemes. When there is no failure, the system operates at a very good performance. When failures occur, the system adapts to the NMR scheme gracefully.	algorithm;byzantine fault tolerance;computer;confidentiality;error-tolerant design;malware;overhead (computing)	Qingkai Ma;Wei Li;I-Ling Yen;Farokh B. Bastani;Ing-Ray Chen	2004	18th International Parallel and Distributed Processing Symposium, 2004. Proceedings.	10.1109/IPDPS.2004.1302997	parallel computing;computer science;operating system;distributed computing;byzantine fault tolerance;computer security;computer network	Embedded	-24.167008041187724	49.580442380055985	151665
25b2fe3d8831cca319d0cca7f0a1d13bf216e9b9	remote high performance visualization of big data for immersive science		Remote visualization has emerged as a necessary tool in the analysis of big data. High-performance computing clusters can provide several benefits in scaling to larger data sizes, from parallel file systems to larger RAM profiles to parallel computation among many CPUs and GPUs. For scalable data visualization, remote visualization tools and infrastructure is critical where only pixels and interaction events are sent over the network instead of the data. In this paper, we present our pipeline using VirtualGL, TurboVNC, and ParaView to render over 40 million points using remote HPC clusters and project over 26 million pixels in a CAVE-style system. We benchmark the system by varying the video stream compression parameters supported by TurboVNC and establish some best practices for typical usage scenarios. This work will help research scientists and academicians in scaling their big data visualizations for remote, real-time interaction.	big data	Faiz Abidi;Nicholas F. Polys;Srijith Rajamohan;Lance Arsenault;Ayat Mohammed	2018			pixel;data visualization;immersion (virtual reality);big data;scalability;scaling;computer graphics (images);visualization;distributed computing;engineering	HPC	-28.367990412810155	52.88182997714799	151764
50ed437e09b71497433e4e76b3f12358f180b9be	simfrastructure: a flexible and adaptable middleware platform for modeling and analysis of socially coupled systems	adaptable middleware platform;libraries;flexible coordination middleware;analytical models;interdependent networks;computational modeling computer architecture data models libraries data transfer middleware analytical models;multiplexing mechanism;simfrastructure;high performance computing;data repository;infrastructure networks;simulation execution;pervasive computing;user interfaces computational complexity data acquisition decision making digital simulation health care middleware parallel processing socio economic effects ubiquitous computing;built in analytics as a service platform;front end systems;computer architecture;software architecture;organizational networks;intuitive user interfaces;computational modeling;computational complexity;social networks;socially coupled system modeling;public health epidemiology;data acquisition systems;economic networks;ubiquitous computing;middleware;integrated modeling environments;high end computing resources;distributed systems;back end systems;high end computational resource access;socially coupled systems;physical networks;data acquisition;user interfaces;socially coupled system analysis;public health epidemiology simfrastructure flexible middleware platform adaptable middleware platform socially coupled system modeling socially coupled system analysis interdependent networks social networks organizational networks economic networks infrastructure networks physical networks pervasive computing data acquisition systems high performance computing decision making integrated modeling environments flexible coordination middleware multiplexing mechanism intuitive user interfaces front end systems high end computing resources back end systems computational complexity data repository simulation execution built in analytics as a service platform high end computational resource access;parallel processing;socio economic effects;data transfer;digital simulation;data models;flexible middleware platform	Socially coupled systems are comprised of interdependent social, organizational, economic, infrastructure and physical networks. Today's urban regions serve as an excellent example of such systems. People and institutions confront the implications of the increasing scale of information becoming available due to a combination of advances in pervasive computing, data acquisition systems as well as high performance computing. Integrated modeling and decision making environments are necessary to support planning, analysis and counter factual experiments to study these complex systems. Here, we describe SIMFRASTRUCTURE - a flexible coordination middleware that supports high performance computing oriented decision and analytics environments to study socially coupled systems. Simfrastructure provides a multiplexing mechanism by which simple and intuitive user-interfaces can be plugged in as front-end systems, and high-end computing resources can be plugged in as back-end systems for execution. This makes the computational complexity of the simulations completely transparent to the users. The decoupling of user interfaces and data repository from simulation execution allows users to access simulation results asynchronously and enables them to add new datasets and simulation models dynamically. Simfrastructure enables implementation of a simple yet powerful modeling environment with built-in analytics-as-a service platform, which provides seamless access to high end computational resources, through an intuitive interface for studying socially coupled systems. We illustrate the applicability of Simfrastructure in the context of an integrated modeling environment to study public health epidemiology.	canonical account;complex systems;computation;computational complexity theory;computational resource;counterfactual conditional;coupling (computer programming);data acquisition;experiment;fits;ibm notes;integrated development environment;interdependence;middleware;multiplexing;requirement;research data archiving;scalability;seamless3d;simulation;space-based architecture;supercomputer;ubiquitous computing;user interface	Keith R. Bisset;Suruchi Deodhar;Hemanth Makkapati;Madhav V. Marathe;Paula Elaine Stretz;Christopher L. Barrett	2013	2013 13th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing	10.1109/CCGrid.2013.78	real-time computing;simulation;computer science;operating system;database;distributed computing;data acquisition;ubiquitous computing	HPC	-29.224374584413408	49.61584232763798	152106
69afb8c04a282f76be9737cde7fa2b2248f1c44a	performance assessment and tuning for exchange of clinical documents cross healthcare enterprises	electronic health records ehrs;performance testing;cross enterprise document sharing xds b;integrating the healthcare enterprise ihe;system interoperability;openxds	BackgroundTo integrate electronic health records (EHRs) from diverse document sources across healthcare providers, facilities, or medical institutions, the IHE XDS.b profile can be considered as one of the solutions. In this research, we have developed an EHR/OpenXDS system which adopted the OpenXDS, an open source software that complied with the IHE XDS.b profile, and which achieved the EHR interoperability. ObjectiveWe conducted performance testing to investigate the performance and limitations of this EHR/OpenXDS system. MethodologyThe performance testing was conducted for three use cases, EHR submission, query, and retrieval, based on the IHE XDS.b profile for EHR sharing. In addition, we also monitored the depletion of hardware resources (including the CPU usage, memory usage, and network usage) during the test cases execution to detect more details of the EHR/OpenXDS system's limitations. ResultsIn this EHR/OpenXDS system, the maximum affordable workload of the EHR submissions were 400 EHR submissions per hour, the DSA CPU usage was 20%, memory usage was 1380MB, the network usages were 0.286KB input and 7.58KB output per minute; the DPA CPU usage was 1%, memory usage was 1770MB, the network usages were 7.75KB input and 1.54KB output per minute; the DGA CPU usage was 24%, memory usage was 2130MB, the network usages were 1.3KB input and 0.174KB output per minute. The maximum affordable workload of the EHR queries were 600 EHR queries per hour, the DCA CPU usage was 66%, the memory usage was 1660MB, the network usages were 0.230KB input and 0.251KB output per minute; the DGA CPU usage was 1%, the memory usage was 1890MB, the network usages were 0.273KB input and 0.22KB output per minute. The maximum affordable workload of the EHR retrievals were 2000 EHR retrievals, the DCA CPU usage was 79%, the memory usage was 1730MB, the network usages were 19.55KB input and 1.12KB output per minute; the DPA CPU usage was 3.75%, the memory usage was 2310MB, and the network usages were 0.956KB input and 19.57KB output per minute. Discussion and conclusionFrom the research results, we suggest that future implementers who deployed the EHR/OpenXDS system should consider the following aspects. First, to ensure how many service volumes would be provided in the environment and then to adjust the hardware resources. Second, the IHE XDS.b profile is adopted by the SOAP (Simple Object Access Protocol) web service, it might then move onto the Restful (representational state transfer) web service which is more efficient than the SOAP web service. Third, the concurrency process ability should be added in the OpenXDS source code to improve the hardware usage more efficiently while processing the ITI-42, ITI-18, and ITI-43 transactions. Four, this research suggests that the work should continue on adjusting the memory usage for the modules of the OpenXDS thereby using the memory resource more efficiently, e.g., the memory configuration of the JVM (Java Virtual Machine), Apache Tomcat, and Apache Axis2. Fifth, to consider if the hardware monitoring would be required in the implementing environment. These research results provided some test figures to refer to, and it also gave some tuning suggestions and future works to continue improving the performance of the OpenXDS. We conducted performance testing to investigate the performance and limitations of this EHR/OpenXDS system.The performance testing was conducted for three use cases, EHR submissions, queries, and retrievals, based on the IHE XDS.b profile for EHR sharing.We retrieved the transaction logs and also monitored the consuming of hardware resources during the test cases execution to detect more details of the EHR/OpenXDS system's limitations.From the testing results, we provided tuning suggestions and future works for the following implementers/studies to keep improving the EHR/OpenXDS system.	performance tuning	Cheng-Yi Yang;Chien-Tsai Liu	2016	Computer Standards & Interfaces	10.1016/j.csi.2016.03.001	software performance testing;computer science;operating system;data mining;database;world wide web;computer security;computer network	ML	-32.84799027919558	58.76651956614254	152139
a82497afb97c763408c8dc65c3feec0059f8d5e3	executing storm surge ensembles on paas cloud		Cloud computing services are becoming increasingly viable for scientific model execution. As a leased computational resource, cloud computing enables a computational modeler at a smaller university to carry out sporadic large-scale experiments, and allows others to pay for CPU cycles as needed, without incurring high maintenance costs of a large compute system. In this chapter, we discuss the issues involved in running high throughput ensemble applications on a Platform-as-a-Service cloud. We compare two frameworks deploying and running these applications, namely Sigiri and MapReduce. We motivate the need for a pipelined architecture to application deployment, and discus a couple of methodologies to balance the loads, minimize storage overhead, and reduce overall execution time.	cloud computing;platform as a service	Abhirup Chakraborty;Milinda Pathirage;Isuru Suriarachchi;Kavitha Chandrasekar;Craig Mattocks;Beth Plale	2014		10.1007/978-1-4939-1905-5_11	software deployment;real-time computing;architecture;throughput;scientific modelling;cloud computing;computational resource;distributed computing;instruction cycle;virtual machine;computer science	Crypto	-21.34101332003122	58.51741038452926	152359
6fdad8efde70a17f8a82e5ad387731df2077ad38	optimistic concurrency for clusters via speculative locking	shared memory;distributed transactions;distributed computing;design space;distributed environment;critical path;optimistic concurrency control;transactional memory;shared memory multiprocessor	"""Transactional memory and speculative locking are optimistic concurrency control mechanisms, whose goal is to enable highly concurrent execution while reducing the programming effort. The same basic idea lies in the heart of both methods: optimistically execute a critical code segment, determine whether there have been data conflicts and roll back in case validation fails. Transactional memory is widely considered to have advantages over lock-based synchronization on shared memory multiprocessors. Several recent works suggest employment of transactional memory in a distributed environment. However, being derived from traditional shared-memory design space, these schemes seem to be not """"optimistic"""" enough for this setting. Each thread must validate the current transaction before proceeding to the next. Hence, blocking remote requests whose purpose is to detect/avoid data conflicts are placed on the critical path and thus delay execution.  In this paper, we investigate whether in light of the above shortcomings speculative locking can be a suitable alternative for transactional memory in a distributed environment. We present a novel distributed speculative locking scheme and compare its properties to the existing distributed transactional memory protocols. Despite the conceptual similarity to transactional memory, the distributed implementation of speculative locking manages to overlap communication with computation. It allows a thread to speculatively acquire multiple locks simultaneously, which is analogous to executing one transaction before validating the previous."""	blocking (computing);code segment;computation;concurrency (computer science);control system;critical path method;lock (computer science);optimistic concurrency control;shared memory;speculative execution;transactional memory;two-phase locking	Michael Factor;Assaf Schuster;Konstantin Shagin;Tal Zamir	2009		10.1145/1534530.1534532	distributed shared memory;shared memory;interleaved memory;optimistic concurrency control;transactional memory;parallel computing;real-time computing;distributed memory;commitment ordering;computer science;software transactional memory;distributed computing;serializability	PL	-21.850358902897852	47.32370410117499	152900
057fa451f98e6b17108572cdadcc2f105f34a3fe	enterprise job scheduling for clustered environments	software portability distributed processing fault tolerant computing java resource allocation;software portability;performance evaluation;fault tolerant;resource allocation;distributed processing;processor scheduling delay effects scalability application software large scale systems engines pervasive computing computer networks operating systems load management;satisfiability;computer network;fault tolerant computing;operating system;design and implementation;system development;load balance;load balancing enterprise job scheduling distributed clustered environment modular job scheduling system java 2 enterprise edition fault tolerance;java 2 enterprise edition;job scheduling;java	The concept of scheduling is relevant to many computer-engineering areas, such as operating systems, computer networks, enterprise platforms and applications. Scheduling at the application level (a.k.a. job scheduling) is a common process in the enterprise domain, but very few IT solutions cover all the required features. Such features include scalability, fault-tolerance and load balancing. In this paper, we present the design and implementation of a modular job scheduling system, developed to work in a distributed clustered environment, which satisfies the aforementioned requirements. Its architecture and implementation is based on the Java 2 Enterprise Edition (J2EE) framework so that it inherently guarantees portability over different platforms through the use of open interfaces. A preliminary performance evaluation of the system provides indications on its behavior under various levels of workload	computer engineering;fault tolerance;functional requirement;java platform, enterprise edition;java version history;job scheduler;job shop scheduling;load balancing (computing);operating system;performance evaluation;requirement;scalability;scheduling (computing);software portability;wildfly	Stratos Paulakis;Vassileios Tsetsos;Stathes Hadjiefthymiades	2007	10th IEEE International Symposium on Object and Component-Oriented Real-Time Distributed Computing (ISORC'07)	10.1109/ISORC.2007.34	software portability;fair-share scheduling;fixed-priority pre-emptive scheduling;embedded system;fault tolerance;parallel computing;real-time computing;enterprise systems engineering;enterprise software;flow shop scheduling;dynamic priority scheduling;resource allocation;computer science;architecture domain;load balancing;job scheduler;operating system;two-level scheduling;distributed computing;programming language;round-robin scheduling;java;satisfiability	HPC	-32.28437715671219	47.109388238386714	153282
2901c764f14dc5d14e1a5df1f113ea9485bb00e7	adaptive execution of continuous and data-intensive workflows with machine learning		To extract value from evergrowing volumes of data and to drive decision making, organizations frequently resort to the composition of data processing workflows. The typical workflow model enforces strict temporal synchronization across processing steps without accounting the actual effect of intermediate computations on the final workflow output. However, this is not the most desirable in a multitude of scenarios. We identify a class of applications for continuous data processing where the workflow output changes slowly and without great significance in a short time window, thus squandering compute resources with current approaches.  To overcome such inefficiency, we introduce a novel workflow model, for continuous and data-intensive processing, capable of relaxing triggering semantics according to the impact that input data is assessed to have on changing the workflow output. To estimate this impact, learn the correlation between input and output variation, and guarantee correctness within a given tolerated error constant, we rely on Machine Learning. The functionality of this model is implemented in SmartFlux, a middleware framework which can be integrated with existing workflow managers. Experimental results indicate substantial savings in resource usage, while not deviating the workflow output beyond a small error constant with a high confidence level.	computation;correctness (computer science);data-intensive computing;input/output;machine learning;middleware;port triggering	Sérgio Esteves;Helena Galhardas;Luís Veiga	2018		10.1145/3274808.3274827	resource efficiency;correctness;semantics;synchronization;input/output;workflow;machine learning;data processing;computer science;artificial intelligence;middleware	DB	-21.076532888136995	56.79918438717619	153353
7206b0e18ca9582e3ac0a8026d32f27be839b862	recent advances in e-science	recent advance		e-science	Daniel S. Katz;David Abramson	2013	Future Generation Comp. Syst.	10.1016/j.future.2012.05.001	computer science;computational science;distributed computing;e-science	Arch	-30.203485242564806	50.58052709515295	153805
799fc1a4ef54122b6135a0ae48f33f9dcb1c6630	framework for automated partitioning and execution of scientific workflows in the cloud	scientific workflows;partitioning;framework;cloud computing;cloudml;metis	Scientific workflows have become a standardized way for scientists to represent a set of tasks to overcome/solve a certain scientific problem. Usually these workflows consist of numerous CPU and I/O-intensive jobs that are executed using workflow management systems (WfMS), on clouds, grids, supercomputers, etc. Previously, it was shown that using k-way partitioning to distribute a workflow’s tasks between multiple machines in the cloud reduces the overall data communication and therefore lowers the cost of the bandwidth usage. A framework was built to automate this process of partitioning and execution of any workflow submitted by a scientist that is meant to be run on Pegasus WfMS, in the cloud, with ease. The framework provisions the instances in the cloud using CloudML, configures and installs all the software needed for the execution, partitions and runs the provided scientific workflow, also showing the estimated makespan and cost.	amazon elastic compute cloud (ec2);approximation algorithm;autoscaling;baseline (configuration management);central processing unit;cloud computing;computation;computer cluster;data point;enterprise software;image scaling;input/output;job stream;metis;makespan;maxima and minima;montagejs;multistage interconnection networks;open-source software;pegasus;provisioning;run time (program lifecycle phase);snapshot (computer storage);software deployment;supercomputer;time complexity;user interface	Jaagup Viil;Satish Narayana Srirama	2018	The Journal of Supercomputing	10.1007/s11227-018-2296-7	metis;distributed computing;workflow management system;computer science;job shop scheduling;software;cloud computing;workflow	HPC	-21.1707196046906	59.163719070072325	153890
2cd38baa31c826365d19569135acd7abac464e17	cuckoo: towards decentralized, socio-aware online microblogging services and data measurements	service provider;microblogging services;data collection;social relationship;heterogeneous environment;p2p;online social networking;online social network;work in progress;peer to peer;operation and maintenance	Online microblogging services, as exemplified by Twitter [9] and Yammer [12], have become immensely popular during the latest three years. Twitter, the most successful microblogging service, has attracted more than 41.7 million users as of July 2009 [25] and is still growing fast. However, current microblogging systems severely suffer from performance bottlenecks and central points of failure due to their centralized architecture. Thus, centralized microblogging systems may threaten the scalability, reliability, as well as availability of the offered services, not to mention the extremely high operational and maintenance cost.  However, it is not trivial to decentralize microblogging services in a peer-to-peer fashion. The challenges first derive from the heterogeneity of the inherent online social network (OSN) features. The non-reciprocation feature of microblogging services also increases the heterogeneity. Moreover, different from traditional approaches used in centralized server-based systems, an efficient, robust and scalable approach for data collection and dissemination in such distributed heterogeneous environments is desirable.  In this paper, we present a decentralized, socio-aware microblogging system named Cuckoo. The design takes advantages of the inherent social relationships while leverages P2P techniques towards scalable, reliable microblogging services. Besides, Cuckoo provides a flexible interface for data collection while circumventing unnecessary traffic on the server. We discuss the benefits that our system may bring for both service providers and end users. We also discuss the technical aspects to be considered and report our work in progress.	bottleneck (software);centralized computing;peer-to-peer;reliability engineering;scalability;server (computing);social network	Tianyin Xu;Yang Chen;Jin Zhao;Xiaoming Fu	2010		10.1145/1834616.1834622	engineering;internet privacy;world wide web;computer security	DB	-25.505692102895257	54.44757908601931	153993
e40ed43ddacda12cc555d382f8d0ea1480fcf1ea	the limited performance benefits of migrating active processes for load sharing	distributed system;queueing model;load sharing;local area network	<italic>Load sharing</italic> in a distributed system is the process of transparently sharing workload among the nodes in the system to achieve improved performance. In <italic>non-migratory</italic> load sharing, jobs may not be transferred once they have commenced execution. In load sharing with <italic>migration</italic>, on the other hand, jobs in execution may be interrupted, moved to other nodes, and then resumed. In this paper we examine the performance benefits offered by migratory load sharing beyond those offered by non-migratory load sharing. We show that while migratory load sharing can offer modest performance benefits under some fairly extreme conditions, there are <italic>no</italic> conditions under which migration yields <italic>major</italic> performance benefits.	distributed computing;interrupt;job stream	Derek L. Eager;Edward D. Lazowska;John Zahorjan	1988		10.1145/55595.55604	local area network;real-time computing;computer science;distributed computing	Metrics	-20.145602673780857	50.52664574114689	154027
d88d4d5c5043c175fd99dd6b604da35230121823	user-profile adaptable resource management system for workstation cluster architectures	distributed memory;management system;resource allocation;network operating systems;job management systems;incomplete information;user profile;concurrent networks;adaptive resource management;resource management workstations power system management computer architecture hardware computer science memory management distributed computing availability costs;workstations;network architecture;network operating systems resource allocation local area networks workstations;workstation clusters;resource management system;local area networks;concurrent network architecture user profile adaptable resource management workstation cluster distributed memory computing remote execution commands incomplete information job management systems interactive users remote workstations resource allocation;workstation cluster	The increasing use of workstation clusters as a substitute for distributed memory computing systems has gained widespread popularity due to the existence of the inexpensive and powerful workstations. Besides using remote execution commands in a workstation cluster users have incomplete information about the state of the whole system. Therefore, a large number of workstations frequently remain idle or lightly loaded where others are heavily loaded. Existing job management systems assign jobs to idle or most lightly loaded workstations according to the state of the underlying resources. Therefore the impact by interactive users on remote workstations is relatively high. We introduce a new concept of a resource management system which allocates the best suited workstation(s) to the user according to his/her former activities (User-Profile). By using this concept, there is no impact by interactive users because workstations have no owners but the workstation pool is public among all users. The possibilities of applying this concept on a traditional workstation cluster architecture and a new workstation cluster with a concurrent network architecture (CNA) is introduced too.	management system;workstation	Mahmoud Mofaddel;Djamshid Tavangarian	1998		10.1109/EURMIC.1998.708130	real-time computing;computer science;operating system;distributed computing	DB	-23.801571400611195	54.054321555220255	154342
2d0d2a65bc78ff20b35f7333aee9fefceadbd543	early observations on the performance of windows azure		A significant open issue in cloud computing is performance. Few, if any, cloud providers or technologies offer quantitative performance guarantees. Regardless of the potential advantages of the cloud in comparison to enterprise-deployed applications, cloud infrastructures may ultimately fail if deployed applications cannot predictably meet behavioral requirements. In this paper, we present the results of comprehensive performance experiments we conducted on Windows Azure from October 2009 to February 2010. In general, we have observed good performance of the Windows Azure mechanisms, although the average 10 minute VM startup time and the worst-case 2x slowdown for SQL Azure in certain situations -relative to commodity hardware within the enterprise- must be accounted for in application design. In addition to a detailed performance evaluation of Windows Azure, we provide recommendations for potential users of Windows Azure based on these early observations. Although the discussion and analysis is tailored to scientific applications, the results are broadly applicable to the range of existing and future applications running in Windows Azure.	microsoft azure	Zach Hill;Jie Li;Ming Mao;Arkaitz Ruiz-Alvarez;Marty Humphrey	2011	Scientific Programming	10.3233/SPR-2011-0323	real-time computing;cloud computing;performance;computer science;operating system;world wide web	HPC	-23.858988981771876	58.54197923375717	154391
d55dc254ee677dcd75d25151efc10fffd4403dd3	autonomic success in database management systems	sql server;autonomic success;human interaction;ibm db2;autonomic database management systems;memory management;database management systems;sql;conference management;software fault tolerance;database systems humans statistical distributions computerized monitoring condition monitoring statistics conference management costs power system management power system reliability;computerized monitoring;indexes;statistical distributions;servers;condition monitoring;complex system;power system management;database systems;statistics;microsoft sql server database management systems autonomic success system complexity computer devices proliferation autonomic computing autonomic database management systems ibm db2 oracle;humans;power system reliability;computer devices proliferation;db2;security;database management system;autonomic dbms;autonomic computing;autonomic dbms autonomic computing db2 oracle sql server;parallel processing;system complexity;oracle;microsoft sql server;sql database management systems software fault tolerance	One of the primary uses of computer is to reduce cost and manage complexity with increase in efficiency and performance. Now system complexity is reaching a level that is beyond human ability. With the development of technology, people want to manage complex systems in an efficient and reliable manner. Development of raw computing power and proliferation of computer devices and usage of internet has grown up to exponential rates. This growth and unprecedented levels of complexity is leading towards new direction - Autonomic Computing. Autonomic features in system increase speed, efficiency, reliability and accuracy with less or no human interaction, ultimately providing error free environment. These autonomic capabilities are important in Database Management Systems (DBMSs). The DBMSs which have the capability to manage and maintain themselves are called Autonomic Database Management Systems (ADBMS). The ADBMSs are evolving from last many years. At present most of the activities in DBMS are performed autonomically and have achieved certain level of autonomicity. The paper identified some autonomic shortcomings in commercial DBMSs up to 2002. We made a survey on achievements of autonomic computing against these shortcomings in current DBMSs. For this purpose, we have studied and analyzed IBM DB2, Oracle and Microsoft SQL Server.	algorithm;autonomic computing;capability maturity model;complex systems;computer;experiment;internet;management system;microsoft sql server;oracle database;throughput;time complexity	Basit Raza;Abdul Mateen;Tauqeer Hussain;Mian M. Awais	2009	2009 Eighth IEEE/ACIS International Conference on Computer and Information Science	10.1109/ICIS.2009.202	real-time computing;computer science;operating system;database;autonomic computing	DB	-27.652364862065497	54.23357947799229	154542
6d4007d127c61a3a9f996ff69a4db353c6431721	a distributed stream query optimization framework through integrated planning and deployment	distributed application;distributed data;distributed system;network partition;search space reduction;systems;bottom up;distributed database;management system;query planning;operator level reuse;computer communication networks;distributed data stream system;query processing;search space;top down;distributed processing;simulation;integrated planning;network placement;distributed computing;hierarchical networks;query optimization;runtime;database management;query processing computer networks partitioning algorithms distributed databases network servers distributed processing distributed computing runtime computer network management algorithm design and analysis;distributed applications;computer networks;adaptive distributed stream management system;network servers;networked computing system;query processing distributed databases distributed processing;computer network management;distributed databases;systems and applications;distributed systems;formal analysis;distributed stream query optimization framework;iflow system;query processing distributed stream query optimization framework integrated planning distributed data stream system static query optimization approach networked computing system network placement search space reduction operator level reuse query planning network partition hybrid algorithm formal analysis iflow system adaptive distributed stream management system emulab simulation distributed databases database management system;database management system;query processing computer communication networks distributed systems distributed databases distributed applications database management systems;algorithm design and analysis;network computing;hybrid algorithm;partitioning algorithms;static query optimization approach;emulab	"""This paper addresses the problem of optimizing multiple distributed stream queries that are executing simultaneously in distributed data stream systems. We argue that the static query optimization approach of """"plan, then deployment"""" is inadequate for handling distributed queries involving multiple streams and node dynamics faced in distributed data stream systems and applications. Thus, the selection of an optimal execution plan in such dynamic and networked computing systems must consider operator ordering, reuse, network placement, and search space reduction. We propose to use hierarchical network partitions to exploit various opportunities for operator-level reuse while utilizing network characteristics to maintain a manageable search space during query planning and deployment. We develop top-down, bottom-up, and hybrid algorithms for exploiting operator-level reuse through hierarchical network partitions. Formal analysis is presented to establish the bounds on the search space and suboptimality of our algorithms. We have implemented our algorithms in the IFLOW system, an adaptive distributed stream management system. Through simulations and experiments using a prototype deployed on Emulab, we demonstrate the effectiveness of our framework and our algorithms."""	algorithm;bottom-up parsing;experiment;mathematical optimization;prototype;query optimization;query plan;simulation;software deployment;top-down and bottom-up design;tree network	Sangeetha Seshadri;Vibhore Kumar;Brian F. Cooper;Ling Liu	2009	IEEE Transactions on Parallel and Distributed Systems	10.1109/TPDS.2008.232	computer science;theoretical computer science;top-down and bottom-up design;database;distributed computing;distributed database	DB	-20.23274120453118	55.42937613918239	154575
534ebc43b4f514b988c57a760d21321b478fb838	gce06 (day 2) - gce06 - grid computing environments 2006		This workshop will focus on projects and technologies that are adopting scientific portals and gateways. These technologies are characterized by delivering well-established mechanisms for providing familiar interfaces to secure Grid resources, services, applications, tools, and collaboration services for communities of scientists. In most cases access is enabled through a web browser without the need to download or install any specialized software or worry about networks and ports. As a result, the science application user is isolated from the complex details and infrastructure needed to operate an application on the Grid. Additional information about this workshop is available at http://www.cogkit.org/GCE06	download;gateway (telecommunications);grid computing;portals	Gregor von Laszewski	2006		10.1145/1188455.1188697		HPC	-32.685242948972245	51.628166765708265	154583
8f9805dc122f0cd86d5252032fa5f3a48aae14a3	decoupling datacenter storage studies from access to large-scale applications	modeling of computer architecture;generators;electronic mail;very large scale integration;storage management;mass storage;modeling techniques;storage area networks;computer centres;storage management computer centres;computational modeling;load modeling very large scale integration throughput storage area networks computational modeling electronic mail generators;super very large computers;workload temporal locality datacenter storage suboptimal storage design large scale datacenter cost impact power impact storage design choice cloud data store storage profile state diagram based storage model storage workload ssd caching enterprise storage defragmentation workload spatial locality;load modeling;modeling techniques modeling of computer architecture super very large computers mass storage;throughput	Suboptimal storage design has significant cost and power impact in large-scale datacenters (DCs). Performance, power and cost-optimized systems require deep understanding of target workloads, and mechanisms to effectively model different storage design choices. Traditional benchmarking is invalid in cloud data-stores, representative storage profiles are hard to obtain, while replaying applications in different storage configurations is impractical both in cost and time. Despite these issues, current workload generators are not able to reproduce key aspects of real application patterns (e.g., spatial/temporal locality, I/O intensity). In this paper, we propose a modeling and generation framework for large-scale storage applications. As part of this framework we use a state diagram-based storage model, extend it to a hierarchical representation, and implement a tool that consistently recreates DC application I/O loads. We present the principal features of the framework that allow accurate modeling and generation of storage workloads, and the validation process performed against ten original DC application traces. Finally, we explore two practical applications of this methodology: SSD caching and defragmentation benefits on enterprise storage. Since knowledge of the workload's spatial and temporal locality is necessary to model these use cases, our framework was instrumental in quantifying their performance benefits. The proposed methodology provides detailed understanding of the storage activity of large-scale applications, and enables a wide spectrum of storage studies, without the requirement to access application code and full application deployment.	cache (computing);coupling (computer programming);data center;input/output;locality of reference;software deployment;solid-state drive;state diagram;storage model;tracing (software)	Christina Delimitrou;Sriram Sankar;Kushagra Vaid;Christoforos E. Kozyrakis	2012	IEEE Computer Architecture Letters	10.1109/L-CA.2011.37	throughput;parallel computing;real-time computing;storage area network;converged storage;computer science;operating system;database;very-large-scale integration;object-modeling technique;computational model;mass storage;computer network	OS	-23.27835818467725	56.74338964075868	154634
0bfbbe1641744b1e48a0c1a62449bca4de3b698b	towards reliable, performant workflows for streaming-applications on cloud platforms	directed acyclic graph;scientific application;fault tolerance streaming scientific workflows clouds abstraction reliability;private eucalyptus cloud performant workflow scientific workflow data streaming energy informatics cloud data services desktop;reliability;streaming;instruments;fault tolerant;sensors;computer model;scientific workflow;data models cloud computing clouds sensors satellites instruments parallel processing;weather forecasting;abstraction;performant workflow;data model;energy use;scientific workflows;programming model;private eucalyptus cloud;large scale;workflow system;energy informatics;clouds;satellites;fault tolerance;workflow management software;power grid;resource availability;near real time;workflow management software cloud computing scientific information systems;data flow;cloud data services;high performance;empirical evaluation;data structure;peak power;parallel processing;data streaming;scientific information systems;historical data;los angeles;cloud computing;data models;desktop	Scientific workflows are commonplace in eScience applications. Yet, the lack of integrated support for data models, including streaming data, structured collections and files, is limiting the ability of workflows to support emerging applications in energy informatics that are stream oriented. This is compounded by the absence of Cloud data services that support reliable and performant streams. In this paper, we propose and present a scientific workflow framework that supports streams as first-class data, and is optimized for performant and reliable execution across desktop and Cloud platforms. The workflow framework features and its empirical evaluation on a private Eucalyptus cloud are presented.	cloud computing;coexist (image);data model;desktop computer;e-science;energy informatics;mathematical optimization;next-generation network;stream (computing)	Daniel Zinn;Quinn J. Hart;Timothy M. McPhillips;Bertram Ludäscher;Yogesh L. Simmhan;Michail Giakkoupis;Viktor K. Prasanna	2011	2011 11th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing	10.1109/CCGrid.2011.74	parallel processing;fault tolerance;data structure;computer science;operating system;data mining;database;programming language;world wide web	Arch	-26.737657045885552	53.9542747724816	154678
dd9ee748917acced7540d53f5330dcefe6141726	a component-based performance comparison of four hypervisors	virtual machine monitors virtualization benchmark testing linux servers dynamic scheduling hardware;benchmark cloud computing hypervisor;xen component based performance comparison hypervisors server resources public cloud platforms private data centers cpu architectures virtualization techniques virtual machines hardware assisted virtualization settings hyper v kvm vsphere;computer centres;computer architecture;virtual machines;virtualisation cloud computing computer architecture computer centres operating systems computers virtual machines;operating systems computers;virtualisation;cloud computing	Virtualization has become a popular way to make more efficient use of server resources within both private data centers and public cloud platforms. While recent advances in CPU architectures and new virtualization techniques have reduced the performance cost of using virtualization, overheads still exist, particularly when multiple virtual machines are competing for resources. We have performed an extensive performance comparison under hardware-assisted virtualization settings considering four popular virtualization platforms, Hyper-V, KVM, vSphere and Xen, and find that the overheads incurred by each hypervisor can vary significantly depending on the type of application and the resources assigned to it. We also find dramatic differences in the performance isolation provided by different hypervisors. However, we find no single hypervisor always outperforms the others. This suggests that effectively managing hypervisor diversity in order to match applications to the best platform is an important, yet unstudied, challenge.	central processing unit;cloud computing;component-based software engineering;data center;hardware-assisted virtualization;hyper-threading;hypervisor;information privacy;kvm switch;server (computing);virtual machine	Jinho Hwang;Sai Zeng;Frederick Wu;Timothy Wood	2013	2013 IFIP/IEEE International Symposium on Integrated Network Management (IM 2013)		embedded system;full virtualization;real-time computing;virtualization;temporal isolation among virtual machines;storage hypervisor;application virtualization;cloud computing;computer science;virtual machine;data virtualization;operating system;hardware virtualization;service virtualization;hypervisor;storage virtualization	OS	-22.254897970576465	60.041487800307735	154683
26375478f58c1ce985d7ca9a3e125f8adf073216	review - a majority consensus approach to concurrency control for multiple copy databases	concurrency control		concurrency control;database	Philip A. Bernstein	1999	ACM SIGMOD Digital Review		optimistic concurrency control;database;concurrency control;non-lock concurrency control;computer science;isolation (database systems);distributed computing;serializability;multiversion concurrency control;timestamp-based concurrency control	DB	-23.515304146694742	47.490527126694154	154747
e411b5b029cd0e03a125b2e0c2bbeda976097151	apache cookbook - solutions and examples for apache administrators: covers apache 2.0 and 1.3				Ken Coar;Rich Bowen	2003				HCI	-19.9464313037552	52.2072455701169	154855
6484c934e5ea33d5fd3f75d38c9a241215f7b926	titian2: a scalable system-level emulator with all programmability for datacenter servers in cloud computing	measurement;fpga;computer architecture;prototyping;servers;soc;scalability;field programmable gate arrays;cloud computing;hardware	In order to provide the cloud computing research community with a full-system level datacenter server emulator with programmable hardware and software, and stimulate more innovative research work, this poster and demo shows a scientific research platform, Titian2, designed and implemented at ICT of CAS. Titian2 has the ability of on-line profiling and measuring, and the scalability of connecting with each other. Researchers can deploy complete software stacks and run normal server workloads on Titian2. As a microscope for cloud computing workloads, Titian2 provides scholars with more details of data movements inside both software stacks and hardware components, compared with an x86 commercial platform.	cloud computing;cloud research;data center;online and offline;profiling (computer programming);scalability;server (computing);server emulator;x86	Ke Zhang;Ran Zhao;Hongxia Zhang;Lei Yu;Yisong Chang;Zhao Zhang;Mingyu Chen	2016	2016 IEEE/ACM 9th International Conference on Utility and Cloud Computing (UCC)	10.1145/2996890.3007891	embedded system;parallel computing;cloud computing;computer science;operating system;field-programmable gate array	HPC	-28.086027699438326	56.43845652053984	155006
b83fec5974be20c616a86de6a61e82c0b372cff8	managing redundancy in can-based networks supporting n-version programming	fault tolerant;system analysis and design;distributed computing;software fault tolerance;controller area network;dependable systems;engineering and technology;teknik och teknologier;forward error correction;consistency management;can protocol;error compensation;error detection;replica determinism;data consistency;n version programming	Software is a major source of reliability degradation in dependable systems. One of the classical remedies is to provide software fault-tolerance by using N-Version Programming (NVP). However, due to requirements on special hardware and the need for changes and additions at all levels of the system, NVP solutions are costly, and have only been used in special cases. In a previous work, a low-cost architecture for NVP execution was developed. The key features of this architecture are the use of off-the-shelf components and that the fault-tolerance functionality, including voting, error detection, fault-masking, consistency management, and recovery, is moved into a separate redundancy management circuitry (one for each redundant computing node). In this article we present an improved design of that architecture, specifically resolving some potential inconsistencies that were not treated in detail in the original design. In particular, we present novel techniques for enforcing replica determinism and a method for reintegration of the redundancy management circuitry after a transient failure. Our improved architecture is based on using the Controller Area Network (CAN). This has several benefits, including low-cost, and that the CAN data consistency allows us to simplify the mechanisms for replica determinism and reintegration. Although initially developed for NVP, our redundancy management circuitry also supports other software replication techniques, such as active replication.	can bus;dependability;electronic circuit;elegant degradation;error detection and correction;n-version programming;replication (computing);requirement;software fault tolerance	Julian Proenza;José Miró-Julià;Hans A. Hansson	2009	Computer Standards & Interfaces	10.1016/j.csi.2007.11.007	real-time computing;n-version programming;can bus;telecommunications;computer science;theoretical computer science;operating system;software engineering;database;distributed computing;programming language;computer security;computer network	Arch	-22.72494607270331	49.40334916286176	155070
9de90b7b2abe5a582c56b98299b65ea0e0c99426	experiences from cyberinfrastructure development for multi-user remote instrumentation	application development;software;groupware;collaboration cyberinfrastructure multiuser remote instrumentation computer controlled scientific instruments electron microscopes spectrometers telescopes;instruments;scanning electron microscopy;user interfaces computerised instrumentation groupware;collaboration;microscopy;multi user;collaborative tools;servers;electron microscope;bandwidth;service level agreement;computerised instrumentation;instruments electron microscopy spectroscopy telescopes costs management training bandwidth collaborative tools collaborative work online communities technical collaboration;user interfaces;data management system	Computer-controlled scientific instruments such as electron microscopes, spectrometers, and telescopes are expensive to purchase and maintain. Also, they generate large amounts of raw and processed data that has to be annotated and archived. Cyber-enabling these instruments and their data sets using remote instrumentation cyberinfrastructures can improve user convenience and significantly reduce costs. In this paper, we discuss our experiences in gathering technical and policy requirements of remote instrumentation for research and training purposes. Next, we describe the cyberinfrastructure solutions we are developing for supporting related multi-user workflows. Finally, we present our solution-deployment experiences in the form of case studies. The case studies cover both technical issues (bandwidth provisioning, collaboration tools, data management, system security) and policy issues (service level agreements, use policy, usage billing). Our experiences suggest that developing cyberinfrastructures for remote instrumentation requires: (a) understanding and overcoming multi-disciplinary challenges, (b) developing reconfigurable-and-integrated solutions, and (c) close collaborations between instrument labs, infrastructure providers, and application developers.	archive;computer security;cyberinfrastructure;electron;electronic billing;emoticon;multi-user;provisioning;requirement;service-level agreement;software deployment	Prasad Calyam;Abdul Kalash;Neil Ludban;Sowmya Gopalan;Siddharth Samsi;Karen A. Tomko;David E. Hudak;Ashok K. Krishnamurthy	2008	2008 IEEE Fourth International Conference on eScience	10.1109/eScience.2008.58	simulation;human–computer interaction;engineering;world wide web	SE	-32.465370797916776	51.767380788627854	155204
6a21bba478809ba3a6d782bacf54ec86804eefc4	supporting dynamic update and resource protection in an embedded operating system	embedded system;operating system;embedded operating system;on the fly;resource protection;dynamic update;critical section	Dynamic update provides more flexibility in the development of embedded systems since it allows embedded systems to update their components on-the-fly without rebooting or stopping system services. However, downloading an incautiously developed component may corrupt the system. In this paper, we have implemented a platform which supports a dynamic update dissemination mechanism for upgrading an embedded operating system without rebooting the whole systems. Besides, a resource protection mechanism is implemented to protect system resources. If an error component has misused resources, the wasted resources will be reclaimed and it will be removed out of our embedded client. Currently, our system can reclaim lost memory space, ensure normal execution of critical sections, and prevent null pointer access. Experimental results demonstrate that our platform can effectively provide dynamic update and resource protection with little overhead.	booting;critical section;dspace;download;embedded operating system;embedded system;overhead (computing);pointer (computer programming);protection mechanism	Mei-Ling Chiang;Hsiang-Yu Hsu	2011		10.1145/1982185.1982316	embedded system;embedded operating system;real-time computing;computer science;operating system;distributed computing;critical section;computer security	Embedded	-24.277990792397954	50.93042190696726	155490
0e562f28c4d8d797c341ebc0743ab253c0b342b2	sadlboib: self-adaptive dynamic load balance over infiniband	design model;system reliability;resource allocation;telecommunication standards resource allocation self adjusting systems;self adjusting systems;adaptive dynamics;rdma operation;system performance;load management web server computer industry computer architecture job shop scheduling runtime dynamic scheduling switches dispatching quality of service;infiniband;data center;self adaptive dynamic load balance;one side communication;telecommunication standards;multilevel design model self adaptive dynamic load balance infiniband rdma operation one side communication self management;load balance;multilevel design model;high speed;self management	InfiniBand, as a new industry standard of advanced interconnection architecture for high-performance computing and data-center applications, provides superior performance, QoS, and management features compared with other classic high-speed interconnections, e.g., Myrinet, GigE and etc. However, how to design and implement a load balancing strategy in an InfiniBand connected system which can make full use of the features of InfiniBand has not been deeply explored yet. This paper presents the self-adaptive dynamic load balance over InfiniBand (SADLBoIB), a load balancing system which efficiently utilizes RDMA-operation and one-side communication capabilities provided by InfiniBand to dramatically reduce load-balance messages overhead and improve the system reliabilities with a new self-management mechanism. Besides, the multi-level design model of SADLBoIB provides more configurability and customization than normal load-balance systems. Performance evaluation shows that compared with traditional load balancing implementation, SADLBoIB can shorten the communication times at least by half through the one-side communication provided by RDMA operation	data center;infiniband;interconnection;level design;load balancing (computing);overhead (computing);performance evaluation;remote direct memory access;self-management (computer science);supercomputer;technical standard	Yonghao Zhou;Jizhong Han;Jin He;Hongwei Zhang;Zifeng Xiao	2006	2006 Japan-China Joint Workshop on Frontier of Computer Science and Technology	10.1109/FCST.2006.28	embedded system;data center;parallel computing;real-time computing;resource allocation;computer science;load balancing;operating system;distributed computing;computer performance;computer network	HPC	-20.399556873003757	59.21929257724266	155896
09afe8a8b5337a37501efac46c8472f87c24163f	access control model for grid virtual organizations	virtual organization;access control models		access control	Bassem Nasser;Abdelmalek Benzekri;Romain Laborde;Frédéric Grasset;François Barrère	2005			semantic grid;computer science;knowledge management;distributed computing;computer network	HPC	-31.583472189700327	46.39245382222667	156073
681c13b2352b14348757ac3703c001d0310e34f5	a reliable storage management layer for distributed information retrieval systems	cluster computing;management system;information retrieval system;storage management;distributed information retrieval;network of workstation;load balance;self managing systems	We present a storage management layer that facilitates the implementation of parallel information retrieval systems, and related applications, on networks of workstations. The storage management layer automates the process of adding and removing nodes, and implements a dispersed mirroring strategy to improve reliability. When nodes are added and removed, the document collection managed by the system is redistributed for load balancing purposes. The use of dispersed mirroring minimizes the impact of node failures and system modifications on query performance.	archive;disk mirroring;information retrieval;load balancing (computing);node (computer science);workstation	Charles L. A. Clarke;Philip L. Tilker;Allen Quoc-Luan Tran;Kevin Harris;Antonio S. Cheng	2003		10.1145/956863.956905	real-time computing;computer cluster;computer science;load balancing;management system;database;distributed computing	Web+IR	-22.086928930062168	51.8577809722708	156162
969633a4bff4718161f1dde7281a856729113257	incentivising resource sharing in edge computing applications		There is increasing realisation that edge devices, which are closer to a user, can play an important part in supporting latency and privacy sensitive applications. Such devices have also continued to increase in capability over recent years, ranging in complexity from embedded resources (e.g. Raspberry Pi, Arduino boards) placed alongside data capture devices to more complex “micro data centres”. Using such resources, a user is able to carry out task execution and data storage in proximity to their location, often making use of computing resources that can have varying ownership and access rights. Increasing performance requirements for stream processing applications (for instance), which incur delays between the client and the cloud have led to newer models of computation, which requires an application workflow to be split across data centre and edge resource capabilities. With recent emergence of edge/fog computing it has become possible to migrate services to microdata centres and to address the performance limitations of traditional (centralised data centre) cloud based applications. Such migration can be represented as a cost function that involves incentives for micro-data centres to host services with associated quality of services and experience. Business models need to be developed for creating an open edge cloud environment where micro-data centres have the right incentives to support service hosting, and for large scale data centre operators to outsource service execution to such micro data centres. We describe potential revenue models for micro-data centers to support service migration and serve incoming requests for edge based applications. We present several cost models which involve combined use of edge devices and centralised data centres.	arduino;business logic;centralisation;cloud computing;computer data storage;data center;edge computing;embedded system;emergence;fog computing;forward error correction;loss function;microdata (html);model of computation;network architecture;outsourcing;raspberry pi 3 model b (latest version);requirement;stream processing;vehicle-to-vehicle	Ioan Petri;Omer F. Rana;Joseph Bignell;Surya Nepal;Nitin Auluck	2017		10.1007/978-3-319-68066-8_16	edge device;pi;computer science;distributed computing;cloud computing;shared resource;workflow;data center;ranging;edge computing	HPC	-30.70344406999617	59.69856388404093	156464
89462e1a2d83ab89fc9bf4673427371e4416f3c5	linux and open source in the academic enterprise	commodity hardware;operating system;beowulf;linux;it management;open source software;open source	"""Open Source Software (OSS) has made great strides toward mainstream acceptance over the past two years. However, many IT managers, both in business and academia, are still cautious about OSS. Is it reliable? Is there support? Will it last? Linux has further complicated the issue not only because its operating system is OSS, but because it runs on inexpensive commodity hardware. Often IT managers are hesitant to move from long trusted proprietary hardware and software and trust major projects to OSS and commodity hardware. Past SIGUCCS presentations by Virginia Commonwealth University have detailed our use of standards based email and directory services to replace legacy systems. That email migration put us on a path toward the implementation of a variety of Open Source Software and commodity hardware solutions. In 1997, the primary Open Source Software in use on our campus was Perl. In the past three years, we have implemented OSS solutions for email, webserving, webmail, software development, directory services, and database development. While implementing OSS we have also begun to implement commodity hardware solutions running the Linux Operating System in those areas where it provides benefits. While Linux webservers have become the norm, we have brought other Linux based machines online for directory services, webmail, and research. Recently, we investigated, benchmarked and purchased a Beowulf Linux cluster to significantly expand our ability to provide resources for our computationally intensive research. Open Source Software and the Linux operating system provide two very important tools to allow universities to leverage skilled and trained staffs to meet user needs and expectations in a highly cost effective manner without sacrificing quality of service. This paper will examine VCU’s transition from proprietary hardware and software solutions to OSS and commodity hardware. It will focus on selection criteria, testing methods, implementation, the evaluation process and the """"selling"""" of OSS and commodity hardware to IT managers."""	beowulf cluster;commodity computing;computer cluster;database;directory service;email;legacy system;linux;open sound system;operating system;perl;proprietary hardware;quality of service;software development;software portability;web server;webmail	Mike Davis;Will O'Donovan;John Fritz;Carlisle Childress	2000		10.1145/354908.354923	red hat enterprise linux;parallel computing;gnu/linux;computer science;operating system;linux unified key setup;database;open system;linux kernel	Security	-27.750343992203963	52.04618024213411	156538
95bcb21cefd4149ea89eb868ffd1a9b9d3322ed3	design and implementation of a resource manager in a distributed database system	distributed system;capacity planning;resource manager;distributed database system;design and implementation;load balance;service delivery	This paper describes a system called Trends for managing IT resources in a production server environment. The objective of Trends is to reduce operational costs associated with unplanned outages, unbalanced utilization of resources, and inconsistent service delivery. The Trends resource manager balances utilization of multiple resources such as processor and disk space, manages growth to extend resource lifetimes, and factors in variability to improve temporal stability of balancing solutions. The methodology applies to systems in which workload has a strong affinity to databases, files, or applications that can be selectively placed on one or more nodes in a distributed system. Studies in a production environment demonstrate that balancing solutions remain stable for as long as the 9–12 months covered by our data. This work takes place in the context of the Lotus Notes distributed database system, and is based on analysis and data from a production server farm hosting over 20,000 databases.	affinity analysis;deployment environment;disk space;distributed computing;distributed database;enterprise resource planning;heart rate variability;ibm notes;itil;lotus 1-2-3;server (computing);server farm;unbalanced circuit	Norman Bobroff;Lily B. Mummert	2005	Journal of Network and Systems Management	10.1007/s10922-005-4439-4	real-time computing;computer science;service delivery framework;load balancing;resource management;database;computer security;computer network	DB	-24.206104924826658	52.64427127936796	156545
7134557ea43021fbfbdb0852b13ba72501e75007	the basics of reliable distributed storage networks	disaster recovery;network server;distributed networks;storage management;distributed storage;storage area networks;data storage;storage area networks computer network reliability storage management;open system;it management;reliable distributed storage networks network server shared storage access storage devices access protocols information management functions backup data mirroring disaster recovery data migration data management disk storage storage capacity business to business e commerce business to consumer e commerce;process improvement;data transfer;possibility distribution;computer network reliability;fabrics storage area networks access protocols wide area networks computer networks switches information management companies optical fiber networks sonet	Because of storage protocols that operate over extended distances, various distributed storage applications that improve the efficiency and reliability of data storage are now possible. Distributed storage applications improve efficiency by allowing any network server to transparently consolidate and access data stored in multiple physical locations. Remote backup and mirroring improve the system's reliability by copying critical data. These processes improve efficiency by eliminating backup downtime and manual backup operations. Business continuity and disaster recovery capabilities enable enterprises to recover quickly and transparently from system failure or data loss. Storage protocols and gateway devices enable rapid and transparent data transfer between mainframe applications and open-systems applications. NAS applications provide shared file access for clients using standard LAN-based technology, and can integrate with SAN architectures to provide truly distributed network capabilities. All these distributed storage network applications enable IT managers to improve data availability and reliability while minimizing management overhead and costs.	backup;business continuity;clustered file system;computer data storage;disaster recovery;disk mirroring;downtime;mainframe computer;overhead (computing);scott continuity;server (computing);storage area network	Thomas C. Jepsen	2004	IT Professional	10.1109/MITP.2004.23	storage area network;converged storage;data loss;computer science;operating system;computer data storage;database;data efficiency;open system;computer security;disaster recovery;server;computer network	OS	-24.331214971471454	50.31391451890151	156757
3324047678a2decd249f15394b639a75a13ddbda	research on dynamic reconfiguration technology of cloud computing virtual services	cloud computing servers monitoring system software hardware aerodynamics;dynamic reconfiguration;multi level hierarchical control;hierarchical control;multi level hierarchical control cloud computing virtual services dynamic reconfiguration reconfigure policy;virtual services;dynamic reconfigure policy dynamic reconfiguration technology cloud computing virtual services virtual services management virtual services maintenance dynamic reconfigure granularity;reconfigure policy;cloud computing	Based on the characteristic of cloud computing, We have operated a research on the dynamic reconfiguration technique of cloud computing virtual services aiming at the management and maintenance of virtual services. The paper plots out three level dynamic reconfigure granularity about virtual services; according to different reconfiguration occasions puts forward dynamic reconfigure policies respectively in allusion to failure, system updating and user request; and designs a concrete scenario.	cloud computing;self-management (computer science);software deployment	Shuai Wang;Fang Du;Xinming Li;Yi Li;Xingye Han	2011	2011 IEEE International Conference on Cloud Computing and Intelligence Systems	10.1109/CCIS.2011.6045088	embedded system;real-time computing;cloud computing;computer science;operating system;distributed computing;services computing	HPC	-26.217871078663915	57.993470279815945	156847
1a7c24695fe665419ef74aa04b4dd6e524a03ef7	ogsa-based grid workload monitoring	performance evaluation;perforation;performance evaluation grid computing open systems petri nets monitoring;dynamic distributed system;system performance;open grid service architecture;monitoring;petri nets;monitoring delay concurrent computing web services grid computing visualization service oriented architecture distributed computing laboratories system performance;open systems;petri net;grid computing;response time service petri net model heterogeneous distributed systems adaptive performance tuning grid workload monitoring open grid service architecture ogsa;performance tuning	In heterogeneous and dynamic distributed systems like the grid, detailed monitoring of workload and its resulting system performance (e.g. response time) is required to facilitate performance diagnosis and adaptive performance tuning. In this paper, we present a workload monitoring infrastructure for this purpose. The infrastructure classifies and monitors workload across components in grids based on the open grid service architecture (OGSA) in an end-to-end manner. It provides the abilities to assess what components are involved in processing a work unit, to report time elapsed at these components, and to capture concurrency and isolate which components are critical to overall performance observed. These are enclosed in an automatically constructed Response Time Service Petri Net (RT-SPN) model. A tool is provided to accept queries about work units and visualise corresponding RTSPNs. The infrastructure is also designed and implemented so as to be portable, scalable and lightweight.	arm architecture;concurrency (computer science);data mining;differentiated services;distributed computing;end-to-end principle;grid computing;inter-domain;machine learning;middleware;open grid services architecture;overhead (computing);performance tuning;petri net;prototype;response time (technology);responsiveness;scalability;software portability;substitution-permutation network	Rui Zhang;Steve Moyle;Steve McKeever;Stephen Heisig	2005	CCGrid 2005. IEEE International Symposium on Cluster Computing and the Grid, 2005.	10.1109/CCGRID.2005.1558628	embedded system;real-time computing;computer science;operating system;database;distributed computing;computer performance;petri net;workflow management system;grid computing	HPC	-30.280264591715838	53.21877376669331	157144
4769fbf0a3213f608da4f83cbb112ff71bc1b602	a novel distributed registry approach for efficient and resilient service discovery in megascale distributed systems?		Service discovery is a well-known but important aspect of dynamic servicebased systems, which is rather unsolved for megascale systems with a huge number of dynamically appearing and vanishing service providers. In this paper first requirements for service discovery in megascale systems are identified from three perspectives: provider, client and system architecture side. Existing approaches are evaluated along these lines and it becomes apparent that modern solutions make advances with respect to the distributed system architecture but fail to support many aspects of client side requirements like persistent queries and elaborate query definitions. Based on these shortcomings a novel solution architecture is presented. It is based on the idea that service description data can be subdivided into static and dynamic properties. The first group remains constant over time while the second is valid only for shorter durations and has to be updated. Expressive service queries rely on both, e.g. service location as example for the first and response time for the latter category. In order to deal with this problem, our main idea is to also subdivide the architecture into two interconnected processing levels that work independently on static and dynamic query parts. Both processing levels consist of interconnected peers allowing to auto-scale the registry dynamically according to the current workload. The implementation using the Jadex middleware is explained and the approach is empirically evaluated using an example scenario.	autonomous robot;client-side;distributed computing;high availability;middleware;relevance;requirement;response time (technology);scalability;service discovery;smart city;solution architecture;systems architecture	Lars Braubach;Kai Jander;Alexander Pokahr	2018	Comput. Sci. Inf. Syst.			OS	-27.796719003967283	59.32668811929338	157492
382043e17fba5c43c8a0af895cf754b82fa350d8	heterogeneity-aware distributed parameter servers	synchronization protocol;parameter server;stochastic gradient descent;straggler;machine learning;heterogeneity environment	We study distributed machine learning in heterogeneous environments in this work. We first conduct a systematic study of existing systems running distributed stochastic gradient descent; we find that, although these systems work well in homogeneous environments, they can suffer performance degradation, sometimes up to 10x, in heterogeneous environments where stragglers are common because their synchronization protocols cannot fit a heterogeneous setting. Our first contribution is a heterogeneity-aware algorithm that uses a constant learning rate schedule for updates before adding them to the global parameter. This allows us to suppress stragglers' harm on robust convergence. As a further improvement, our second contribution is a more sophisticated learning rate schedule that takes into consideration the delayed information of each update. We theoretically prove the valid convergence of both approaches and implement a prototype system in the production cluster of our industrial partner Tencent Inc. We validate the performance of this prototype using a range of machine-learning workloads. Our prototype is 2-12x faster than other state-of-the-art systems, such as Spark, Petuum, and TensorFlow; and our proposed algorithm takes up to 6x fewer iterations to converge.	algorithm;converge;elegant degradation;iteration;machine learning;prototype;stochastic gradient descent;tensorflow	Jiawei Jiang;Bin Cui;Ce Zhang;Lele Yu	2017		10.1145/3035918.3035933	real-time computing;computer science;machine learning;stochastic gradient descent;distributed computing	DB	-23.480926991543583	53.389354126437844	157507
69f88f0e726ec80f0a4760d16fedc20c7e880686	performance evaluation studies of client-server models using specweb99 benchmarks	integrated information system;file servers;network measurement;information systems;performance evaluation;client server systems performance evaluation file servers internet benchmark testing web sites information systems;design and development;web computing performance evaluation client server models spec web99 benchmarks world wide web server load system bound queries web techniques information discovery information reuse information mining information fusion system performance integrated information systems web based computer systems web servers apache server software sun sparc ultra 80 machine windows based clients workload parameters server configuration memory sizes;client server systems;information discovery;system performance;world wide web web server system performance benchmark testing web sites network servers time measurement information systems sun size measurement;client server;internet;web sites;world wide web;performance measurement system;benchmark testing	The World Wide Web (WWW) has experienced tremendous growth over the past few years. This increase in use makes server load heavier and leads to more systembounded queries due to newlj sought emerging web techniques, such as information discovery and reuse, mining andfirsion. There is an obvious interest in assessing the consequences of this growth and understanding the system ‘s behavior under varying conditions and workloads. The overall performance of the WWW is affected by various criteria in terms of clients, servers, and the network. Measuring performance of a computer system is a complex, difficult and important task. Time and rates are usually the basic measures of system performance. Having more realistic data about system performance will be necessav in the design and development of integrated information systems and systems for reuse. The main objective of this work i s to measure the performance of various web-based computer systems and web servers using the SPEC Web99 benchmarking toolset. In the experiments that we carried out, the server is set up using freely available Apache server software run on a Sun Sparc-Ultra 80 machine, and six Windows-based clients that are used to load the server using the benchmark suite. First, the gfects of vaiying resources, such as memory sizes and processors, on server’s performance are measured and analyzed. Then, we tested and evaluated the effects of six different clients and workload parameters on the sewer configuration. Amongst the six experiments that were performed, two were to set up in order to test the sewerside setup while the remaining four were setup to test the clients-side. Some of the parameters varied were memory sizes, number of processors, the workload, and the benchmark parameters. Some interesting results were obtained while others were as expected. The results indicate that good improvements can be achieved if workload and system resources are carefully chosen to optimize metrics of interest.	benchmark (computing);central processing unit;client–server model;computer;experiment;information discovery;information system;microsoft windows;performance evaluation;sparc;server (computing);spec#;www;web application;web server;world wide web	Ashish Godbole;Seung-yun Kim;Rodrigo Guzman;Waleed W. Smari	2003		10.1109/IRI.2003.1251444	file server;benchmark;the internet;computer science;operating system;data mining;database;computer performance;world wide web;information system;client–server model	Metrics	-23.839661485987584	57.16344209665114	157708
3bb2f341f17863c6185a3884394fa02ce1410c2c	efficient and transparent application recovery in client-server information systems	distributed system;database system;fault tolerant;application server;object relational database systems;user defined functions;client server;aggregates;parallel query processing;information system;message logging	Database systems recover persistent data, providing high database availability. However, database applications, typically residing on client or “middle-tier” application-server machines, may lose work because of a server failure. This prevents the masking of server failures from the human user and substantially degrades application availability. This paper aims to enable high application availability with an integrated method for database server recovery and transparent application recovery in a client-server system. The approach, based on application message logging, is similar to earlier work on distributed system fault tolerance. However, we exploit advanced database logging and recovery techniques and request/reply messaging properties to significantly improve efficiency. Forced log I/Os, frequently required by other methods, are usually avoided. Restart time, for both failed server and failed client, is reduced by checkpointing and log truncation. Our method ensures that a server can recover independently of clients. A client may reduce logging overhead in return for dependency on server availability during client restart.	application checkpointing;application server;client (computing);client–server model;database server;distributed computing;information system;multitier architecture;overhead (computing);request–response;server (computing);system fault tolerance;truncation	David B. Lomet;Gerhard Weikum	1998		10.1145/276304.276345	log shipping;fault tolerance;real-time computing;database server;database tuning;computer science;user-defined function;database;distributed computing;fat client;information system;database testing;application server;client–server model;server farm	DB	-22.716902310304768	49.760602614276344	157742
63b7c60eaac6052058b136d7ea94c6b1b96cb894	experiences with self-organizing, decentralized grids using the grid appliance	vpn;cluster;cloud;p2p;grid;parallel applications	"""""""Give a man a fish, feed him for a day. Teach a man to fish, feed him for a lifetime"""" -- Lau Tzu Large-scale grid computing projects such as TeraGrid and Open Science Grid provide researchers vast amounts of compute resources but with requirements that could limit access, results delayed due to potentially long job queues, and environments and policies that might affect a user's work flow. In many scenarios and in particular with the advent of Infrastructure-as-a-Service (IaaS) cloud computing, individual users and communities can benefit from less restrictive, dynamic systems that include a combination of local resources and on-demand resources provisioned by one or more IaaS provider. These types of scenarios benefit from flexibility in deploying resources, remote access, and environment configuration.  In this paper, we address how small groups can dynamically create, join, and manage grid infrastructures with low administrative overhead. Our work distinguishes itself from other projects with similar objects by enabling a combination of decentralized system organization and user access for job submission in addition to a web 2.0 interfaces for managing grid membership and automate certificate management. These components contribute to the design of the """"Grid Appliance,"""" an implementation of a wide area overlay network of virtual workstations (WOW), which has developed over the past six years into a mature system with several deployments and many users. In addition to an architectural description, this paper contains lessons learned during the development and deployment of """"Grid Appliance"""" systems and a case study backed by quantitative analysis that verifies the utility of our approach."""	cloud computing;decentralised system;dynamical system;grid computing;join (sql);open science grid consortium;organizing (structure);overhead (computing);overlay network;requirement;self-organization;software deployment;teragrid;the mythical man-month;web 2.0;workstation	David Wolinsky;Panoat Chuchaisri;Kyungyong Lee;Renato J. O. Figueiredo	2011	Cluster Computing	10.1007/s10586-011-0195-2	parallel computing;simulation;cloud computing;semantic grid;computer science;operating system;peer-to-peer;distributed computing;grid;world wide web;grid computing;computer network;cluster	HPC	-26.24125786796651	55.00137910664797	157906
4a2546b4b956d78a585f0b7dc10a6c33fec051b7	intelligent mobile agents: towards network fault management automation	network fault management automation;protocols;new technology;mobile agents;telecommunication computing;intelligent agent intelligent networks mobile agents automation humans technology management computer network management protocols scalability network servers;technology management;software agents;active network;network servers;mobile code intelligent mobile agents network fault management automation;distributed programming;fault tolerance;computer network management;mobile code;intelligent agent;intelligent mobile agents;humans;network management;intelligent networks;scalability;mobile agent;fault tolerance distributed programming software agents telecommunication network management telecommunication computing;programmable networks;fault management;telecommunication network management;automation	Mobile agents, equipped with intelligence, provide a relatively new technology that will help automate Network Management activities, which are becoming increasingly data intensive, thus demanding more direct human manager expertise and involvement. The research reported in this paper is concerned with the design of an Intelligent Mobile Agent, which accomplishes a set of Network Management tasks delegated to it from the human manager. The agent exploits the mobile code technology to roam the network from one node to another, accessing information from each node, processing this information at each node locally, and carrying the results of this processing during the migration. The agent possesses intelligence that allows it to carry out the tasks without involving the human manager. The objective is to present the manager with a set of conclusions or recommendations rather than large volumes of raw alarm data. This paper focuses on applying an Intelligent Mobile Agent to automate simple fault management tasks and proposes a framework for realizing this automation.	automation;code mobility;data-intensive computing;mobile agent	Mohamed El-Darieby;Andrzej Bieszczad	1999		10.1109/INM.1999.770711	network management;embedded system;communications protocol;active networking;intelligent network;fault tolerance;real-time computing;scalability;computer science;technology management;software agent;automation;fault management;mobile agent;distributed computing;computer security;intelligent agent;computer network	AI	-33.23510647609053	48.013598109059274	157927
5782c01d5ef7d260798474d5edb15d17fa98148b	implementing large-scale autonomic server monitoring using process query systems	file servers;degradation;process detection software;server state;user space monitoring;service state estimation;query processing;software platform;feeds;advanced behavioral models;process query system;system monitoring;actionable system;system turning;state estimation;process based analytic engine;system monitoring file servers query processing state estimation;data analysis;large scale;dynamic data;network servers;actionable system large scale autonomic server monitoring process query systems dynamic data analysis user space monitoring advanced behavioral models server state service state estimation server network cross infection performance degradation process detection software system level information autonomic computing process based analytic engine system turning;system level information;server network;engines;cross infection;monitoring;large scale systems monitoring network servers state estimation data analysis power system modeling feeds degradation information analysis engines;process query systems;power system modeling;performance degradation;information analysis;autonomic computing;large scale autonomic server monitoring;large scale systems;dynamic data analysis	In this paper we present a new server monitoring method based on a new and powerful approach to dynamic data analysis: process query systems (PQS). PQS enables user-space monitoring of servers and, by using advanced behavioral models, makes accurate and fast decisions regarding server and service state. Data to support state estimation come from multiple sensor feeds located within a server network. By post-processing a system's state estimates, it becomes possible to identify, isolate and/or restart anomalous systems, thus avoiding cross-infection or prolonging performance degradation. The PQS system we use is a generic process detection software platform. It builds on the wide variety of system-level information that past autonomic computing research has studied by implementing a highly flexible, scalable and efficient process-based analytic engine for turning raw system information into actionable system and service state estimates	analytical engine;autonomic computing;dynamic data;elegant degradation;information retrieval;pqs (chemical);scalability;server (computing);system information (windows);user space;video post-processing	Christopher Roblee;Vincent H. Berk;George Cybenko	2005	Second International Conference on Autonomic Computing (ICAC'05)	10.1109/ICAC.2005.34	real-time computing;computer science;operating system;database;distributed computing;data analysis;world wide web;statistics;computer network;autonomic computing	SE	-24.13621422925524	55.896502761857555	157938
a1676c285935d4d29f09c1668d0b0b78248e42ac	dependable cloud resources with guardian		"""Despite advances in making datacenters dependable, failures still happen. This is particularly onerous for long-running """"big data"""" applications, where partial failures can lead to significant losses and lengthy recomputations. Big data processing frameworks like Hadoop MapReduce include fault tolerance (FT) mechanisms, but these are commonly targeted at specific system/failure models, and are often redundant between frameworks. This paper proposes the paradigm of dependable resources: big data processing frameworks are typically built on top of resource management systems (RMSs), and proposing FT support at the level of such an RMS yields generic FT mechanisms, which can be provided with low overhead by leveraging constraints on resources. We demonstrate our concepts through Guardian, a robust RMS based on YARN. Guardian allows frameworks to run their applications with individually configurable FT granularity and degree, with only minor changes to their implementation. We demonstrate the benefits of our approach by evaluating Hadoop, Tez, Spark and Pig on Guardian in Amazon-EC2, improving completion time by around 68% in the presence of failures, while maintaining around 6% overhead."""	apache hadoop;batch processing;big data;cloud computing;fault tolerance;generic programming;mapreduce;overhead (computing);programming paradigm;record management services;spark;stream processing	Bara Abusalah;Derek Schatzlein;Julian James Stephen;Masoud Saeida Ardekani;Patrick Th. Eugster	2017	2017 IEEE 37th International Conference on Distributed Computing Systems (ICDCS)	10.1109/ICDCS.2017.158	resource management;fault tolerance;computer network;distributed computing;cloud computing;big data;data modeling;real-time computing;computer science;granularity	DB	-20.76519344669202	56.26076189374593	158101
264cc379f76b36bd4d87add06e426cbde5f40367	central or distributed energy storage for processors with energy harvesting	computers;epn modelling paradigm interconnected distributed computer system multiple computation centre multiple cc distributed energy storage central energy storage energy harvesting distributed es system battery energy leakage transmission line power losses energy packet network modelling paradigm;propagation losses;energy packet network energy harvesting distributed computer system;energy harvesting;servers;computational modeling;time factors;energy storage;energy storage computers energy harvesting propagation losses computational modeling servers time factors;power transmission lines energy harvesting energy storage power aware computing power system interconnection	We consider an interconnected distributed computer system with multiple computation centres (CC) that operate with energy harvesting to improve sustainability. The intermittent energy harvesting is matched with steady demand from the CCs using energy storage (ES), e.g. batteries. Based on energy leakage from batteries, and power losses over transmission lines, we examine whether a centralised or distributed ES system provides the solution that offers the smallest response time to a fixed workload of computer jobs using the Energy Packet Network (EPN) modelling paradigm.	central processing unit;centralisation;computation;computer;distributed computing;euref permanent network;programming paradigm;response time (technology);spectral leakage;transmission line	Erol Gelenbe;Elif Tugce Ceran	2015	2015 Sustainable Internet and ICT for Sustainability (SustainIT)	10.1109/SustainIT.2015.7101380	embedded system;electronic engineering;real-time computing;engineering	Arch	-20.119139117131994	60.08607232916835	158130
7deb41c62a2868bd4edacb7b1273a6500a2ac691	a technique for constructing highly available services	distributed system;mobile;nodes;reliability;availability;data replication;detection;satisfiability;stability;strategy;communications centrals;general methods;communications networks;crashes;replicas;control;systems approach	This paper describes a general method for constructing a highly available service for use in a distributed system. It gives a specific implementation of the method and proves the implementation correct. The service consists of replicas that reside at several different locations in a network. It presents its clients with a consistent view of its state, but the view may contain old information. Clients can indicate how recent the information must be. The method can be used in applications satisfying certain semantic constraints. For applications that can use it, the method performs better than other replication techniques.	distributed computing;high availability	Rivka Ladin;Barbara Liskov;Liuba Shrira	1988	Algorithmica	10.1007/BF01762124	availability;real-time computing;stability;strategy;computer science;theoretical computer science;mobile technology;reliability;distributed computing;node;systems thinking;scientific control;replication;statistics;satisfiability	Comp.	-21.874950985017275	46.53873505517959	158871
eb69f4e9203c188024753db1094d693c5b86a4ee	a scalable blackbox-oriented e-learning system based on desktop grid over private cloud	volunteer computing;e learning;desktop grid;cloud computing	Traditional web-based e-learning system suffers from unstable workloads and security risks of incorporating external executable objects to servers. This paper addresses these issues with emerging technologies, as desktop grid and cloud computing. Learning users are motivated to be volunteers by hosting the virtual machines equipped with e-learning desktop grid applications. We develop components to integrate the e-learning system and desktop grid into the circumstance in which each user serves not only a task producer, but also a volunteer that solves tasks. In order to enhance the responsiveness between the passive desktop grid server and e-learning system, we have also developed asynchronous processes to enable the server and volunteer workers to cooperate in a tightly coupled manner. The system achieves the scalability bymaintaining the ratio between the number of volunteers and the number of online users beyond certain threshold. © 2014 Elsevier B.V. All rights reserved.	cloud computing;computation;control theory;desktop computer;executable;parallel computing;responsiveness;scalability;server (computing);virtual machine;volunteer computing;web application	Lung-Pin Chen;Jien-An Lin;Kuan-Ching Li;Ching-Hsien Hsu;Zhi-Xian Chen	2014	Future Generation Comp. Syst.	10.1016/j.future.2014.02.017	parallel computing;simulation;cloud computing;desktop management interface;computer science;operating system;database;distributed computing;virtual desktop;world wide web;grid computing	HPC	-29.600879536966758	53.357641682042505	158972
56cd539733d7abf2230745978629d709f3515f6f	performance analysis of restful api and rabbitmq for microservice web application		In order to explore the communication methods of microservice web application, this paper uses RabbitMQ and REST API respectively as the message-oriented middleware of microservice web applications. We do experiments with both of the methods under various number of users to compare and evaluate their performance in different circumstances. The purpose is to provide understanding inside the two methods for microservice web applications so that service providers can select the appropriate method based on their need. Obtained experimental results show that when a large number of users send requests to the web application at the same time, it is more stable to use RabbitMQ as the Message-oriented middleware than the REST API communication method.	application programming interface;experiment;message-oriented middleware;microservices;profiling (computer programming);rabbitmq;representational state transfer;web application	Xian Jun Hong;Hyun Sik Yang;Young Han Kim	2018	2018 International Conference on Information and Communication Technology Convergence (ICTC)	10.1109/ICTC.2018.8539409	service provider;database;message-oriented middleware;web application;service-oriented architecture;middleware;computer science	HPC	-33.38358171038138	51.69660841820501	158992
f27849a56e273a40cacaaba3d01d2214bc19199e	mobisim: a simulation library for resource prediction of smartphones and wireless sensor networks	lte;resource prediction;smartphones;energy modeling;wireless sensor networks	The prediction of resource consumption is an essential task in the development of heavily resource-constrained systems, like smartphones and wireless sensor networks. In particular, good estimations on energy consumption are difficult to achieve, as they depend on the application's behavior, the used hardware and environment parameters. To address these issues, we introduce MobiSIM, an open-source OMNeT++ simulation library. MobiSIM allows the modeling of embedded systems to gain valuable information on the system's dynamic characteristics of resource consumption. The benefit of MobiSIM over other simulation systems is the focus on resource usage. Most other simulators focus on highly detailed system models to allow for the execution of native program code. These simulators are highly platform specific and complex to handle. We decided to use a coarse-grained, event-driven and platform-independent system model while still using detailed resource models for various devices. We will show that for several simulation purposes the application code can be modeled by a behavioral description, rendering instruction-accurate code execution unnecessary.	simulation;smartphone	Markus Buschhoff;Jochen Streicher;Bjoern Dusza;Christian Wietfeld;Olaf Spinczyk	2013			embedded system;real-time computing;simulation;computer science	Mobile	-24.957212188273022	56.58252471171461	159327
5930a729c61711694a6c6a8a501899323f1e1e65	toward predictive failure management for distributed stream processing systems	software;sensor systems and applications;sensor data analysis;cost function;query processing;network security;data stream;predictive failure management;distributed processing;size measurement;online learning;data stream processing;system mining;online failure prediction;reservoir sampling;data analysis;fault tolerant system;fault tolerant computing;continuous query processing predictive failure management ibm system s distributed stream processing systems sensor data analysis network security business intelligence online failure prediction light weight stream based classification methods failure penalty reduction failure penalty prevention adaptive data stream sampling schemes measurement sampling rates reservoir sampling;condition monitoring;monitoring;streaming media;failure penalty prevention;fault tolerance;classification algorithms;continuous query processing;failure penalty reduction;predictive models;intelligent networks;business intelligence;sampling methods;stream processing;predictive models monitoring software decision trees streaming media classification algorithms computer bugs;sampling methods distributed processing fault tolerant computing query processing;data stream processing failure prediction system mining fault tolerance;decision trees;computer bugs;measurement sampling rates;light weight stream based classification methods;adaptive data stream sampling schemes;intelligent sensors;failure prediction;ibm system s distributed stream processing systems;data security	Distributed stream processing systems (DSPSs) have many important applications such as sensor data analysis, network security, and business intelligence. Failure management is essential for DSPSs that often require highly-available system operations. In this paper, we explore a new predictive failure management approach that employs online failure prediction to achieve more efficient failure management than previous reactive or proactive failure management approaches. We employ light-weight stream-based classification methods to perform online failure forecast. Based on the prediction results, the system can take differentiated failure preventions on abnormal components only. Our failure prediction model is tunable, which can achieve a desired tradeoff between failure penalty reduction and prevention cost based on a user-defined reward function. To achieve low-overhead online learning, we propose adaptive data stream sampling schemes to adaptively adjust measurement sampling rates based on the states of monitored components, and maintain a limited size of historical training data using reservoir sampling. We have implemented an initial prototype of the predictive failure management framework within the IBM System S distributed stream processing system. Experiment results show that our system can achieve more efficient failure management than conventional reactive and proactive approaches, while imposing low overhead to the DSPS.	algorithm;decision tree learning;fault tolerance;iterative method;just-in-time compilation;network security;overhead (computing);prototype;reinforcement learning;reservoir sampling;sampling (signal processing);stream processing	Xiaohui Gu;Spiros Papadimitriou;Philip S. Yu;Shu-Ping Chang	2008	2008 The 28th International Conference on Distributed Computing Systems	10.1109/ICDCS.2008.34	fault tolerance;real-time computing;predictive failure analysis;computer science;network security;operating system;data mining;database;distributed computing;business intelligence;computer security	HPC	-21.17434299494445	55.543889374908744	159477
1a85b7fff238e7efcdc39eb295b5f552a3376595	a framework for scheduling parallel dbms user-defined programs on an attached high-performance computer	parallel user defined programs;high performance computing;database accelerators;high performance computer;software framework	We describe a software framework for deploying, scheduling and executing parallel DBMS user-defined programs on an attached high-performance computer (HPC) platform. This framework is advantageous for many DBMS workloads in the following two aspects. First, the long-running user-defined programs can be speeded up by taking advantage of the greater hardware parallel-ism available on the attached HPC platform. Second, the interac-tive response time of the remaining applications on the database server platform is improved by the off-loading of long-running user-defined programs to the attached HPC platform. Our frame-work provides a new approach for integrating high-performance computing into the workflow of query-oriented, computationally-intensive applications.	database server;response time (technology);scheduling (computing);server (computing);software framework;supercomputer	Michael Andreas Kochte;Ramesh Natarajan	2008		10.1145/1366230.1366245	parallel computing;computer science;operating system;distributed computing	DB	-20.334191850292655	53.75817046338708	159483
b495d0b00f6ee21ea6b6cfe65bef2764e25288b1	collective consistency	collective consistency	We present collective consistency, a new paradigm for multi-process coordination arising from our experience building a parallel version of a software package for computer aided engineering. A straightforward checkpointing and reorganization strategy will permit a parallel computation to tolerate failed or slow processes if the failure or tardi-ness is consistently detected by all participants. To this end, we propose the addition of autonomous failure detection and a consistent reporting protocol to the collective communication library supporting the industry standard Message Passing Interface. The focus of this paper is the consistent reporting protocol that must provide an approximate solution to the collective consistency problem. We give a family of knowledge-based speciications for a collective consistency protocol and discuss several implementations. We support our implementation choices with an extremely general proof that no trivially dominating solution exists.	anomaly detection;application checkpointing;approximation algorithm;autonomous robot;collective intelligence;computation;earthbound;message passing interface;parallel computing;programming paradigm;technical standard;the industry standard	Cynthia Dwork;C. T. Howard Ho;H. Raymond Strong	1996		10.1007/3-540-61769-8_16		HPC	-19.396726595850843	49.352278563999995	159530
1109a52506331b0576b18267e0c56158752f27a5	network-aware stream query processing in mobile ad-hoc networks	routing;telecommunication network topology centralised control decision support systems mobile ad hoc networks query processing telecommunication computing;conference paper;adaptive systems;heuristic algorithms;network aware stream query processing network path conditions operator replicas path diversity windowed stream query operators stream query planning fixed query plan network topology distributed bandwidth constrained nature centralized control target data center environments distributed stream query execution limited backend connectivity manet tactical edge networks data stream management model time varying data streams continuous stream queries sensing applications real time decision support applications mobile ad hoc networks;ad hoc networks;planning;mobile computing ad hoc networks planning routing data models heuristic algorithms adaptive systems;mobile computing;data models	Many real-time decision support and sensing applications can be expressed as continuous stream queries over time-varying data streams, following a data stream management model. We consider the problem of the efficient and resilient execution of continuous stream queries in tactical edge networks formed from mobile ad-hoc networks (MANETs) with limited backend connectivity. Previous approaches for distributed stream query execution target data center environments in which networks are static, and centralized control is feasible. The distributed, bandwidth-constrained and highly dynamic nature of MANETs render such approaches insufficient-while a stream query executes in a MANET, changes in the network topology mean that any fixed query plan eventually becomes outdated. We introduce an adaptive, network-aware approach for stream query planning in MANETs, which supports both single- and multi-input windowed stream query operators. The basic idea is to increase the path diversity available when executing stream queries by replicating query operators across many nodes in the MANET. During execution, it becomes possible to dynamically switch between different operator replicas based on connectivity and other network path conditions. We evaluate our approach in emulated MANETs, showing that it can increase substantially the robustness of distributed stream query processing under mobility.	centralized computing;data center;database;decision support system;emulator;hoc (programming language);network topology;query plan;real-time clock;real-time locating system;routing;software deployment;stream processing;window function	Dan O'Keeffe;Theodoros Salonidis;Peter R. Pietzuch	2015	MILCOM 2015 - 2015 IEEE Military Communications Conference	10.1109/MILCOM.2015.7357630	query optimization;computer science;database;distributed computing;data stream mining;computer network	DB	-20.215836078681377	55.553209209874026	159909
10096038b403c03744aaf77160694b89325f2722	impediments to analytical modeling of multi-tiered web applications	databases;analytical models;domain specific modeling;capacity planning;service provider;application specific modeling;analytical modeling;performance estimation;cost analysis;data analysis;multitiered web application;computational modeling;web services;service deployment multi tier applications performance estimation;multi tier applications;analytical models load modeling databases optimization computational modeling capacity planning program processors;optimization;service deployment;load modeling;system management;configuration management;program processors;feedback control;analytical model;domain specific modeling analytical modeling multitiered web application service provider system management application specific modeling;web services data analysis	Service providers hosting multi-tiered applications require accurate analytical models of the applications they will host for different system management activities, such as capacity planning, configuration management, cost analysis and feedback control. Due to the complexity of real world scenarios, developing accurate analytical models is hard. This paper presents the commonly faced challenges in developing these analytical models that stem from real-world issues, such as excessive system activity, presence of multiple cores or processors, and concurrency management. Presence of multi-tiered applications further compounds the challenges faced. We sketch preliminary ideas based on application-specific and/or domain-specific modeling techniques to overcome these limitations.	activity recognition;application domain;central processing unit;concurrency (computer science);configuration management;domain-specific language;domain-specific modeling;feedback;job stream;systems management;web application	Nilabja Roy;Aniruddha S. Gokhale;Lawrence W. Dowdy	2010	2010 IEEE International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems	10.1109/MASCOTS.2010.60	service provider;web service;systems management;simulation;computer science;cost–benefit analysis;feedback;management science;configuration management;data analysis;computational model	Embedded	-24.161122101946237	58.067274213286076	159998
32b5c5afc35646bbf140176d1532f374919026c6	the performance of checkpointing and replication schemes for fault tolerant mobile agent systems	performance evaluation;fault tolerant;virtual machines system recovery software fault tolerance software performance evaluation mobile agents;mole mobile agent system;mobile agents;simulation;distributed computing;software performance evaluation;software fault tolerance;replication schemes;checkpointing;software performance;experimental system;fault tolerant system;computational modeling;mobile agent system;system recovery;fault tolerant mobile agent systems performance evaluation experimental system mole mobile agent system simulation controllable system parameter values checkpointing replication schemes;controllable system parameter values;virtual machines;guidelines;fault tolerant systems;fault tolerance;fault tolerant mobile agent systems;mobile computing;checkpointing fault tolerant systems mobile agents costs fault tolerance mobile computing computational modeling guidelines software performance distributed computing	This paper evaluates the performance of checkpointing and replication schemes for the fault tolerant mobile agent system. For the quantitative comparison, we have implemented an experimental system on top of the Mole mobile agent system and also built a simulation system to include various failure cases. Our experiment aims to have the insight into the behavior of agents under two schemes and provide a guideline for the fault tolerant system design. The experimental results show that the checkpointing scheme shows a very stable performance; and for the replication scheme, some controllable system parameter values should be chosen carefully to achieve the desirable performance.	application checkpointing;blocking (computing);experimental system;failure rate;fault tolerance;mobile agent;overhead (computing);simulation;systems design	Taesoon Park;Ilsoo Byun;Hyunjoo Kim;Heon Young Yeom	2002		10.1109/RELDIS.2002.1180195	embedded system;fault tolerance;real-time computing;computer science;operating system;distributed computing;mobile computing	Mobile	-22.303340846875347	52.84100033200483	160159
8b473b38c2f46ffc3fb6a5aecd7f4c9bb9b63df3	performance and efficiency in distributed enterprise management	queuing model;application management;information caching;enterprise networks;performance improvement;management protocols;integrated enterprise management;distributed management architectures;networked systems;overall response;cache management;distributed management	This paper is motivated by the increasing needfor scaleable, distributed management architectures forintegrated network, system and application managementwithin the enterprise network environment. Such integration, extension and wide area deploymentof management functionality impose heavy performancerequirements and produce increased management traffic.Aiming to minimize this traffic and the overall response time, we propose a distributedhierarchical caching scheme that attempts to takeadvantage of the diverse consistency requirements ofmanagement applications. We define coherency conditionsand update policies, identify the appropriateinteraction semantics, and discuss an SNMP-basedimplementation. In order to evaluate the proposed modeland to quantify the expected performance gains weconstruct a simple queuing model that provides analyticalresults on the improvement of response time and thereduction of management traffic. Finally, the analysisof experimental results provides some insight on performance improvement for specific classes ofmanaged objects.	cache (computing);common object request broker architecture;exodus;object storage;object type (object-oriented programming);prototype;queueing theory;real life;requirement;response time (technology);simple network management protocol	Fotis Stamatelopoulos;Basil S. Maglaris	1999	Journal of Network and Systems Management	10.1023/A:1018765816263	fcaps;element management system;real-time computing;systems management;simulation;network management station;enterprise architecture management;network management application;structure of management information;application lifecycle management;management;computer security;computer network	DB	-22.23419977743354	53.96584805204908	160271
16224d813feaac8f346a0f6b0536c60ef830bc5d	systems for interprocess communication in a resource sharing computer network			inter-process communication	David C. Walden	1970	RFC	10.17487/RFC0062		Theory	-28.390822108495254	46.66364533418823	160618
403fa2851b75a0475ed5cb9eda6b5795ca58179d	database replication: if you must be lazy, be consistent	isochronous replication;electrical capacitance tomography;protocols;transactional semantics;write anywhere capability;asynchronous replica management protocols;communication networks;memory protocols;memory protocols replicated databases;maintenance;clocks;database replication;write anywhere capability database replication performance penalties isochronous replication asynchronous replica management protocols lazy protocols consistency serializability transactional semantics data placement data object update protocol epidemic communication;epidemic communication;network servers;col;transaction databases;lazy protocols;serializability;protocols clocks transaction databases electrical capacitance tomography operating systems network servers computer science human resource management maintenance communication networks;computer science;replicated databases;data placement;consistency;human resource management;data object update protocol;operating systems;performance penalties	Due to severe performance penalties associated with synchronous replication, there is a significant interest in asynchronous replica management protocols. Lazy protocols currently in use either do not guarantee consistency and serializability as needed by transactional semantics o r they impose restrictions on placement of data and which data object can be updated. In this paper we consider an alternative update protocol based on epidemic communication that guarantees consistency and serializability in sp ite of a write-anywhere capability.	lazy evaluation;replication (computing);serializability	JoAnne Holliday;Divyakant Agrawal;Amr El Abbadi	1999		10.1109/RELDIS.1999.805112	global serializability;communications protocol;real-time computing;commitment ordering;computer science;two-phase locking;operating system;human resource management;database;distributed computing;consistency;serializability;replication;computer network	DB	-22.31348776423487	49.00155419291484	160656
cb66b8801f50d10cee502ebf3a6d3a6adabb5083	unifying concurrency control and recovery of transactions with semantically rich operations	transaction management;concurrency control;reading and writing	The classical theory of transaction management contains two different aspects, namely concurrency control and recovery, which ensure serializability and atomicity of transaction executions, respectively. Although concurrency control and recovery are not independent of each other, the criteria for these two aspects were developed orthogonally and as a result, in most cases these criteria are incompatible with each other. Recently a unified theory of concurrency control and recovery for databases with read and write operations has been introduced in [19, l] that allows reasoning about serializability and atomic@ within the same framework. In [19, 11 a class of schedules (called prejix reducible), which guarantees both serializability and atomicity in a failure prone environment with read/write operations was introduced. Several protocols were developed to generate such schedules by a database concurrency control mechanism. We present here a unified transaction model for databases with an arbitrary set of semantically rich operations. We investigate constructive characterization of the class of prefix reducible schedules with semantically rich operations. It turns out that unlike databases with only read/write operations, the exact characterization of prefix reducible schedules in databases with arbitrary operations is rather infeasible. Thus, we propose here several sufficiently rich subclasses of prefix reducible schedules, and design concurrency control protocols that guarantee both serializability and atomic&-y for schedules from these classes.	abstract data type;atomicity (database systems);concurrency (computer science);concurrency control;correctness (computer science);database;database transaction;distributed database;gerhard j. woeginger;read-write memory;schedule (computer science);semantic html;serializability;transaction processing;undo	Radek Vingralek;Haiyan Hasse-Ye;Yuri Breitbart;Hans-Jörg Schek	1998	Theor. Comput. Sci.	10.1016/S0304-3975(97)00095-9	global serializability;timestamp-based concurrency control;optimistic concurrency control;real-time computing;isolation;commitment ordering;distributed transaction;computer science;two-phase locking;concurrency control;database;distributed computing;multiversion concurrency control;non-lock concurrency control;programming language;serializability;algorithm;snapshot isolation;schedule;distributed concurrency control	DB	-23.335130529343854	47.766306453574906	160823
c9544bcd3e312c8af926d14ad638da281a1d2a14	garbage collection and dsm consistency	garbage collection	This paper presents the design of a copying garbage collector for persistent distributed shared objects in a loosely coupled network with weakly consistent distributed shared memory (DSM). The main goal of the design for this garbage collector is to minimize the communication overhead due to collection between nodes of the system, and to avoid any interference with the DSM memory consistency protocol. Our design is based on the observation that, in a weakly consistent DSM system, the memory consistency requirements of the garbage collector are less strict than those of the applications. Thus, the garbage collector reclaims objects independently of other copies of the same objects without interfering with the DSM consistency protocol. Furthermore, our design does not require reliable communication support, and is capable of reclaiming distributed cycles of dead objects.	cheney's algorithm;consistency model;distributed shared memory;garbage collection (computer science);interference (communication);loose coupling;overhead (computing);requirement	Paulo José Azevedo Vianna Ferreira;Marc Shapiro	1994			manual memory management;garbage;parallel computing;real-time computing;computer science;distributed computing;garbage collection	OS	-23.83095599675056	46.524953566584436	161310
c2453e5c83fd78421f8ce1df6ecb5127a97cced1	estimating the reliability of regeneration-based replica control protocols	databases;analytical models;distributed system;evaluation performance;system reliability;protocols;replication;fiabilite systeme;systeme reparti;closed form solution;performance evaluation;protocole transmission;data integrity;numerical solution;maintenance;availability;evaluacion prestacion;sistema informatico;distributed processing;replica maintenance;differential equation;computer system;replica maintenance several sites consistency control protocol replicated data object;replicacion;fiabilidad sistema;data analysis;protocolo transmision;sistema repartido;access protocols costs availability maintenance analytical models fault tolerance partitioning algorithms closed form solution databases data analysis;consistency control protocol;replicated data;fault tolerance;concurrency control;replicated data object;access protocols;protocols concurrency control data integrity distributed processing performance evaluation;systeme informatique;several sites;partitioning algorithms;transmission protocol	The accessibility of vital information can be enhanced by replicating the data on several sites and employing a consistency control protocol to manage the replicas. The reliability of a replicated data object depends on maintaining a viable set of current replicas. When storage is limited, it may not be feasible to simply replicate a data object at enough sites to achieve the desired level of reliability. Regeneration approximates the reliability provided by additional replicas for a modest increase in storage costs, and is applicable whenever a new replica of a data object can be created faster than a system failure can be repaired. Regeneration enhances reliability by creating new replicas on other sites in response to site failures. Several strategies for replica maintenance are considered, and the benefits of each are analyzed using simulation and both algebraic and numeric solutions to systems of differential equations. >		Darrell D. E. Long;John L. Carroll;Kris Stewart	1989	IEEE Trans. Computers	10.1109/12.40847	embedded system;availability;fault tolerance;replication;parallel computing;real-time computing;computer science;operating system;concurrency control;data integrity;database;distributed computing;data analysis;differential equation	Arch	-21.2324593917369	47.33244013677353	161391
1031fefd3c3e52a3c640f8318f3e55f71669e94d	distributed computing economics	cluster computing;distributed computing;computational economics;internet architecture;network traffic	Computing economics are changing. Today there is rough price parity between: (1) one database access; (2) 10 bytes of network traffic; (3) 100,000 instructions; (4) 10 bytes of disk storage; and (5) a megabyte of disk bandwidth. This has implications for how one structures Internet-scale distributed computing: one puts computing as close to the data as possible in order to avoid expensive network traffic.	byte;disk storage;distributed computing;megabyte;network packet	Jim Gray	2008	ACM Queue	10.1145/1394127.1394131	computer cluster;computer science;theoretical computer science;operating system;software engineering;database;distributed computing;utility computing;computer security;grid computing;autonomic computing	Theory	-28.676528859015015	57.81769756494434	161535
2aa4a44f8d025c45ad562203ba12397df609c7ca	volume leases for consistency in large-scale systems	tolerancia falta;distributed system;cache storage;information resources;systeme reparti;lease;fichier;data integrity;fault tolerant;cache consistency;perforation;sistema;information resources data integrity cache storage software performance evaluation software fault tolerance internet;performance;web accessibility;serveur informatique;volume;software performance evaluation;software fault tolerance;large scale system;fichero;algorithme;algorithm;large scale;message traffic large scale systems consistency volume leases server driven cache consistency large scale networks performance fault tolerance server scalability client driven protocols world wide web object leases trace driven simulation cache consistency;multiple objectives;volumen;sistema repartido;internet;strong consistency;file;file system;system;fault tolerance;large scale systems network servers file servers file systems access protocols fault tolerance scalability fault tolerant systems humans delay;servidor informatico;distributed file system;systeme;spatial locality;rendimiento;trace driven simulation;tolerance faute;scalable server;geographic distribution;computer server;algoritmo	This article introduces volume leases as a mechanism for providing server-driven cache consistency for large-scale, geographically distributed networks. Volume leases retain the good performance, fault tolerance, and server scalability of the semantically weaker client-driven protocols that are now used on the web. Volume leases are a variation of object leases, which were originally designed for distributed file systems. However, whereas traditional object leases amortize overheads over long lease periods, volume leases exploit spatial locality to amortize overheads across multiple objects in a volume. This approach allows systems to maintain good write performance even in the presence of failures. Using trace-driven simulation, we compare three volume lease algorithms against four existing cache consistency algorithms and show that our new algorithms provide strong consistency while maintaining scalability and fault-tolerance. For a trace-based workload of web accesses, we find that volumes can reduce message traffic at servers by 40% compared to a standard lease algorithm, and that volumes can considerably reduce the peak load at servers when popular objects are modified.	algorithm;amortized analysis;cache coherence;fault tolerance;load profile;locality of reference;principle of locality;scalability;server (computing);simulation;strong consistency	Jian Yin;Lorenzo Alvisi;Michael Dahlin;Calvin Lin	1999	IEEE Trans. Knowl. Data Eng.	10.1109/69.790806	fault tolerance;real-time computing;computer science;operating system;database;world wide web	OS	-22.142961893998418	49.22010121349369	161669
49399f7217eaceb07401e045f781c8ba35900e45	virtual machine-based simulation of distributed computing and network computing	virtual machine;iterative algorithms;distributed computing;queueing networks;distributed computing system;performance analysis;load dependent queues;analytic models;network computing;modeling and analysis;approximations;product form	This paper proposes the use of virtual machine architectures as a means of modeling and analyzing networks and distributed computing systems. The requirements for such modeling and analysis are explored and defined along with an illustrative study of an X.25 link-level protocol performance under normal execution conditions. The virtualizable architecture used in this work is the Data General Nova 3/D.	data general nova;distributed computing;requirement;simulation;virtual machine	Richard T. Wang;James C. Browne	1981		10.1145/800189.805485	distributed algorithm;real-time computing;failure semantics;computer science;virtual machine;theoretical computer science;operating system;distributed computing;utility computing;distributed design patterns;grid computing;autonomic computing	HPC	-29.303280401580448	47.29565515838932	161679
0ee5abec0c7002c759d70e4d75921b65a6d8666a	bubing: massive crawling for the masses	high throughput;twenty year;time scales linearly;single-machine tool;massive crawling;open-source crawling software;web crawler	Although web crawlers have been around for twenty years by now, there is virtually no freely available, open-source crawling software that guarantees high throughput, overcomes the limits of single-machine systems, and, at the same time, scales linearly with the amount of resources available. This article aims at filling this gap, through the description of BUbiNG, our next-generation web crawler built upon the authors’ experience with UbiCrawler [9] and on the last ten years of research on the topic. BUbiNG is an open-source Java fully distributed crawler; a single BUbiNG agent, using sizeable hardware, can crawl several thousand pages per second respecting strict politeness constraints, both host- and IP-based. Unlike existing open-source distributed crawlers that rely on batch techniques (like MapReduce), BUbiNG job distribution is based on modern high-speed protocols to achieve very high throughput.	advanced synchronization facility;apache hbase;asynchronous i/o;berkeley db;clustered file system;component-based software engineering;cyberith virtualizer;data structure;distributed web crawling;download;fifo (computing and electronics);fallout;html;hypertable;hypertext transfer protocol;i/o bound;input/output;java;jira (software);library (computing);lock (computer science);mapreduce;non-blocking algorithm;object lifetime;open-source software;parallel computing;software regression;spamming;spider trap;throughput;time complexity;user space;web crawler;workbench;world wide web	Paolo Boldi;Andrea Marino;Massimo Santini;Sebastiano Vigna	2014	TWEB	10.1145/3160017	simulation;computer science;web crawler;distributed web crawling;multimedia;world wide web	OS	-26.539445513354355	53.15024921318357	161779
0dd2f6dd94553089913c12897c995c7e6505da04	degrees of isolation, concurrency control protocols, and commit protocols	concurrency control		concurrency control	Vijayalakshmi Atluri;Elisa Bertino;Sushil Jajodia	1994			timestamp-based concurrency control;optimistic concurrency control;isolation;concurrency control;multiversion concurrency control;non-lock concurrency control;serializability;distributed concurrency control	Theory	-23.447638922878298	47.53129255239696	161791
1ea2dbd5d0b13e2ee32156a8ac6f7f10620643bc	on a unified framework for the evaluation of distributed quorum attainment protocols	tolerancia falta;analytical models;distributed quorum attainment protocols;distributed algorithms;distributed system;availability delay effects performance analysis access protocols computer science time measurement costs analytical models upper bound fault tolerant systems;performability metric;protocols;software fault tolerance distributed algorithms protocols software performance evaluation;parallel version;systeme reparti;tree based mutual exclusion;time measurement;availability;performability;primary site approach;software performance evaluation;delay effects;software fault tolerance;unified framework;indexing terms;performance metric;exclusion mutual;algorithme;mutual exclusion;upper bound;fault tolerant distributed systems;algorithm;tree based mutual exclusion protocols unified framework distributed quorum attainment protocols mutual exclusion algorithms protocol performance unified analytical model network delay performability metric majority consensus algorithms primary site approach performability parallel version performance analysis fault tolerance distributed systems delay analysis;unified analytical model;it value;sistema repartido;fault tolerant systems;fault tolerance;analyse performance;primary site protocol;retard;protocol performance;performance analysis;access protocols;majority consensus algorithms;delay analysis;network delay;exclusion mutuelle;computer science;distributed systems;unified evaluation model;retraso;tree based mutual exclusion protocols;tolerance faute;mutual exclusion algorithms;lower bound;analytical model;majority consensus;algoritmo;analisis eficacia	Abstmct~uorum attainment protocols are an important part of many mutual exclusion algorithms. Assessing the performance of such protocols in terms of number of messages, as is usually done, may be less significant than being able to compute the delay in attaining the quorum. Some protocols achieve higher reliability at the expense of increased message cost or delay. A unified analytical model which takes into account the network delay and its effect on the time needed to obtain a quorum is presented. A combined performability metric, which takes into account both availability and delay, is defined in this paper, and expressions to calculate its value are derived for two different reliable quorum attainment protocols: Agrawal and El Abbadi’s and Majority Consensus algorithms. Expressions for the Primary Site approach are also given as upper bound on performability and lower bound on delay. A parallel version of the Agrawal and El Abbadi protocol is introduced and evaluated. This new algorithm is shown to exhibit lower delay at the expense of a negligible increase in the number of messages exchanged. Numerical results derived from the model are discussed	algorithm;mutual exclusion;numerical method	Daniel A. Menascé;Yelena Yesha;Konstantinos Kalpakis	1994	IEEE Trans. Software Eng.	10.1109/32.368122	distributed algorithm;real-time computing;computer science;theoretical computer science;distributed computing;upper and lower bounds	Metrics	-21.26019661849824	46.65750745470873	162028
0993570abec71727ef363d2e92433fb66d3b14ed	an architecture of data processing using deluge computing in internet of things	granular computing;adaptive coding and modulation;modulation computer architecture data processing servers cloud computing;data processing;internet of things;computer architecture;servers;granular computing cloud computing data handling;data reduction;data handling;data reduction data processing architecture deluge computing internet of things heterogenous network connection network technology cloud computing sea computing adaptive coding adaptive modulation granular computing;heterogeneous network;cloud computing;modulation	Internet of Things (IoT) is an upcoming network technology well studied by many researchers. Mass applications and terminal nodes with heterogenous network connection make the tremendous scale of data process extremely difficult. A combined architecture with Cloud Computing and Sea Computing is proposed. Deluge Computing, a new concept, using Adaptive Coding and Modulation and Granular Computing aims to release the pressure of data process when data deluge bursts. Granular Computing is emphasized in this architecture for data reduction.	adaptive coding;cloud computing;granular computing;information explosion;internet of things;modulation	Yang Liu;Zhikui Chen;Haozhe Wang;Xiaoning Lv	2011	2011 International Conference on Internet of Things and 4th International Conference on Cyber, Physical and Social Computing	10.1109/iThings/CPSCom.2011.17	fabric computing;data reduction;real-time computing;heterogeneous network;data processing;cloud computing;granular computing;computer science;theoretical computer science;operating system;end-user computing;group method of data handling;data-intensive computing;distributed computing;utility computing;internet of things;grid computing;server;unconventional computing;computer network;autonomic computing;modulation	HPC	-27.759742957444875	55.11439721081318	162100
0f2f9eb2b8c01ef54ba5dfcba7eed7cf5a60c3a5	a teragrid-enabled distributed discrete event agent-based epidemiological simulation	linux;discrete event simulation;diseases;grid computing;medical computing;multi-agent systems;episims;midas project;agent-based model;epidemiological simulation;grid-aware version;multiple linux cluster;teragrid-enabled distributed discrete event agent	"""We discuss design issues related to the transformation of a mature Agent-Based Model (ABM) for computational epidemiology into a """"grid-aware"""" version. EpiSims is a distributed discrete event ABM that has been in production for nearly a decade. Working under a grant from the National Science Foundation and the NIH (NIGMS) funded MIDAS project, we are reengineering EpiSims to run as a single job on multiple Linux clusters on the NSF TeraGrid."""	agent-based model;code refactoring;computation;computational epidemiology;ibm notes;linux;simulation;teragrid	Douglas J. Roberts;Diglio A. Simoni	2007	2007 Winter Simulation Conference		real-time computing;computer cluster;computer science;discrete event simulation;operating system;multi-agent system;distributed computing;linux kernel;grid computing	AI	-29.043011150570614	50.58213552835429	162217
57c8c4100126d8b8481fe569b6db3747cbfb317f	modular synchronization in distributed, multiversion databases: version control and concurrency control	read only transactions;phase locking;extensibility;distributed system;protocols;base donnee repartie;base donnee;systeme reparti;distributed database;concurrent computing;database;two phase locking;base repartida dato;base dato;multiversion databases;indexing terms;synchronisation;distributed databases concurrent computing concurrency control transaction databases access protocols computer science spatial databases distributed control database systems system recovery;sistema repartido;system recovery;transaction databases;distributed environment;spatial databases;database systems;concurrency control;distributed databases;access protocols;proving the correctness;multiversion protocols;read only transactions distributed databases modular synchronisation multiversion databases version control concurrency control modularity extensibility multiversion protocols proving the correctness two phase locking time stamp;modularity;controle concurrence;control concurrencia;computer science;read only transaction;version control;base donnee multiversion;synchronisation concurrency control configuration management distributed databases protocols;distributed control;configuration management;time stamp;transaction exclusive lecture;multiversion database;modular synchronisation	A version control mechanism is proposed that enhances the modularity and extensibility of multiversion concurrency control algorithms. We decouple the multiversion algorithms into two components: version control and concurrency control. This permits modular development of multiversion protocols, and simplifies the task of proving the correctness of these protocols. A set of procedures for version control is described that defines the interface to the version control component. We show that the same interface can be used by the database actions of both two-phase locking and time-stamp concurrency control protocols to access multiversion data. An interesting feature of our framework is that the execution of read-only transactions becomes completely independent of the underlying concurrency control implementation. Unlike other multiversion algorithms, read-only transactions in this scheme do not modify any version related information, and therefore, do not interfere with the execution of read-write transactions. Finally, the extension of the multiversion algorithms to a distributed environment becomes very simple.	algorithm;basic stamp;correctness (computer science);coupling (computer programming);database;extensibility;lock (computer science);modularity (networks);multiversion concurrency control;overhead (computing);read-only memory;read-write memory;transaction processing;two-phase locking;version control	Divyakant Agrawal;Soumitra Sengupta	1993	IEEE Trans. Knowl. Data Eng.	10.1109/69.204097	timestamp;communications protocol;synchronization;optimistic concurrency control;real-time computing;isolation;index term;extensibility;computer science;revision control;two-phase locking;concurrency control;database;modularity;distributed computing;configuration management;multiversion concurrency control;non-lock concurrency control;distributed database;acid;distributed computing environment;snapshot isolation;distributed concurrency control	DB	-24.163523428296656	46.96086662898817	162340
442a36f74a92579ce86aed3ceb5da64ef3b9b301	monitoring and predicting hardware failures in hpc clusters with ftb-ipmi	libraries;clusters;external components;software;resource utilization;ftb hpc clusters;cloud computing systems;high performance computing clusters;pattern clustering;multi threading;fault prediction engine;user interfaces computerised monitoring decision making fault diagnosis fault tolerant computing middleware multi threading online front ends pattern clustering resource allocation;infrastructure monitors;resource allocation;failing nodes;distributed fault monitoring;structured software events;publish subscribe framework;temperature sensors;intelligent platform management interface;ipmi;fault tolerance backplane;libraries monitoring temperature sensors fault tolerant systems fault tolerance software hardware;distributed failure information;reactive mechanisms;ftb ipmi;light weight multithreaded service;hardware software failure;online front ends;hardware failure prediction;ipmi sensor information;fault tolerant computing;decision making engines;cpu temperature;healthy spare nodes;monitoring;fault tolerant systems;hardware stack;coordinated fault propogation;proactive mechanisms;system logs;fault detection;fault tolerance;proactive fault tolerance mechanisms;raw hardware events;middleware	Fault-detection and prediction in HPC clusters and Cloud-computing systems are increasingly challenging issues. Several system middleware such as job schedulers and MPI implementations provide support for both reactive and proactive mechanisms to tolerate faults. These techniques rely on external components such as system logs and infrastructure monitors to provide information about hardware/software failure either through detection, or as a prediction. However, these middleware work in isolation, without disseminating the knowledge of faults encountered. In this context, we propose a light-weight multi-threaded service, namely FTB-IPMI, which provides distributed fault-monitoring using the Intelligent Platform Management Interface (IPMI) and coordinated propagation of fault information using the Fault-Tolerance Backplane (FTB). In essence, it serves as a middleman between system hardware and the software stack by translating raw hardware events to structured software events and delivering it to any interested component using a publish-subscribe framework. Fault-predictors and other decision-making engines that rely on distributed failure information can benefit from FTB-IPMI to facilitate proactive fault-tolerance mechanisms such as preemptive job migration. We have developed a fault-prediction engine within MVAPICH2, an RDMA-based MPI implementation, to demonstrate this capability. Failure predictions made by this engine are used to trigger migration of processes from failing nodes to healthy spare nodes, thereby providing resilience to the MPI application. Experimental evaluation clearly indicates that a single instance of FTB-IPMI can scale to several hundreds of nodes with a remarkably low resource-utilization footprint. A deployment of FTB-IPMI that services a cluster with 128 compute-nodes, sweeps the entire cluster and collects IPMI sensor information on CPU temperature, system voltages and fan speeds in about 0.75 seconds. The average CPU utilization of this service running on a single node is 0.35%.	backplane;central processing unit;cloud computing;failure;fault tolerance;ieee-488;intelligent platform management interface;job scheduler;l (complexity);logic programming;message passing interface;middleware;publish–subscribe pattern;remote direct memory access;scalability;single-instance storage;software bug;software deployment;software propagation;supercomputer;thread (computing)	Raghunath Rajachandrasekar;Xavier Besseron;Dhabaleswar K. Panda	2012	2012 IEEE 26th International Parallel and Distributed Processing Symposium Workshops & PhD Forum	10.1109/IPDPSW.2012.139	embedded system;intelligent platform management interface;parallel computing;real-time computing;computer science;operating system;distributed computing;computer network	HPC	-23.964596792718304	55.75283712380789	162377
1276649457aefdf398670587e348d64de912e518	an efficient merging algorithm for recovery and garbage collection in incremental checkpointing.	garbage collection			Junyoung Heo;Sangho Yi;Jiman Hong;Yookun Cho;Jongmoo Choi	2004			parallel computing;real-time computing;database;mark-compact algorithm	DB	-20.10822159630502	48.574817762744416	162452
fb04aa89ac31125a2db4911d3ca7fb7b4da1c136	on a data-driven environment for multiphysics applications	information technology;distributed computing;distributed sensors;software architecture;multidisciplinary problem solving environment;interactive system;multiphysics;modelling and simulation;distributed sensor network;heterogeneous networks;structural design;problem solving environment;data driven;heterogeneous network	The combination of the recent advances in computational and distributed sensor network technologies provide a unique opportunity for focused efforts on low uncertainty modelling and simulation of multiphysics systems. Responding to this opportunity, we present in this paper the architecture of a data-driven environment for multiphysics applications (DDEMA) as a multidisciplinary problem solving environment (MPSE). The goal of this environment is to support the automated identification and efficient prediction of the behavioral response of multiphysics continuous interacting systems. The design takes into consideration heterogeneous and distributed information technologies, coupled multiphysics sciences, and sensor-supported data-driveness to steer adaptive modelling and simulation of the underlying systemic behavior. The design objectives and proposed software architecture are described in the context of two multidisciplinary applications related to material-structure design of supersonic platforms and fire/material/environment interaction monitoring, assessment and management. These applications of DDEMA will be distributed over a highly heterogeneous networks that extend from light and ubiquitous resources (thin portable devices/clients) to heavy GRID-based computational infrastructure.	decision support system;encapsulation (networking);grid computing;interaction;mobile device;multiphysics;nonlinear system;problem solving environment;requirement;simulation;software architecture	John Michopoulos;Panagiota Tsompanopoulou;Elias N. Houstis;Charbel Farhat;Michel Lesoinne;John R. Rice;Anupam Joshi	2005	Future Generation Comp. Syst.	10.1016/j.future.2003.12.023	computational science;real-time computing;simulation;heterogeneous network;computer science;distributed computing;information technology;computer network	HPC	-31.840239407903127	47.72698465019855	162517
1906f1561076ce8c25b5ffe540f45312720fb94a	state maintenance and its impact on the performability of multi-tiered internet services	electronic commerce;software maintenance;electronic commerce internet software maintenance;service availability multitiered internet services performance evaluation availability evaluation soft state maintenance online book store auction service soft state latency service latency quantification methodology availability metrics performability metrics;performance metric;internet;web and internet services availability databases books performance evaluation computer science delay scalability costs cryptography;internet services;service design	In this paper, we evaluate the performance, availability, and combined performability of four soft state maintenance strategies in two multitier Internet services, an online book store and an auction service. To take soft state and service latency into account, we propose an extension of our previous quantification methodology, and novel availability and performability metrics. Our results demonstrate that storing the soft state in a database can achieve better performability than storing it in main memory, even when the state is efficiently replicated. Strategies that offload the handling of soft state from the database increase the load on other tiers and, consequently, increase the impact of faults in these tiers on service availability. Based on these results, we conclude that service designers need to provision the cluster and balance the load with availability and cost, as well as performance, in mind.	computer data storage;experiment;fault injection;load balancing (computing);multitier architecture;online book;provisioning;server (computing);service pack;soft state;unavailability;web service	Gustavo Machado Campagnani Gama;Kiran Nagaraja;Ricardo Bianchini;Richard P. Martin;Wagner Meira;Thu D. Nguyen	2004	Proceedings of the 23rd IEEE International Symposium on Reliable Distributed Systems, 2004.	10.1109/RELDIS.2004.1353015	e-commerce;reliability engineering;the internet;computer science;operating system;service design;database;distributed computing;software maintenance;computer security;computer network	OS	-25.66072365233009	58.920480812542905	162804
2f0062f38e8b9016b18c2dc1af40682e432c7536	an adaptive data-shipping architecture for client caching data management systems	resource utilization;client server architecture;cache consistency;caching;odbmss;data shipping;buffer management;recovery;client server dbmss;data distribution;client server;file system;pointer swizzling;database management system;mobile data management;data management system;data transfer	Data-shipping is an important form of data distribution architecture where data objects are retrieved from the server, and are cached and operated upon at the client nodes. This architecture reduces network latency and increases resource utilization at the client. Object database management systems (ODBMS), file-systems, mobile data management systems, multi-tiered Web-server systems and hybrid query-shipping/data-shipping architectures all use some variant of the data-shipping. Despite a decade of research, there is still a lack of consensus amongst the proponents of ODBMSs as to the type of data shipping architectures and algorithms that should be used. The absence of both robust (with respect to performance) algorithms, and a comprehensive performance study comparing the competing algorithms are the key reasons for this lack of agreement. In this paper we address both of these problems. We first present an adaptive data-shipping architecture which utilizes adaptive data transfer, cache consistency and recovery algorithms to improve the robustness (with respect to performance) of a data-shipping ODBMS. We then present a comprehensive performance study which evaluates the competing client-server architectures and algorithms. The study verifies the robustness of the new adaptive data-shipping architecture, provides new insights into the performance of the different competing algorithms, and helps to overturn some existing notions about some of the algorithms.	algorithm;cache (computing);cache coherence;client–server model;database;management system;server (computing);web server	Kaladhar Voruganti;M. Tamer Özsu;Ronald C. Unrau	2004	Distributed and Parallel Databases	10.1023/B:DAPD.0000013069.97679.62	real-time computing;computer science;operating system;database;distributed computing;client–server model	DB	-21.3671305187862	50.36914255610012	162905
6b957489670c227918465251bc63420a50cafcea	performance interference of memory thrashing in virtualized cloud environments: a study of consolidated n-tier applications	virtualization;interference;virtual machine monitors;servers;time factors;cloud computing;hardware	Modern datacenters employ server virtualization and consolidation to reduce the cost of operation and to maximize profit. However, interference among consolidated virtual machines (VMs) has barred mission-critical applications due to unpredictable performance. Through extensive measurements of RUBBoS n-tier benchmark, we found a major source of performance unpredictability: the memory thrashing caused by VM consolidation can reduce the system throughput by 46% although memory was not over-committed. On a physical host with 4 consolidated VMs, we observed two distinct operational modes during a typical RUBBoS benchmark experiment. Over the first half of run-time session we found frequent CPU IOwait causing very long response time requests even though the system is under read-only CPU intensive workload, however, the latter half showed no such CPU abnormalities (IOwait). Using ElbaLens - a lightweight tracing tool, we conducted fine-grain analyses at time granularities as short as 50ms and found that the abnormal IOwait is caused by transient memory thrashing among consolidated VMs. The abnormal IOwait induces queue overflows that propagate through the entire n-tier system, resulting in very long response time requests due to frequent TCP retransmissions. We provide three practical techniques such as VM migration, memory reallocation, soft resource reallocation and show that they can mitigate the effects of performance interference among consolidated VMs.	benchmark (computing);central processing unit;interference (communication);memory overcommitment;mission critical;multitier architecture;read-only memory;response time (technology);semiconductor consolidation;statistical interference;thrashing (computer science);throughput;virtual machine;virtual private server;z/vm	Junhee Park;Qingyang Wang;Jack Li;Chien-An Lai;Tao Zhu;Calton Pu	2016	2016 IEEE 9th International Conference on Cloud Computing (CLOUD)	10.1109/CLOUD.2016.0045	embedded system;real-time computing;virtualization;cloud computing;computer science;operating system;interference;server	HPC	-21.838583699431492	55.551130621434346	163141
6fb96523d467a0c84af1980ab3abe268dfb32a13	mission assurance: beyond secure processing		The processor of a drone runs essential functions of sensing, communications, coordination, and control. This is the conventional view. But in today's cyber environment, the processor must also provide security to assure mission completion. We have been developing a secure processing architecture for mission assurance. A study on state-of-the-art secure processing technologies has revealed that no one-size-fits-all solution can fully meet our requirements. In fact, we have concluded that the provision of a secure processor as a mission assurance foundation must be holistic and should be approached from a systems perspective. We have thus applied a systems analysis approach to create a secure base for the system. This paper describes our journey of adapting and synergizing various secure processing technologies into a baseline asymmetric multicore processing architecture. We will also describe a functional and security co-design environment, created to customize and optimize the architecture in a design space consisting of hardware, software, performance, and assurance.	baseline (configuration management);fits;holism;mission assurance;multi-core processor;requirement;unmanned aerial vehicle	Michael Vai;David Whelihan;Jacob Leemaster;Haley Whitman;Willahelm Wan;Yunsi Fei;Roger I. Khazan;Ilia A. Lebedev;Kyle Hogan;Srinivas Devadas	2018	2018 IEEE International Conference on Software Quality, Reliability and Security Companion (QRS-C)	10.1109/QRS-C.2018.00104	mission assurance;computer engineering;systems analysis;architecture;software;multi-core processor;computer science	Arch	-31.574308216255524	58.82128938675837	163184
4cb6aefafd22f1ca84b966718d57ff08d93a5ce2	facility information management on hbase: large-scale storage for time-series data	sensors;time series access protocols cloud computing distributed databases facilities information management resource allocation sensor fusion storage management;hadoop internet of things time series data hbase;internet of things;servers;logic gates;distributed data storage facility information management large scale storage time series data hvac light control systems electric power meters status information cloud platform distributed application layer storage fiap storage peta petabyte scale storage facility information access protocol data management hbase row key design scalable data retrieval load balancing;hbase;distributed databases;time series data;scalability;hadoop;memory;sensors servers distributed databases memory logic gates educational institutions scalability	A very large number of sensors on facilities such as HVAC, light control systems and electric power meters, periodically submit their status information to Cloud platforms these days. As the amount of data can easily get petabyte scale, we must consider the use of distributed application layer storage for managing such facility information, which is often formatted on time-series data. This paper describes FIAP Storage Peta, petabyte scale storage for facility information access protocol (FIAP), proposing the architecture and the scheme of such data management on HBase. In this work, we have identified three requirements to the design of HBase row keys for implementing this storage using HBase. Though, we have not finished petabyte scale experiments, our preliminary evaluation results have shown good performance for managing large scale facility information. It has achieved scalable data retrieval on the data of 10 million sensors with properly balancing loads on distributed data storages.	apache hbase;cloud computing;control system;data retrieval;distributed computing;experiment;information access;information management;norm (social);petabyte;requirement;scalability;sensor;sequential access;time series	Hideya Ochiai;Hiroyuki Ikegami;Yuuichi Teranishi;Hiroshi Esaki	2014	2014 IEEE 38th International Computer Software and Applications Conference Workshops	10.1109/COMPSACW.2014.54	real-time computing;scalability;logic gate;computer science;sensor;operating system;time series;data mining;database;memory;distributed database;computer security;internet of things;server	HPC	-20.070259619019776	53.245571438338	163220
1875357c4bfb659722d960cdfa4ec94aeaa6623b	pride: a data abstraction layer for large-scale 2-tier sensor networks	predictive replication in distributed embedded system pride data abstraction layer large scale 2 tier sensor network data storage architecture collaborative real time application data stream management model driven full replication scheme adaptive data quality control mechanism wireless connectivity;search and rescue;collaborative real time application;large scale systems scalability memory collaboration buildings fires programmable control adaptive control quality control testing;data stream;temperature sensors;kalman filters;predictive replication in distributed embedded system;data quality control;wireless connectivity;sensor network;model driven full replication scheme;data storage architecture;adaptive data quality control mechanism;embedded systems;data stream management;large scale;data storage;computational modeling;data abstraction;mathematical model;pride data abstraction layer;predictive models;peer to peer computing;wireless sensor networks embedded systems quality control;quality control;real time application;wireless sensor networks;large scale 2 tier sensor network;data models	It is a challenging task to provide timely access to global data from sensors in large-scale sensor network applications. Current data storage architectures for sensor networks have to make trade-offs between timeliness and scalability. PRIDE is a data abstraction layer for 2-tier sensor networks, which enables timely access to global data from the sensor tier to all participating nodes in the upper storage tier. The design of PRIDE is heavily influenced by collaborative real-time applications such as search-and-rescue tasks for high-rise building fires, in which multiple devices have to collect and manage data streams from massive sensors in cooperation. PRIDE achieves scalability, timeliness, and flexibility simultaneously for such applications by combining a model-driven full replication scheme and adaptive data quality control mechanism in the storage-tier. We show the viability of the proposed solution by implementing and evaluating it on a large-scale 2-tier sensor network testbed. The experiment results show that the model-driven replication provides the benefit of full replication in a scalable and controlled manner.	abstraction (software engineering);abstraction layer;client–server model;computer data storage;data quality;model-driven architecture;model-driven integration;multitier architecture;overhead (computing);real-time clock;scalability;sensor;testbed	Woochul Kang;Sang Hyuk Son;John A. Stankovic	2009	2009 6th Annual IEEE Communications Society Conference on Sensor, Mesh and Ad Hoc Communications and Networks	10.1109/SAHCN.2009.5168963	embedded system;real-time computing;wireless sensor network;computer science;operating system;database;distributed computing;computer network	Mobile	-25.570488107717196	52.67216712400136	163241
45d3359493c028b6cfba0e60850837780e0672e3	optimal availability quorum systems: theory and practice	distributed system;replication;quorum system;theory and practice;fault tolerant;efficient algorithm;quorum systems;distributed computing;optimization method;systeme informatique tolerant panne;metodo optimizacion;distributed computer systems;fault tolerant computer systems;mutual exclusion;fault tolerance;methode optimisation;distributed access control;systeme informatique reparti;failure probability	Quorum systems serve as a basic tool providing a uniform and reliable way to achieve coordination in a distributed system. They are useful for distributed and replicated databases, name servers, mutual exclusion, and distributed access control and signatures. The un-availability of a quorum system is the probability of the event that no live quorum exists in the system. When such an event occurs the service is completely halted. The un-availability is widely accepted as the measure by which quorum systems are evaluated. In this paper we characterize the optimal availability quorum system in the general case, when the failure probabilities may take any value in the range 0 < pi < 1. Then we deal with the practical scenario in which the failure probabilities are unknown, but can be estimated. We give a robust and efficient algorithm that calculates a near optimal quorum system based on the estimated failure probabilities. @ 1998 Elsevier Science B.V.	access control;algorithm;antivirus software;database;distributed computing;mutual exclusion;quorum (distributed computing);systems theory	Yair Amir;Avishai Wool	1998	Inf. Process. Lett.	10.1016/S0020-0190(98)00017-9	fault tolerance;real-time computing;computer science;distributed computing;computer security	DB	-21.67251913564259	46.490452891306205	163267
f5ec934b4e77899c989e8f067938248b3854774e	performance of data mining, media, and financial applications under private cloud conditions		This paper contributes to a performance analysis of real-world workloads under private cloud conditions. We selected six benchmarks from PARSEC related to three mainstream application domains (financial, data mining, and media processing). Our goal was to evaluate these application domains in different cloud instances and deployment environments, concerning container or kernel-based instances and using dedicated or shared machine resources. Experiments have shown that performance varies according to the application characteristics, virtualization technology, and cloud environment. Results highlighted that financial, data mining, and media processing applications running in the LXC instances tend to outperform KVM when there is a dedicated machine resource environment. However, when two instances are sharing the same machine resources, these applications tend to achieve better performance in the KVM instances. Finally, financial applications achieved better performance in the cloud than media and data mining.	cloud computing;data mining;docker;experiment;input/output;lxc;parsec;profiling (computer programming);software deployment;x86 virtualization	Dalvan Griebler;Adriano Vogel;Carlos A. F. Maron;Anderson M. Maliszewski;Claudio Schepke;Luiz Gustavo Fernandes	2018	2018 IEEE Symposium on Computers and Communications (ISCC)	10.1109/ISCC.2018.8538759	software deployment;virtualization;distributed computing;data mining;cloud computing;finance;computer science;benchmark (computing)	OS	-22.37643885867482	59.82807916751806	163324
60ec9c07d95a645d26bee9040ebabca9bffb5fa4	peer coordination through distributed triggers	swinburne;distributed triggers;peer coordination;peer data management	This is a demonstration of data coordination in a peer data management system through the employment of distributed triggers. The latter express in a declarative manner individual security and consistency requirements of peers, that cannot be ensured by default in the P2P environment. Peers achieve to handle in a transparent way data changes that come from local and remote actions and events. The distributed triggers are implemented as an extension of the active functionality of a centralized commercial DBMS. The language and execution semantics of distributed triggers are integrated in the kernel of the DBMS such that the latter handles transparently and simultaneously both centralized and distributed triggers. Moreover, the management of distributed triggers is associated with a set of peer acquaintance and termination protocols which are incorporated in the centralized DBMS.	centralized computing;data hub;database trigger;peer-to-peer;programming language;requirement	Verena Kantere;Maher Manoubi;Iluju Kiringa;Timos K. Sellis;John Mylopoulos	2010	PVLDB	10.14778/1920841.1921038	real-time computing;computer science;database;distributed computing	DB	-26.89824954595345	46.9666245632518	163457
5ffb72fdeb8bb01e818149e79fcb91d1baf228de	loose synchronization for large-scale networked systems	large scale;networked systems	Traditionally, synchronization barriers ensure that no co operating process advances beyond a specified point until all processes have reached that point. In heterogeneous large-scale distributed computing environments, with unreliable network links and machines that may become overloaded and unresponsive, traditional barrier semantics are too strict to be effective for a range of emerging applications. In this paper, we explore several relaxations, and introduce a partial barrier, a synchronization primitive designed to enhance liveness in loosely coupled networked systems. Partial barriers are robust to variable network conditions; rather than attempting to hide the asynchrony inherent to wide-area settings, they enable appropriate application-level responses. We evaluat e the improved performance of partial barriers by integrating them into three publicly available distributed applications running across PlanetLab. Further, we show how partial barriers simplify a re-implementation of MapReduce that targets wide-area environments.	as-easy-as;asynchrony (computer programming);distributed computing;linear programming relaxation;liveness;loose coupling;mapreduce;multimodal interaction;parallel algorithm;planetlab;synchronization (computer science)	Jeannie R. Albrecht;Christopher Tuttle;Alex C. Snoeren;Amin Vahdat	2006			real-time computing;computer science;theoretical computer science;operating system;distributed computing	OS	-23.72515877224191	53.20244418288888	163530
62ff7e453fafa3f4fb4b44dcac018cf75a69b3c8	storage workload isolation via tier warming: how models can help.		Storage systems are often deployed in a tiered form to enable high performance and availability. These tiers utilize all possible volatile and non-volatile storage technologies, including DRAM, SSD, and HDD. The tradeoffs among their cost, features, and capabilities can make their effective integration into a single storage entity complex. Here, we propose an autonomic technique that learns user traffic patterns in a storage system over long time-scales to optimize user performance but also volume of completed system work. Our purpose is to multiplex as best as possible user workload with storage system features (e.g., voluminous internal system work) such that the latter is not starved but rather completed with minimal impact on user performance. Key to achieving the above is to use an autonomic learning engine to predict when the user workload intensity increases/decreases and then proactively stop/start bulky internal system work. Being proactive allows the system to effectively bring into the fast tier the active user working set just-in-time and right before it is needed most, i.e., when user traffic suddenly peaks. We illustrate the effectiveness of this mechanism by using both trace driven simulations from production systems as well experiments on a real testbed.	allocate-on-flush;computer data storage;hierarchical storage management;memory hierarchy;multitier architecture;scheduling (computing);working set	Ji Xue;Feng Yan;Alma Riska;Evgenia Smirni	2014			embedded system;real-time computing;simulation	OS	-22.607716430289113	59.293240412552976	163860
53434eafb6502691f9639df630752b89fd2fe715	thermo-fluids provisioning of a high performance high density data center	provisioning of data centers;heat;power density;high density;cfd analysis of data center;transient analysis;exergy;data center;room air;cooling of data center;transient analysis of data center;smart cooling;computer room cooling;high performance;load migration;high power;cooling	Consolidation and dense aggregation of slim compute, storage and networking hardware has resulted in high power density data centers. The high power density resulting from current and future generations of servers necessitates detailed thermo-fluids analysis to provision the cooling resources in a given data center for reliable operation. The analysis must also predict the impact on the thermo-fluid distribution due to changes in hardware configuration and building infrastructure such as a sudden failure in data center cooling resources. The objective of the analysis is to assure availability of adequate cooling resources to match the heat load, which is typically non-uniformly distributed and characterized by high-localized power density. This study presents an analysis of an example modern data center with a view of the magnitude of temperature variation and impact of a failure. Initially, static provisioning for a given distribution of heat loads and cooling resources is achieved to produce a reference state. A perturbation in reference state is introduced to simulate a very plausible scenario—failure of a computer room air conditioning (CRAC) unit. The transient model shows the “redlining” of inlet temperature of systems in the area that is most influenced by the failed CRAC. In this example high-density data center, the time to reach unacceptable inlet temperature is less than 80 seconds based on an example temperature set point limit of 40°C (most of today's servers would require an inlet temperature below 35°C to operate). An effective approach to resolve this issue, if there is adequate capacity, is to migrate the compute workload to other available systems within the data center to reduce the inlet temperature to the servers to an acceptable level.	computer cooling;data center;networking hardware;provisioning;semiconductor consolidation;simulation	Abdlmonem H. Beitelmal;Chandrakant D. Patel	2005	Distributed and Parallel Databases	10.1007/s10619-005-0413-0	data center;real-time computing;room air distribution;simulation;operating system;power density;exergy	HPC	-23.217490801713826	59.50008077787996	164033
249df786dd340486aafc845217779bb5683ceac7	availability and network-aware mapreduce task scheduling over the internet		MapReduce offers an ease-of-use programming paradigm for processing large datasets. In our previous work, we have designed a MapReduce framework called BitDew-MapReduce for desktop grid and volunteer computing environment, that allows nonexpert users to run data-intensive MapReduce jobs on top of volunteer resources over the Internet. However, network distance and resource availability have great impact on MapReduce applications running over the Internet. To address this, an availability and network-aware MapReduce framework over the Internet is proposed. Simulation results show that the MapReduce job response time could be decreased by 27.15%, thanks to Naive Bayes Classifier-based availability prediction and landmark-based network estimation.	academy;data-intensive computing;desktop computer;internet;mapreduce;naive bayes classifier;newton's method;performance evaluation;programming paradigm;response time (technology);seti@home;schedule (project management);scheduling (computing);simulation;tracing (software);volunteer computing	Bing Tang;Qi Xie;Haiwu He;Gilles Fedak	2015		10.1007/978-3-319-27119-4_15	fair-share scheduling;parallel computing;real-time computing;distributed computing	Metrics	-19.96875753170558	59.36981357825863	164070
2ec6bdb34c5ddd1b9528eea1aa377e1f7c8703fc	remora: a resource monitoring tool for everyone	high performance computing;data archival;best practices;parallelization;data transfer	Knowing about the requirements of HPC applications is a common question that users of high performance systems ask often. However, answering this question requires the collaboration of administrators and sometimes the answer does not contain the amount of detail that users demand. This work introduces a new user space resource monitoring tool, REMORA. REMORA stands for REsource MOnitoring for Remote Applications, and provides a simple interface to gather important system utilization data while running on HPC systems. REMORA is designed to provide a brief text report and post-processing tools to analyze the very detailed records taken during an application run. Users can configure the tool to achieve the amount of detail that they want and perform the analysis of the results at any point in time. REMORA helps users achieving a better understanding of their applications by providing a high level profile of their executions and users can take advantage of that information to improve their codes.	central processing unit;code;expect;graphics processing unit;high-level programming language;input/output;overhead (computing);plug-in (computing);requirement;user space;video post-processing	Carlos Rosales;Antonio Gómez-Iglesias;Andrew Predoehl	2015		10.1145/2834996.2834999	computer science;data mining;database;world wide web	HPC	-28.450594393994603	52.624246228926474	164132
1b336783e729b42a3375f63425d5c6bfb21e624c	techniques for handling scale and distribution in virtual worlds	conference paper;computer science;virtual worlds	Lack of bandwidth and network latency are known to be major impediments to achieving realism in distributed virtual world (vw) applications with a large number of, potentially geographically dispersed, entities. This paper describes a combination of techniques that we are using to overcome these twin problems. The techniques described are intended to reduce both the volume and frequency of communication between the entities that make up the virtual world and include the use of anonymous event-based communication with notify constraints, scoping of event propagation with zones, and use of predictive approaches to replica management. Each of these techniques is described in turn.	entity;scope (computer science);software propagation;virtual world	Karl O'Connell;Tom Dinneen;Steven Collins;Brendan Tangney;Neville Harris;Vinny Cahill	1996		10.1145/504450.504454	simulation;computer science;theoretical computer science;distributed computing	DB	-25.391763105134384	52.02819613925944	164157
a16b5f0eb75f52cbe807824c583a8a3d98938fc0	load balancing in distributed systems with multiple classes and site constraints	distributed system;load balance		distributed computing;load balancing (computing)	Edmundo de Souza e Silva;Mario Gerla	1984			real-time computing;computer science;load balancing;distributed computing	DB	-29.037066915014854	47.021326385508765	164425
259a1204326a5ee325a55937075d685d86220d36	virtualized environments for the harness high performance computing workbench	computers;scientific application;xml data structures data visualisation natural sciences computing;general and miscellaneous mathematics computing and information science;performance;data processing;data visualisation;high performance computing application software application virtualization hardware virtual prototyping system software computer science xml system testing computer architecture;file system;data structures;high performance computer;xml;xml format high performance computing scientific application development scientific application deployment chroot visualization approach unix type systems virtualized file system structure virtualized shell environment variables virtualized environment configuration descriptions extensible markup language;virtual environment;natural sciences computing;type system	This paper describes recent accomplishments in providing a virtualized environment concept and prototype for scientific application development and deployment as part of the Harness High-Performance Computing (HPC) Workbench research effort. The presented work focuses on tools and mechanisms that simplify scientific application development and deployment tasks, such that only minimal adaptation is needed when moving from one HPC system to another or after HPC system upgrades. The overall technical approach focuses on the concept of adapting the HPC system environment to the actual needs of individual scientific applications instead of the traditional scheme of adapting scientific applications to individual HPC system environment properties. The presented prototype implementation is based on the mature and lightweight chroot visualization approach for Unix-type systems with a focus on virtualized file system structure and virtualized shell environment variables utilizing virtualized environment configuration descriptions in Extensible Markup Language (XML) format. The presented work can be easily extended to other visualization technologies, such as system-level visualization solutions using hypervisors.	binary file;capability maturity model;chroot;computational science;continuation;directory (computing);eclipse;environment variable;file system api;hypervisor;library (computing);markup language;picture transfer protocol;prototype;runtime system;software deployment;supercomputer;system administrator;test harness;type system;unix;usability;workbench;wrapping (graphics);x86 virtualization;xml	Björn Könning;Christian Engelmann;Stephen L. Scott;Al Geist	2008	16th Euromicro Conference on Parallel, Distributed and Network-Based Processing (PDP 2008)	10.1109/PDP.2008.14	parallel computing;xml;type system;data processing;performance;computer science;virtual machine;operating system;database;distributed computing;programming language;world wide web	HPC	-29.404220918217657	51.30682043183474	164533
8bd0b4aa261a163848751786ecc982bc026834a8	scalable view expansion in a peer mediator system	peer mediator system;file servers;data models query processing distributed databases merging;query fragments;query processing;application software;scalable view expansion;web and internet services;costing;multidatabase;full view expansion;query compilation scalable view expansion peer mediator system data sources query fragments distributed databases transparent boxes merging full view expansion multidatabase query execution;query compilation;computer architecture;file servers data models computer architecture application software web and internet services mediation large scale integration query processing web server costing;transparent boxes;large scale integration;mediation;data sources;distributed databases;merging;query execution;datavetenskap datalogi;web server;computer science;data models	To integrate many data sources we use a peer mediator-framework where views defined in the peers are logically composed in terms of each other A common approach to execute queries over mediators is to treat views in data sources as 'black boxes'. The mediators locally decompose queries into query fragments and submit them to the data sources for processing. Another approach, used in distributed DBMSs, is to treat the views as 'transparent boxes' by importing and fully expanding all views and merge them with the query. The black box approach often leads to inefficient query plans. However, in a peer mediator framework full view expansion (VE) leads to prohibitively long query compilation times when many peers are involved. It also limits peer autonomy since peers must reveal their view definitions. We investigate in a peer mediator framework the tradeoffs between none, partial, and full VE in two different distributed view composition scenarios. We show that it is often favorable with respect to query execution and sometimes even with respect to query compilation time to expand those views having common hidden peer subviews. However, in other cases it is better to use the 'black box' approach, in particular when peer autonomy prohibits view importation. Based on this, a hybrid strategy for VE in peer mediators is proposed.	black box;community climate system model;compiler;database;experiment;mathematical optimization;peer-to-peer;run time (program lifecycle phase);sql;scalability;unified extensible firmware interface	Timour Katchaounov;Vanja Josifovski;Tore Risch	2003	Eighth International Conference on Database Systems for Advanced Applications, 2003. (DASFAA 2003). Proceedings.	10.1109/DASFAA.2003.1192374	data modeling;file server;application software;computer science;data mining;database;mediation;world wide web;distributed database;web server;activity-based costing	DB	-31.833208163571964	57.04630419756561	164752
0f5b03174f06ff64010171420b0c3ac1c53783a5	heterogeneous multilevel transaction management with multiple subtransactions	transaction management;conflict detection;multidatabase system	Heterogeneous transaction management has been an active research field for a few years. Based on the degree of local transaction autonomy we distinguish between update autonomous, semi-autonomous, and non-autonomous multidatabase systems. In the semi-autonomous environments one problem is the inter-site correctness problem, i.e. that a mix of pre-existing local and global transactions might not enforce any unique global consistency or might violate local consistency. Independently of this, it is not possible to dynamically detect exactly those global subtransactions that have local RW conflicts, without considerably restricting at least some aspect of local autonomy. No optimal global syntactical conflict detection mechanism is practical and even a syntactically correct mechanism is difficult to achieve, without restricting autonomy. Semantically oriented multilevel transaction management scheme with multiple subtransactions per site is therefore favourable. The binary valued concept of consistency should also be replaced. Locally serializable schedules (LSR) seems to contain all reasonable RW schedules in the environment.		Jari Veijalainen	1993		10.1007/3-540-57234-1_16	real-time computing;distributed transaction;knowledge management;distributed computing	DB	-24.33547573770621	46.4512401240682	164880
de8f21cd74e64df62818521a98110d0055351540	an application-centric model for cloud management	libraries;information resources;network centric application infrastructures;random access memory;api;application management;libraries application program interfaces grid computing information resources internet;resource cloud;resource manager;resource management;utility resource providers;cloud application management;media;api application centric model network centric application infrastructures resource cloud utility resource providers libraries resource management cloud application management;computational modeling;internet;application centric model;clouds;application program interfaces;cost effectiveness;clouds media random access memory computational modeling operating systems resource management;grid computing;operating systems	The cloud model is increasingly popular as a means of creating dynamic, flexible and cost effective networkcentric application infrastructures. The model separates the applications, or application cloud, from the resources, or resource cloud, upon which the applications will be hosted. There are an increasing number of utility resource providers that aim to provide cloud infrastructure on demand to users and libraries that aim to manage owned infrastructure as a resource cloud. There is, unfortunately, no common API for cloud resources and it is unlikely that one will emerge soon given the immaturity of the area and the competing commercial interests in the domain. In this paper, we outline our commodity and application-centric approach to resource management, and describe our integration framework for cloud application management-illustrating its use in a field deployed application and a particular dynamic component within that application.	application lifecycle management;application programming interface;cloud management;content-control software;digital media;floor and ceiling functions;library (computing);metadata repository;requirement;software as a service	Terence J. Harmer;Peter Wright;Christina Cunningham;John Hawkins;Ronald H. Perrott	2010	2010 6th World Congress on Services	10.1109/SERVICES.2010.132	cloud computing security;simulation;computer science;knowledge management;cloud testing;world wide web	HPC	-30.88181096522081	56.44056212143309	165004
bc631e10de057f1ae6f65cb1b6f4baac1024e449	challenges to adopting stronger consistency at scale		There have been many recent advances in distributed systems that provide stronger semantics for geo-replicated data stores like those underlying Facebook. These research systems provide a range of consistency models and transactional abilities while demonstrating good performance and scalability on experimental workloads. At Facebook we are excited by these lines of research, but fundamental and operational challenges currently make it infeasible to incorporate these advances into deployed systems. This paper describes some of these challenges with the hope that future advances will address them.	consistency model;data store;distributed computing;scalability;transaction processing	Phillipe Ajoux;Nathan Bronson;Sanjeev Kumar;Wyatt Lloyd;Kaushik Veeraraghavan	2015			simulation;computer science;data mining;management science	OS	-25.495971629741433	54.26345555238381	165144
bbfb2e8b0fb41802dbd139642bb046127f7c77e7	towards memory management for service-oriented real-time systems	memory management;dynamic reconfiguration;service orientation;real time specification for java;software systems;garbage collection;dynamic software architecture;quality of service;service oriented architecture;real time systems	Dynamically discoverable units of software (services) are the centerpiece of service-oriented architecture (SOA). Such dynamic software architectures closely match the dynamics of businesses, and for that reason, SOA is becoming an increasingly important approach to the development of software. However, one aspect of deploying such dynamic software, that is frequently neglected, is the impact that it has on the availability of hardware resources such as CPU utilization and memory consumption. All software systems require the system load to be controlled in order to provide the service user with some level of quality-of-service. Furthermore, one type of software, which is particularly difficult to develop and would certainly benefit from the use of service-orientation, is real-time systems. Such systems, however, require resource guarantees and therefore are currently prohibited from using service-orientation in their design.  In this paper we propose solutions to the problems relating to providing memory management in service-oriented real-time systems (RT-SOA).	central processing unit;computer memory;data structure;discoverability;dynamic data;dynamization;flash memory;garbage collection (computer science);load (computing);memory management;oracle soa suite;programming paradigm;quality of service;real time java;real-time clock;real-time computing;real-time transcription;requirement;service-orientation;service-oriented architecture;service-oriented device architecture;software system	Thomas Richardson;Andy J. Wellings;Jose Ángel Dianes;Manuel Díaz	2010		10.1145/1850771.1850790	reference architecture;embedded system;verification and validation;real-time computing;software quality management;quality of service;software sizing;computer science;package development process;backporting;software design;software framework;component-based software engineering;software development;software design description;operating system;service-oriented architecture;middleware;software construction;software as a service;systems development life cycle;garbage collection;programming language;resource-oriented architecture;software deployment;software metric;software system;memory management;avionics software	Embedded	-26.981580105213066	57.2562516381485	165210
14e8fd0db0b0143a1bb5d472ae5a70d58c9ee61a	the virtual data grid: a new model and architecture for data-intensive collaboration	physics computing data models statistical databases data analysis distributed databases distributed processing astronomy computing;distributed processing;statistical databases;collaboration resource management data analysis productivity computer architecture distributed computing grid computing prototypes physics astronomy;physics computing;data analysis;astronomy computing;community engagement;high energy physics;distributed databases;distributed resource management;data management system;distributed system collaborative analysis data transformation data analysis data management thsom data communication data storage data collaboration data computation dataset analysis tool grid mechanism chimera physics astronomy data processing data model distributed database distributed processing data handling natural science computing;data grid;data models	It is increasingly common to encounter communities engaged in the collaborative analysis and transformation of large quantities of data over extended periods of time. We argue that these communities require a scalable system for managing, tracing, exploring and communicating the derivation and analysis of diverse data objects. Such a system could bring significant productivity increases facilitating discovery, understanding, assessment, and sharing of both data and transformation resources, as well as facilitating the productive use of distributed resources for computation, storage, and collaboration. Thus, we define a model and architecture for a virtual data grid capable of addressing this requirement. We define a broadly applicable model of a “typed dataset” as the unit of derivation tracking, and simple constructs for describing how datasets are derived from transformations and from other datasets. We also define mechanisms for integrating with, and adapting to, existing data management systems and transformation and analysis tools, as well as Grid mechanisms for distributed resource management and computation planning. We report on successful application results obtained with a prototype implementation called Chimera, involving challenging analyses of high-energy physics and	chimera;computation;job scheduler;prototype;scalability;tracing (software)	Ian T. Foster;Jens-S. Vöckler;Michael Wilde;Yong Zhao	2003		10.1109/SSDM.2003.1214945	data modeling;distributed algorithm;data model;computer science;data virtualization;data science;data warehouse;data grid;data mining;database;data analysis;distributed database;grid computing;data architecture;data mapping	HPC	-30.537937694203805	50.355742423854124	165227
ed2c5f872c238b3a458d9e7002818351c0c69b2e	adaptive mirroring of system of systems architectures	agent based;system of systems;workflow system;self healing architectures;agent architectures;adaptive mirroring;agent architecture;system architecture;service and contract workflow	In this paper, we identify an agent-based workflow system to mirror critical elements in large systems architectures. We propose the use of this light-weight agent-based service channel to complement existing systems. By pushing the adaptive smarts into critical application connectors, we believe that we can improve the ability of large systems to self-heal and scale.	agent-based model;disk mirroring;system of systems	Nathan Combs;Jeff Vagle	2002		10.1145/582128.582147	agent architecture;embedded system;real-time computing;computer science;distributed computing;workflow management system;workflow engine;workflow technology	HPC	-31.14179295088935	46.999538446312805	165612
ebf50a7c1fb37bd30ed81ab048eba68cdad46c1f	supporting distributed, interactive jupyter and rstudio in a scheduled hpc environment with spark using open ondemand		Open OnDemand supports Interactive HPC web applications enabling the interactive and distributed environments for Jupyter and RStudio running on an HPC cluster. These web applications provide a simple user-interface for building and submitting the batch job responsible for launching the interactive environment as well as proxying the connection between the user's browser and the web server running on the cluster. Support for distributive computing through a Jupyter notebook and RStudio session is provided by an Apache Spark cluster launched concurrently in standalone mode on the allocated nodes within the batch job. Alternatively, users can directly use the corresponding MPI bindings for either R or Python.  This paper describes the design of Interactive HPC web applications on an Open OnDemand deployment for launching and connecting to Jupyter notebooks and RStudio sessions as well as the architecture and software required for supporting Jupyter, RStudio, and Apache Spark on the corresponding HPC cluster. Singularity can be leveraged for packaging and portability of this architecture across HPC clusters. This paper also discusses the challenges encountered in providing interactive access to HPC resources that are in need of general solutions.	apache spark;batch processing;big data;ipython;interaction;interactivity;job stream;python;r language;rstudio;scheduling (computing);server (computing);software deployment;user interface;web application;web server	Jeremy W. Nicklas;Douglas Johnson;Shameema Oottikkal;Eric Franz;Brian McMichael;Alan Chalker;David E. Hudak	2018		10.1145/3219104.3219149	architecture;batch processing;web application;software;supercomputer;operating system;python (programming language);software deployment;software portability;computer science	HPC	-29.796967918739913	52.44773858937165	165641
086b9c724989027878a3fc5cb1ebf05669ce9f43	towards a scalable real-time cyberinfrastructure for online computer games	distributed system;online computer game;online game;real time;resource management;online interaction;multiplayer online game scalable real time cyberinfrastructure online computer game internet real time online interactive application;grid middleware;data mining;servers;internet;games;distributed computing concurrent computing application software real time systems scalability internet middleware computational modeling runtime virtual environment;cyberinfrastructure;multi player online games;real time online interactive application;real time middleware cyberinfrastructure multi player online games grid middleware scalability of distributed systems;internet computer games interactive systems;scalable real time cyberinfrastructure;middleware;scalability;multiplayer online game;computer games;interactive systems;real time middleware;computer game;real time systems;scalability of distributed systems	We propose a novel cyberinfrastructure for an emerging class of Internet-based Real-Time Online Interactive Applications (ROIA). The most challenging representative of this application class are massively multi-player online games. We present the results of the European project Edutain@Grid on the development of efficient cyberinfrastructure and scalable applications. We report experimental results demonstrating the performance and scalability of our approach.	cyberinfrastructure;pc game;real-time transcription;scalability	Sergei Gorlatch;Frank Glinka;Alexander Ploss	2009	2009 15th International Conference on Parallel and Distributed Systems	10.1109/ICPADS.2009.94	games;scalability;the internet;simulation;computer science;resource management;operating system;middleware;distributed computing;world wide web;server	Robotics	-27.92289476255491	49.68574720073181	165667
076396c6455e64446d1731aae736e5e7bfc0ff26	infn-cnaf activity in the tier-1 and grid for lhc experiments	virtual organization membership service;hierarchical storage management infn cnaf activity tier 1 grid lhc experiments high energy physics detectors large hadron collider european organization for nuclear research national institute of nuclear physics grid infrastructure world lhc computing grid data storage computing infrastructure wlcg distributed computing grid middleware virtual organization membership service authorization storm storage resource manager posix file systems general parallel file system tivoli storage manager;high energy physics instrumentation computing;storm;lhc experiments;authorisation;general parallel file system;storage management;large hadron collider;resource manager;distributed computing;grid middleware;hierarchical storage management;european organization for nuclear research;wlcg distributed computing;optical switches;data mining;storage area networks;grid;data storage;servers;storage resource manager;computing infrastructure;tivoli storage manager;high energy physics;file system;world lhc computing grid;virtual organization;parallel file system;grid infrastructure;tier 1;lhc computing grid;middleware;infn cnaf activity;authorization;national institute of nuclear physics;communities;grid computing;high energy physics detectors;posix file systems;storage management authorisation grid computing high energy physics instrumentation computing middleware;large hadron collider grid computing storms file systems detectors nuclear physics europe memory distributed computing middleware	The four High Energy Physics (HEP) detectors at the Large Hadron Collider (LHC) at the European Organization for Nuclear Research (CERN) are among the most important experiments where the National Institute of Nuclear Physics (INFN) is being actively involved. A Grid infrastructure of the World LHC Computing Grid (WLCG) has been developed by the HEP community leveraging on broader initiatives (e.g. EGEE in Europe, OSG in northen America) as a framework to exchange and maintain data storage and provide computing infrastructure for the entire LHC community. INFN-CNAF in Bologna hosts the Italian Tier-1 site, which represents the biggest italian center in the WLCG distributed computing. In the first part of this paper we will describe on the building of the Italian Tier-1 to cope with the WLCG computing requirements focusing on some peculiarities; in the second part we will analyze the INFN-CNAF contribution for the developement of the grid middleware, stressing in particular the characteristics of the Virtual Organization Membership Service (VOMS), the de facto standard for authorization on a grid, and StoRM, an implementation of the Storage Resource Manager (SRM) specifications for POSIX file systems. In particular StoRM is used at INFN-CNAF in conjunction with General Parallel File System (GPFS) and we are also testing an integration with Tivoli Storage Manager (TSM) to realize a complete Hierarchical Storage Management (HSM).	authorization;computer data storage;distributed computing;experiment;heterogeneous element processor;hierarchical storage management;ibm gpfs;ibm spectrum protect (tivoli storage manager);large hadron collider;middleware;open science grid consortium;posix;requirement;sensor;storage resource manager;voms;virtual organization;worldwide lhc computing grid	Marco Bencivenni;Marco Canaparo;F. Capannini;Luciana Carota;Michele Carpenè;Alessandro Cavalli;Andrea Ceccanti;Marco Cecchi;Daniele Cesini;Andrea Chierici;Vincenzo Ciaschini;A. Cristofori;Stefano Dal Pra;Luca dell'Agnello;D. De Girolamo;M. Donatelli;Danilo N. Dongiovanni;Enrico Fattibene	2009	2009 IEEE International Symposium on Parallel & Distributed Processing	10.1109/IPDPS.2009.5160968	parallel computing;computer science;resource management;operating system;database;distributed computing;authorization;world wide web;grid computing	HPC	-30.51769164932328	51.1346732752382	165821
c7ebfc8203e70f3127c1f2cd08172ad317f03107	architecting malleable mpi applications for priority-driven adaptive scheduling	malleability;resource management;scheduling;mpi;interactive supercomputing	Future supercomputers will need to support both traditional HPC applications and Big Data/High Performance Analysis applications seamlessly in a common environment. This motivates traditional job scheduling systems to support malleable jobs along with allocations that can dynamically change in size, in order to adapt the amount of resources to the actual current need of the different applications. It also calls for future innovative HPC applications to adapt to this environment, and provide some level of malleability for releasing underutilized resources to other tasks. In this paper, we present and compare two different methodologies to support such malleable MPI applications: 1)using checkpoint/restart and the SCR library, and 2) using dynamic data redistribution and the ULFM API and runtime. We examine their effects on application execution times as well as their impact on resource management.	application checkpointing;application programming interface;big data;dynamic data;job scheduler;job stream;message passing interface;scheduling (computing);supercomputer;transaction processing system	Pierre Lemarinier;Khalid Hasanov;Srikumar Venugopal;Kostas Katrinis	2016		10.1145/2966884.2966907	parallel computing;real-time computing;computer science;distributed computing	HPC	-21.63896045885607	58.043400506298966	165896
4b00cfbafbd046e015d1fffef482515a56d17145	distributed file system based on erasure coding for i/o-intensive applications	rozofs;video editing;erasure coding;distributed file system;iozone;mojette transform	Distributed storage systems take advantage of the network, storage and computational resources to provide a scalable infrastructure. But in such large system, failures are frequent and expected. Data replication is the common technique to provide fault-tolerance but suffers from its important storage consumption. Erasure coding is an alternative that offers the same data protection but reduces significantly the storage consumption. As it entails additional workload, current storage providers limit its use for longterm storage. We present the Mojette Transform (MT), an erasure code whose computations rely on fast XOR operations. The MT is part of RozoFS, a distributed file system that provides a global namespace relying on a cluster of storage nodes. This work is part of our ongoing effort to prove that erasure coding is not necessarily a bottleneck for intense I/O applications. In order to validate our approach, we consider a case study involving a storage cluster of RozoFS that supports video editing as an I/O intensive application.		Dimitri Pertin;Sylvain David;Pierre Évenou;Benoît Parrein;Nicolas Normand	2014		10.5220/0004960604510456	erasure code;real-time computing;computer science;theoretical computer science;operating system;distributed computing;distributed file system	OS	-22.641548024581972	52.54239459925681	165957
3024387602ab001c5dde7ef3ce367f9993fd651c	task allocation for safety and reliability in distributed systems	distributed system		distributed computing	Santhanam Srinivasan;Niraj K. Jha	1995			distributed computing;distributed algorithm;computer science	OS	-29.39903949140292	46.750294880016334	166003
c41af60926fdfa20d70d462c485ba626ba1e00b7	embedded computing systems: hands-on approach	embedded computing		embedded system;hands-on computing	Mahmoud A. Manzoul	2001			autonomic computing;grid computing;fabric computing;unconventional computing;distributed computing;computer science;end-user computing	EDA	-30.882225882093056	48.20153958753763	166035
b5c757f5b6cb0efb5eb0f213fe136d7cdc82ce2b	distributed transactions for google app engine: optimistic distributed transactions built upon local multi-version concurrency control	distributed system;cluster computing;time consistency;distributed transactions;software engineering;concurrency control;data structure	"""Massively scalable web applications encounter a fundamental tension in computing between""""performance""""and""""correctness"""": performance is often addressed by using a large and therefore distributed machine where programs are multi-threaded and interruptible, whereas correctness requires data invariants to be maintained with certainty. A solution to this problem is""""transactions""""[Gray-Reuter]. Some distributed systems such as Google App Engine [http://code.google.com/appengine/docs/] provide transaction semantics but only for functions that access one of a set of predefined local regions of the database: a""""Local Transaction""""(LT) [http://code.google.com/appengine/docs/python/datastore/transactions.html]. To address this problem we give a""""Distributed Transaction""""(DT) algorithm which provides transaction semantics for functions that operate on any set of objects distributed across the machine. Our algorithm is in an""""optimistic""""[http://en.wikipedia.org/wiki/Optimistic_concurrency_control] style. We assume Sequential [Time-]Consistency [http://en.wikipedia.org/wiki/Sequential_consistency] for Local Transactions."""	algorithm;concurrency control;correctness (computer science);distributed computing;distributed transaction;google app engine;scalability;thread (computing);web application;wiki	Daniel Shawcross Wilkerson;Simon Goldsmith;Ryan Barrett;Erick Armbrust;Robert Johnson;Alfred Fuller	2011	CoRR		parallel computing;real-time computing;data structure;database transaction;distributed transaction;computer cluster;computer science;operating system;concurrency control;database;distributed computing;virtual synchrony;programming language;serializability;acid;time consistency	DB	-22.198069055529867	48.36831453703355	166042
e1fe25393a7717da69206da64e679a42640a8ab2	parallel nfs (pnfs) block disk protection		Parallel NFS (pNFS) extends the Network File System version 4 (NFSv4)#N#to enable direct client access to file data on storage devices and#N#bypass the NFSv4 server. This can increase both performance and#N#parallelism, but it requires additional client functionality, some of#N#which depends upon the type of storage used. The pNFS specification#N#for block storage (RFC 5663) describes how clients can identify the#N#volumes used for pNFS, but this mechanism requires communication with#N#the NFSv4 server. This document updates RFC 5663 to add a mechanism#N#that enables identification of block storage devices used by pNFS file#N#systems without communicating with the server. This enables clients to#N#control access to pNFS block devices when the client initially boots,#N#as opposed to waiting until the client can communicate with the NFSv4#N#server. [STANDARDS-TRACK]		David L. Black;Jason Glasgow;Sorin Faibish	2012	RFC	10.17487/RFC6688	real-time computing;computer science;operating system;database	OS	-21.801706239134415	51.176791720035084	166253
322fe70408d33e72403d67f1aee6ac2772793390	a commutative replicated data type for cooperative editing	automatic control;distributed algorithms;groupware;binary trees concurrency control history convergence concurrent computing compaction distributed computing delay automatic control writing;convergence;pediatrics;concurrent computing;cooperative text editing;history;extended binary tree;dense identifier space;tree compacting;probability density function;storage management;radiation detectors;distributed computing;concurrent operations;trees mathematics;data mining;binary trees;treedoc;arrays;co operative editing;tree compacting commutative replicated data type concurrent operations treedoc cooperative text editing identifier space extended binary tree metadata storage;compaction;replicated data;concurrency control;commutative replicated data type;dense identifier space commutative replicated data type distributed algorithms replicated data co operative editing;distributed databases;writing;identifier space;metadata storage;trees mathematics concurrency control groupware storage management text editing;distributed algorithm;text editing;binary tree	A Commutative Replicated Data Type (CRDT) is one where all concurrent operations commute. The replicas of a CRDT converge automatically, without complex concurrency control. This paper describes Treedoc, a novel CRDT design for cooperative text editing. An essential property is that the identifiers of Treedoc atoms are selected from a dense space. We discuss practical alternatives for implementing the identifier space based on an extended binary tree. We also discuss storage alternatives for data and meta-data, and mechanisms for compacting the tree. In the best case, Treedoc incurs no overhead with respect to a linear text buffer. We validate the results with traces from existing edit histories.	best, worst and average case;binary tree;concurrency (computer science);concurrency control;conflict-free replicated data type;converge;essence;happened-before;identifier;overhead (computing);programming paradigm;text editor;tombstone (programming);tracing (software);unbalanced circuit;unique key;wiki	Nuno M. Preguiça;Joan Manuel Marquès;Marc Shapiro;Mihai Letia	2009	2009 29th IEEE International Conference on Distributed Computing Systems	10.1109/ICDCS.2009.20	distributed algorithm;binary tree;computer science;theoretical computer science;operating system;database;distributed computing;programming language	DB	-24.47681890970822	47.15033907198526	166751
82fe6bbdc170248f4cfc51ac988752b429d677ad	distributed simulation and manufacturing: time management issues in cots distributed simulation: a case study	time management;information sharing;commercial off the shelf;distributed simulation	Commercial off-the-shelf (COTS) simulation packages are widely used in industry. Several international groups are currently investigating techniques to integrate distributed simulation facilities in these packages. Through the use of a case study developed with the Ford Motor Company, this paper investigates time management issues in COTS simulation packages. Time management is classified on the basis of the ordering of events that are externally produced to a federate and the ordering of these with events that occur within a COTS simulation package federate. Several approaches to the latter are discussed and one approach is presented as the most effective. Finally the paper presents a bounded buffer problem and proposes the classification of information sharing with respect to the certification of solution.	distributed computing;federated identity;producer–consumer problem;regular expression;requirement;simulation;soft-in soft-out decoder	Simon J. E. Taylor;Jon Sharpe;John Ladbrook	2003		10.1145/1030818.1030929	real-time computing;simulation;time management;engineering;world wide web	Embedded	-27.617506131560667	48.50873525337592	167132
a575c0b1abdb590bed09a0b97c652ca149d950cd	efficient incremental checkpoint algorithm for primary-backup replication		Replication protocols are widely used for enabling fault tolerance and reliability features in distributed systems aiming fast recovery and seamless transition. In this study, we propose an efficient incremental checkpoint algorithm for primary-backup replication protocols to increase the system thro­ughput. We developed an in-memory key-value store configured by the primary-backup replication protocol and set it up on the geographically distributed nodes of the PlanetLab overlay network. We performed measurements for metrics of interest on both the client and the primary replica side. Our findings show that the proposed incremental checkpoint algorithm not only assures 2–3 times lower average blocking times but also guarantees a near-steady minimum average blocking time.	algorithm;backup;transaction processing system	Berkin Guler;Öznur Özkasap	2017		10.1109/SIU.2017.7960709	fault tolerance;computer science;real-time computing;algorithm;backup;overlay network;planetlab;distributed computing	EDA	-22.137515926897862	51.17448577965249	167196
c75c7eff3a2a92b24b744dfbcf0690e09ed6cb47	apl in the workstation environment	workstation environment	Computer industry analysts expect this trend to continue in the next few :years. They see a widespread move away from the traditional way of computing -using corporate mainframes -towards a more distributed method of computing, using departmental computers networked to mainframes and desktop computers. Workstations, located firmly in the middle between desktop mach.ines and mainframes in their computing speed and storage capacity, are bound to play an important role in this evolution.	apl;desktop computer;mainframe computer;workstation	J. R. Turner	1989		10.1145/75144.75196	programming language;workstation;computer science;computer architecture	HPC	-27.64571203320991	53.933221260907516	167424
81aa63c9be8650a8aebfc4c6dddca1b433dfdc03	early identification of critical blocks: making replicated distributed storage systems reliable against node failures		In large-scale replicated distributed storage systems consisting of hundreds to thousands of nodes, node failures are not rare and can cause data blocks to lose their replicas and become faulty. A simple but effective approach to prevent data loss from the node failures, i.e., ensuring reliability, is to shorten the identification time of the node failures and faulty blocks, which is determined by both timeouts and check intervals for node states. However, to maintain low repair network traffic, the identification time is actually relatively long and even dominates repair processes of critical blocks. In this paper, we propose a novel scheme, named RICK, to explore the potential in the identification time, and thus improve data reliability of replicated distributed storage systems while maintaining a low repair cost. First, by introducing an additional replica state, critical blocks (with two or more lost replicas) have individual short timeouts while sick blocks (with only one lost replica) preserve the long timeouts. Second, by replacing the static check intervals for node states with adaptive ones, the check intervals and the identification time of critical blocks are further shortened, which improves data reliability. Meanwhile, due to the low ratio of critical blocks in all faulty blocks, the repair network traffic remains low. The results from our simulation and prototype implementation show that RICK improves data reliability of replicated distributed storage systems by a factor of up to 14 in terms of mean time to data loss. Meanwhile, the extra repair network traffic caused by RICK is less than 1.5 percent of the total network traffic for data repairs.	clustered file system;critical section;data recovery;data redundancy;network packet;network traffic control;prototype;replication (computing);simulation	Juntao Fang;Shenggang Wan;Ping H. Huang;Changsheng Xie;Xubin He	2018	IEEE Transactions on Parallel and Distributed Systems	10.1109/TPDS.2018.2833457	real-time computing;distributed computing;distributed database;computer science;data loss;distributed data store	Metrics	-22.402751197876437	50.515455156554246	167448
c71927e37b07c0c4d55473db91fad06066326bb9	blue gene/q defragmentation for energy waste minimisation	defragmentation;allocation;blue gene	In this research, we explore the defragmentation of allocated compute resources so as to conserve energy on an IBM Blue Gene/Q. We examine a real trace from a new four-rack system and explore through simulation three heuristics to minimise energy waste through defragmentation. We describe a number of heuristics for detecting when it is desirable from an energy standpoint to defragment the computing resource through checkpoint/restart. Using heuristics, we were able to gain a simulated saving of 4.36 % of total system power. When applied to all BlueGene/Qs on the Top500 list, this is the equivalent of running the average US household for 698.5 years per annum.	application checkpointing;blue gene;heuristic (computer science);sensor;simulation;top500;total system power;transaction processing system	Timothy M. Lynar;Mark D. Nelson	2014	The Journal of Supercomputing	10.1007/s11227-014-1293-8	parallel computing;real-time computing;simulation;computer science;operating system;defragmentation	HPC	-19.32575863238665	57.244783058143994	167473
2cffd582a8e6f32bce47843ee29e40097c7a4cf0	integrated management of iaas resources	iaas;multiple iaas interoperable management;bookpart;deltacloud paci driver	ion Solutions Deltacloud Framework jClouds Library Libcloud Library Ruby 17 IaaS platforms Operations: • Compute • Storage • Network Drivers API: • Deltacloud REST • DMTF CIMI REST • AWS EC2 Query Web dashboard Java 30 IaaS platforms Operations: • Compute • Storage Maven dependencies Python 38 IaaS platforms Operations: • Compute • Storage • Network Drivers	amazon elastic compute cloud (ec2);amazon web services;application programming interface;cloud infrastructure management interface;cloud computing;deltacloud;java;python;representational state transfer;ruby	Fernando Meireles;Benedita Malheiro	2014		10.1007/978-3-319-14313-2_7	distributed computing;world wide web;computer security	OS	-30.4325554298043	54.1607373638777	167561
7888c2f273196e00256fe3a2081a3bb6e8e50731	a novel pilot job approach for improving the execution of distributed codes: application to the study of ordering in collisional transport in fusion plasmas	fusion plasma;resource provisioning;pilot jobs;application porting;grid computing	There are many applications in every scientific and engineering fields that have high computational requirements. They have to be adapted to either high-performance computing or high-throughput computing platforms, depending on their characteristics. For the latter case, the high volume of resources available in grid infrastructures should allow for a reduction of their calculation times and an increase of the accuracy of the results. Nevertheless, the low performance achieved by early-binding methods is noticeable, and the current systems based on pilot jobs are not suitable for running legacy applications. Therefore, running a wide range of codes quickly and easily can be infeasible for many users and developers. For this reason, it would be of great advantage to count on pilot job techniques that truly exploit grid resources in an efficient way while maintaining compatibility with legacy applications. In this work, GWpilot is presented as a complete pilot job framework that overcomes the adaptability limitations of other pilot job systems to tackle the challenge of efficiently accomplishing high-throughput calculations. To test the GWpilot system, two applications widely used for studying the properties of collisional transport in fusion plasmas (drift kinetic equation solver code and integrator of stochastic differential equations in plasmas) have been chosen as a proof of concept. Real calculations were performed with them to illustrate the adaptation process to the new framework and its performance. Additionally, the physics results obtained were analysed for completeness and further reading. Both applications also represent two different approaches further extended in the scientific community, that is, parameter sweep and Monte Carlo codes. Therefore, the general suitability of GWpilot for scientific and industrial applications belonging to other fields is demonstrated in this work. Copyright © 2014 John Wiley & Sons, Ltd.	code;computation;grid computing;high-throughput computing;job stream;john d. wiley;monte carlo method;pilot job;requirement;solver;supercomputer;throughput	Antonio J. Rubio-Montero;Francisco Castejón-Magaña;Eduardo Huedo;Rafael Mayo García	2015	Concurrency and Computation: Practice and Experience	10.1002/cpe.3301	parallel computing;simulation;computer science;theoretical computer science;operating system;database;distributed computing;grid computing	HPC	-21.69191009368964	58.13677793815053	167825
800913d8addc6a1a21fa12e0b3e6abeed6f7fa43	cloud computing: it as a service	ieee computer society conference;it professional;data center;beijing;data center it professional cloud computing trends;web services;grid computing;cloud computing costs computer industry web and internet services runtime environment ip networks application software investments software maintenance web services;beijing cloud computing ieee computer society conference;trends;cloud computing	Industry panelists at an IEEE Computer Society conference in Beijing look at the opportunities and challenges emerging from cloud computing and how their companies are addressing them.	cloud computing	Kenneth C. Smith	1988	IT Professional	10.1109/MITP.2009.22	web service;cloud computing security;data center;cloud computing;computer science;cloud testing;utility computing;services computing;law;information technology;world wide web;computer security;grid computing	Visualization	-32.29476158184767	55.25623573026454	167954
cf5402e23d31a60535952024e4ece0e07bf15b00	web services communication within the progress grid-portal environment	web service;distributed environment	Abstract  The,grid  is  the  next, generation  computing  infrastructure  able  to  handle  the  growing  requirements  for  computing  power.  Portals  are ,anticipated  as ,the  user’s access  point  to  these ,resources.  The  whole  grid- portal  infrastructure  constitutes ,a  distributed   environment,in  which ,efficient ,and ,flexible  communication,manners ,play  a  key  role.  The  emerging  web  services  technology  has  been  chosen ,as  the  best  solution  for  organizing  communication ,in  grid-portal  systems.  In  this  paper ,we  would  like  to  present  the  PROGRESS grid-portal ,environment  in  which  we  implemented  web  services ,communication  between  distributed modules oft he system.	web service	Piotr Grzybowski;Michal Kosiedowski;Cezary Mazurek	2003			database;data web;computer science;web development;world wide web;web service;web navigation;web 2.0;web-based simulation;web application security;ws-policy	DB	-31.601320788271977	51.31230154689783	168018
a15b32773bc700033b94aab206518082b24a4c8a	the design and implementation of a real-time walkthrough system supporting cooperative manipulation	groupware;real time systems rendering computer graphics peer to peer computing layout parallel processing processor scheduling concurrent computing application software military computing computer graphics;real time;virtual reality;parallel programming;virtual scene real time walkthrough system cooperative manipulation dis simulated scenes group members schedule based parallel processing model rendering pipeline oads cooperative interface workload balance real time performance distributed interactive simulation real time rendering;design and implementation;groupware parallel programming rendering computer graphics virtual reality;rendering computer graphics;parallel processing	In DIS, simulated scenes with extreme complexity become much more common, which will definitely advance much higher requirement on both real-time interaction and cooperative manipulation among group members. We thus propose a kind of schedule based parallel processing model, in which the parallel processing of the whole rendering pipeline and cooperation among members are combined together effectively. The constitution of our model and its implementation are discussed in detail. This model has been successfully applied in the development of OADS, and its satisfactory results, including the fine image, real-time performance, good workload balance at the server, friendly cooperative interface and so on, illustrate its efficiency.	cognitive walkthrough;real-time transcription	Hongmei Sun;Shenquan Liu	2001		10.1109/CSCWD.2001.942225	parallel processing;tiled rendering;real-time computing;simulation;rendering;computer science;operating system;parallel rendering;virtual reality;real-time rendering;computer graphics;alternate frame rendering;collaborative software;software rendering;computer graphics (images)	Robotics	-27.775710725488914	49.55947031478959	168336
a10ffa43518ec4255dabe944c6e5e9edb4855a24	methodology for trade-off analysis when moving scientific applications to cloud	virtualization;complexity theory;trade off analysis computing infrastructure comparison methodology cloud computing;comparison methodology;resource allocation;software performance evaluation;software performance evaluation cloud computing natural sciences computing resource allocation;computing infrastructure;performance based infrastructure cost trade off analysis scientific applications large scale distributed system utilization computational grids cloud computing distributed computing fully customizable infrastructure self managing infrastructure scalable on demand resources cloud infrastructure resource based infrastructure cost energy usage;mathematical model;complexity theory mathematical model equations virtualization cloud computing hardware;trade off analysis;natural sciences computing;cloud computing;hardware	Scientific applications have always been one of the major driving forces for the development and efficient utilization of large scale distributed systems - computational Grids represent one of the prominent examples. While these infrastructures, such as Grids or Clusters, are widely used for running most of the scientific applications, they still use bare physical machines with fixed configurations and very little customizability. Today, Clouds represent another step forward in advanced utilization of distributed computing. They provide a fully customizable and self-managing infrastructure with scalable on-demand resources. However, true benefits and trade-offs of running scientific applications on a cloud infrastructure are still obscure, due to the lack of decision making support, which would provide a systematic approach for comparing these infrastructures. In this paper we introduce a comprehensive methodology for comparing the costs of using both infrastructures based on resource and energy usage, as well as their performance. We also introduce a novel approach for comparing the complexity of setting up and administrating such an infrastructure.	agile software development;bespoke;cloud computing;computation;computer cluster;distributed computing;grid computing;parallel computing;scalability;self-management (computer science);task parallelism	Toni Mastelic;Drazen Lucanin;Andreas Ipp;Ivona Brandic	2012	4th IEEE International Conference on Cloud Computing Technology and Science Proceedings	10.1109/CloudCom.2012.6427575	cloud computing security;virtualization;simulation;cloud computing;resource allocation;computer science;theoretical computer science;operating system;cloud testing;mathematical model;distributed computing;utility computing;converged infrastructure	HPC	-21.85015443210378	58.558718365011714	168860
59b79dc22ba9cb18ad63ed9ad303e6f2b3287bfc	data ingestion and storage performance of iot platforms: study of openiot		Internet of Things is a very active research area with great commercialisation potential. The number of IoT platforms is already exceeding 300 and still growing. However, performance evaluation and benchmarking of IoT platforms are still in their infancy. As a step towards developing a performance benchmarking approach for IoT platforms, this paper analyses and compares a number of popular IoT platforms from data ingestion and storage capability perspectives. In order to test the proposed approach, we use the widely used open source IoT platform, OpenIoT. The results of the experiments and the lessons learnt are presented and discussed. While having a great research promise and pioneering contribution to semantic interoperability of IoT silos, the experimental results indicate OpenIoT platform needs more development effort to be ready for any substantial deployment in commercial IoT applications.		Alexey Medvedev;Alireza Hassani;Arkady B. Zaslavsky;Prem Prakash Jayaraman;Maria Indrawan;Pari Delir Haghighi;Sea Ling	2016		10.1007/978-3-319-56877-5_9	embedded system;computer hardware;operating system	DB	-27.17676023447344	55.92048182672692	168937
be5487918a8a852a5647c4d6e185a582ba1b7e66	handling performance sensitive native cloud applications with distributed cloud computing and sla management	service level agreements;public safety;contracts;safety cloud computing contracts metering public administration;cloud sla management;cloud orchestration;internet of things;servers;distributed cloud computing;abstracts;safety;distributed databases;public safety distributed cloud computing cloud orchestration service level agreements cloud sla management utility demand response internet of things;distributed cloud orchestration system performance sensitive native cloud applications distributed cloud computing sla management cost reduction deployment flexibility enterprise three tier web video streaming applications fine grained constraints storage networking resources mission critical enterprise use cases service level agreements distributed cloud platforms gaming component mobile end user low latency requirements complex performance requirement support electric utility metering electric utility control public safety application management tool abstract service manager asm distributed cloud native applications;utility demand response;metering;delays;cloud computing;cloud computing distributed databases safety servers delays abstracts;public administration	Cloud computing has been used as a platform to reduce cost and increase deployment flexibility for traditional enterprise three-tier web, and some video streaming applications. Typically these types of applications have fairly simple and self-understood performance requirements. Fine-grained constraints on the computation, storage, and networking resources are required support mission-critical enterprise use-cases at a reasonable cost. They are spelled out by service level agreements (SLAs) between the application and the cloud platform. Moreover, new distributed cloud platforms allow for additional deployment patterns, supporting more performance sensitive applications. For example, a specific gaming component will benefit being deployed in the proximity of the (mobile) end-user due to low-latency requirements. In this paper, we motivate the need for more complex performance requirement support with two use cases, electric utility metering and control and public safety. We describe an application management tool, called the Abstract Service Manager (ASM), which is designed to allow the expression of performance requirements in the automated deployment of distributed cloud-native applications. Together with a distributed cloud orchestration system, the ASM automatically mitigates the complexity of constructing performance sensitive applications and their deployment on a distributed cloud.	access network;application lifecycle management;cloud computing;cloud storage;component-based software engineering;computation;data center;high- and low-level;internet access;machine code;middleware;mission critical;multitier architecture;provisioning;real-time locating system;real-time transcription;requirement;server (computing);service-level agreement;software deployment;streaming media;virtual private network	Dimitri Mazmanov;Calin Curescu;Hjalmar Olsson;Andrew Ton;James Kempf	2013	2013 IEEE/ACM 6th International Conference on Utility and Cloud Computing	10.1109/UCC.2013.92	real-time computing;cloud computing;computer science;operating system;cloud testing;world wide web;distributed database;computer security;internet of things;metering mode;server	HPC	-30.7571993408466	55.89067618111548	169150
3a58f8efa2dde8e148c705f0604048cd30dd7aad	storage system problem troubleshooting andsystem logs				Weihang Jiang;Chongfeng Hu;Shankar Pasupathy;Arkady Kanevsky;Yuanyuan Zhou	2009	;login:			OS	-19.674742436091172	52.104037760888744	169218
69ff910213a9b0a266c52e26a56194229392fa99	network virtualization with cloud virtual switch	virtual machine;open vswtich;network virtualization;opennebula;software virtual switch network virtualization cloud virtual switch cloud computing server virtualization virtualization hypervisor software vmware software xen software kvm software virtual machines open source hypervisor;public domain software;business model;virtual machines;virtualisation cloud computing public domain software virtual machines;switches virtual machine monitors virtual machining bridges servers network interfaces software;opennebula cloud computing network virtualization virtual switch open vswtich;physical environment;virtualisation;cloud computing;open source;virtual switch	Recently, Cloud Computing is getting considerable attentions not only as the technology trend but also the new business model. Virtualization plays an extremely important role in Cloud Computing. Server virtualization has been achieved using virtualization hypervisor software such as VMware, Xen, and KVM,etc. Virtual Machines (VMs) created by hypervisors share the same physical environment with each other. Network virtualization is made by virtually connecting each virtual machine to a port on the virtual switch, which can be implemented by software or hardware. In this paper, we review both software and hardware network virtualization techniques and present the integration of open-source hypervisor with software virtual switch, which provides cloud users a more elastic and secure network environment.	cloud computing;hypervisor;kvm switch;network function virtualization;open-source software;virtual machine	Hui-Min Tseng;Hui-Lan Lee;Jen-Wei Hu;Te-Lung Liu;Jee-Gong Chang;Wei-Cheng Huang	2011	2011 IEEE 17th International Conference on Parallel and Distributed Systems	10.1109/ICPADS.2011.159	embedded system;full virtualization;virtualization;thin provisioning;temporal isolation among virtual machines;storage hypervisor;application virtualization;cloud computing;computer science;virtual machine;data virtualization;operating system;hardware virtualization;page table;distributed computing;service virtualization;hypervisor;storage virtualization	Arch	-28.67554721148057	56.199248288559325	169291
799c360922fc405b57a68c3f4275783dcbde4198	distributed programming framework for fast iterative optimization in networked cyber-physical systems	subgradient methods;distributed optimization;wireless sensor actuator networks;computer science distributed programming framework for fast iterative optimization in networked cyber physical systems university of california;synchronization;rahul;electrical engineering;distributed shared memory;los angeles mani b srivastava balani	Large-scale coordination and control problems in cyber-physical systems are often expressed within the networked optimization model. While significant advances have taken place in optimization techniques, their widespread adoption in practical implementations has been impeded by the complexity of internode coordination and lack of programming support for the same. Currently, application developers build their own elaborate coordination mechanisms for synchronized execution and coherent access to shared resources via distributed and concurrent controller processes. However, they typically tend to be error prone and inefficient due to tight constraints on application development time and cost. This is unacceptable in many CPS applications, as it can result in expensive and often irreversible side-effects in the environment due to inaccurate or delayed reaction of the control system.  This article explores the design of a distributed shared memory (DSM) architecture that abstracts the details of internode coordination. It simplifies application design by transparently managing routing, messaging, and discovery of nodes for coherent access to shared resources. Our key contribution is the design of provably correct locality-sensitive synchronization mechanisms that exploit the spatial locality inherent in actuation to drive faster and scalable application execution through opportunistic data parallel operation. As a result, applications encoded in the proposed Hotline Application Programming Framework are error free, and in many scenarios, exhibit faster reactions to environmental events over conventional implementations.  Relative to our prior work, this article extends Hotline with a new locality-sensitive coordination mechanism for improved reaction times and two tunable iteration control schemes for lower message costs. Our extensive evaluation demonstrates that realistic performance and cost of applications are highly sensitive to the prevalent deployment, network, and environmental characteristics. This highlights the importance of Hotline, which provides user-configurable options to trivially tune these metrics and thus affords time to the developers for implementing, evaluating, and comparing multiple algorithms.	algorithm;cognitive dimensions of notations;coherence (physics);control system;correctness (computer science);cyber-physical system;data parallelism;distributed computing;distributed shared memory;floor and ceiling functions;iteration;iterative method;locality of reference;locality-sensitive hashing;mathematical optimization;principle of locality;routing;scalability;software deployment	Rahul Balani;Lucas Francisco Wanner;Mani B. Srivastava	2014	ACM Trans. Embedded Comput. Syst.	10.1145/2544375.2544386	distributed shared memory;embedded system;synchronization;real-time computing;simulation;computer science;operating system;distributed computing	OS	-19.750215758368547	56.13359834853386	169334
2b6d5a2fea92a716ecdf285c8f904b47faff0a2f	a push-based prefetching for remote caching ram grid		As an innovative grid computing technique for sharing the distributed memory resources in a high-speed widearea network, RAM Grid exploits the distributed computing nodes, and provides remote memory for the user nodes which are short of memory. The performance of RAM Grid is constrained with the expensive network communication cost. In order to hide the latency of remote memory access and improve the performance, the authors proposed the push-based prefetching to enable the memory providers to push the potential useful pages to the user nodes. For each provider, it employs sequential pattern mining techniques, which adapts to the characteristics of memory page access sequences, on locating useful memory pages for prefetching. They have verified the effectiveness of the proposed method through trace-driven simulations. DOI: 10.4018/jghpc.2009070801 IGI PUBLISHING This paper appears in the publication, International Journal of Grid and High Performance Computing,Volume 1, Issue 4 edited by Emmanuel Udoh and Frank Zhigang Wang © 2009, IGI Global 701 E. Ch colate Avenue, H rshey PA 17033-1240, USA Tel: 717/533-8845; Fax 717/533-8661; URL-http://www.igi-global.com ITJ 5396 2 International Journal of Grid and High Performance Computing, 1(4), 1-15, October-December 2009 Copyright © 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited. heterogeneous nodes that are connected with a high-speed wide-area network. Using RAM Grid for caching will meet the characteristic of loosely coupled distributed computing environment, which emphasizes to provide “best effort” service, while does not guarantee the degree of performance improvement. In worst case, it is also acceptable that the performance does not raise (and does not drop), while in better network environment, it will gain much more benefits. Nowadays the campus or enterprise network is fast enough to meet the requirements of remote memory sharing, and the rapidly developing network technologies will make our approach more and more attracting. To facilitate later description, we classify the nodes in RAM Grid (Chu, et al., 2006) into five types. The user node is the consumer of remote memory, while the corresponding memory provider is called the busy node, which comes from the available node. A deputy node serves for one user node, and it acts as a broker and automatically searches available nodes for the user node. The intermediate node does not provide or consume any remote memory. It is ready to become a user node or available node. In order to study the potential performance improvement, we compare the overheads of data access for an 8KB block over local disk, NFS and RAM grid, which accesses remote memory through the wide area network with 2ms round-trip latency and 2MB bandwidth. From Table 1 we can observe that the caching mechanism in DRACO only reduces the overhead by 25%~30% compared to local disk or NFS access, and the major data access overhead in DRACO mainly comes from the network transmission cost (nearly 60%). Therefore, the performance of DRACO can obviously be more improved if we reduce or hide some of the transmission cost. Prefetching is an approach to hide the cost of low speed media among different levels of storage devices. In this article, we employ prefetching in DRACO in order to improve the performance. Differing from traditional I/O devices, in DRACO, the busy nodes, which provide remote memory for caching, often have extra CPU cycles. Therefore, the busy nodes can decide the prefetching policy and parameters by themselves, thus releasing the user nodes of DRACO, which are often dedicated to mass of computing tasks, from the process of prefetching. In contrast to traditional approaches, in which the prefetching data are decided by a rather simple algorithm in a user node, such a push-based prefetching scheme can be more effective.	algorithm;best, worst and average case;best-effort delivery;cpu cache;cache (computing);central processing unit;data access;data mining;distributed computing environment;distributed memory;fax;frank zhigang wang;grid computing;input/output;intel technology journal;link prefetching;loose coupling;mega man network transmission;overhead (computing);paging;random-access memory;requirement;sequential pattern mining;simulation;uniform resource identifier	Rui Chu;Nong Xiao;Xicheng Lu	2009	IJGHPC	10.4018/jghpc.2009070801	grid;computer network;grid computing;distributed memory;computer science;distributed computing environment;data access;instruction cycle;wide area network;distributed computing;page	HPC	-25.667829582789953	55.80440738488445	169347
4ee8a6754d64b58850f07146720d56bb73a90ae8	transparent data structures, or how to make search trees robust in a distributed environment	storage management;distributed processing;tree data structures robustness data structures read write memory computer science scalability peer to peer computing costs intelligent systems intelligent structures;tree data structures;search trees;distributed environment;distributed processing tree data structures tree searching storage management;tree searching;hypertree memory model transparent data structures search trees distributed environment transparent memory models pointer model linear addressable memory model;data structure;memory model	In this paper we propose a new class of memory models, called transparent memory models, for implementing data structures so that they can be emulated in a distributed environment in a scalable, efficient and robust way. Transparent memory models aim at combining the advantages of the pointer model and the linear addressable memory model without inheriting their disadvantages. We demonstrate the effectiveness of our approach by looking at a specific memory model, called the hypertree memory model, and by implementing a search tree in it that matches, in an amortized sense, the performance of the best search trees in the pointer model yet can efficiently recover from arbitrary memory faults.	amortized analysis;data structure;emulator;memory model (programming);pointer (computer programming);scalability;search tree	Miroslaw Korzeniowski;Christian Scheideler	2005	8th International Symposium on Parallel Architectures,Algorithms and Networks (ISPAN'05)	10.1109/ISPAN.2005.87	memory address;uniform memory access;distributed shared memory;shared memory;distributed memory;computer science;physical address;theoretical computer science;database;distributed computing;overlay;flat memory model;k-d-b-tree;data diffusion machine;computing with memory;memory map;memory management	Arch	-21.338909588675982	50.47372873311354	169620
ea1e81c829d9ffe98366b0b4d6251358962efbc2	a generic framework for testing parallel file systems		Large-scale parallel file systems are of prime importance today. However, despite of the importance, their failure-recovery capability is much less studied compared with local storage systems. Recent studies on local storage systems have exposed various vulnerabilities that could lead to data loss under failure events, which raise the concern for parallel file systems built on top of them.This paper proposes a generic framework for testing the failure handling of large-scale parallel file systems. The framework captures all disk I/O commands on all storage nodes of the target system to emulate realistic failure states, and checks if the target system can recover to a consistent state without incurring data loss. We have built a prototype for the Lustre file system. Our preliminary results show that the framework is able to uncover the internal I/O behavior of Lustre under different workloads and failure conditions, which provides a solid foundation for further analyzing the failure recovery of parallel file systems.	clustered file system;data center;data recovery;emulator;input/output;lustre;parallel virtual file system;prototype;thread-local storage;vii	Jinrui Cao;Simeng Wang;Dong Dai;Mai Zheng;Yong P Chen	2016	2016 1st Joint International Workshop on Parallel Data Storage and data Intensive Scalable Computing Systems (PDSW-DISCS)		self-certifying file system;embedded system;real-time computing;device file;engineering;operating system	OS	-23.42109957277233	52.223769963983635	169719
bfeada61b3f641b1181d0dffaeb9e29bf5e234a3	an auxiliary storage subsystem to distributed computing systems for external storage service	external storage;network capacity;iscsi;large scale;distributed computing system;storage subsystem;scientific computing;utility computing;storage cloud	Demands for efficient and effective utilization of computing resources have derived distributed computing systems of the large scaled such as Grid for scientific computing and Cloud for utility computing In these distributed computing systems the total capacity of consolidated storages should expand as the amount of user data grows The user's data are necessarily kept in the distributed computing system such as the Grid and the Cloud, which some users don't allow the system to In this work, an auxiliary storage subsystem is proposed to provide external storages to the distributed computing system The storage subsystem lets the iSCSI devices be connected to the distributed computing system as needed As iSCSI is a recent, standard, and widely deployed protocol for storage networking, the subsystem is able to accommodate many types of storages outside of the distributed computing system The proposed subsystem could transmit data by streams attaining high bit rate of the network capacity in the experiment.	auxiliary memory;distributed computing;external storage	MinHwan Ok	2010		10.1007/978-3-642-13119-6_22	fabric computing;distributed algorithm;real-time computing;cloud computing;computer science;operating system;database;distributed computing;utility computing;grid computing;computer network;autonomic computing	HPC	-25.891749157007744	55.77856895693146	169947
680a6b36ff03187283fe0b608f3f1703b9eb4a29	asep: a secure and flexible commit protocol for mls distributed database systems	access protocols distributed databases concurrency control transaction processing security of data;multi level security;and forward;database management systems;distributed processing;distributed transactions;distributed database system;multilevel systems database systems access protocols concurrency control uncertainty distributed databases transaction databases distributed processing robustness concurrent computing;concurrency control;distributed databases;access protocols;commit protocol;transaction processing;security;security of data;multilevel security;atomicity asep commit protocol advanced secure early prepare protocol secure flexible commit protocol multi level secure distributed database systems locking protocol concurrency control low level read lock release uncertainty window partial rollback low level data objects forward recovery distributed transaction aborting failure robustness language primitives system calls distributed transactions data consistency	The classical Early Prepare commit protocol (EP), used in many commercial systems, is not suitable for use in multilevel secure distributed databases systems that employ a locking protocol for concurrency control. This is because EP requires that read locks are not released by a participant during its window of uncertainty; however, it is not possible for a locking protocol to provide this guarantee in a multilevel secure system (since the read lock of a higher level transaction on a lower level data object must be released whenever a lower level transaction wants to write the same data). The only available work in the literature, namely the Secure Early Prepare protocol (SEP), overcomes this difficulty by aborting those distributed transactions that release their low level read locks prematurely. We see this approach as being too restrictive. One of the major benefits of distributed processing is its robustness to failures, and SEP fails to take advantage of this. In this work, we propose the Advanced Secure Early Prepare commit protocol (ASEP) to solve the above problem together with a number of language primitives that can be used as system calls in distributed transactions. These primitives permit features like partial rollback and forward recovery to be incorporated within the transaction model, and allow a distributed transaction to proceed even when a participant has released its low level read locks prematurely. A preliminary version of this paper under the title “An Advanced Commit Protocol for MLS Distributed Database Systems” appeared in the Proceedings of the 3rd ACM Conference on Computer and Communications Security, New Delhi, India, pages 119–128.	avalon;communications protocol;communications security;concurrency (computer science);concurrency control;distributed computing;distributed database;distributed transaction;expectation propagation;lock (computer science);multilevel security;symantec endpoint protection;system call;transaction processing;two-phase commit protocol	Indrajit Ray;Luigi V. Mancini;Sushil Jajodia;Elisa Bertino	2000	IEEE Trans. Knowl. Data Eng.	10.1109/69.895800	three-phase commit protocol;commit;real-time computing;two-phase commit protocol;database transaction;transaction processing;distributed transaction;computer science;two-phase locking;information security;concurrency control;x/open xa;database;distributed computing;compensating transaction;serializability;distributed database;acid	Security	-23.248945747962054	47.98293646744488	169971
22569fc83cd6b6a4ed5c0bde9a25dab27bc2f760	application performance management using learning, optimization, and control	learning;performance;control;optimization	"""In the past decade, the IT industry has experienced a paradigm shift as computing resources became available as a utility through cloud based services. In spite of the wider adoption of cloud computing platforms, some businesses and organizations hesitate to move all their applications to the cloud due to performance concerns. Existing practices in application performance management rely heavily on white-box modeling and diagnosis approaches or on performance troubleshooting """"cookbooks"""" to find potential bottlenecks and remediation steps. However, the scalability and adaptivity of such approaches remain severely constrained, especially in a highly-dynamic, consolidated cloud environment. For performance isolation and differentiation, most modern hypervisors offer powerful resource control primitives such as reservations, limits, and shares for individual virtual machines (VMs). Even so, with the exploding growth of virtual machine sprawl, setting these controls properly such that co-located virtualized applications get enough resources to meet their respective service level objectives (SLOs) becomes a nearly insoluble task. These challenges present unique opportunities in leveraging the rich telemetry collected from applications and systems in the cloud, and in applying statistical learning, optimization, and control based techniques to developing model-based, automated application performance management frameworks. There has been a large body of research in this area in the last several years, but many problems remain. In this talk, I'll highlight some of the automated and data-driven performance management techniques we have developed, along with related technical challenges. I'll then discuss open research problems, in hope to attract more innovative ideas and solutions from a larger community of researchers and practitioners."""	box modeling;cloud computing;hypervisor;machine learning;mathematical optimization;open research;programming paradigm;scalability;service-level agreement;virtual machine	Xiaoyun Zhu	2014		10.1145/2568088.2576098	simulation;performance;computer science;engineering;electrical engineering;management science;scientific control	HPC	-29.110913929701553	60.30543768379835	169972
fee998d33fe0f68c6b5d35eb3563a25aaad793fb	performance versus cost in a cloud computing environment			cloud computing	Yiping Ding	2010			cloud computing;utility computing;cloud testing;distributed computing;computer science	HPC	-26.303144520900535	59.35281979317353	170171
b5e416732e543de0a34babfefcc0219fa4a96bdd	computers don't sweat or performance metrics in the distributed computing environment	distributed computing environment		distributed computing environment	James P. Quigley	1995			autonomic computing;real-time computing;distributed algorithm;distributed computing environment;performance metric;computer science;distributed computing	HPC	-29.590487736033737	46.98861749086714	170245
d49a42a54df02b9dde60bc853aceae171b8f3f0b	on the synchronization bottleneck of openstack swift-like cloud storage systems		"""As one type of the most popular cloud storage services, OpenStack Swift and its follow-up systems replicate each object across multiple storage nodes and leverage <italic>object sync protocols</italic> to achieve high reliability and <italic>eventual consistency</italic>. The performance of object sync protocols heavily relies on two key parameters: <inline-formula><tex-math notation=""""LaTeX"""">$r$</tex-math><alternatives> <inline-graphic xlink:href=""""ruan-ieq1-2810179.gif""""/></alternatives></inline-formula> (number of replicas for each object) and <inline-formula><tex-math notation=""""LaTeX"""">$n$</tex-math><alternatives> <inline-graphic xlink:href=""""ruan-ieq2-2810179.gif""""/></alternatives></inline-formula> (number of objects hosted by each storage node). In existing tutorials and demos, the configurations are usually <inline-formula> <tex-math notation=""""LaTeX"""">$r=3$</tex-math><alternatives><inline-graphic xlink:href=""""ruan-ieq3-2810179.gif""""/> </alternatives></inline-formula> and <inline-formula><tex-math notation=""""LaTeX"""">$n<1,000$</tex-math><alternatives> <inline-graphic xlink:href=""""ruan-ieq4-2810179.gif""""/></alternatives></inline-formula> by default, and the sync process seems to perform well. However, we discover in data-intensive scenarios, e.g., when <inline-formula> <tex-math notation=""""LaTeX"""">$r>3$</tex-math><alternatives><inline-graphic xlink:href=""""ruan-ieq5-2810179.gif""""/> </alternatives></inline-formula> and <inline-formula><tex-math notation=""""LaTeX"""">$n\gg 1,000$</tex-math><alternatives> <inline-graphic xlink:href=""""ruan-ieq6-2810179.gif""""/></alternatives></inline-formula>, the sync process is significantly delayed and produces massive network overhead, referred to as the <italic>sync bottleneck problem</italic>. By reviewing the source code of OpenStack Swift, we find that its object sync protocol utilizes a fairly simple and network-intensive approach to check the consistency among replicas of objects. Hence in a sync round, the number of exchanged hash values per node is <inline-formula><tex-math notation=""""LaTeX"""">$\Theta (n\times r)$</tex-math> <alternatives><inline-graphic xlink:href=""""ruan-ieq7-2810179.gif""""/></alternatives></inline-formula>. To tackle the problem, we propose a lightweight and practical object sync protocol, <italic>LightSync</italic>, which not only remarkably reduces the sync overhead, but also preserves high reliability and eventual consistency. LightSync derives this capability from three novel building blocks: 1) <italic>Hashing of Hashes</italic>, which aggregates all the <inline-formula><tex-math notation=""""LaTeX"""">$h$</tex-math><alternatives> <inline-graphic xlink:href=""""ruan-ieq8-2810179.gif""""/></alternatives></inline-formula> hash values of each data partition into a single but representative hash value with the Merkle tree; 2) <italic>Circular Hash Checking</italic>, which checks the consistency of different partition replicas by only sending the aggregated hash value to the clockwise neighbor; and 3) <italic>Failed Neighbor Handling</italic>, which properly detects and handles node failures with moderate overhead to effectively strengthen the robustness of LightSync. The design of LightSync offers provable guarantee on reducing the per-node network overhead from <inline-formula><tex-math notation=""""LaTeX"""">$\Theta (n\times r)$</tex-math><alternatives><inline-graphic xlink:href=""""ruan-ieq9-2810179.gif""""/></alternatives></inline-formula> to <inline-formula><tex-math notation=""""LaTeX"""">$\Theta (\frac{n}{h})$</tex-math><alternatives> <inline-graphic xlink:href=""""ruan-ieq10-2810179.gif""""/></alternatives></inline-formula>. Furthermore, we have implemented LightSync as an open-source patch and adopted it to OpenStack Swift, thus reducing the sync delay by up to 879 <inline-formula><tex-math notation=""""LaTeX"""">$\times$</tex-math><alternatives> <inline-graphic xlink:href=""""ruan-ieq11-2810179.gif""""/></alternatives></inline-formula> and the network overhead by up to 47.5<inline-formula><tex-math notation=""""LaTeX"""">$\times$</tex-math><alternatives> <inline-graphic xlink:href=""""ruan-ieq12-2810179.gif""""/></alternatives></inline-formula>."""	cloud storage;data-intensive computing;eventual consistency;hash function;merkle tree;open-source software;overhead (computing);provable prime;self-replicating machine;swift (programming language);the sync;xlink	Mingkang Ruan;Thierry Titcheu;Ennan Zhai;Zhenhua Li;Yao Liu;Jinlong E;Yong Cui;Hong Xu	2018	IEEE Transactions on Parallel and Distributed Systems	10.1109/TPDS.2018.2810179	swift;sync;synchronization;cloud storage;computer science;hash function;cloud computing;merkle tree;distributed computing;eventual consistency	OS	-24.018094160875663	53.69457921119457	170510
541c2067f6eff388b14eb8d338aac255f21ca3cb	secure one snapshot protocol for concurrency control in real-time stock trading systems	database system;transaction management;view consistency;secure real time database systems;perforation;real time;trading system;covert channels;covert channel;simulation methods;concurrency control;database management system;multilevel security	To prevent any data from being accessed by unauthorized users, it is necessary for stock trading systems (STS) to use multilevel secure database management systems in controlling concurrent executions among multiple transactions. In STS, analytical transactions as well as mission critical transactions are executed concurrently, which makes it difficult to use traditional secure real-time transaction management schemes for STS environment. In this paper, we propose the read-down relationship-based secure one snapshot protocol (SOS) that is devised for the secure real-time transaction management in STS. By maintaining an additional one snapshot as well as working database, SOS blocks covert-channels without causing the priority inversion phenomenon. We Introduce the process of SOS protocol with some examples, present the proofs of devised protocol, and then evaluate the performance gains by means of simulation method.	algorithmic trading;concurrency (computer science);concurrency control;real-time transcription;snapshot (computer storage)	Namgyu Kim;Songchun Moon;Yonglak Sohn	2004	Journal of Systems and Software	10.1016/j.jss.2003.08.236	database transaction;covert channel;computer science;operating system;database;distributed computing;computer security;acid	Embedded	-23.349978903508777	48.00599233431595	170744
545939f61e18d6dddbe477823a7b78d58e9638f1	scoring system utilization through business profiles	resource utilization;it asset scoring system utilization business profile current monitoring tool narrow technical context heterogeneous it infrastructure cloud computing provider computer hardware workload migration investment requirement business purpose resource utilization information raw monitoring data profile based method;random access memory;measurement;info eu repo semantics conferenceobject;database management systems;resource allocation;resource manager;resource management;systems utilization;servers;monitoring measurement random access memory servers resource management organizations;it;monitoring;business data processing;feature extraction;resource allocation business data processing cloud computing database management systems feature extraction organisational aspects;software as a service;organizations;cloud computing;scoring system;organisational aspects	Understanding system utilization is currently a difficult challenge for industry. Current monitoring tools tend to focus on monitoring critical servers and databases within a narrow technical context, and have not been designed to to manage extremely heterogeneous IT infrastructure such as desktops, laptops, and servers, where the number of devices can be in the order of tens of thousands. This is an issue for many different domains (organizations with large IT infrastructures, cloud computing providers, or software as a service providers) where an understanding of how computer hardware is being utilized is essential for understanding business cost, workload migrations and future investment requirements. Furthermore, organizations find it difficult to understand the raw metrics collected by current monitoring tools, in particular when trying to understand to what degree their systems are being utilized in the context of different business purposes. This paper presents different techniques for the extraction of meaningful resource utilization information from raw monitoring data, a utilization scoring algorithm, and then subsequently outlines a profile-based method for tracking the utilization of IT assets (systems) in large heterogeneous IT environments. We intend to determine how efficiently system resources are utilized considering their business use. We will provide to the end-user an assessment of the system utilization together with additional information to perform remedial action.	algorithm;business process;cloud computing;color;computer hardware;database;iteration;laptop;requirement;server (computing);software as a service	Jesus Omana Iglesias;James Thorburn;Trevor Parsons;John Murphy;Patrick O'Sullivan	2011	2011 IEEE Third International Conference on Cloud Computing Technology and Science	10.1109/CloudCom.2011.71	in situ resource utilization;cloud computing;feature extraction;resource allocation;computer science;organization;knowledge management;resource management;operating system;software as a service;data mining;database;server;measurement	DB	-30.66107752184974	56.99067496102554	171036
75136368b41cb4fc8bcf84076b4de66c61c65dff	cloud data management for online games: potentials and open issues		The number of players, for massively multiplayer online role-playing games (MMORPG), typically reaches millions of people, geographically distributed throughout the world. Worldwide revenues for these games increase by billions of dollars each year. Unfortunately, their complex architecture makes them hard to maintain, resulting in considerable costs and development risks. For normal operation, MMORPGs have to access huge amounts of diverse data. With increasing numbers of players, managing growing volumes of data in a relational database becomes a big challenge, which cannot be overcome by simply adding new servers. Cloud storage systems are emerging solutions focusing on providing scalability and high performance for Cloud applications, social media, etc. However, Cloud storage systems are in general not designed for processing transactions or providing high levels of consistency. In this paper, we present our current work-in-progress by analyzing the existing architecture of MMORPGs and classifying relevant data. Based on this, we highlight the design requirements, identify the major research challenges, and propose a Cloud-based model for MMORPGs that we currently implement as a testbed for further evaluation.	cloud storage;computer data storage;massively multiplayer online role-playing game;relational database;requirement;scalability;social media;strong consistency;testbed	Ziqiang Diao;Eike Schallehn;Shuo Wang;Siba Mohammad	2013	Datenbank-Spektrum	10.1007/s13222-013-0142-x	simulation;computer science;operating system;data mining;database;distributed computing;world wide web	OS	-27.71338611269689	58.99828945915673	171409
c1adfea88bcb0ebfecf1d90fb3f37c13049bf5a3	a tight upper bound on the benefits of replica control protocols	high availability;evaluation performance;replication;base donnee repartie;distributed database;performance evaluation;availability;disponibilidad;evaluacion prestacion;base repartida dato;metric;optimal location;replicacion;access protocol;consistencia;mutual exclusion;upper bound;dynamic data;consistance;metrico;protocole acces;borne superieure;disponibilite;acceso protocolo;consistency;metrique;cota superior	We present an upper bound on the performance provided by a protocol guaranteeing mutually exclusive access to a replicated resource in a network subject to component failure and subsequent partitioning. The bound is presented in terms of the performance of a single resource in the same network. The bound is tight and is the first such bound known to us. Since mutual exclusion is one of the requirements for maintaining the consistency of a database object, this bound provides an upper limit on the availability provided by any database consistency control protocol, including those employing dynamic data relocation and replication. We show that if a well-placed single copy provides availability A for 0 ? A ? 1, then no scheme can achieve availability greater than ?A in the same network. We show this bound to be the best possible for any network with availability greater than 0.25. We also prove that the problem of calculating A is #P-complete and describe a method for approximating the optimal location for a single copy which adjusts dynamically to current network characteristics. The bound presented here is most useful for high availabilities, which tend to be obtainable with modern networks and their constituent sites and links.		Donald B. Johnson;Larry Raab	1995	J. Comput. Syst. Sci.	10.1006/jcss.1995.1059	availability;replication;real-time computing;dynamic data;mutual exclusion;metric;computer science;database;mathematics;distributed computing;upper and lower bounds;high availability;consistency;distributed database;computer security	Theory	-21.249370871749694	46.577743853128354	171501
494b70ce1bd4e55041c2ba97038238319c67a8a3	evaluation of virtual world systems	delay network servers scalability real time systems hardware computer science games mobile handsets collaboration videoconference;software performance evaluation;virtual reality;object oriented framework;client server systems;object oriented programming;object oriented framework virtual world systems evaluation artificial environment performance evaluation multiuser virtual world systems;client server systems virtual reality object oriented programming software performance evaluation;virtual worlds	A virtual world system is an artificial environment, created inside a computer, which mimics some aspect of the real world. These systems are multiuser, allowing many people to be present and to interact simultaneously in the virtual world. The performance of virtual world systems is important because the quality of the user’s experience depends on the responsiveness of the system. This paper looks at issues involved in evaluating the performance of such multiuser virtual world systems. A Jexible, object-oriented framework is presented for supporting these evaluations experimentally. As an example of its usage, this framework is applied to real virtual world system and some results are presented and discussed.	computer;experiment;multi-user;responsiveness;virtual world;world-system	Kevin Pulo;Michael E. Houle	2001		10.1109/ASWEC.2001.948503	real-time computing;simulation;computer science;virtual machine;instructional simulation;kernel virtual address space;virtual reality;multimedia;programming language;object-oriented programming;virtual finite-state machine	DB	-27.76029426665107	49.50491304493379	171779
febbd6b08c6bd968caa727c16ed24cf7f3b6c592	grid service spread applied to remote sensing processing	web and internet services;gss;pervasive computing;distributed computing;data communication;computational modeling;remote sensing;web services;grid service;remote sensing grid computing computational efficiency distributed computing pervasive computing web services space technology web and internet services data communication computational modeling;space technology;computational efficiency;grid computing	To accomplish a computational task, it requires at least two components, data and executable codes. In Grid computing environment, executable codes are provided as grid services and data are frequently transmitted in Internet or Intranet. Remote sensing applications always process large quantitative data. When these applications are implemented in Grid environment, data transmission will badly decrease the computational efficiency. To solve this problem, a new Grid computational model, Grid Service Spread (GSS) model, is presented in this paper. It transmits Grid services instead of data. Compared to remote sensing data, remote sensing Grid services’ quantity is very small. So that it can improve the computational efficiency. Many experiments proved that GSS is doable and effective for remote sensing processing. The results of the experiments are provided in the paper. KeywordsGrid Service; GSS; remote sensing	code;computation;computational model;executable;experiment;grid computing;internet;intranet;remote sensing application	Yanguang Wang;Yong Xue;Shaobo Zhong	2005	Proceedings. 2005 IEEE International Geoscience and Remote Sensing Symposium, 2005. IGARSS '05.	10.1109/IGARSS.2005.1526596	web service;distributed gis;semantic grid;computer science;theoretical computer science;distributed web crawling;distributed computing;utility computing;space technology;services computing;computational model;world wide web;ubiquitous computing;grid computing	HPC	-31.394350139974318	49.76316453553403	172158
cf6137d98dc5b5f29ef73aa40c7f5d858c1befdd	layered queueing models for enterprise javabean applications	benchmarking;topology;business to business;capacity management computers;electronic commerce;capacity planning;e commerce;enterprise systems;software performance evaluation java capacity management computers distributed object management electronic commerce;software performance evaluation;software systems;capacity sizing;layered queueing models;system performance;queueing model;bean model;distributed enterprise applications;business;distributed object management;performance analysis;enterprise javabean;distributed enterprise systems;middleware;predictive models;pattern analysis;enterprise system;business to business e commerce platform layered queueing models enterprise javabean capacity sizing enterprise systems benchmarking capacity planning software systems performance system performance distributed enterprise applications distributed enterprise systems;software systems performance;java predictive models performance analysis topology bean model capacity planning pattern analysis system performance business middleware;business to business e commerce platform;java	Traditional capacity sizing of enterprise systems relies on benchmarking a system using benchmark clients that generate a workload pattern similar to real-world workload. When new functions are added to the system or when the workload pattern changes, benchmarking has to be performed again. This is a costly and time-consuming approach for capacity planning. Layered queueing models have been used to study the performance of software systems. The approach is able to identify major performance parameters of software systems. Given a workload pattern, the models can be solved analytically to predict the system performance quickly. This paper proposes a layered queueing model for predicting the performance of distributed enterprise applications built on Enterprise JavaBeans (EJB) technology. We show how such models can be applied for capacity sizing of distributed enterprise systems. We demonstrate this by using this methodology to predict the performance of a sample application built on an EJB-based business-to-business e-commerce platform. We compare deployment options and study the effect of different workload patterns on system capacity.	enterprise javabeans;queueing theory	Te-Kai Liu;Santhosh Kumaran;Zongwei Luo	2001		10.1109/EDOC.2001.950435	e-commerce;enterprise system;real-time computing;computer science;systems engineering;operating system;software engineering;database	OS	-24.6075717157477	57.89543935092689	172200
bd71428c70b4129d160223fb72b99b2f3dbb10ba	management of real-time streaming data grid services	management system;data stream;audio video;global position system;collaborative system;real time data;network topology;quality of service;data grid	We discuss the architectural and management support for real time data stream applications, both in terms of lower level messaging and higher level service, filter and session structures. In our approach, messaging systems act as a Grid substrate that can provide qualities of service to various streaming applications ranging from audio-video collaboration to sensor grids. The messaging substrate is composed of distributed, hierarchically arranged message brokers that form networks. We discuss approaches to managing systems for both broker networks and application filters: broker network topologies must be created and maintained, and distributed filters must be arranged in appropriate sequences. These managed broker networks may be applied to a wide range of problems. We discuss applications to audio/video collaboration in some detail and also describe applications to streaming Global Positioning System data streams.		Geoffrey C. Fox;Galip Aydin;Harshawardhan Gadgil;Shrideep Pallickara;Marlon E. Pierce;Wenjun Wu	2005		10.1007/11590354_2	real-time data;real-time computing;quality of service;telecommunications;computer science;operating system;data grid;management system;database;distributed computing;world wide web;computer security;network topology;computer network	HPC	-31.78946889583202	47.02506091968489	172255
dc0b737edca3126880bacc8ae4ac72630729c6f9	performance characteristics of an sdn-enhanced job management system for cluster systems with fat-tree interconnect	cluster system with fat tree interconnect;topology;resource management;software defined networking;network topology;open flow;bandwidth;job management system;switches;cloud computing	In the era of cloud computing, data centers that accommodate a series of user-requested jobs with a diversity of resource usage pattern need to have the capability of efficiently distributing resources to each user job, based on individual resource usage patterns. In particular, for high-performance computing as a cloud service which allows many users to benefit from a large-scale computing system, a new framework for resource management that treats not only the CPU resources, but also the network resources in the data center is essential. In this paper, an SDN-enhanced JMS that efficiently handles both network and CPU resources and as a result accelerates the execution time of user jobs is introduced as a building block technology for such a HPC cloud. Our evaluation shows that the SDN-enhanced JMS efficiently leverages the fat-tree interconnect of cluster systems running behind the cloud to suppress the collision of communications generated by different jobs.	central processing unit;cloud computing;clustered file system;data center;fat tree;file allocation table;java message service;job stream;management system;run time (program lifecycle phase);software-defined networking;supercomputer	Yasuhiro Watashiba;Susumu Date;Hirotake Abe;Yoshiyuki Kido;Kohei Ichikawa;Hiroaki Yamanaka;Eiji Kawai;Shinji Shimojo;Haruo Takemura	2014	2014 IEEE 6th International Conference on Cloud Computing Technology and Science	10.1109/CloudCom.2014.82	openflow;real-time computing;cloud computing;network switch;computer science;resource management;operating system;distributed computing;utility computing;software-defined networking;network topology;bandwidth	HPC	-20.917774175598144	59.435467310974566	172268
746e4de10ea9105a81fcf2fc84baa47f6ae4e80f	building and validating a reduced tpc-h benchmark	application software;perforation;embedded system;computer architecture;decision support system;computational modeling;transaction databases;technology and engineering;decision support systems;performance analysis;bandwidth;computer simulation;decision support systems computational modeling computer simulation performance analysis application software computer architecture embedded system transaction databases delay bandwidth	The properties of computer system such as performance, energy, reliability, etc. are commonly evaluated by running benchmarks. However, the benchmarking process is complicated to set-up and use and running the benchmarks takes a substantial amount of time. Furthermore, when designing a computer, architects resort to simulation of the system, increasing the benchmarking time by several orders of magnitude. This problem can be alleviated by reducing the execution time of the benchmark suite. In this paper, we investigate a method to reduce the number of queries in TPC-H, a decision support system benchmark. Our evaluation shows that out of the 22 original queries, a subset of only 6 achieves a high representativeness at only 40% of the original execution time. The results also show that subsets built for a particular database size may be used for evaluating computer systems with other database sizes. Finally, we validate our approach against the full benchmark execution for a set of architecture case studies. The validation results show that the proposed subsets lead to similar conclusions than the ones drawn using the full benchmark.	benchmark (computing);computer;decision support system;ibm tivoli storage productivity center;run time (program lifecycle phase);simulation	Hans Vandierendonck;Pedro Trancoso	2006	14th IEEE International Symposium on Modeling, Analysis, and Simulation	10.1109/MASCOTS.2006.16	benchmark;sdet;application software;real-time computing;simulation;decision support system;computer science;operating system;data mining;computational model;bandwidth	Arch	-20.9127175505236	56.43377646871982	172276
84bdc9ef5485893394a33e0734e1d1a4cba0601a	precision locking algorithm for nested transactions systems.	nested transaction	This paper presents and verifies the improved version of precision locking that eliminates the locking overhead and deadlock prone aspects of the idea described in [5] and avoids the phantom problem. It extends the algorithm to nested transactions systems.	algorithm;deadlock;lock (computer science);nested transaction;overhead (computing);phantom reference	John K. Lee	1993		10.1145/170088.170451	computer science;database;nested transaction	DB	-22.132781770699122	47.78699183374624	172371
541162a8b82773c3b482569db0ecb1f8c5df8f40	automatic workload management for enterprise data warehouses	data warehouse	Modern enterprise data warehouses have complex workloads that are notoriously difficult to manage. Additionally, RDBMSs have many “knobs” for managing workloads efficiently. These knobs affect the performance of query workloads in complex interrelated ways and require expert manual attention to change. It often takes a long time for a performance expert to get enough experience with a large warehouse to be able to set the knobs optimally. Typically the warehouse and its workload change significantly within that time. This makes the task of manually optimizing the knob settings on a warehouse an impossible one. In this context, our goal is to create self managing Enterprise Data Warehouses. In this paper we describe some recent advances in building an automatic workload management system. We test this system against real workloads against real enterprise data warehouses.	control knob;real life;relational database management system;glite	Abhay Mehta;Chetan Gupta;Song Wang;Umeshwar Dayal	2008	IEEE Data Eng. Bull.		database;electromagnetic radiation;data mining;ionization;coating;cathode;computer science;radiation;anode;ultraviolet;inert	DB	-21.10110552412838	57.03735019162356	172409
054383d121fef561a3d83882e5f2bae53bbe1a66	the neeshub cyberinfrastructure for earthquake engineering	structural engineeirng;earthquake engineering;collaboration;distribution system operators;earthquakes;software engineering;civil engineering computing;research and development;earthquake engineering civil engineering computing digital simulation;structural engineering computing;scientific computing system applications and experience distributed systems operating systems software software engineering;scientific computing;software software engineering;earthquakes structural engineeirng collaboration research and development;collaborative research;distributed systems;collaboration tools cyberinfrastructure earthquake engineering us network for earthquake engineering simulation civil engineering experimental facility earthquake damage mitigation loss of life mitigation neeshub gateway;system applications and experience;digital simulation;operating systems	The US Network for Earthquake Engineering Simulation (NEES) operates a shared network of civil engineering experimental facilities aimed at facilitating research on mitigating earthquake damage and loss of life. The NEEShub gateway was created in response to the NEES community's needs, combining data, simulation, and analysis functionality with collaboration tools.	cyberinfrastructure;network for earthquake engineering simulation	Thomas J. Hacker;Rudolf Eigenmann;Saurabh Bagchi;Ayhan Irfanoglu;Santiago Pujol;Ann Christine Catlin;Ellen Rathje	2011	Computing in Science & Engineering	10.1109/MCSE.2011.70	computational science;earthquake engineering;computer science;management;collaboration	DB	-30.292830904690028	49.18960130718113	172514
26a2ae0a9d19ea3f94b62a96463b1435a3b1bb7c	space-efficient page-level incremental checkpointing	page level incremental checkpointing;linux kernel;fault tolerant;fault tolerance;cumulant;checkpoint and recovery	Incremental checkpointing, which is intended to minimize checkpointing overhead, saves only the modified pages of a process. However, the cumulative size of incremental checkpoints increases at a steady rate over time because many updated values may be saved for the same page. In this paper, we present a comprehensive overview of Pickpt, which is a page-level incremental checkpointing facility. Pickpt provides space-efficient techniques for minimizing the use of disk space. For our experiments, the results show that the use of disk space of Pickpt was significantly reduced compared with existing incremental checkpointing.	application checkpointing;disk space;experiment;incremental backup;overhead (computing)	Junyoung Heo;Sangho Yi;Yookun Cho;Jiman Hong;Sung Y. Shin	2005		10.1145/1066677.1067026	fault tolerance;parallel computing;real-time computing;computer science;operating system;distributed computing	DB	-19.808902718705117	49.55714636891532	172758
1fe8516e04d60ddcde792625946b7b8e16638635	isds: a self-configurable software-defined storage system for enterprise	machine learning;software defined storage;lightweight container;storage;neural network	ABSTRACTStorage is one of the most important aspects of IT infrastructure for various enterprises. But, enterprises are interested in more than just data storage; they are interested in such things as more reliable data protection, higher performance and reduced resource consumption. Traditional enterprise-grade storage satisfies these requirements at high cost. It is because traditional enterprise-grade storage is usually designed and constructed by customised field-programmable gate array to achieve high-end functionality. However, in this ever-changing environment, enterprises request storage with more flexible deployment and at lower cost. Moreover, the rise of new application fields, such as social media, big data, video streaming service etc., makes operational tasks for administrators more complex. In this article, a new storage system called intelligent software-defined storage (iSDS), based on software-defined storage, is described. More specifically, this approach advocates using software to rep...	computer data storage;software-defined storage	Wen-Shyen E. Chen;Chun-Fang Huang;Ming-Jen Huang	2018	Enterprise IS	10.1080/17517575.2016.1218055	systems engineering;software deployment;computer science;storage area network;software-defined storage;object storage;real-time computing;emc invista;information repository;computer data storage;converged storage	OS	-29.337156566749133	58.635548976869345	172825
11f09c23f84ee85f7e1d422e4f2d450bf4c538e6	serviceware - a service based management approach for wsn cloud infrastructures	wireless sensor networks virtualization cloud computing hardware service oriented architecture instruction sets;virtual sensor network serviceware service based management approach wsn cloud infrastructure smart environment systems of system internet of things wireless sensor network infrastructure wsn infrastructure management cost maintenance cost shared infrastructure approach cloud computing infrastructure as a service model iaas model middleware approach infrastructure virtualization;internet of things;cloud computing wsn serviceware wsn sora;middleware;wireless sensor networks;virtualisation;wireless sensor networks cloud computing internet of things middleware telecommunication network management virtualisation;cloud computing;telecommunication network management	With the push for smart environments and the advent of concepts like Systems of Systems and Internet of Things, the need for underlying large-scale wireless sensor networks (WSNs) infrastructures is evident, however it is likely that no single private user could justify the costs to be incurred for a large-scale WSN deployment and the subsequent management and maintenance costs. In order to drive down costs and maximize the WSN utility a shared infrastructure approach makes large-scale multipurpose WSNs viable. The shared infrastructure paradigm where multiple applications and end users act on a single physical WSN infrastructure in parallel requires a fundamental change in the way WSN resources are managed are, specifically at the WSN device level. This paper draws on the cloud computing infrastructure as a service (IaaS) model and presents Serviceware - a middleware approach for infrastructure virtualization in next generation WSNs, where the WSN resources are exposed as services. Virtualization enables the slicing of the physical infrastructure into unique segments or virtual sensor networks that can be configured for individual end users according to their specific requirements.	central processing unit;cloud computing;hardware virtualization;internet of things;middleware;networking hardware;next-generation network;operating system;programming paradigm;provisioning;requirement;smart environment;software deployment;system of systems;thread (computing);virtual lan;x86 virtualization	Susan Rea;Muhammad Sohaib Aslam;Dirk Pesch	2013	2013 IEEE International Conference on Pervasive Computing and Communications Workshops (PERCOM Workshops)	10.1109/PerComW.2013.6529470	embedded system;wireless sensor network;cloud computing;computer science;operating system;middleware;distributed computing;converged infrastructure;internet of things;computer network	Mobile	-29.832830647834232	57.16913086206307	172895
f0408e93c0e6f42613d190ee2f94c2f5cb02c411	the value of automation: a study of mpi application migration methods	application migration;user study;debugging manuals automation standards communities electronic publishing timing;message passing application program interfaces;application program interfaces;message passing;mpi;application migration user study mpi;automation value mpi application migration methods study data feam framework for efficient application migration process minimal user interaction	This paper presents a user study of the process of migrating MPI applications manually. The gathered data quantifies the scale of the challenge that researchers face when attempting to use shared computing resources. Migrating to one site took on average 2.5 hours where the majority of the time was spent on learning, compiling, and debugging. Less experienced researchers took almost 50% more time and overall spread their effort across more days. Consequently, they took on average three weeks to migrate to four sites. The study data is used to investigate the speedup that can be gained by automating the migration process. How much more efficient application migration can be when utilizing FEAM, the Framework for Efficient Application Migration, instead of just manual methods is modeled in terms of time. While an ideal solution for general automation of application migration would enable computations to run on any available compute resource with minimal user interaction and in a tuned manner, FEAM focuses on what can be automated without recompiling applications. The calculations predict that using FEAM results in a 1.6x speedup on average or just under an hour of effort saved per site. This equates to more than two day of saved effort per site. The results presented in this paper underline the value of automation in the context of application migration.	automation;compiler;computation;debugging;grid computing;keystroke-level model;message passing interface;speedup;usability testing	Karolina Sarnowska-Upton;Andrew S. Grimshaw	2013	2013 12th IEEE International Conference on Trust, Security and Privacy in Computing and Communications	10.1109/TrustCom.2013.149	parallel computing;real-time computing;computer science;operating system	HPC	-27.97594397718936	52.57489233663868	172927
90d8e0b9df8aec3550361db22a08e8b941fb50c6	how many planet-wide leaders should there be?	eurecom ecole d ingenieur telecommunication centre de recherche graduate school research center communication systems	"""Geo-replication becomes increasingly important for modern planetary scale distributed systems, yet it comes with a specific challenge: latency, bounded by the speed of light. In particular, clients of a geo-replicated system must communicate with a leader which must in turn communicate with other replicas: wrong selection of a leader may result in unnecessary round-trips across the globe. Classical protocols such as celebrated Paxos, have a single leader making them unsuitable for serving widely dispersed clients. To address this issue, several all-leader geo-replication protocols have been proposed recently, in which every replica acts as a leader. However, because these protocols require coordination among all replicas, commiting a client's request at some replica may incure the so-called """"delayed commit"""" problem, which can introduce even a higher latency than a classical single-leader majority-based protocol such as Paxos.  In this paper, we argue that the """"right"""" choice of the number of leaders in a geo-replication protocol depends on a given replica configuration and propose Droopy, an optimization for state machine replication protocols that explores the space between single-leader and all-leader by dynamically reconfiguring the leader set. We implement Droopy on top of Clock-RSM, a state-of-the-art all-leader protocol. Our evaluation on Amazon EC2 shows that, under typical imbalanced workloads, Droopy-enabled Clock-RSM efficiently reduces latency compared to native Clock-RSM, whereas in other cases the latency is the same as that of the native Clock-RSM."""	amazon elastic compute cloud (ec2);distributed computing;mathematical optimization;paxos (computer science);planetary scanner;response surface methodology;state machine replication	Shengyun Liu;Marko Vukolic	2015	SIGMETRICS Performance Evaluation Review	10.1145/2847220.2847222	real-time computing;simulation;computer science;operating system;distributed computing;computer network	Networks	-19.143528707739396	50.21501759555956	173194
735c5d27df48d38db5c6625ffb5f761d4bbb86cb	application and network performance of amazon elastic compute cloud instances	graphics processing units cloud computing bandwidth central processing unit servers conferences time measurement;comparing cloud services;network performance cloud computing comparing cloud services amazon ec2 gpu instance performance t2 medium instance performance;network performance;gpu instance performance;t2 medium instance performance;amazon ec2;cloud computing	The computing and networking infrastructure in public clouds is shared between multiple users and can create abnormal variations in the performance of applications running in cloud. In this paper we compare the performance of compute-intensive applications on CPUs and GPUs offered by Amazon's cloud and show that not all applications exhibit speedups when executed on the GPU. Even for applications that exhibit speedup on the GPU, the overall application performance may be bottlenecked by network delay. For such cases, the high cost of GPU instances is inefficient since it does not improve application performance. We also show that the network performance of different instances can vary significantly overtime and public clouds may throttle network bandwidth for applications that generate significant network traffic.	amazon elastic compute cloud (ec2);central processing unit;cloud computing;graphics processing unit;multi-user;network performance;network traffic control;speedup	Mehrin Gilani;Catherine Inibhunu;Qusay H. Mahmoud	2015	2015 IEEE 4th International Conference on Cloud Networking (CloudNet)	10.1109/CloudNet.2015.7335328	parallel computing;real-time computing;cloud computing;computer science;distributed computing	HPC	-22.078026124521564	60.050277633316796	173279
45c75e700da7d52ccfadc67941f1578b2728eade	application performance optimization through passive packet capture and analysis	performance optimization	An application’ s response determines its performance in a networking environment, which introduces application latency through processing delays in clients, servers, and network elements. Performance optimization of applications partly depends on identifying and subsequently minimizing / removing latency bottleneck(s) between servers and clients. As networks become more complex and support a rapidly growing number of dynamically inter-dependent applications, identifying application performance bottlenecks is becoming correspondingly and increasingly dif® cult. Constant modi® cations in networking infrastructures, and deployment of new software releases and applications further complicate application performance analysis. In short, being able to decipher and analyze individual application performance and identify performance bottlenecks amid an avalanche of inter-dependent applications, network elements, and potential performance hotspots in the intranets and internets is essential to optimizing application performance and infrastructure capacity planning. Examples of application performance data that are useful to performance optimization and capacity planning include:	internet;intranet;mathematical optimization;packet analyzer;performance tuning;s/pdif;software deployment	L. Lawrence Ho	1999	Journal of Network and Systems Management	10.1023/A:1018748101667	computer science	Metrics	-23.938576642792576	57.14602031522628	173291
39b7dd9b3d0437a2e1cc870a34e51838bb54a526	distributed results checking for mapreduce in volunteer computing	volunteer computing;protocols;middleware grid computing internet;majority voting method;bitdew;computational modeling protocols internet middleware security servers distributed databases;majority voting method distributed results checking mapreduce volunteer computing data intensive applications middleware bitdew desktop grid environment internet;data intensive applications;servers;computational modeling;internet;desktop grid environment;distributed databases;middleware;mapreduce;security;grid computing;distributed results checking	MapReduce is a promising approach to support data-intensive applications on Volunteer Computing Systems. Existent middleware like Bit Dew allows running MapReduce applications in a Desktop Grid environment. If the Desktop Grid is deployed in the Internet under the Volunteer Computing paradigm, it harnesses untrustable, volatile and heterogeneous resources and the results produced by MapReduce applications can be subject of sabotage. However, the implementation of large-scale MapReduce presents significant challenges with respect to the state of the art in Desktop Grid. A key issue is the design of the result certification, an operation needed to verify that malicious volunteers do not tamper with the results of computations. Because the volume of data produced and processed is so large that cannot be sent back to the server, the result certification cannot be centralized as it is currently implemented in Desktop Grid systems. In this paper we present a distributed result checker based on the Majority Voting method. We evaluate the efficiency of our approach using a model for characterizing errors and sabotage in the MapReduce paradigm. With this model, we can compute the aggregated probability with which a MapReduce implementation produces an erroneous result. The challenge is to capture the aggregated probability for the entire system, composed from probabilities resulted from the two phases of computation: Map and Reduce. We provide a detailed analysis on the performance of the result verification method and also discuss the generated overhead of managing security. We also give guidelines about how the result verification phase should be configured, given a MapReduce application.	centralized computing;computation;data-intensive computing;desktop computer;mapreduce;middleware;overhead (computing);programming paradigm;reduce;server (computing);volunteer computing	Mircea Moca;Gheorghe Cosmin Silaghi;Gilles Fedak	2011	2011 IEEE International Symposium on Parallel and Distributed Processing Workshops and Phd Forum	10.1109/IPDPS.2011.351	communications protocol;parallel computing;the internet;computer science;information security;operating system;middleware;database;distributed computing;computational model;world wide web;distributed database;grid computing;server;computer network	HPC	-29.931725648752064	51.980147344242376	173480
882f0359b615f5adfcca2d33cfb8509152760a3c	using distributed computing platform to solve high computing and data processing problems in bioinformatics	distributed algorithms;massive data;biology computing;distributed computing data processing bioinformatics peer to peer computing supercomputers high speed networks costs high performance computing processor scheduling load management;peer to peer file sharing;data integrity;high performance computing;processor scheduling;high speed networks;massive data distributed computing platform bioinformatics peer to peer file sharing scheduling load balancing data integrity maintenance massive computing;user interfaces biology computing computational complexity distributed algorithms scheduling data integrity;distributed computing;data processing;massive computing;computational complexity;scheduling;high performance computer;load management;load balancing;load balance;file sharing;peer to peer computing;data integrity maintenance;peer to peer;distributed computing platform;off the shelf;user interfaces;supercomputers;bioinformatics	Since the problems in bioinformatics are related to massive computing and massive data. In recent years, due to distributed computing is gaining recognition. The task originally requiring high computing power does not only rely on supercomputer. Distributed computing used off-the-shelf PC with high speed network can offer low cost and high performance computing power to handle the task. Therefore, the purpose of this paper is to implement a complete distributed computing platform based on peer-to-peer file sharing technology. The platform integrated scheduling, load balancing, file sharing, maintenance of data integrity, and user-friendly interface etc. functions. Through the platform can assist bioinformaticists in massive computing and massive data problems. Besides, the platform is easier use, more reliable, and more helpful than others for researchers to conduct bioinformatics research.	algorithm;bioinformatics;data integrity;distributed computing;load balancing (computing);manual handling of loads;peer-to-peer file sharing;scalability;scheduling (computing);supercomputer;usability	Shih-Nung Chen;Jeffrey J. P. Tsai;Chih-Wei Huang;Rong-Ming Chen;Raymond C. K. Lin	2004	Proceedings. Fourth IEEE Symposium on Bioinformatics and Bioengineering	10.1109/BIBE.2004.1317336	distributed algorithm;parallel computing;data processing;cloud computing;computer science;load balancing;theoretical computer science;end-user computing;data-intensive computing;distributed computing;utility computing;distributed design patterns;grid computing;autonomic computing	HPC	-29.810496358238346	49.3682344751079	173693
89beb4bf107b4451a807628fb4aec640992b0aa3	maintaining page coherence for dynamic html pages	digital library;digital libraries;database caching;internet intranet;cache coherence;world wide web	"""Fueled la.rgely by the gaining popularity of World Wide Web (web) servers and browsers, more and more sites have published their databases to the Internet/Intranet. This allows users to retrieve and explore database items as """"dynamic"""" HTML pages. Existing dynamic HTML pages are passive. When the content of a database is changed, the corresponding dynamic HTML page does not get reflected. We term this, the page coherence problem. In this paper, we illustrate the limitations of current web browsers in addressing the page coherence problem. We next illustrate that since a dynamic HTML page is maintained in the storage cache of a web client, maintaining the coherence of a dynamic HTML page is similar in spirit to cache coherence problem. However, since a web server is both stateless and connectionless, conventional """"push-based"""" cache coherence approaches that require a server to invalidate and propagate the updates to the cache are not feasible. We propose a page coherence mechanism which requires a web client to take an active role in invalidating and updating its cache and thus, a dynamic HTML page. We illustrate our' implementation on Netscape Navigator using Netscape Plugins, The effectweness of our"""" mechanism is also studied"""	cache (computing);cache coherence;connectionless communication;database;dynamic html;embedded system;energy (psychological);entity;experiment;ico (file format);intranet;java;plug-in (computing);prototype;server (computing);simulation;stateless protocol;web server;world wide web	Antonio Si;Hong Va Leong;Stanley M. T. Yau	1998		10.1145/330560.331104	cache coherence;static web page;digital library;computer science;database;world wide web;information retrieval	DB	-26.15992918582635	49.46271037345765	173789
7bb22b37c6748f82fc9ac6df4cf026a0bf1957e8	high performance gravitational n-body simulations on a planet-wide distributed supercomputer	optical network;ucl;n body simulations;discovery;theses;conference proceedings;cold dark matter;digital web resources;ucl discovery;open access;ucl library;book chapters;open access repository;high performance;ucl research	We report on the performance of our cold-dark matter cosmological N -body simulation which was carried out concurrently using supercomputers across the globe. We ran simulations on 60 to 750 cores distributed over a variety of supercomputers in Amsterdam (the Netherlands, Europe), in Tokyo (Japan, Asia), Edinburgh (UK, Europe) and Espoo (Finland, Europe). Regardless the network latency of 0.32 seconds and the communication over 30.000 km of optical network cable we are able to achieve ∼ 87% of the performance compared to an equal number of cores on a single supercomputer. We argue that using widely distributed supercomputers in order to acquire more compute power is technically feasible, and that the largest obstacle is introduced by local scheduling and reservation policies.	code;computer simulation;consortium;dr-dos;dark matter;experiment;file spanning;grape;global network;information science;lex (software);maxine d. brown;message passing;meta-scheduling;national fund for scientific research;networking cables;overhead (computing);petri net;robot fetishism;scalability;scheduling (computing);search engine optimization;supercomputer;sushi;traverse;walter pitts	Derek Groen;Simon Portegies Zwart;Tomoaki Ishiyama;Junichiro Makino	2011	CoRR	10.1088/1749-4699/4/1/015001	cold dark matter;simulation;computer science;operations research;physics;quantum mechanics	HPC	-28.713602441801623	51.73129424913806	173830
61b5e9673d9c182f4a8c6a813482b380458afbb7	opennebula: a cloud management tool	standards;cloud computing opennebula;application software;computer model;cloud computing computational modeling application software standards computer network management;opennebula;computational modeling;computer network management;management tool;cloud computing	In this installment of Trend Wars, we discuss cloud computing and OpenNebula with Ignacio M. Llorente and Rubén S. Montero, who are the principal investigator and the chief architect, respectively, of the open source OpenNebula project.	cloud management;ignacio m. llorente;open-source software	Dejan S. Milojicic;Ignacio Martín Llorente;Rubén S. Montero	2011	IEEE Internet Computing	10.1109/MIC.2011.44	computer simulation;application software;cloud computing;computer science;operating system;distributed computing;computational model;world wide web	Visualization	-32.201867596797186	55.07198338637835	173864
8f5b351610f8153f7631b7afaafc6381e2353755	an ldap-based user modeling server and its evaluation	directory server;database system;performance;user adaptation;lightweight directory access protocol;data type;service model;simulation experiment;architecture evaluation;evaluation;scalability;it evaluation;user modeling server;architecture;ldap;user model	Representation components of user modeling servers have been traditionally based on simple file structures and database systems. We propose directory systems as an alternative, which offer numerous advantages over the more traditional approaches: international vendor-independent standardization, demonstrated performance and scalability, dynamic and transparent management of distributed information, built-in replication and synchronization, a rich number of pre-defined types of user information, and extensibility of the core representation language for new information types and for data types with associated semantics. Directories also allow for the virtual centralization of distributed user models and their selective centralized replication if better performance is needed. We present UMS, a user modeling server that is based on the Lightweight Directory Access Protocol (LDAP). UMS allows for the representation of different models (such as user and usage profiles, and system and service models), and for the attachment of arbitrary components that perform user modeling tasks upon these models. External clients such as user-adaptive applications can submit and retrieve information about users. We describe a simulation experiment to test the runtime performance of this server, and present a theory of how the parameters of such an experiment can be derived from empirical web usage research. The results show that the performance of UMS meets the requirements of current small and medium websites already on very modest hardware platforms, and those of very large websites in an entry-level business server configuration.	attachments;canonical account;centralized computing;database;directory (computing);extensibility;lightweight directory access protocol;requirement;run time (program lifecycle phase);scalability;server (computing);simulation;theory;usb mass storage device class;user modeling	Alfred Kobsa;Josef Fink	2006	User Modeling and User-Adapted Interaction	10.1007/s11257-006-9006-5	lightweight directory access protocol;user modeling;human–computer interaction;computer science;operating system;database;world wide web	DB	-31.758854779980524	52.0047920174236	173941
0e3d28905c4626c25bd78e6bbc76891e3e639096	towards an autonomic computing environment	databases;desktop computers;reliability engineering;low cost computing;mathematics;expert systems;maintenance;heterogeneous computing;installation;distributed computing;qadpz;large scale system;autonomic nervous system;computer networks;computer architecture;pc network;autonomic distributed computing;distributed computing system;distributed programming computer networks;distributed computing computer architecture humans conferences databases expert systems mathematics reliability engineering informatics autonomic nervous system;distributed programming;quite advanced distributed parallel zystem;informatics;distributed computing environment;humans;distributed heterogenous computers;autonomic computing;self management;autonomic distributed computing autonomic computing desktop computers low cost computing installation maintenance distributed heterogenous computers self management qadpz quite advanced distributed parallel zystem pc network;conferences;open source	Autonomic Computing is a promising new concept in system development. It aims to (i) increase reliability by designing systems to be self-protecting and self-healing; and (ii) increase autonomy and performance by enabling systems to adapt to changing circumstances, using self-configuring and self-optimizing mechanisms. This paper discusses the type of system architecture needed to support such objectives.	autonomic computing;systems architecture	Roy Sterritt;David W. Bustard	2003		10.1109/DEXA.2003.1232102	distributed algorithm;installation;computer science;theoretical computer science;distributed computing;distributed design patterns;informatics;expert system;symmetric multiprocessor system;grid computing;distributed computing environment;autonomic computing	HPC	-31.189084040032686	48.239742552306495	173944
1beee6797da5388bae02bbebd9010237542518a6	a peer-to-peer replica management service for high-throughput grids	location service;fault tolerant computing grid computing open systems peer to peer computing file organisation;protocols;fault tolerant;peer to peer network;distributed hash table;replica location service;resonance light scattering;data engineering;peer to peer system;systems engineering and theory;data storage;fault tolerant computing;fault tolerant systems;distributed hash table grid computing open systems replica location service structured peer to peer system fault tolerance;fault tolerance;peer to peer computing grid computing resonance light scattering memory fault tolerant systems delay data engineering systems engineering and theory laboratories protocols;structured peer to peer system;peer to peer computing;high throughput;open systems;peer to peer;grid computing;high performance;data grid;memory;file organisation	Future high-throughput grids may integrate millions or even billions of processing and data storage nodes. Services provided by the underlying grid infrastructure may have to be able to scale to capacities not even imaginable today. In this paper we concentrate on one of the core components of the data grid architecture - the replica location service - and evaluate a redesign of the system based on a structured peer-to-peer network overlay. We argue that the architecture of the currently most widespread solution for file replica location on the grid is biased towards high-performance deployments and cannot scale to the future needs of a global grid. Structured peer-to-peer systems can provide the same functionality, while being much more manageable, scalable and fault-tolerant. However, they are only capable of storing read-only data. To this end, we propose a revised protocol for distributed hash tables that allows data to be changed in a distributed and scalable fashion. Results from a prototype implementation of the system suggest that grids can truly benefit from the scalability and fault-tolerance properties of such peer-to-peer algorithms.	algorithm;attribute–value pair;computer data storage;distributed hash table;experiment;fault tolerance;grid computing;high-throughput computing;immutable object;location-based service;lookup table;overlay network;peer-to-peer;prototype;read-only memory;real life;recursive least squares filter;requirement;scalability;software deployment;throughput	Antony Chazapis;Antonis Zissimos;Nectarios Koziris	2005	2005 International Conference on Parallel Processing (ICPP'05)	10.1109/ICPP.2005.12	fault tolerance;grid file;parallel computing;information engineering;computer science;operating system;database;distributed computing;computer network	HPC	-25.328197829475236	55.08239331752752	173949
76efbcf1603bf4baf57069d4fd94fada4026ff1e	mobile cloud computing: the taxonomy and comparison of mobile cloud computing application models			mobile cloud computing;taxonomy (general)		2016	Wireless Personal Communications	10.1007/s11277-016-3339-0	cloud computing;computer science;theoretical computer science;cloud testing;distributed computing;world wide web	Mobile	-33.40550556468919	58.433227235890264	174169
be42e7f9de8b8edf74c67fbf0d194791de9e3b75	wide area resource allocation policy based on routing behaviors for computational grids	resource allocation		computation;routing	Sangjin Jeong;Byungsang Kim;Dong Su Nam;Chan-Hyun Youn;Hyoung-Jun Kim	2003			computer science;resource allocation;distributed computing	HPC	-29.19502244373972	47.22182235281575	174793
76fa8241c10e2498b01de46f439542c3d36143fd	deploying on the grid with deployware	virtual machine;deployment;components grid computing deployment middleware distributed software systems;distributed software systems;components;heterogeneous software systems;software systems;large scale;large scale infrastructures;metamodel;grid infrastructure grid computing deployware distributed software systems heterogeneous software systems large scale infrastructures heterogeneity physical infrastructures software composing metamodel virtual machine;software composing;grid infrastructure;deployware;physical infrastructures;middleware;peer to peer computing middleware grid computing software systems application software large scale systems scalability virtual machining concrete software libraries;grid computing;early detection;heterogeneity	In this paper, we present DeployWare to address the deployment of distributed and heterogeneous software systems on large scale infrastructures such as grids. Deployment of software systems on grids raises many challenges like: 1) the complexity to take into account orchestration of all the deployment tasks and management of software dependencies; 2) the heterogeneity of both physical infrastructures and software composing the system to deploy; 3) the validation to early detect errors before concrete deployments; and 4) scalability to tackle thousands of nodes. To address these challenges, DeployWare provides a metamodel that abstracts concepts of the deployment, a virtual machine that executes deployment processes on grids from DeployWare descriptions, and a graphical console that allows to manage deployed systems, at runtime. To validate our approach, we have experimented DeployWare with a lot of software technologies, such as CORBA and SOA-based systems, on one thousand of nodes of Grid'5000, the french experimental grid infrastructure.	common object request broker architecture;metamodeling;run time (program lifecycle phase);scalability;service-oriented architecture;software deployment;software system;virtual machine	Areski Flissi;Jérémy Dubus;Nicolas Dolet;Philippe Merle	2008	2008 Eighth IEEE International Symposium on Cluster Computing and the Grid (CCGRID)	10.1109/CCGRID.2008.59	metamodeling;deployment diagram;computer science;virtual machine;heterogeneity;operating system;middleware;database;distributed computing;software deployment;grid computing;software system	HPC	-31.886213782718116	49.11499114022906	175098
24c7c6551c9ee0b2c11722f3e7ca679752def362	an adaptive i/o load distribution scheme for distributed systems	distributed algorithms;distributed system;object storage device adaptive i o load distribution scheme input output programs distributed systems heterogeneous machines node capabilities static workload distribution scheme pvfs2 distributed file system;heterogeneous machines;performance evaluation;resource allocation distributed algorithms input output programs;resource allocation;input output programs;layout;system performance;adaptive load distribution;pvfs2 distributed file system;servers;monitoring;adaptive load distribution parallel file system;static workload distribution scheme;parallel file system;load distribution;distributed file system;object storage device;node capabilities;strips;large scale distributed systems;distributed systems;adaptive i o load distribution scheme;distributed computing performance loss large scale systems file systems performance gain computer networks parallel processing helium adaptive systems telecommunication traffic;throughput	A fundamental issue in a large-scale distributed system consisting of heterogeneous machines which vary in both I/O and computing capabilities is to distribute workloads with respect to the capabilities of each node to achieve the optimal performance. However, node capabilities are often not stable due to various factors. Simply using a static workload distribution scheme may not well match the capability of each node. To address this issue, we distribute workload adaptively to the change of system node capability. In this paper we present an adaptive I/O load distribution scheme to dynamically capture the I/O capabilities among system nodes and to predictively determine an suitable load distribution pattern. A case study is conducted by applying our load distribution scheme into a popular distributed file system PVFS2. Experiments results show that our adaptive load distribution scheme can dramatically improve the performance: up to 70% performance gain for writes and 80% for reads, and up to 63% overall performance loss can be avoided in the presence of an unstable Object Storage Device (OSD).	clustered file system;control theory;dce distributed file system;disk mirroring;distributed computing;experiment;heterogeneous computing;input/output;load balancing (computing);object storage;parallel virtual file system;raid	Xin Chen;Jeremy Langston;Xubin He;Fengjiang Mao	2010	2010 IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum (IPDPSW)	10.1109/IPDPSW.2010.5470787	parallel computing;real-time computing;computer science;distributed computing	HPC	-20.753765018840053	52.15438362026031	175132
23f13512d8a14c72c417c7a4d027b533ce80ba4b	application-objected workflow management system based on abstract service	scientific application;extensible markup language;relational data;workflow management;complexity theory;management system;abstract workflow;application management;abstract service;probability density function;scientific workflow;management control;xml grid computing workflow management software;web service;data mining;workflow application management system;large scale;visualization;extensible markup language application objected workflow management system scientific workflow grid environment orchestration language abstract combination service group web service center abstract workflow graphical representation requirement description document xml grammar workflow application management system grid platform control flow data flow;execution environment;graphical representation;web services;abstract combination service;control flow;application objected workflow management system;grid environment;xml;abstract service grid computing;orchestration language;xml grammar;workflow management software web services grid computing concrete large scale systems control systems computer interfaces computer networks computer applications computer science;workflow management software;requirement description document;workflow management system;grid platform;data flow;similarity function;grid computing;group web service center;concrete	Since numerous large-scale scientific applications executed on Grids are expressed as complex scientific workflows, workflow management has appeared as one of the most important service in Grid environment in past few years. However, most of the management systems do not fully hide the complexity of the underlying orchestration language. Meanwhile, Grid environment is dynamic when each scientist involves in different requirements to submit their computational jobs to heterogeneous executing environment. Therefore, in order to solve the issues of complexity, heterogeneity and dynamism, in this paper we developed Application- objected Workflow Management System based on Abstract Service. The Abstract Service is an abstract combination service for a group web service with similar function. And we presented Service Center, which contains a large number of abstract services to form a virtual uniform view to shield the diversity among various web services. We also developed a visual workflow designer, which allows scientists to orchestrate an abstract workflow by just dragging or dropping the graphical representations of abstract services to (or from) the canvas. Especially, the Requirement Description Document (RDD) was proposed to describe the special demands of individual computational jobs in XML grammar. And we implemented a Workflow Application Management System (WFAMS), which translates abstract workflow in well-defined order to concrete workflow, and manages, controls and decomposes the workflow, and then combines the scientist’s relational data and submits workflow to job pool in Grid platform. Finally, the solutions to the key problems in the workflow management system are considered, including the definition of abstract workflow, the mechanism to support failure handling, the workflow template and the mechanism to isolate the control flow and data flow.	aggregate data;application lifecycle management;complexity;control flow;dataflow;drag and drop;experiment;graphical user interface;management system;requirement;web service;xml	Jiazao Lin;Zhili Zhao;Shoubo Li;Huajian Zhang;Lifen Li;Caihong Li;Lian Li	2009	2009 Eighth International Conference on Grid and Cooperative Computing	10.1109/GCC.2009.32	web service;workflow;xml;xpdl;computer science;data mining;database;windows workflow foundation;world wide web;workflow management system;workflow engine;workflow technology	HPC	-31.48095268119269	49.998456179789216	175254
56b960c342d2355ed8d9ffbcdf4bee44800414af	the design and implementation of an extensible, user-management service for communications applications	databases;extensible user management service;communication service providers;costs runtime customer service;servers;general solution;design and implementation;synchronization;business;communications applications;network administration organizations;xml;telecommunication services telecommunication network management;telecommunication services;redundant data entry;redundant data entry extensible user management service communications applications communication service providers network administration organizations;object oriented modeling;communication service;telecommunication network management;java	Many network administration organizations, including communication service providers, are faced with the problem of managing a growing number of applications with fewer administrators. Administrators are required to provision and administer application-specific data in order for users (e.g., subscribers) to access and use communication applications. Although there are several approaches for solving the problem of sharing identity data across applications, a general solution for provisioning and sharing the complete user or subscriber profile has not been defined. This article describes the design and implementation of an extensible, user-management service that solves the problem of the redundant data entry required in provisioning and maintaining multiple communication applications. An important aspect of the design is the capability of the user-management service to be extended at run time without requiring an upgrade of the service and without impacting already deployed applications. Key lessons that can be applied to reduce the time and cost for other organizations faced with implementing user- or subscriber-management projects also are described.	provisioning;run time (program lifecycle phase)	Deborah Hill	2009	IEEE Communications Magazine	10.1109/MCOM.2009.4907415	synchronization;xml;telecommunications;computer science;telecommunications service;operating system;service design;distributed computing;java;world wide web;server;computer network	Networks	-30.56088481879016	56.13579343240347	175299
2bcef8a6da745add45e5a67fab43517365908c0a	gridp2p: resource usage in grids and peer-to-peer systems	computers;long period;resource discovery;cpu intensive applications;application software;application management;availability;peer to peer systems;peer to peer computing internet grid computing resource management application software distributed computing power system management central processing unit computer network management power system modeling;resource manager;overlay network management;resource management;distributed computing;usage semantics gridp2p resource usage peer to peer systems computer technology internet cpu intensive applications grid systems distributed cycle sharing remote idle cycles application management job creation scheduling resource discovery security policy overlay network management history based policy;peer to peer system;usage semantics;remote idle cycles;internet;engines;distributed cycle sharing;power system management;scheduling;open access;computer network management;overlay network;gridp2p;grid systems;security of data grid computing internet peer to peer computing scheduling;peer to peer computing;resource usage;power system modeling;peer to peer;security;security policy;grid computing;configuration management;history based policy;computer technology;security of data;grid system;central processing unit;job creation	The last few years have witnessed huge growth in computer technology and available resources throughout the Internet. These resources can be used to run CPU-intensive applications requiring long periods of processing time. Grid systems allow us to take advantage of available resources lying over a network. However, these systems impose several difficulties to their usage (e.g. heavy authentication and configuration management); in order to overcome them, Peer-to-Peer systems provide open access making the Grid available to any user. Our solution consists of a platform for distributed cycle sharing which attempts to combine Grid and Peer-to-Peer models. A major goal is to allow any ordinary user to use remote idle cycles in order to speedup commodity applications. On the other hand, users can also provide spare cycles of their machines when they are not using them. Our solution encompasses the following functionalities: application management, job creation and scheduling, resource discovery, security policies, and overlay network management. The simple and modular organization of this system allows that components can be changed at minimum cost. In addition, the use of history-based policies provides powerful usage semantics concerning the resource management.	application lifecycle management;authentication;central processing unit;computer;configuration management;data rate units;denotational semantics;download;downstream (software development);grid systems corporation;grid computing;internet;job stream;overlay network;peer-to-peer;pre-installed software;scalability;scheduling (computing);software deployment;speedup;throughput;upload;virtual organization;xml	Sérgio Esteves;Luís Veiga;Paulo José Azevedo Vianna Ferreira	2010	2010 IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum (IPDPSW)	10.1109/IPDPSW.2010.5470917	computer science;database;distributed computing;world wide web	HPC	-27.399427375894096	54.833887574211055	175392
1dff0f67ce6e3e9b4bd37f8ceb416c4da0be5282	some considerations about distributed data bases on public networks			database	Radu Popescu-Zeletin;Herbert Weber	1980			theoretical computer science;distributed algorithm;distributed computing;computer science	ML	-28.587063617215797	46.99456152755503	175447
a5a106b6cbd7c8267c060b6ea094b61247210355	modelnet: towards a datacenter emulation environment	protocols;virtualization;sophisticated infrastructure software;multicore architectures;network protocol;network emulation;high capacity networks;virtualization modelnet data center emulation environment network emulator large scale experimentation network protocols high capacity networks sophisticated infrastructure software complex network load multicore architectures;complex network;biological system modeling;emulation;computer networks;computer centres;modelnet;large scale;network protocols;data center emulation environment;computational modeling;virtual machines;network emulator;emulation large scale systems testing next generation networking protocols application software computer networks complex networks multicore processing computer architecture;next generation;bandwidth;peer to peer computing;networked systems;complex network load;virtual machines computer centres computer networks protocols;large scale experimentation;operating systems	ModelNet is a network emulator designed for repeatable, large-scale experimentation with real networked systems. This talk introduces the ideas behind ModelNet that have made it a successful experimental platform. Beyond these core concepts, the talk highlights the latest additions to our methodology to test the next generation of network protocols and applications. Many of these developments address the datacenter compute environment: high-capacity networks, sophisticated infrastructure software (storage and virtualization), and complex network load. While these efforts significantly extend ModelNet's capabilities, there remain a number of open challenges, including incorporating new performance objectives (energy) and multicore architectures.	communications protocol;complex network;data center;emulator;multi-core processor;next-generation network	Kashi Venkatesh Vishwanath;Amin Vahdat;Ken Yocum;Diwaker Gupta	2009	2009 IEEE Ninth International Conference on Peer-to-Peer Computing	10.1109/P2P.2009.5284497	communications protocol;real-time computing;computer science;operating system;distributed computing;computer network	HPC	-27.424990002380486	56.519221832833935	175604
fc7a5ba8a3067eca07bd9e1a82b2cf265263f9b2	a cloud provisioning system for deploying complex application services	software;virtual machine;virtual machines instances;virtual machining;provisioning;home appliances;cloud provisioning system;it costs;computer architecture;servers;complex application services;composite appliance cloud computing provisioning virtual machine;virtual machines instances cloud provisioning system complex application services cloud computing on demand provisioning capability it costs cloud infrastructure composite appliance 3 tier application services manual intervention;3 tier application services;home appliances servers software cloud computing virtual machining hardware computer architecture;composite appliance;cloud infrastructure;manual intervention;on demand provisioning capability;cloud computing;hardware	Cloud Computing, with its on-demand provisioning capability on shared resources, has emerged as a new paradigm for reducing IT costs. In this paper, we present the architecture of a provisioning system that simplifies the deployment of complex application services on a Cloud infrastructure. We will introduce the concept of Composite Appliance and explain how it can be implemented and utilized to simplify management tasks and to reduce costs. We illustrate the extensibility and advantages of our design with a prototype solution consisting of a 3-tier application services that are deployed and configured automatically without manual intervention on a set of virtual machines instances in a Cloud.	business analytics;cloud computing;extensibility;multitier architecture;programming paradigm;prototype;provisioning;requirement;simpletext;software deployment;system administrator;system deployment;text-based (computing);virtual machine;xml schema	Trieu C. Chieu;Ajay Mohindra;Alexei A. Karve;Alla Segal	2010	2010 IEEE 7th International Conference on E-Business Engineering	10.1109/ICEBE.2010.40	embedded system;real-time computing;cloud computing;computer science;operating system;provisioning	HPC	-30.800682366671417	55.503195702265074	175605
39631266f984191a8c8d5571c0eef44b6b0bb76b	integration of grid cost model into iss/viola meta-scheduler environment	cost function;parallel machines;cost model	The Broker with the cost function model of the ISS/VIOLA Meta-Scheduling System implementation is described in details. The Broker includes all the algorithmic steps needed to determine a well suited machine for an application component. This judicious choice is based on a deterministic cost function model including a set of parameters that can be adapted to policies set up by computing centres or application owners. All the quantities needed for the cost function can be found in the DataWarehouse, or are available through the schedulers of the different machines forming the Grid. An ISS-Simulator has been designed to simulate the real-life scheduling of existent clusters and to virtually include new parallel machines. It will be used to validate the cost model and to tune the different free parameters.	meta-scheduling;scheduling (computing)	Ralf Gruber;Vincent Keller;Michela Thiémard;Oliver Wäldrich;Philipp Wieder;Wolfgang Ziegler;Pierre Manneback	2006		10.1007/978-3-540-72337-0_21	parallel computing;real-time computing;simulation;computer science;operating system;database;distributed computing;loss function	HPC	-19.30801196535464	60.15813549445685	175732
82fb5c75593905470c46ca9f35ae9a8ce9f1f3ec	the medusa project: autonomous data management in a shared-nothing parallel database machine	data management;parallel databases	"""speculated about the need and design of """" Information Refineries """" , machines capable of taking massive amounts of data and converting it into knowledge. In order to investigate the design of a database machine which would be capable of acting as the data storage engine of Gelemter's """" Information Refineries """" the authors initiated the MEDUSA Project as a joint undertaking between the Hardware Architecture The current MEDUSA prototype utilises a shared-nothing architecture based on the INMOS Transputer. Each of the three processing nodes used in the prototype consists of two T805 transputers with a T222 SCSI interface to a Maxtor 180 MByte disk unit as shown in figure 1. Principle Goals of the Project The principle goals of the MEDUSA Project are to develop a prototype database machine based on a shared-nothing architecture using low cost """" off the shelf """" components to SUpport research in the following areas: autonomous data management user data interfaces; backup and security systems. If the full potential of these machines is to be exploited they must exhibit a level of operational autonomy similar to the existing file server technology used on local area networks. That is, they should be capable of seamless integration into a network without requiring changes to existing software development practices or additional specialised staff to maintain their operational efficiency. Operational autonomy can be achieved by a self-organising or self-tuning database, In a self-organising database environment the database management system (DBMS) is responsible for the system tuning and data re-organisation, to achieve optimal performance. Human intervention in the system's maintenance task is reduced to no more than the changing of backup media or carrying out hardware maintenance. The traditional approach taken to performance tuning is normally heuristically bas~ with the DBMS providing the administrator with a number of tuning parameters which are adjusted based on the DBAs experience. The lack of an experienced DBA forces most sites to rely on intuition or guesswork the latter being more common. so L Interface An SQL interface, MedusaSQL, is available for data retrieval on MEDUSA. Currently, a reduced version of SQL92 standard is operational. This version allows retrieval of selected attribute values from tuples which meet criteria specified in the WHERE clause. Simple projections, joins and selections are possible using the features of the SELECT, FROM and WHERE clauses implemented thus far. Full implementation of SQL92 is planned to be completed by …"""	autonomous robot;backup;computer data storage;data retrieval;database engine;database machine;file server;heuristic;medusa4;parallel database;pareto efficiency;performance tuning;prototype;scsi;sql;sql-92;seamless3d;select (sql);self-organization;self-tuning;server (computing);shared nothing architecture;software development;transputer	George M. Bryan;Wayne E. Moore;B. J. Curry;K. W. Lodge;J. Geyer	1994		10.1145/191839.191955	data management;computer science;data mining;database;distributed computing	DB	-20.745402788069626	52.09782283188594	175889
544106be73fc1e5941348efa387a4232afb03c3e	optimization problems in the implementation of distributed mergesort on networked computers.	optimization problem;network computing			Zhivko Prodanov Nedev;Tao Gong;Bryan Hill	2004			parallel computing;computer science;theoretical computer science;distributed computing	HPC	-29.24245400366998	46.848876469667466	175909
926fb4cce27ff83aa104b881088756f20c59811a	the seamless peer and cloud evolution framework	evolutionary computation;distributed computing	Evolutionary algorithms are increasingly being applied to problems that are too computationally expensive to run on a single personal computer due to costly fitness function evaluations and/or large numbers of fitness evaluations. Here, we introduce the Seamless Peer And Cloud Evolution (SPACE) framework, which leverages bleeding edge web technologies to allow the computational resources necessary for running large scale evolutionary experiments to be made available to amateur and professional researchers alike, in a scalable and cost-effective manner, directly from their web browsers. The SPACE framework accomplishes this by distributing fitness evaluations across a heterogeneous pool of cloud compute nodes and peer computers. As a proof of concept, this framework has been attached to the \hbox{RoboGen\texttrademark} open-source platform for the co-evolution of robot bodies and brains, but importantly the framework has been built in a modular fashion such that it can be easily coupled with other evolutionary computation systems.	analysis of algorithms;computational resource;evolution;evolutionary algorithm;evolutionary computation;experiment;fitness function;open-source software;personal computer;scalability;seamless3d	Guillaume Leclerc;Joshua Evan Auerbach;Giovanni Iacca;Dario Floreano	2016		10.1145/2908812.2908886	mathematical optimization;simulation;computer science;artificial intelligence;theoretical computer science;machine learning;distributed computing;evolutionary computation	Web+IR	-28.70467919440971	54.42599278500406	176264
1c338b67d745a793f675a79c90132ac1722e507a	towards autonomic virtual applications in the in-vigo system	autonomic virtual applications;performance guarantee;autonomic component;job rescheduling;grid resource;information systems;history;fault tolerant;grid environments;application software;application management;availability;nondedicated resource sharing;resource allocation;storage management;object oriented programming;automatic programming;system performance;cpu intensive job;system recovery automatic programming fault tolerant computing grid computing learning artificial intelligence middleware object oriented programming resource allocation storage management;autonomic virtual application manager;fault tolerant computing;system recovery;application centric middleware component;in vigo system;fault tolerance;performance model;middleware;predictive models;memory based learning algorithm;rescheduling decision;resource availability;failure recover;memory based learning;learning artificial intelligence;grid computing history mesh generation application software fault tolerance predictive models quality of service information systems middleware availability;quality of service;dynamic adaptation;mesh generation;grid computing;application execution history;job rescheduling autonomic virtual applications in vigo system grid environments nondedicated resource sharing performance guarantee application centric middleware component failure recover dynamic adaptation resource availability fault tolerance system performance autonomic component grid resource application execution history rescheduling decision failure prediction autonomic virtual application manager cpu intensive job memory based learning algorithm;failure prediction	Grid environments enable users to share nondedicated resources that lack performance guarantees. This paper describes the design of application-centric middleware components to automatically recover from failures and dynamically adapt to grid environments with changing resource availabilities, improving fault-tolerance and performance. The key components of the application-centric approach are a global per-application execution history and an autonomic component that tracks the performance of a job on a grid resource against predictions based on the application execution history, to guide rescheduling decisions. Performance models of unmodified applications built using their execution history are used to predict failure as well as poor performance. A prototype of the proposed approach, an autonomic virtual application manager (AVAM), has been implemented in the context of the In-VIGO grid environment and its effectiveness has been evaluated for applications that generate CPU-intensive jobs with relatively short execution times (ranging from tens of seconds to less than an hour) on resources with highly variable loads - a workload generated by typical educational usage scenarios of In-VIGO-like grid environments. A memory-based learning algorithm is used to build the performance models for CPU-intensive applications that are used to predict the need for rescheduling. Results show that In-VIGO jobs managed by the AVAM consistently meet their execution deadlines under varying load conditions and gracefully recover from unexpected failures	algorithm;autonomic computing;autonomic networking;central processing unit;fault tolerance;instance-based learning;job stream;middleware;prototype	Jing Xu;Sumalatha Adabala;José A. B. Fortes	2005	Second International Conference on Autonomic Computing (ICAC'05)	10.1109/ICAC.2005.62	fault tolerance;real-time computing;computer science;operating system;database;distributed computing;computer performance	HPC	-23.17556597881241	60.37487212424513	176683
237ba5cda1f0fb8f9562664797eb142ff9ec3257	grid-level computing needs pervasive debugging	grid computing;program debugging;software architecture;software portability;ubiquitous computing;distributed debugging tools;distributed systems;extensibility;grid-level computing;hierarchical scalable architecture;parallel systems;pervasive debugging;portability	Developing applications for parallel and distributed systems is hard due to their nondeterministic nature; developing debugging tools for such systems and applications is even harder. A number of distributed debugging tools and techniques exist; however, we believe that they lack the infrastructure to scale to large-scale distributed systems, systems with hundreds and thousands of nodes, such as grids. In this paper, we introduce PDB, our prototype debugger, which is based on a hierarchical, scalable architecture. We explain the design of the PDB, highlight its functionality, and demonstrate its usability with two case studies. Before concluding, we discuss portability and extensibility issues for PDB, and discuss some solutions.	debugger;debugging;distributed computing;extensibility;nondeterministic algorithm;protein data bank;prototype;scalability;software portability;usability	Rashid Mehmood;Jon A Crowcroft;Steven Hand;Steven A F Smith	2005	The 6th IEEE/ACM International Workshop on Grid Computing, 2005.		software portability;software architecture;computer architecture;parallel computing;uncertainty principle;computer science;operating system;distributed computing;ubiquitous computing;grid computing	HPC	-30.87596937708225	48.71610538035955	177093
14756efe369ba31bb883c64b4c2fb487c61d4b9a	the net’s going green: multipronged approach might save costs, energy — and the climate	autonegation;internet efficiency;autonegation green computing internet efficiency;ethernet energy efficient internet usage climate savers computing initiative project green grid project;energy efficient;ethernet;climate savers computing initiative project;best practice;green grid project;energy use;telecommunication network management energy consumption internet local area networks;internet;energy consumption;communication channels;costs internet energy efficiency ip networks computer industry spine handheld computers grid computing cultural differences global communication;local area networks;energy efficient internet usage;green computing;telecommunication network management	This article deals with effort to reduce the Internet's energy usage. The Energy Efficient Internet Project explores the best ways to reduce energy use at the network's edge. The computer industry's most powerful companies are beginning a massive push to make the entire Internet efficient, from the backbone to the BlackBerry. Some of the work, such as the Climate Savers Computing Initiative (www.climatesaverscomputing.org) and the Green Grid (www.thegreen grid.org), are still in their infancy, having been recently formed. However, each is extremely earnest about accomplishing the task of squeezing the most efficiency possible out of its selected piece of the network puzzle. The next step is to define the technical and cultural best practices and communications channels.	best practice;blackberry;channel (communications);grid.org;internet backbone	Greg Goth	2008	IEEE Internet Computing	10.1109/MIC.2008.22	local area network;green computing;the internet;simulation;computer science;operating system;database;distributed computing;efficient energy use;law;world wide web;computer security;ethernet;best practice;computer network;channel	Metrics	-32.68678931927416	56.40472539274559	177609
8c1eb7aed124413525731af39d0249ce3663588e	high-performance virtual machine migration framework for mpi applications on sr-iov enabled infiniband clusters		High-speed interconnects (e.g. InfiniBand) have been widely deployed on modern HPC clusters. With the emergence of HPC in the cloud, high-speed interconnects have paved their way into the cloud with recently introduced Single Root I/O Virtualization (SR-IOV) technology, which is able to provide efficient sharing of high-speed interconnect resources and achieve near-native I/O performance. However, recent studies have shown that SR-IOV-based virtual networks prevent virtual machine migration, which is an essential virtualization capability towards high availability and resource provisioning. Although several initial solutions have been pro- posed in the literature to solve this problem, our investigations show that there are still many restrictions on these proposed approaches, such as depending on specific network adapters and/or hypervisors, which will limit the usage scope of these solutions on HPC environments. In this paper, we propose a high-performance virtual machine migration framework for MPI applications on SR-IOV enabled InfiniBand clusters. Our proposed method does not need any modification to the hypervisor and InfiniBand drivers and it can efficiently handle virtual machine (VM) migration with SR-IOV IB device. Our evaluation results indicate that the proposed design is able to not only achieve fast VM migration speed but also guarantee the high performance for MPI applications during the migration in the HPC cloud. At the application level, for NPB LU benchmark running inside VM, our proposed design is able to completely hide the migration overhead through the computation and migration overlapping. Furthermore, our proposed design shows good scaling when migrating multiple VMs.	benchmark (computing);cloud computing;computation;electrical connection;emergence;giove;high availability;hypervisor;image scaling;infiniband;lu decomposition;openvms;overhead (computing);provisioning;single-root input/output virtualization;virtual machine	Jie Zhang;Xiaoyi Lu;Dhabaleswar K. Panda	2017	2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)	10.1109/IPDPS.2017.43	virtualization;parallel computing;computer science;full virtualization;distributed computing;cloud computing;high availability;hypervisor;provisioning;virtual machine;infiniband	HPC	-21.386680803969604	59.55597957453936	177640
8d1d9625546c93ded12f5cf46326a390f9b815d5	neptune: scalable replication management and programming support for cluster-based network services	data replication;consistency model;middleware;network services	Previous research has addressed the scalability and availability issues associated with the construction of cluster-based network services. This paper studies the clustering of replicated services when the persistent service data is frequently updated. To this end we propose Neptune, an infrastructural middleware that provides a flexible interface to aggregate and replicate existing service modules. Neptune accommodates a variety of underlying storage mechanisms, maintains dynamic and location-transparent service mapping to isolate faulty modules and enforce replica consistency. Furthermore, it allows efficient use of a multi-level replica consistency model with staleness control at its highest level. This paper describes Neptune’s overall architecture, data replication support, and the results of our performance evaluation.	aggregate data;cluster analysis;consistency model;middleware;neptune;performance evaluation;replication (computing);scalability;self-replicating machine	Kai Shen;Tao Yang;Lingkun Chu;JoAnne Holliday;Douglas A. Kuschner;Huican Zhu	2001			real-time computing;computer science;database;distributed computing	OS	-24.45848410366027	52.25051611419402	177666
13f5ca3df44a82cfe6674626815033d9bbbb7e15	context-oriented programming for customizable saas applications	institutional repositories;fedora;customization;maintenance cost;multi tenancy;web service;vital;context oriented programming;software as a service;vtls;ils;google app engine	Software-as-a-Service (SaaS) applications are multi-tenant software applications that are delivered as highly configurable web services to individual customers, which are called tenants in this context. For reasons of complexity management and to lower maintenance cost, SaaS providers maintain and deploy a single version of the application code for all tenants. As a result, however, custom-made extensions for individual tenants cannot be efficiently integrated and managed. In this paper we show that by using a context-oriented programming model, cross-tier tenant-specific software variations can be easily integrated into the single-version application code base. Moreover, the selection of which variations to execute can be configured on a per tenant basis. Concretely, we provide a technical case study based on Google App Engine (GAE), a cloud platform for building multi-tenant web applications. We contribute by showing: (a) how ContextJ, a context-oriented programming (COP) language, can be used with GAE, (b) the increase in flexibility and customizability of tenant-specific software variations using ContextJ as compared to Google's dependency injection framework Guice, and (c) that the performance of using ContextJ is comparable to Guice. Based on these observations, we come to the conclusion that COP can be helpful for providing software variations in SaaS.	bespoke;cloud computing;dependency injection;emoticon;google app engine;google guice;multitenancy;multitier architecture;programming model;software as a service;web application;web service	Eddy Truyen;Nicolás Cardozo;Stefan Walraven;Jorge Vallejos;Engineer Bainomugisha;Sebastian Günther;Theo D'Hondt;Wouter Joosen	2012		10.1145/2245276.2245358	web service;computer science;operating system;software engineering;multitenancy;software as a service;database;world wide web;computer security	SE	-27.074650736996755	58.837924236869384	177683
3cc2336cb701ab40273d0b5603064a70a209b4c6	dare: high-performance state machine replication on rdma networks	reliability;performance;replicated state machine;rdma	The increasing amount of data that needs to be collected and analyzed requires large-scale datacenter architectures that are naturally more susceptible to faults of single components. One way to offer consistent services on such unreliable systems are replicated state machines (RSMs). Yet, traditional RSM protocols cannot deliver the needed latency and request rates for future large-scale systems. In this paper, we propose a new set of protocols based on Remote Direct Memory Access (RDMA) primitives. To asses these mechanisms, we use a strongly consistent key-value store; the evaluation shows that our simple protocols improve RSM performance by more than an order of magnitude. Furthermore, we show that RDMA introduces various new options, such as log access management. Our protocols enable operators to fully utilize the new capabilities of the quickly growing number of RDMA-capable datacenter networks.	data center;key-value database;remote direct memory access;response surface methodology;software bug;state machine replication	Marius Poke;Torsten Hoefler	2015		10.1145/2749246.2749267	parallel computing;real-time computing;remote direct memory access;performance;computer science;operating system;state machine replication;reliability;distributed computing;computer network	HPC	-24.29806595840981	54.03045322805341	177953
c3f21f488f4dd7c1f42a6619eae48194c411b3e4	a volunteer computing in high performance environment based on availability model	computers;software;xtremweb;volunteer computing;fractals;xtremweb platform volunteer computing high performance environment availability model distributed computing scientific research projects rescaled range analysis fractal theory time series nonlinear dynamic system node availability;xtremweb volunteer computing availability;availability;range analysis;availability model;distributed processing;distributed computing;node availability;time series;fractal dimension;computer architecture;adaptation model;time series analysis;fractal theory;high performance environment;high performance computing availability distributed computing computer networks concurrent computing educational institutions fractals personal communication networks parallel processing time series analysis;non linear dynamical systems;scientific research projects;xtremweb platform;high performance;scientific research;nonlinear dynamic system;rescaled range analysis	Volunteer computing is a form of distributed computing in which the general public volunteers processing and storage to scientific research projects. This paper proposes a novel approach based on Hurst's rescaled range analysis and fractal theory for time series are applied to study the deformation fractal characteristics of the volunteer computing for HPC project. It is found that the volunteer may be regarded as a non linear dynamic system and the fractal dimension can be used to describe the dynamic variation characteristics of the whole platform and applied to diagnose the probable problem of the node's availability. The approach has been implemented and validated on a platform XtremWeb.	distributed computing;dynamical system;fractal dimension;hurst exponent;time series;volunteer computing	Wang Yu;Zhijian Wang;Xiaofeng Zhou	2008	2008 IFIP International Conference on Network and Parallel Computing	10.1109/NPC.2008.95	parallel computing;simulation;fractal;computer science;theoretical computer science;operating system;time series;database;distributed computing;computer security;statistics	HPC	-29.16074204226924	50.698956333673934	178294
98e3d0715456379b620c6fc88ae5aa21b96e0d32	on consistency of operational transformation approach		The Operational Transformation (OT) approach, used in many collaborative editors, allows a group of users to concurrently update replicas of a shared object a nd exchange their updates in any order. The basic idea of this approach is to transform any received u pdate operation before its execution on a replica of the object. This transformation aims to ensure t he convergence of the different replicas of the object, even though the operations are executed in dif ferent orders. However, designing transformation functions for achieving convergence is a cr itical and challenging issue. Indeed, the transformation functions proposed in the literature are al l revealed incorrect.	library (computing);operational transformation;type signature	Aurel Randolph;Hanifa Boucheneb;Abdessamad Imine;Alejandro Quintero	2012		10.4204/EPTCS.107.5	computer science;theoretical computer science;data mining;mathematics;algorithm	DB	-25.311213746134097	47.296720073746116	178380
b74c84d392e3adee1ec576bb8e7cb13e96a67102	a web-based platform for publication and distributed execution of computing applications	distributed computing;application composition distributed computing web based platform web services rest api software as a service platform as a service resource integration;web service web based platform computing applications distributed execution everest publication application composition distributed computing resources web technologies cloud computing platform as a service paas cloud delivery model programming interfaces;web services programming computational modeling libraries skeleton computer architecture;application composition;web services cloud computing internet;platform as a service;web services;resource integration;software as a service;rest api;web based platform	Researchers increasingly rely on using web-based systems for accessing and running scientific applications across distributed computing resources. However existing systems lack a number of important features, such as publication and sharing of scientific applications as online services, decoupling of applications from computing resources and providing remote programmatic access. This paper presents Everest, a Web-based platform for researchers supporting publication, execution and composition of applications running across distributed computing resources. Everest addresses the described challenges by relying on modern Web technologies and cloud computing models. It follows the Platform as a Service (PaaS) cloud delivery model by providing all its functionality via remote Web and programming interfaces. Any application added to Everest is automatically published both as a user-facing Web form and a Web service. Another distinct feature of Everest is the ability to attach external computing resources by any user and flexibly use these resources for running applications. The paper provides an overview of the platform's architecture and its main components, describes recent developments, presents results of experimental evaluation of the platform and discusses remaining challenges.	application programming interface;cloud computing;composite application;coupling (computer programming);distributed computing;e-services;form (html);platform as a service;quality of service;scalability;scheduling (computing);server (computing);web application;web service	Oleg Sukhoroslov;Sergey Volkov;Alexander Afanasiev	2015	2015 14th International Symposium on Parallel and Distributed Computing	10.1109/ISPDC.2015.27	web service;web application security;web development;web application;web modeling;data web;web-based simulation;cloud computing;web standards;computer science;web api;operating system;service-oriented architecture;web navigation;cloud testing;web page;open platform;software as a service;database;distributed computing;utility computing;web 2.0;world wide web;mashup	HPC	-30.98225201911635	52.98998701703112	178385
e1e5ec934ba1d2cb308cb672fe13dbebe717ddb8	agent assisted servicebsp model in grids	globus toolkit;satisfiability;quality of service grid computing concurrent computing monitoring computational modeling computer networks distributed computing delay software libraries ontologies;ontologies artificial intelligence;software agents;software agents grid computing ontologies artificial intelligence parallel processing quality of service;qos aware service selection;agent assisted servicebsp model;bulk synchronous parallel;grid service;grid environment;parallel computer;quality of service;bulk synchronous parallelism model;ontology repository;grid computing;computational services;parallel processing;bsp development library;quality of service agent assisted servicebsp model bulk synchronous parallelism model grid environment bsp development library globus toolkit qos aware service selection computational services ontology repository	The bulk synchronous parallelism (BSP) model is useful in developing scalable software for parallel computation. To adapt the BSP model to the grid environment, the BSP-G, a BSP development library based on the services of Globus Toolkit has been proposed. In this paper, we further the work of BSP-G and introduce an agent assisted ServiceBSP model. The agent in this model could make a QoS-aware selection of computational services and monitor the status of those services. During the selection phase, the agent can handle the diversity of QoS information of services by referring to an ontology repository of quality of service (QoS). An experiment shows that the agent could provide users with satisfied supersteps by selecting Grid services on QoS demand	algorithm;bulk synchronous parallel;computation;parallel computing;quality of service;randomness;scalability	Junqiang Zhu;Weiqin Tong;Xiaojie Dong	2006	2006 Fifth International Conference on Grid and Cooperative Computing (GCC'06)	10.1109/GCC.2006.24	parallel processing;quality of service;computer science;software agent;operating system;database;distributed computing;world wide web;grid computing;bulk synchronous parallel;satisfiability	Robotics	-31.064751359306324	50.21355762405388	178538
19efe5e732345d4ca0b64d256ac7f2ad7e943fda	gatorshare: a file system framework for high-throughput data management	data intensive application;architectural design;client server architecture;desktop grids;data distribution service;voluntary computing;data management;data distribution;file system;bittorrent;middleware;high throughput;scientific research;quantitative evaluation;data transfer;data dissemination;large data;high throughput computing	Voluntary Computing systems or Desktop Grids (DGs) enable sharing of commodity computing resources across the globe and have gained tremendous popularity among scientific research communities. Data management is one of the major challenges of adopting the Voluntary Computing paradigm for large data-intensive applications. To date, middleware for supporting such applications either lacks an efficient cooperative data distribution scheme or cannot easily accommodate existing data-intensive applications due to the requirement for using middleware-specific APIs.  To address this challenge, in this paper we introduce Gator-Share, a data management framework that offers a file system interface and an extensible architecture designed to support multiple data transfer protocols, including BitTorrent, based on which we implement a cooperative data distribution service for DGs. It eases the integration with Desktop Grids and enables high-throughput data management for unmodified data-intensive applications. To improve the performance of BitTorrent in Desktop Grids, we have enhanced BitTorrent by making it fully decentralized and capable of supporting partial file downloading in an on-demand fashion.  To justify this approach we present a quantitative evaluation of the framework in terms of data distribution efficiency. Experimental results show that the framework significantly improves the data dissemination performance for unmodified data-intensive applications compared to a traditional client/server architecture.	application programming interface;bandwidth (signal processing);bittorrent;centralized computing;client–server model;commodity computing;data distribution service;data-intensive computing;download;file system api;high-throughput computing;internet;middleware;programming paradigm;server (computing);throughput;usb on-the-go	Jiangyan Xu;Renato J. O. Figueiredo	2010		10.1145/1851476.1851588	high-throughput screening;parallel computing;scientific method;bittorrent;data management;computer science;operating system;middleware;database;distributed computing;world wide web;dissemination;client–server model;computer network	HPC	-21.2276711484907	52.88012598816533	178567
1b228078960b933e5793b71eeb6bd9ea5a4e9d59	transparent structurization of parallel processes for backward recovery	consistent recovery;dynamic conversations;fault tolerant;parallel processes;dynamic backward error recovery;asynchronous process systems;logic;fault tolerant computing;fault tolerance;parallel processing fault tolerant computing;transparent dynamic structurization;backward error;fault tolerance parallel processes dynamic backward error recovery asynchronous process systems transparent dynamic structurization dynamic conversations consistent recovery resource expenditures;parallel processing;resource expenditures	Problems of dynamic backward error recovery for asynchronous process systems are considered. A new technique of recovery providing for transparent dynamic structurization of processes based on the concept of dynamic conversations is suggested. A dynamic scheme of consistent recovery based on them is proposed. To control resource expenditures on fault-tolerance, four types of control over conversations are introduced. >		Alexander Romanovsky;I. V. Sturtz	1990		10.1109/SPDP.1990.143630	parallel processing;fault tolerance;parallel computing;real-time computing;computer science;distributed computing	SE	-19.372480878983247	48.04843521131531	178591
