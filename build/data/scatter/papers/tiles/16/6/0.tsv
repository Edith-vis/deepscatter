id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
90178bc28dfa82df14f77ec6499eca48e6f5bff9	margin-space integration of mpe loss via differencing of mmi functionals for generalized error-weighted discriminative training	objective function;finite difference;generalization error	Using the central observation that margin-based weighted classification error (modeled using Minimum Phone Error (MPE)) corresponds to the derivative with respect to the margin term of margin-based hinge loss (modeled using Maximum Mutual Information (MMI)), this article subsumes and extends margin-based MPE and MMI within a broader framework in which the objective function is an integral of MPE loss over a range of margin values. Applying the Fundamental Theorem of Calculus, this integral is easily evaluated using finite differences of MMI functionals; lattice-based training using the new criterion can then be carried out using differences of MMI gradients. Experimental results comparing the new framework with margin-based MMI, MCE and MPE on the Corpus of Spontaneous Japanese and the MIT OpenCourseWare/MIT-World corpus are presented.	autoregressive integrated moving average;discriminative model;finite difference;gradient;hinge loss;loss function;mutual information;opencourseware;optimization problem;spontaneous order;tinymce	Erik McDermott;Shinji Watanabe;Atsushi Nakamura	2009			hidden markov model;hinge loss;speech recognition;hamming distance;artificial intelligence;discriminative model;generalization error;pattern recognition;computer science;support vector machine	AI	-19.733819874113188	-94.46492293868404	120727
276a81d2ad66c4a0318c63f7fa68623f6ecd0a60	discriminative training and acoustic modeling for automatic speech recognition	diskriminatives training;acoustic modeling;minimum error rate training;wortgraphen;automatic speech recognition;word lattices;akustische modellierung;informatik;discriminative training;diskriminative lernverfahren	Discriminative training has become an important means for estimating model parameters in many statistical pattern recognition tasks. While standard learning methods based on the Maximum Likelihood criterion aim at optimizing model parameters only class individually, discriminative approaches benefit from taking all competing classes into account, thus leading to enhanced class separability which is often accompanied by reduced error rates and improved system performance. Motivated by learning algorithms evolved from neural networks, discriminative methods established as training methods for classification problems such as complex as automatic speech recognition. In this thesis, an extended unifying approach for a class of discriminative training criteria is suggested that, in addition to the Maximum Mutual Information (MMI) criterion and the Minimum Classification Error (MCE) criterion, also captures other criteria more recently proposed as, for example, the Minimum Word Error (MWE) criterion and the closely related Minimum Phone Error (MPE) criterion. The new approach allows for investigating a large number of different training criteria within a single framework and thus to yield consistent analytical and experimental results about their training behavior and recognition performance. This thesis also presents the first successful implementation of a large scale, lattice-based MCE training. Experiments conducted on several speech recognition corpora show that the MCE criterion yields recognition results that are similar to or even outperform the performance gains obtained with both the MWE and the MPE criterion. The parameter optimization problem is discussed for Gaussian mixture models where the covariance matrices can be subject to arbitrary tying schemes. The re-estimation equations as well as the choice of the iteration constants for controlling the convergence rate are discussed for the case that full or diagonal covariance matrices are used. In case of full covariance matrices, the problem of choosing the iteration constants in the Extended Baum (EB) algorithm is shown to result in the solution of a quadratic eigenvalue problem. Two novel methods on setting the iteration constants are proposed that provide faster convergence rates across different variance tying schemes. This thesis also suggests a novel framework that models the posterior distribution directly as a log-linear model. The direct model follows the principle of Maximum Entropy and can effectively be trained using the Generalized Iterative Scaling (GIS) algorithm. Both the direct model and its optimization via the GIS algorithm are compared analytically and experimentally with the MMI criterion and the EB algorithm. Finally, this thesis presents a novel algorithm to efficiently compute and represent the exact and unsmoothed error surface over all sentence hypotheses that are encoded in a word lattice if all parameter settings of a log-linear model are considered that lie along an arbitrary line in the parameter space. While the number of sentence hypotheses encoded in a word lattice is exponential in the lattice size, the complexity of the error surface is shown to be always linearly bounded in the number of lattice arcs. This bound is independent of the underlying error metric. Experiments were conducted on several standardized speech recognition tasks that capture different levels of difficulty, ranging from elementary digit recognition (SieTill) over read speech (Wall Street Journal and North American Business news texts) up to broadcast news transcription tasks (Hub-4). Questions pursued in this context address the effect that different variance tying schemes have on the recognition performance and to what extent increasing the model complexity affects the performance gain of the discriminative training procedure. All experiments were carried out in the extended, unifying approach for a large number of different training criteria. Zusammenfassung Diskriminative Lernverfahren haben sich zu einem wichtigen Instrument der Parameterschätzung in vielen Mustererkennungsaufgaben entwickelt. Während konventionelle, auf dem Maximum Likelihood Prinzip basierende Verfahren die Modellparameter nur klassenindividuell schätzen, berücksichtigen diskriminative Verfahren auch klassenfremde Trainingsdaten und führen so zu einer verbesserten Klassentrennbarkeit, was sich oftmals in einer niedrigeren Fehlerrate niederschlägt. Motiviert durch Lernverfahren, die in dem Bereich der neuronalen Netze entwickelt worden sind, haben sich diskriminative Methoden inzwischen als Trainingsverfahren in komplexen Klassifikationsproblemen wie die automatische Spracherkennung etabliert. In dieser Arbeit wird ein erweiterter, vereinheitlichender Ansatz für eine Klasse diskriminativer Trainingskriterien vorgestellt, der neben dem Maximum Mutual Information Kriterium und dem Minimum Classification Error Kriterium weitere Kriterien wie zum Beispiel das Minimum Word Error Kriterium oder das hiermit nah verwandte Minimum Phone Error Kriterium umfaßt. Der neue Ansatz ermöglicht es, die genannten sowie zahlreiche weitere Kriterien in einer einheitlichen Theorie darzustellen und somit zu klaren Aussagen in der theoretischen sowie experimentellen Performanzanalyse zu kommen. In dieser Arbeit wird ferner die erste erfolgreiche Implementierung eines rein Wortgraphbasierten MCE Trainings für großes Vokabular vorgestellt. Experimente, die auf zahlreichen Spracherkennugskorpora durchgeführt wurden, zeigen, daß die mit Hilfe des MCE Kriteriums erzielten Performanzen in derselben Größenordnung liegen wie die mit dem MWE und dem MPE Kriterium erzielten Fehlerraten, beziehungsweise diese sogar zu übertreffen vermögen. Das Parameteroptimierungsproblem wird für Hidden Markov Modelle mit Gaußschen Mischverteilungsdichten formuliert, wobei die Reestimationsgleichungen als auch die Wahl der Iterationskonstanten für die Fälle diskutiert werden, daß entweder voll besetzte oder diagonale Kovarianzmatrizen verwendet werden. Die Kovarianzmatrizen können hierbei als gemeinsamer Parameter in die Mischverteilungsdichten beliebiger Zustände eingehen (sogenannte Varianz tying Schemata). Speziell für den Fall voll besetzer Kovarianzmatrizen wird gezeigt, daß die Wahl der Iterationskonstanten im erweiterten Baum (EB) Algorithmus auf die Lösung eines quadratischen Eigenwertproblemes zurückgeführt werden kann. Zwei neue Methoden zur Wahl der Iterationskonstanten werden vorgeschlagen, die unabhängig vom verwendeten Varianz tying Schema zu einer schnelleren Konvergenzrate führen als dies beispielsweise mit dem traditionellen EB Algorithmus möglich ist. In dieser Arbeit wird darüber hinaus ein neuer Ansatz vorgestellt, der eine direkte Beschreibung der Posterior-Verteilung mittels eines log-linearen Modells ermöglicht. Es wird gezeigt, daß das direkte Modell dem Prinzip der Maximalen Entropie folgt und mit Hilfe des Generalized Iterative Scaling (GIS) Algorithmus effektiv trainiert werden kann. Das direkte Modell sowie die Optimierung mittels des GIS Algorithmus werden analytisch und experimentell mit dem MMI Kriterium sowie dem EB Algorithmus verglichen. Schließlich wird in dieser Arbeit ein neuer Algorithmus vorgestellt, mit dessen Hilfe sich die exakte und ungeglättete Fehleroberfläche sämtlicher in einem Wortgraphen repräsentierten Satzhypothesen berechnen und speichern läßt, falls alle möglichen Parameterkonstellationen eines log-linearen Modells, die auf einer beliebigen Geraden im Parameterraum liegen, betrachtet werden. Während die Anzahl der in einem Wortgraphen repräsentierten Satzhypothesen exponentiell in der Größe des Wortgraphen ist, wird gezeigt, daß die Komplexität einer solchen Fehleroberfläche stets linear in der Anzahl der Kanten des Wortgraphen beschränkt ist. Diese Schranke ist unabhängig vom betrachteten Fehlermaß. Experimente wurden auf verschiedenen, standardisierten Spracherkennungskorpora mit unterschiedlichem Schwierigkeitsgrad durchgeführt, die von elementarer Verbundziffernkettenerkennung (SieTill) über gelesene Meldungen aus Writschaftsfachblättern (Wall Street Journal und North American Business News) bis hin zur automatischen Verschriftung von Rundfunknachrichten (Hub-4) reichen. Die hierbei untersuchten Fragestellugen betreffen den Einfluss verschiedener Varianzmodelle auf die Erkennungsperformanz sowie den Effekt, den eine Zunahme der Modellkomplexität auf die zu erwartende Erkennugsfehlerrate ausübt. Sämtliche Untersuchungen wurden im erweiterten, vereinheitlichenden Ansatz für eine Reihe verschiedener diskriminativer Kriterien durchgeführt.	acoustic cryptanalysis;acoustic model;artificial neural network;baum–welch algorithm;citeseerx;die (integrated circuit);discriminative model;eb-eye;eine and zwei;experiment;generalized iterative scaling;geographic information system;gesellschaft für informatik;internet explorer;iterative method;landweber iteration;line level;linear bounded automaton;linear model;linear separability;linuxmce;log-linear model;machine learning;markov chain;mathematical optimization;minimal working example;mixture model;mutual information;optimization problem;pattern recognition;principle of maximum entropy;rate of convergence;scalability;speech recognition;statistical classification;text corpus;the wall street journal;time complexity;tinymce;transcription (software);v-model	Wolfgang Macherey	2010			natural language processing;speech recognition;computer science;pattern recognition	ML	-19.72446278306903	-94.38382334782851	128312
1302319324d2ad831a5da553d03aba2c9259fc21	a digital liquid state machine with biologically inspired learning and its application to speech recognition	reservoirs;vlsi backpropagation finite state machines hidden markov models neural nets speech recognition;spike based learning hardware implementation liquid state machine lsm speech recognition;speech;neurons speech recognition reservoirs biological neural networks hidden markov models speech;hidden markov models;speech recognition;sphinx 4 digital liquid state machine speech recognition bioinspired digital lsm very large scale integration based machine learning vlsi based machine learning bioinspired spike based learning algorithm data storage presynaptic neuron postsynaptic neuron backpropagation based learning parallel vlsi implementation ti46 speech corpus spiking neural network model synaptic weight resolution isolated word recognition markov model based recognizer;neurons;biological neural networks	This paper presents a bioinspired digital liquid-state machine (LSM) for low-power very-large-scale-integration (VLSI)-based machine learning applications. To the best of the authors' knowledge, this is the first work that employs a bioinspired spike-based learning algorithm for the LSM. With the proposed online learning, the LSM extracts information from input patterns on the fly without needing intermediate data storage as required in offline learning methods such as ridge regression. The proposed learning rule is local such that each synaptic weight update is based only upon the firing activities of the corresponding presynaptic and postsynaptic neurons without incurring global communications across the neural network. Compared with the backpropagation-based learning, the locality of computation in the proposed approach lends itself to efficient parallel VLSI implementation. We use subsets of the TI46 speech corpus to benchmark the bioinspired digital LSM. To reduce the complexity of the spiking neural network model without performance degradation for speech recognition, we study the impacts of synaptic models on the fading memory of the reservoir and hence the network performance. Moreover, we examine the tradeoffs between synaptic weight resolution, reservoir size, and recognition performance and present techniques to further reduce the overhead of hardware implementation. Our simulation results show that in terms of isolated word recognition evaluated using the TI46 speech corpus, the proposed digital LSM rivals the state-of-the-art hidden Markov-model-based recognizer Sphinx-4 and outperforms all other reported recognizers including the ones that are based upon the LSM or neural networks.	algorithm;artificial neural network;backpropagation;benchmark (computing);biological neural networks;body of uterus;computation;computational complexity theory;computer data storage;cost effectiveness;elegant degradation;hidden markov model;learning rule;liquid state machine;locality of reference;low-power broadcasting;lymphoma, large-cell, follicular;machine learning;markov chain;network model;network performance;neural network simulation;offline learning;on the fly;online and offline;overhead (computing);reservoir device component;speech corpus;speech recognition;spiking neural network;subgroup;synaptic package manager;synaptic weight;very-large-scale integration	Yong Zhang;Peng Li;Yingyezhe Jin;Yoonsuck Choe	2015	IEEE Transactions on Neural Networks and Learning Systems	10.1109/TNNLS.2015.2388544	speech recognition;computer science;speech;artificial intelligence;machine learning;artificial neural network;hidden markov model;reservoir	ML	-22.158695699866172	-95.03217522045848	183283
