id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
5513af843e5cde52e72e320efb74fd1a628b2211	mining sequential features and opinion words in user generated contents				Tanvir Ahmad;Mohammad Najmud Doja	2011			computer science;data mining	AI	-24.23996035752411	-61.34788339513489	52613
ac5038d7b423e0e9649c07d64e92a9f99759a650	what men say, what women hear: finding gender-specific meaning shades	gender discrimination;context awareness;gender based word disambiguation;speech;gender issues;intelligent systems;sentiment analysis;writing;computational linguistics;twitter;blogs;time frequency analysis;affective computing	"""The authors examine the problem of gender discrimination and attempt to move beyond the typical surface-level text classification approach by identifying differences between genders in the ways they use the same words. They present several experiments using data from a large collection of blogs authored by men and women, and they report results for a new task of """"gender-based word disambiguation"""" for a set of over 350 words."""	blog;document classification;experiment;word-sense disambiguation	Rada Mihalcea;Aparna Garimella	2016	IEEE Intelligent Systems	10.1109/MIS.2016.71	natural language processing;time–frequency analysis;intelligent decision support system;computer science;speech;artificial intelligence;computational linguistics;affective computing;writing;sentiment analysis	NLP	-21.787531490677917	-59.5736453287001	52769
a97e18d4828daf5fae355196a970c69dc6ade693	discovering potential drug abuse with fuzzy sets	drugs;fuzzy set;fuzzy set theory data mining drugs;fuzzy set drug abuse web mining blog;blog;data mining;fuzzy set theory;web mining drug abuse fuzzy sets students psychotropic substance hong kong police d minerb algorithm language parsing techniques;engines;web mining;engines drugs information filters;information filters;drug abuse	According to the recent CRDA and drug statistics in Hong Kong, the age range for students abusing psychotropic substance is getting younger emphatically. In 2009, we have collaborated with the RPCO KE of the Hong Kong Police for the development of a web miner to detect information related to potential drug abusers on the web. This paper describes the D-MinerB algorithm for detecting possible patterns posted on blogs. Fuzzy set and some language parsing techniques have been adopted. For the preliminary test of the collected set, we are able to identify 21 blog posts from 472 blog posts while over 84% of the identified ones are relevant.	algorithm;blog;fuzzy set;knowledge engineering;parsing;sensor	Li Ho Leung;Vincent T. Y. Ng	2010	2010 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2010.5641896	web mining;computer science;data science;data mining;fuzzy set;world wide web	Robotics	-21.367382347880554	-56.35835406769713	52840
74927f97cdcc72ca3f66dd2f4f0d14f7a15a132c	semantic relation extraction with kernels over typed dependency trees	information extraction;text mining;relation extraction;natural language;dependency parsing;semantic relations;structured data	An important step for understanding the semantic content of text is the extraction of semantic relations between entities in natural language documents. Automatic extraction techniques have to be able to identify different versions of the same relation which usually may be expressed in a great variety of ways. Therefore these techniques benefit from taking into account many syntactic and semantic features, especially parse trees generated by automatic sentence parsers. Typed dependency parse trees are edge and node labeled parse trees whose labels and topology contains valuable semantic clues. This information can be exploited for relation extraction by the use of kernels over structured data for classification. In this paper we present new tree kernels for relation extraction over typed dependency parse trees. On a public benchmark data set we are able to demonstrate a significant improvement in terms of relation extraction quality of our new kernels over other state-of-the-art kernels.	benchmark (computing);dependency grammar;entity;information extraction;natural language;parse tree;parsing;relationship extraction	Frank Reichartz;Hannes Korte;Gerhard Paass	2010		10.1145/1835804.1835902	natural language processing;relationship extraction;semantic computing;text mining;data model;computer science;pattern recognition;database;natural language;information extraction;dependency grammar	NLP	-25.86692166704876	-65.86831027808992	52858
91be8f9102a4806ed70bfaf2d27d5e828058b250	are buildings only instances?: exploration in architectural style categories	mining;discovery;classification;architecture	Instance retrieval has emerged as a promising research area with buildings as the popular test subject. Given a query image or region, the objective is to find images in the database containing the same object or scene. There has been a recent surge in efforts in finding instances of the same building in challenging datasets such as the Oxford 5k dataset [19], Oxford 100k dataset and the Paris dataset [20].  We ascend one level higher and pose the question: Are Buildings Only Instances? Buildings located in the same geographical region or constructed in a certain time period in history often follow a specific method of construction. These architectural styles are characterized by certain features which distinguish them from other styles of architecture. We explore, beyond the idea of buildings as instances, the possibility that buildings can be categorized based on the architectural style. Certain characteristic features distinguish an architectural style from others. We perform experiments to evaluate how characteristic information obtained from low-level feature configurations can help in classification of buildings into architectural style categories. Encouraged by our observations, we mine characteristic features with semantic utility for different architectural styles from our dataset of European monuments. These mined features are of various scales, and provide an insight into what makes a particular architectural style category distinct. The utility of the mined characteristics is verified from Wikipedia.	ascend;categorization;experiment;high- and low-level;mined;wikipedia	Abhinav Goel;Mayank Juneja;C. V. Jawahar	2012		10.1145/2425333.2425334	engineering;artificial intelligence;data science;data mining	Vision	-25.34109394000843	-52.50263845658923	52872
54be24fdc1acddc572790419b127209f54606496	correction de césures et enrichissement de requêtes pour la recherche de livres		Digitized books are now a common source of information on the Web, however OCR sometimes introduces errors that can penalize Information Retrieval. In this paper we propose a method for correcting hyphenations and we analyse its impact on a standard book retrieval task. We also experiment query expansion with words extracted from the Wikipedia page related to the query. We show that there is a significant improvement over the state-of-the-art when using a large weighted list of words. MOTS-CLÉS : Livres numérisés, césures, enrichissement de requête, Wikipédia.	book;information retrieval;information source;linear algebra;query expansion;tag cloud;wikipedia;world wide web	Romain Deveaud;Florian Boudin;Eric SanJuan;Patrice Bellot	2011		10.24348/coria.2011.89	query expansion;information retrieval;computer science	NLP	-29.996797150449773	-64.43499923173732	52873
fcb9629e9c9b50f3d879da68201ffa013d0f014f	a phased ranking model for question answering	answer ranking;learning to rank;question answering	We describe a general result ranking approach for multi-phase, multi strategy information systems, which has been applied to the task of question answering (QA). Many information systems incorporate multiple steps and each step or phase may incorporate multiple component algorithms to achieve acceptable robustness and overall performance. Such systems may produce and rank a large number of candidate results. Prior work includes many models that rank a particular type of information object (e.g. a retrieved document, a factoid answer) using features specific to that information type, without attempting to make use of other non-local features (e.g. features of the upstream information source). We propose an approach that allows each phase in a system to leverage information propagated from preceding phases to inform the ranking decision. This is accomplished by a system object graph which represents all of the objects created during system execution, object dependencies (e.g. provenance), and ranking feature values extracted for a specific object. We evaluate the effectiveness of the proposed ranking approach in a multi-phase question answering system built by recombining pre-existing software modules. Experimental results show that our proposed approach significantly outperforms comparable answer ranking models.	algorithm;information source;information system;object graph;question answering;software quality assurance	Rui Liu;Eric Nyberg	2013		10.1145/2505515.2505678	ranking;question answering;computer science;machine learning;data mining;database;ranking svm;world wide web;information retrieval;learning to rank	Web+IR	-29.602105676654386	-59.95170080829862	52883
be70790da7d9b75a83ff739912d8898651b1b17d	on near-uniform url sampling	random graph;web graph;search engine;probability;web pages;search engines;resource allocation;internet domain distribution;test bed;search engine size;random walks;sampling;html;random walk;uniform resource locator;indexation;random processes;url sampling;distributed search;world wide web;markov processes	We consider the problem of sampling URLs uniformly at random from the Web. A tool for sampling URLs uniformly can be used to estimate various properties of Web pages, such as the fraction of pages in various Internet domains or written in various languages. Moreover, uniform URL sampling can be used to determine the sizes of various search engines relative to the entire Web. In this paper, we consider sampling approaches based on random walks of the Web graph. In particular, we suggest ways of improving sampling based on random walks to make the samples closer to uniform. We suggest a natural test bed based on random graphs for testing the effectiveness of our procedures. We then use our sampling approach to estimate the distribution of pages over various Internet domains and to estimate the coverage of various search engine indexes.  2000 Published by Elsevier Science B.V. All rights reserved.	random graph;sampling (signal processing);testbed;web page;web search engine;webgraph;world wide web	Monika Henzinger;Allan Heydon;Michael Mitzenmacher;Marc Najork	2000	Computer Networks	10.1016/S1389-1286(00)00055-4	stochastic process;computer science;data mining;world wide web;random walk;information retrieval;search engine	Web+IR	-31.766234470264298	-55.78264303885382	52894
0a3c061e24a3b715d640515a25cf6f23290cb79d	spam detection on twitter using traditional classifiers	social network security;machine learning;spam detection	Social networking sites have become very popular in recent years. Users use them to find new friends, updates their existing friends with their latest thoughts and activities. Among these sites, Twitter is the fastest growing site. Its popularity also attracts many spammers to infiltrate legitimate users’ accounts with a large amount of spam messages. In this paper, we discuss some userbased and content-based features that are different between spammers and legitimate users. Then, we use these features to facilitate spam detection. Using the API methods provided by Twitter, we crawled active Twitter users, their followers/following information and their most recent 100 tweets. Then, we analyzed the collected dataset and evaluated our detection scheme based on the suggested user and content-based features. Our results show that among the four classifiers we evaluated, the Random Forest classifier produces the best results. Our results based on the 100 most recent tweets also show that spam detection based on our suggested features can achieve 95.7% precision and 95.7% Fmeasure using the Random Forest classifier.	anti-spam techniques;application programming interface;fastest;random forest;spamming;the 100	M. McCord;M. Chuah	2011		10.1007/978-3-642-23496-5_13	computer science;social spam;spambot;data mining;internet privacy;world wide web	Web+IR	-19.93273858843227	-55.251446134220394	53116
3f47ef54afb8e129fcbf4e2e89374b445c243472	similarity analysis of legal judgments	search engine;legal judgments;information retrieval;case retrieval;case citation	In this paper, we have made an effort to propose approaches to find similar legal judgements by extending the popular techniques used in information retrieval and search engines. Legal judgements are complex in nature and refer other judgements. We have analyzed all-term, legal-term, co-citation and bibliographic coupling-based similarity methods to find similar judgements. The experimental results show that the legal-term cosine similarity method performs better than all-term cosine similarity method. Also, the results show that bibliographic coupling similarity method improves the performance over co-citation approach.	bibliographic coupling;co-citation;cosine similarity;information retrieval;web search engine	Sushanta Kumar;P. Krishna Reddy;V. Balakista Reddy;Aditya Singh	2011		10.1145/1980422.1980439	computer science;data mining;world wide web;information retrieval;search engine;similarity heuristic	NLP	-32.05804035320963	-58.89066961232449	53188
b0adc537eed5db6c03f22b42edf4f01dd61937e8	graph summarization in annotated data using probabilistic soft logic		Annotation graphs, made available through the Linked Data initiative and Semantic Web, have significant scientific value. However, their increasing complexity makes it difficult to fully exploit this value. Graph summaries, which group similar entities and relations for a more abstract view on the data, can help alleviate this problem, but new methods for graph summarization are needed that handle uncertainty present within and across these sources. Here, we propose the use of probabilistic soft logic (PSL) [1] as a general framework for reasoning about annotation graphs, similarities, and the possibly confounding evidence arising from these. We show preliminary results using two simple graph summarization heuristics in PSL for a plant biology domain.	automatic summarization;cluster analysis;computer cluster;entity;graph (discrete mathematics);heuristic (computer science);ibm notes;linked data;national fund for scientific research;ontology (information science);probabilistic soft logic;semantic web	Alex Memory;Angelika Kimmig;Stephen H. Bach;Louiqa Raschid;Lise Getoor	2012			computer science;theoretical computer science;automatic summarization;machine learning;data mining;graph	NLP	-28.383815655156077	-65.18961224933825	53284
df8d7651b8a523564284241f612b6f8b502cbd20	revealing trends based on defined queries in biological publications using cosine similarity	environmental factors;biology computing;vector space model defined queries biological publications cosine similarity valuable information extraction published papers decision making text mining trendfinder content based trends expert defined queries data sets keywords title downloading published date downloading abstract downloading conservation biology ecology the american naturalist;trends in biological publications;market research;vector space model;query processing;information retrieval;vectors abstracts market research environmental factors evolution biology biodiversity;text analysis;vector space model trends in biological publications information retrieval queries cosine similarity;data mining;evolution biology;vectors;abstracts;pattern matching;vectors biology computing content based retrieval data mining decision making electronic publishing pattern matching query processing text analysis;electronic publishing;biodiversity;content based retrieval;queries;cosine similarity	"""Extracting valuable information in terms of number and content of published papers in any field of research will simplify decision making for future researches and investments. A novel and simple text mining approach, called TrendFinder, has been developed in this paper to reveal the content-based trends of expert-defined queries in selected biological published papers during the last five decades. So, in order to evaluate the results, three different data sets were collected and four vectors of selected keywords were considered as the four queries. """"Title"""", """"Published date"""" and the """"Abstract"""" were downloaded for three series of journals namely, """"Conservation Biology"""", """"Ecology"""", and """"The American Naturalist"""" as data sets, including total number of 19,010 papers. In order to show the trend between each query and the Abstract of each paper, Cosine similarity method was used by TrendFinder. Afterwards, three diagrams were demonstrated content-based trends of the four defined queries on the three provided data sets."""	cosine similarity;diagram;ecology;simpletext;text mining	Hadi Mohammadzadeh;Omid Paknia;Franz Schweiggert;Thomas Gottron	2012	2012 23rd International Workshop on Database and Expert Systems Applications	10.1109/DEXA.2012.16	market research;computer science;artificial intelligence;data science;data mining;database;electronic publishing;world wide web;vector space model;information retrieval	DB	-27.419905859633644	-57.29538273115197	53402
39210c16f77f7018007dfa20dc7a7b615b73303b	a document-centered approach to a natural language music search engine	search engine;vector space model;contextual information;natural language;web retrieval	We propose a new approach to a music search engine that can be accessed via natural language queries. As with existing approaches, we try to gather as much contextual information as possible for individual pieces in a (possibly large) music collection by means of Web retrieval. While existing approaches use this textual information to construct representations of music pieces in a vector space model, in this paper, we propose a document-centered technique to retrieve music pieces relevant to arbitrary natural language queries. This technique improves the quality of the resulting document rankings substantially. We report on the current state of the research and discuss current limitations, as well as possible directions to overcome them. 1 Motivation and Context While digital music databases contain several millions of audio pieces nowadays, indexing of these collections is in general still accomplished using a limited set of traditional meta-data descriptors like artist name, track name, album, or year. In most cases, also some sort of classification into coarse genres or different styles is available. Since this may not be sufficient for intuitive retrieval, several innovative (content-based) approaches to access music collections have been presented in the past years. However, the majority of these retrieval systems is based on query-by-example methods, i.e. the user must enter a query in a musical representation which is uncommon to most users and thus lacks acceptance. To address this issue, recently, different approaches to music search engines that can be accessed via textual queries have been proposed [4–6, 9]. In [6], we presented an approach that exploits contextual information related to the music pieces in a collection. To this end, tf × idf features are extracted from Web pages associated with the pieces and their corresponding artist. Furthermore, to represent audio pieces with no (or only little) Web information associated, also audio similarity is incorporated. This technique enables the user to issue queries like “rock with great riffs” to express the intention to find pieces that contain energetic guitar phrases instead of just finding tracks that have been labeled as rock by some authority. The general intention of the system presented in [6] is to allow for virtually any possible query and return the most appropriate pieces according to their “Web context” (comparable to e.g. Google’s image search function). In this paper, we present an alternative method to obtain a relevance ranking of music pieces wrt. a given query. Instead of constructing vector space representations for each music piece, we apply a traditional document indexing approach to the set of retrieved music-related Web pages and introduce a simple ranking function which improves the overall retrieval performance substantially. 2 Technical Background Prior to presenting the modified retrieval approach, we briefly review the vector space-based method described in [6]. The first data acquisition step is identical for both approaches. 2.1 Vector Space Model approach (VSM) To obtain as much track specific information as possible while preserving a high number of Web pages, for each track in the collection, three queries are issued to Google (at most 100 of the top-ranked Web pages are retrieved per query and joined into a single set): 1. “artist” music 2. “artist” “album” music review 3. “artist” “title” music review -lyrics After HTML tag and stop word removal, for each piece, all associated documents are treated as one large document and a weighted term vector representation is calculated using a modification of the tf × idf function. In addition to the context-based features, information on the (timbral) content of the music is derived by calculating a Single Gaussian MFCC (Mel Frequency Cepstral Coefficients) distribution model for each track. Acoustic similarity between two pieces can be assessed by computing the Kullback-Leibler divergence on their models [8]. Based on the audio similarity information, feature space pruning is performed by applying a modified χ test that simulates a 2-class discrimination task between the most similar sounding and the most dissimilar sounding tracks for each piece. For the evaluation collection used in [6], this step reduces the feature space from about 78,000 dimensions to about 4,700. Beside feature space reduction, the audio similarity measure can also be used to emphasize terms that occur frequently among similar sounding pieces, and – most important – to describe music pieces with no (or few) associated information present on the Web. These two tasks are achieved by performing a Gaussian weighting over the 10 acoustically nearest neighbors’ term vectors. After obtaining a term weight vector for each track in the music collection, natural language queries to the system are processed by adding the constraint music to the query and sending it to Google. From the 10 top-ranked Web pages, a query vector is constructed in the feature space. This query vector can then be compared to the music pieces in the collection by calculating cosine distances. Based on the distances, a relevance ranking is obtained. 2.2 Rank-based Relevance Scoring (RRS) In contrast to the VSM method that relies on the availability of Google to process queries, we propose to directly utilize the Web content that has been retrieved in the data acquisition step. To this end, we create an off-line index of all pages using the open source package Lucene [1]. The usage of an off-line index allows to apply an alternative relevance ranking method since all indexed documents are at least relevant to one of the music pieces in the archive. Thus, we can take advantage of this information by exploiting these relations. More precisely, when querying the Lucene off-line index, a relevance ranking of the indexed documents according to the query is returned. Since we know for which music pieces these documents have been retrieved (and are thus relevant), we can simply create a set of music pieces relevant to the query by gathering all music pieces that are associated with at least one of the returned documents. Moreover, we can exploit the ranking information of the returned documents to introduce a very simple (but effective) relevance scoring function. Hence, for a given query q, we calculate the rank-based relevance scoring (RRS) for each music piece m as	archive;automatic sounding;coefficient;data acquisition;database;discrete cosine transform;document;exploit (computer security);feature vector;html;image retrieval;kullback–leibler divergence;list of online music databases;mel-frequency cepstrum;natural language;online and offline;open-source software;query by example;ranking (information retrieval);relevance;similarity measure;tf–idf;viable system model;web content;web page;web search engine;world wide web	Peter Knees;Tim Pohle;Markus Schedl;Dominik Schnitzer;Klaus Seyerlehner	2008		10.1007/978-3-540-78646-7_68	natural language processing;computer science;linguistics;natural language;world wide web;vector space model;information retrieval;search engine	Web+IR	-30.181839315371747	-57.78527397154735	53464
cc1c7fb363a914f7b1c447aa1f9017a355b20b44	a social network approach to software development risk correlation analysis	social network services;software;software correlation social network services time frequency analysis risk management density measurement educational institutions;co word analysis risk correlation social network analysis meta analysis;density measurement;risk analysis;software management;risk management;risk correlation;meta analysis;matrix algebra;correlation methods;software engineering;word processing correlation methods matrix algebra risk analysis social networking online software engineering software management;social networking online;software risk management social network approach software development risk correlation analysis meta analysis risk checklists natural language description set co word analysis word co occurrence matrix density centrality distance core periphery high frequency risk word list network conclusions;social network analysis;co word analysis;correlation;time frequency analysis;word processing	Risk correlation is an important topic in software development. This study applied meta-analysis to both collect risk checklists and compose a natural language description set of 1932 sentences. Co-word analysis was performed to obtain a word co-occurrence matrix. The correlation network features were then analyzed in the form of a social network. Characters including density, centrality, distance and core/periphery were put forward about the risk correlation network. The high frequency risk word list and related network conclusions are helpful for software risk management.	centrality;co-occurrence matrix;document-term matrix;natural language;risk management;social network;software development	Hao Song;Chen Cai	2012	2012 Fifth International Conference on Business Intelligence and Financial Engineering	10.1109/BIFE.2012.71	computer science;data science;theoretical computer science;data mining	SE	-24.523785705332738	-57.85952134630082	53490
5fade5ce20053c53348869ad9a89b4f05475ef73	user-centric ontology population		Ontologies are a basic tool to formalize and share knowledge. However, very often the conceptualization of a specific domain depends on the particular user’s needs. We propose a methodology to perform user-centric ontology population that efficiently includes human-in-the-loop at each step. Given the existence of suitable target ontologies, our methodology supports the alignment of concepts in the user’s conceptualization with concepts of the target ontologies, using a novel hierarchical classification approach. Our methodology also helps the user to build, alter and grow their initial conceptualization, exploiting both the target ontologies and new facts extracted from unstructured data. We evaluate our approach on a real-world example in the healthcare domain, in which adverse phrases for drug reactions, as extracted from user blogs, are aligned with MedDRA concepts. The evaluation shows that our approach has high efficacy in assisting the user to both build the initial ontology (({{mathrm{{textit{HITS},@10}}}}) up to 99.5%) and to maintain it (({{mathrm{{textit{HITS},@10}}}}) up to 99.1%).	blog;conceptualization (information science);experiment;hierarchical clustering;meddra;ontology (information science);requirement	Kenneth Clarkson;Anna Lisa Gentile;Daniel Gruhl;Petar Ristoski;Joseph Terdiman;Steve Welch	2018		10.1007/978-3-319-93417-4_8	information retrieval;conceptualization;ontology (information science);computer science;ontology;population;user-centered design;unstructured data	AI	-27.670779924577282	-58.687976443598316	53491
ad59fd64e1e39279854a6080447ba656d567cf85	hltcoe participation in tac kbp 2017: cold start and edl pilot		The JHU HLTCOE participated in the Cold Start and the Entity Discovery and Linking tasks of the 2017 Text Analysis Conference Knowledge Base Population evaluation. For our sixth year of participation in Cold Start we continued our research with the KELVIN system. We submitted experimental variants that explore use of linking to Freebase across English and Chinese languages and add relations beyond those required by Cold Start. This is our third year of participation in Tri-lingual EDL and first year for the EDL Pilot for 10 Low-resource Languages. We used KELVIN in the Cold Start and Tri-lingual tasks and a new custom system for the Low-resource EDL task.	blender (software);cold start;freebase;information extraction;knowledge base;language technology;test data	Timothy W. Finin;Dawn J. Lawrie;James Mayfield;Paul McNamee;Cash Costello	2017			information retrieval;computer science;cold start (automotive);double layer (surface science)	NLP	-31.4331953704678	-64.25653100557017	53497
0043b24c0d65f6a7de8aeeeba208bfda2e4f70b7	on adverse drug event extractions using twitter sentiment analysis		Extensive clinical trials are required before a drug is placed on the market. It is, however, difficult to discover all the side effects, or adverse drug effects (ADE), for any approved drugs due to the limited number of required clinical trials before a drug is approved. The pervasive online social networks, such as Twitter, can provide additional information on ADE. Concurrently, advancements in social media technology have resulted in the booming of massive public data; the availability of these huge datasets offers numerous research opportunities for extracting ADEs. Towards this purpose, in this paper two effective computation pipelines are proposed, which use drug-related classification and sentiment analysis to extract ADEs on Twitter. The two pipelines are described in detail, and are implemented into automatic processes. Both pipelines are first separately evaluated, and then compared in parallel. Based on 25 days of Twitter data, the first pipeline has successfully predicted 79.4% of drug-related tweets with user opinions. The second pipeline, with its much simpler design, is able to identify much more ADE, as well as discover more new ADE. Based on 4 months of Twitter data collected, the second design is able to successfully capture 5 times more valid ADE than the first design, with 12 times more new ADEs discovered. We believe that these two proposed pipelines are promising methods for extracting ADEs in social networks, and may be applied to additional areas such as food, beverages, and other daily consumer products for identifying side effects and user opinions.	computation;information privacy;pervasive informatics;pipeline (computing);process (computing);sentiment analysis;side effect (computer science);social media;social network	Melody Moh;Teng-Sheng Moh;Yang Peng;Liang Wu	2017	Network Modeling Analysis in Health Informatics and Bioinformatics	10.1007/s13721-017-0159-4	data science;world wide web;sentiment analysis;biology;social media;adverse drug effects;social network	Web+IR	-22.128635097725223	-53.069908991712765	53576
f1d321a78ef0e9232bdc245dbb3ef4251d038efe	deformation analysis of modified maps based on geographical accuracy and spatial context	accuracy context web pages feature extraction atmospheric measurements shape particle measurements;spatial context;atmospheric measurements;web pages;particle measurements;accuracy;internet;shape;feature extraction;internet cartography;cartography;deformation analysis;web page deformation analysis modified maps geographical accuracy spatial context dynamic real world information map information general oriented map path oriented map position oriented map;context	Modified maps are widely used for a variety of purposes such as in tourist guides to help people find geographical objects using simple figures. However, modified maps might become problematic if they contain inaccurate information. This is because most modified maps are not updated with dynamic real-world information, and they might contain incorrect or superfluous information in that some objects on the map are intentionally enlarged or omitted. We propose a deformation analyzing method based on geographical accuracy using the map information and surrounding text. In other words, our proposed method detects the tolerance level of a deformation by the objective of the modified map. We assume that there exist three types of spatial contexts for modifying maps: general oriented map, path oriented map, and position oriented map. We also assume that deformation analysis involves looking at a modified map's geographical accuracy and Web page according to the spatial context. In this paper, we describe deformation analysis based on the geographical accuracy of the modified map and how much they differ from the spatial context of the modified map.	existential quantification;map;web page	Daisuke Kitayama;Ryong Lee;Kazutoshi Sumiya	2011	2011 44th Hawaii International Conference on System Sciences	10.1109/HICSS.2011.139	locator map;the internet;feature extraction;shape;computer science;spatial contextual awareness;web page;data mining;accuracy and precision;law;information retrieval	Visualization	-28.36077863895895	-54.10563515136704	53593
67d1193270cd664c3f5ef0e6070bd3e2ebb34ed7	reranking and classifying search results exhaustively based on edit-and-propagate operations	search engine;web search;drag and drop;exhaustive search	Search engines return a huge number of Web search results, and the user usually checks merely the top 5 or 10 results. However, the user sometimes must collect information exhaustively such as collecting all the publications which a certain person had written, or gathering a lot of useful information which assists the user to buy. In this case, the user must repeatedly check search results that are clearly irrelevant. We believe that people would use a search system which provides the reranking or classifying functions by the user's interaction. We have already proposed a reranking system based on the user's edit-and-propagate operations. In this paper, we introduce the drag-and-drop operation into our system to support the user's exhaustive search.		Takehiro Yamamoto;Satoshi Nakamura;Katsumi Tanaka	2009		10.1007/978-3-642-03573-9_73	beam search;database search engine;computer science;brute-force search;data mining;search analytics;web search query;world wide web;information retrieval;search engine	HPC	-30.955612237199922	-54.125841572481924	53771
0662a6ea12bf9a72f5b87e0421d73e7742643125	thuir at ntcir-9 intent task		This is the first year IR group of Tsinghua University (THUIR) participates in NTCIR. We register the INTENT task and focus on the Chinese topics of subtopic mining and document ranking subtask. In our experiments, we try to mine subtopics from different resources, namely query recommendation, Wikipedia and the query-URL bipartite graph which is constructed by clickthrough data. We also develop some methods to re-rank the subtopics and remove reduplicate ones with query log and search result snippets in search engines. In the document ranking task, methods applied to diversify English documents are used to validate their effectiveness on Chinese pages, such as HITS, Novelty-Result Selection and Documents Duplication Elimination. Based on the new metric, called D#-nDCG, we propose a DocumentDiversification algorithm to select the documents retrieved for subtopics mined in the subtopic mining task, and user browse logs are also leveraged to re-rank these selected results.	algorithm;browsing;experiment;mined;ranking (information retrieval);web search engine;wikipedia	Yufei Xue;Fei Chen;Tong Zhu;Chao Wang;Zhichao Li;Yiqun Liu;Min Zhang;Yijiang Jin;Shaoping Ma	2011			computer science;data mining;world wide web;information retrieval	Web+IR	-31.102491151456032	-62.70820340444751	53853
c88ac790c4bb2778e4d5bbc50f11d15684198cba	multilingual people search	multilingual name search;people search;enterprise search;social network	People Search is an important search service with multiple applications (eg. looking up a friend on Facebook, finding colleagues in corporate email directories etc). With the proportion of non-English users on a steady rise, people search services are being used by users from diverse language demographics. Users may issue name search queries against these directories in languages other than the language of the directory, in which case the present monolingual name search approaches will not work. In this demo, we present a Multilingual People Search system capable of performing fast name lookups on large user directories, independent of the directory language. Our system has applications in areas like social networking, enterprise search and email address book search.	directory (computing);email;web search engine;web search query	Shaishav Kumar;Raghavendra Udupa	2010		10.1145/1835449.1835575	semantic search;computer science;database;multimedia;search analytics;world wide web;search engine;social network	Web+IR	-31.55667541422948	-56.10876544047867	53971
88645faee0345333496ba853305196bd01b3820a	multiple feature-classifier combination in automated text classification	multiple feature classifier combination;classifier combination;feature reduction feature classifier combination multi classifier combination ensembles text classification categorization;learning algorithm;support vector machines;information retrieval;training;text analysis;information access;automated text classification;text classification;feature reduction;vectors;ensembles;machine learning;classification effectiveness;text collection;text classification categorization;principal component analysis;multi classifier combination;feature classifier combination;comparative method;pattern classification;text analysis learning artificial intelligence pattern classification;text collection multiple feature classifier combination automated text classification electronic document information access information retrieval feature set learning algorithm classification effectiveness;learning artificial intelligence;empirical evaluation;text categorization;support vector machines text categorization principal component analysis machine learning information retrieval training vectors;electronic document;feature set	Automatic text classification (ATC) is important in applications such as indexing and organizing electronic documents in databases leading to enhancement of information access and retrieval. We propose a method which employs various types of feature sets and learning algorithms to improve classification effectiveness. Unlike the conventional methods of multi-classifier combination, the proposed method considers the contributions of various types of feature sets and classifiers. It can therefore be known as multiple feature-classifier combination (MFC) method. In this paper we present empirical evaluation of MFC using two benchmarks of text collections to determine its effectiveness. Empirical evaluation show that MFC consistently outperformed all compared methods.	algorithm;database;document classification;email filtering;experiment;feature model;information access;machine learning;microsoft foundation class library;organizing (structure);statistical classification	Lazaro S. P. Busagala;Wataru Ohyama;Tetsushi Wakabayashi;Fumitaka Kimura	2012	2012 10th IAPR International Workshop on Document Analysis Systems	10.1109/DAS.2012.56	support vector machine;text mining;computer science;machine learning;pattern recognition;comparative method;data mining;principal component analysis	Web+IR	-21.054051950918183	-63.7096316820042	53990
712a84b177ca6b087bb8fe4e50ed27844ec14c79	analysis of automatic translation of questions for question-answering systems	information retrieval;info eu repo semantics article;informacion documentacion;grupo a;ciencias sociales;grupo b;search systems;automatic translation	Introduction. Multilingual question-answering systems can provide users with specific data in response to queries by searching for a minimal fragment of text that applies to the query, regardless of the language in which the question is formulated and the answer is found. The aim of this paper is to analyse the automatic translation of questions (intended as queries input to a cross-language, question-answering system) from German and French into the Spanish language. Method. The methodology used for evaluation, based on automatic and subjective measures, appraises whether the translation will serve as input to a system. That is, does the question retain its validity and fulfil its function, allowing a proper response to be found? Analysis. The main features of multilingual questionanswering systems are described and then we analyse the effectiveness of the translations achieved through three popular online translating tools: Google Translator, Promt and Worldlingo. Results. Our findings serve to identify which is the most reliable translator for both pairs of languages overall. However, an even more reliable option would be to use two different translators, depending on which of the two source languages is being dealt with. Conclusions. The results contribute to the realm of innovative search systems by enhancing our understanding of online translators and their potential in the context of multilingual information retrieval.	information retrieval;machine translation;question answering	Lola García-Santiago;María-Dolores Olvera-Lobo	2010	Inf. Res.		natural language processing;library science;computer science;world wide web;information retrieval	Web+IR	-32.593616457888544	-65.42202905498921	54169
e99618c661c30e181497fbf8bf24bbcc72ac7947	label micro-blog topics using the bayesian inference method	bayesian inference;classification;information organization;micro-blog	Classifying Micro-blog content is a popular research topic in social media, which can help users access their favorite information quickly. Much research focuses on classifying Micro-blog content with short text dataset. The challenge is the classification effect may be hampered by content ambiguity. To address this challenge, we propose a novel classification framework using external knowledge base and the Bayesian inference method. We first introduce Baidu Encyclopedia to extract effective features, and then we train efficient classifiers with the probabilistic graphic model. The proposed model can classify micro-blog content reliably. Experiments on Sina-weibo dataset demonstrate the effectiveness of the proposed method.		Heng Gao;Qiudan Li;Xiaolong Zheng	2013		10.1007/978-3-642-39693-9_3	frequentist inference;pattern recognition;data mining;bayesian statistics;statistics	AI	-22.764669508261647	-58.76485956069635	54237
3701d887b2575a03370df1e687ff1197f6f83c17	searching social media streams on the web	feed search;dynamic real time stream retrieval social media stream searching web feedmil search engine;search engine;reseau social;text;red www;search engines;information retrieval;web;feeds;social networking online information retrieval search engines;reseau web;stream search;intelligence artificielle;texte;social media stream searching;media;social network;feedmil;internet;indexing;blogs search engines feeds indexing media feature extraction twitter;feature extraction;social networking online;artificial intelligence;world wide web;inteligencia artificial;dynamic real time stream retrieval;twitter;texto;social media;search engine social media stream search feed search;article;blogs;red social	FeedMil, a dedicated stream search engine, can help users retrieve dynamic real-time streams, focusing on quality and topic relevance rather than simple query matching.	real-time transcription;relevance;social media;web search engine;world wide web	Jonghun Park;Yongwook Shin;Kwanho Kim;Beom-Suk Chung	2010	IEEE Intelligent Systems	10.1109/MIS.2010.150	computer science;multimedia;web search query;world wide web;information retrieval;search engine	DB	-28.880193072665552	-53.00851330447498	54295
6813c4b0a9f5867338292f0582bfcb1424b9053f	extracting traffic information from web texts with a d-s evidence theory based approach	pattern clustering;text analysis;internet encyclopedias electronic publishing semantics data mining uncertainty;text clustering algorithm web texts d s evidence theory based approach web pages bbs microblogs real time traffic information extraction data source city traffic collection traffic condition description natural language spatial temporal contexts information fusion process web sources real time traffic collection traffic state extraction approach evaluation index system semantic similarity wikipedia websites;street traffic control;traffic information systems;web sites;natural language processing systems;web sites information theory natural language processing pattern clustering text analysis traffic information systems;websites;natural language processing;information theory;text clustering web texts traffic state d s evidence theory wikipedia	Web texts, such as web pages, BBS, or microblogs, usually contain a great amount of real-time traffic information, which can be expected to become an important data source for city traffic collection. However, due to the characteristics of ambiguity and uncertainty in the description of traffic condition with natural language, and the difference of description quality for web texts among various publishers and text types, there may exist much inconsistency, or even contradiction for the traffic condition on similar spatial-temporal contexts. An efficient information fusion process is crucial to take advantage of the mass web sources for real-time traffic collection. In this paper, we propose a traffic state extraction approach from massive web texts based on D-S evidence theory to solve the above problem. Firstly, an evaluation index system for the traffic state information collected from the web texts is built with the help of semantic similarity based on Wikipedia, to eliminate ambiguity. Then, D-S evidence theory is adopted to judge and fuse the extracted traffic state information, with evidence combination and decision, which can solve the problem of uncertainty and difference. An experiment shows that the presented approach can effectively judge the traffic state information contained in massive web texts, and can fully utilize the data from different websites. Meanwhile, the proposed approach is arguably more accurate than the traditional text clustering algorithm.	algorithm;blog;cluster analysis;information source;knowledge base;natural language;real-time clock;real-time locating system;real-time transcription;real-time web;semantic similarity;web page;wikipedia	Peiyuan Qiu;Feng Lu;Hengcai Zhang	2013	2013 21st International Conference on Geoinformatics	10.1109/Geoinformatics.2013.6626207	web modeling;web analytics;web mapping;computer science;social semantic web;data mining;web intelligence;world wide web;information extraction;information retrieval	AI	-25.548242792379376	-64.06772228095005	54416
1df6edb783019a0be356ea0244cbdd8dedbe2980	evolutionary optimization for ranking how-to questions based on user-generated contents	hpsg parsing;evolutionary computation;question answering systems;concept clustering;community question answering	In this work, a new evolutionary model is proposed for ranking answers to non-factoid (how-to) questions in community question-answering platforms. The approach combines evolutionary computation techniques and clustering methods to effectively rate best answers from web-based user-generated contents, so as to generate new rankings of answers. Discovered clusters contain semantically related triplets representing question–answers pairs in terms of subject-verb-object, which is hypothesized to improve the ranking of candidate answers. Experiments were conducted using our evolutionary model and concept clustering operating on large-scale data extracted from Yahoo! Answers. Results show the promise of the approach to effectively discovering semantically similar questions and improving the ranking as compared to state-of-the-art methods. 2013 Elsevier Ltd. All rights reserved.	baseline (configuration management);cluster analysis;dimensionality reduction;evaluation function;evolutionary algorithm;evolutionary computation;experiment;genetic algorithm;genetic operator;iterative and incremental development;mathematical optimization;models of dna evolution;question answering;rapid refresh;software quality assurance;software release life cycle;sparse voxel octree;triplet state;user-generated content;web application;wordnet;yahoo! answers	John Atkinson;Alejandro Figueroa;Christian Andrade	2013	Expert Syst. Appl.	10.1016/j.eswa.2013.06.017	computer science;artificial intelligence;machine learning;data mining;information retrieval;evolutionary computation	AI	-27.001946479906216	-61.787348093780615	54506
f8cf3009b4933329bd9a70c4f55c90fb67d4a3a4	ranking of documents by measures considering conceptual dependence between terms	thesaurus;search method;document s topic;similarity measurement;ranking	edness. Concerning Property 3, the influence on ranking exerted by query modification was examined. For this purpose, some terms in every query were chosen at random, and replaced by terms standing one level higher in the concept hierarchy. Cases in which the substitution term happened to be root, or coincided with other terms, were excluded. Five modifications were prepared for each query, with the original query denoted by q0, and its modifications by q1, q2, q3, q4, and q5, the latter being the most divergent from the original. In the experiments, the robustness of the measures Sab, Srg, Srl was verified by using the aforementioned index dƒ. Evidently, the smaller dƒ is in the ranking by q0 and its modifications qi, 1 L i L 5, the more robust the algorithm is. Table 6 presents the results of a sign test comparing pairs of measures for each query modification. In the table, every line shows comparative results for two measures obtained in 750 test runs with the figures representing the number of times that the right (left) measure was considered better (equal estimations were divided in half). For example, if the ranking based on Sab gave smaller dƒ between q0 and qi than Srg, then Sab was considered better than Srg. In the comparison between Sab and Srg, the former had a lower score (54.5) than the latter (695.5), and was also smaller than 351; hence, the latter may be considered more stable (robust) at a confidence level of 95%. On the whole, Srg appears to be most robust, followed by Srl. Concerning Property 4, one may say that differences in specificity between terms should be represented as distinctly as possible. Thus, the distribution of specificity was examined in the same experimental environment as described above. Figure 2 shows the relations between dƒ (horizontal axis) and the distributions for absolute specificity, partially relative specificity, and totally relative specificity (vertical axes) in the case of modification q1. The distributions are represented by the mean values and standard deviations. In the ideal case, dƒ is as low as possible while the specificity is distributed uniformly over the interval [0, 1]. Any bias would reduce robustness to query modifications. As seen from the diagram, Srl is best in terms of Property 4, followed by Sab. 5.2. Retrieval efficiency The efficiency of retrieval using the proposed measures was also verified. Since the journals presented in Table 3 are collections of documents related to the same research field, the documents which were in the same journal as a query were judged to be relevant. Normally, retrieval efficiency is estimated by recall and precision factors. With a ranked representation of results, however, efficiency is judged by the proportion of relevant documents (as decided by the user) among the M highest-ranked documents. In other words, the efficiency of retrieval is determined by the number nr of relevant documents with respect to M. In a plot with M on the abscissa, and nr on the ordinate, the smaller the value of M producing results in the upper part of the diagram, the better is the search criterion (measure). Figures 3 and 4 present average results when documents from TOCHI and TODS, respectively, were used as Table 4. Values of df ± standard errors Table 6. Sign test results Table 5. Rates of correctly ranked pairs ± standard errors	acm transactions on database systems;algorithm;apache axis;diagram;experiment;natural language;precision and recall;relevance feedback;sensitivity and specificity;technical standard;user interface	Makoto Nakashima;Yuichi Kaneko;Tetsuro Ito	2003	Systems and Computers in Japan	10.1002/scj.1205	ranking;ranking;computer science;artificial intelligence;data mining;database;information retrieval;statistics	Web+IR	-33.53672944619327	-62.365956055698675	54509
2bf35c6fe97d68e9b599a241c54ffeef22401708	analysis of short texts on the web: introduction to special issue	articulo	Analysis of web and social media data is a rapidly growing area of research. Researchers seek to extract a wide variety of information from these texts in order to address specific user needs, profile attitudes and intentions, and target advertising, etc., which may require application of the full range of natural processing techniques. However, many of the texts in question—including news feeds, document titles, FAQs, and tweets—exist as short, sometimes barely sentence-like snippets that do not always follow the lexical and syntactic conventions assumed by many language processing tools. Many NLP analyses rely on the repetition of specific lexical items throughout the text in order to identify topic, genre, and other features; without sufficient context to enable such analyses, and because of their often eccentric grammatical style, short texts pose a new kind of challenge for language processing research (Errecalde et al. 2008; Pinto et al. 2011). With Web 2.0, the largest communication and collaborative platform, new short texts are created on daily basis in the form of on-line reviews of commercial products, blog posts, or opinions in social media (Liu 2012). Twitter is a new	blog;natural language processing;online and offline;social media;web 2.0;word lists by frequency;world wide web	Paolo Rosso;Marcelo Luis Errecalde;David Pinto	2013	Language Resources and Evaluation	10.1007/s10579-013-9220-9	natural language processing;computer science;linguistics;multimedia;world wide web	NLP	-24.7904290695031	-55.74288706662263	54574
bce86ff70f254517b67fde152b3b861fd535d27c	sorm: a social opinion relevance model	social network services;opinion relevance;opinion mining;opinion mining sorm social opinion relevance model social network social opinion relevance corpus sorc electronic games information retrieval metrics ndcg qmeasure map statistical significance test;information retrieval;social network services games mathematical model books computational modeling gold;books;text analysis computer games information retrieval natural language processing social networking online;computational modeling;gold;games;mathematical model;social search;social search opinion relevance opinion mining information retrieval	This paper presents a generic and domain independent opinion relevance model for a Social Network user. The Social Opinion Relevance Model (SORM) is able to estimate an opinion's relevance based on twelve different parameters. Compared to other models, SORM's main distinction is its ability to provide customized results according to whom the opinion relevance is being estimated for. Due to the lack of opinion relevance corpuses able to properly test our model, we have created a new one called Social Opinion Relevance Corpus (SORC). Using SORC, we carried out some experiments on the Electronic Games domain that illustrate the importance of the customizing the opinion relevance in order to achieve better results on typical Information Retrieval metrics, such as NDCG, QMeasure and MAP. We also performed a statistical significance test that reinforces and corroborates the advantages that SORM offers.	experiment;information retrieval;map;relevance;social network	Allan Diego Silva Lima;Jaime Simão Sichman	2014	2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)	10.1109/WI-IAT.2014.19	data science;political science;data mining;information retrieval	Web+IR	-23.018095791774204	-61.07927404046738	54624
d83facebfee7bb062c3dc8e84eb401c4164931b0	cbad: icdar2017 competition on baseline detection		The cBAD competition aims at benchmarking state-of-the-art baseline detection algorithms. It is in line with previous competitions such as the ICDAR 2013 Handwriting Segmentation Contest. A new, challenging, dataset was created to test the behavior of state-of-the-art systems on real world data. Since traditional evaluation schemes are not applicable to the size and modality of this dataset, we present a new one that introduces baselines to measure performance. We received submissions from five different teams.	algorithm;baseline (configuration management);international conference on document analysis and recognition;modality (human–computer interaction)	Markus Diem;Florian Kleber;Stefan Fiel;Tobias Grüning;Basilios Gatos	2017	2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)	10.1109/ICDAR.2017.222	computer science;xml;artificial intelligence;baseline (configuration management);pattern recognition;image segmentation;benchmarking;handwriting;contest;text mining	Vision	-30.85271472973948	-64.25966764739968	54658
c66af85ffcff46dfc70d616c432c1979925dfbad	"""on the """"calligraphy"""" of books"""		Authorship attribution is a natural language processing task that has been widely studied, often by considering small order statistics. In this paper, we explore a complex network approach to assign the authorship of texts based on their mesoscopic representation, in an attempt to capture the flow of the narrative. Indeed, as reported in this work, such an approach allowed the identification of the dominant narrative structure of the studied authors. This has been achieved due to the ability of the mesoscopic approach to take into account relationships between different, not necessarily adjacent, parts of the text, which is able to capture the story flow. The potential of the proposed approach has been illustrated through principal component analysis, a comparison with the chance baseline method, and network visualization. Such visualizations reveal individual characteristics of the authors, which can be understood as a kind of calligraphy.	baseline (configuration management);complex network;graph drawing;mesoscopic physics;natural language processing;principal component analysis	Vanessa Q. Marinho;Henrique Ferraz de Arruda;Thales S. Lima;Luciano da Fontoura Costa;Diego R. Amancio	2017		10.18653/v1/w17-2401	natural language processing;computer science;artificial intelligence;machine learning;linguistics	NLP	-23.319601766553294	-59.599441122977446	54709
1d00a2551765a72931d29c2a5c5db46e94a5332b	a business modeled approach for trust management in p2p	text analysis fuzzy set theory matrix algebra pattern classification support vector machines;multisubject text;classification algorithm;support vector machines;subclassifiers training;classification algorithms support vector machines support vector machine classification text categorization artificial intelligence testing fuzzy sets lagrangian functions software engineering distributed computing;text analysis;matrix algebra;fuzzy support vector machine;fuzzy set theory;text classification;pattern classification;reuters 21578;fuzzy support vector machines;membership matrix;1 a 1 method;reuters 21578 classification algorithm multisubject text fuzzy support vector machines 1 a 1 method subclassifiers training membership matrix	"""P2P communities, is a method for arranging large numbers of peers in a self configuring peer relationship based on declared attributes (or interests) of the participating peers. This method is expected to have an impact in sharing of resources and pruning of search spaces based on the interests of the clients. Current peer- to-peer systems are targeted for information sharing, file storage, searching and indexing often using an overlay network. In this paper we expand the scope of peer-to-peer systems to include the concept of a business environment analogous to a """"stock market"""". Our work focuses on efficient methods to discover trustworthy peers in the P2P network. We investigate the behavior of randomly created relationships formed during transaction between Vendors and Emptors. Discovering services on the fly is essential to being able to identify profitable oriented transactions. In addition, efficient Vendor/Emptor based algorithms allow us to manage quickly changing market trends. Moreover the inclusion of the concept of trading policies among business communities enhances the probability of mutual gain."""	algorithm;on the fly;overlay network;peer-to-peer;randomness;trust management (managerial science)	Thanukrishnan Srinivasan;N. Vidyapriyadarshini;Smrithika Appaiah	2007	Eighth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing (SNPD 2007)	10.1109/SNPD.2007.511	support vector machine;text mining;computer science;machine learning;pattern recognition;data mining;fuzzy set	DB	-22.938277881299598	-57.133873439449246	54776
3899713811ad0a58b081107541a9d9dca802be84	should i care about your opinion? detection of opinion interestingness and dynamics in social media	social media;opinion mining	In this paper, we describe a set of reusable text processing components for extracting opinionated information from social media, rating it for interestingness, and for detecting opinion events. We have developed applications in GATE to extract named entities, terms and events and to detect opinions about them, which are then used as the starting point for opinion event detection. The opinions are then aggregated over larger sections of text, to give some overall sentiment about topics and documents, and also some degree of information about interestingness based on opinion diversity. We go beyond traditional opinion mining techniques in a number of ways: by focusing on specific opinion-target extraction related to key terms and events, by examining and dealing with a number of specific linguistic phenomena, by analysing and visualising opinion dynamics over time, and by aggregating the opinions in different ways for a more flexible view of the information contained in the documents.	algorithm;flickr;gate;named entity;relevance;sensor;social media;time series;timeline;web content	Diana Maynard;Gerhard Gossen;Adam Funk;Marco Fisichella	2014	Future Internet	10.3390/fi6030457		ML	-23.65231976006277	-55.46193147587804	54803
b741241673ec340cf3aa1184ad2ebbf3e3c4c24e	automatically predicting information quality in news documents	multiple research method;empirical result;information quality;statistical analysis;machine prediction;user experiment;news document;good level;data analysis technique;data analysis techniques;user experience	We report here empirical results of a series of studies aimed at automatically predicting information quality in news documents. Multiple research methods and data analysis techniques enabled a good level of machine prediction of information quality. Procedures regarding user experiments and statistical analysis are described.	experiment;information quality	Rong Tang;Kwong Bor Ng;Tomek Strzalkowski;Paul B. Kantor	2003			user experience design;computer science;data science;data mining;information quality;data analysis;information retrieval;statistics	AI	-25.957112062589353	-55.12525667177379	54827
36a5dbd8660b12e0110d8d8585354df82c39adae	on the importance of text analysis for stock price prediction		We investigate the importance of text analysis for stock price prediction. In particular, we introduce a system that forecasts companies’ stock price changes (UP, DOWN, STAY) in response to financial events reported in 8-K documents. Our results indicate that using text boosts prediction accuracy over 10% (relative) over a strong baseline that incorporates many financially-rooted features. This impact is most important in the short term (i.e., the next day after the financial event) but persists for up to five days.	baseline (configuration management);text mining	Heeyoung Lee;Mihai Surdeanu;Bill MacCartney;Daniel Jurafsky	2014			cost price	NLP	-20.20930983506721	-52.87530254767331	54854
e3d38a18461127a24612a4beb2c55b02bf7163cc	an improved method for automatically determining webpage cohesiveness for quality information retrieval from the world wide web	information retrieval;world wide web		group cohesiveness;information retrieval;web page;world wide web	Surya B. Yadav;Jeremy Bellah	2006			web intelligence;web standards;world wide web;web page;web service;web server;information retrieval;web 2.0;web-based simulation;web navigation;computer science	Web+IR	-29.715783785398603	-52.618550563113814	54941
3ccc293066ad9ad9435718a7a01ea77552ba0c97	efficient algorithms for context query evaluation over a tagged corpus	query processing;efficient algorithm;text analysis;trees mathematics;adaptive algorithms;adaptive algorithm;query evaluation;pattern matching;query processing xml books database languages acquired immune deficiency syndrome africa computer science read write memory html adaptive algorithm;labeling index;standard ram model context query evaluation tagged corpus optimal adaptive algorithm document tree;adaptive algorithms pattern matching;trees mathematics query processing text analysis	We present an optimal adaptive algorithm for context queries in tagged content. The queries consist of locating instances of a tag within a context specified by the query using patterns with preorder, ancestor-descendant and proximity operators in the document tree implied by the tagged content. The time taken to resolve a query $Q$ on a document tree $T$ is logarithmic in the size of $T$, proportional to the size of $Q$, and to the difficulty of the combination of $Q$ with $T$, as measured by the minimal size of a certificate of the answer. The performance of the algorithm is no worse than the classical worst-case optimal, while provably better on simpler queries and corpora. More formally, the algorithm runs in time $\bigo(\difficulty\nbkeywords\lg(\nbobjects/\difficulty\nbkeywords))$ in the standard RAM model and in time $\bigo(\difficulty\nbkeywords\lg\lg\min(\nbobjects,\nblabels))$ in the $\Theta(\lg(\nbobjects))$-word RAM model, where $\nbkeywords$ is the number of edges in the query, $\difficulty$ is the minimum number of operations required to certify the answer to the query, $\nbobjects$ is the number of nodes in the tree, and $\nblabels$ is the number of labels indexed.	adaptive algorithm;best, worst and average case;data structure;public key certificate;random-access machine;random-access memory;randomized algorithm;run time (program lifecycle phase);text corpus	Jérémy Barbay;Alejandro López-Ortiz	2009	2009 International Conference of the Chilean Computer Science Society	10.1109/SCCC.2009.16	computer science;theoretical computer science;database;information retrieval	DB	-31.87914843293392	-58.43891339623039	55169
f69407c58310c74463130b7f4f95ee6fd3b0bde6	role of author personality traits for identifying intent based racist posts	social network services;electronic mail;semantics;feature extraction;taxonomy;informatics;security	Research shows that people misuse freedom of expressions to post hateful comments about various religion and race. In this paper, we define our problem of radicalization and racism detection as an intent based classification problem that identifies a post to be radicalized or racist based on the motive of the author. We demonstrate the effectiveness of authors' personality traits in order to identify a post having such intent.	misuse case	Swati Agarwal;Ashish Sureka	2016	2016 European Intelligence and Security Informatics Conference (EISIC)	10.1109/EISIC.2016.051	feature extraction;computer science;information security;semantics;informatics;world wide web;computer security;taxonomy	SE	-20.316777836135458	-56.723893105046386	55176
7403c889a92cb42046ce5d7d871645aa41e02b22	text relevance analysis method over large-scale high-dimensional text data processing		As the amount of digital information is exploding in social, industry and scientific areas, MapReduce is a distributed computation framework, which has become widely adopted for analytics on large-scale data. Also, the idea which is used to solve the large-scale data problem by the use of approximation algorithms has become a very important solution in recent years. Especially for solving high-dimensional text data processing, semantic Web and search engine are required to pay attention to proximity searches and text relevance analysis. The difficulties of large-scale text processing mainly include its quick comparison and relevance judgment. In this paper, we propose an approximate bit string for approximation search method on MapReduce platform. Experiments exhibits excellent performance on efficiency effectiveness and scalability of the proposed algorithms.	relevance;text corpus	Ling Wang;Wei Ding;Tie Hua Zhou;Keun Ho Ryu	2015		10.1007/978-3-319-24069-5_35	text mining	ML	-25.075449219930558	-56.945677910826866	55390
7091011b514efa2a8735ff98ceb1ca1d98cc8394	semantic-based social media threats detection		The exponential growth of internet access and computing devices, has led to the growth of social media usage and changed the way of our lives in society. Due to the growth of social media usage such as Facebook, Twitter, Instagram, YouTube, etc., there are possibilities that users share information which may pose a threat to the national security. For example, the threats might be propagated by a certain group of terrorists. The threats can be written either in Malay or mixed with other languages; especially English and in a short form basis. The major issue is to extract the u0027meaningu0027 of the messages for further processing tasks (e.g. categorizing and identifying the insight of threats). Therefore, a semantic-based grammar approach helps to fill this u0027knowledge gapu0027 for threat analysis at the early stage. This research will focus on knowledge extraction (based-on Malay words) through semantic relations between words and their dependents.	categorization;imperative programming;instagram;internet access;scalability;semantic network;social media;text mining;threat (computer);time complexity	Hassan Mohamed;Syahaneim Marzukhi;Zuraini Zainol;Tengku M. T. Sembok;Omar Zakaria	2018		10.1145/3164541.3164620	computer science;computer network;malay;internet privacy;knowledge extraction;national security;internet access;ontology;social media;grammar	AI	-21.82913346190854	-55.51827112948446	55393
6b173db579e05a1bdcf8a84080b8e4a81fe4a2ee	food hazard event extraction based on news and social media: a preliminary work	news;social media food hazard event extraction news food hazard event detection information template informative keyword extraction website ministry of food and drug safety food safety korea;hazards;presses;data mining;companies;social networking online food safety information retrieval;media;social media food hazard event extraction news;food hazard;feature extraction;social media;event extraction;hazards data mining companies media feature extraction presses	Detecting and disseminating food hazard information is a critical task that directly affects the public health. In spite of its importance, however, few systems are developed to automatically gather and analyze food hazard information. In this paper, we introduce our preliminary work to build such system. Our final system aims to detect and extract food hazard event from the live data shared on the Web. We defined information template for food hazard event. Then we used the template to extract informative keywords from the website of Ministry of Food and Drug Safety, the governmental organization responsible for ensuring food safety in Korea. We explain our work process, considerations, as well as our future work to implement our system.	information;social media;world wide web	Kyoungrok Jang;Kangwook Lee;Gwan Jang;Seungyong Jung;Min-Gwan Seo;Sung-Hyon Myaeng	2016	2016 International Conference on Big Data and Smart Computing (BigComp)	10.1109/BIGCOMP.2016.7425972	engineering;data mining;advertising;internet privacy	SE	-21.77148118516828	-55.91826629424256	55453
c77118267f719ae0ddb264246e68ab1ab9d7dece	fuzzy opinion: detection of opinion based on sentiwordnet dictionary by using fuzzy logic	opinion mining;positive;negative;sentiwordnet;neuter;fuzzy;fuzzification;defuzzification;tweet	In this paper, the authors propose a new approach to detect opinion by SentiWorNet with the introduction of the concept of fuzzy logic. In this vein, the authors will build detection system fuzzy opinion “Fuzzy Opinion.” To give flexibility to their system, they will use a threshold of opinion. The texts are represented by a vector of word (bag of words) which will be reduced to vector the word bearer opinion by filtering with SentiWordNet. Consequently, the heart of their approach is to associate each text into two scores (biscoring): Sp represents the positivity of text and Sn represents the negativity of text; this is the stage of Fuzzification. To identify opinion of a text and to ensure flexibility, the authors have used a threshold of opinion. Further, they have adapted the defuzzification step for identifying opinion. Finally, they compared the results of this approach with the results of the same approach without fuzzy logic in using the same corpus. KeyWORDS Defuzzification, Fuzzification, Fuzzy, Negative, Neuter, Opinion Mining, Positive, SentiWordNet, Tweet	bag-of-words model;defuzzification;dictionary;fuzzy logic;fuzzy set;negativity (quantum mechanics)	Amine Boudia;Reda Mohamed Hamou;Abdelmalek Amine	2016	IJIRR	10.4018/IJIRR.2016070101	defuzzification;artificial intelligence;machine learning;data mining;mathematics	AI	-22.89813060785303	-65.79871617516066	55535
07eee4922d2e42039e9a230d07a2fc3bb1b68bde	extraction and annotation of personal cliques from social networks	social networking online internet;history;sports equipment;text mining;twitter feature extraction history sports equipment accuracy laboratories;accuracy;internet;keyword extraction personal cliques social networks microblogging services twitter network connections information overload;social networks;feature extraction;keyword extraction;social networking online;twitter;text mining social networks keyword extraction	"""In microblogging services such as Twitter, users can choose whose posts they want to read by """"following"""" other user accounts. Twitter users often have large social networks, thus many of them are overwhelmed with managing their network connections and dealing with information overload. We want to address this problem by automatically dividing the social network of a Twitter user into personal cliques, and annotating each clique with keywords to identify the common ground of a clique. Our proposed clique annotation method extracts keywords from the tweet history of the clique members and individually weights the extracted keywords of each clique member according to the relevance of their tweets for the clique. The keyword weight is influenced by two factors. The first factor is calculated based on the number of connections of a user within the clique, and the second factor depends on whether the user mainly publishes personal information or information of general interest. In an experiment, on average 36.25% of the keywords extracted from our proposed method were relevant for the cliques, as opposed to 31.78% for the baseline method, which does not weight keywords but only calculates term frequency. When we annotated only cliques formed around common interests, such as """"baseball"""", our proposed method even extracted 50.67% of relevant keywords, as opposed to 42% for the baseline method. These results clearly indicate that our approach can improve clique annotation in social networks."""	baseline (configuration management);clique (graph theory);information overload;personally identifiable information;relevance;social network;tf–idf;user (computing)	Maike Erdmann;Tomoya Takeyoshi;Gen Hattori;Chihiro Ono	2012	2012 IEEE/IPSJ 12th International Symposium on Applications and the Internet	10.1109/SAINT.2012.32	text mining;the internet;feature extraction;computer science;data mining;database;accuracy and precision;internet privacy;law;world wide web;statistics;social network	Web+IR	-24.8264517282108	-53.99779766722872	55763
ae2eaefdb99b007f6d05aabdffa963f459d7e02b	combining multiple evidence from different properties of weighting schemes	multiple representation;summarization of multiple texts;natural language summarization;document representation;natural language generation;difference set;technical report;computer science	It has been known that using different representations of either queries or documents, or different retrieval techniques retrieves different sets of documents. Recent work suggests that significant improvements in retrieval performance can be achieved by combining multiple representations or multiple retrieval techniques. In this paper we propose a simple method for retrieving different documents within a single query representation, a single document representation and a single retrieval technique. We classify the types of documents, and describe the properties of weighting schemes. Then, we explain that different properties of weighting schemes may retrieve different types of documents. Experimental results show that significant improvements can be obtained by combining the retrieval results from different properties of weighting schemes.		Joon Ho Lee	1995		10.1145/215206.215358	natural language processing;computer science;technical report;data mining;world wide web;information retrieval;difference set	ML	-33.60724390268117	-61.560144443218114	55860
025f1db9b015739493fef58c5d0193c84b1e05a4	inferring what a user is not interested in	software libraries;success rate	This paper describes a system to improve the speed and success rate with which users browse software libraries. The system is a learning apprentice: it monitors the user’s normal browsing actions and from these infers the goal of the user’s search. It then searches the library being browsed, uses the inferred goal to evaluate items and presents to the user those that are most relevant. The main contribution of this paper is the development of rules for negative inference (i.e. inferring features that the user is not interested in). These produce a dramatic improvement in the system’s performance. The new system is more than twice as effective at identifying the user’s search goal than the original, and it ranks the target much more accurately at all stages of search.	browsing;computer performance;library (computing)	Robert C. Holte;John Ng Yuen Yan	1996		10.1007/3-540-61291-2_49	computer science;database;multimedia;world wide web	AI	-33.34503959033116	-52.18141173265669	55945
308d3b8f231543a113a973627a0111f30ff78c46	cat: credibility analysis of arabic content on twitter		Data generated on Twitter has become a rich source for various data mining tasks. Those data analysis tasks that are dependent on the tweet semantics, such as sentiment analysis, emotion mining, and rumor detection among others, suffer considerably if the tweet is not credible, not real, or spam. In this paper, we perform an extensive analysis on credibility of Arabic content on Twitter. We also build a classification model (CAT) to automatically predict the credibility of a given Arabic tweet. Of particular originality is the inclusion of features extracted directly or indirectly from the author’s profile and timeline. To train and test CAT, we annotated for credibility a data set of 9, 000 Arabic tweets that are topic independent. CAT achieved consistent improvements in predicting the credibility of the tweets when compared to several baselines and when compared to the state-of-the-art approach with an improvement of 21% in weighted average Fmeasure. We also conducted experiments to highlight the importance of the userbased features as opposed to the contentbased features. We conclude our work with a feature reduction experiment that highlights the best indicative features of credibility.	baseline (configuration management);binary classification;brown corpus;data mining;experiment;machine learning;sentiment analysis;spamming;statistical classification;text corpus;timeline	Rim El Ballouli;Wassim El-Hajj;Ahmad Ghandour;Shady Elbassuoni;Hazem M. Hajj;Khaled Bashir Shaban	2017			arabic;credibility;internet privacy;advertising;art	Web+IR	-21.49015311877215	-57.132796868896264	56130
cba8be480028ca8a011c7ebcd7f53d176fd53ba0	cluster-centric approach to news event extraction	shallow text processing;european commission;linguistic relativity;information aggregation;text processing;real time;joint research centre;articles in periodicals and books;natural disaster;system development;information fusion;event extraction	This paper presents a real-time and multilingual news event extraction system developed at the Joint Research Centre of the European Commission. It is capable of accurately and efficiently extracting violent and natural disaster events from online news. In particular, a linguistically relatively lightweight approach is deployed, in which clustered news are heavily exploited at all stages of processing. The paper focuses on the system’s architecture, real-time news clustering, geolocating clusters, event extraction grammar development, adapting the system to the processing of new languages, cluster-level information fusion, visual event tracking and accuracy evaluation.	algorithm;cluster analysis;computer cluster;extensibility;extension (mac os);google earth;real-time clock;real-time locating system;usability;web mining;web page;world wide web	Jakub Piskorski;Hristo Tanev;Martin Atkinson;Erik Van der Goot	2008		10.3233/978-1-58603-904-2-276	speech recognition;engineering;complex event processing;data mining;world wide web	NLP	-29.75885005228008	-63.13397036607537	56172
d494539a26f7c4dddd042bd5e97f031fc31ce834	analysis-aware approach to improving social data quality		OF THE DISSERTATION Analysis-Aware Approach to Improving Social Data Quality By Mehdi Sadri Doctor of Philosophy in Computer Science University of California, Irvine, 2017 Professor Sharad Mehrotra, Chair In the era of real time big data stream processing, the past decade has witnessed emergence of wide variety of applications to tap into live social data feed to gain awareness about events, opinions and sentiments of the community. Social data has the potential to bring new functionalities and improvements in a wide variety of domains from Emergency Response to Political Analysis. Thus, there is an increasing demand for executing up-to-the-minute analysis tasks on top of these dynamic data sources by modern applications. Such new requirements have created new challenges for traditional data processing techniques. In this thesis, we respond to some of these challenges. First, we explore the problem of online adaptive topic focused tweet acquisition. Specifically, we propose a Tweet Acquisition System (TAS ), that iteratively selects phrases to track according to a reinforcement learning algorithm. The selection follows an explore-exploit policy to approximate the effectiveness of different phrases in retrieving relevant tweets based on Bayesian inferences. We also develop a tweet relevance model, which enables checking the relevance of collected tweets to the topic of interest based on multiple criteria. The objective of TAS is to improve the recall of collected relevant tweets. Our experimental studies show significant improvements over the state-of-the-art, furthermore the performance gap increases when the topics are more specific. xi Subsequently, efficient processing of top-k mentioned entities query posed on a stream of tweets has become a key part of a broad class of real-time applications, ranging from content search to marketing. Given that words are often ambiguous, entity linking becomes an important step towards answering such queries. Furthermore, the continuous and fast generation of tweets makes it crucial for such applications to process those queries at an equally fast pace. In order to address these requirements, we propose TkET (pronounced ticket) as an analysis-aware entity linking framework for efficiently answering top-k entities query over Twitter stream in an sliding window fashion. The comprehensive empirical evaluation of the proposed solution demonstrates its significant advantage in terms of efficiency over the traditional techniques for the given problem settings.		Mehdi Sadri	2017			data mining;reinforcement learning;data feed;big data;entity linking;ticket;ranging;dynamic data;data quality;computer science	DB	-27.090354405277463	-53.50684166046936	56288
87320e7003a31e36953997f80369caa479473121	framing matters: predicting framing changes and legislation from topic news patterns		News has traditionally been well researched, with studies ranging from sentiment analysis to event detection and topic tracking. We extend the focus to two surprisingly under-researched aspects of news: framing and predictive utility. We demonstrate that framing influences public opinion and behavior, and present a simple entropic algorithm to characterize and detect framing changes. We introduce a dataset of news topics with framing changes, harvested from manual surveys in previous research. Our approach achieves an F-measure of F1 = 0.96 on our data, whereas dynamic topic modeling returns F1 = 0.1. We also establish that news has predictive utility, by showing that legislation in topics of current interest can be foreshadowed and predicted from news patterns.	algorithm;computer science;counter (digital);f1 score;framing (world wide web);framing (social sciences);semantic similarity;sentiment analysis;time-domain reflectometry;topic model;utility computing	Karthik Sheshadri;Chung-Wei Hang;Munindar P. Singh	2018	CoRR		data mining;framing (construction);sentiment analysis;topic model;ranging;computer science;public opinion;legislation	NLP	-23.38039103401554	-54.44455623485194	56302
0f49f7647f6c39dd0e2607781a263bf70a837a85	data mining approach to monitoring the requirements of the job market: a case study	web data extraction;text mining;latent semantic indexing;job market analysis	In challenging economic times, the ability to monitor trends and shifts in the job market would be hugely valuable to job-seekers, employers, policy makers and investors. To analyze the job market, researchers are increasingly turning to data science and related techniques which are able to extract underlying patterns from large collections of data. One database which is of particular relevance in the presence context is O*NET, which is one of the most comprehensive publicly accessible databases of occupational requirements for skills, abilities and knowledge. However, by itself the information in ONET is not enough to characterize the distribution of occupations required in a given market or region. In this paper, we suggest a data mining based approach for identifying the most in-demand occupations in the modern job market. To achieve this, a Latent Semantic Indexing (LSI) model was developed that is capable of matching job advertisement extracted from the Web with occupation description data in the ONET database. The findings of this study demonstrate the general usefulness and applicability of the proposed method for highlighting job trends in different industries and geographical areas, identifying occupational clusters, studying the changes in jobs context over time and for various other research embodiments.	data mining;data science;database;geographic coordinate system;information source;job stream;preprocessor;relevance;requirement;text corpus;web scraping;world wide web	Ioannis Karakatsanis;Wala AlKhader;Frank MacCrory;Armin Alibasic;Mohammad Atif Omar;Zeyar Aung;Wei Lee Woon	2017	Inf. Syst.	10.1016/j.is.2016.10.009	text mining;latent semantic indexing;computer science;knowledge management;data science;machine learning;data mining;database;computer security;job analysis	ML	-21.767820476331394	-52.70612541050863	56311
4847c165254a42bcd01a28a5aa3a7a0f960a2a7a	automatic clustering and summarisation of microblogs: a multi-subtopic phrase reinforcement algorithm		There is a phenomenal growth of microblogging-based social communication services and subscriptions in recent years. Through these services, users publish a large number of posts within a short period time, making it extremely hard for readers to keep track of a trending topic. A solution to this issue is text summarisation, which can generate a short summary of a trending topic from multiple posts. Most of the existing summarisation algorithms were proposed for long documents and do not work well for short microblogging posts. The PR (Phrase Reinforcement) algorithm was particularly designed to summarise microblogs, however it is merely able to generate a single-post summary that conveys a single topic, potentially overlooking other important information from the posts. In this paper, we contribute the PRICE (Phrase Reinforcement: Iteration, Clustering and Extraction) algorithm by extending the original PR algorithm with the ability to generate both multi-post and single-post summaries that span over multiple subtopics. Experimental evaluation results show that the PRICE algorithm outperforms the original PR algorithm in terms of both ROUGE-1 and Content metrics.	algorithm	Mahfouth Alghamdi;Haifeng Shen	2017		10.1007/978-3-319-51691-2_8	natural language processing;machine learning;pattern recognition	NLP	-25.524082035469966	-56.179116677710006	56361
340f9e587346a22ef95a148327484e82ec69a264	effective multi-query expansions: robust landmark retrieval	multi query expansions;landmark photo retrieval	Given a query photo issued by a user (q-user), the landmark retrieval is to return a set of photos with their landmarks similar to those of the query, while the existing studies on the landmark retrieval focus on exploiting geometries of landmarks for similarity matches between candidate photos and a query photo. We observe that the same landmarks provided by different users may convey different geometry information depending on the viewpoints and/or angles, and may subsequently yield very different results. In fact, dealing with the landmarks with shapes caused by the photography of q-users is often nontrivial and has never been studied.  Motivated by this, in this paper we propose a novel framework, namely multi-query expansions, to retrieve semantically robust landmarks by two steps. Firstly, we identify the top-k photos regarding the latent topics of a query landmark to construct multi-query set so as to remedy its possible shape. For this purpose, we significantly extend the techniques of Latent Dirichlet Allocation. Secondly, we propose a novel technique to generate the robust yet compact pattern set from the multi-query photos. To ensure redundancy-free and enhance the efficiency, we adopt the existing minimum-description-length-principle based pattern mining techniques to remove similar query photos from the (k+1) selected query photos. Then, a landmark retrieval rule is developed to calculate the ranking scores between mined pattern set and each photo in the database, which are ranked to serve as the final ranking list of landmark retrieval. Extensive experiments are conducted on real-world landmark datasets, validating the significantly higher accuracy of our approach.	data mining;experiment;latent dirichlet allocation;mined;minimum description length	Yang Wang;Xuemin Lin;Lin Wu;Wenjie Zhang	2015		10.1145/2733373.2806233	computer vision;query expansion;computer science;data mining;information retrieval	AI	-27.158036216630958	-52.48567000538366	56375
24fa1820737bcde4b2bbd4feaf95a506c629f88a	aspect based summarization of context dependent opinion words	sentiment analysis;text summarization;opinion mining	Popularity and availability of opinion-rich resources in e-commerce platform is growing rapidly. Before buying any product, one is interested to know the opinion of other people about that product. For any product, there are hundreds of reviews available online so it becomes very difficult for the customers to read all the reviews. Also, one cannot set his mind based on reading some of the review since it gives him a biased view about that product. So we need to automate this process. As we know, there are lots of opinion words present in the sentences of a review which will tell about the polarity of that product. Out of all the opinion words, some words behave in the same manner means they have the same polarity in all contexts, but some words are context dependent means they have different polarity in different context. In this paper, we proposed an Aspect Based Sentiment Analysis and Summarization (ASAS) System, which handles the context dependent opinion words that has been the cause of major difficulties. For finding the opinion polarity, first, we used an online dictionary for classifying the context independent opinion word. Second, we used natural linguistic rules for assigning the polarity to maximum possible context dependent words. These steps create the training data set. Third, for classification of the remaining opinion words, we used opinion words and feature together rather than opinion words alone, because the same opinion word can have different polarity in the same domain. Then we used our Interaction Information method to classify the feature-opinion pairs. Fourth, as negation plays a very crucial role, we found negation words and flipped the polarity of the corresponding opinion word. Finally, after classifying each opinion word, the system generated a short summary for that particular product based on each feature © 2014 The Authors. Published by Elsevier B.V. Selection and peer-review under responsibility of KES International.	automatic summarization;dictionary;e-commerce;feature extraction;interaction information;mind;natural language;sentiment analysis;test set	Hitesh Kansal;Durga Toshniwal	2014		10.1016/j.procs.2014.08.096	natural language processing;computer science;artificial intelligence;machine learning;data mining	NLP	-22.75391660145543	-59.85540412621301	56600
2110e773542cadc6d5157e5951c880cc88cbbf88	the pareto principle is everywhere: finding informative sentences for opinion summarization through leader detection		Most previous works on opinion summarization focus on summarizing sentiment polarity distribution toward different aspects of an entity (e.g., battery life and screen of a mobile phone). However, users’ demand may be more beyond this kind of opinion summarization. Besides such coarse-grained summarization on aspects, one may prefer to read detailed but concise text of the opinion data for more information. In this paper, we propose a new framework for opinion summarization. Our goal is to assist users to get helpful opinion suggestions from reviews by only reading a short summary with a few informative sentences, where the quality of summary is evaluated in terms of both aspect coverage and viewpoints preservation. More specifically, we formulate the informative sentence selection problem in opinion summarization as a community leader detection problem, where a community consists of a cluster of sentences toward the same aspect of an entity and leaders can be considered as the most informative sentences of the corresponding aspect. We develop two effective algorithms to identify communities and leaders. Reviews of six products from Amazon.com are used to verify the effectiveness of our method for opinion summarization. L. Zhu (B) Information Sciences Institute, Los Angeles, USA e-mail: linhong@isi.edu S. Gao · S.J. Pan · H. Li Institute for Infocomm Research, Singapore, Singapore e-mail: gaosheng@i2r.a-star.edu.sg S.J. Pan e-mail: jspan@i2r.a-star.edu.sg H. Li e-mail: hli@i2r.a-star.edu.sg D. Deng · C. Shahabi University of Southern California, Los Angeles, USA e-mail: dingxiong.deng@usc.edu C. Shahabi e-mail: shahabi@usc.edu © Springer International Publishing Switzerland 2015 Ö. Ulusoy et al. (eds.), Recommendation and Search in Social Networks, Lecture Notes in Social Networks, DOI 10.1007/978-3-319-14379-8_9 165	automatic summarization;encode;evaluation function;information;mobile phone;pareto efficiency;selection algorithm;sentence extraction;spamming	Linhong Zhu;Sheng Gao;Sinno Jialin Pan;Haizhou Li;Dingxiong Deng;Cyrus Shahabi	2015		10.1007/978-3-319-14379-8_9	automatic summarization;political science;machine learning;pattern recognition;data mining	Web+IR	-26.492513495783783	-61.21711050741699	56724
9964013c44df530feb0d7d0eeebd16c3d80f0d45	a machine learning approach for semantic structuring of scientific charts in scholarly documents		Large scholarly repositories are designed to provide scientists and researchers with a wealth of information that is retrieved from data present in a variety of formats. A typical scholarly document contains information in a combined layout of texts and graphic images. Common types of graphics found in these documents are scientific charts that are used to represent data values in a visual format. Experimental results are rarely described without the aid of one form of a chart or another, whether it is 2D plot, bar chart, pie chart, etc. Metadata of these graphics is usually the only content that is made available for search by user queries. By processing the image content and extracting the data represented in the graphics, search engines will be able to handle more specific queries related to the data itself. In this paper we describe a machine learning based system that extracts and recognizes the various data fields present in a bar chart for semantic labeling. Our approach comprises of a graphics and text separation and extraction phase, followed by a component role classification for both text and graphic components that are in turn used for semantic analysis and representation of the chart. The proposed system is tested on a set of over 200 bar charts extracted from over 1,000 scientific articles in PDF format.	binary classification;chart;decision tree;graphical user interface;graphics;logic programming;machine learning;portable document format;random forest;scientific literature;semantic analysis (compilers);software deployment;video synopsis;web application;web search engine;web service	Rabah A. Al-Zaidy;C. Lee Giles	2017			machine learning;data mining;artificial intelligence;pie chart;computer science;graphics;information retrieval;bar chart;data field;structuring;metadata;search engine;chart	AI	-32.93549443233768	-56.70615031570107	56849
5f955abb1e950aa80ea2d67a745696403f54d001	the added value of facebook friends data in event attendance prediction	facebook classification learning algorithms;customer retention;logistic regression;random forests;media;network data;social networks;events;facebook;classifiers;predictive models;business and economics;social media	Article history: Received 23 April 2015 Received in revised form 20 November 2015 Accepted 21 November 2015 Available online 28 November 2015 This paper seeks to assess the added value of a Facebook user's friends data in event attendance prediction over and above user data. For this purpose we gathered data of users that have liked an anonymous European soccer teamon Facebook. In additionwe obtaineddata from all their friends. In order to assess the added value of friends data we have built two models for five different algorithms (Logistic Regression, Random Forest, Adaboost, Neural Networks and Naive Bayes). The baseline model contained only user data and the augmented model contained both user and friends data.We employedfive times two-fold cross-validation and theWilcoxon signed rank test to validate our findings. The results suggest that the inclusion of friends data in our predictive model increases the area under the receiver operating characteristic curve (AUC). Out of five algorithms, the increase is significant for three algorithms, marginally significant for one algorithm, and not significant for one algorithm. The increase inAUC ranged from0.21%-points to 0.82%-points. The analyses show that a top predictor is thenumber of friends that are attending the focal event. To the best of our knowledge this is the first study that evaluates the added value of friends network data over and above user data in event attendance prediction on Facebook. These findings clearly indicate that including network data in event predictionmodels is a viable strategy for improving model performance. © 2015 Elsevier B.V. All rights reserved.	adaboost;algorithm;baseline (configuration management);cross-validation (statistics);focal (programming language);kerrison predictor;logistic regression;naive bayes classifier;neural networks;random forest;receiver operating characteristic	Matthias Bogaert;Michel Ballings;Dirk Van den Poel	2016	Decision Support Systems	10.1016/j.dss.2015.11.003	random forest;media;social media;computer science;marketing;machine learning;data mining;predictive modelling;logistic regression;internet privacy;customer retention;world wide web;statistics;social network	HCI	-19.84400468393599	-52.805035737812474	57098
3cd7b15f5647e650db66fbe2ce1852e00c05b2e4	active, an extensible cataloging platform for automatic indexing of audiovisual content		The cost of manual metadata production is high, especially for audiovisual content, where a time-consuming inspection is usually required in order to identify the most appropriate annotations. There is a growing need from digital content industries for solutions capable of automating such a process. In this work we present ACTIVE, a platform for indexing and cataloging audiovisual collections through the automatic recognition of faces and speakers. Adopted algorithms are described and our main contributions on people clustering and caption-based people identification are presented. Results of experiments carried out on a set of TV shows and audio files are reported and analyzed. An overview of the whole architecture is presented as well, with a focus on chosen solutions for making the platform easily extensible (plug-ins) and for distributing CPU-intensive calculations across a network of computers.	algorithm;central processing unit;cluster analysis;computer;digital asset;digital recording;embedded system;experiment;general-purpose markup language;operating environment;plug-in (computing);search engine indexing;user profile;watermark (data file)	Maurizio Pintus;Maurizio Agelli;Felice Colucci;Nicola Corona;Alessandro Sassu;Federico Santamaria	2016		10.5220/0005722205740581	computer science;multimedia;world wide web;information retrieval	Vision	-30.71392144303234	-56.960007461498	57128
119d9e5efd7ba0b9210fca044544f7749536dbae	a very efficient approach to news title and content extraction on the web	content extraction;web news;web pages;machine learning;data extraction;web mining;computational efficiency;extraction method	We consider the problem of efficient and template-independent news extraction on the Web. The popular news extraction methods are based on visual information, and they can achieve good accuracy performance, but the computational efficiency is poor, because it is very time-consuming to render web page to obtain visual information. In this paper we propose an efficient and effective news extraction approach based on novel features. Our approach neither needs training nor needs visual information, so it is simple and very efficient. And it can extract news information from various news sites without using templates. In our experiments, the proposed approach achieves 99% accuracy over 5,671 news pages from 20 different news sites. And the efficiency is much faster than the baseline machine learning method using visual information.	baseline (configuration management);computation;experiment;machine learning;web page;world wide web	Hualiang Yan;Jianwu Yang	2011		10.1145/1998076.1998149	web mining;computer science;web page;multimedia;world wide web;information extraction;information retrieval	ML	-30.339439523360532	-58.14348289017593	57196
86ae6644830a104749bfde680f15edad80d1fa9e	learning in a pairwise term-term proximity framework for information retrieval	za4050 electronic information resources;information retrieval;proximity;retrieval model;learning to rank;qa76 computer software	Traditional ad hoc retrieval models do not take into account the closeness or proximity of terms. Document scores in these models are primarily based on the occurrences or non-occurrences of query-terms considered independently of each other. Intuitively, documents in which query-terms occur closer together should be ranked higher than documents in which the query-terms appear far apart.  This paper outlines several term-term proximity measures and develops an intuitive framework in which they can be used to fully model the proximity of all query-terms for a particular topic. As useful proximity functions may be constructed from many proximity measures, we use a learning approach to combine proximity measures to develop a useful proximity function in the framework. An evaluation of the best proximity functions show that there is a significant improvement over the baseline ad hoc retrieval model and over other more recent methods that employ the use of single proximity measures.	baseline (configuration management);centrality;hoc (programming language);information retrieval	Ronan Cummins;Colm O'Riordan	2009		10.1145/1571941.1571986	proximity search;computer science;machine learning;data mining;distance;information retrieval;learning to rank	Web+IR	-27.334405425223768	-62.58891082933539	57432
e0732dd16669f2e843159fb25f240a2d22e74d3d	beyond topicality : a two stage view of relevance and the retrieval process	information systems;information retrieval;user satisfaction information;algorithms;search strategies;relevance information retrieval	Abstract   Topicality is an operationally necessary but insufficient condition for requestor judged relevance. Documents are independent of one another as to any judgement of their topicality but not independent as to any judgement of their relevance which is a function of their informativeness to a requestor. Recall depends solely upon topicality but precision depends upon informativeness as well.  A retrieval system which aspires to the retrieval of relevant documents should have a second stage which will order the topical set in a manner so as to provide maximum informativeness to the requestor. Should a system be concerned only with topicality then a two stage system which generates a high recall set and discards imprecise documents by measuring their distance from a seed document can be iterated to provide topicality feedback without user input.	relevance	Bert R. Boyce	1982	Inf. Process. Manage.	10.1016/0306-4573(82)90033-4	relevance;computer science;data mining;database;information retrieval;information system	Web+IR	-33.32114455727049	-58.62724133965458	57700
e3719b83b139879a8fb8d48b3dba768f6bd9ccab	interaction mining: the new frontier of call center analytics	argumentative analysis;customer rating	In this paper, we present our solution for pragmatic analysis of call center conversations in order to provide useful insights for enhancing Call Center Analytics to a level that will enable new metrics and key performance indicators (KPIs) beyond the standard approach. These metrics rely on understanding the dynamics of conversations by highlighting the way participants discuss about topics. By doing that we can detect situations that are simply impossible to detect with standard approaches such as controversial topics, customer-oriented behaviors and also predict customer ratings.		Vincenzo Pallotta;Rodolfo Delmonte;Lammert Vrieling;David Walker	2011			analytics;engineering;data science;marketing;data mining;customer intelligence	ML	-23.60455104564592	-58.911111800151076	57836
497dd1b49496d71305bdf89b7cec1babe3af171f	asynchronous iterative computations with web information retrieval structures: the pagerank case	search engine;cluster computing;web pages;computer model;distributed memory machine;ranking function;web search engine;large scale;web information retrieval;common sense	There are several ideas being used today for Web information retrieval, and specifically in Web search engines. The PageRank algorithm is one of those that introduce a content-neutral ranking function over Web pages. This ranking is applied to the set of pages returned by the Google search engine in response to posting a search query. PageRank is based in part on two simple common sense concepts: (i)A page is important if many important pages include links to it. (ii)A page containing many links has reduced impact on the importance of the pages it links to. In this paper we focus on asynchronous iterative schemes to compute PageRank over large sets of Web pages. The elimination of the synchronizing phases is expected to be advantageous on heterogeneous platforms. The motivation for a possible move to such large scale distributed platforms lies in the size of matrices representing Web structure. In orders of magnitude: $10^{10}$ pages with $10^{11}$ nonzero elements and $10^{12}$ bytes just to store a small percentage of the Web (the already crawled); distributed memory machines are necessary for such computations. The present research is part of our general objective, to explore the potential of asynchronous computational models as an underlying framework for very large scale computations over the Grid. The area of ``internet algorithmics'' appears to offer many occasions for computations of unprecedent dimensionality that would be good candidates for this framework.	computation;information retrieval;iterative method;pagerank	Giorgios Kollias;Efstratios Gallopoulos;Daniel B. Szyld	2005			backlink;web modeling;web query classification;web search engine;computer cluster;computer science;theoretical computer science;web crawler;web page;hits algorithm;web search query;world wide web;rewrite engine;information retrieval;search engine	Web+IR	-31.80842415740001	-54.93599716846387	57844
5a7e809c725eb00b324813c738836b626566f663	extracting adverse drug reactions from social media		The potential benefits of mining social media to learn about adverse drug reactions (ADRs) are rapidly increasing with the increasing popularity of social media. Unknown ADRs have traditionally been discovered by expensive post-marketing trials, but recent work has suggested that some unknown ADRs may be discovered by analyzing social media. We propose three methods for extracting ADRs from forum posts and tweets, and compare our methods with several existing methods. Our methods outperform the existing methods in several scenarios; our filtering method achieves the highest F1 and precision on forum posts, and our CRF method achieves the highest precision on tweets. Furthermore, we address the difficulty of annotating social media on a large scale with an alternate evaluation scheme that takes advantage of the ADRs listed on drug labels. We investigate how well this alternate evaluation approximates a traditional evaluation using human annotations.	conditional random field;multinomial logistic regression;naive bayes classifier;sensor;social media	Andrew Yates;Nazli Goharian;Ophir Frieder	2015			data mining;internet privacy;world wide web	AI	-22.4617281413014	-53.45579031872711	57863
af64ef0853d5afc22b37b32fc2589cd4098f7b73	the blogosphere as an excitable social medium: richter's and omori's law in media coverage	excitable media;word frequency;social system;statistical human dynamics;empirical evidence;media coverage;empirical power laws;opinion formation;power law;excitable social system	We study the dynamics of public media attention by monitoring the content of online blogs. Social and media events can be traced by the propagation of word frequencies of related keywords. Media events are classified as exogenous – where blogging activity is triggered by an external news item – or endogenous where word frequencies build up within a blogging community without external influences. We show that word occurrences show statistical similarities to earthquakes. The size distribution of media events follows a Gutenberg-Richter law, the dynamics of media attention before and after the media event follows Omori’s law. We present further empirical evidence that for media events of endogenous origin the overall public reception of the event is correlated with the behavior of word frequencies at the beginning of the event, and is to a certain degree predictable. These results may imply that the process of opinion formation in a human society might be related to effects known from excitable media.	blog;blogosphere;excitable medium;existential quantification;software propagation;virtual reality;word lists by frequency	Peter Klimek;Werner Bayer;Stefan Thurner	2011	CoRR	10.1016/j.physa.2011.05.033	power law;empirical evidence;social system;word lists by frequency	Web+IR	-22.998686714693335	-52.71567647090194	57909
9a4f3f87bfe008a2a396b712bc73beaf419ebc42	ad hoc table retrieval using semantic similarity		We introduce and address the problem of ad hoc table retrieval: answering a keyword query with a ranked list of tables. This task is not only interesting on its own account, but is also being used as a core component in many other table-based information access scenarios, such as table completion or table mining. The main novel contribution of this work is a method for performing semantic matching between queries and tables. Specifically, we (i) represent queries and tables in multiple semantic spaces (both discrete sparse and continuous dense vector representations) and (ii) introduce various similarity measures for matching those semantic representations. We consider all possible combinations of semantic representations and similarity measures and use these as features in a supervised learning model. Using a purpose-built test collection based on Wikipedia tables, we demonstrate significant and substantial improvements over a state-of-the-art baseline.	baseline (configuration management);entity;hoc (programming language);information access;requirement;semantic matching;semantic similarity;sparse matrix;supervised learning;web application;wikipedia	Shuo Zhang;Krisztian Balog	2018		10.1145/3178876.3186067	data mining;semantic similarity;supervised learning;computer science;semantic matching;information access;ranking	Web+IR	-28.273323874505348	-62.576447790541074	58060
2840ad17294e360f48981faf4c89ff75a9be4827	mining web informative structures and contents based on entropy analysis	information structure;search engine;information extraction web informative structure mining entropy analysis news web site hyperlinked documents index pages table of contents toc pages hyperlink induced topics search hits entropy based analysis mechanism anchor texts lamis intrasite redundant information navigation panels advertisements copy announcements infodiscoverer article pages entropy information information measures informative content blocks search engines information agents web crawlers;information extraction;search engines;information retrieval;complex structure;publishing;text analysis;table of contents;data mining;information content;link analysis;performance improvement;informative structure;indexation;web sites;entropy navigation ieee news information analysis data mining search engines algorithm design and analysis internet web pages computer society;entropy;information agent;anchor text;hubs and authorities;search engines publishing web sites data mining information retrieval text analysis	We study the problem of mining the informative structure of a news Web site that consists of thousands of hyperlinked documents. We define the informative structure of a news Web site as a set of index pages (or referred to as TOC, i.e., table of contents, pages) and a set of article pages linked by these TOC pages. Based on the Hyperlink Induced Topics Search (HITS) algorithm, we propose an entropy-based analysis (LAMIS) mechanism for analyzing the entropy of anchor texts and links to eliminate the redundancy of the hyperlinked structure so that the complex structure of a Web site can be distilled. However, to increase the value and the accessibility of pages, most of the content sites tend to publish their pages with intrasite redundant information, such as navigation panels, advertisements, copy announcements, etc. To further eliminate such redundancy, we propose another mechanism, called InfoDiscoverer, which applies the distilled structure to identify sets of article pages. InfoDiscoverer also employs the entropy information to analyze the information measures of article sets and to extract informative content blocks from these sets. Our result is useful for search engines, information agents, and crawlers to index, extract, and navigate significant information from a Web site. Experiments on several real news Web sites show that the precision and the recall of our approaches are much superior to those obtained by conventional methods in mining the informative structures of news Web sites. On the average, the augmented LAMIS leads to prominent performance improvement and increases the precision by a factor ranging from 122 to 257 percent when the desired recall falls between 0.5 and 1. In comparison with manual heuristics, the precision and the recall of InfoDiscoverer are greater than 0.956.	accessibility;algorithm;cb unix;experiment;hand coding;heuristic (computer science);hyperlink;ibm notes;information extraction;intelligent agent;mahdiyar;optical disc authoring;parsing;partial template specialization;precision and recall;principle of good enough;sensitivity and specificity;software deployment;web application;web search engine	Hung-Yu Kao;Shian-Hua Lin;Jan-Ming Ho;Ming-Syan Chen	2004	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2004.1264821	computer science;data mining;database;world wide web;information extraction;information retrieval;search engine	Web+IR	-29.68208189901077	-55.462036711938694	58305
67ea0bc6d4e2123ad4618976bc0e48b43ac2081b	analysis and early detection of rumors in a post disaster scenario	microblogs;twitter;rumor;disaster;chennai floods	The use of online social media for post-disaster situation analysis has recently become popular. However, utilizing information posted on social media has some potential hazards, one of which is rumor. For instance, on Twitter, thousands of verified and non-verified users post tweets to convey information, and not all information posted on Twitter is genuine. Some of them contain fraudulent and unverified information about different facts/incidents - such information are termed as rumors. Identification of such rumor tweets at early stage in the aftermath of a disaster is the main focus of the current work. To this end, a probabilistic model is adopted by combining prominent features of rumor propagation. Each feature has been coded individually in order to extract tweets that have at least one rumor propagation feature. In addition, content-based analysis has been performed to ensure the contribution of the extracted tweets in terms of probability of being a rumor. The proposed model has been tested over a large set of tweets posted during the 2015 Chennai Floods. The proposed model and other four popular baseline rumor detection techniques have been compared with human annotated real rumor data, to check the efficiency of the models in terms of (i) detection of belief rumors and (ii) accuracy at early stage. It has been observed that around 70% of the total endorsed belief rumors have been detected by proposed model, which is superior to other techniques. Finally, in terms of accuracy, the proposed technique also achieved 0.9904 for the considered disaster scenario, which is better than the other methods.		Tamal Mondal;Prithviraj Pramanik;Indrajit Bhattacharya;Naiwrita Boral;Saptarshi Ghosh	2018	Information Systems Frontiers	10.1007/s10796-018-9837-8	computer science;data mining;management science;situation analysis;rumor;microblogging;social media;statistical model	AI	-20.929691506613693	-55.35832008279728	58313
396addc8b7514f16515f363d5281cd4c39c35fdc	icdar2015 competition on video script identification (cvsi 2015)	google;competition;multi lingual ocr;competition video scripts identification multi lingual ocr multi lingual video text;video scripts identification;multi lingual video text	This paper presents the final results of the ICDAR 2015 Competition on Video Script Identification. A description and performance of the participating systems in the competition are reported. The general objective of the competition is to evaluate and benchmark the available methods on word-wise video script identification. It also provides a platform for researchers around the globe to particularly address the video script identification problem and video text recognition in general. The competition was organised around four different tasks involving various combinations of scripts comprising tri-script and multi-script scenarios. The dataset used in the competition comprised ten different scripts. In total, six systems were received from five participants over the tasks offered. This report details the competition dataset specifications, evaluation criteria, summary of the participating systems and their performance across different tasks. The systems submitted by Google Inc. were the winner of the competition for all the tasks, whereas the systems received from Huazhong University of Science and Technology (HUST) and Computer Vision Center (CVC) were very close competitors.	benchmark (computing);common criteria;computer vision;international conference on document analysis and recognition;microsoft script editor;optical character recognition;triangular function	Nabin Sharma;Ranju Mandal;Rabi Sharma;Umapada Pal;Michael Blumenstein	2015	2015 13th International Conference on Document Analysis and Recognition (ICDAR)	10.1109/ICDAR.2015.7333950	competition;speech recognition;computer science;multimedia		-31.22182029360996	-64.11040092924017	58478
e553ca34a5fdc5880d17e24b470ef561e9a10225	making use of category structure for multi-class classification	feature selection;machine learning;classification;deep web;data mining	Multi-class classification is the task of organizing data samples into multiple predefined categories. In this thesis, we address two different research problems of multi-class classification, one specific and the other general. The first and specific problem is to categorize structured data sources on the Web. While prior works use all features, once extracted from search interfaces, we further refine the feature set. In our approach, we use only the text content of the search interfaces. We choose a subset of features, which is suited to classify web sources, by our feature selection technique with a new metric and selection scheme. Using the aggressive feature selection approach, together with a multi-class Support Vector Machine categorizer, we obtained high classification performance in an evaluation over real web data. The second and general task is to develop a multi-label classification algorithm. In a multi-label classification problem, a data sample can be assigned to one or more categories. Given a multi-label problem of m categories, the commonly used One-Vs-All (OVA) approach transforms the problem into m independent binary classifications between each category and the rest (the category’s complement). Based on the OVA approach, we propose a new method named Multi-Pair (MP). This MP method decomposes further each of the OVA binary classifications into multiple smaller and easier pair comparisons between a category and a subset of the category’s complement. Furthermore, we incorporate the SCutFBR.1 thresholding strategy into the MP method. In our experiments with three benchmark text collections, the MP method outperforms the OVA approach in both cases with and without SCutFBR.1. A common aspect of our works is that we make use of category structure in our feature selection and multi-label classification methods. This is the aspect that distinguishes our works from prior researches.	algorithm;benchmark (computing);binary classification;categorization;directed acyclic graph;experiment;feature selection;multi-label classification;organizing (structure);sinewave synthesis;support vector machine;thresholding (image processing);world wide web;xiii	Hieu Quang Le	2010			support vector machine;web query classification;data model;binary number;categorization;machine learning;artificial intelligence;thresholding;feature selection;multiclass classification;computer science;pattern recognition	Web+IR	-20.811995063597376	-62.332515376717225	58480
5e0c8c8d0114e8ba1ef7b8d2c62171878c7c4d78	applying optimal stopping theory to improve the performance of ontology refinement methods	particle measurements;data collection;semantics;text analysis;ontologies artificial intelligence;textual data optimal stopping theory ontology refinement methods web 2 0 applications data acquisition;internet;force measurement;optimal stopping;coherence;ontologies;ontologies semantics coherence force measurement particle measurements meteorology correlation;data quality;text analysis data acquisition internet ontologies artificial intelligence;correlation;ontology construction;processing speed;domain ontology;data acquisition;meteorology	Recent research shows the potential of utilizing data collected through Web 2.0 applications to capture domain evolution. Relying on external data sources, however, often introduces delays due to the time spent retrieving data from these sources. The method introduced in this paper streamlines the data acquisition process by applying optimal stopping theory. An extensive evaluation demonstrates how such an optimization improves the processing speed of an ontology refinement component which uses Delicious to refine ontologies constructed from unstructured textual data while having no significant impact on the quality of the refinement process. Domain experts compare the results retrieved from optimal stopping with data obtained from standardized techniques to assess the effect of optimal stopping on data quality and the created domain ontology.	cache (computing);computation;data acquisition;data quality;diagram;mathematical optimization;ontology (information science);ontology learning;optimal stopping;refinement (computing);text corpus;web 2.0	Albert Weichselbraun;Gerhard Wohlgenannt;Arno Scharl	2011	2011 44th Hawaii International Conference on System Sciences	10.1109/HICSS.2011.72	the internet;data quality;coherence;optimal stopping;computer science;ontology;artificial intelligence;data science;data mining;database;semantics;data acquisition;world wide web;correlation;information retrieval;statistics;data collection	DB	-29.65569915817476	-58.10242475043515	58509
b1e66aa2133ee45582b6e8d8bd4fb2f11381d2e4	university of glasgow at trec 2015: experiments with terrier in contextual suggestion, temporal summarisation and dynamic domain tracks		In TREC 2015, we focus on tackling the challenges posed by the Contextual Suggestion, Temporal Summarisation and Dynamic Domain tracks. For Contextual Suggestion, we investigate the use of user-generated data in location-based social networks (LBSN) to suggest venues. For Temporal Summarisation, we examine features for event summarisation that explicitly model the entities involved in the events. Meanwhile, for the Dynamic Domain track, we explore resource selection techniques for identifying the domain of interest and diversifying sub-topic intents.	entity;location-based service;social network;user-generated content	Richard McCreadie;Saul Vargas;Craig MacDonald;Iadh Ounis;Stuart Mackie;Jarana Manotumruksa;Graham McDonald	2015			data mining;world wide web;information retrieval	Web+IR	-26.018094358174416	-52.243262273389064	58519
3d798b253b08f3645eb6f5b092b2219b25df6ca6	mining missing hyperlinks from human navigation traces: a case study of wikipedia	wikipedia;browsing;link prediction;navigation;human computation;wikispeedia	Hyperlinks are an essential feature of the World Wide Web. They are especially important for online encyclopedias such as Wikipedia: an article can often only be understood in the context of related articles, and hyperlinks make it easy to explore this context. But important links are often missing, and several methods have been proposed to alleviate this problem by learning a linking model based on the structure of the existing links. Here we propose a novel approach to identifying missing links in Wikipedia. We build on the fact that the ultimate purpose of Wikipedia links is to aid navigation. Rather than merely suggesting new links that are in tune with the structure of existing links, our method finds missing links that would immediately enhance Wikipedia's navigability. We leverage data sets of navigation paths collected through a Wikipedia-based human-computation game in which users must find a short path from a start to a target article by only clicking links encountered along the way. We harness human navigational traces to identify a set of candidates for missing links and then rank these candidates. Experiments show that our procedure identifies missing links of high quality.	digital footprint;display resolution;encyclopedias;experiment;human-based computation;hyperlink;navigation;tracing (software);wikipedia;world wide web	Robert West;Ashwin Paranjape;Jure Leskovec	2015	Proceedings of the ... International World-Wide Web Conference. International WWW Conference	10.1145/2736277.2741666	navigation;data mining;brand;internet privacy;world wide web	Web+IR	-31.20084096737261	-53.12831920883859	58692
9a4801513b965f9991359387b0daf7fc7bf8a0fc	on the performance of evolutionary algorithms in biomedical keyword clustering	document clustering;information retrieval;data collection;non dominated sorting genetic algorithm;life sciences;evolutionary algorithms;query extension;evolution strategy;genetic algorithm;keyword clustering;evolutionary algorithm;bioinformatics	In the field of life sciences it often turns out to be a challenge to quickly find the desired information due to the huge amount of available data. The research area of information retrieval (IR) addresses this problem and tries to provide suitable solutions. One of the approaches used in IR is query extension based on keyword or document clusters.  In this paper we present a deep analysis of a keyword clustering approach using four different kinds of evolutionary algorithms, namely evolution strategy (ES), genetic algorithm (GA), genetic algorithm with strict offspring selection (OSGA), and the multi-objective elitist non-dominated sorting genetic algorithm (NSGA-II).  We have identified features that characterize solution candidates for the keyword clustering problem, e.g., the number of documents covered and how well the identified clusters of keywords match with the occurrence of keywords in the given set of documents. The use of these features and how evolutionary algorithms can be used to solve the optimization of keyword clusters is shown in this paper.  To test the here presented approach we used a real world data set provided within the TREC-9 conference; this data collection includes information about approximately 36,000 documents collected from the PubMed database.  In the results section we compare the performance of the here tested evolutionary algorithms and see that especially ES and NSGA-II produce meaningful results for this documents collection. This approach based on evolutionary algorithms shall be used further on in automated query extension for biomedical information retrieval in PubMed.	cluster analysis;evolution strategy;evolutionary algorithm;genetic algorithm;information retrieval;mathematical optimization;multi-objective optimization;pubmed;sorting	Viktoria Dorfer;Stephan M. Winkler;Thomas Kern;Sophie A. Blank;Gerald Petz;Patrizia Faschang	2011		10.1145/2001858.2002041	evolutionary music;genetic algorithm;cultural algorithm;computer science;artificial intelligence;machine learning;evolutionary algorithm;data mining;evolution strategy;information retrieval;data collection	DB	-27.335006636113864	-59.891635832130795	58742
66a27a3f487c6b1a479af7db7b63aa1ad968dc34	enterprise, qa, robust and terabyte experiments with hummingbird searchserver at trec 2005	question answering	Hummingbird participated in 6 tasks of TREC 2005: the email known-item search task of the Enterprise Track, the document ranking task of the Question Answering Track, the ad hoc topic relevance task of the Robust Retrieval Track, and the adhoc, efficiency and named page finding tasks of the Terabyte Track. In the email known-item task, SearchServer found the desired message in the first 10 rows for more than 80% of the 125 queries. In the document ranking task, SearchServer returned an answering document in the first 10 rows for more than 90% of the 50 questions. In the robustness task, SearchServer found a relevant document in the first 10 rows for 88% of the 50 short (title) topics. In the terabyte adhoc and efficiency tasks, SearchServer found a relevant document in the first 10 rows for more than 90% of the 50 title topics. A new retrieval measure, First Relevant Score, is investigated; it is found to more accurately reflect known-item differences than reciprocal rank and to better reflect robustness across topics than the primary measure of the Robust track.	email;hoc (programming language);question answering;ranking (information retrieval);relevance;software quality assurance;terabyte	Stephen Tomlinson	2005			information retrieval;data mining;robustness (computer science);computer science;row;question answering;hummingbird;terabyte;ranking	Web+IR	-31.202171845739162	-62.88917328861532	58746
0c8803338b1e362982ab545c928875ae859020e5	gender inference using statistical name characteristics in twitter	gender inference;classification;social networks;twitter;experimentation	Much attention has been given to the task of gender inference of Twitter users. Although names are strong gender indicators, the names of Twitter users are rarely used as a feature; probably due to the high number of ill-formed names, which cannot be found in any name dictionary. Instead of relying solely on a name database, we propose a novel name classifier. Our approach extracts characteristics from the user names and uses those in order to assign the names to a gender. This enables us to classify international first names as well as ill-formed names.	computation;dictionary;feature vector;ground truth;linear classifier;reblogging;stemming;user (computing)	Juergen Mueller;Gerd Stumme	2016		10.1145/2955129.2955182	biological classification;computer science;data science;data mining;world wide web;social network	NLP	-21.81964429674823	-59.288728116002936	58870
742f0d4ea14a0f4c95b0f90dfa557fb452690781	applying data mining to pseudo-relevance feedback for high performance text retrieval	text analysis data mining pattern classification relevance feedback;probabilistic weighting function data mining pseudo relevance feedback high performance text retrieval system text classification labeled passages blind feedback;pseudo relevance feedback;text analysis;data mining;probabilistic weighting function;text classification;blind feedback;text retrieval;pattern classification;weight function;labeled passages;high performance text retrieval system;high performance;relevance feedback;data mining feedback information retrieval text categorization training data information technology testing computer science labeling supervised learning	In this paper, we investigate the use of data mining, in particular the text classification and co-training techniques, to identify more relevant passages based on a small set of labeled passages obtained from the blind feedback of a retrieval system. The data mining results are used to expand query terms and to re-estimate some of the parameters used in a probabilistic weighting function. We evaluate the data mining based feedback method on the TREC HARD data set. The results show that data mining can be successfully applied to improve the text retrieval performance. We report our experimental findings in detail.	co-training;data mining;document classification;document retrieval;relevance feedback;text mining;weight function	Xiangji Huang;Yan Rui Huang;Miao Wen;Aijun An;Yang Liu;Josiah Poon	2006	Sixth International Conference on Data Mining (ICDM'06)	10.1109/ICDM.2006.22	concept mining;text mining;weight function;computer science;pattern recognition;data mining;mathematics;data stream mining;information retrieval;statistics	DB	-21.252509279772266	-63.416738356264354	58886
fc5034f9104bfce6695d490aa5547fa4e35b8cc0	a prospect-guided global query expansion strategy using word embeddings		Abstract The effectiveness of query expansion methods depends essentially on identifying good candidates, or prospects, semantically related to query terms. Word embeddings have been used recently in an attempt to address this problem. Nevertheless query disambiguation is still necessary as the semantic relatedness of each word in the corpus is modeled, but choosing the right terms for expansion from the standpoint of the un-modeled query semantics remains an open issue. In this paper we propose a novel query expansion method using word embeddings that models the global query semantics from the standpoint of prospect vocabulary terms. The proposed method allows to explore query-vocabulary semantic closeness in such a way that new terms, semantically related to more relevant topics, are elicited and added in function of the query as a whole. The method includes candidates pooling strategies that address disambiguation issues without using exogenous resources. We tested our method with three topic sets over CLEF corpora and compared it across different Information Retrieval models and against another expansion technique using word embeddings as well. Our experiments indicate that our method achieves significant results that outperform the baselines, improving both recall and precision metrics without relevance feedback.	query expansion;word embedding	Francis C. Fernández-Reyes;Jorge Hermosillo Valadez;Manuel Montes-y-Gómez	2018	Inf. Process. Manage.	10.1016/j.ipm.2017.09.001	information retrieval;data mining;computer science;web query classification;query expansion;ranking (information retrieval);relevance feedback;sargable;query language;boolean conjunctive query;query optimization	NLP	-31.115409567392707	-60.04316652900333	58962
18f931e4540e0bc9a9d2055eb96716b17ddd52f0	fusing visual and textual retrieval techniques to effectively search large collections of wikipedia images	color histogram;col;080100 artificial intelligence and image processing;visual search;xml document;hough transform;content based image retrieval	This paper presents an experimental study that examines the performance of various combination techniques for content-based image retrieval using a fusion of visual and textual search results. The evaluation is comprehensively benchmarked using more than 160,000 samples from INEX-MM2006 images dataset and the corresponding XML documents. For visual search, we have successfully combined Hough transform, Object’s color histogram, and Texture (H.O.T). For comparison purposes, we used the provided UvA features. Based on the evaluation, our submissions show that Uva+Text combination performs most effectively, but it is closely followed by our H.O.T(visual only) feature. Moreover, H.O.T+Text performance is still better than UvA (visual) only. These findings show that the combination of effective text and visual search results can improve the overall performance of CBIR in Wikipedia collections which contain a heterogeneous (i.e. wide) range of genres and topics.	color histogram;content-based image retrieval;experiment;hough transform;modal logic;ontology components;text-based (computing);wikipedia;xml	C. Lau;Dian Tjondronegoro;Jinglan Zhang;Shlomo Geva;Yue Liu	2006		10.1007/978-3-540-73888-6_34	computer vision;visual word;computer science;multimedia;information retrieval	Web+IR	-30.757318530060026	-62.13490224706396	59011
c1a8f49423c38d69f945432afb94972bd4c46029	multi-document abstractive summarization using chunk-graph and recurrent neural network		Automatic multi-document abstractive summarization system is used to summarize several documents into a short one with generated new sentences. Many of them are based on word-graph and ILP method, and lots of sentences are ignored because of the heavy computation load. To reduce computation and generate readable and informative summaries, we propose a novel abstractive multi-document summarization system based on chunk-graph (CG) and recurrent neural network language model (RNNLM). In our approach, A CG which is based on word-graph is constructed to organize all information in a sentence cluster, CG can reduce the size of graph and keep more semantic information than word-graph. We use beam search and character-level RNNLM to generate readable and informative summaries from the CG for each sentence cluster, RNNLM is a better model to evaluate sentence linguistic quality than n-gram language model. Experimental results show that our proposed system outperforms all baseline systems and reach the state-of-art systems, and the system with CG can generate better summaries than that with ordinary word-graph.	artificial neural network;automatic summarization;baseline (configuration management);beam search;computation;human-readable medium;information;language model;microsoft word for mac;multi-document summarization;n-gram;recurrent neural network;upsampling	Jianwei Niu;Huan Chen;Qingjuan Zhao;Limin Su;Mohammed Atiquzzaman	2017	2017 IEEE International Conference on Communications (ICC)	10.1109/ICC.2017.7996331	automatic summarization;big data;semantics;cluster analysis;computer science;artificial intelligence;language model;recurrent neural network;machine learning;sentence;beam search	NLP	-25.868820036954098	-65.86381045358961	59016
527d2036b4b860b35ac3e175cd79fd7580205e00	measuring semantic relatedness with vector space models and random walks	graph random walk model;human similarity rating;concept categorization;vector space model;vector space;standard task;semantic relatedness;semantic priming;local view	Both vector space models and graph random walk models can be used to determine similarity between concepts. Noting that vectors can be regarded as local views of a graph, we directly compare vector space models and graph random walk models on standard tasks of predicting human similarity ratings, concept categorization, and semantic priming, varying the size of the dataset from which vector space and graph are extracted.	categorization;semantic similarity	Amac Herdagdelen;Katrin Erk;Marco Baroni	2009			random graph;semantic similarity;combinatorics;discrete mathematics;null model;pattern recognition;mathematics;exponential random graph models;random geometric graph	NLP	-26.138394628058364	-64.6563706504668	59026
3abdd75db95d168defe452ef6013aa60796039bf	improving the effectiveness of multimedia summarization of judicial debates through ontological query expansion	ontology based query expansion;multimedia summarization	The growing amount of multimedia data acquired during courtroom debates makes information and knowledge management in judicial domain a real challenge. In this paper we tackle the problem of summarizing this large amount of multimedia data in order to support fast navigation of the streams, efficient access to the information and effective representation of relevant contents needed during the judicial process. In particular, we propose an ontology enhanced multimedia summarization environment able to derive a synthetic representation of audio/video contents by a limited loss of meaningful information while overcoming the information overload problem.	automatic summarization;browsing;information overload;knowledge management;query expansion;semantic query;storyboard;synthetic intelligence	Elisabetta Fersini;Fabio Sartori	2011		10.1007/978-3-642-22056-2_48	query expansion;multi-document summarization;computer science;automatic summarization;data mining;database;information retrieval	AI	-32.77365157454793	-56.21197821337224	59159
080d5d455f3293ab9558e646a55d2828f0efae2f	question identification on twitter	empirical study;question identification;rule based;twitter;microblogs	"""In this paper, we investigate the novel problem of automatic question identification in the microblog environment. It contains two steps: detecting tweets that contain questions (we call them """"interrogative tweets"""") and extracting the tweets which really seek information or ask for help (so called """"qweets"""") from interrogative tweets. To detect interrogative tweets, both traditional rule-based approach and state-of-the-art learning-based method are employed. To extract qweets, context features like short urls and Tweet-specific features like Retweets are elaborately selected for classification. We conduct an empirical study with sampled one hour's English tweets and report our experimental results for question identification on Twitter."""	logic programming;sensor;url shortening	Baichuan Li;Xiance Si;Michael R. Lyu;Irwin King;Edward Y. Chang	2011		10.1145/2063576.2063996	rule-based system;computer science;artificial intelligence;microblogging;data mining;internet privacy;empirical research;world wide web	NLP	-21.276348695870936	-56.790171400224125	59191
81a3170a1878e416fc3d4f72c37c78ce1d3eceb9	a semantic approach to concept lattice-based information retrieval	cousin concepts;concept lattice based information retrieval;68p20;information retrieval;concept lattices;semantic information retrieval;similarity;06b99;formal concept analysis	The volume of available information is growing, especially on the web, and in parallel the questions of the users are changing and becoming harder to satisfy. Thus there is a need for organizing the available information in a meaningful way in order to guide and improve document indexing for information retrieval applications taking into account more complex data such as semantic relations. In this paper we show that Formal Concept Analysis (FCA) and concept lattices provide a suitable and powerful support for such a task. Accordingly, we use FCA to compute a concept lattice, which is considered both a semantic index to organize documents and a search space to model terms. We introduce the notions of cousin concepts and classification-based reasoning for navigating the concept lattice and retrieve relevant information based on the content of concepts. Finally, we detail a real-world experiment and show that the present approach has very good capabilities for semantic indexing and document retrieval.	algorithm;archive;centrality;computer cluster;data mining;data structure;database;disjunctive normal form;document retrieval;formal concept analysis;information retrieval;latent semantic analysis;numerical analysis;organizing (structure);parallel computing;world wide web	Víctor Codocedo;Ioanna Lykourentzou;Amedeo Napoli	2014	Annals of Mathematics and Artificial Intelligence	10.1007/s10472-014-9403-0	explicit semantic analysis;relevance;similarity;cognitive models of information retrieval;computer science;formal concept analysis;theoretical computer science;machine learning;concept search;data mining;lattice miner;vector space model;information retrieval;human–computer information retrieval	Web+IR	-29.265649911685074	-57.65149279171899	59232
3028a3dd3ae9d65fb585a41e4566c582aaa74a59	finding similarity relations in presence of taxonomic relations in ontology learning systems	selected works;ontologies learning systems process control intelligent control computational intelligence data mining learning automata humans information retrieval terminology;similarity relation;context extraction taxonomic relations ontology learning systems ontological relations nontaxonomic similarity relation extraction;ontologies artificial intelligence;ontology learning;ontologies artificial intelligence learning artificial intelligence;bepress;learning artificial intelligence;extraction method	Ontology learning tries to find ontological relations, by an automatic process. Similarity relationships are one of non-taxonomic relations which may be included in ontology. Our idea is that in presence of taxonomic relations we are able to extract more useful non-taxonomic similarity relations. In this paper we investigate the specifications of an implemented system for extracting these relations by means of new context extraction method which uses taxonomic relations	coefficient;experiment;jaccard index;lexico;ontology learning;similarity measure;taxonomy (general)	A. R. Vazifedoost;Farhad Oroumchian;Masoud Rahgozar	2007	2007 IEEE Symposium on Computational Intelligence and Data Mining	10.1109/CIDM.2007.368875	natural language processing;computer science;ontology;artificial intelligence;machine learning;data mining	AI	-24.99022983106162	-63.94849658094421	59233
6092d508dbf2934372fe7a4c12670e8623d14c6a	evolution of communities on twitter and the role of their leaders during emergencies	cohesive behaviors twitter community evolution emergencies leader role information dissemination communication channel nongovernment emergency management organizations government emergency management organizations natural language processing social network analysis sna nlp 2011 japan tsunami qualitative data community members;emergency management;communities twitter tsunami natural language processing logic gates communication networks;social sciences computing;information dissemination;social networking online;social influence;twitter;social media;social sciences computing emergency management information dissemination natural language processing social networking online;natural language processing	Twitter is presently utilized as a channel of communication and information dissemination. At present, government and non-government emergency management organizations utilize Twitter to disseminate emergency relevant information. However, these organizations have limited ability to evaluate the Twitter communication in order to discover communication patterns, key players, and messages that are being propagated through Twitter regarding the event. More importantly there is a general lack of knowledge of who are the individuals or organizations that disseminate warning information, provide confirmations of an event and associated actions, and urge others to take action. This paper presents a methodology that shows how Natural Language Processing (NLP) and Social Network Analysis (SNA) can aid in addressing these issues. The methodology, in addition to qualitative data collected during on-site interviews and publicly available information, was successfully applied to a Twitter data set collected during 2011 Japan tsunami. NLP techniques were applied to extract actionable messages. Based on the messages extracted by NLP, SNA was used to construct a network of actionable messages. While SNA discovered communities and extracted the community leaders, NLP was used to determine the behavior of the community members and the role of the community leaders. Therefore, the proposed methodology automatically finds communities, evaluates its members' behaviors, and authenticates cohesive behaviors of the community members during emergencies. Moreover, the methodology efficiently finds the leaders of the communities, while also identifying their role in communities.	authentication;natural language processing;social network analysis	Yulia Tyshchuk;Hao Li;Heng Ji;William A. Wallace	2013	2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2013)	10.1145/2492517.2492657	social media;social influence;computer science;data mining;world wide web;computer security;emergency management	AI	-22.178551109835187	-54.30551471577069	59429
2c2af6a5a49f465f977c889c315fc8a6445404e0	automs: automated ontology mapping through synthesis of methods	ontology mapping	AUTOMS is a tool for the automatic alignment of domain ontologies. To ensure high precision and recall with the minimum human involvement, AUTOMS integrates several matching methods. This paper presents the tool and the results obtained for the ontologies within the framework of the OAEI 2006 contest. Particularly, the synthesis of lexical, semantic and structural matching methods, together with the exploitation of concept instances resulted in a rather high recall with space of improvement, and a quite high precision that shows the accuracy of the individual methods, as well as of their synthesis. 1 Presentation of the system 1.1 State, purpose, general statement In this paper we present the AUTOMS tool for the automatic alignment of ontologies. The proposed tool exploits the HCONE-merge [1] ontology mapping method, which is based on “uncovering” the informal intended meaning of concepts by mapping them to WordNet senses. Furthermore, AUTOMS integrates the HCONE-merge method with an innovative lexical matcher named COCLU (COmpression-based CLUstering) [2], as well as with matching heuristics that exploit structural features of the source ontologies. The synthesis of these methods contributes towards automating the mapping process of concepts and properties of OWL ontologies, by exploiting different features of them: lexical, structural and semantic features. The WordNet lexicon and concept instances provide additional information towards unveiling mappings in cases where features such as labels and comments are missing or in cases where names are replaced by random strings. Structure matching heuristic rules are exploited to discover mappings in situations where lexical and semantic methods do not have enough information to proceed. AUTOMS provides mappings between concept/property pairs with high precision. However, it must be stated that it does not achieve a satisfactory recall for the experiments contacted. This suggests that further improvements are necessary both to the individual methods as well as to the sophistication of the synthesis of results. Since the execution times for obtaining these results using OAEI contest’s benchmark ontologies were quite high, a trade-off between lower time and better results was to be made. Finally, it must be stated that AUTOMS has been improved much due to the experience gained within the OAEI contest. 1.2 Specific techniques used The methods integrated within AUTOMS run in a particular sequence: Mappings computed by a method are being exploited by subsequent methods so as new mappings to be produced. The following paragraphs present the individual methods in the sequence of their execution. AUTOMS is mainly based on its lexical matching method, which is applied first in the sequence of the methods employed. COCLU exploits lexical information concerning names, labels and comments of ontologies’ concepts and properties, in order to compute their similarity. Although labels are considered the most important, comments and names are also examined. COCLU was originally proposed as a method for discovering typographic similarities between strings, sequences of characters over an alphabet (ASCII or UTF character set), with the aim to reveal the similarity of concepts instances’ lexicalizations during ontology population [2]. It is a partition-based clustering algorithm which divides data into clusters and searches the space of possible clusters using a greedy heuristic. Each cluster is represented by a model, rather than by the collection of data assigned to it. The cluster model is realized by a corresponding Huffman tree which is incrementally constructed as the algorithm dynamically generates and updates the clusters by processing one string (instance’s surface appearance) at a time. The use of a model classifies the algorithm to the conceptual or model based learning algorithms. To decide whether a new string should be added in a cluster (and therefore, that it lexicalizes the same class/property as the other strings in the cluster do) the algorithm employs a score function that measures the compactness and homogeneity of a cluster. This score function, Cluster Code Difference (CCDiff), is defined as the difference of the summed length of the coded string tokens that are members of the cluster, and the length of the cluster when it is updated with the candidate string. This score function groups together strings that contain the same set of frequent characters according to the model of a cluster (e.g. Pentium III and PIII). A string that lexicalizes an OWL class or property belongs in a particular cluster when its CCDiff is below a specific threshold and it is the smallest between the CCDiff’s of the given string and all existing clusters. Based on our experience with COCLU, the similarity threshold (ranging in [0,1]) was set to 0.986. A new cluster is created if the candidate string cannot be assigned to any of the existing clusters. As a result, it is possible to use the algorithm even when no initial clusters are available. Next to the computation of the lexically matching pairs is the computation of the semantic morphism (s-morphism) which is the core technique behind the HCONEmerge method. Given two ontologies, the algorithm computes a morphism between each of these two ontologies and a “hidden intermediate” ontology. This morphism is computed by the Latent Semantic Indexing (LSI) method and associates ontology concepts with WordNet senses. Latent Semantic Indexing (LSI) [4] is a vector space technique originally proposed for information retrieval and indexing. It assumes that there is an underlying latent semantic space that it estimates by means of statistical techniques using an association matrix (n×m) of term-document data (WordNet senses in our case). It must be emphasized that although LSI exploits structural information of ontologies and WordNet, it ends up with semantic associations between terms. As it is specified in the HCONE-merge approach, WordNet is not considered to include any intermediate ontology, as this would be very restrictive for the specification of the original ontologies (i.e. the method would work only for those ontologies that preserve the inclusion relations among WordNet senses). Actually, it is assumed that the intermediate ontology is “hidden” and the method constructs this ontology while mapping concepts to the WordNet senses. We have used WordNet since it is a well-thought and widely available lexical resource with a large number of entries and semantic relations. The mappings computed by the lexical and semantic matching methods are then used as input to a simple structural matching algorithm which exploits similarities in the vicinities of concepts/properties. Here, the vicinity of a concept/property includes only the subsumers and subsumees. The matching method has been implemented to improve the performance of the lexical and semantic matching methods by exploiting simple structural features. Consider the matching between two concepts c1 and c2 of source ontologies O1 and O2, respectively. The heuristic is: “if at least two neighbor concepts of c1 have already been (lexically or semantically) mapped to two neighbor concepts of c2 such that the mappings respect the ontology axioms of inclusion and equivalence, i.e. a sub-concept of c1 has been mapped to a sub-concept of c2, then c1 and c2 are consider to structurally match”. The threshold of two (2) neighbor concepts has been considered after conducting several experiments. It should be noticed that further in the alignment process, AUTOMS uses an enhanced structure matching method that runs iteratively using the mappings of all the methods. This method expands the vicinity of concepts to include object properties as well. The fourth method in sequence utilizes concept instances (individuals). Particularly, for the concepts that have not been determined to be similar to any other concept, AUTOMS compares their individuals, if any. For those concept pairs that have at least one matching instance, AUTOMS discovers a possible mapping. The matching of concept instances is currently based on the similarity of their local names, which is their Uniform Resource Identifier (URI). The fifth method utilizes information about properties. For the concepts that have not been determined to be similar to any other concept, AUTOMS compares their properties, if any. For those concept pairs that have at least two matching properties, AUTOMS identifies a possible mapping. The matching of object properties is based on the similarity of their property names, as well as on the similarities of their domain and range. The final step in the alignment process is the execution of an enhanced iterative structure matching method. This method uses the proposed matching pairs from all the previous methods in order to compute mappings based on concepts’ enhanced vicinity: The enhanced vicinity of the concept includes all the concepts related to it. This method runs iteratively for 2 times, updating the list of proposed matching pairs with the pairs discovered in each iteration. It has been observed during the specific benchmark experiments that although new mappings are discovered in each iteration, there is no change to the set of mappings after the second execution. Further experimentation and investigation is needed for improving this method. 1.3 Adaptations made for the evaluation AUTOMS is an evolving tool that integrates new methods, which are being tested in new cases. The OAEI contest provides challenging cases that require the exploitation of special features for AUTOMS (and any other tool) to perform efficiently and effectively. Specific adaptations have been made with respect to the utilization of ‘comments’, to the existence of the ‘lang’ property, and to the use of ‘random strings’ for concept/property names. Implementation adjustments have also been made in order to be able to run large sets of	benchmark (computing);character encoding;cluster analysis;comment (computer programming);computation;experiment;greedy algorithm;heuristic (computer science);huffman coding;information retrieval;iteration;iterative method;lexicon;machine learning;ontology (information science);pattern matching;precision and recall;semantic integration;semantic matching;turing completeness;uniform resource identifier;web ontology language;wordnet	Konstantinos Kotis;Alexandros G. Valarakos;George A. Vouros	2006			computer science;bioinformatics;data mining;information retrieval	AI	-29.27023665933442	-65.99487756439154	59530
400cc9ae0a817de0cdbbf17ee22a85426c057f34	feature analysis in microblog retrieval based on learning to rank		Learning to rank, which can fuse various of features, performs well in microblog retrieval. However, it is still unclear how the features function in microblog ranking. To address this issue, this paper examines the contribution of each single feature together with the contribution of the feature combinations via the ranking SVM for microblog retrieval modeling. The experimental results on the TREC microblog collection show that textual features, i.e. content relevance between a query and a microblog, contribute most to the retrieval performance. And the combination of certain non-textual features and textual features can further enhance the retrieval performance, though non-textual features alone produce rather weak results.	learning to rank;ranking svm;relevance;support vector machine;text retrieval conference	Zhongyuan Han;Xuwei Li;Muyun Yang;Haoliang Qi;Sheng Li	2013		10.1007/978-3-642-41644-6_40	data science;pattern recognition;information retrieval	Web+IR	-25.789461623097775	-60.98830937072713	59575
3260196011ae22d287326ba53c7b24bfd1f899dd	a data-parallel toolkit for information retrieval	distributed system;data parallel;information retrieval;distributed programs;infromation retrieval	Due to the explosive growth of the web that has occurred throughout its history, many researchers working on web corpora have begun to move toward distributed, data parallel computing. The size of the ClueWeb09 [2] corpus, at approximately one billion documents, is an indication of this. Even limiting the collection to only documents in the English language only halves the size of the collection. In this work, we describe the collection of information retrieval algorithms we have implemented using DryadLINQ [8]. DryadLINQ is a data parallel processing system that allows programmers to write distributed programs without worrying about the implementation of a distributed system. DryadLINQ executes programs containing SQL-like Language Integrated Query statements (LINQ) by shipping the computation to nodes in the cluster for parallel execution. The ability to break a computation into many pieces that can be processed on individual machines means that even a small number of computers can be leveraged to reduce the time necessary to process large collections. When researchers first obtain a collection of web documents, there is a substantial amount of preprocessing before analysis can commence. The toolkit assists with parsing, link extraction, associating discovered anchor text with the referenced document. Once the document content and links are in a standard format, then further processing can be performed. The toolkit provides implementations of textbased retrieval methods (BM25 [7] and BM25F [9]), queryindependent link based scoring functions (PageRank, indegree, and trans-domain indegree), query-dependent linkbased scoring functions (SALSA-SETR [6]). Additionally, the toolkit provides an implementation of shingle based duplicate document detection [1], n-gram extraction, and a mechanism to build an inverted index. The algorithms included in this toolkit include both traditional algorithms as well as recent research results. Elements	algorithm;anchor text;computation;computer;data parallelism;directed graph;distributed computing;information retrieval;inverted index;language integrated query;n-gram;pagerank;parallel computing;parsing;preprocessor;programmer;salsa;sql;scoring functions for docking;shingled magnetic recording;source-to-source compiler;text corpus;web page	Dennis Fetterly;Frank McSherry	2010		10.1145/1835449.1835568	computer science;theoretical computer science;database;data retrieval;information retrieval;human–computer information retrieval	Web+IR	-32.684085250125506	-57.625419384992085	59895
bf7a3a2e99febfd82400d8e2663bd847a0b7ae4c	improving short text clustering by similarity matrix sparsification		Short text clustering is an important but challenging task. We investigate impact of similarity matrix sparsification on the performance of short text clustering. We show that two sparsification methods (the proposed Similarity Distribution based, and k-nearest neighbors) that aim to retain a prescribed number of similarity elements per text, improve hierarchical clustering quality of short texts for various text similarities. These methods using a word embedding based similarity yield competitive results with state-of-the-art methods for short text clustering especially for general domain, and are faster than the main state-of-the-art baseline.	artificial neural network;baseline (configuration management);cluster analysis;hierarchical clustering;k-nearest neighbors algorithm;non-maskable interrupt;similarity measure;word embedding	Md. Rashadul Hasan Rakib;Magdalena Jankowska;Norbert Zeh;Evangelos E. Milios	2018		10.1145/3209280.3229114	word embedding;information retrieval;computer science;hierarchical clustering;pattern recognition;artificial intelligence;similarity matrix;document clustering	Web+IR	-24.832879693404063	-65.4303890397825	59981
a62aee83789003435a5610c05283cb2c2b85a77a	an optimization framework for weighting implicit relevance labels for personalized web search	personalization;gradient descent;click prediction	Implicit feedback from users of a web search engine is an essential source providing consistent personal relevance labels from the actual population of users. However, previous studies on personalized search employ this source in a rather straightforward manner. Basically, documents that were clicked on get maximal gain, and the rest of the documents are assigned the zero gain. As we demonstrate in our paper, a ranking algorithm trained using these gains directly as the ground truth relevance labels leads to a suboptimal personalized ranking.  In this paper we develop a framework for automatic reweighting of these labels. Our approach is based on more subtle aspects of user interaction with the result page. We propose an efficient methodology for deriving confidence levels for relevance labels that relies directly on the objective ranking measure. All our algorithms are evaluated on a large-scale query log provided by a major commercial search engine. The results of the experiments prove that the current state-of-the-art personalization approaches could be significantly improved by enriching relevance grades with weights extracted from post-impression user behavior.	algorithm;experiment;ground truth;maximal set;personalization;personalized search;relevance;web search engine	Yury Ustinovsky;Gleb Gusev;Pavel Serdyukov	2015		10.1145/2736277.2741105	gradient descent;ranking;computer science;machine learning;data mining;personalization;world wide web;information retrieval	Web+IR	-27.17384822715105	-53.64348866560308	60171
e2e97297bbce3bf679831742de2566cd3f4eed2c	an mcl-based approach for spam profile detection in online social networks	graph theory;pattern clustering;shared url mcl based approach spam profile detection online social networks information sharing media osn sites social spammers spam messages personal blogs markov clustering based approach facebook profiles weighted graph nodes representation edges interactions user profiles real social interactions active friends page likes;social network security;cyber security;spam profile detection;spam campaign identification social network analysis cyber security social network security spam profile detection;unsolicited e mail graph theory markov processes pattern clustering security of data social networking online;social networking online;social network analysis;markov processes;unsolicited e mail;facebook unsolicited electronic mail twitter equations mathematical model communities;spam campaign identification;security of data	Over the past few years, Online Social Networks (OSNs) have emerged as cheap and popular communication and information sharing media. Huge amount of information is being shared through popular OSN sites. This aspect of sharing information to a large number of individuals with ease has attracted social spammers to exploit the network of trust for spreading spam messages to promote personal blogs, advertisements, phishing, scam and so on. In this paper, we present a Markov Clustering (MCL) based approach for the detection of spam profiles on OSNs. Our study is based on a real dataset of Facebook profiles, which includes both benign and spam profiles. We model social network using a weighted graph in which profiles are represented as nodes and their interactions as edges. The weight of an edge, connecting a pair of user profiles, is calculated as a function of their real social interactions in terms of active friends, page likes and shared URLs within the network. MCL is applied on the weighted graph to generate different clusters containing different categories of profiles. Majority voting is applied to handle the cases in which a cluster contains both spam and normal profiles. Our experimental results show that majority voting not only reduces the number of clusters to a minimum, but also increases the performance values in terms of FP and FB measures from FP=0.85 and FB=0.75 to FP=0.88 and FB=0.79, respectively.	algorithm;blog;cluster analysis;decision tree;interaction;macintosh common lisp;markov chain monte carlo;monte carlo localization;naive bayes classifier;phishing;scalability;social graph;social network;spamming;supervised learning;user profile	Faraz Ahmed;Muhammad Abulaish	2012	2012 IEEE 11th International Conference on Trust, Security and Privacy in Computing and Communications	10.1109/TrustCom.2012.83	forum spam;computer science;social spam;spambot;internet privacy;world wide web;computer security	Metrics	-19.879028303220203	-55.34044833763424	60172
b17b08ce0e0d485d8e20303608d098d4bc472501	effectively classifying short texts via improved lexical category and semantic features		Classification of short text is challenging due to its severe sparseness and high dimension, which are typical characteristics of short text. In this paper, we propose a novel approach to classify short texts based on both lexical and semantic features. Firstly, the term dictionary is constructed by selecting lexical features that are most representative words of a certain category, and then the optimal topic distribution from the background knowledge repository is extracted via Latent Dirichlet Allocation. The new feature for short text is thereafter constructed. The experimental results show that our method achieved significant quality enhancement in terms of short text classification.		Huifang Ma;Runan Zhou;Fang Liu;Xiaoyong Lu	2016		10.1007/978-3-319-42291-6_16	natural language processing;data science;data mining	NLP	-23.788873852848656	-66.1012458656396	60325
1c07b04a1f2f9963369a970e0b6eded4043ad27f	finding science with science: evaluating a domain and scientific ontology user interface for the discovery of scientific resources		Current approaches to the discovery of scientific resources (publications, data sets and web services) are dominated by keyword search. These approaches do not allow scientists to search on the deeper semantics of scientific resources, or to discover resources on the basis of the scientific approaches taken. This article evaluates a user interface that allows users to discover scientific resources through structured knowledge in the form of ontologies describing the domain and the scientific knowledge inherent within the scientific resource, and also through informal user tags. These combined capabilities provide scientists with new and powerful options for resource discovery. A qualitative user evaluation explored how scientists felt about the approach for resource discovery in the context of their scientific work. The study showed that marine scientists were enthusiastic about the capabilities of such an approach and appreciated the ability to browse the visual structure of the knowledge and query on scientific method but, overall, preferred the use of tags over ontologies. The exploratory nature of the user study was used to identify future directions for such improvements.	browsing;ontology (information science);scientific literature;search algorithm;tag (metadata);usability testing;user interface;web service	Kristin Stock;Vera Karasova;Anne Robertson;Guillaume Roger;Mark Small;Mohamed Bishr;Jens Ortmann;Tim Stojanovic;Femke Reitsma;Lukasz Korczynski;Boyan Brodaric;Zoe Gardner	2013	Trans. GIS	10.1111/j.1467-9671.2012.01370.x	computer science;knowledge management;data science;data mining;database	HPC	-32.860871169407716	-55.753322086349215	60428
ab96ce81f3651576d517f224749bcc2d17990257	study on merging multiple results from information retrieval system		We participated in Web retrieval tasks in order to investigate the integration of multiple retrieval results. We also examined the possibility of using a general retrieval system designed for organized documents, such as news articles that match users’ queries, to retrieve unorganized documents, such as Web documents. Our results show that if information retrieval systems use the OKAPI method, results can be used for Web retrieval that is based on scored results without score normalization. On the other hand, if the systems use Rocchio’s feedback method, ranked results are shown to be the best.	database normalization;information retrieval;web page	Hiromi Ozaku;Masao Utiyama;Hitoshi Isahara;Yasuyuki Kono;Masatsugu Kidode	2002			visual word;relevance;cognitive models of information retrieval;image retrieval;computer science;data mining;adversarial information retrieval;okapi bm25;term discrimination;world wide web;data retrieval;information retrieval;human–computer information retrieval	Web+IR	-30.816669461596494	-58.1128000118231	60644
e7bbe174c3a1b75886e810cbecab0563bf7db051	automatic fake followers detection in chinese micro-blogging system	sina weibo;micro blogging;fake followers;micro blogging platforms;会议论文;legitimate users;social media networks;binary classifiers	Micro-blogging, which has greatly influenced people’s life, is experiencing fantastic success in the worldwide. However, during its rapid development, it has encountered the problem of content pollution. Various pollution in the micro-blogging platforms has hurt the credibility of micro-blogging and caused significantly negative effect. In this paper, we mainly focus on detecting fake followers which may lead to a problematic situation on social media networks. By extracting major features of fake followers in Sina Weibo, we propose a binary classifier to distinguish fake followers from the legitimate users. The experiments show that all the proposed features are important and our method greatly outperforms to detect fake followers. We also present an elaborate analysis on the phenomenon of fake followers, infer the supported algorithms and principles behind them, and finally provide several suggestions for micro-blogging systems and ordinary users to deal with the fake followers.	blog	Yi Shen;Jianjun Yu;Kejun Dong;Kai Nan	2014		10.1007/978-3-319-06605-9_49	computer science;microblogging;internet privacy;world wide web	Logic	-20.425705730579672	-55.25185457764367	60712
12a1dfe644a8286027a95b5cc15a5d9842b82555	analysing customer churn in insurance data - a case study	busqueda informacion;seguro;preprocessing for kdd;analisis datos;customer relationship management;information retrieval;heuristic method;metodo heuristico;data mining;feature space;transformation donnee;data analysis;transformacion dato;assurance;fouille donnee;recherche information;decouverte connaissance;knowledge discovery process;data transformation;descubrimiento conocimiento;analyse donnee;insurance data analysis;methode heuristique;time stamped data;busca dato;insurance;knowledge discovery	Designing a new application of knowledge discovery is a very tedious task. The success is determined to a great extent by an adequate example representation. The transformation of given data to the example representation is a matter of feature generation and selection. The search for an appropriate approach is difficult. In particular, if time data are involved, there exist a large variety of how to handle them. Reports on successful cases can provide case designers with a guideline for the design of new, similar cases. In this paper we present a complete knowledge discovery process applied to insurance data. We use the TF/IDF representation from information retrieval for compiling time-related features of the data set. Experimental reasults show that these new features lead to superior results in terms of accuracy, precision and recall. A heuristic is given which calculates how much the feature space is enlarged or shrinked by the transformation to TF/IDF.	compiler;existential quantification;feature vector;heuristic;information retrieval;precision and recall;selection (user interface);tf–idf	Katharina Morik;Hanna Köpcke	2004		10.1007/978-3-540-30116-5_31	customer relationship management;insurance;computer science;artificial intelligence;data science;machine learning;data mining;database;knowledge extraction;data analysis;data transformation;computer security;algorithm;statistics	AI	-32.6197826969277	-59.13328956137185	60721
8652ed2c7e14e6045d76a3979c73209c6354b58e	findings of the 2014 workshop on statistical machine translation		This paper presents the results of the WMT14 shared tasks, which included a standard news translation task, a separate medical translation task, a task for run-time estimation of machine translation quality, and a metrics task. This year, 143 machine translation systems from 23 institutions were submitted to the ten translation directions in the standard translation task. An additional 6 anonymized systems were included, and were then evaluated both automatically and manually. The quality estimation task had four subtasks, with a total of 10 teams, submitting 57 entries.	software quality assurance;standard translation;statistical machine translation	Ondrej Bojar;Christian Buck;Christian Federmann;Barry Haddow;Philipp Koehn;Johannes Leveling;Christof Monz;Pavel Pecina;Matt Post;Herve Saint-Amand;Radu Soricut;Lucia Specia;Ales Tamchyna	2014			data science	NLP	-31.725449515021438	-64.44199660754683	60722
0bdb20371122bf404fa85508bbd9dbf0398e6fc7	extração de dados de conferências a partir da web		Choosing the most suitable conference to submit a paper is a task that depends on a number of factors including: (i) the topic of the paper needs to be among the topics of interest of the conference; (ii) submission deadlines need to be compatible with the necessary time for paper writing; and (iii) the quality or impact of the conference. These factors allied to the existence of thousands of conferences, make the search of the right event very time consuming, especially when researching in a new area. Intending to help researchers finding conferences, this paper presents a method developed to retrieve and extract data from conferences web sites. Our method combines the identification of conference URL and deadline extraction. The retrieved data is stored in a database to be searched with an online tool. The paper also reports on experiments that evaluate the quality of the extracted data, focusing on the deadlines. Resumo. A escolha da conferência adequada para o envio de um artigo é uma tarefa que depende de vários fatores incluindo: (i) o tema do artigo deve estar entre os temas de interesse do evento; (ii) o prazo de submissão do evento deve ser compatı́vel com tempo necessário para a escrita do artigo; e (iii) a qualidade da conferência. Esses fatores aliados à existência de milhares de conferências tornam a busca pelo evento adequado bastante demorada, em especial quando se está pesquisando em uma área nova. A fim de auxiliar os pesquisadores na busca de conferências, esse artigo apresenta um método desenvolvido para a coleta e extração de dados de sites de conferências. Este método combina a identificação de URLs de conferências da Tabela Qualis à identificação de deadlines. Os dados coletados populam uma base de dados que poderá ser consultada através de uma ferramenta online. O artigo também relata experimentos que avaliam a qualidade dos dados extraı́dos, enfatizando a extração dos deadlines. 1. Introdução O processo de escrita e submissão de artigos cientı́ficos é crucial na vida dos pesquisadores. A escolha do periódico ou conferência mais adequados para a divulgação da pesquisa realizada é uma tarefa bastante importante e que por vezes toma bastante tempo dos pesquisadores. Existem milhares de conferências cientı́ficas que ocorrem anualmente. Quando se deseja submeter um artigo para uma conferência, vários aspectos precisam ser levados em consideração: (i) tema do trabalho deve estar entre os temas de interesse do evento para que ele possa ser considerado; (ii) é necessário saber se os prazos (deadlines) do evento são compatı́veis com os do término da escrita do artigo (ou algum outro critério temporal como o prazo para a conclusão do curso, por exemplo); (iii) questões de valores 32nd SBBD – Full Papers – ISSN 2316-5170 October 2-5, 2017 – Uberlândia, MG, Brazil	comparison of em simulation software;database;estar project;experiment;international standard serial number;mg (editor);numerical aperture;power-on reset;qualis (capes);unified model	Cássio Alan Garcia;Viviane P. Moreira	2017			database;computer science	Visualization	-32.3072238105995	-61.85255702645288	60734
a7582c3a55a6d76d0fee63a80d946d796fb4e34e	real-time traffic event detection from social media		Smart communities are composed of groups, organizations, and individuals who share information and make use of that shared information for better decision making. Shared information can come from many sources, particularly, but not exclusively, from sensors and social media. Social media has become an important source of near-instantaneous user-generated information that can be shared and analyzed to support better decision making. One domain where social media data can add value is transportation and traffic management. This article looks at the exploitation of Twitter data in the traffic reporting domain. A key challenge is how to identify relevant information from a huge amount of user-generated data and then analyze the relevant data for automatic geocoded incident detection. The article proposes an instant traffic alert and warning system based on a novel latent Dirichlet allocation (LDA) approach (“tweet-LDA”). The system is evaluated and shown to perform better than related approaches.	accessibility;geolocation;latent dirichlet allocation;preprocessor;real-time transcription;sensor;smart tv;social media;text mining;user-generated content	Di Wang;Ahmad Al-Rubaie;Sandra Stincic;John Davies	2017	ACM Trans. Internet Techn.	10.1145/3122982	latent dirichlet allocation;incremental learning;data mining;computer science;social media;geocoding;traffic reporting;text mining;warning system	AI	-22.47610783843989	-54.90867992223606	60932
c3a97a2a20cb81dcefa608e99d753ccf3cfd7c22	spam web page detection using combined content and link features	keyword stuffing;link spam;web spamming;pos ratio;spam webpages;keyword density;anchor text;spam detection;content spam	Web spamming refers to actions that have intentions to mislead search engines by ranking some irrelevant web pages higher in the search results than they deserve. It is thus a roadblock in obtaining high-quality information retrieval from the web. Spam web pages are often littered with irrelevant and meaningless content. Therefore, spam detection methods have been proposed as a solution for web spam in order to minimise the adverse effects of spam web pages. There has been no single defining profile that can encompass all types of spam websites. As such, this makes spam web page detection extremely difficult. In this paper, the proposed technique combines the content and link-based features of web pages to classify them as spam or non-spam. For experimental purpose, WEBSPAM-UK2006 dataset has been used. The results of the proposed approach were compared with the existing approaches and it has been found that the F-measure of the proposed approach outperformed the others.	web page	Rajendra Kumar Roul;Shubham Rohan Asthana;Gaurav Kumar	2016	IJDMMM	10.1504/IJDMMM.2016.079063	spam blog;content farm;forum spam;anchor text;computer science;keyword stuffing;spamdexing;spambot;spam and open relay blocking system;adversarial information retrieval;keyword density;internet privacy;spamtrap;world wide web;information retrieval	Vision	-25.26889361369134	-54.326133462741716	60936
59b229c5d94807a484ac3f80c048cdc8b8297002	full-subtopic retrieval with keyphrase-based search results clustering	subtopic retrieval;intelligent agent clustering algorithms algorithm design and analysis length measurement search engines conferences testing diversity methods information retrieval system performance;search engines;information retrieval;generalized suffix tree;generalized suffix tree subtopic retrieval full subtopic retrieval search results clustering keyphrase extraction;search results clustering;keyphrase extraction;testing;length measurement;system performance;hierarchical agglomerative clustering;suffix tree;intelligent agent;search result clustering;clustering algorithms;full subtopic retrieval;algorithm design and analysis;test collection;conferences;diversity methods	"""We consider the problem of retrieving multiple documents relevant to the single subtopics of a given web query, termed """"full-subtopic retrieval"""". To solve this problem we present a novel search results clustering algorithm that generates clusters labeled by keyphrases. The keyphrases are extracted from the generalized suffix tree built from the search results and merged through an improved hierarchical agglomerative clustering procedure. We also introduce a novel measure for evaluating full-subtopic retrieval performance, namely """"Subtopic Search Length under k document sufficiency"""". Using a test collection specifically designed for evaluating subtopic retrieval, we found that our algorithm outperformed both other existing search results clustering algorithms and also a search results re-ranking method that emphasized diversity of results (at least for k>1; i.e., when we are interested in retrieving more than one relevant document per subtopic). Our approach has been implemented into KeySRC (Keyphrase-based Search Results Clustering), a full web clustering engine available online at http://keysrc.fub.it."""	algorithm;cluster analysis;generalized suffix tree;hierarchical clustering	Andrea Bernardini;Claudio Carpineto;Massimiliano D'Amico	2009	2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology	10.1109/WI-IAT.2009.37	generalized suffix tree;correlation clustering;algorithm design;document clustering;fuzzy clustering;length measurement;computer science;artificial intelligence;pattern recognition;data mining;software testing;cluster analysis;intelligent agent;information retrieval	Web+IR	-29.90301992298435	-56.65678509472836	60987
7d9da71d1b0185bebaa708639c7b70b3bfa07a75	scoring and selecting terms for text categorization	busqueda informacion;gestion informacion;linguistique;support vector machines;information retrieval learning artificial intelligence text analysis classification word processing feature extraction;information retrieval;text categorization gain measurement information retrieval information theory frequency estimation frequency measurement probability entropy statistical distributions learning systems;text analysis;classification;statistical properties;feature reduction;information retrieval feature selection text categorization support vector machines machine learning;linguistica;machine learning;recherche information;traitement document;feature extraction;information management;categorisation texte;feature selection;document processing;support vector machine;gestion information;theorie information;learning artificial intelligence;selection forme;text categorization;information theory;word processing;tratamiento documento;information retrieval machine learning ml based scoring measures feature selection text categorization;teoria informacion;linguistics	We propose a set of (machine learning) ML-based scoring measures for conducting feature selection. We've tested these measures on documents from two well-known corpora, comparing them with other measures previously applied for this purpose. In particular, we've analyzed which measure obtains the best overall classification performance in terms of properties such as precision and recall, emphasizing to what extent some statistical properties of the corpus affects performance. The results show that some of our measures outperform the traditional measures in certain situations.	categorization;document classification;feature selection;machine learning;precision and recall;text corpus	Elena Montañés;Irene Díaz;José Ranilla;Elías F. Combarro;Javier Fernández	2005	IEEE Intelligent Systems	10.1109/MIS.2005.49	support vector machine;information theory;computer science;artificial intelligence;machine learning;pattern recognition;information management;feature selection;information retrieval	Web+IR	-22.087448204103218	-64.25958505395974	61048
cfe1e01d80f05402e0a0804d8350fd7d0abebfe1	indexfinder: a method of extracting key concepts from clinical texts for indexing	internet;unified medical language system;semantics;natural language processing;algorithms;digital library;indexation;noun phrase	Extracting key concepts from clinical texts for indexing is an important task in implementing a medical digital library. Several methods are proposed for mapping free text into standard terms defined by the Unified Medical Language System (UMLS). For example, natural language processing techniques are used to map identified noun phrases into concepts. They are, however, not appropriate for real time applications. Therefore, in this paper, we present a new algorithm for generating all valid UMLS concepts by permuting the set of words in the input text and then filtering out the irrelevant concepts via syntactic and semantic filtering. We have implemented the algorithm as a web-based service that provides a search interface for researchers and computer programs. Our preliminary experiment shows that the algorithm is effective at discovering relevant UMLS concepts while achieving a throughput of 43K bytes of text per second. The tool can extract key concepts from clinical texts for indexing.	algorithm;byte;computer program;digital library;indexes;interface device component;natural language processing;phrases;real-time computing;relevance;throughput;unified medical language system;web application	Qinghua Zou;Wesley W. Chu;Craig A. Morioka;Gregory H. Leazer;Hooshang Kangarloo	2003	AMIA ... Annual Symposium proceedings. AMIA Symposium		syntax;noun phrase;digital library;information retrieval;computer program;semantics;search engine indexing;indexation;unified medical language system;computer science	Logic	-33.266037524497314	-60.22486433759065	61053
938d5e18419b845740db0e57d550be4d5f7b3f7b	interpreting reputation through frequent named entities in twitter		Twitter is a social network that provides a powerful source of data. The analysis of those data offers many challenges among those stands out the opportunity to find the reputation of a product, of a person, or of any other entity of interest. Several tools for sentiment analysis have been built in order to calculate the general opinion of an entity using a static analysis of the sentiments expressed in tweets. However, entities are not static; they collaborate with other entities and get involved in events. A simple aggregation of sentiments is then not sufficient to represent this dynamism. In this paper, we present a new approach that identifies the reputation of an entity on the basis of the set of events it is involved into by providing a transparent and self explanatory way for interpreting reputation. In order to perform this analysis we define a new sampling method based on a tweet weighting to retrieve relevant information. In our experiments we show that the 90% of the reputation of the entity originates from the events it is involved into, especially in the case of entities that represent public figures.	entity;named-entity recognition	Nacéra Bennacer;Francesca Bugiotti;Moditha Hewasinghage;Suela Isaj;Gianluca Quercini	2017		10.1007/978-3-319-68783-4_4	data mining;dynamism;sentiment analysis;computer science;static analysis;social network;reputation;weighting	ML	-24.56831024483275	-52.46741976301638	61587
8fd943fb0cc409823549d92ea91b1e4abcb192f1	fouilla: navigating dbpedia by topic		Navigating large knowledge bases made of billions of triples is very challenging. In this demonstration, we showcase Fouilla, a topical Knowledge Base browser that offers a seamless navigational experience of DBpedia. We propose an original approach that leverages both structural and semantic contents of Wikipedia to enable a topic-oriented filter on DBpedia entities. We devise an approach to drastically reduce the query time and to ensure a seamless browsing experience to the end user. We demonstrate how our system offers a novel and meaningful experience of DBpedia browsing by challenging the user to search for relevant information within the Knowledge Base in different use cases.		Tanguy Raynaud;Julien Subercaze;Frédérique Laforest	2018		10.1145/3269206.3269210	information retrieval;end user;knowledge base;computer science;use case;ranking	Web+IR	-30.86177112557701	-53.440297638113925	61975
112fa9db21a6b1bedcb3dcd9ae029c6f2b8b7972	instant web retrieval for instance-attribute queries	search engine;information sources;motion pictures;information extraction;search engines;information retrieval;matchmaking;data mining;customer service;search engines data mining information retrieval humans motion pictures navigation customer service educational institutions computer science distributed databases;navigation;web services;distributed databases;semantic web;web retrieval;humans;computer science;information need;ontology	As the Web becomes the major information source of our daily activities, tools for finding various information on it are indispensable. This paper addresses theWeb retrieval of instance-attribute information, e.g., the contact addresses and research interests (attributes) of faculty and students (instances). This kind of information need is very common but cannot be directly supported by current keywordmatching-based search engines. People commonly use a two-phase search: First, locate the candidate pages, e.g., a faculty page, and then search within them for the desired information, e.g., contact information. Based on the stimulation of such human search behavior, we design a retrieval engine, upon general search engines, to help find the instance-attribute information from the Web. The experiment on several faculty members has shown the feasibility of the approach.	information needs;information source;two-phase commit protocol;web search engine;world wide web	Hai H. Wang;Ahmed Saleh;Terry R. Payne;Nicholas Gibbins	2007	IEEE/WIC/ACM International Conference on Web Intelligence (WI'07)	10.1109/WI.2007.67	search engine indexing;query expansion;cognitive models of information retrieval;computer science;concept search;ontology;data mining;adversarial information retrieval;world wide web;information extraction;information retrieval;search engine;human–computer information retrieval	Web+IR	-30.015678751111132	-52.651303229550614	62028
3ac2d60f1060f487c1357fc71f13071255d1622a	extracting relational data from html repositories	relational data;information extraction;pattern;automatic detection;duplication;information quality;error rate;world wide web;coverage estimation;capture recapture	There is a vast amount of valuable information in HTML documents, widely distributed across the World Wide Web and across corporate intranets. Unfortunately, HTML is mainly presentation oriented and hard to query. In this paper, we develop a system to extract desired information (records) from thousands of HTML documents, starting from a small set of examples. Duplicates in the result are automatically detected and eliminated. We propose a novel method to estimate the current coverage of results by the system, based on capture-recapture models with unequal capture probabilities. We also propose techniques for estimating the error rate of the extracted information and an interactive the technique for enhancing information quality. To evaluate the method and ideas proposed in this paper, we conducted an extensive set of experiments. Our experimental results validate the effectiveness and utility of our system, and demonstrate interesting tradeoffs between running time of information extraction and coverage of results.	bit error rate;experiment;html;information extraction;information quality;intranet;mark and recapture;time complexity;world wide web	Ruth Yuee Zhang;Laks V. S. Lakshmanan;Ruben H. Zamar	2004	SIGKDD Explorations	10.1145/1046456.1046458	mark and recapture;word error rate;relational database;computer science;data mining;pattern;information quality;world wide web;information extraction;information retrieval;gene duplication	DB	-30.830512820084692	-54.00854603366072	62162
9e1e658f73d71053db0a59c25ee6612154aeefda	a deep modular rnn approach for ethos mining		Automatically recognising and extracting the reasoning expressed in natural language text is extremely demanding and only very recently has there been significant headway. While such argument mining focuses on logos (the content of what is said) evidence has demonstrated that using ethos (the character of the speaker) can sometimes be an even more powerful tool of influence. We study the UK parliamentary debates which furnish a rich source of ethos with linguistic material signalling the ethotic relationships between politicians. We then develop a novel deep modular recurrent neural network, DMRNN, approach and employ proven methods from argument mining and sentiment analysis to create an ethos mining pipeline. Annotation of ethotic statements is reliable and its extraction is robust (macro-F1 = 0.83), while annotation of polarity is perfect and its extraction is solid (macro-F1 = 0.84). By exploring correspondences between ethos in political discourse and major events in the political landscape through ethos analytics, we uncover tantalising evidence that identifying expressions of positive and negative ethotic sentiment is a powerful instrument for understanding the dynamics of governments.		Rory Duthie;Katarzyna Budzynska	2018		10.24963/ijcai.2018/562	machine learning;artificial intelligence;modular design;ethos;computer science	AI	-21.631182291505453	-58.64640719115651	62259
1c5fb6075a15408edcc92d07454b4fa0487866d9	similarity search using concept graphs	concept graph;e reader application;similarity search;augmenting documents	The rapid proliferation of hand-held devices has led to the development of rich, interactive and immersive applications, such as e-readers for electronic books. These applications motivate retrieval systems that can implicitly satisfy any information need of the reader by exploiting the context of the user's interactions. Such retrieval systems differ from traditional search engines in that the queries constructed using the context are typically complex objects (including the document and its structure).  In this paper, we develop an efficient retrieval system, only assuming an oracle access to a traditional search engine that admits 'succinct' keyword queries for retrieving objects of a desired media type. As part of query generation, we first map the complex query object to a concept graph and then use the concepts along with their relationships in the graph to compute a small set of keyword queries to the search engine. Next, as part of the result generation, we aggregate the results of these queries to identify relevant web content of the desired type, thereby eliminating the need for explicitly computing similarity between the query object and all web content. We present a theoretical analysis of our approach and carry out a detailed empirical evaluation to show the practicality of the approach for the task of augmenting electronic documents with high quality videos from the web.	aggregate data;book;display resolution;e-book;e-reader;experiment;information needs;interaction;mobile device;similarity search;text corpus;theory;web content;web search engine	Rakesh Agrawal;Sreenivas Gollapudi;Anitha Kannan;Krishnaram Kenthapadi	2014		10.1145/2661829.2661995	natural language processing;query expansion;web query classification;computer science;machine learning;data mining;database;nearest neighbor search;web search query;world wide web;information retrieval;search engine	Web+IR	-31.463077437302655	-53.13979744770574	62292
a7fbf06312e2e137c609206f2a582f6f4d7ec902	incremental knowledge acquisition approach for information extraction on both semi-structured and unstructured text from the open domain web		Extracting information from semistructured text has been studied only for limited domain sources due to its heterogeneous formats. This paper proposes a Ripple-Down Rules (RDR) based approach to extract relations from both semistructured and unstructured text in open domain Web pages. We find that RDR's 'case-by-case' incremental knowledge acquisition approach provides practical flexibility for (1) handling heterogeneous formats of semi-structured text; (2) conducting knowledge engineering on any Web pages with minimum start-up cost and (3) allowing open-ended settings on relation schema. The efficacy of the approach has been demonstrated by extracting contact information from randomly collected open domain Web pages. The rGALA system achieved 0.87 F1 score on a testing dataset of 100 Web pages, after only 7 hours of knowledge engineering on a training set of	f1 score;information extraction;knowledge acquisition;nonlinear gameplay;question answering;randomness;relation (database);restrictive design rules;ripple;semiconductor industry;structured text;test set;web page	Maria Myung Hee Kim	2017			information extraction;data mining;knowledge acquisition;computer science	AI	-28.485875733813728	-64.67873038009573	62320
e3d8d3d5a8149d2d114e03683df29cabc92c5775	real-time photo mining from the twitter stream: event photo discovery and food photo detection	databases;real time photo mining;food photo recognition;event detection;twitter databases streaming media real time systems feature extraction visualization image color analysis;food photo detection real time photo mining twitter stream event photo discovery;visualization;streaming media;twitter stream;image color analysis;feature extraction;twitter;social networking online data mining;food photo recognition real time photo mining twitter stream event detection;real time systems	So many people are posting photos as well as short messages to Twitter every minutes from everywhere on the earth. By monitoring the Twitter stream, we can obtain various kinds of photos with texts. In this paper, as case studies of real-time Twitter photo mining, we introduce our current on-going projects on event photo discovery and food photo mining from the Twitter stream.	photocopier;real-time locating system;real-time transcription	Keiji Yanai;Takamu Kaneko;Yoshiyuki Kawano	2014	2014 IEEE International Symposium on Multimedia	10.1109/ISM.2014.72	visualization;feature extraction;computer science;machine learning;database;multimedia;internet privacy;world wide web	Arch	-23.268442582903027	-54.82697525518166	62347
d1d9e742cef8ef63fe888f83996149e612174210	overview of the special issue on trust and veracity of information in social media	rumour propagation;fake content;information veracity;news verification;social media	From a business and government point of view, there is an increasing need to interpret and act upon information from large-volume media, such as Twitter, Facebook and Web news. However, knowledge gathered from such online sources comes with a major caveat—it cannot always be trusted, nor is it always factual or of high quality. Rumors tend to spread rapidly through social networks, and their veracity is hard to establish in a timely fashion. For instance, during an earthquake in Chile, rumors spread through Twitter that a volcano became active and there was a tsunami warning in Valparaiso [Castillo et al. 2013]. Later, these reports were found to be false. Another example concerns “astroturf campaigns”—a malicious use of Twitter and other social media during election campaigns to provide fake support of a message or project by grassroots participants, while at the same time hiding the original campaign sponsors (usually elite groups or their lobbies). Researchers have identified numerous sources of untrusted content, and through them they found that several online communities interact with narratives stemming from conspiracy theories [Bessi et al. 2015]. A 2012	display resolution;malware;online community;social media;social network;stemming;theory;veracity	Symeon Papadopoulos;Kalina Bontcheva;Eva Jaho;Mihai Lupu;Carlos Castillo	2016	ACM Trans. Inf. Syst.	10.1145/2870630	social media;computer science;internet privacy;world wide web;computer security	Web+IR	-21.53127801714615	-54.103212487521105	62409
65acce9ac23b637d4fa2c9febf61bd58ab147b10	a fast image spam filter based on orb	scalable vocabulary tree sub block color histogram orb;information filtering;image spam detection image spam filtering orb text based spam filter subblock color histogram image feature scalable vocabulary tree;会议论文;unsolicited e mail feature extraction image colour analysis information filtering;image colour analysis;feature extraction;feature extraction vocabulary databases image color analysis histograms filtering electronic mail;unsolicited e mail	Image spam has become a new obfuscating method to bypass conventional text based spam filters. In this paper, a new kind of image spam filtering method is proposed based on the characteristics of the spam being sent repeatedly and their contents being highly resemble to each other. We extract sub-block color histogram and ORB as image feature and run a scalable vocabulary tree to detect image spam. The system is tested on Mark Dredze's dataset and our own Chinese image spam corpus. Experimental results demonstrate that the proposed method can achieve good accuracy while having a less than 0.02% false positive rate.	color histogram;email filtering;feature (computer vision);scalability;spamming;text-based (computing);vocabulary	Yixin Hou;Bo Zhao;Honggang Zhang;Hanbing Yan	2012	2012 3rd IEEE International Conference on Network Infrastructure and Digital Content	10.1109/ICNIDC.2012.6418804	computer science;pattern recognition;world wide web;information retrieval	Vision	-19.17349177391699	-58.27034905424847	62468
069326f49d98875fb437577d23d5280b8808363e	patentrank: an ontology-based approach to patent search	patent search;ipc;semantic search;ontology;lucene	There has been much research proposed to use ontology for improving the effectiveness of search. However, there are few studies focusing on the patent area. Since patents are domain-specific, traditional search methods may not achieve a high performance without knowledge bases. To address this issue, we propose PatentRank, an ontology-based method for patent search. We utilize International Patent Classification (IPC) as an ontology to enable computer to better understand the domain-specific knowledge. In this way, the proposed method is able to well disambiguate user's search intents. And also this method discovers the relationship between patents and employs it to improve the ranking algorithm. The empirical experiments have been conducted to demonstrate the effectiveness of our method.	artificial intelligence;peer-to-patent	Ming Li;Hai-Tao Zheng;Yong Jiang;Shu-Tao Xia	2011		10.1007/978-3-642-24958-7_47	semantic search;computer science;data science;ontology;data mining;ontology-based data integration;information retrieval;inter-process communication	AI	-29.963567597608925	-62.05285214544565	62469
31bb1394e13e687784b407189b301b9fc53fa442	mining for topics to suggest knowledge model extensions	intelligent suggesters;knowledge construction;concept mapping;web mining;knowledge discovery	Electronic concept maps, interlinked with other concept maps and multimedia resources, can provide rich knowledge models to capture and share human knowledge. This article presents and evaluates methods to support experts as they extend existing knowledge models, by suggesting new context-relevant topics mined from Web search engines. The task of generating topics to support knowledge model extension raises two research questions: first, how to extract topic descriptors and discriminators from concept maps; and second, how to use these topic descriptors and discriminators to identify candidate topics on the Web with the right balance of novelty and relevance. To address these questions, this article first develops the theoretical framework required for a “topic suggester” to aid information search in the context of a knowledge model under construction. It then presents and evaluates algorithms based on this framework and applied in Extender, an implemented tool for topic suggestion. Extender has been developed and tested within CmapTools, a widely used system for supporting knowledge modeling using concept maps. However, the generality of the algorithms makes them applicable to a broad class of knowledge modeling systems, and to Web search in general.	algorithm;concept map;knowledge management;knowledge modeling;knowledge representation and reasoning;mined;relevance;web search engine;world wide web	Carlos M. Lorenzetti;Ana Gabriela Maguitman;David B. Leake;Filippo Menczer;Thomas Reichherzer	2016	TKDD	10.1145/2997657	concept map;web mining;knowledge integration;computer science;knowledge management;data science;knowledge-based systems;machine learning;data mining;knowledge extraction;personal knowledge management;domain knowledge	Web+IR	-33.69580456547387	-56.96439641811928	62480
9223416bff83cb481e43d730a04931dc34bf8381	conceptual clustering of documents for automatic ontology generation	information retrieval;conference paper;feature vector;indexing;clustering;self organizing map;polysemy;homonymy	In Information retrieval, Keyword based retrieval is unsatisfactory for user needs since it can't always retrieve relevant words according to the concept. Since different words can represent the same concept (polysemy) and one word can represent different concepts (homonymy), mapping problem will lead to word sense Disambiguation. Through the implementation of domain dependent ontology, concept based information retrieval (IR) can be achieved. Since Semantic concept extraction from keywords is the initial phase for automatic construction of ontology process, this paper propose an effective method for it. Reuters21578 is used as the input of this process, followed by indexing, training and clustering using self-Organizing Map. Based on the feature vector, the clustering of documents are formed using automatic concept selections, in order to make the hierarchy. Clusters are represented hierarchically based on the topics assigned .Ontology will be generated automatically for each cluster, based on the topic assigned.	conceptual clustering;ontology learning	Reshmy Krishnan;Amir Hussain;P. C. Sherimon	2013		10.1007/978-3-642-38786-9_27	natural language processing;document clustering;computer science;concept search;data mining;information retrieval	NLP	-27.85216237934551	-64.07083798363058	62510
fc528ac9484d26d6ae9ec3b3d89a4905042eb154	approximate string matching techniques	record matching;data warehousing;data quality;string matching;record linkage	Data quality is a key to success for all kinds of businesses that have information applications involved, such as data integration for data warehouses, text and web mining, information retrieval, search engine for web applications, etc. In such applications, matching strings is one of the popular tasks. There are a number of approximate string matching techniques available. However, there is still a problem that remains unanswered: for a given dataset, how to select an appropriate technique and a threshold value required by this technique for the purpose of string matching. To challenge this problem, this paper analyses and evaluates a set of popular token-based string matching techniques on several carefully designed different datasets. A thorough experimental comparison confirms the statement that there is no clear overall best technique. However, some techniques do perform significantly better in some cases. Some suggestions have been presented, which can be used as guidance for researchers and practitioners to select an appropriate string matching technique and a corresponding threshold value for a given dataset.	approximate string matching;data quality;information retrieval;string searching algorithm;web application;web mining;web search engine	Taoxin Peng;Calum Mackay	2014		10.5220/0004892802170224	record linkage;data quality;computer science;machine learning;data warehouse;data mining;database;string metric;world wide web;string searching algorithm	DB	-32.5926516319475	-59.02587897176631	62544
c1ef9e880a04ac9cc2c1fc638cf4189926bba52e	exploiting latent information in relational databases via word embedding and application to degrees of disclosure				Rajesh Bordawekar;Oded Shmueli	2019				NLP	-24.136553679123736	-61.130836810840705	62682
bd5d105f4fa032e58e0a112d9316bc4301976e06	overview of the ntcir-9 intent task		This is an overview of the NTCIR-9 INTENT task, which comprises the Subtopic Mining and the Document Ranking subtasks. The INTENT task attracted participating teams from seven different countries/regions – 16 teams for Subtopic Mining and 8 teams for Document Ranking. The Subtopic Mining subtask received 42 Chinese runs and 14 Japanese runs; the Document Ranking subtask received 24 Chinese runs and 18 Japanese runs. We describe the subtasks, data and evaluation methods, and then report on the results.	precision and recall;ranking (information retrieval);relevance;usability	Ruihua Song;Min Zhang;Tetsuya Sakai;Makoto P. Kato;Yiqun Liu;Miho Sugimoto;Qinglei Wang;Naoki Orii	2011			data mining;ranking;computer science	NLP	-31.228123650483685	-63.58457012375947	62888
24986435c73066aaea0b21066db4539270106bee	novelty and redundancy detection in adaptive filtering	similarity metric;information extraction;information filtering;visualization;interactive ir;adaptive filter;language model	This paper addresses the problem of extending an adaptive information filtering system to make decisions about the novelty and redundancy of relevant documents. It argues that relevance and redundance should each be modelled explicitly and separately. A set of five redundancy measures are proposed and evaluated in experiments with and without redundancy thresholds. The experimental results demonstrate that the cosine similarity metric and a redundancy measure based on a mixture of language models are both effective for identifying redundant documents.	adaptive filter;cosine similarity;document;experiment;information filtering system;language model;relevance	Yi Zhang;James P. Callan;Tom Minka	2002		10.1145/564376.564393	natural language processing;adaptive filter;visualization;redundancy;computer science;machine learning;pattern recognition;data mining;information extraction;information retrieval;language model	Web+IR	-25.60955542393365	-62.298741515234866	62914
baecf6e0c1d12bb2d77b37dde8ffb9d2acde7ccc	a real-time architecture for detection of diseases using social networks: design, implementation and evaluation	real time;disease surveillance;data mining;social network;indexation;twitter;real time search;public health	In this work we developed a surveillance architecture to detect diseases-related postings in social networks using Twitter as an example for a high-traffic social network. Our real-time architecture uses Twitter streaming API to crawl Twitter messages as they are posted. Data mining techniques have been used to index, extract and classify postings. Finally, we evaluate the performance of the classifier with a dataset of public health postings and also evaluate the run-time performance of whole system with respect to latency and throughput.	application programming interface;data mining;real-time clock;real-time web;social network;throughput	Mustafa Sofean;Matthew Smith	2012		10.1145/2309996.2310048	public health;computer science;data mining;internet privacy;world wide web;social network	ML	-22.114304424138663	-54.62687086829242	62968
8644d9c135a0817d4009941869b1ac87f1f4c2f5	mercure at clef-2		1 Summary This paper presents the experiments undertaken by our team (IRIT team) in multilingual, bilingual and monolingual tasks at CLEF programme. Our approach to CLIR is based on query translation. In bilingual experiment a dictionary is used to translate the queries from French t o English and two techniques for desambiguiation were tested: aligned corpus and dictionary strategy. D e s a m biguiation technique is applied to select the best terms from the (translated) targed queries. All these experiments were done using Mercure system 2] which is presented in section 2 of this paper. The section 3 describes our general CLIR methodology, and nally, section 4 describes experiments and results performed at CLEF programme. 2 Mercure model 2.1 Model description Mercure is an information retrieval system based on a connectionist approach and modelled by a m ulti-layered network. The network is composed of a query layer (set of query terms), a term layer representing the indexing terms and a document l a yer 1],,2]. Mercure includes the implementation of a retrieval process based on spreading activation forward and backward through the weighted links. Queries and documents can be either inputs or outputs of the network.The links between two l a yers are symmetric and their weights are based on the tf idf measure inspired from the OKAPII3] term weighting formula. the term-document l i n k w eights are expressed by: d ij = tf ij (h 1 + h 2 log(N ni)) h 3 + h 4 dlj d + h 5 tf ij (1) the query-term (at stage s) links are weighted as follows: q (s) ui = nquqtfui nqu;qtfui si (nq u > q t f ui) qtf ui otherwise (2)	connectionism;cross-language information retrieval;dictionary;experiment;mercure;mathematical model;spreading activation;tf–idf	Nawel Nassr;Mohand Boughanem	2001				Web+IR	-33.49899311262043	-64.7197281322058	63210
350abc5c9bbd891ddd45da97d6d59a983fe582ae	automated assessment of review quality using latent semantic analysis	text analysis computer aided instruction natural language processing pattern classification reviews;text mining;computer aided instruction;quality of reviews;training;semantics;text analysis;review quality assessment;data pre processing;metareview scores;thumb;text classification;accuracy;machine learning techniques;machine learning;syntactics;matrix decomposition;quantifiable factors;pattern classification;humans;semantics matrix decomposition accuracy syntactics training humans thumb;student reviews;data quality;latent semantic analysis quality of reviews automated metareviewing text mining;automated metareviewing;reviews;metareview scores review quality assessment latent semantic analysis quantifiable factors machine learning techniques data pre processing data quality text classification student reviews;latent semantic analysis;natural language processing	Quality of a review can be identified by reviewing a review. Quantifiable factors that help identify the quality of a review include quality and tone of review comments, and the number of tokens each contains. We use machine-learning techniques such as latent semantic analysis (LSA) and cosine similarity to classify comments based on their quality and tone. Our paper details experiments that were conducted on student review and metareview data by using different data pre-processing steps. We compare these pre-processing steps and show that when applied to student review data, they help improve data quality by providing better text classification. Our technique helps predict metareview scores for student reviews.	data pre-processing;data quality;experiment;latent semantic analysis;machine learning;preprocessor	Lakshmi Ramachandran;Edward F. Gehringer	2011	2011 IEEE 11th International Conference on Advanced Learning Technologies	10.1109/ICALT.2011.46	natural language processing;text mining;data quality;latent semantic analysis;computer science;data science;data mining;database;accuracy and precision;data pre-processing;matrix decomposition	SE	-22.753709171492133	-65.24405045103595	63373
22402c623000ca7dba76abbec313d8baeb2a2c45	clustering of optimized data for email forensics	optimisation;spam;lsi;data mining;email;svd;clustering;feronsics	Forensics is a study of evidence to help the police solving crimes. If we apply (Forensics) in Computer Sciences domain, crimes are mainly network attacks found more in emails; which become nowadays the most popular way of communication accessible via Internet. We receive in our Inboxes emails gangs without being aware of them. Therefore, it is necessary to build an automatic checking system to filter good emails from bad ones. In this paper, we propose a new emails processing approach using Singular Value Decomposition method (SVD) to optimize emails data before applying Data Mining techniques (Clustering) to extract bad emails located in the mail servers where the user’s inboxes are hosted. Our study is based on filtering Emails (bads and goods) by the clustering of optimized data compared with unoptimized one. Mathematics Subject Classification. 05C12, 05C50, 05B10, 91C20, 15A18, 34A05. Received October 29, 2013. Accepted November 12, 2015.		Dhai Eddine Salhi;Abdelkamel Tari;M. Tahar Kechadi	2016	RAIRO - Operations Research	10.1051/ro/2015057	spam;computer science;data mining;mathematics;internet privacy;cluster analysis;singular value decomposition;world wide web	AI	-26.273084952099058	-56.304499230149695	63409
34eced683e3ac78f5b6cd3fdbeb5e0c571cea207	towards better understanding and utilizing relations in dbpedia	dbpedia;relation understanding;relation utilization	This paper is concerned with the problems of understanding the relations in automatically extracted semantic datasets such as DBpedia and utilizing them in semantic queries such as SPARQL. Although DBpedia has achieved a great success in supporting convenient navigation and complex queries over the extracted semantic data from Wikipedia, the browsing mechanism and the organization of the relations in the extracted data are far from satisfactory. Some relations have anomalous names and are hard to be understood even by experts if looking at the relation names only; there exist synonymous and polysemous relations which may cause incomplete or noisy query results. In this paper, we propose to solve these problems by 1 exploiting the Wikipedia category system to facilitate relation understanding and query constraint selection, 2 exploring various relation representation models for similar/super-/sub-relation detection to help the users select proper relations in their queries. A prototype system has been implemented and extensive experiments are performed to illustrate the effectiveness of the proposed approach.	dbpedia	Linyun Fu;Haofen Wang;Wei Jin;Yong Yu	2012	Web Intelligence and Agent Systems	10.3233/WIA-2012-0247	computer science;data mining;database;world wide web;information retrieval	AI	-27.68360395912109	-58.64826560283165	63522
545bac46a3389798a5900778eaea99b2cdc0e7d1	umass at trec 2017 common core track		This is an overview of University of Massachusetts efforts in providing document retrieval run submissions for the TREC Common Core Track with the goal of using newly developed techniques in retrieval and ranking to provide many new documents for relevance judgments. It is hoped these new techniques will reveal new documents not seen via traditional techniques, that will increase the numbers of relevant judged documents for the research collection.	document retrieval;relevance	Qingyao Ai;Hamed Zamani;Stephen M. Harding;Shahrzad Naseri;James Allan;W. Bruce Croft	2017			information retrieval;computer science	Web+IR	-32.05536697729617	-62.93241915874441	63533
a0cb7c675d45d7ac6ea6f7079a7ff462507eba4e	methods for miningweb communities: bibliometric, spectral, and flow		In this chapter, we examine the problem of Web community identification expressed in terms of the graph or network structure induced by the Web. While the task of community identification is obviously related to the more fundamental problems of graph partitioning and clustering, the basic task is differentiated from other problems by being within the Web domain. This single difference has many implications for how effective methods work, both in theory and in practice. In order of presentation, we will examine bibliometric similarity measures, bipartite community cores, the HITS algorithm, PageRank, and maximum flow-based Web communities. Interestingly, each of these topics relate to one-another in a non-trivial manner.	algorithm;art & architecture thesaurus;bibliometrics;cluster analysis;data mining;directed graph;effective method;graph partition;hyperlink;maximum flow problem;pagerank;webgraph;word lists by frequency;world wide web	Gary William Flake;Kostas Tsioutsiouliklis;Leonid Zhukov	2004				ML	-26.056131667740445	-57.3702410821828	63538
f1164bf540abaf0e865362db033ac8e73e02a9fa	a printing workflow recommendation tool--exploiting correlations between highly sparse case logs	pattern clustering;electronic commerce;pattern clustering xerox printing workflow configuration system recommendation tool sparse case logs user preference prediction mechanism personalized information filtering e commerce applications latent semantic indexing sparse data records;user preference prediction mechanism;xerox printing workflow configuration system;e commerce;information filtering;user preferences;personalized information filtering;workflow management software digital printing electronic commerce indexing information filtering information filters internet pattern clustering;e commerce applications;internet;sparse data records;indexing;latent semantic indexing;sparse case logs;digital printing;workflow management software;printing books large scale integration information filtering indexing machine learning sun uncertainty collaborative work information filters;sparse data;information filters;recommendation tool	As a user preference prediction mechanism, recommendation techniques have been widely used to support personalized information filtering in current e-commerce applications. We build a recommendation tool into the existing Xerox printing workflow configuration system in order to provide new users with a number of solutions that are possibly of their interests. Such solution recommendations can significantly improve the system efficiency and accuracy by reducing workflow generation overhead and helping users quickly identify their needs. In our work, the main challenge is the high sparsity inherent to our application data - most fields have missing values due to a customer's lack of background or uncertainty on their specific needs. We address this problem by using latent semantic indexing (LSI) to merge original sparse data records into dense and semantic records. The generated dense data are then grouped into clusters based on their correlations. These clusters, together with their user patterns and representative workflows, are used to support efficient online workflow recommendation. Our implemented tool is able to achieve 83% accuracy on a dataset of 4569 case logs with 91% average sparseness	e-commerce;information filtering system;knowledge-based configuration;latent semantic analysis;missing data;neural coding;overhead (computing);personalization;printing;sparse matrix	Ming Zhong;Tong Sun	2006	2006 5th International Conference on Machine Learning and Applications (ICMLA'06)	10.1109/ICMLA.2006.10	e-commerce;search engine indexing;latent semantic indexing;the internet;sparse matrix;computer science;data mining;digital printing;world wide web;information retrieval	DB	-29.422936211754433	-53.1485268480018	63572
28057c662471dfec2d3caa91d123a9559293eed1	combination of feature selection methods for text categorisation	feature selection;test collection	Feature selection plays a vital role in text categorisation. A range of different methods have been developed, each having unique properties and selecting different features. We show some results of an extensive study of feature selection approaches using a wide range of combination methods. We performed experiments on 18 test collections and report a subset of the results.	categorization;document classification;experiment;feature selection	Robert Neumayer;Rudolf Mayer;Kjetil Nørvåg	2011		10.1007/978-3-642-20161-5_89	computer science;machine learning;pattern recognition;data mining	Web+IR	-20.80837079020659	-63.851041714916356	63583
fad4cf87ca2f3948b7a71b306e321454af7b346b	web credibility: features exploration and credibility prediction	classification;regression;feature analysis;web credibility	The open nature of the World Wide Web makes evaluating webpage credibility challenging for users. In this paper, we aim to automatically assess web credibility by investigating various characteristics of webpages. Specifically, we first identify features from textual content, link structure, webpages design, as well as their social popularity learned from popular social media sites (e.g., Facebook, Twitter). A set of statistical analyses methods are applied to select the most informative features, which are then used to infer webpages credibility by employing supervised learning algorithms. Real dataset-based experiments under two application settings show that we attain an accuracy of 75% for classification, and an improvement of 53% for the mean absolute error (MAE), with respect to the random baseline approach, for regression.	algorithm;approximation error;baseline (configuration management);experiment;general-purpose modeling;information;machine learning;social media;stanford web credibility project;supervised learning;web page;world wide web	Alexandra Olteanu;Stanislav Peshterliev;Xin Liu;Karl Aberer	2013		10.1007/978-3-642-36973-5_47	pattern recognition;regression;biological classification;computer science;data mining;world wide web;information retrieval	Web+IR	-21.70030644338102	-57.19047689104144	63736
6f04fe28b4637351875f86749025284d650b2279	unsupervised storyline extraction from news articles		Storyline extraction from news streams aims to extract events under a certain news topic and reveal how those events evolve over time. It requires algorithms capable of accurately extracting events from news articles published in different time periods and linking these extracted events into coherent stories. The two tasks are often solved separately, which might suffer from the problem of error propagation. Existing unified approaches often consider events as topics, ignoring their structured representations. In this paper, we propose a non-parametric generative model to extract structured representations and evolution patterns of storylines simultaneously. In the model, each storyline is modelled as a joint distribution over some locations, organizations, persons, keywords and a set of topics. We further combine this model with the Chinese restaurant process so that the number of storylines can be determined automatically without human intervention. Moreover, per-token Metropolis-Hastings sampler based on light latent Dirichlet allocation is employed to reduce sampling complexity. The proposed model has been evaluated on three news corpora and the experimental results show that it outperforms several baseline approaches.	baseline (configuration management);bayesian network;coherence (physics);generative model;latent dirichlet allocation;metropolis;metropolis–hastings algorithm;modified huffman coding;propagation of uncertainty;sampling (signal processing);software propagation;text corpus	Deyu Zhou;Haiyang Xu;Xin-Yu Dai;Yulan He	2016			computer science;artificial intelligence;data science;machine learning;data mining	Web+IR	-23.620635620766055	-55.886858094260724	63838
5a80fde30bb7502f4e5cc76a66e0c2b62abe1efb	weakly supervised relevance feedback based on an improved language model	relevance feedback natural language processing pattern clustering;cluster algorithm;pattern clustering;cluster;relevant documents;clustering algorithm;query expansion information retrieval ir relevance feedback cluster relevant documents;information retrieval;clustering algorithm relevance feedback language model;html;query expansion;relevance feedback;natural language processing;language model;information retrieval ir	Relevance feedback, which traditionally uses the terms in the relevant documents to enrich the user's initial query, is an effective method for improving retrieval performance. This approach has another problem is that Relevance feedback assumes that most frequent terms in the feedback documents are useful for the retrieval. In fact, the reports of some experiments show that it does not hold in reality many expansion terms identified in traditional approaches are indeed unrelated to the query and harmful to the retrieval. In this paper, we propose to select better and more relevant documents with a clustering algorithm. And then we present an improved Language Model to help us identify the good terms from those relevant documents. Ours experiments on the 2008 TREC collection show that retrieval effectiveness can be much improved when the improved Language Model is used.	cluster analysis;effective method;experiment;k-nearest neighbors algorithm;language model;nn (newsreader);relevance feedback;supervised learning;text retrieval conference	Xinsheng Li;Si Li;Weiran Xu;Guang Chen;Jun Guo	2010	Proceedings of the 6th International Conference on Natural Language Processing and Knowledge Engineering(NLPKE-2010)	10.1109/NLPKE.2010.5587859	natural language processing;query expansion;ranking;relevance;computer science;concept search;data mining;information retrieval;human–computer information retrieval	Web+IR	-28.436976498241062	-58.95406761239678	64049
aeebc7b9b39dd5d2cf4f91de3d2251893cd543a8	learning to extract information from large domain-specific websites using sequential models	generic model;information extraction;exponential model;rule based;news summarization;correlated summarization;biclustering;conditional random field;bipartite graph;named entity;domain specificity;problem solving	In this article we describe a novel information extraction task on the web and show how it can be solved effectively using the emerging conditional exponential models. The task involves learning to find specific goal pages on large domain-specific websites. An example of such a task is to find computer science publications starting from university root pages. We encode this as a sequential labeling problem solved using Conditional Random Fields (CRFs). These models enable us to exploit a wide variety of features including keywords and patterns extracted from and around hyperlinks and HTML pages, dependency among labels of adjacent pages, and existing databases of named entities in a unified probabilistic framework. This is an important advantage over previous rule-based or generative models for tackling the challenges of diversity on web data.	computer science;conditional random field;database;encode;exponential time hypothesis;generative model;html;hyperlink;information extraction;logic programming;named entity;root-finding algorithm;time complexity	Sunita Sarawagi;V. G. Vinod Vydiswaran	2004	SIGKDD Explorations	10.1145/1046456.1046464	bipartite graph;computer science;data science;machine learning;data mining;conditional random field;information extraction;biclustering;information retrieval	ML	-25.27713100939446	-59.63697411172748	64094
1d2a3ba720c466028591977ef6841ebaf227c30f	web-based text classification in the absence of manually labeled training documents		Most text classification techniques assume that manually labeled documents (corpora) can be easily obtained while learning text classifiers. However, labeled training documents are sometimes unavailable or inadequate even if they are available. The goal of this article is to present a self-learned approach to extract high-quality training documents from the Web when the required manually labeled documents are unavailable or of poor quality. To learn a text classifier automatically, we need only a set of user-defined categories and some highly related keywords. Extensive experiments are conducted to evaluate the performance of the proposed approach using the test set from the Reuters-21578 news data set. The experiments show that very promising results can be achieved only by using automatically extracted documents from the Web. © 2007 Wiley Periodicals, Inc.	document classification	Chen-Ming Hung;Lee-Feng Chien	2007	JASIST	10.1002/asi.20442	greedy algorithm;classifier;computer science;data mining;world wide web;information extraction;information retrieval	Web+IR	-24.2461221640094	-64.72010594828953	64124
4d02c92c654d2e9b88e11d316a62f31d7f9e7f2a	using community structure to categorize computer science conferences: initial results		Research in computer science (CS) is published mainly in conferences. We investigate the possibility of automatically categorizing CS conferences by using exemplars (influential conferences). We propose an automatic exemplars selection method. Our experiments show that categorizing by exemplars matches well with curated topic classification from the Chinese CCF conference list. The results also accord with manual judgement which show promise as a practical and robust method for categorizing CS conferences.	cs games;categorization;computer science;experiment;microsoft customer care framework;selection (genetic algorithm)	Suhendry Effendy;Roland H. C. Yap	2017		10.1145/3110025.3110102	data mining;data science;categorization;judgement;community structure;computer science	Logic	-26.640839670881814	-61.066969189949724	64253
adf726bdcdddacee1c70d911b8f84b6a16841a32	extra: extracting prominent review aspects from customer feedback		Many existing systems for analyzing and summarizing customer reviews about products or service are based on a number of prominent review aspects. Conventionally, the prominent review aspects of a product type are determined manually. This costly approach cannot scale to large and cross-domain services such as Amazon.com, Taobao.com or Yelp.com where there are a large number of product types and new products emerge almost everyday. In this paper, we propose a novel framework, for extracting the most prominent aspects of a given product type from textual reviews. The proposed framework, ExtRA, extracts K most prominent aspect terms or phrases which do not overlap semantically automatically without supervision. Extensive experiments show that ExtRA is effective and achieves the state-of-the-art performance on a dataset consisting of different product types.	baseline (configuration management);experiment;internet backbone;pagerank;personalization;product type;taobao marketplace;unsupervised learning;user review;wordnet	Zhiyi Luo;Shanshan Huang;Frank F. Xu;Christian A Siebra;Hanyuan Shi;Kenny Q. Zhu	2018			artificial intelligence;natural language processing;machine learning;computer science	NLP	-21.497652774162322	-66.0461556281571	64337
1fd3bf126d4cf039eb79121151a65aa20745de4b	peq: an explainable, specification-based, aspect-oriented product comparator for e-commerce	comparison mining;decision support systems	While purchasing a product, consumers often rely on specifications as well as online reviews of the product for decision-making. While comparing, one often has in mind a specific aspect or a set of aspects which are of interest to them. Previous work has used comparative sentences, where two entities are compared directly in a single sentence by the review author, towards the comparison task. In this paper, we extend the existing model by incorporating the feature specifications of the products, which are easily available, and learn the importance to be associated with each of them. To test the validity of these product ranking measures, we comprehensively test it on a digital camera dataset from Amazon.com and the results show good empirical outperformance over the state-of-the-art baselines.	aspect-oriented software development;baseline (configuration management);comparator;digital camera;e-commerce payment system;entity;everquest;mind;purchasing	Abhishek Sikchi;Pawan Goyal;Samik Datta	2016		10.1145/2983323.2983901	computer science;artificial intelligence;data mining;database;world wide web;information retrieval	NLP	-22.967244541902804	-57.192134992763165	64436
a825df111a1c4e94192011269499116f39eaff95	text classification using stochastic keyword generation	text classification	This paper considers improving the performance of text classification, when summaries of the texts, as well as the texts themselves, are available during learning. Summaries can be more accurately classified than texts, so the question is how to effectively use the summaries in learning. This paper proposes a new method for addressing the problem, using a technique referred to as ’stochastic keyword generation’ (SKG). In the proposed method, the SKG model is trained using the texts and their associated summaries. In classification, a text is first mapped, with SKG, into a vector of probability values, each of which corresponds to a keyword. Text classification is then conducted on the mapped vector. This method has been applied to email classification for an automated help desk. Experimental results indicate that the proposed method based on SKG significantly outperforms other methods.	automatic summarization;baseline (configuration management);cluster analysis;document classification;email;experiment;preprocessor;supervised learning;vector graphics	Cong Li;Ji-Rong Wen;Hang Li	2003			natural language processing;computer science;data mining;information retrieval	NLP	-24.82161091335692	-66.12801640362856	64470
2d47720a8b0825e655a59f142d722924c396563b	emotions on bengali blog texts: role of holder and topic	emotions;pattern clustering;semantic clustering bengali blog text emotion analysis blogger emotion identification bengali blog documents emotion holders topic identification emotional expression support vector machine based supervised framework svm blog sentences;support vector machines;rule based;text analysis;emotion recognition;topic emotions blogs holder;holder;blogs semantics feature extraction syntactics support vector machines rhetoric dictionaries;web sites emotion recognition pattern clustering support vector machines text analysis;web sites;support vector machine;blogs;topic;emotional expression	The paper presents an approach to identify the emotions of the bloggers on different topics provided in the Bengali blog documents. The rule based identification of emotion holders and topics along with their corresponding emotional expressions forms the baseline system. A Support Vector Machine (SVM) based supervised framework is also employed to identify the three components from the blog sentences and it outperforms the baseline system. As the topic of a document is not always conveyed at the sentence level, the similarity between a document's overall topic and sentential topic is measured through semantic clustering approach. Two different approaches are adopted to identify the many to many relationships among the holders and topics on Ekman's six emotions. One is based from the perspectives of the holders and other is with respect to topics. The two way evaluation of Ekman's six emotions achieves precision, recall and F-Score of 65.02%, 76.23% and 70.18% for 10 bloggers and 71.02%, 78.47% and 74.55% for 8 different topics on 512 test sentences respectively.	baseline (configuration management);blog;cluster analysis;f1 score;many-to-many;support vector machine	Dipankar Das;Sivaji Bandyopadhyay	2011	2011 International Conference on Advances in Social Networks Analysis and Mining	10.1109/ASONAM.2011.106	rule-based system;natural language processing;support vector machine;text mining;speech recognition;computer science;machine learning;pattern recognition	NLP	-23.206359183452104	-65.90486533056388	64776
bbb06e2fe0873797218e694a63a18b4eeddee1cb	improved spam filtering by extraction of information from text embedded image e-mail	content based filtering;e mail classification;text messaging;text classification;spam filtering;machine learning;ocr;feature selection;ontology	The increase of image spam, a kind of spam in which the text message is embedded into an attached image to defeat spam filtering techniques, is becoming an increasingly major problem. For nearly a decade, content based filtering using text classification or machine learning has been a major trend of antispam filtering systems. A Key technique being used by spammers is to embed text into image(s) in spam email. In [4], we proposed two levels of ontology spam filters: a first level global ontology filter and a second level user-customized ontology filter. However, that previous system handles only text e-mail and the percentage of attached images is increasing sharply. The contribution of the paper is that we add an image e-mail handling capability to the previous anti-spam filtering system, enhancing the effectiveness of spam filtering.	anti-spam techniques;document classification;email filtering;embedded system;information extraction;key;machine learning;spamming	Seongwook Youn;Dennis McLeod	2009		10.1145/1529282.1529677	computer science;bag-of-words model;machine learning;ontology;data mining;feature selection;world wide web;information retrieval	Web+IR	-19.649100433657907	-58.414807823416275	64906
3c2c130739370e11d17e79a50c1bafb1c070945a	user preference mining through hybrid collaborative filtering and content-based filtering in recommendation system	content based filtering;tecnologia electronica telecomunicaciones;data mining;machine learning;collaborative filtering;hybrid filtering;tecnologias;grupo a;retrieval information		collaborative filtering;recommender system	Kyung-Yong Jung;Jung-Hyun Lee	2004	IEICE Transactions		computer science;collaborative filtering;information filtering system;machine learning;data mining;world wide web;information retrieval;recommender system	Web+IR	-28.9656380972853	-52.856905397892156	64943
5e7760aa108bb0e56a9f0baa5a5a08de6234085b	a neural networks-based graph algorithm for cross-document coreference resolution	neural networks nbcut maximum entropy min cut bestcut;graph theory;pattern clustering;document handling;named noun phrase coreference task natural language processing systems classification phase clusterization phase graph cutting algorithm neural networks based bestcut statistical model graph partitioning ace 2008 cross document coreference resolution data sets;ace 2008 cross document coreference resolution data sets;clusterization phase;neural networks based bestcut;neural nets;noun phrase;training;data mining;neural networks nbcut;statistical model;neural networks clustering algorithms telecommunications research and development natural language processing noise measurement performance analysis entropy helium partitioning algorithms;artificial neural networks;computational modeling;graph partitioning;statistical analysis;classification phase;min cut bestcut;graph cut;classification algorithms;pattern classification;graph algorithm;natural language processing systems;entropy;named noun phrase coreference task;coreference resolution;load modeling;statistical analysis document handling graph theory natural language processing neural nets pattern classification pattern clustering;natural language processing;maximum entropy;graph cutting algorithm;neural network	Cross-document coreference resolution, which is an important subtask in natural language processing systems, focus on the problem of determining if two mentions from different documents refer to the same entity in the world. In this paper we present a two-step approach, employing a classification and clusterization phase. In a novel way, the clusterization is produced as a graph cutting algorithm, namely, neural networks-based BestCut (NBCut). To our knowledge, our system is the first that employs a statistical model in graph partitioning. We evaluate our approach on ACE 2008 cross-document coreference resolution data sets and obtain encouraging result, indicating that on named noun phrase coreference task, the approach holds promise and achieves competitive performance.	ace;algorithm;artificial neural network;coefficient;dijkstra's algorithm;experiment;feature extraction;graph partition;list of algorithms;natural language processing;neural networks;statistical classification;statistical model;windows me	Saike He;Yuan Dong;Haila Wang	2008	2008 International Conference on Natural Language Processing and Knowledge Engineering	10.1109/NLPKE.2008.4906800	natural language processing;computer science;machine learning;pattern recognition	NLP	-23.44569936257622	-64.20444380944706	65057
dafad0a0fce3f9cd684d8386547360c84154742d	pooling for user-oriented evaluation measures	pooling;user models;evaluation	Traditional TREC-style pooling methodology relies on using predicted relevance by systems to select documents for judgment. This coincides with typical search behaviour (e.g., web search). In the case of temporally ordered streams of documents, the order that users encounter documents is in this temporal order and not some predetermined rank order. We investigate a user oriented pooling methodology focusing on the documents that simulated users would likely read in such temporally ordered streams. Under this user model, many of the relevant documents found in the TREC 2013 Temporal Summarization Track's pooling effort would never be read. Not only does our pooling strategy focus on pooling documents that will be read by (simulated) users, the resultant pools are different from the standard TREC pools.	document;pool (computer science);relevance;resultant;text retrieval conference;web search engine	Gaurav Baruah;Adam Roegiest;Mark D. Smucker	2015		10.1145/2808194.2809493	computer science;data mining;world wide web;information retrieval	Web+IR	-33.526507722445444	-52.507499027969665	65064
534e9fc84514f51bf3d167757b698a8e1245a319	diversifying top-k service retrieval	semantics linear programming google context search problems algorithm design and analysis electronic mail;service oriented architecture approximation theory cloud computing query processing search problems;2014 services collection top k service retrieval diversification soa service oriented architecture service discovery cloud environment document search technique semantic aspect query service content service topic candidate services search algorithm approximation guarantee greedy search algorithm trec benchmark;κ service retrieval;会议论文;lda;kappa;greedy algorithm kappa;submodularity;service retrieval submodularity lda;greedy algorithm;amp;service retrieval	"""As more and more applications are based on SOA (service oriented architecture), effective service discovery is an urgent requirement for such service applications in the cloud environment. Some existing work focus on taking service content as text and using document search technique to implement service discovery. However services are designed to implement some specific functions or objectives, which leads to the underlying """"topic"""" or """"semantic"""" of services. In this paper, we study the top-k service retrieval problem from both the text perspective and the semantic aspect, which is to find a set of k services that can best answer a query and the result set is to balance between the content relevance and the topic diversity among the returned services. Both service content and service topic are considered to identify the candidate services. We propose the objective function which is sub-modular, and we design the search algorithm with a approximation guarantee of factor 1 - 1/e for the (best-first) greedy search algorithm. Experiments on a large TREC benchmark and services collection show the effectiveness of our approach."""	approximation;benchmark (computing);cloud computing;greedy algorithm;loss function;optimization problem;relevance;result set;search algorithm;service discovery;service-oriented architecture;text retrieval conference;yahoo! answers	Chaofeng Sha;Keqiang Wang;Kai Zhang;Xiaoling Wang;Aoying Zhou	2014	2014 IEEE International Conference on Services Computing	10.1109/SCC.2014.38	computer science;data mining;data as a service;world wide web;information retrieval	Web+IR	-29.72872936498643	-55.88630878002614	65135
2f9b097604ea0ed9f5a36111468841d692a00fa2	latent customer needs elicitation for big-data analysis of online product reviews	support vector machines;training;semantics;redundancy;sentiment analysis latent customer needs elicitation online product reviews;feature extraction;sentiment analysis;support vector machines big data customer services sentiment analysis;support vector machines feature extraction sentiment analysis refining semantics redundancy training;refining;wordnet latent customer needs elicitation big data analysis online product reviews linguistic analysis sentiment analysis support vector machines english words	Traditional customer needs elicitation methods are often time and cost consuming due to the linguistic analysis of customer needs. Furthermore, many of them are unable to identify latent customer needs, such as interviews and focus groups. This paper proposes a new paradigm of customer needs elicitation based on sentiment analysis of individual product attributes of online product reviews. Support vector machines are used to build prediction models built on the features extracted from a list of affective lexicons based on affective norms for English words and WordNet. The proposed method is able to compile sentiment information on individual product attributes. Such information greatly facilitates the process to elicit customer needs, especially latent ones. We also present a case study to show the potential and feasibility of the proposed method.	big data;compiler;focus group;lexicon;programming paradigm;sentiment analysis;support vector machine;wordnet	Feng Zhou;R. J. Jiao	2015	2015 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM)	10.1109/IEEM.2015.7385968	natural language processing;voice of the customer;support vector machine;refining;feature extraction;computer science;data science;machine learning;data mining;semantics;customer intelligence;redundancy;sentiment analysis	SE	-22.188621775011	-58.06772982707097	65305
e19b60e5b8083828285a2baa781ceaad27f6353c	the accuracy and value of machine-generated image tags: design and user evaluation of an end-to-end image tagging system	user evaluation;image content;classif image tagging;end to end systems;user study;user evaluations;error tolerance;user value;ranking and selection;conference paper;in buildings;base line performance;user input;system evaluation;photo sharing;relative importance;image tagging;tagging systems;social media;keywords automatic classifiers	Automated image tagging is a problem of great interest, due to the proliferation of photo sharing services. Researchers have achieved considerable advances in understanding motivations and usage of tags, recognizing relevant tags from image content, and leveraging community input to recommend more tags. In this work we address several important issues in building an end-to-end image tagging application, including tagging vocabulary design, taxonomy-based tag refinement, classifier score calibration for effective tag ranking, and selection of valuable tags, rather than just accurate ones. We surveyed users to quantify tag utility and error tolerance, and use this data in both calibrating scores from automatic classifiers and in taxonomy based tag expansion. We also compute the relative importance among tags based on user input and statistics from Flickr. We present an end-to-end system evaluated on thousands of user-contributed photos using 60 popular tags. We can issue four tags per image with over 80% accuracy, up from 50% baseline performance, and we confirm through a comparative user study that value-ranked tags are preferable to accuracy-ranked tags.	apache axis;automatic image annotation;baseline (configuration management);cvpr;chua's circuit;color;commonsense reasoning;computer vision;content-based image retrieval;cyc;database;end system;end-to-end encryption;end-to-end principle;error-tolerant design;faceted classification;flickr;human factors and ergonomics;hypermedia;hypertext;ibm research;iccv;ieee multimedia;ieee transactions on pattern analysis and machine intelligence;information processing;information retrieval;jun wang (scientist);lisp machine;machine learning;multi-label classification;object detection;ontology (information science);open mind common sense;outline of object recognition;p (complexity);pc game;part-based models;real-time transcription;reed–solomon error correction;refinement (computing);sigchi;semantic web;springer (tank);statistical learning theory;systemverilog;tag (metadata);tag cloud;taxonomy (general);text-based (computing);usability testing;vocabulary;www;yang	Lexing Xie;Apostol Natsev;Matthew L. Hill;John R. Smith;Alex Phillips	2010		10.1145/1816041.1816052	social media;computer science;data mining;world wide web;information retrieval	HCI	-20.918725691616377	-60.27985543767575	65324
52ac19b8f8d0a74453512eb1846ce2bbc55086f6	reverse mapping of referral links from storage hierarchy for web documents	databases;thesaurus;research resource;researcher;information resources;organization;web documents;search engine;science and technology;paper;integrated search;web pages;idea;url address reverse mapping referral links storage hierarchy web documents referral parent web page web surfing information discovery hierarchical hyperlink structure storage path directory information exhaustibility first search generalization first search;jst;search engines;url address;storage path directory information;search space;information retrieval;technical term;institute;japan science and technology agency;reverse mapping of referral links from storage hierarchy for web documents;sparkingarticle;expanding;database;text analysis;magazine;patent;technical trend;information discovery;chemical substance;journal;professional;linking;storage hierarchy;referral links;hierarchical hyperlink structure;j global;search;reverse mapping;research and development;bobliography;internet;ｊｇｌｏｂａｌ;indexing;comprehensive search;material;funding;facility;r d;web sites;jglobal;jdream;uniform resource locators web pages databases indexing information retrieval web sites search engines text analysis internet clustering algorithms;gene;ｊ ｇｌｏｂａｌ;clustering algorithms;world wide web;referral parent;web surfing;imagination;web page;information retrieval information resources;research project;related search;generalization first search;exhaustibility first search;uniform resource locators;ｊｓｔ;article;linkcenter	In world wide web, a document is usually made up of multiple pages, each one of which has a unique URL address and links to each other by hyperlink pointers. Related documents are also connected together through hyperlinks. Thus, it is important for web information discovery to know the referral parent(s) of a given web page. However, due to the lack of back-pointers, such information is usually not available to web surfers. This reduces the effectiveness of web surfing and information discovery significantly. Major search engines such as AltaVista and HotBot try to solve this problem from the server side by maintaining the hyperlink structure of each web site registered with them. Although effective, the drawbacks of this approach are the huge effort to store and maintain this link database and the limitation to their registered members only. In this paper, we propose an alternative mechanism to locate the referral parent(s) for a given fragment of a web document (in terms of an URL address) from the client side. The basic idea is to explore the hierarchical hyperlink structure of a web document fragment from its storage path directory information that is embedded in its URL. Two mechanisms were proposed: exhaustibility first search (EFS) and generaliz ation first search (GFS). Results showed that through direct mapping of the storage hierarchy path to URL address and then performing reverse tracing along the hyperlink structure, the chance to discover a referral parent ranged from 41% to 51%. Compared to EFS, GFS was found to reduce the average search space from 26 pages to 4 pages. This result is important to web information discovery because no link database needs to be maintained and the overhead of finding referral page is very low. Submitted to: ICTAI2000	approximation algorithm;client-side;cluster analysis;complement (complexity);directory (computing);embedded system;encrypting file system;hotbot;hyperlink;information discovery;memory hierarchy;overhead (computing);referral marketing;search algorithm;server (computing);server-side;star catalogue;web page;web search engine;world wide web	Chen Ding;Chi-Hung Chi;Vincent W. L. Tam	2000		10.1109/TAI.2000.889873	text mining;static web page;computer science;web page;data mining;world wide web;rewrite engine;information retrieval;search engine;research	Web+IR	-30.011112597472113	-53.822661708140146	65354
a7eb72ca7ae9c6c6105a7d98dd9a8fd02a46f963	semantic pattern based dependency matching for exact answer retrieval		In this paper, we retrieve exact answer from a question/answer pair based on semantic pattern and dependency matching. Question target and the weight information of main phrase are gained from semantic pattern. Candidate exact answers to a question are retrieved from its corresponding sentence-form answer by dependency relation matching. A probabilistic model, in which weight information is included, is then adopted to select the most appropriate one. The semantic correctness of selected exact answer can be validated by question target. Our experiments achieve almost an accuracy of 70% in the open domain and 80% in the medicine domain.	correctness (computer science);dependency relation;experiment;statistical model	Xiaoli Liu;Dawei Hu;Min Feng;Wenyin Liu	2007	Third International Conference on Semantics, Knowledge and Grid (SKG 2007)	10.1109/SKG.2007.14	natural language processing;statistical model;question answering;computer science;data mining;database;information retrieval;divergence-from-randomness model	DB	-28.236354310047822	-64.94510098163526	65400
d7c9b64bcf4a9d1fa08199389a9f626fd594d431	approximating google's rankings with latent semantic analysis	google;search engine;semantic web approximation theory query processing search engines;google search;query processing;r precision google ranking approximation latent semantic analysis google search correlation coefficient url snippet;search engines;google ranking approximation;semantics;semantics google correlation matrix decomposition search engines patents portable media players;approximation theory;search results ranking latent semantic analysis web search;matrix decomposition;patents;weighted sums;semantic web;search results ranking;web search;portable media players;correlation;correlation matrix;correlation coefficient;latent semantic analysis;r precision;snippet;url	This study proposed a Latent Semantic Analysis based method to approximate Google's ranking. We conduct Latent Semantic Analysis on Google's search results for a given query to find terms with highest LSA weights as features. Then the correlation coefficients between the features and the given query are obtained for use as the feature values. Each result is scored and re-ranked based on a linear combination of weighted sum of feature values that appear in its title, snippet and URL. Experimental results on a small number of popular keywords show that this method is promising to achieve R-Precision up to 0.8 for some combination of search results and features used.	approximation algorithm;coefficient;latent semantic analysis;weight function	Cheng-Jye Luh;Chitsanzo Wesley Kazembe;Chun-Ju Li	2011	2011 Eighth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)	10.1109/FSKD.2011.6019915	computer science;data mining;semantics;probabilistic latent semantic analysis;world wide web;information retrieval;search engine	Web+IR	-30.083394760148025	-58.71330855606748	65453
fa1378e0a23f449eb2b8f16bbc64ff2203c292d3	an evidential reasoning approach to weighted combination of classifiers for word sense disambiguation	experimental tests;combination of classifiers;confiance;theoretical framework;dempster shafer theory of evidence;intelligence artificielle;data mining;classification;word sense disambiguation;evidential reasoning;confidence;confianza;teoria dempster shafer;fouille donnee;computational linguistic;polisemia;dempster shafer theory;polysemy;pattern recognition;polysemie;weighted combination of classifiers;artificial intelligence;computational linguistics;inteligencia artificial;reconnaissance forme;reconocimiento patron;word sense dis ambiguation;busca dato;clasificacion;theorie dempster shafer	Arguing that various ways of using context in word sense disambiguation (WSD) can be considered as distinct representations of a polysemous word, a theoretical framework for the weighted combination of soft decisions generated by experts employing these distinct representations is proposed in this paper. Essentially, this approach is based on the Dempster-Shafer theory of evidence. By taking the confidence of individual classifiers into account, a general rule of weighted combination for classifiers is formulated, and then two particular combination schemes are derived. These proposed strategies are experimentally tested on the datasets for four polysemous words, namely interest, line, serve, and hard.	emergent;experiment;web services for devices;word sense;word-sense disambiguation	Cuong Anh Le;Van-Nam Huynh;Akira Shimazu	2005		10.1007/11510888_51	natural language processing;dempster–shafer theory;biological classification;computer science;artificial intelligence;computational linguistics;machine learning;pattern recognition;mathematics;confidence;evidential reasoning approach	NLP	-22.245063365692054	-64.01777603538972	65488
cb13613cab309c9a5a831af87148d9c5d81bc0f7	efficient and effective prediction of social tags to enhance web search	web pages;collaborative indexing;retrieval effectiveness;predictive models;automatic categorization	As the web has grown into an integral part of daily life, social annotation has become a popular manner for web users to manage resources. This method of management has many potential applications, but it is limited in applicability by the cold-start problem, especially for new resources on the web. In this article, we study automatic tag prediction for web pages comprehensively and utilize the predicted tags to improve search performance. First, we explore the stabilizing phenomenon of tag usage in a social bookmarking system. Then, we propose a two-stage tag prediction approach, which is efficient and is effective in making use of early annotations from users. In the first stage, content-based ranking, candidate tags are selected and ranked to generate an initial tag list. In the second stage, random-walk re-ranking, we adopt a random-walk model that utilizes tag co-occurrence information to re-rank the initial list. The experimental results show that our algorithm effectively proposes appropriate tags for target web pages. In addition, we present a framework to incorporate tag prediction in a general web search. The experimental results of the web search validate the hypothesis that the proposed framework significantly enhances the typical retrieval model. © 2011 Wiley Periodicals, Inc.	web search engine	Ming-Hung Hsu;Hsin-Hsi Chen	2011	JASIST	10.1002/asi.21558	web modeling;data web;web analytics;computer science;social semantic web;web page;data mining;predictive modelling;noindex;web intelligence;web search query;world wide web;website parse template;information retrieval	ML	-27.439294191893165	-52.442860935748584	65589
d8b7f522041126c27474a6b67d7851469b4c32f5	the term vector database: fast access to indexing terms for web pages	term vectors;web pages;web connectivity;indexing terms;web search engine;topic distillation;page classification;web search;large classes	We have built a database that provides term vector information for large numbers of pages (hundreds of millions). The basic operation of the database is to take URLs and return term vectors. Compared to computing vectors by downloading pages via HTTP, the Term Vector Database is several orders of magnitude faster, enabling a large class of applications that would be impractical without such a database. This paper describes the Term Vector Database in detail. It also reports on two applications built on top of the database. The first application is an optimization of connectivity-based topic distillation. The second application is a Web page classifier used to annotate results returned by a Web search engine.  2000 Published by Elsevier Science B.V. All rights reserved.	data structure;database;download;hypertext transfer protocol;identifier;information retrieval;mathematical optimization;web page;web search engine	Raymie Stata;Krishna Bharat;Farzin Maghoul	2000	Computer Networks	10.1016/S1389-1286(00)00046-3	web service;static web page;web modeling;site map;database search engine;data web;index term;web mapping;web search engine;computer science;web page;database;world wide web;database schema;rewrite engine;information retrieval	DB	-31.07842132669207	-56.36179498723404	65928
b9716fbe0d45d2eb94828cabe252af37a0668afb	hierarchical label propagation and discovery for machine generated email	machine generated email;structural template;hierarchical label propagation	Machine-generated documents such as email or dynamic web pages are single instantiations of a pre-defined structural template. As such, they can be viewed as a hierarchy of template and document specific content. This hierarchical template representation has several important advantages for document clustering and classification. First, templates capture common topics among the documents, while filtering out the potentially noisy variabilities such as personal information. Second, template representations scale far better than document representations since a single template captures numerous documents. Finally, since templates group together structurally similar documents, they can propagate properties between all the documents that match the template. In this paper, we use these advantages for document classification by formulating an efficient and effective hierarchical label propagation and discovery algorithm. The labels are propagated first over a template graph (constructed based on either term-based or topic-based similarities), and then to the matching documents. We evaluate the performance of the proposed algorithm using a large donated email corpus and show that the resulting template graph is significantly more compact than the corresponding document graph and the hierarchical label propagation is both efficient and effective in increasing the coverage of the baseline document classification algorithm. We demonstrate that the template label propagation achieves more than 91% precision and 93% recall, while increasing the label coverage by more than 11%.	algorithm;baseline (configuration management);cluster analysis;document classification;dynamic web page;email;personally identifiable information;software propagation;statistical classification	James Bradley Wendt;Michael Bendersky;Lluis Garcia Pueyo;Vanja Josifovski;Balint Miklos;Ivo Krka;Amitabh Saikia;Jie Yang;Marc-Allen Cartright;Sujith Ravi	2016		10.1145/2835776.2835780	template method pattern;computer science;pattern recognition;data mining;world wide web	Web+IR	-26.975292811668787	-63.772532359758415	65944
c2377725842d0b7a82df5edd99ccf95e3c56aa1d	evaluating author name disambiguation for digital libraries: a case of dblp	author name disambiguation;digital library;triangulation;disambiguation evaluation;dblp	Author name ambiguity in a digital library may affect the findings of research that mines authorship data of the library. This study evaluates author name disambiguation in DBLP, a widely used but insufficiently evaluated digital library for its disambiguation performance. In doing so, this study takes a triangulation approach that author name disambiguation for a digital library can be better evaluated when its performance is assessed on multiple labeled datasets with comparison to baselines. Tested on three types of labeled data containing 5000 to 6 M disambiguated names, DBLP is shown to assign author names quite accurately to distinct authors, resulting in pairwise precision, recall, and F1 measures around 0.90 or above overall. DBLP’s author name disambiguation performs well even on large ambiguous name blocks but deficiently on distinguishing authors with the same names. Compared to other disambiguation algorithms, DBLP’s disambiguation performance is quite competitive, possibly due to its hybrid disambiguation approach combining algorithmic disambiguation and manual error correction. A discussion follows on strengths and weaknesses of labeled datasets used in this study for future efforts to evaluate author name disambiguation on a digital library scale.	algorithm;dbl-browser;digital library;error detection and correction;f1 score;internationalized domain name;library (computing);word-sense disambiguation	Jinseok Kim	2018	Scientometrics	10.1007/s11192-018-2824-5	data mining;error detection and correction;natural language processing;labeled data;digital library;ambiguity;author name;strengths and weaknesses;triangulation (social science);pairwise comparison;computer science;artificial intelligence	NLP	-27.300637072000193	-63.95508057571751	66273
3d83d28aa00dca02c9e1c2d25f28404330f4fb5c	repeat visits to vivisimo.com: implications for successive web searching	search engine;web usage studies;online searching;search engines;data collection;end user searching;web search engine;ls search engines;bi user interfaces usability;web search;usage records;bh information needs and information requirements analysis	We investigate the occurrence of Web searchers returning the same information course, in this case the Vivisimo Web#N#search engine. We analyze data from a transaction log spanning a roughly 8-day period with approximately 1,200,000#N#records. During this time 40,227 users made repeated daily visits to the search engine, submitting 648,897 queries. Our#N#findings show that repeat users make up about 21% of the user base. These repeat users account for over 45% of all#N#sessions and nearly 60% of all queries submitted. Most repeat users (46%) visited the search engine on two days during the data collection period, but a sizeable percentage (17%) made 5 or more daily visits to Vivisimo. We discuss the implications for successive searching and future research.		Bernard J. Jansen;Sherry Koshman;Amanda Spink	2005		10.1002/meet.1450420149	database search engine;metasearch engine;web search engine;computer science;database;search analytics;web search query;world wide web;information retrieval;search engine	HCI	-33.65738300762178	-53.40405078266074	66371
9c87b3160dfa8bb4c1146d70d899b79c4ab0318d	a comparative analysis of similarity measurement techniques through simreq framework	semantic similarity;simreq;comparative analysis;measurement;information retrieval;requirements;requirement engineering;similarity;similarity measure;natural language processing	The study of semantic similarity has long been an essential part of natural language processing. In informational retrieval, usage of similarity measurement is getting popularity day by day, but usage of similarity measurements in requirements engineering is not very common. Previously, we have presented SimReq, a Similarity Measurement Framework for requirements engineering. In this paper, we have used different similarity measurement techniques with SimReq framework, and perform a comparative analysis among those similarity measurement techniques. Results have shown that efficiency of Cosine similarity measurement is always better than rest of other similarity measurement techniques.	cosine similarity;natural language processing;qualitative comparative analysis;requirements engineering;semantic similarity	Muhammad Ilyas;Josef Küng	2009		10.1145/1838002.1838056	natural language processing;semantic similarity;computer science;normalized compression distance;data mining;information retrieval	Web+IR	-29.47315059064557	-62.41079271407009	66382
14ab32caee9d2a46b704db08e93336949a2dfc05	continuous top-k monitoring on document streams (extended abstract)		The efficient processing of document streams plays an important role in many information filtering systems. Emerging applications, such as news update filtering and social network notifications, demand presenting end-users with the most relevant content to their preferences. In this work, user preferences are indicated by a set of keywords. A central server monitors the document stream and continuously reports to each user the top-k documents that are most relevant to her keywords. The objective is to support large numbers of users and high stream rates, while refreshing the top-k results almost instantaneously. Our solution abandons the traditional frequency-ordered indexing approach, and follows an identifier-ordering paradigm that suits better the nature of the problem. When complemented with a locally adaptive technique, our method offers (i) optimality w.r.t. the number of considered queries per stream event, and (ii) an order of magnitude shorter response time than the state-of-the-art.	identifier;information filtering system;programming paradigm;response time (technology);server (computing);social network;user (computing)	U Hou LeongHou;Junjie Zhang;Kyriakos Mouratidis;Ye Li	2018	2018 IEEE 34th International Conference on Data Engineering (ICDE)	10.1109/ICDE.2018.00259	data mining;order of magnitude;streams;filter (signal processing);search engine indexing;response time;social network;computer science	DB	-26.70637930189585	-53.40243613570161	66393
58f1ce20871e58724f5de69d135e2a5730354909	user interest modeling by labeled lda with topic features	unsupervised learning;user modelling;human computer interaction;web pages;information retrieval;efficient algorithm;topic feature;text analysis;latent dirichlet allocation;labeled latent dirichlet allocation with topic feature llda tf;topic feature labeled latent dirichlet allocation with topic feature llda tf browsing history topic model user interest model;unsupervised model user interest modeling labeled lda latent dirichlet allocation topic features web browsing history user interest extraction innovative method web pages text extraction llda tf model;internet;user interest model;browsing history;web browsing;web pages feature extraction internet data models training encyclopedias;topic model;user modelling human computer interaction information retrieval internet text analysis unsupervised learning	As well known, the user interest is carried in the user's web browsing history that can be mined out. This paper presents an innovative method to extract user's interests from his/her web browsing history. We first apply an efficient algorithm to extract useful texts from the web pages in user's browsed URL sequence. We then proposed a Labeled Latent Dirichlet Allocation with Topic Feature (LLDA-TF) to mine user's interests from the texts. Unlike other works that need a lot of training data to train a model to adopt supervised information, we directly introduce the raw supervised information to the procedure of LLDA-TF. As shown in the experimental results, results given by LLDA-TF fit predefined categories well. Furthermore, LLDA-TF model can name the user interests by category words as well as a keyword list for each category.	algorithm;feature extraction;latent dirichlet allocation;linear discriminant analysis;loss function;mined;mined-out;optimization problem;web page	Wenfeng Li;Xiaojie Wang;Rile Hu;Jilei Tian	2011	2011 IEEE International Conference on Cloud Computing and Intelligence Systems	10.1109/CCIS.2011.6045022	latent dirichlet allocation;unsupervised learning;the internet;user modeling;computer science;web page;data mining;topic model;world wide web;information retrieval	Robotics	-26.138120716428112	-53.41505972509224	66399
a18ca7bc76007b4c4d5873d6ba844b643aca78a0	analyzing the public opinion on the brazilian political and corruption issues		The last decades have been characterized by the design and development of new technological tools to allow faster communication among people. In that context, the social network Twitter has reached an enormous popularity around the World as a microblogging service, where users under a given nickname can publish short messages called tweets. In particular, the social network disseminates the use of hashtags, which are part of the text, to promote topics, subjects and/or causes. In 2015, we started the development of the TsViz project to monitor and extract useful information from tweets. As main goal, we intend to model the public opinion along time and its relationship with different topics in an incremental and online manner. In that sense, we have been monitoring the Brazilian political and economic situation and the results provided evidence on how people have reacted by facing particular issues. Our results were obtained after analyzing hashtags and usernames associated with the corruption and the Brazilian political system, including the major Brazilian oil company involved, politicians, and the Chamber of Deputies and the Federal Senate of the National Congress of Brazil.	hashtag;social network;user (computing)	Ricardo Araújo Rios;Caio S. Lopes;Fabio H. G. Sikansi;Paulo A. Pagliosa;Rodrigo Fernandes de Mello	2017	2017 Brazilian Conference on Intelligent Systems (BRACIS)	10.1109/BRACIS.2017.37	public relations;microblogging;political system;publication;social media;corruption;popularity;politics;social network;public opinion;business	Web+IR	-22.16002894661965	-55.48169894528301	66428
7240db91b655d64d848157ae7842e1bb402307d1	little data, big stories: taking the pulse of large-scaled events on twitter	sentiment analysis;social network analysis;social media;natural language processing	With the proliferation of Big Data, Social Science projects being developed, this work takes a step back to design research avenues that specifically look at smaller, real-time Social Science projects. Building on an already developed platform, called Dynamic Twitter Network Analysis (DTNA), we build out exploration into multiple world event types, which were captured in real-time and used smaller datasets to allow the user the ability to seek location and topic-specific data collections in parallel to events occurring. With these datasets, we first establish what could be learned during the event that mimics larger projects in the same domain. Secondly, we compare the events to help bring awareness to strategies that can evolve as specific events occur. The datasets examined are from a 24-hour period from specific locations of relevance with a focus on polarizing events. This includes: 1)Boston Marathon Bombing, 2) Sandy Hook Elementary Shooting, 3) Gezi Park Riots, 4) Hurricane Sandy, 5) Batkid, Make-a-Wish Foundation, 6) Brazil World Cup Protests, and 7) 2014 NBA Championship (Game 5). These networks will be analyzed both from social network analysis (SNA) and natural language processing (NLP) approaches (including sentiment analysis and part of speech tagging comparing personal pronoun use).	24-hour clock;big data;categorization;hashtag;marathon;natural language processing;network formation;part-of-speech tagging;populous;real-time clock;real-time transcription;relevance;sentiment analysis;social network analysis	Patrick M. Dudas;Samantha Weirman;Christopher Griffin	2016	2016 IEEE 2nd International Conference on Collaboration and Internet Computing (CIC)	10.1109/CIC.2016.071	social network analysis;simulation;social media;computer science;machine learning;data mining;world wide web;algorithm;sentiment analysis	Visualization	-23.674937185483483	-54.69360022229068	66431
60ee5f88a7eed36babd15fa0a26605eb438c815f	geolocation for twitter: timing matters		Automated geolocation of social media messages can benefit a variety of downstream applications. However, these geolocation systems are typically evaluated without attention to how changes in time impact geolocation. Since different people, in different locations write messages at different times, these factors can significantly vary the performance of a geolocation system over time. We demonstrate cyclical temporal effects on geolocation accuracy in Twitter, as well as rapid drops as test data moves beyond the time period of training data. We show that temporal drift can effectively be countered with even modest online model updates.	concept drift;downstream (software development);geocoding;geolocation;social media;temporal database;test data	Mark Dredze;Miles Osborne;Prabhanjan Kambadur	2016			world wide web;computer science;internet privacy;geolocation	NLP	-22.89848501082736	-53.887429899122544	66551
540fadebab21f865fd27cb81875ee0e2b2ea730b	a concept lattice-based kernel for svm text classification	kernel function;feature space;text classification;concept lattice;feature selection;support vector machine;bag of words	Standard Support Vector Machines (SVM) text classification relies on bag-of-words kernel to express the similarity between documents. We show that a document lattice can be used to define a valid kernel function that takes into account the relations between different terms. Such a kernel is based on the notion of conceptual proximity between pairs of terms, as encoded in the document lattice. We describe a method to perform SVM text classification with concept lattice-based kernel, which consists of text pre-processing, feature selection, lattice construction, computation of pairwise term similarity and kernel matrix, and SVM classification in the transformed feature space. We tested the accuracy of the proposed method on the 20NewsGroup database: the results show an improvement over the standard SVM when very little training data are available.	annual review of information science and technology;artificial neural network;bag-of-words model;categorization;computation;de morgan's laws;digital library;document classification;ecml pkdd;emoticon;european neural network society;experiment;feature selection;feature vector;formal concept analysis;information retrieval;information theory;international conference on machine learning;k-nearest neighbors algorithm;lecture notes in computer science;morgan;neural networks;preprocessor;proceedings of the ieee;springer (tank);statistical classification;statistical learning theory;support vector machine	Claudio Carpineto;Carla Michini;Raffaele Nicolussi	2009		10.1007/978-3-642-01815-2_18	kernel;support vector machine;least squares support vector machine;kernel method;string kernel;kernel embedding of distributions;feature vector;radial basis function kernel;kernel principal component analysis;computer science;bag-of-words model;machine learning;pattern recognition;data mining;graph kernel;mathematics;tree kernel;feature selection;variable kernel density estimation;polynomial kernel	Web+IR	-21.718264851772897	-65.1621492345934	66630
0773be101bcb6a1e0a34fd764bab677cacab203a	two stage genetic approach for bio-chemical named entity recognition	chemistry computing;single objective optimization two stage genetic approach bio chemical named entity recognition chemical names identification chemical names classification feature selection approach feature subset selection supervised machine learning approach conditional random field based classifier crf feature identification features extraction iupac names iupac like names supervised classification technique single objective genetic algorithm f measure;pattern classification chemistry computing feature extraction genetic algorithms learning artificial intelligence;chemicals genetic algorithms sociology statistics biological cells feature extraction patents;feature extraction;conditional random field mention detection and classification from biochemical domain feature selection genetic algorithm;pattern classification;genetic algorithms;learning artificial intelligence	Determining different mentions of chemical names from texts has a wide-spread application in real life. Chemical names are complex in nature and there exist several representations and nomenclatures (like SMILES, InChI, IUPAC) which create a big challenge to their automatic identification and classification. In this paper we present a feature selection approach for appropriate feature subset selection from a well-known supervised machine learning approach namely conditional random field based classifier (CRF). Several features are identified and extracted without using any domain specific knowledge and/or resources for determining mentions of IUPAC and IUPAC-like names from scientific text using some supervised classification technique. The appropriate set of features for a particular supervised classification technique is extracted from this huge collection of features using some single objective genetic algorithm based feature selection technique. Experiments are carried out on the benchmark patent dataset. Evaluation shows encouraging performance with the overall F-measure values of 70.01% by single objective optimization based approach on patent 2008 test data set.	automatic identification and data capture;benchmark (computing);british informatics olympiad;conditional random field;decision tree;existential quantification;feature selection;genetic algorithm;inchi;machine learning;mathematical optimization;named-entity recognition;optimization problem;overfitting;real life;simplified molecular-input line-entry system;software patent;test data	Asif Ekbal;Sriparna Saha	2013	2013 International Conference on Advances in Computing, Communications and Informatics (ICACCI)	10.1109/ICACCI.2013.6637260	genetic algorithm;feature extraction;computer science;artificial intelligence;machine learning;linear classifier;pattern recognition;data mining;feature	AI	-19.363024737174747	-65.1080988084801	66662
019f32e0c6e3e614ba2d4f5e76de731c1c532b37	improving annotation categorization performance through integrated social annotation computation	text classification;annotation categorization;annotation systems;social annotation	People can identify and organize their ideas and comments with respect to relevant concept topics through using annotation systems. Those annotation systems are obvious that only supports a simple and manual categorization approach. The manual approach is a difficult and time-consuming task for general annotators. Therefore, we propose a requirement annotation categorization which helps annotators to promote the manual annotation categorization effectiveness. Moreover, we propose an integrated social annotation computation which improves the performance of our annotation categorization. In summary, the proposed annotation categorization is verified through experiments using real users' data sets. We achieved the 83.11% average accuracy for the proposed annotation categorization with integrated social annotation computation. We also show that the proposed annotation categorization requires only 17-20% average processing time (in comparison with the manual approach) is efficient.		Addison Y. S. Sue;Stephen J. H. Yang	2010	Expert Syst. Appl.	10.1016/j.eswa.2010.06.041	minimum information required in the annotation of models;image retrieval;computer science;bioinformatics;data mining;temporal annotation;information retrieval	Web+IR	-26.201468485225035	-65.26518429923506	66745
5c14e3b2b1cb888474176cd05a1afe88a118555e	morphological query expansion and language-filtering words for improving basque web retrieval	search engines;web-as-corpus;basque;nlp;morphological query expansion;language-filtering words	The experience of a user of major search engines or other web information retrieval services looking for information in the Basque language is far from satisfactory: they only return pages with exact matches but no inflections (necessary for an agglutinative language like Basque), many results in other languages (no search engine gives the option to restrict its results to Basque), etc. This paper proposes using morphological query expansion and language-filtering words in combination with the APIs of search engines as a very cost-effective solution to build appropriate web search services for Basque. The implementation details of the methodology (choosing the most appropriate language-filtering words, the number of them, the most frequent inflections for the morphological query expansion, etc.) have been specified by corpora-based studies. The improvements produced have been measured in terms of precision and recall both over corpora and real web searches. Morphological query expansion can improve recall up to 47 % and language-filtering words can raise precision from 15 % to around 90 %, although with a loss in recall of about 30–35 %. The proposed methodology has already been successfully used in the Basque search service Elebila (http://www.elebila.eu) and the web-as-corpus tool CorpEus (http://www.corpeus.org), and the approach could be applied to other morphologically rich or under-resourced languages as well.	query expansion	Igor Leturia;Antton Gurrutxaga;Nerea Areta;Iñaki Alegria;Aitzol Ezeiza	2013	Language Resources and Evaluation	10.1007/s10579-012-9208-x	natural language processing;query expansion;web query classification;computer science;world wide web;information retrieval	NLP	-31.65070358344562	-65.6995572756562	66778
392febc5c5cb2d5c8e11729d731ef7352ee0c6f7	summary of the ntcir-10 intent-2 task: subtopic mining and search result diversification	diversity;test collections;intents;subtopics;evaluation	The NTCIR INTENT task comprises two subtasks: {\em Subtopic Mining}, where systems are required to return a ranked list of {\em subtopic strings} for each given query; and {\em Document Ranking}, where systems are required to return a diversified web search result for each given query. This paper summarises the novel features of the Second INTENT task at NTCIR-10 and its main findings, and poses some questions for future diversified search evaluation.	diversification (finance);ranking (information retrieval);web search engine	Tetsuya Sakai;Zhicheng Dou;Takehiro Yamamoto;Yiqun Liu;Min Zhang;Makoto P. Kato;Ruihua Song;Mayu Iwata	2013		10.1145/2484028.2484104	evaluation;data mining;world wide web;information retrieval	Web+IR	-30.924741478460128	-62.21561123807431	66815
977a9422d923046c9892b39a0f6451249bfaa462	uestc at imageclef 2012 medical tasks		This paper describes the methods used and results archived by our research group in the ImageCLEF 2012 medical retrieval and classification tasks. We performed three sub-tasks, ad-hoc retrieval, case-based retrieval, and modality classification. For the retrieval tasks, we combined semantic-based retrieval with traditional text-based retrieval. The semantic-based retrieval was conducted by comparing query concepts and document concepts with semantic similarity measure, and asymmetric similarity measures were also proposed by modifying the existing symmetric measures. For the modality classification task, we used multiple kernel learning to combine various visual features.	archive;hoc (programming language);modality (human–computer interaction);multiple kernel learning;semantic similarity;similarity measure;text-based (computing)	Hong Wu;Kuangkai Sun;Xianzhi Deng;Yi Zhang;Bili Che	2012			document retrieval;visual word;computer science;pattern recognition;data mining;information retrieval;human–computer information retrieval	Web+IR	-25.723928620891932	-61.69407420076407	66836
6b703a6d068986f81bb0e2d02f5ee0117535bc97	patentlight: a patent search application	query language;patent retrieval;settore inf 01 informatica;user interface;patent search;xml retrieval;flexible query language	Patent retrieval is a complex challenging task; despite of the numerous patent search applications, more flexible solutions are needed to help users to easily identify relevant patents. This paper presents the PatentLight search tool that offers novel and flexible functionalities to help users looking for relevant patents represented in the XML format.		Silvia Calegari;Emanuele Panzeri;Gabriella Pasi	2012		10.1145/2362724.2362765	patent visualisation;query expansion;computer science;data mining;database;user interface;world wide web;information retrieval;query language	HCI	-32.326743119249954	-56.41652374081648	66878
b0a290cffbc9a9bbcde79dd68e51991269fae4ee	topic-based agreement and disagreement in us electoral manifestos		We present a topic-based analysis of agreement and disagreement in political manifestos, which relies on a new method for topic detection based on key concept clustering. Our approach outperforms both standard techniques like LDA and a state-of-the-art graph-based method, and provides promising initial results for this new task in computational social science.	cluster analysis;computation;computational social science;local-density approximation;polarization (waves)	Stefano Menini;Federico Nanni;Simone Paolo Ponzetto;Sara Tonelli	2017			machine learning;computer science;computational sociology;artificial intelligence;cluster analysis;data mining;graph	NLP	-24.005674013827214	-60.104191997129895	67009
a0eb9c4b70e7f63fef5faef0ac9282416e1b58a9	finding expert users in community question answering	expert recommendation;information retrieval;topic modeling;latent dirichlet allocation;tf idf;community question answering;article;language model;question answering	Community Question Answering (CQA) websites provide a rapidly growing source of information in many areas. This rapid growth, while offering new opportunities, puts forward new challenges. In most CQA implementations there is little effort in directing new questions to the right group of experts. This means that experts are not provided with questions matching their expertise, and therefore new matching questions may be missed and not receive a proper answer. We focus on finding experts for a newly posted question. We investigate the suitability of two statistical topic models for solving this issue and compare these methods against more traditional Information Retrieval approaches. We show that for a dataset constructed from the Stackoverflow website, these topic models outperform other methods in retrieving a candidate set of best experts for a question. We also show that the Segmented Topic Model gives consistently better performance compared to the Latent Dirichlet Allocation Model.	information retrieval;information source;latent dirichlet allocation;question answering;stack overflow;topic model	Fatemeh Riahi;Zainab Zolaktaf;M. Mahdi Shafiei;Evangelos E. Milios	2012		10.1145/2187980.2188202	latent dirichlet allocation;question answering;computer science;data science;machine learning;data mining;topic model;tf–idf;world wide web;information retrieval;language model	Web+IR	-24.766831844853247	-58.829082232382234	67271
ffa9d2d46d93bf45afd5ba685d5e3f845f706a79	mining online text data for sentiment and news impact analysis	informasjons og kommunikasjonsvitenskap;information and communication science;doctoral thesis	As continuous growth of Internet, an ever increasing amount of information becomes available on the World Wide Web (WWW). Information on the WWW has never been so exploded that search engines using traditional keyword-based searching strategies hardly meet people’s needs to retrieve knowledge from online massive text data. The motivation of this thesis comes from the great demands on discovering implicit knowledge and rich semantics from online documents. This thesis focuses on analyzing online business news, a representative of objective information, and online customer reviews, a representative of subjective information. For online business news, a topic driven impact analysis model is proposed that quantifies the impact of topic of a news article. With the proposed topic driven impact analysis model, an explorative visual analysis system called ImpactWheel is developed to help users better navigate and understand topic-specific companies’ impact relationships through mining rich information source of online business news. For online customer reviews, both document overall sentiment classification and attributedbased sentiment analysis are performed. In the regard of document overall sentiment classification, taking advantages of high frequency of Co-occurring Term (CoT) patterns in customer reviews, a frequency-based algorithm is proposed to generate complex features which benefits sentiment classifiers. In order to search for effective features and ignore useless ones produced by the frequency-based complex feature generation algorithm, an Effective Feature Search (EFS) framework is proposed, which makes a novel connection between feature candidate generation and a Stochastic Local Search process. In the regard of attributed-based sentiment analysis, the concept of Sentiment Ontology Tree is proposed, which organizes a product’s domain specific knowledge as well as sentiments in a tree-like ontology structure. With the concept of SOT, a Hierarchial Learning via Sentiment Ontology Tree (HL-SOT) approach is proposed to solve the sentiment analysis tasks in a hierarchical classification process. To enhance the classification performance and computational efficiency of the HL-SOT approach which encodes texts using a globally unified index term space, a Localized Feature Selection (LFS) framework is developed which generates the customized index term space for each node of SOT. Since that the HL-SOT approach was estimated by a RLS estimator which is not competent enough to find max class separation and that the statistical linear classifier has been evidently proven its fallibility on classifying sentiment, a more pragmatic Hybrid Hierarchical Classification Process (HHCP) is proposed. The HHCP approach employs a linear classifier that is capable of maximizing the class separation while minimizing the within-class variance for attribute detection and turns to a rule-based solution for sentiment orientation.	algorithm;computation;electronic business;encrypting file system;experiment;feature selection;fisher information;fisher–yates shuffle;heuristic;horseland;information source;inner class;internet;lexicon;linear classifier;logic file system;logic programming;naive bayes classifier;performance;recursive least squares filter;regularized least squares;sentiment analysis;small-outline transistor;statistical classification;text corpus;www;web search engine;world wide web	Wei Wei	2013			computer science;data science;world wide web;information retrieval	Web+IR	-21.81662120649718	-62.113116626555595	67350
3849994ac61a63bdb9854695b311bcd3d5b5ae45	performance improvement in automatic question answering system based on dependency term	automatic question answering system;information retrieval;dependency grammar;information retrieval model;satisfiability;field experiment;system performance;xml information retrieval;question and answer web forum;performance improvement;question answering system;natural language;dependency term;text retrieval;dependence structure;question answering	Automatic Question Answering (QA) system has become quite popular in recent years, especially since the QA tracks appeared at Text REtrieval Conference (TREC). However, using only lexical information, the keyword-based information retrieval cannot fully describe the characteristics of natural language, thus the system performance cannot make people satisfied. It is proposed in this paper a definition of dependency term, based on the dependency grammar, employing the natural language dependency structure, as the improvement of the term, to support the typical information retrieval models. It is in fact a solution for a special application in XML information retrieval (XML IR) field. Experiments show that: dependency-term-based information retrieval model effectively describes the characteristics of natural language questions, and improves the performance of automatic question answering system.	question answering	Jianxing Shi;Xiaojie Yuan;Shitao Yu;Hua Ning;Chenying Wang	2009		10.1007/978-3-642-05250-7_2	natural language processing;field experiment;question answering;cognitive models of information retrieval;computer science;database;natural language;information retrieval;satisfiability;dependency grammar;human–computer information retrieval	NLP	-30.85621520494106	-61.073581273354144	67367
aadc7379b3e2257c1a1863e3e52807e81fa1f27f	learning landmarks by exploiting social media	text tagging;indexing method;spatial distribution;automatic annotation;visual features;association analysis;social media	This paper introduces methods for automatic annotation of landmark photographs via learning textual tags and visual features of landmarks from landmark photographs that are appropriately location-tagged from social media. By analyzing spatial distributions of text tags from Flickr’s geotagged photos, we identify thousands of tags that likely refer to landmarks. Further verification by utilizing Wikipedia articles filters out non-landmark tags. Association analysis is used to find the containment relationship between landmark tags and other geographic names, thus forming a geographic hierarchy. Photographs relevant to each landmark tag were retrieved from Flickr and distinctive visual features were extracted from them. The results form ontology for landmarks, including their names, equivalent names, geographic hierarchy, and visual features. We also propose an efficient indexing method for content-based landmark search. The resultant ontology could be used in tag suggestion and content-relevant re-ranking.	flickr;geotagging;landmark point;norm (social);resultant;social media;wiki;wikipedia	Chia-Kai Liang;Yu-Ting Hsieh;Tien-Jung Chuang;Yin Wang;Ming-Fang Weng;Yung-Yu Chuang	2010		10.1007/978-3-642-11301-7_23	social media;computer science;genetic association;data mining;world wide web;information retrieval	AI	-27.144460898376185	-52.19772439453582	67630
fcc4781d31cd9387d129ba217be2bd475d09179d	poster: digitization and search: a non-traditional use of hpc	big data;digitization;indexing text	We describe our efforts to provide a form of automated search of handwritten content for digitized document archives. To carry out the search we use a computer vision technique called word spotting. A form of content based image retrieval, it avoids the still difficult task of directly recognizing text by allowing a user to search using a query image containing handwritten text and ranking a database of images in terms of those that contain more similar looking content. In order to make this search capability available on an archive three computationally expensive pre-processing steps are required. We augment this automated portion of the process with a passive crowd sourcing element that mines queries from the systems users in order to then improve the results of future queries. We benchmark the proposed framework on 1930s Census data, a collection of roughly 3.6 million forms and 7 billion individual units of information.	analysis of algorithms;archive;benchmark (computing);computer vision;content-based image retrieval;crowdsourcing;preprocessor;units of information	Liana Diesendruck;Luigi Marini;Rob Kooper;Mayank Kejriwal;Kenton McHenry	2012	2012 SC Companion: High Performance Computing, Networking Storage and Analysis	10.1109/SC.Companion.2012.260		Web+IR	-31.969376215522367	-59.80339453251322	67784
00c3d6b59f46139eb8b50c4e1e1e9e9e6c8d0273	urank: visual analytics approach for search result exploration	visual analytics bars inspection recommender systems encoding atmospheric measurements;search engines data analysis document handling internet;list based representation urank visual analytics approach search result exploration web based tool lightweight text analytics visual method topic wise exploration document set dynamic document ranking view inspection reorganizing document cognitive load	uRank is a Web-based tool combining lightweight text analytics and visual methods for topic-wise exploration of document sets. It includes a view summarizing the content of the document set in meaningful terms, a dynamic document ranking view and a detailed view for further inspection of individual documents. Its major strength lies in how it supports users in reorganizing documents on-the-fly as their information interests change. We present a preliminary evaluation showing that uRank helps to reduce cognitive load compared to a traditional list-based representation.	living document;ranking (information retrieval);text mining;visual analytics	Cecilia di Sciascio;Vedran Sabol;Eduardo E. Veas	2015	2015 IEEE Conference on Visual Analytics Science and Technology (VAST)	10.1109/VAST.2015.7347686	analytics;visual analytics;web analytics;computer science;data mining;cultural analytics;world wide web;information retrieval;semantic analytics;design document listing	Visualization	-33.57203992837109	-55.273717990567235	67826
05d5a28fd29fdbd405743cd282888e463c8cb26a	overview of the 2003 kdd cup	concept drift;spam;class skew;imbalanced data;data streams;text classification;social network;challenge problems;knowledge discovery and data mining;data cleaning;cost sensitive learning	This paper surveys the 2003 KDD Cup, a competition held in conjunction with the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD) in August 2003. The competition focused on mining the complex real-life social network inherent in the e-print arXiv (arXiv.org). We describe the four KDD Cup tasks: citation prediction, download prediction, data cleaning, and an open task.	data mining;download;plasma cleaning;real life;sigkdd;social network	Johannes Gehrke;Paul Ginsparg;Jon M. Kleinberg	2003	SIGKDD Explorations	10.1145/980972.980992	spam;computer science;data science;concept drift;machine learning;data mining;data stream mining;world wide web;social network	ML	-20.508873189550304	-53.66721315482517	67909
9a726322b5df7200d99fa74c70012f1cc5e7109a	multiresolution web link analysis using generalized link relations	eigenvalues and eigenfunctions;graph theory;symmetric nonnegative matrix factorization;web pages;approximation method;in degree;web link analysis symmetric nonnegative matrix factorization pagerank salsa in degree;web link analysis;pagerank;symmetric matrices;link analysis;communities web pages matrix decomposition equations approximation methods symmetric matrices eigenvalues and eigenfunctions;matrix decomposition;nonnegative matrix factorization;web sites;salsa;web sites graph theory;approximation methods;communities;multiresolution popularity list multiresolution web link analysis pagerank relation hits salsa web page graph in degree relation	Web link analysis methods such as PageRank, HITS, and SALSA have focused on obtaining global popularity or authority of the set of Web pages in question. Although global popularity is useful for general queries, we find that global popularity is not as useful for queries in which the global population has less knowledge of. By examining the many different communities that appear within a Web page graph, we are able to compute the popularity or authority from a specific community. Multiresolution popularity lists allow us to observe the popularity of Web pages with respect to communities at different resolutions within the Web. Multiresolution popularity lists have been shown to have high potential when compared against PageRank. In this paper, we generalize the multiresolution popularity analysis to use any form of Web page link relations. We provide results for both the PageRank relations and the In-degree relations. By utilizing the multiresolution popularity lists, we achieve a 13 percent and 25 percent improvement in mean average precision over In-degree and PageRank, respectively.	hyperlink;information retrieval;link analysis;link relation;multiresolution analysis;pagerank;salsa;web page;world wide web	Laurence Anthony F. Park;Kotagiri Ramamohanarao	2011	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2011.107	combinatorics;link analysis;computer science;graph theory;theoretical computer science;web page;mathematics;matrix decomposition;world wide web;non-negative matrix factorization;symmetric matrix	Web+IR	-31.736488875215002	-54.87530071102399	68105
aa1e1fefa17c2731b81ec6dad85a0edf20693c74	continuous top-k processing of social network information streams: a vision		With the huge popularity of social networks, publishing and consuming content through information streams is nowadays at the heart of the new Web. Top-k queries over the streams of interest allow limiting results to relevant content, while continuous processing of such queries is the most effective approach in large scale systems. Current systems fail in combining continuous top-k processing with rich scoring models including social network criteria. We present in this paper our vision on the possible features of a social network of information streams, with a rich scoring model compatible with continuous top-k processing.	social network	Abdulhafiz Alkhouli;Dan Vodislav;Boris Borzic	2014		10.1007/978-3-319-38901-1_3	computer vision;machine learning;data mining	DB	-26.557775438355907	-53.347134809634255	68231
f0a49432ccec8689dd5dd77d98d5d07646c3c9f0	detecting link hijacking by web spammers	link hijacking;search engine;information retrieval;web spam;link analysis;large scale;ranking algorithm	Since current search engines employ link-based ranking algorithms as an important tool to decide a ranking of sites, web spammers are making a significant effort to manipulate the link structure of the Web, so called, link spamming. Link hijacking is an indispensable technique for link spamming to bring ranking scores from normal sites to target spam sites. In this paper, we propose a link analysis technique for finding link hijacked sites using modified PageRank algorithms. We performed experiments on the large scale Japanese Web archive and evaluated the accuracy of our method. Detection precision of our approach was improved about 25% from a baseline approach.	algorithm;baseline (configuration management);experiment;link analysis;page hijacking;pagerank;sensor;spamdexing;spamming;web archive;web search engine;world wide web	Young-joo Chung;Masashi Toyoda;Masaru Kitsuregawa	2009		10.1007/978-3-642-01307-2_32	link analysis;computer science;spamdexing;data mining;internet privacy;world wide web;link farm;search engine;page hijacking	Web+IR	-29.55703998263907	-55.42182906076748	68232
672d709139fd78b07918655606df6e2bbf912b0e	sentiment classification using word sub-sequences and dependency sub-trees	sentiment classification;analisis estadistico;analisis datos;text mining;frase;data mining;classification;data analysis;sentence;statistical analysis;polaridad;fouille donnee;decouverte connaissance;machine exemple support;syntactic relation;analyse statistique;descubrimiento conocimiento;analyse donnee;phrase;support vector machine;maquina ejemplo soporte;polarity;relation syntaxique;vector support machine;relacion sintactica;polarite;busca dato;clasificacion;knowledge discovery	Document sentiment classification is a task to classify a document according to the positive or negative polarity of its opinion (favorable or unfavorable). We propose using syntactic relations between words in sentences for document sentiment classification. Specifically, we use text mining techniques to extract frequent word sub-sequences and dependency sub-trees from sentences in a document dataset and use them as features of support vector machines. In experiments on movie review datasets, our classifiers obtained the best results yet published using these data.	experiment;support vector machine;text mining	Shotaro Matsumoto;Hiroya Takamura;Manabu Okumura	2005		10.1007/11430919_37	natural language processing;support vector machine;text mining;polarity;biological classification;computer science;pattern recognition;data mining;knowledge extraction;data analysis	NLP	-21.86088305985998	-64.22969395018434	68259
2ba9058cbaac414f693e5454f597f666b19a910d	concept identification using co-occurrence graph				Anoop Kumar Pandey	2018	IJWP	10.4018/IJWP.2018010103	knowledge management;data mining;co-occurrence;computer science;graph	NLP	-24.488642376708636	-61.04386963244523	68284
ac9f3a017d92ab15078d541155dace7f076c60cc	a semantic clustering approach for indexing documents		Information retrieval (IR) models process documents for preparing them for search by humans or computers. In the early models, the general idea was making a lexico-syntactic processing of documents, where the importance of the documents retrieved by a query is based on the frequency of its terms in the document. Another approach is return predefined documents based on the type of query the user make. Recently, some researchers have combined text mining techniques to enhance the document retrieval. This paper proposes a semantic clustering approach to improve traditional information retrieval models by representing topics associated to documents. This proposal combines text mining algorithms and natural language processing. The approach does not use a priori queries, instead clusters terms, where each cluster is a set of related words according to the content of documents. As result, a document-topic matrix representation is obtained denoting the importance of topics inside documents. For query processing, each query is represented as a set of clusters considering its terms. Thus, a similarity measure (e.g. cosine similarity) can be applied over this array and the matrix of documents to retrieve the most relevant documents.	algorithm;brill tagger;cluster analysis;computer;computer cluster;cosine similarity;database;document retrieval;experiment;information retrieval;lexico;matrix representation;natural language processing;part-of-speech tagging;similarity measure;text mining;the matrix;web search engine	Daniel Osuna-Ontiveros;Ivan López-Arévalo;Víctor Jesús Sosa Sosa	2011			computer science;data mining;search engine indexing;cluster analysis	Web+IR	-28.54304165519641	-63.695134765809804	68324
20f3866d200a7329ac46e19a82c614708404335e	using agreementmaker to align ontologies for oaei 2010	semantic web;data integration;ontology matching	The AgreementMaker system is unique in that it features a powerful user interface, a flexible and extensible architecture, an integrated evaluation engine that relies on inherent quality measures, and semi-automatic and automatic methods. This paper describes the participation of AgreementMaker in the 2010 OAEI competition in three tracks: benchmarks, anatomy, and conference. After its successful participation in 2009, where it ranked first in the conference track, second in the anatomy track, and obtained good results in the benchmarks track, the goal in this year’s participation is to increase the values of precision, recall, and F-measure for each of those tracks. 1 Presentation of the system We have been developing the AgreementMaker system since 2001, with a focus on real-world applications [5, 8] and in particular on geospatial applications [4, 6, 7, 9–13]. However, the current version of AgreementMaker, whose development started two years ago, represents a whole new effort. 1.1 State, purpose, general statement The new AgreementMaker system [1–3] supports: (1) user requirements, as expressed by domain experts; (2) a wide range of input (ontology) and output (agreement file) formats; (3) a large choice of matching methods depending, on the different granularity of the set of components being matched (local vs. global), on different features considered in the comparison (conceptual vs. structural), on the amount of intervention that they require from users (manual vs. automatic), on usage (standalone vs. composed), and on the types of components to consider (schema only or schema and instances); (4) improved performance, that is, accuracy (precision, recall, F-measure) and efficiency (execution time) for the automatic methods; (5) an extensible architecture to incorporate new methods easily and to tune their performance; (6) the capability to evaluate, compare, and combine different strategies and matching results; (7) a comprehensive user interface that supports advanced visualization techniques and a control panel that ? Research supported by NSF Awards IIS-0513553 and IIS-0812258. ?? Additional affiliation: University of Milan-Bicocca, Italy. drives all the matching methods and evaluation strategies; (8) a feedback loop that accepts suggestions and corrections by users and extrapolates new mappings. In 2009 AgreementMaker was very successful in the OAEI competition. In particular, AgreementMaker ranked (a close) second among ten systems in the anatomy track. AgreementMaker also participated successfully in two other tracks: benchmarks and conference. In the former track, AgreementMaker was ranked first in terms of precision and seventh in terms of recall among thirteen systems and in the latter track AgreementMaker was ranked first with the highest F-measure (57% at a threshold of 75%) among seven competing systems. 1.2 Specific techniques used AgreementMaker comprises several matching algorithms or matchers that can be used for matching (or aligning) the source and target ontologies. The matchers are not restricted to any particular domain. The architecture of AgreementMaker relies on a stack of matchers that belong to three different layers (see Figure 1). Specific configurations of the stack have been used for the benchmarks, anatomy, and conference tracks, as discussed in what follows. However, we describe first the different components in the stack: the matchers, the combination and evaluation modules, and the final alignment module. Fig. 1. AgreementMaker OAEI 2010 matcher stack. Matchers can be concept-based (if they consider only one concept) or structural (if they consider a subgraph of the ontology). The concept-based matchers support the comparison of strings. They include: the Base Similarity Matcher (BSM) [7], the Parametric String-based Matcher (PSM) [2] and the Vector-based Multi-Word Matcher (VMM) [2]. BSM is a basic string matcher that computes the similarity between concepts by comparing all the strings associated with them. PSM is a more in-depth string matcher, which for the competition is set to use a substring measure and an edit distance measure. VMM compiles a virtual document for every concept of an ontology, transforms the resulting strings into TF-IDF vectors and then computes their similarity using the cosine similarity measure. These matchers have been extended in the AgreementMaker configuration used this year by plugging in a set of lexicons, which are used to expand the set of strings with synonyms. The extended matchers are therefore called BSM, PSM, and VMM. The Advanced Similarity Matcher (ASM) is a string-based matcher that computes mappings between source and target concepts (including their properties) by comparing their local names, and providing better similarity evaluation in particular when compound terms are used. ASM outperforms generic string-based similarity matchers because it is based on a deeper linguistic analysis. Structural matchers include the Descendants’ Similarity Inheritance (DSI) matcher [7]. This matcher is based on the idea that if two nodes are similar, then their descendants should be similar. The Group Finder Matcher (GFM) is another structural matcher that filters out the mappings provided by another matcher (the input matcher). It identifies groups of concepts and properties in the ontologies and assumes that two concepts (or properties) that belong to two groups that were not mapped by the input matcher will likely have different meanings and should not be mapped. The Iterative Instance Structural Matcher (IISM) takes into account instances. Classes that have mapped individuals can then be aligned. In addition, values of the properties are also considered. The structural part of IISM is quite complex and takes into account superclasses, subclasses, properties, subproperties, cardinalities, and the range and domain of properties. The combination and evaluation modules are used together, as follows. The Linear Weighted Combination (LWC) [2] combines its inputs (e.g., from several string matchers), using a local confidence quality measure provided by the evaluation module, in order to automatically assign weights to each result computed by the input matchers. After this step, we have a single combined set of alignments that includes the best alignments from each of the input matchers. The final alignment module is given as input a mapping cardinality (e.g., 1:1) and a threshold and outputs the best set of alignments given those two inputs [2]. Benchmarks For the benchmarks track we used the following configuration: IISM( LWC(ASM,PSM,VMM,BSM) ) LWC is adopted to combine the results of four string-based matchers, namely ASM, PSM, PSM, and BSM; the last three make use of two lexicons, namely WordNet and a dictionary built from the ontologies; the similarity values computed at this step are then given as input to the IISM structural matcher. Anatomy For the anatomy track we used the following configuration: LWC(PSM,VMM,BSM) LWC is adopted to combine the results of four string-based matchers, namely PSM, VMM, and BSM; the last three make use of two lexicons, namely WordNet and a dictionary built from the ontologies. Conference For the conference track we used the following configuration:	1:1 pixel mapping;algorithm;align (company);benchmark (computing);black–scholes model;catia;cosine similarity;dictionary;edit distance;emoticon;extensibility;f1 score;feedback;ibm notes;lexicon;ontology (information science);openbsm;plugboard;requirement;run time (program lifecycle phase);semiconductor industry;similarity measure;substring;tf–idf;user interface;user requirements document;virtual machine manager;wordnet	Isabel F. Cruz;Cosmin Stroe;Federico Caimi;Alessio Fabiani;Catia Pesquita;Francisco M. Couto;Matteo Palmonari	2010			computer science;artificial intelligence;data integration;semantic web;data mining;database;world wide web;information retrieval	DB	-32.241235033087726	-65.57757567283869	68349
8941c929571ace512189ade2de668ba5b3aca67b	how to find valuable references? application of text mining in abstract clustering		The emergence of text mining has enabled firms to understand the requirement of consumers in real-time by mining publicly available information from Internet. More and more researchers pay more attention to improving the methods of text mining and the role of text mining has become increasingly prominent. This paper introduces the use of text mining in article screening and classification to help graduate students select more valuable articles and quickly have a clear understand of research status and content. Using data from a popular Chinese database CKNI, we find that thousands of articles appear when we use keywords to search in the database and these articles are complex and disorganized. Our study provide a method to screen and filter these articles, and extract topics for each class. Our study has a great significance to the improvement of graduate students' research ability in a short time.	chaos theory;cluster analysis;emergence;real-time transcription;text mining	Yanfang Zhang;Yan Wan	2017	2017 13th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)	10.1109/FSKD.2017.8393112	computer science;data mining;artificial intelligence;machine learning;the internet;text mining;cluster analysis	ML	-21.25834512379166	-52.65889683494715	68415
51f8829d88eef79f58dc6816343c1771d59893ae	reducing the uncertainty in resource selection	resource discovery;distributed information retrieval;uncertainty in search;information seeking behaviour	The distributed retrieval process is plagued by uncertainty. Sampling, selection, merging and ranking are all based on very limited information compared to centralized retrieval. In this paper, we focus our attention on reducing the uncertainty within the resource selection phase by obtaining a number of estimates, rather than relying upon only one point estimate. We propose three methods for reducing uncertainty which are compared against state-of-the-art baselines across three distributed retrieval testbeds. Our results show that the proposed methods significantly improve baselines, reduce the uncertainty and improve robustness of resource selection.	baseline (configuration management);centralized computing;gibbs sampling;list of code lyoko episodes	Ilya Markov;Leif Azzopardi;Fabio Crestani	2013		10.1007/978-3-642-36973-5_43	computer science;machine learning;data mining;information retrieval	Web+IR	-29.747368591913695	-60.32283472133925	68447
82e2669a075a9acd70bdbdf6b52994e2bae593f2	chinese semantic parsing based on dependency graph and feature structure	graph theory;natural language processing feature extraction grammars graph theory;dependence graph;structuration theory;semantics labeling syntactics pragmatics grammar educational institutions tagging;large scale;grammars;semantic parsing feature structure semantic labeling dependency structure dependency graph;feature extraction;semantic description;chinese sentence chinese semantic parsing labeling corpus large scale semantic description dependency graph theory feature structure theory chinese phrase;dependence structure;semantic relations;natural language processing	Labeling corpus with Feature Structure is a new attempt to incorporate large-scale semantic descriptions and semantic parsing. In this paper, we propose Dependency Graph Theory and Feature Structure Theory to resolve the semantic parsing of Chinese phrases and sentences, and have built a large-scale Chinese labeling corpus using Feature Structure. Feature Structure can represent the semantic relations of words in Chinese sentences more clearly and accurately.	graph theory;machine translation;parsing;relationship extraction	Bo Chen;Dong-Hong Ji	2011	Proceedings of 2011 International Conference on Electronic & Mechanical Engineering and Information Technology	10.1109/EMEIT.2011.6023005	natural language processing;semantic role labeling;semantic similarity;semantic computing;feature extraction;computer science;graph theory;s-attributed grammar;pattern recognition;semantic compression;graph database	NLP	-25.00132704567087	-65.47293505035826	68485
a77e72129479c82859d13739f590af4dd1429ffa	csku gprf-qe for medical topic web retrieval		Patients and their relatives have more chances to access their healthinformation in a form of discharge summary. Most of them do not totally understand contents in the discharge summary. The ShARe/CLEF eHealth Evaluation Lab organized a shared task for improving retrieval medical information from the web. Queries of this task are formulated based on information in discharge summaries. This paper investigates efficiency of query expansion using external collection. Co-occur terms in pseudo-relevance feedback of Genomics collection are selected and re-weighted based on Rocchio’s formula with dynamic tunable parameters of pseudo-relevance part. LUCENE, vector space model, is baseline retrieval tool. The proposed expansion method improves from baseline in all level cut of nDCG and best perform in P@10 of 3 topics. Using biomedical related collection such as Genomics is useful for medical topics retrieval.	baseline (configuration management);discharger;performance prediction;query expansion;relevance feedback;web page	Ornuma Thesprasith;Chuleerat Jaruskulchai	2014			learning to rank;web intelligence;query expansion;image retrieval;human–computer information retrieval;information retrieval;vector space model;document clustering;clef;computer science	Web+IR	-31.44911058257248	-62.8379624717112	68526
14b598682213db4fb102b1714feb88c42af20a3b	a method of geographical name extraction from japanese text for thematic geographical search	search method;conceptual neighbourhoods;spatiotemporal reasoning;spatiotemporal relations;text retrieval	A text retrieval method called the thematic geographical search method has been developed and applied to a Japanese encyclopedia called the World Encyclopædia. In this method, the user specifies a search theme using free words, then obtains a sorted list of excerpts and hyperlinks to encyclopedia sentences that contain geographical names. Using this list, the user can also open maps that indicate the locations of the names. To generate an index of names for this searching, a method of extracting geographical names has been developed. In this method, geographical names are extracted, matched to names in a geographical name database, and identified. Geographical names, however, often have several types of ambiguities. Ambiguities are resolved by using non-local context analysis, which uses a stack and several other techniques. As a result, the precision of extracted names is more than 96% on average. This method depends on features of the Japanese language, but the strategy and most of the techniques can be applied to texts in English or other languages.	document retrieval;hyperlink;map;precision and recall;sorting algorithm;www	Yasusi Kanada	1999		10.1145/319950.323229	computer science;data mining;database;information retrieval	Web+IR	-30.423362658370067	-60.278079478940995	68568
5a68e8fc156f551363780f92b93c89355ecf23be	indexing of reading paths for a structured information retrieval on the web	atomic measurements;web pages;reading path;information retrieval system;internet indexing information retrieval;information retrieval;web;indexing information retrieval html intelligent agent search engines intelligent structures content based retrieval context modeling web search web pages;html;indexes;internet;indexing;indexing process reading paths world wide web hyperdocument model nonlinear reading hypertext structure structured information retrieval system querying web site;indexation;web sites;indexing structure web reading path information retrieval;point of view;context modeling;structure	In this paper, we present a hyperdocument model taking into account the essential aspects of information on the Web: content, composition (logical structure) and non-linear reading (hypertext structure). We have developed a Structured Information Retrieval System (SIRS) based on this model. Its phases of indexing and querying are based on a “reading paths” point of view of the Web: a Web site is considered as a set of potential reading paths, instead of a set of atomic and flat pages. We have developed an specific algorithm to index the reading paths. We present some experiments aiming at evaluating the interest of our indexing process of reading paths.	algorithm;experiment;hypertext;information retrieval;nonlinear system;reading path;world wide web	Mathias Géry	2008	2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology	10.1109/WIIAT.2008.386	database index;structure;search engine indexing;web modeling;the internet;html;computer science;web page;database;context model;world wide web;information retrieval	DB	-29.71623472922335	-56.923670948468505	68604
52b626880dd3bf7220d1fcc1b0c355c49a5eef62	extracting sentiment knowledge from pros/cons product reviews: discovering features along with the polarity strength of their associated opinions		Abstract Sentiment knowledge extraction is a growing area of research in the literature. It helps in analyzing users’ opinions about different entities or events, which can then be utilized by analysts for various purposes. Particularly, feature-based sentiment analysis is one of the challenging research areas that analyzes users’ opinions on various features of a product or service. Of the three formats for the product reviews, our focus in this paper is limited to analyzing the pros/cons type. Due to the nature of pros/cons reviews, they are mostly concise and follow a different structure from other review types. Therefore, specialized techniques are needed to analyze these reviews and extract the customers’ discussed product features along with their personal attitudes. In this paper, we propose the Pros/Cons Sentiment Analyzer (PCSA) framework that exploits dependency relations in extracting sentiment knowledge from pros/cons reviews. We also utilize two different lexicons to ascertain the polarity strength of the extracted features based on the customers’ opinions. Several experiments are conducted to evaluate the performance of PCSA in its different phases.		Monireh Alsadat Mirtalaie;Omar Khadeer Hussain;Elizabeth Chang;Farookh Khadeer Hussain	2018	Expert Syst. Appl.	10.1016/j.eswa.2018.07.046	data mining;knowledge extraction;sentiment analysis;cons;computer science	NLP	-22.49762155574489	-57.289046056590216	68822
5507306380a2fd3fd3294d4338d7f21e814fb166	cross-language retrieval for the clef collections - comparing multiple methods of retrieval	thesaurus;information retrieval;recherche information;tesaurus;recuperacion informacion;multilinguisme;query expansion;multilingualism;multilinguismo	For our participation in CLEF, the Berkeley group participated in the monolingual, multilingual and GIRT tasks. To help enrich the CLEF relevance set for future training, we prepared a manual reformulation of the original German queries which achieved excellent performance, more than 110% better than average of median precision. The GIRT task performed EnglishGerman Cross-Language IR by comparing commercial machine translation with thesaurus lookup techniques and query expansion techniques. Combining all techniques using simple data fusion produced the best results.	arbitrary-precision arithmetic;information and computer science;information retrieval;information science;lookup table;machine translation;map;query expansion;relevance;stemming;thesaurus	Fredric C. Gey;Hailing Jiang;Vivien Petras;Aitao Chen	2000		10.1007/3-540-44645-1_11	natural language processing;query expansion;computer science;database;information retrieval	Web+IR	-33.057210436234875	-64.23532542604516	68995
1587c9170ebc36369667c840ea112b1834f451e7	the research of document clustering topical concept based on neural networks		Nowadays, document clustering technology has been extensively used in text mining, information retrieval systems and etc. The input of network is the key problem for topical concept utilizing the Neural Network. This paper presents an input model of Neural Network that calculates the Mutual Information between contextual words and ambiguous word by using statistical method and taking the contextual words to certain number beside the topical concept according to (-M, +N). In this paper, we introduce a novel topical document clustering method called Document Characters Indexing Clustering (DCIC), which can identify topics accurately and cluster documents according to these topics. In DCIC, “topic elements” are defined and extracted for indexing base clusters. Additionally, document characters are investigated and exploited. Experimental results show that DCIC based on BP Neural Networks models can gain a higher precision (92.76%) than some widely used traditional clustering methods.	neural networks	Xian Fu;Yi Ding	2014		10.1007/978-3-319-12436-0_69	document clustering;data science;data mining;information retrieval	NLP	-27.049606587170206	-60.113459393689574	69013
5d652a7a1533e5b0ce310248e6b7cf49eb95ff85	cross-media retrieval using query dependent search methods	multimedia retrieval;search method;multimedia document;learning methods;content based multimedia retrieval;indexation;multimedia data;query expansion;relevance feedback	The content-based cross-media retrieval is a new type of multimedia retrieval in which the media types of query examples and the returned results can be different. In order to learn the semantic correlations among multimedia objects of different modalities, the heterogeneous multimedia objects are analyzed in the form of multimedia document (MMD), which is a set of multimedia objects that are of different media types but carry the same semantics. We first construct an MMD semi-semantic graph (MMDSSG) by jointly analyzing the heterogeneous multimedia data. After that, cross-media indexing space (CMIS) is constructed. For each query, the optimal dimension of CMIS is automatically determined and the cross-media retrieval is performed on a per-query basis. By doing this, the most appropriate retrieval approach for each query is selected, i.e. different search methods are used for different queries. The query dependent search methods make cross-media retrieval performance not only accurate but also stable. We also propose different learning methods of relevance feedback (RF) to improve the performance. Experiment is encouraging and validates the proposed methods. & 2010 Elsevier Ltd. All rights reserved.	algorithm;content-based image retrieval;feature vector;keyboard shortcut;mikumikudance;online and offline;query expansion;radio frequency;relevance feedback;semiconductor industry;whole earth 'lectronic link	Yi Yang;Fei Wu;Dong Xu;Yueting Zhuang;Liang-Tien Chia	2010	Pattern Recognition	10.1016/j.patcog.2010.02.015	query expansion;computer science;concept search;database;multimedia;web search query;information retrieval;query language	Web+IR	-28.344148936363613	-52.51715448375356	69132
1e8cbddcf0da2959a72b80688f358a0a3a509683	persian sentiment analyzer: a framework based on a novel feature selection method	sentiment classification;persian language;naive bayes algorithm;sentiment analysis;feature selection;natural language processing	In the recent decade, with the enormous growth of digital content in internet and databases, sentiment analysis has received more and more attention between information retrieval and natural language processing researchers. Sentiment analysis aims to use automated tools to detect subjective information from reviews. One of the main challenges in sentiment analysis is feature selection. Feature selection is widely used as the first stage of analysis and classification tasks to reduce the dimension of problem, and improve speed by the elimination of irrelevant and redundant features. Up to now as there are few researches conducted on feature selection in sentiment analysis, there are very rare works for Persian sentiment analysis. This paper considers the problem of sentiment classification using different feature selection methods for online customer reviews in Persian language. Three of the challenges of Persian text are using of a wide variety of declensional suffixes, different word spacing and many informal or colloquial words. In this paper we study these challenges by proposing a model for sentiment classification of Persian review documents. The proposed model is based on lemmatization and feature selection and is employed Naive Bayes algorithm for classification. We evaluate the performance of the model on a manually gathered collection of cellphone reviews, where the results show the effectiveness of the proposed approaches.	algorithm;alloy analyzer;artificial intelligence;data mining;database;digital recording;direction finding;f1 score;feature selection;information retrieval;lemmatisation;mobile phone;naive bayes classifier;natural language processing;open-source software;relevance;sentiment analysis	Ayoub Bagheri;Mohammad Saraee	2014	CoRR		natural language processing;naive bayes classifier;speech recognition;computer science;machine learning;pattern recognition;data mining;feature selection;sentiment analysis	AI	-21.324607108862693	-61.21905432410462	69223
eb7fa0c7440f43bd8a938449abd2b84c4e867e76	opinionit: a text mining system for cross-lingual opinion analysis	opinion mining;text mining;product feature categorization;cross lingual opinion mining;aspect oriented;latent semantic association;cross cultural differences;semantic association	Opinion mining focuses on extracting customers' opinions from the reviews and predicting their sentiment orientation. Reviewers usually praise a product in some aspects and bemoan it in other aspects. With the business globalization, it is very important for enterprises to extract the opinions toward different aspects and find out cross-lingual/cross-culture difference in opinions. Cross-lingual opinion mining is a very challenging task as amounts of opinions are written in different languages, and not well structured. Since people usually use different words to describe the same aspect in the reviews, product-feature (PF) categorization becomes very critical in cross-lingual opinion mining. Manual cross-lingual PF categorization is time consuming, and practically infeasible for the massive amount of data written in different languages. In order to effectively find out cross-lingual difference in opinions, we present an aspect-oriented opinion mining method with Cross-lingual Latent Semantic Association (CLaSA). We first construct CLaSA model to learn the cross-lingual latent semantic association among all the PFs from multi-dimension semantic clues in the review corpus. Then we employ CLaSA model to categorize all the multilingual PFs into semantic aspects, and summarize cross-lingual difference in opinions towards different aspects. Experimental results show that our method achieves better performance compared with the existing approaches. With CLaSA model, our text mining system OpinionIt can effectively discover cross-lingual difference in opinions.	aspect-oriented software development;categorization;sentiment analysis;text mining	Honglei Guo;Huijia Zhu;Zhili Guo;Xiaoxun Zhang;Zhong Su	2010		10.1145/1871437.1871589	natural language processing;text mining;aspect-oriented programming;computer science;data science;data mining;database;world wide web;information retrieval	NLP	-23.72722013530675	-66.1013251851459	69263
cee8b0127ef75fba6df597886bdf52465491c3d2	predicting failing queries in video search	video retrieval human computer interaction internet search engines text analysis ubiquitous computing user interfaces;visual consistency query failure query performance prediction transaction log analysis video search;textual metadata failing query prediction video search query context aware query failure prediction approach query performance prediction text based web search query retrieval performance estimation user indicators engine indicators transaction logs user interactions video search engine visual search consistency measurement;visualization search engines semantics engines context optimization web search	The ability to predict when a video search query is not likely to deliver satisfying search results is expected to enable more effective search results optimizations and improved search experience for users. In this paper, we propose a novel context-aware query failure prediction approach that predicts whether a particular query submitted in a user's search session is likely to fail. The approach builds on the well-known concept of query performance prediction introduced in conventional text-based Web search to estimate the query's retrieval performance, but extends this concept with two novel characteristics, user indicators and engine indicators. User indicators are derived from transaction logs, capture the patterns of user interactions with the video search engine, and exploit the context in which a particular query was submitted. Engine indicators are derived from the search results list and measure the consistency of visual search results at the level of visual concepts and textual metadata associated with videos. Extensive evaluation of the approach on a test set containing over one million video search queries shows its effectiveness and demonstrates a significant improvement over traditional and state-of-the-art baseline approaches.	algorithm;baseline (configuration management);experiment;failure;high- and low-level;interaction;key frame;local search (optimization);mathematical optimization;microsoft outlook for mac;multimodal interaction;online and offline;online search;performance prediction;protein structure prediction;search engine optimization;session (web analytics);test set;text-based (computing);thumbnail;transaction log;web search engine;web search query;whole earth 'lectronic link	Christoph Kofler;Linjun Yang;Martha Larson;Tao Mei;Alan Hanjalic;Shipeng Li	2014	IEEE Transactions on Multimedia	10.1109/TMM.2014.2347937	search-oriented architecture;sargable;search engine indexing;full text search;query optimization;query expansion;web query classification;ranking;computer science;concept search;database;search analytics;web search query;world wide web;information retrieval;query language;search engine	Web+IR	-33.22386717256789	-52.63356987185356	69456
b4a36e9468502053269d8f7798da2d75c0556237	sentiment mining in webfountain	opinion mining;web pages;application software;information retrieval;feeds;text analysis;web services algorithm design and analysis application software information analysis large scale systems feeds performance analysis data mining natural language processing web pages;natural languages;web service;data mining;corpus level miner;reputation management sentiment mining webfountain text analytics application document level miner corpus level miner web service end user application natural language processing online product review article;large scale;internet;document level miner;web services;performance analysis;information retrieval data mining internet text analysis natural languages;reputation management;information analysis;webfountain;natural language processing;algorithm design and analysis;text analytics application;sentiment mining;online product review article;large scale systems;end user application	WebFountain is a platform for very large-scale text analytics applications that allows uniform access to a wide variety of sources. It enables the deployment of a variety of document-level and corpus-level miners in a scalable manner, and feeds information that drives end-user applications through a set of hosted Web services. Sentiment (or opinion) mining is one of the most useful analyses for various end-user applications, such as reputation management. Instead of classifying the sentiment of an entire document about a subject, our sentiment miner determines sentiment of each subject reference using natural language processing techniques. In this paper, we describe the fully functional system environment and the algorithms, and report the performance of the sentiment miner. The performance of the algorithms was verified on online product review articles, and more general documents including Web pages and news articles.	algorithm;environment variable;ibm webfountain;natural language processing;reputation management;scalability;sentiment analysis;software deployment;text mining;web page;web service	Jeonghee Yi;Wayne Niblack	2005	21st International Conference on Data Engineering (ICDE'05)	10.1109/ICDE.2005.132	web service;text mining;computer science;data science;data mining;database;world wide web;sentiment analysis	DB	-29.681653998173594	-57.344291021835375	69500
6e486480d25fd6994204ad56d8f1c109c6522fdf	a text classifier based on sentence category vsm	conference paper	VSM is a mature model of text representation for categorization. Words are commonly used as dimensions of feature space of VSM, but words only provide little semantic information. Sentence category theory is an important component of HNC theory and can provide abundant information about meaning, structure and style of a sentence. We use sentence categories as dimensions of feature space, reduce the dimensionality by dividing mixed sentence categories and reform the weights by tfc-weighting algorithm. By simple vector distance calculation, we can get the parameters of the classifier and execute the categorization. The average precision and recall of our classifier are acceptable and can be improved by other HNC techniques.	algorithm;categorization;category theory;comparison of raster-to-vector conversion software;domain analysis;feature vector;information retrieval;precision and recall;viable system model	Yunliang Zhang;Quan Zhang	2006			natural language processing;speech recognition;linguistics	Web+IR	-22.86927111077179	-65.92992197067151	69614
87bd04a6c4a1c2538c818b0457e4ae0432134bc8	utilizing inter-passage and inter-document similarities for re-ranking search results	centrality;passages;model integration;empirical evaluation;re ranking;language model;document similarity;language models	We present a novel language-model-based approach to re-ranking an initially retrieved list so as to improve precision at top ranks. Our model integrates whole-document information with that induced from passages. Specifically, inter-passage, inter-document, and query-based similarities are integrated in our model. Empirical evaluation demonstrates the effectiveness of our approach.	language model;semantic similarity	Eyal Krikon;Oren Kurland;Michael Bendersky	2009		10.1145/1645953.1646181	natural language processing;computer science;data mining;centrality;information retrieval;language model	SE	-28.205783914587542	-62.248193600073385	69727
3a2e446a037240fac23c94bf6a6066e489138a12	identifying the original contribution of a document via language modeling	language modeling;topic modeling;original contributions	One goal of text mining is to provide readers with automatic methods for quickly finding the key ideas in individual documents and whole corpora. To this effect, we propose a statistically well-founded method for identifying the original ideas that a document contributes to a corpus, focusing on self-referential diachronic corpora such as research publications, blogs, email, and news articles. Our statistical model of passage impact defines (interesting) original content through a combination of impact and novelty, and it can be used to identify the most original passages in a document. Unlike heuristic approaches, this statistical model is extensible and open to analysis. We evaluate the approach on both synthetic and real data, showing that the passage impact model outperforms a heuristic baseline method.	baseline (configuration management);blog;email;heuristic;language model;self-reference;statistical model;synthetic intelligence;text corpus;text mining;user-generated content	Benyah Shaparenko;Thorsten Joachims	2009		10.1145/1571941.1572083	natural language processing;computer science;machine learning;data mining;topic model;world wide web;information retrieval;language model	Web+IR	-25.215427358731286	-59.16861592596882	69731
3474111c7650b200412c397d2401cc2b3ea0e2c2	finding and classifying web units in websites	web classification;web pages;data mining;web units;internet;web unit mining;web unit;world wide web;websites	In Web classification, most researcher assume that the objects to be classified are individual Web pages from one or more Web sites. In practice, the assumption is too restrictive since a Web page itself may not carry sufficient information for it to be treated as an instance of some semantic class or concept. In this paper, we relax this assumption and allow a subgraph of Web pages to represent an instance of semantic concept. Such a subgraph of Web pages is known as a Web unit. To construct and classify Web units, we formulate the Web unit mining problem and propose an iterative Web unit mining (iWUM) method. The iWUM method first finds subgraphs of Web pages using knowledge about Web site structure and connectivity among the Web pages. From these Web subgraphs, Web units are constructed and classified into categories in an iterative manner. Our experiments using the WebKB dataset showed that iWUM was able to construct Web units and classify Web units with high accuracy for the more structured parts of a Web site.	algorithm;baseline (configuration management);experiment;feature selection;horner's method;iteration;iterative method;monte carlo method;organizing (structure);statistical classification;topological index;web page;world wide web	Aixin Sun;Ee-Peng Lim	2005	IJBIDM	10.1504/IJBIDM.2005.008361	web service;web application security;web mining;static web page;web development;web modeling;the internet;data web;web mapping;web design;web standards;computer science;data science;semantic web;web navigation;social semantic web;web page;data mining;semantic web stack;database;web intelligence;web 2.0;world wide web;website parse template	Web+IR	-24.89608627979048	-62.99450582926011	69749
7df26a85e5f71ac226433732e197b317b293ce9b	medical image retrieval in healthcare social networks		In﻿this﻿article,﻿the﻿authors﻿present﻿a﻿multimodal﻿research﻿model﻿to﻿research﻿medical﻿images﻿based﻿on﻿ multimedia﻿information﻿that﻿is﻿extracted﻿from﻿a﻿radiological﻿collaborative﻿social﻿network.﻿The﻿opinions﻿ shared﻿on﻿a﻿medical﻿image﻿in﻿a﻿medico-social﻿network﻿is﻿a﻿textual﻿description﻿which﻿in﻿most﻿cases﻿ requires﻿cleaning﻿by﻿using﻿a﻿medical﻿thesaurus.﻿In﻿addition,﻿they﻿describe﻿the﻿textual﻿description﻿and﻿ medical﻿image﻿in﻿a﻿TF-IDF﻿weight﻿vector﻿using﻿a﻿“bag-of-words”﻿approach.﻿The﻿authors﻿then﻿use﻿ latent﻿semantic﻿analysis﻿to﻿establish﻿relationships﻿between﻿textual﻿terms﻿and﻿visual﻿terms﻿in﻿shared﻿ opinions﻿on﻿the﻿medical﻿image.﻿The﻿model﻿is﻿evaluated﻿against﻿the﻿ImageCLEFmedbaseline,﻿which﻿ is﻿ the﻿ground﻿truth﻿for﻿the﻿experiments.﻿The﻿authors﻿have﻿conducted﻿numerous﻿experiments﻿with﻿ different﻿descriptors﻿and﻿many﻿combinations﻿of﻿modalities.﻿The﻿analysis﻿of﻿results﻿shows﻿that﻿when﻿ the﻿model﻿is﻿based﻿on﻿two﻿methods﻿it﻿can﻿increase﻿the﻿performance﻿of﻿a﻿research﻿system﻿based﻿on﻿a﻿ single﻿modality﻿both﻿visually﻿or﻿textually. KeywoRdS Bag-of-Word, Latent Semantic Analysis, Medical Image Retrieval, Medical Social Network, Multimodal Fusion	image retrieval;latent semantic analysis;multimodal interaction;social network	Riadh Bouslimi;Mouhamed Gaith Ayadi;Jalel Akaichi	2018	IJHISI	10.4018/IJHISI.2018040102	modalities;data mining;image retrieval;health care;ground truth;latent semantic analysis;social network;medicine	Vision	-24.941270284131978	-60.786306442047135	69783
060e06b1b7a14b30820c23b6be5d6d6bd5db0e70	using the revised em algorithm to remove noisy data for improving the one-against-the-rest method in binary text classification	extraction information;categorisation;information extraction;noisy data;the one against the rest method;binary text classification;experimental result;text classification;categorizacion;multi class classification;the sliding window technique;expectation maximization algorithm;resultado experimental;the em algorithm;algorithme em;binary classification;algoritmo em;classification automatique;resultat experimental;automatic classification;em algorithm;clasificacion automatica;extraccion informacion;sliding window;categorization	Automatic text classification is the problem of automatically assigning predefined categories to free text documents, thus allowing for less manual labors required by traditional classification methods. When we apply binary classification to multi-class classification for text classification, we usually use the one-against-the-rest method. In this method, if a document belongs to a particular category, the document is regarded as a positive example of that category; otherwise, the document is regarded as a negative example. Finally, each category has a positive data set and a negative data set. But, this one-against-the-rest method has a problem. That is, the documents of a negative data set are not labeled manually, while those of a positive set are labeled by human. Therefore, the negative data set probably includes a lot of noisy data. In this paper, we propose that the sliding window technique and the revised EM (Expectation Maximization) algorithm are applied to binary text classification for solving this problem. As a result, we can improve binary text classification through extracting potentially noisy documents from the negative data set using the sliding window technique and removing actually noisy documents using the revised EM algorithm. The results of our experiments showed that our method achieved better performance than the original one-against-the-rest method in all the data sets and all the classifiers used in the experiments.	binary classification;binary data;categorization;document classification;expectation–maximization algorithm;experiment;multiclass classification;performance;signal-to-noise ratio;text-based user interface	Hyoungdong Han;Youngjoong Ko;Jungyun Seo	2007	Inf. Process. Manage.	10.1016/j.ipm.2006.11.003	expectation–maximization algorithm;computer science;machine learning;pattern recognition;data mining;information extraction;one-class classification;statistics	Web+IR	-21.613211124121005	-64.19089890131424	69907
4a65c273f735dbe36fa348d11fcdfb8e362199e3	feature selection on heterogeneous graph	feature selection;heterogeneous graph mining	Heterogeneous graph based information recommendation have been proved useful in recent studies. Given a heterogeneous graph scheme, there are many possible meta paths between the query node and the result node, and each meta path addresses a hypothesis-based ranking function. In prior researches, meta paths are manually selected by domain experts. However, when the graph scheme becomes complex, this method can be inefficient. In this study, we propose feature generation tree, a novel feature selection method for heterogeneous graph mining based recommendation algorithms, which adds graph structure information into the original “feature selection for ranking” algorithm and saves a fair amount of time for feature computation. In our preliminary experiment, the proposed method outperforms the original “feature selection for ranking” algorithm in both efficiency and effectiveness.	algorithm;baseline (configuration management);collaborative filtering;computation;feature selection;ground truth;learning to rank;naivety;ranking (information retrieval);structure mining	Chun Guo;Xiaozhong Liu	2015		10.1002/pra2.2015.1450520100119	null model;computer science;machine learning;pattern recognition;data mining;moral graph;graph database	ML	-26.92785271660718	-62.28732557660955	70010
285465771033726a0531011ce2104431aefc6cde	evaluating and enhancing meta-search performance in digital libraries	metasearch software libraries query processing delay computer science context aware services search engines prototypes;duplicate detection;query processing;digital library;digital libraries;satisfiability;internet;digital libraries query processing internet;web server performance;simulation tool;user satisfaction;web server meta search system performance digital libraries information splitting strategies literature services personalization query processing retrieval simulation tool simpson response times metrics user satisfaction duplicate detection query specification	Applying meta search systems is a suitable method to support the user if there are many different retrieval services available in the Web. Due to information splitting strategies of literature services existing meta search systems either provide minimal integration of results or slow response times. In this paper, we present an approach that combines techniques of personalization and query processing in order to satisfy the user ́s demand for both fast and comprehensive results. In order to evaluate and compare different query processing strategies and additional influencing parameters we developed a simulation tool called SIMPSON. Thereby, we can observe the performance of query processing within the context of different response times of the underlying digital library services in the Web, with different kinds of user queries, and with different sizes of query results. To evaluate and compare the performance of different query processing and duplicate detection strategies we developed metrics, particularly with regard to user satisfaction. In this paper, we present results from our first experiments with SIMPSON, focusing on duplicate detection, query specification, and Web server performances of the underlying digital library services.	database;digital library;experiment;library (computing);performance;personalization;server (computing);simpson's rule;simulation;web server;world wide web	Bethina Schmitt;Sven Oberländer	2002		10.1109/WISE.2002.1181647	online aggregation;sargable;query optimization;query expansion;web query classification;digital library;the internet;ranking;computer science;database;web search query;world wide web;information retrieval;query language;satisfiability	Web+IR	-31.856288097888797	-55.169044497654404	70058
2731a1c51c6c0f99556db96683e9c328dab5710e	relating dependent indexes using dempster-shafer theory	term dependency;information retrieval;dempster shafer theory of evidence;indexing terms;indexing;query evaluation;indexation;dempster shafer theory;chinese	Traditional information retrieval (IR) approaches assume that the indexing terms are independent, which is not true in reality. Although some previous studies have tried to consider term relationships, strong simplifications had to be made at the very basic indexing step, namely, dependent terms are assigned independent counts or probabilities.  In this study, we propose to consider dependencies between terms using Dempster-Shafer theory of evidence. An occurrence of a string in a document is considered to represent the set of all the terms implied in it. Probability is assigned to such a set of terms instead of individual terms. During query evaluation phase, a part of the probability of a set can be transferred to those of the query that are related, allowing us to integrate language-dependent relations in IR.  This approach has been tested on several Chinese IR collections. Our experimental results show that our model can outperform the existing state-of-the-art approaches. The proposed method can be used as a general way to consider different types of relationship between terms and for other languages.	information retrieval;string (computer science)	Lixin Shi;Jian-Yun Nie;Guihong Cao	2008		10.1145/1458082.1458140	computer science;data mining;world wide web;chinese;information retrieval;algorithm	Web+IR	-33.12935834303925	-61.436414326440854	70073
bc07240fc2b2220d170affc04bf652284a8f9af5	jobandtalent at recsys challenge 2016	information retrieval;learning to rank;recommendation systems	In this paper we describe the system built by the Jobandtalent Recommendation Team to compete in the RecSys Challenge 2016. The task consisted in predicting future interactions between Users and Items within the XING platform. The data provided by XING consists of users, items, plus interactions, and impressions of items showed to those users. We decided to apply a Learning to Rank approach to find the best combination of relevance features. We finally achieved the 11th position.	interaction;learning to rank;relevance	Jose Ignacio Honrado;Oscar Huarte;Cesar Jimenez;Sebastian Ortega;José R. Pérez-Agüera;Joaquín Pérez-Iglesias;Álvaro Polo;Gabriel Rodríguez	2016		10.1145/2987538.2987547	computer science;machine learning;data mining;world wide web;information retrieval;learning to rank	Web+IR	-26.179815688247544	-60.46810151136707	70140
85dd85be4a0c6a900146d44b8042a1e311991820	university of delaware at diverstiy task of web track 2010		We report our systems and experiments in the diversity task of TREC 2010 Web track. Our goal is to evaluate the effectiveness of the proposed methods for search result diversification on the large data collection. In the diversification systems, we use the greedy algorithm to select the document with the highest diversity score on each position and return a re-ranked list of diversified documents based on the query subtopics. The system extracts different groups of semantically related terms from the original retrieved documents as the subtopics of the query. It then uses the proposed diversity retrieval functions to compute the diversity score of each document on a particular position based on the similarity between the document and each subtopic, the relevance score of the subtopic given the query and the novelty of the subtopic given the previously selected documents.	diversification (finance);experiment;greedy algorithm;relevance	Wei Zheng;Hui Fang;Xuanhui Wang	2010			data collection;novelty;data mining;information retrieval;computer science;diversification (marketing strategy);greedy algorithm	Web+IR	-30.484734254440273	-60.922094846790195	70163
e2260807e6f33eda4a7de72073edb194289ba9ea	identifying context information in datasets		Datasets are used in various applications assisting in performing reasoning and grouping actions on available data (e.g., clustering, classification, recommendations). Such sources of information may contain aspects relevant to context. In order to use to the fullest this context and draw useful conclusions, it is vital to have intelligent techniques that understand which portions of the dataset are relevant to context and what kind of context they represent. In this work we address the above issue by proposing a context extraction technique from existing datasets. We present a process that maps the given data of a dataset to a specific context concept. The prototype of our work is evaluated through an initial collection of datasets collected from various online sources.	application domain;cluster analysis;map;prototype;recommender system;software engineer	Georgia M. Kapitsaki;Giouliana Kalaitzidou;Christos Mettouris;Achilleas Achilleos;George Angelos Papadopoulos	2015		10.1007/978-3-319-25591-0_16	computer science;artificial intelligence;data mining;natural language processing;cluster analysis	Visualization	-27.75687829849723	-58.62923135282146	70248
0f28ab6dfc3c54c230bba26280a74d2fdc71af42	gender prediction for authors of russian texts using regression and classification techniques		Automatic extraction of information about authors of texts (gender, age, psychological type, etc.) based on the analysis of linguistic parameters has gained a particular significance as there are more online texts whose authors either avoid providing any personal data or make it intentionally deceptive despite of it being of practical importance in marketing, forensics, sociology. These studies have been performed over the last 10 years and mainly for English. The paper presents the results of the study of a corpus of Russian-language texts RusPersonality that addressed automatic identification of the gender of the author of a Russian text using mostly topic-independent text parameters. The identification of the gender of authors of texts was addressed as a classification as well as regression task. For the first time for Russian texts we have obtained the models classifying authors of texts according to their gender with the accuracy identical to the state-of-the-art one.	algorithm;automatic identification and data capture;machine learning;personally identifiable information;statistical classification;text corpus	Tatiana Litvinova;Pavel Seredin;Olga Litvinova;Olga Zagorovskaya;Aleksandr Sboev;Dmitry Gudovskih;Ivan Moloshnikov;Roman Rybka	2016			regression;data science;natural language processing;computer science;artificial intelligence	NLP	-21.667613501377037	-59.42989840463825	70252
1490df30c1faeaa86d46fbe5c42b84894aabc72e	selecting a relevant set of examples to learn ie-rules	extraction information;donnee textuelle;adquisicion del conocimiento;information extraction;dato textual;availability;disponibilidad;saisie donnee;educational software program;apprentissage conceptuel;acquisition connaissances;didacticiel;acquisition automatique;aprendizaje conceptual;toma dato;knowledge acquisition;textual data;concept learning;programa didactico;disponibilite;data acquisition;extraction informacion	The growing availability of online text has lead to an increase in the use of automatic knowledge acquisition approaches from textual data, as in Information Extraction (IE). Some IE systems use knowledge learned by single-concept learning systems, as sets of IE rules. Most of such systems need both sets of positive and negative examples. However, the manual selection of positive examples can be a very hard task for experts, while automatic methods for selecting negative examples can generate extremely large example sets, in spite of the fact that only a small subset of them is relevant to learn. This paper brieey describes a more portable multi-concept learning system and presents a methodology to select a relevant set of training examples.	concept learning;information extraction;knowledge acquisition;text corpus	Jordi Turmo;Horacio Rodríguez	2001		10.1007/3-540-45517-5_59	availability;concept learning;computer science;artificial intelligence;machine learning;data mining;data acquisition;information extraction	AI	-22.81592259102138	-63.58479744157738	70324
1b41abbf9d3707a1a5c0fcf8e1f7734da0e61703	beyond the stars: improving rating predictions using review text content	user experience	Online reviews are an important asset for users deciding to buy a product, see a movie, or go to a restaurant, as well as for businesses tracking user feedback. However, most reviews are written in a free-text format, and are therefore difficult for computer systems to understand, analyze, and aggregate. One consequence of this lack of structure is that searching text reviews is often frustrating for users. User experience would be greatly improved if the structure and sentiment conveyed in the content of the reviews were taken into account. Our work focuses on identifying this information from free-form text reviews, and using the knowledge to improve user experience in accessing reviews. Specifically, we focused on improving recommendation accuracy in a restaurant review scenario. In this paper, we report on our classification effort, and on the insight on user-reviewing behavior that we gained in the process. We propose new ad-hoc and regression-based recomme ndation measures, that both take into account the textual component of user reviews. Our results show that using textual information results in better general or personalized review score predictions than those derived from the numerical star ratings given by the users.	aggregate data;goto;hoc (programming language);numerical analysis;personalization;user experience;user review	Gayatree Ganu;Noémie Elhadad;Amélie Marian	2009			data mining;database;user experience design;computer science;stars	Web+IR	-22.29314827360284	-52.0899427219388	70359
b3f43d79b6e1dfe87f9c03acaf08f96cf45b48a0	constructing coherent event hierarchies from news stories		News describe real-world events of varying granularity, and recognition of internal structure of events is important for automated reasoning over events. We propose an approach for constructing coherent event hierarchies from news by enforcing document-level coherence over pairwise decisions of spatiotemporal containment. Evaluation on a news corpus annotated with event hierarchies shows that enforcing global spatiotemporal coreference of events leads to significant improvements (7.6% F1-score) in the accuracy of pairwise decisions.	automated reasoning;coherent;f1 score;mathematical optimization;relationship extraction	Goran Glavas;Jan Snajder	2014			theoretical computer science;data mining;mathematics;world wide web	NLP	-27.004589271810165	-65.88638679760449	70394
a7317a8260f51fbfd883616cfa37f23ef429842c	exploring the topic hierarchy of digital library research in china using keyword networks: a k-core decomposition approach	topic hierarchy;keyword network;digital library in china;k core decomposition;intellectual structure	Exploring the topic hierarchy of a research field can help us better recognize its intellectual structure. This paper proposes a new method to automatically discover the topic hierarchy, in which the keyword network is constructed to represent topics and their relations, and then decomposed hierarchically into shells using the K-core decomposition method. Adjacent shells with similar morphology are merged into layers according to their density and clustering coefficient. In the keyword network of the digital library field in China, we discover four different layers. The basic layer contains 17 tightly-interconnected core concepts which form the knowledge base of the field. The middle layer contains 13 mediator concepts which are directly connected to technology concepts in the basic layer, showing the knowledge evolution of the field. The detail layer contains 65 concrete concepts which can be grouped into 13 clusters, indicating the research specializations of the field. The marginal layer contains peripheral or isolated concepts.	clustering coefficient;digital library;knowledge base;marginal model;mathematical morphology;peripheral	Lu Xiao;Guo Chen;Jianjun Sun;Shuguang Han;Chengzhi Zhang	2016	Scientometrics	10.1007/s11192-016-2051-x	computer science;data science;data mining;world wide web	AI	-26.085976829174893	-57.74537996866748	70444
ea21aedd19f11f4b644ec826f5f19eee83c37fd0	infoseek's experiences searching the internet		This article recounts both amusing and useful data Infoseek has gathered over the past four years as a leading search site on the Internet. Topics include what people ask on the web, factors for relevance ranking web pages, techniques for searching an infinite number of web pages, and the reasons why Infoseek began writing a search engine in Java a year ago.	centralized computing;extensibility;infoseek;internet;java;presence information;relevance;software portability;spamming;web page;web search engine	Steve Kirsch	1998	SIGIR Forum	10.1145/305110.305112	information retrieval;computer science;the internet	Web+IR	-31.532547811672952	-55.9530936267017	70445
5613ea0df8810917a1da0941a6d8a4889e2be709	on-topic cover stories from news archives		While Web or newspaper archives store large amounts of articles, they also contain a lot of near-duplicate information. Examples include articles about the same event published by multiple news agencies or articles about evolving events that lead to copies of paragraphs to provide background information. To support journalists, who attempt to read all information on a given topic at once, we propose an approach that, given a topic and a text collection, extracts a set of articles with broad coverage of the topic and minimum amount of duplicates. We start by extracting articles related to the input topic and detecting duplicate paragraphs. We keep only one instance from each group of duplicates by using a weighted quadratic optimization problem. It finds the best position for all paragraphs, such that some articles consist mainly of distinct paragraphs and others consist mainly of duplicates. Finally, we present to the reader the articles with more distinct paragraphs. Our experiments show the high precision and recall of our approach.	archive;experiment;mathematical optimization;optimization problem;precision and recall;sensor;world wide web	Christian Schulte;Bilyana Taneva;Gerhard Weikum	2015		10.1007/978-3-319-16354-3_4	computer science;data mining;world wide web;information retrieval	Web+IR	-26.956792139579836	-59.893592201265086	70505
022a584d583ccafeba8b4f371c4927a6f6e72b30	wikimirs 3.0: a hybrid mir system based on the context, structure and importance of formulae in a document	structure matching;importance of formulae;context information;mathematical information retrieval	"""Nowadays, mathematical information is increasingly available in websites and repositories, such like ArXiv, Wikipedia and growing numbers of digital libraries. Mathematical formulae are highly structured and usually presented in layout presentations, such as PDF, LATEX and Presentation MathML. The differences of presentation between text and formulae challenge traditional text-based index and retrieval methods. To address the challenge, this paper proposes an upgraded Mathematical Information Retrieval (MIR) system, namely WikiMirs 3.0, based on the context, structure and importance of formulae in a document. In WikiMirs 3.0, users can easily """"cut"""" formulae and contexts from PDF documents as well as type in queries. Furthermore, a novel hybrid indexing and matching model is proposed to support both exact and fuzzy matching. In the hybrid model, both context and structure information of formulae are taken into consideration. In addition, the concept of formula importance within a document is introduced into the model for more reasonable ranking. Experimental results, compared with two classical MIR systems, demonstrate that the proposed system along with the novel model provides higher accuracy and better ranking results over Wikipedia."""	digital library;information retrieval;latex;library (computing);portable document format;text-based (computing);wikipedia	Yuehan Wang;Liangcai Gao;Simeng Wang;Zhi Tang;Xiaozhong Liu;Ke Yuan	2015		10.1145/2756406.2756918	computer science;theoretical computer science;data mining;world wide web;information retrieval	Web+IR	-30.309241398540955	-58.426132405857174	70770
911ea16dd6549396462735f74220202935568c46	uhyg at the ntcir-12 mobileclick task: link-based ranking on iunit-page bipartite graph		We participated in the iUnit ranking subtask and the iUnit summarization subtask of the NTCIR-12 MobileClick for the Japanese and English languages. Our strategy is based on link analysis on an iUnit-page bipartite graph. First, we constructed an iUnit-page bipartite graph considering the entailment relationship between the iUnits and the pages. Then, we ranked the iUnits by their scores based on link analysis. For the iUnit ranking subtask, we examined three types of entailment relationships and three types of link analysis, the degree of nodes, PageRank, and HITS. For the iUnit summarization subtask, we propose an intent-sensitive PageRank that is an extended version of the topic-sensitive PageRank based on the probability that users visit pages in a search result page.	automatic summarization;link analysis;pagerank	Sho Iizuka;Takayuki Yumoto;Manabu Nii;Naotake Kamiura	2016			information retrieval;graph power;factor-critical graph;complete bipartite graph;line graph;artificial intelligence;machine learning;distance-hereditary graph;voltage graph;3-dimensional matching;computer science;simplex graph	Web+IR	-26.890427966558743	-64.08217763157809	70785
1f2c57b958635cd6d77ea4ce9b60aaa4a4163b0f	rank based clustering for document retrieval from biomedical databases	document clustering;search engine;information retrieval;document retrieval	Now a day’s, search engines are been most widely used for extracting information’s from various resources throughout the world. Where, majority of searches lies in the field of biomedical for retrieving related documents from various biomedical databases. Currently search engines lacks in document clustering and representing relativeness level of documents extracted from the databases. In order to overcome these pitfalls a text based search engine have been developed for retrieving documents from Medline and PubMed biomedical databases. The search engine has incorporated page ranking bases clustering concept which automatically represents relativeness on clustering bases. Apart from this graph tree construction is made for representing the level of relatedness of the documents that are networked together. This advance functionality incorporation for biomedical document based search engine found to provide better results in reviewing related documents based on relativeness. KeywordsBiomedical, Clustering, Databases, Information Retrieval, Text Mining, Web-based.	cluster analysis;database;document retrieval;information retrieval;medline;pagerank;pubmed;text mining;text-based (computing);web search engine	Jayanthi Manicassamy;Dhavachelvan Ponnurangam	2009	CoRR		document retrieval;document clustering;computer science;data mining;world wide web;information retrieval;search engine	Web+IR	-28.829872248456443	-57.27977388032606	70860
25281379c77a8c6548a238464680484e8180a37e	videoqa: question answering on news video	search engine;video summarization;video retrieval;transcript error correction;error correction;natural language;video question answering;speech recognition;user interaction;question answering	When querying a news video archive, the users are interested in retrieving precise answers in the form of a summary that best answers the query. However, current video retrieval systems, including the search engines on the web, are designed to retrieve documents instead of precise answers. This research explores the use of question answering (QA) techniques to support personalized news video retrieval. Users interact with our system, VideoQA, using short natural language questions with implicit constraints on contents, context, duration, and genre of expected videos. VideoQA returns short precise news video summaries as answers. The main contributions of this research are: (a) the extension of QA technology to support QA in news video; and (b) the use of multi-modal features, including visual, audio, textual, and external resources, to help correct speech recognition errors and to perform precise question answering. The system has been tested on 7 days of news video and has been found to be effective.	archive;emoticon;modal logic;natural language;personalization;question answering;speech recognition;web search engine;yahoo! answers	Grace Hui Yang;Lekha Chaisorn;Yunlong Zhao;Shi-Yong Neo;Tat-Seng Chua	2003		10.1145/957013.957146	error detection and correction;question answering;computer science;multimedia;natural language;world wide web;information retrieval;search engine	Web+IR	-26.46328159995594	-54.32714636200332	70962
9285b95f6021f3882fa7690352dcf8442d25e9a2	analyzing and comparing on-line news sources via (two-layer) incremental clustering	004;text mining incremental clustering on line news	In this paper, we analyse the contents of the web site of two Italian news agencies and of four of the most popular Italian newspapers, in order to answer questions such as what are the most relevant news, what is the average life of news, and how much different are different sites. To this aim, we have developed a web-based application which hourly collects the articles in the main column of the six web sites, implements an incremental clustering algorithm for grouping the articles into news, and finally allows the user to see the answer to the above questions. We have also designed and implemented a two-layer modification of the incremental clustering algorithm and executed some preliminary experimental evaluation of this modification: it turns out that the two-layer clustering is extremely efficient in terms of time performances, and it has quite good performances in terms of precision and recall. 1998 ACM Subject Classification H.2.8 Database Applications: Data mining, H.3.3 Information Search and Retrieval: Clustering, H.3.5 Online Information Services: Web-based services	algorithm;alice and bob;cluster analysis;data mining;online and offline;performance;precision and recall;web application;web scraping	Francesco Cambi;Pierluigi Crescenzi;Linda Pagli	2016		10.4230/LIPIcs.FUN.2016.9	data stream clustering;fuzzy clustering;computer science;data science;data mining;world wide web	Web+IR	-31.601090991573646	-61.2519075627334	71023
4d2e3102c34d023a0376157b9b12b7040d7b2f9c	the nlpr_tac entity linking system at tac 2011		In this paper, our system in the KBP Entity Linking task of TAC 2011 is described. Our system mainly contains three components. 1) Entity Candidate Detector, in this component, out system identifies all the possible entities for an entity mention through a variety of knowledge sources, such as the Wikipedia Anchor Dictionary, the Web, etc. Specially, for acronym mentions, we detect their entities by expanding acronym in the source documents. 2) Entity Linker, in which the similarity between an entity mention and an entity candidate in KB is computed. The entity candidate with higher scores than a threshold will be extracted as the real entity of the mention. 3) In the final component, our system clusters all the NIL entity mentions which refer to the same entity using Hierarchical Agglomerative Clustering algorithm.	algorithm;dictionary;entity linking;wikipedia;world wide web	Tao Zhang;Kang Liu;Jian Zhao	2011			data mining;entity linking;world wide web;weak entity;information retrieval;sgml entity	NLP	-29.32187504607226	-65.37791613920191	71113
a8237c94ffaf948dcc7b75110da877f0802fa57c	seeking for high level lexical association in texts		Searching information in a huge amount of data can be a difficult task. To support this task several strategies are used. Classification of data and labeling are two of these strategies. Used separately each of these strategies have certain limitations. Algorithms used to support the process of automated classification influence the result. In addition, many noisy classes can be generated. On the other hand, labeling of document can help recall but it can be time consuming to find metadata. This paper presents a method that exploits the notion of association rules and maximal association rules, in order to assist textual data processing, these two strategies are combined.		Ismaïl Biskri;Louis Rompre;Christophe Jouis;Abdelghani Achouri;Steve Descoteaux;Boucif Amar Bensaber	2013		10.1007/978-3-319-00560-7_8	natural language processing;linguistics;communication	NLP	-26.24584198595835	-65.33653350264355	71219
6d66e92505607fa38b336563013169afb2d40037	semtec: social emotion mining techniques for analysis and prediction of facebook post reactions		Nowadays social media are utilized by many people in order to review products and services. Subsequently, companies can use this feedback in order to improve customer experience. Facebook provided its users with the ability to express their experienced emotions by using five so-called ‘reactions’. Since this launch happened in 2016, this paper is one of the first approaches to provide a complete framework for evaluating different techniques for predicting reactions to user posts on public pages. For this purpose, we used the FacebookR dataset that contains Facebook posts (along with their comments and reactions) of the biggest international supermarket chains. In order to build a robust and accurate prediction pipeline state-of-the-art neural network architectures (convolutional and recurrent neural networks) were tested using pretrained word embeddings. The models are further improved by introducing a bootstrapping approach for sentiment and emotion mining on the comments for each post and a data augmentation technique to obtain an even more robust predictor. The final proposed pipeline is a combination of a neural network and a baseline emotion miner and is able to predict the reaction distribution on Facebook posts with a mean squared error (or misclassification rate) of 0.1326.		Tobias Moers;Florian Krebs;Gerasimos Spanakis	2018		10.1007/978-3-030-05453-3_17	computer science;machine learning;deep learning;artificial neural network;social media;bootstrapping;recurrent neural network;artificial intelligence;mean squared error	NLP	-19.399341898654807	-53.137937003565554	71388
aa942821ba95c7c358a9ecaed732d0a0b409eb48	where in the world are you? geolocation and language identification in twitter	idioma;地理;推特;language;geografia;twitter;语言;geography	The movements of ideas and content between locations and languages are unquestionably crucial concerns to researchers of the information age, and Twitter has emerged as a central, global platform on which hundreds of millions of people share knowledge and information. A variety of research has attempted to harvest locational and linguistic metadata from tweets in order to understand important questions related to the 300 million tweets that flow through the platform each day. However, much of this work is carried out with only limited understandings of how best to work with the spatial and linguistic contexts in which the information was produced. Furthermore, standard, well-accepted practices have yet to emerge. As such, this paper studies the reliability of key methods used to determine language and location of content in Twitter. It compares three automated language identification packages to Twitter’s user interface language setting and to a human coding of languages in order to identify common sources of disagreement. The paper also demonstrates that in many cases user-entered profile locations differ from the physical locations users are actually tweeting from. As such, these open-ended, user-generated, profile locations cannot be used as useful proxies for the physical locations from which information is published to Twitter.	geolocation;language identification;nonlinear gameplay;user interface;user-generated content	Mark Graham;Scott A. Hale;Devin Gaffney	2013	CoRR	10.1080/00330124.2014.907699	geography;data mining;sociology;language;internet privacy;world wide web	NLP	-24.022362364855994	-54.98355070458054	71395
0d370e770cc92c5f1c89a3390012a478ddc28b0f	thesaurus-based method of increasing text-via-keyphrase graph connectivity during keyphrase extraction for e-tourism applications		The paper is devoted to solving the task of automatic extraction of keyphrases from a text corpus relating to a specific domain so that the texts linked by common keyphrases would form a well-connected graph. The authors developed a new method that uses a combination of a well-known keyphrase extraction algorithm (e.g., TextRank, Topical PageRank, KEA, Maui) with thesaurus-based procedure that improves the text-via-keyphrase graph connectivity and simultaneously raises the quality of the extracted keyphrases in terms of precision and recall. The effectiveness of the proposed method is demonstrated on the text corpus of the Open Karelia tourist information system.	connectivity (graph theory);thesaurus	Ilya Paramonov;Ksenia Lagutina;Eldar Mamedov;Nadezhda Lagutina	2016		10.1007/978-3-319-45880-9_11	natural language processing;machine learning;pattern recognition	EDA	-27.04294751732873	-63.99527200143646	71517
3657637c5807c130935f1ae8e30d38b09e61b078	relevance stability in blog retrieval	blog search;statistical significance;relevance stability;temporal analysis	This paper investigates blog distillation where the goal is to rank blogs according to their recurrent relevance to the topic of the query. One of the main features of blogs is their relation to time but this important feature is under-utilized in the current blog retrieval methods. We propose a probabilistic framework to measure the stability of blogs relevance over time. We then study the effect of the proposed stability measure in the blog retrieval performance. We evaluate the proposed framework on the standard TREC Blog08 collection. The results show statistically significant improvements over state of the art models.	blog;relevance;text retrieval conference	Mostafa Keikha;Shima Gerani;Fabio Crestani	2011		10.1145/1982185.1982432	computer science;data mining;statistical significance;world wide web;information retrieval	Web+IR	-26.021940251656744	-60.80516703357366	71635
3769c824968598827400b018f94409d02a813a67	automated emergence of a crisis situation model in crisis response based on tweets		During a crisis, being able to understand quickly the situation on-site is crucial for the responders to take relevant decisions together. Social media, in particular Twitter, have proved to be a means for rapidly getting information from the field. However, the deluge of data is heterogeneous in many ways (location, trust, content, vocabulary, etc.), and getting a model of the crisis situation still requires laborious human actions. In addition, depending on which kind of information is mined from them, tweets have to be handle one-by-one (e.g. find victims), or as a whole - amount of tweets - (e.g. occurence of an event). This paper proposes a framework for automatically extracting, interpreting and aggregating streams of tweets to characterize crisis situations. It is based on a specific metamodel that determines the different concepts required to model a crisis situation.	emergence	Aurélie Montarnal;Shane E. Halse;Andrea H. Tapia;Sébastien Truptil;Frédérick Bénaben	2017		10.1007/978-3-319-65151-4_58	metamodeling;crisis management;streams;knowledge management;management science;decision support system;social media;computer science;public relations;vocabulary	NLP	-22.159554288313032	-55.20756830130271	71669
234939d794c2202e589580fae86a59efc08bac52	web news categorization using a cross-media document graph	web pages;cross media documents;graph representation;web news categorization;cross media correlations;document graph	In this paper we propose a multimedia categorization framework that is able to exploit information across different parts of a multimedia document (e.g., a Web page, a PDF, a Microsoft Office document). For example, a Web news page is composed by text describing some event (e.g., a car accident) and a picture containing additional information regarding the real extent of the event (e.g., how damaged the car is) or providing evidence corroborating the text part. The framework handles multimedia information by considering not only the document's text and images data but also the layout structure which determines how a given text block is related to a particular image. The novelties and contributions of the proposed framework are: (1) support of heterogeneous types of multimedia documents; (2) a document-graph representation method; and (3) the computation of cross-media correlations. Moreover, we applied the framework to the tasks of categorising Web news feed data, and our results show a significant improvement over a single-medium based framework.	categorization;computation;graph (abstract data type);portable document format;web feed;web page	José Iria;Fabio Ciravegna;João Magalhães	2009		10.1145/1646396.1646431	static web page;computer science;web page;graph;multimedia;world wide web;information retrieval	Web+IR	-24.381384373992425	-55.13707359100352	71750
da2c983b56648aa87ee3da943ade57dd9ee666b4	predicting personality traits and social context based on mining the smartphones sms data		Reality Mining is one of the first efforts that have been exerted to utilize smartphone’s data; to analyze human behavior. The smartphone data are used to identify human behavior and discover more attributes about smartphone users, such as their personality traits and their relationship status. Text messages and SMS logs are two of the main data resources from the smartphones. In this paper, The proposed system define the user personality by observing behavioral characteristics derived from smartphone logs and the language used in text messages. Hence, The supervised machine learning methods (K-nearest nighbor (KNN), support vector machine, and Naive Bayes) and text mining techniques are used in studying the textual matter messages. From this study, The correlation between text messages and predicate users personality traits is broken down. The results provided an overview on how text messages and smartphone logs represent the user behavior; as they chew over the user personality traits with accuracy up to 70 %.	smartphone	Fatma Yakoub;Moustafa Zein;Khaled Yasser;Ammar Adl;Aboul Ella Hassanien	2015		10.1007/978-3-319-21206-7_44	data mining;internet privacy;world wide web	ML	-20.834593184356912	-54.70286699159165	71877
32b910a3788c2ac94fbfc15fe8d603cd92358491	a new application of a fuzzy linguistic quality evaluation system in digital libraries	libraries;pragmatics;service provider;computing with words;libraries pragmatics quality assessment web sites computational modeling context intelligent systems;digital library;users perception fuzzy linguistic quality evaluation digital library web sites;digital libraries;linguistic modeling;fuzzy set theory;computational modeling;quality assessment;aggregation operator;web sites computational linguistics digital libraries fuzzy set theory user interfaces;quality evaluation;web sites;intelligent systems;users perception;fuzzy linguistic;computational linguistics;fuzzy linguistic modeling;fuzzy linguistic modeling quality evaluation digital libraries;user interfaces;context	In this contribution, we present a new application based on fuzzy linguistic information to evaluate the quality of digital libraries. The quality evaluation of digital libraries is defined using users' perceptions on the quality of digital services provided through their Web sites. We assume a fuzzy linguistic modeling to represent the users' perception and apply automatic tools of fuzzy computing with words based on some weighted aggregation operators to compute global quality evaluations of digital libraries.	computing with words and perceptions;digital library;library (computing)	Ignacio J. Pérez;Enrique Herrera-Viedma;Javier López Gijón;Francisco Javier Cabrerizo	2010	2010 10th International Conference on Intelligent Systems Design and Applications	10.1109/ISDA.2010.5687193	service provider;natural language processing;digital library;computer science;data mining;fuzzy set;user interface;computational model;world wide web	EDA	-28.67715440494711	-58.11604511493346	71948
3f10df7b5d33730eefded12298ff4e50d853667f	instant search: a hands-on tutorial	instant search;information retrieval;query understanding	"""Instant search has become a common part of the search experience in most popular search engines and social networking websites. The goal is to provide instant feedback to the user in terms of query completions (""""instant suggestions"""") or directly provide search results (""""instant results"""") as the user is typing their query. The need for instant search has been further amplified by the proliferation of mobile devices and services like Siri and Google Now that aim to address the user's information need as quickly as possible. Examples of instant results include web queries like """"weather san jose"""" (which directly provides the current temperature), social network queries like searching for someone's name on Facebook or LinkedIn (which directly provide the people matching the query). In each of these cases, instant search constitutes a superior user experience, as opposed to making the user complete their query before the system returns a list of results on the traditional search engine results page (SERP).  We consider instant search experience to be a combination of instant results and instant suggestions, with the goal of satisfying the user's information need as quickly as possible with minimal effort on the part of the user. We first present the challenges involved in putting together an instant search solution at scale, followed by a survey of IR and NLP techniques that can be used to address them. We will also conduct a hands-on session aimed at putting together an end-to-end instant search system using open source tools and publicly available data sets. These tools include typeahead.js from Twitter for the frontend and Lucene/elasticsearch for the backend. We present techniques for prefix-based retrieval as well as injecting custom ranking functions into elasticsearch. For the search index, we will use the dataset made available by Stackoverflow.  This tutorial is aimed at both researchers interested in knowing about retrieval techniques used for instant search as well as practitioners interested in deploying an instant search system at scale. The authors have worked extensively on building and scaling LinkedIn's instant search experience. To the best of our knowledge, this is the first tutorial that covers both theoretical and practical aspects of instant search."""	elasticsearch;end-to-end principle;google now;google search;hands-on computing;image scaling;information needs;mobile device;natural language processing;open-source software;search engine indexing;search engine results page;siri;social network;stack overflow;typeahead;user experience;web search engine	Ganesh Venkataraman;Abhimanyu Lad;Viet Ha-Thuc;Dhruv Arya	2016		10.1145/2911451.2914806	computer science;data science;machine learning;data mining;world wide web;information retrieval	Web+IR	-31.177954584653417	-54.23099875095233	72073
e53058bf8658985262e244ab2f66a5a18a6927be	entity linking with people entity on wikipedia		This paper introduces a new model that uses named entity recognition, coreference resolution, and entity linking techniques, to approach the task of linking people entities on Wikipedia people pages to their corresponding Wikipedia pages if applicable. Our task is different from general and traditional entity linking because we are working in a limited domain, namely, people entities, and we are including pronouns as entities, whereas in the past, pronouns were never considered as entities in entity linking. We have built 2 models, both outperforms our baseline model significantly. The purpose of our project is to build a model that could be use to generate cleaner data for future entity linking tasks. Our contribution include a clean data set consisting of 50 Wikipedia people pages, and 2 entity linking models, specifically tuned for this domain.	baseline (configuration management);entity linking;named-entity recognition;wikipedia	Weiqian Yan;Kanchan Khurad	2017	CoRR		natural language processing;computer science;data mining;entity linking;weak entity;information retrieval	NLP	-27.543052279337036	-65.35488357531132	72222
599ad39b6a36742c60681887d8ed4f70ad220e8a	a skip-gram-based framework to extract knowledge from chinese reviews in cloud environment	knowledge space;skip gram model;information extraction;euclidean distance;期刊论文;reviews	With the development of cloud computing technologies, eBusiness systems and applications pay more attention on customer reviews, such as commodity, customer’s emotion. These review data contain a vast amount of valuable information. It is challenging to extract knowledge from these reviews in cloud environment, because they are massive, usually distributed, and keep constantly changing. In this paper, a novel framework to extract knowledge from Chinese review data is proposed, which mainly includes building knowledge space, retrieving knowledge and optimizing results. For Chinese reviews, a skip-grambased model is used to train review data and generate the knowledge space. To quickly build knowledge space, an algorithm based on hierarchical softmax is proposed, which does not need any feature extraction and modelization. This algorithm is applicable for massive data and conveniently extended in cloud environment. When retrieving knowledge and optimizing results, our framework uses euclidean distance to find the knowledge, closely linked to the query, and uses 2-gram algorithm to optimize the results. Experimental results show that our framework is practical and efficient.	algorithm;cloud computing;euclidean distance;feature extraction;home automation;knowledge space;mathematical model;skip list;softmax function	Feng Zhao;Hang Zhu;Hai Jin;Weizhong Qiang	2015	MONET	10.1007/s11036-015-0612-5	computer science;knowledge management;data science;knowledge-based systems;data mining;euclidean distance;knowledge extraction;computer security;information extraction	Web+IR	-23.202649966269718	-57.618224090130596	72432
1b58b09d44b935f51022344915584938b3382cfa	the recommendation mechanism in an internet information system with time impact coefficient	recommendation systems;user assessment.;information filtering;information system;recommender system	In this paper we propose two generic mechanisms implemented in a cadastre internet information system. The first one is the list of last queries submitted by a given user and the second one is the list of page profiles recommended to a user. The idea of page recommendation is based on the concept of a page profile which represents a system option, type of retrieval mechanisms and search criteria. The calculation of rank values for page profiles is based on the usage frequency and the time impact coefficient. A recommended page is selected by a user from a list facilitates and accelerates his searches by moving him directly to the chosen option page with search form filled with the most expected criteria values. As an additional complementary mechanism the list of last submitted queries is available to each user.	algorithm;association rule learning;coefficient;experiment;fuzzy logic;information system;internet;option type;page view;web search engine	Dariusz Król;Michal Szymanski;Bogdan Trawinski	2006	IJCSA		page view;computer science;machine learning;data mining;world wide web;information retrieval;information system;recommender system	Web+IR	-30.654479192549704	-53.76784774836531	72523
6b99d8f337a8a6d56d86a9092cfce9e48413832f	compression-based algorithms for deception detection		In this work we extend compression-based algorithms for deception detection in text. In contrast to approaches that rely on theories for deception to identify feature sets, compression automatically identifies the most significant features. We consider two datasets that allow us to explore deception in opinion (content) and deception in identity (stylometry). Our first approach is to use unsupervised clustering based on a normalized compression distance (NCD) between documents. Our second approach is to use Prediction by Partial Matching (PPM) to train a classifier with conditional probabilities from labeled documents, followed by arithmetic coding (AC) to classify an unknown document based on which label gives the best compression. We find a significant dependence of the classifier on the relative volume of training data used to build the conditional probability distributions of the different labels. Methods are demonstrated to overcome the data size-dependence when analytics, not information transfer, is the goal. Our results indicate that deceptive text contains structure statistically distinct from truthful text, and that this structure can be automatically detected using compression-based algorithms.	algorithm;arithmetic coding;cluster analysis;network computing devices;prediction by partial matching;stylometry;theory	Christina L. Ting;Andrew N. Fisher;Travis L. Bauer	2017		10.1007/978-3-319-67217-5_16	computer science;prediction by partial matching;normalized compression distance;deception;cluster analysis;stylometry;conditional probability;arithmetic coding;classifier (linguistics);algorithm	ML	-19.594131463298147	-61.67422932077442	72708
07b2fdef3d60ef9324ec7bfcad1c848cf2a65285	a text similarity meta-search engine based on document fingerprints and search results records	search engines engines metasearch feature extraction web search fingerprint recognition web servers;ranking fusion meta search engine similar document retrieval document fingerprinting query generation;file servers;web documents;search engine;document handling;similar document retrieval;text analysis document handling file servers information retrieval records management search engines;search engines;information retrieval;exact solution;document fingerprinting;text analysis;records management;web search engine;ranking fusion;fingerprint recognition;approximate solution;feature extraction;query generation;meta search engine;web search;web server text similarity meta search engine document fingerprint search result record web document retrieval web search engines url text comparison algorithm;document retrieval;similarity function	The retrieval of similar documents from the Web using documents as input instead of key-term queries is not currently supported by traditional Web search engines. One approach for solving the problem consists of fingerprint the document's content into a set of queries that are submitted to a list of Web search engines. Afterward, results are merged, their URLs are fetched and their content is compared with the given document using text comparison algorithms. However, the action of requesting results to multiple web servers could take a significant amount of time and effort. In this work, a similarity function between the given document and retrieved results is estimated. The function uses as variables features that come from information provided by search engine results records, like rankings, titles and snippets. Avoiding therefore, the bottleneck of requesting external Web Servers. We created a collection of around 10,000 search engine results by generating queries from 2,000 crawled Web documents. Then we fitted the similarity function using the cosine similarity between the input and results content as the target variable. The execution time between the exact and approximated solution was compared. Results obtained for our approximated solution showed a reduction of computational time of 86% at an acceptable level of precision with respect to the exact solution of the web document retrieval problem.	approximation algorithm;computation;cosine similarity;document retrieval;fingerprint;run time (program lifecycle phase);similarity measure;time complexity;web page;web search engine;web server;world wide web	Felipe Bravo-Marquez;Gaston L'Huillier;Sebastián A. Ríos;Juan D. Velásquez	2011	2011 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology	10.1109/WI-IAT.2011.27	document retrieval;metasearch engine;web search engine;computer science;database;world wide web;information retrieval;search engine	Web+IR	-30.722474239301523	-55.740454047122064	72730
02ad6598233fb6bc2e7ccab728c9375a74e52bb9	the extraction method of the service improvement information from guests' review		Abstract   The online hotel reservation service becomes so popular that the number of transactions has been growing year by year. Before making such reservation, travelers would find it important to refer other guests’ opinions about accomodations. In those reviews, both dissatisfied and satisfied impressions used to appear in the same comments. In order to analyze this tendency, we employ text mining and investigate dissatisfied topic from their expressions. We propose some ways to extract useful information from guest review for accommodation's service improvement.		Koichi Tsujii;Yoshikatsu Fujita;Kazuhiko Tsuda	2013		10.1016/j.procs.2013.09.218	data mining;multimedia;world wide web	HPC	-26.7857117150074	-52.81084616393313	72738
5928196cb2a9f752ec906e93235f54fecadcf48b	clustering scientific documents with topic modeling	text analysis;topic modeling;atent dirichlet allocation	Topic modeling is a type of statistical model for discovering the latent “topics” that occur in a collection of documents through machine learning. Currently, latent Dirichlet allocation (LDA) is a popular and common modeling approach. In this paper, we investigate methods, including LDA and its extensions, for separating a set of scientific publications into several clusters. To evaluate the results, we generate a collection of documents that contain academic papers from several different fields and see whether papers in the same field will be clustered together. We explore potential scientometric applications of such text analysis capabilities.	algorithm;f1 score;latent dirichlet allocation;machine learning;scientific literature;scientometrics;statistical model;topic model	Chyi-Kwei Yau;Alan L. Porter;Nils C. Newman;Arho Suominen	2014	Scientometrics	10.1007/s11192-014-1321-8	latent dirichlet allocation;dynamic topic model;text mining;pachinko allocation;computer science;data science;data mining;topic model;information retrieval	ML	-25.495801423102037	-59.47806487353303	72756
19fa6ac229431e0f112d6f717f7de288b46d73e4	design and implementation-algorithms of amharic search engine system for amharic web contents	search engines natural languages internet crawlers web search information retrieval information resources web pages computer science electronic mail;web documents;search engine;indexer;web pages;query processing;search engines;information retrieval;ethiopia;amharic search engine algorithm;amharic on world wide web;indexing terms;amharic web contents;crawler;web search engine;amharic alias support amharic search engine system amharic web contents amharic language web documents internet users ethiopia crawler indexer query engine;ir in amharic language;indexes;internet users;internet;engines;search engines indexing internet natural language processing query processing;indexing;design and implementation;indexation;query engine;world wide web;crawlers;amharic search engine system;languages other than english;encoding;amharic language;natural language processing;amharic alias support	Amharic language) has been growing exponentially. The number of web documents in Amharic language as well as Internet users in Ethiopia is growing dramatically. However, the major search engines have been lagging behind in providing indexes, stemming and search features to handle this language. Therefore, the design and implementation of web search engine that considers the typical characteristics of the Amharic language is needed. In this paper, we design Amharic Search Engine system for Amharic language web documents and briefly discuss the algorithms for implementing the engine. The Crawler, Indexer and Query Engine are the basic components of this search engine. Typical characteristics of the Amharic language were considered by testing the engine for morphological variants as well as Amharic aliases support. For experimentation, two runs of the crawler were conducted by using 10 threads that crawl in parallel	algorithm;internet;search engine indexing;stemming;web crawler;web page;web search engine	Hassen Redwan;Solomon Atnafu	2009	2009 3rd International Conference on New Technologies, Mobility and Security	10.1109/NTMS.2009.5384814	web search engine;computer science;database;world wide web;information retrieval;search engine	DB	-31.425128490212774	-55.84456647591856	73290
8cc4748d8dacde191a0a15e3a7a138ef66ad3e65	keyword query cleaning using hidden markov models	hidden markov model;segmentation;hidden markov models;keyword search;query cleaning	In this paper, we consider the problem of keyword query cleaning for structured databases from a probabilistic approach. Keyword query cleaning consists of rewriting the user query, segmenting the keywords, matching each segment to database items, and finally tagging the segments by their meta-data information. We present an efficient and robust solution using Hidden Markov Models (HMM). By modeling user keyword queries using a generative probabilistic HMM-based model, we construct a HMM from the user specified keyword query (and the database instance). The optimal statistical keyword cleaning is computed as the most likely path of the constructed HMM. Furthermore, we demonstrate how the optimal HMM-based keyword cleaning algorithm can be generalized to compute a stream of clean queries ranked from the most likely clean query to the least likely clean query. Finally, we present the implementation of the proposed system and its preliminary performance.	algorithm;database;hidden markov model;markov chain;plasma cleaning;rewriting	Ken Q. Pu	2009		10.1145/1557670.1557681	query optimization;query expansion;computer science;pattern recognition;data mining;database;web search query;segmentation;hidden markov model	DB	-28.470339749082367	-61.70792088732862	73400
1d223cc74af06fd3c8865f57875b374f21c86448	the search for expertise: to the documents and beyond	expert finding;enterprise search;expertise retrieval	Expert finding is a rapidly developing Information Retrieval task and a popular research domain. The opportunity of search for knowledgeable people in the scope of an organization or world-wide is a feature which makes modern Enterprise search systems commercially successful and socially demanded. A number of efficient expert finding approaches was proposed recently. Despite that most of them are based on theoretically sound measures of expertness, they still use rather unrealistic and oversimplified principles. In our research we try to avoid these limitations and come up with models that go beyond the assumptions used in state-of-theart expert finding methods. The fundamental principle of existing approaches to expert finding is to infer expertise by analyzing the co-occurrence of personal identifiers and query terms in the scope of top ranked documents. While, the degree of co-occurrence of a person with topical terms is a reasonable evidence of personal expertness, the assumption about their independent occurrence seems not so adequate. In our methods, we consider that the occurrence of terms in the document is not independent from the presence of a candidate expert and vice versa. In one model, we regard people as generators of the expertise accumulated in the top retrieved documents. We extract their topic-specific personal language models that are further matched to a query [1]. In another model we simply assume that the responsibility of a person for the content of a document depends on its position in a document with respect to positions of the query terms and then just aggregate scores of documents related to a person for measuring personal expertness [3]. Suppose we still assume the independence of persons and terms in a document when measuring their co-occurrence. In this case we would in fact model the manual search for expertise by representing it as the following probabilistic process. The user selects a document among the ones appearing in the initial ranking, looks through the document,	aggregate data;identifier;information retrieval;language model	Pavel Serdyukov	2008		10.1145/1390334.1390568	computer science;knowledge management;machine learning;data mining;management science;information retrieval	Web+IR	-33.31251594110464	-58.64573526456792	73625
d91f0a255d9c04544af9b6037aabbd71e3834331	usage-oriented topic maps building approach	relational data;topic maps;data type;semi structured data;resource availability;non native speaker	In this paper, we present a collaborative and incremental construction approach of multilingual Topic Maps based on enrichment and merging techniques. In recent years, several Topic Map building approaches have been proposed endowed with different characteristics. Generally, they are dedicated to particular data types like text, semi-structured data, relational data, etc. We note also that most of these approaches take as input monolingual documents to build the Topic Map. The problem is that the large majority of resources available today are written in various languages, and these resources could be relevant even to non-native speakers. Thus, our work is driven towards a collaborative and incremental method for Topic Map construction from textual documents available in different languages. To enrich the Topic Map, we take as input a domain thesaurus and we propose also to explore the Topic Map usage which means available potential questions related to the source documents.	gene ontology term enrichment;kernel same-page merging;semi-structured data;semiconductor industry;thesaurus;topic maps	Nebrasse Ellouze;Nadira Lammari;Elisabeth Métais;Mohamed Ben Ahmed	2009		10.1007/978-3-642-04590-5_2	topic maps;semi-structured data;data type;relational database;computer science;artificial intelligence;pattern recognition;data mining;database	Web+IR	-33.19415619021101	-66.13345993811224	73648
378d035fd360caaa4e95fcd9e2da63af5779534d	a reference collection for web spam	web pages;web spam	We describe the WEBSPAM-UK2006 collection, a large set of Web pages that have been manually annotated with labels indicating if the hosts are include Web spam aspects or not. This is the first publicly available Web spam collection that includes page contents and links, and that has been labelled by a large and diverse set of judges.	spamdexing;spamming;web page;world wide web	Carlos Castillo;Debora Donato;Luca Becchetti;Paolo Boldi;Stefano Leonardi;Massimo Santini;Sebastiano Vigna	2006	SIGIR Forum	10.1145/1189702.1189703	web service;forum spam;static web page;web development;site map;data web;web analytics;computer science;spamdexing;spamming;spambot;web page;internet privacy;web 2.0;world wide web;website parse template;information retrieval	Web+IR	-29.458884332119606	-55.555087088817714	73842
c670e900bf613709c2d6c9d7edb31adab4b18d91	an ontology based model for document clustering	information systems;information retrieval;clustering;particle swarm optimization;ontology	Clustering is an important topic to find relevant content from a document collection and it also reduces the search space. The current clustering research emphasizes the development of a more efficient clustering method without considering the domain knowledge and user’s need. In recent years the semantics of documents have been utilized in document clustering. The discussed work focuses on the clustering model where ontology approach is applied. The major challenge is to use the background knowledge in the similarity measure. This paper presents an ontology based annotation of documents and clustering system. The semi-automatic document annotation and concept weighting scheme is used to create an ontology based knowledge base. The Particle Swarm Optimization (PSO) clustering algorithm can be applied to obtain the clustering solution. The accuracy of clustering has been computed before and after combining ontology with Vector Space Model (VSM). The proposed ontology based framework gives improved performance and better clustering compared to the traditional vector space model. The result using ontology was significant and promising.	algorithm;archive;cluster analysis;knowledge base;particle swarm optimization;semiconductor industry;similarity measure;viable system model	U. K. Sridevi;N. Nagaveni	2011	IJIIT	10.4018/jiit.2011070105	correlation clustering;data stream clustering;document clustering;fuzzy clustering;flame clustering;computer science;data science;canopy clustering algorithm;machine learning;consensus clustering;ontology;cure data clustering algorithm;data mining;cluster analysis;ontology-based data integration;brown clustering;particle swarm optimization;biclustering;information retrieval;information system;clustering high-dimensional data;conceptual clustering	Web+IR	-27.565831509131577	-60.05871363199206	74034
fd62472291ec87977624b5a498995a5b3024756b	aggregated search result diversification	multiple search vertical;standard diversity metrics;aggregated search result diversification;multiple place name;product search;multiple information need;search result diversification;web search;different search vertical;commercial search engine	Search result diversification has been effectively employed to tackle query ambiguity, particularly in the context of web search. However, ambiguity can manifest differently in different search verticals, with ambiguous queries spanning, e.g., multiple place names, content genres, or time periods. In this paper, we empirically investigate the need for diversity across four different verticals of a commercial search engine, including web, image, news, and product search. As a result, we introduce the problem of aggregated search result diversification as the task of satisfying multiple information needs across multiple search verticals. Moreover, we propose a probabilistic approach to tackle this problem, as a natural extension of state-of-the-art diversification approaches. Finally, we generalise standard diversity metrics, such as ERR-IA and α-nDCG, into a framework for evaluating diversity across multiple search verticals.	diversification (finance);file spanning;information needs;web search engine;web search query	Rodrygo L. T. Santos;Craig MacDonald;Iadh Ounis	2011		10.1007/978-3-642-23318-0_23	marketing;data mining;mathematics;world wide web	Web+IR	-32.684851755854744	-54.69734637292969	74049
2b9e9603828e3adba4c76fb61c1543167b6fbde0	formal language models for finding groups of experts	entity retrieval;universiteitsbibliotheek;enterprise search;group finding	The task of finding groups or teams has recently received increased attention, as a natural and challenging extension of search tasks aimed at retrieving individual entities. We introduce a new group finding task: given a query topic, we try to find knowledgeable groups that have expertise on that topic. We present five general strategies for this group finding task, given a heterogenous document repository. The models are formalized using generative language models. Two of the models aggregate expertise scores of the experts in the same group for the task, one locates documents associated with experts in the group and then determines how closely the documents are associated with the topic, whilst the remaining two models directly estimate the degree to which a group is a knowledgeable group for a given topic. For evaluation purposes we construct a test collection based on the TREC 2005 and 2006 Enterprise collections, and define three types of ground truth for our task. Experimental results show that our five knowledgeable group finding models achieve high absolute scores. We also find significant differences between different ways of estimating the association between a topic and a group. © 2015 Elsevier Ltd. All rights reserved.	aggregate data;entity;formal language;ground truth;language model;text retrieval conference;while	Shangsong Liang;Maarten de Rijke	2016	Inf. Process. Manage.	10.1016/j.ipm.2015.11.005	computer science;machine learning;data mining;world wide web;information retrieval	Web+IR	-30.097331259299555	-62.47057010675524	74245
db0066839bd0315fd7f1ec322ae30d881f11c208	vireo at trecvid 2010: semantic indexing, known-item search, and content-based copy detection		This paper presents our approaches and the comparative analysis of our results for the three TRECVID 2010 tasks that we participated in: semantic indexing, known-item search and content-based copy detection. Semantic Indexing (SIN): Our main focus for the SIN task is on the study of the following two issues: 1) the effectiveness of concept detectors for indexing web video dataset, and 2) how to leverage the ontology relationships to reinforce concept detection. Our baseline detectors are similar to those of our TRECVID 2009 system, where both local and global features are employed to train the SVM model for each concept. Based upon the baseline detectors, we propose two approaches to refine the detection scores. The first integrates the ontology information into the random walk framework. The second seeks the agreement among the set of ranked lists generated by semantically related concepts. Our four submitted runs are summarized below: F A VIREO.randomwalk 1: perform a random walk over the baseline result using local feature alone. Flickr distance and ontology relationship are used to build the context. F A VIREO.agreement 2: re-rank the videos by seeking the agreement among semantically related concepts on the relevant videos. F A VIREO.baseline vk 3: local feature alone multiple detectors. F A VIREO.baseline vk cm 4: average fusion of local feature and global feature. Known-Item Search (KIS): In this new task, there is a shift in the search requirement compared to previous years’ search tasks, from general queries with multiple solutions to specific queries with single solution. This has rendered previously proposed algorithms ineffective. In this first attempt, our objective is to observe the effectiveness of different modalities (metadata, automatic speech recognition (ASR) and concepts) towards known-item search. Evaluation result shows that for known-item search, textual-based modalities are useful, where the metadata is the most effective while ASR plays a complementary role. We submitted four runs for the fully automatic settings as follows: F A YES vireo run1 metadata asr 1: metadata + ASR F A YES vireo run2 metadata 2: metadata only F A YES vireo run3 asr 3: ASR only F A YES vireo run4 concept 4: concept only Content-Based Video Copy Detection (CCD): We submitted three runs based on our video-only detection framework. Since we only conduct video-only copy detection, for the video+audio detection task, we simply submit the same detection results. The three submissions are defined as follows: Vireo.m.BALANCED.srpe: employ Hamming Embedding (HE), Enhanced Weak Geometric Consistency Checking (EWGC), Scale-Rotation Invariant Pattern Entropy (SR-PE) and 2D Hough Transform (2D HT) Vireo.m.BALANCED.srpeflip: repeat the same procedure as the previous run but with the flipped queries. The retrieval results are then linearly fused with the previous run before being fed into 2D HT. Vireo.m.NOFA.srpeflip: same as the second run except that a higher threshold is chosen.	algorithm;automated system recovery;baseline (configuration management);charge-coupled device;complexity;dictionary;flickr;geographic information system;hough transform;modality (human–computer interaction);qualitative comparative analysis;refinement (computing);sensor;speech recognition;text-based (computing);vector quantization;video clip;video copy detection;window function	Chong-Wah Ngo;Shiai Zhu;Hung-Khoon Tan;Wanlei Zhao;Xiao-Yong Wei	2010			computer science;data mining;world wide web;information retrieval	Web+IR	-31.037070884036485	-62.3539439224509	74253
46b1c17cd65f9224c6b3be1ae4a93afae188b04b	profile-based focused crawling for social media-sharing websites	signal image and speech processing;focused crawling;biometrics;pattern recognition;image processing and computer vision;social media	We present a novel profile-based focused crawling system for dealing with the increasingly popular social media-sharing websites. In this system, we treat the user profiles as ranking criteria for guiding the crawling process. Furthermore, we divide a user's profile into two parts, an internal part, which comes from the user's own contribution, and an external part, which comes from the user's social contacts. In order to expand the crawling topic, a cotagging topic-discovery scheme was adopted for social media-sharing websites. In order to efficiently and effectively extract data for the focused crawling, a path string-based page classification method is first developed for identifying list pages, detail pages, and profile pages. The identification of the correct type of page is essential for our crawling, since we want to distinguish between list, profile, and detail pages in order to extract the correct information from each type of page, and subsequently estimate a reasonable ranking for each link that is encountered while crawling. Our experiments prove the robustness of our profile-based focused crawler, as well as a significant improvement in harvest ratio, compared to breadth-first and online page importance computation (OPIC) crawlers, when crawling the Flickr website for two different topics.	focused crawler;social media	Zhiyong Zhang;Olfa Nasraoui	2009	EURASIP J. Image and Video Processing	10.1155/2009/856037	social media;computer science;archaeology;pattern recognition;distributed web crawling;multimedia;world wide web;information retrieval;biometrics	DB	-26.446209985530402	-52.10527549602675	74411
0e0e9361fff86f87d964984d8001c90c68a1c3a9	musicmood: predicting the mood of music from song lyrics using machine learning		Sentiment prediction of contemporary music can have a wide-range of applications in modern society, for instance, selecting music for public institutions such as hospitals or restaurants to potentially improve the emotional well-being of personnel, patients, and customers, respectively. In this project, music recommendation system built upon on a naive Bayes classifier, trained to predict the sentiment of songs based on song lyrics alone. The experimental results show that music corresponding to a happy mood can be detected with high precision based on text features obtained from song lyrics.	feedback;naive bayes classifier;online machine learning;recommender system;web application	Sebastian Raschka	2016	CoRR		speech recognition;multimedia	Web+IR	-20.21352514679967	-52.45880620304344	74803
0cbbf51c6dfcc71e30349624db673e73a259a5b2	using topic shifts in content-oriented xml retrieval	xml retrieval;xml document	Content-oriented XML retrieval systems support access to XML repositories by retrieving, in response to user queries, XML document components (XML elements) instead of whole documents. The retrieved XML elements should not only contain information relevant to the query, but also should be specific to the given query (i.e. do not discuss other irrelevant topics).  To score XML elements according to how relevant and specific they are given a query, the content and logical structure of XML documents have been widely used. This thesis aims to examine a new source of evidence deriving from the semantic decomposition of XML documents. We consider that XML documents can be semantically decomposed through the application of a topic segmentation algorithm. Using the semantic decomposition and the logical structure of XML documents, we define the notion of topic shifts in an XML element. We then formalise the number of topic shifts to reflect the element's relevance, and more particularly its specificness, to the given user's query.  This thesis investigates the use of topic shifts in content-oriented XML retrieval, which is mainly involved in retrieving information from semi-structured (XML) documents. First, we examine the characteristics of XML elements reflected by their number of topic shifts. Second, we use the number of topic shifts to estimate the relevance of the elements in the collection. Finally, we use topic shifts to provide a focused access to XML documents, which aims to determine not only relevant elements, but those at the right level of granularity.  The main contributions of this thesis are the introduction of topic shifts in the context of content-oriented XML retrieval and the extensive evaluation of the ways this evidence can be employed in retrieving XML elements. This thesis demonstrates that topic shifts in XML elements constitute a useful source of evidence for both improving the ranking of XML elements, and determining elements at the right level of granularity in content-oriented XML retrieval.  The thesis is available online at http://elham.ashoori.org/publications/phd-thesis.pdf.	algorithm;relevance;semi-structured data;semiconductor industry;shlaer–mellor method;text segmentation;xml retrieval	Elham Ashoori	2009	SIGIR Forum	10.1145/1670598.1670614	well-formed document;xml catalog;xml validation;xml encryption;xml namespace;simple api for xml;xml;xml schema;streaming xml;computer science;document type definition;document structure description;xml framework;data mining;xml database;xml schema;database;xml signature;xml schema editor;information retrieval;efficient xml interchange	Web+IR	-28.700616530322296	-62.97754088947337	74818
da6f9b0199c46c4b0afe37a1633e0b60e2bc9fce	multiple factor hierarchical clustering algorithm for large scale web page and search engine clickstream data	multiple factor hierarchical algorithm;information retrieval;multiple criteria decision making;web page clustering;clickstream analysis;k means algorithm	The developments in World Wide Web and the advances in digital data collection and storage technologies during the last two decades allow companies and organizations to store and share huge amounts of electronic documents. It is hard and inefficient to manually organize, analyze and present these documents. Search engine helps users to find relevant information by present a list of web pages in response to queries. How to assist users to find the most relevant web pages from vast text collections efficiently is a big challenge. The purpose of this study is to propose a hierarchical clustering method that combines multiple factors to identify clusters of web pages that can satisfy users’ information needs. The clusters are primarily envisioned to be used for search and navigation and potentially for some form of visualization as well. An experiment on Clickstream data from a processional search engine was conducted to examine the results shown that the clustering method is effective and efficient, in terms of both objective and subjective measures. Copyright Springer Science+Business Media, LLC 2012	algorithm;clickstream;cluster analysis;hierarchical clustering;web page;web search engine	Gang Kou;Chunwei Lou	2012	Annals OR	10.1007/s10479-010-0704-3	site map;clickstream;computer science;data mining;world wide web;information retrieval;search engine;k-means clustering	ML	-33.55841559656102	-55.69153628953204	74842
c0ff3975b8863c481fbc49058c6c3c2f8ed7f617	clustering and summarization topics of subject knowledge through analyzing internal links of wikipedia	pattern clustering;semantic networks;clustering navigation tool intrinsic evaluation methods sna based topic summarization social network analysis read subject related articles semantic relationships semantic relatedness analysis subject based network internal link based web sites wnavis semantics based navigation application wikipedia subject knowledge summarization topics;social networking online pattern clustering semantic networks semantic web;summarization intrinsic evaluation navigation application social network analysis;social networking online;semantic web;encyclopedias electronic publishing internet semantics social network services knowledge engineering	This work introduces a semantics-based navigation application called WNavis. It facilitates informationseeking activities in internal link-based websites within Wikipedia. Our goal is to develop an application that helps users easily find related articles on a given topic and then quickly check the content of articles to explore concepts in Wikipedia. We constructed a subject-based network by analyzing the internal links of Wikipedia and applying a semantic relatedness analysis to measure the strength of the semantic relationships between articles. In order to locate specific information and enable users to quickly explore and read subject-related articles, we propose a social network analysis (SNA)-based topic summarization technique that extracts meaningful sentences from articles. We applied a number of intrinsic evaluation methods to demonstrate the efficacy of the summarization techniques. Our findings have implications for the design of a navigation tool that can help users explore topics and increase their subject knowledge.	automatic summarization;internal link;natural language processing;semantic similarity;social network analysis;wikipedia	I-Chin Wu;Chi-Hong Tsai;Yuquan Lin	2013	2013 IEEE 14th International Conference on Information Reuse & Integration (IRI)	10.1109/IRI.2013.6642458	computer science;artificial intelligence;semantic web;social semantic web;data mining;semantic web stack;database;semantic network;world wide web;information retrieval	Web+IR	-25.97084100657596	-57.95649617132294	74960
0ff900407cd587545cbd45807f755cf640aa1f88	meta-database and search agent for multimedia database access over internet	query processing;information retrieval;web based multimedia information retrieval environment metadatabase search agent multimedia database access internet metaserver global visual access metadata visual content images remote multimedia database database site ranking user query distribution;multimedia computing;internet;distributed databases;internet multimedia computing visual databases distributed databases query processing;multimedia database;multimedia databases internet image databases visual databases spatial databases shape information retrieval web sites frequency computer science;visual databases	In this paper we investigate approaches to the creations of a metadatabase and a search agent in a metasewer which support global visual access to multimedia databases over Internet. The metadata for inclusion within the metadatabase is formulated on the basis of visual content of the images housed at each remote multimedia database. The search agent, located within the metaservel; accesses the metadatabase and derives a ranking of database sites for the distribution of user queries. The proposed metadatabase and search agent are implemented in a web-based multimedia information retrieval environment.	database;information retrieval;internet;metadatabase;web application	Aidong Zhang;Wendy Chang;Deepak Murthy;Tanveer F. Syeda-Mahmood	1997		10.1109/MMCS.1997.609788	the internet;computer science;database;world wide web;distributed database;information retrieval	Web+IR	-29.82228536169244	-52.535224377074776	75010
3de77147f46f9360b5a6d10bb15ba6c0d29c3007	bag-of-foods: analysis of personal foodlogging data		Food has great influence on our health, but at the same time it delights us. Food plays complexed role in our life because of these different aspects, thus dietary preferences should clearly show oneu0027s characteristics. However, since large food-intake record dataset was not obtained, very few researches have attacked analyzing peopleu0027s dietary preferences statistically. Today, recording food-intake, or foodlogging, is becoming popular. There are several services help people do foodlogging in easier process, which enables us to gain access to the large foodlog dataset. In this paper, we focus on analysis of dietary preferences based on nutrition intake using foodlog dataset. We have found that clustering people by average nutrition intake per one meal gives fair result, which proves analyzing preference by nutrition as fair approach. We have also proposed a Bag-of-Words based method, Bag-of-Foods, to represent oneu0027s dietary preference feature, and shown that it certainly represents usersu0027 preference with richer expression than nutrition itself.	bag-of-words model;cluster analysis	Yuji Goda;Sosuke Amano;Yoko Yamakata;Kiyoharu Aizawa	2018		10.1145/3230519.3230596	machine learning;computer science;artificial intelligence;data mining;k-means clustering;bag-of-words model;cluster analysis;meal	Web+IR	-20.528556702783693	-52.11915388684064	75027
19bbba6b213d5c25ca866b34d83236ebd9d23112	automatic term recognition based on data-mining techniques	automatic term recognition;training;training dataset;text analysis automatic term recognition data mining automatic term extraction inductive model term identification statistical rule linguistic rule czech national corpus training dataset feature ranking;text analysis;term identification;text analysis computational linguistics data mining learning by example statistical analysis;statistical rule;data mining;automatic term extraction;corpus linguistic;corpus linguistics automatic term extraction data mining feature ranking;accuracy;learning by example;statistical analysis;linguistic rule;czech national corpus;feature extraction;games;inductive model;mathematical model;corpus linguistics;data mining terminology equations transfer functions computer science data engineering art frequency software performance humans;computational linguistics;feature ranking	We present a new method for automatic term extraction which is based on training datasets created to build inductive models for term identi¿cation. Existing approaches employ simple statistical and linguistic rules designed merely ad-hoc and are unable to utilize complex relations of linguistic units. In contrast to those approaches, our method does not require such manually ascribed rules of extraction. The data for our research is taken from the Czech National Corpus which is lemmatised and morphologically tagged. Statistical information (frequency, distribution etc.) is generated automatically and thus the only expert contribution needed is to label terms in the training dataset.The data mining software creates models that perform the extraction without any further human input. Additionally, feature ranking can serve as valuable aid for understanding of the extraction process and its future development and in terminology research.	data mining;hoc (programming language);terminology extraction	Dominika Srajerova;Oleg Kovárík;Václav Cvrcek	2009	2009 WRI World Congress on Computer Science and Information Engineering	10.1109/CSIE.2009.935	natural language processing;games;feature extraction;computer science;artificial intelligence;computational linguistics;machine learning;corpus linguistics;pattern recognition;mathematical model;data mining;accuracy and precision;statistics	NLP	-24.2647337152393	-64.42040349428147	75030
f38ae6200115e5e1dc709b0ad11284236a069968	a survey of fuzzy web mining	world-wide-web;association rules;documents;quality;algorithms;patterns;internet;systems;things;words	The Internet has become an unlimited resource of knowledge, and is thus widely used in many applications. Web mining plays an important role in discovering such knowledge. This mining can be roughly divided into three categories, including Web usage mining, Web content mining, and Web structure mining. Data and knowledge on the Web may, however, consist of imprecise, incomplete, and uncertain data. Because fuzzy-set theory is often used to handle such data, several fuzzy Web-mining techniques have been proposed to reveal fuzzy and linguistic knowledge. This paper reviews these techniques according to the three Web-mining categories above—fuzzy Web usage mining, fuzzy Web content mining, and fuzzy Web structure mining. Some representative approaches in each category are introduced and compared. C © 2013 Wiley Periodicals, Inc.	big data;cloud computing;computation;computational intelligence;data mining;fuzzy set;internet;john d. wiley;set theory;structure mining;uncertain data;web content;web mining;web resource;world wide web	Chun-Wei Lin;Tzung-Pei Hong	2013	Wiley Interdiscip. Rev. Data Min. Knowl. Discov.	10.1002/widm.1091	concept mining;web mining;text mining;web modeling;computer science;data science;social semantic web;data mining;data stream mining;web intelligence;world wide web	ML	-24.145179768329555	-57.34735114900394	75085
037ec23be930849771cc39971ccedb0cd10508cb	significance of html tags for document indexing and retrieval		Indexing quality has an overwhelming effect on retrieval effectiveness of search engines. In the past few years it has become one of the major challenges in the search engines area, particularly the task of automatically assigning highquality terms to Web documents, which remains elusive. High indexing and retrieval quality requires work on term selection algorithms. This paper investigates the feasibility of HTML tags to represent the contents of Web documents. Experiments were performed on the WT10g collection of a 1.69-million page corpus using many different combinations of term selection and information retrieval algorithms.	html;information retrieval;selection algorithm;web page;web search engine	Byurhan Hyusein;Ahmed Patel	2003			index term;html element;search engine indexing;search engine;information retrieval;document clustering;computer science	Web+IR	-30.846058906032106	-58.51418387437345	75113
0560b48347316d7f9aef29605cac1560aa6fc315	filtering internet image search results towards keyword based category recognition	keyword based category recognition;filtering;search engines;noisy data;information filtering information filters internet image recognition training data search engines filtering algorithms testing noise reduction computer science;training;information filtering;filtering internet image search results;internet image search engines;noise measurement;dataset gathering;computer vision;search engines computer vision information filtering internet;training data;visualization;internet;iterative feature elimination algorithm filtering internet image search results keyword based category recognition internet image search engines dataset gathering holistic image representations;image representation;classification algorithms;image search;robustness;holistic image representations;iterative feature elimination algorithm	In this work we aim to capitalize on the availability of Internet image search engines to automatically create image training sets from user provided queries. This problem is particularly difficult due to the low precision of image search results. Unlike many existing dataset gathering approaches, we do not assume a category model based on a small subset of the noisy data or an ad-hoc validation set. Instead we use a nonparametric measure of strangeness [8] in the space of holistic image representations, and perform an iterative feature elimination algorithm to remove the most strange examples from the category. This is the equivalent of keeping only features that are found to be consistent with others in the class. We show that applying our method to image search data before training improves average recognition performance, and demonstrate that we obtain comparative precision and recall results to the current state of the art, all the while maintaining a significantly simpler approach. In the process we also extend the strangeness-based feature elimination algorithm to automatically select good threshold values and perform filtering of a single class when the background is given.	algorithm;caltech 101;experiment;heuristic;hoc (programming language);holism;image retrieval;internet;iterative method;precision and recall;search engine optimization;selection (genetic algorithm);signal-to-noise ratio;web search engine	Kamil Wnuk;Stefano Soatto	2008	2008 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2008.4587621	filter;statistical classification;computer vision;training set;the internet;visualization;computer science;noise measurement;machine learning;data mining;information retrieval;robustness	Vision	-20.385794554131785	-62.7893759499898	75233
88299b14fdf1baea321713861b9a62f0473b273b	a comparative study of biomedical named entity recognition methods based machine learning approach	decision support systems hidden markov models support vector machines niobium entropy proteins markov processes;dt bioner biones machine learning memm hmm crfs me svm nb;support vector machines;statistics data mining learning artificial intelligence medical computing;niobium;hidden markov models;proteins;decision support systems;entropy;markov processes;biomedical named entity recognition methods conditional random field method crf method jnlpba biomedical corpora genia biomedical corpora hybrid based approach statistical based approach rule based approach dictionary based approach bioner biomedical text mining applications machine learning approach	Recognizing Biomedical Named Entities (BioNEs) such as genes, proteins, cells, drugs, diseases, etc. play a vital role in many Biomedical Text Mining applications. BioNER fall into five approaches: Dictionary-Based, Rule-Based, Machine-Learning-Based, Statistical-Based, and Hybrid-Based. Methods Based Machine Learning approach, are more effective than those of other approaches, and therefore have been widely used for learning to recognize BioNEs. In this paper, we present a comparative theoretical and experimental study between seven Machine Learning methods, by summarizing their advantages and weaknesses, and comparing their performance on two standard biomedical Corpora (GENIA and JNLPBA). The obtained results show that CRF outperforms all the other Machine-Learning methods on both corpora. That method (CRF) will be integrated in our future works.	biomedical text mining;conditional random field;dictionary;entity;experiment;machine learning;named-entity recognition;text corpus	Mohammed Rais;Abdelmonaime Lachkar;Abdelhamid Lachkar;Saïd El Alaoui Ouatik	2014	2014 Third IEEE International Colloquium in Information Science and Technology (CIST)	10.1109/CIST.2014.7016641	natural language processing;computer science;machine learning;pattern recognition	SE	-20.980549199528603	-65.87439356045539	75311
9fdc9683568c1a7fafff3b1ff5ab08d36c2f1fbc	indri at trec 2004: terabyte track	document structure;search engine;networks;metadata;information retrieval;queueing theory;query optimization;markup languages;statistical inference;distributed search;operational effectiveness;optimization;efficient query processing;models;hypertext	This paper provides an overview of experiments carried out at the TREC 2004 Terabyte Track using the Indri search engine. Indri is an efficient, effective distributed search engine. Like INQUERY, it is based on the inference network framework and supports structured queries, but unlike INQUERY, it uses language modeling probabilities within the network which allows for added flexibility. We describe our approaches to the Terabyte Track, all of which involved automatically constructing structured queries from the title portions of the TREC topics. Our methods use term proximity information and HTML document structure. In addition, a number of optimization procedures for efficient query processing are explained.	bayesian network;database;distributed web crawling;experiment;html;language model;link analysis;mathematical optimization;query expansion;signal-to-noise ratio;terabyte;web search engine	Donald Metzler;Trevor Strohman;Howard R. Turtle;W. Bruce Croft	2004			query optimization;statistical inference;hypertext;computer science;document structure description;data mining;database;markup language;queueing theory;metadata;world wide web;information retrieval;search engine	Web+IR	-29.664893908019366	-60.791542450268544	75317
284314d8418ba969c3ef2d14d9544e1dc555a5a0	overview of the inex 2011 books and social search track		The goal of the INEX 2011 Books and Social Search Track is to evaluate approaches for supporting users in reading, searching, and navigating book metadata and full texts of digitized books. The investigation is focused around four tasks: 1) the Social Search for Best Books task aims at comparing traditional and user-generated book metadata for retrieval, 2) the Prove It task evaluates focused retrieval approaches for searching books, 3) the Structure Extraction task tests automatic techniques for deriving structure from OCR and layout information, and 4) the Active Reading task aims to explore suitable user interfaces for eBooks enabling reading, annotation, review, and summary across multiple books. We report on the setup and the results of the track.	book;optical character recognition;social search;user interface;user-generated content	Marijn Koolen;Gabriella Kazai;Jaap Kamps;Antoine Doucet;Monica Landoni	2011		10.1007/978-3-642-35734-3_1	computer science;multimedia;world wide web;information retrieval	NLP	-32.085021654655776	-62.70861070996544	75357
19198bf8412ec944d1ac7d3113188a91227ed266	a collaborative filtering approach based on user's reviews	collaborative filtering approach structured metadata recommenders movielens dataset natural language processing sentiment analysis user provided text user reviews;sentiment analysis recommender systems collaborative filtering item representation unstructured information;item representation;motion pictures vocabulary sentiment analysis collaboration vectors feature extraction;collaborative filtering;recommender systems collaborative filtering natural language processing;sentiment analysis;unstructured information;recommender systems	This paper proposes a collaborative filtering approach that uses users' reviews to produce item descriptions that represent a consensus of users regarding items' features. While earlier works focused on using structured metadata to represent items, recent approaches study how to use user-provided text, such as reviews, to produce better insights about the semantics in the content. Some involved problems, such as noise, personal opinions and false information are reduced by an algorithm based on sentiment analysis and natural language processing. We provide an evaluation using the MovieLens dataset, and the results are promising when compared to recommenders based only on structured metadata.	algorithm;baseline (configuration management);collaborative filtering;experiment;movielens;natural language processing;recommender system;sentiment analysis	Rafael Martins D'Addio;Marcelo G. Manzato	2014	2014 Brazilian Conference on Intelligent Systems	10.1109/BRACIS.2014.45	computer science;collaborative filtering;information filtering system;data mining;world wide web;information retrieval;sentiment analysis;recommender system	NLP	-25.887392382347972	-59.94906707955708	75389
46fb0085beb7f46888d0cfa777238e406b10e16b	editorial message: special track on information access and retrieval	mobile agent;data visualisation;information retrieval;information;system architecture;message;ubiquitous computing;indexing;user interface;information need;track;framework	Information Retrieval (IR) aims at modelling, designing and implementing systems able to provide fast and effective content-based access to a large amount of information. Information can be of any kind: textual, visual, or auditory. The aim of such systems is to estimate the relevance of documents to a user information need. This is a very hard and complex task for many different reasons that a large volume of research has attempted to explain and tackle. Nowadays, research in information retrieval is central to the design and development of advanced information access technologies and spans a number of research topics including document modelling, document classification and categorization, system architecture, user interfaces, data visualisation, languages, topic detection, etc.	information access	Fabio Crestani;Gabriella Pasi	2004		10.1145/967900.968111	information needs;search engine indexing;message;track;relevance;document clustering;information;cognitive models of information retrieval;computer science;artificial intelligence;software framework;operating system;machine learning;data mining;mobile agent;database;programming language;user interface;world wide web;computer security;ubiquitous computing;information retrieval;statistics;human–computer information retrieval	Vision	-31.358443903923334	-57.46205343575885	75447
9a049379c07ffc9630cbb1b6fdf43fdea94f6ca6	trec 2017 common core track overview		The primary goal of the TREC Common Core track is three-fold: (a) bring the information retrieval community back into a traditional ad-hoc search task; (b) attract a diverse set of participating runs and build a new test collections using more recently created documents; (c) establish a (new) test collection construction methodology that avoids the pitfalls of depth-k pooling. A number of side-goals are also set, including studying the shortcomings of test collections constructed in the past; experimenting with new ideas for constructing test collections; expand test collections by new participant tasks (ad-hoc/interactive), new relevance judgments (binary/multilevel), new pooling methods, new assessment resources (NIST / crowd-sourcing) and new retrieval systems contributing documents (manual/neural/strong baselines).	crowdsourcing;emoticon;experiment;hoc (programming language);information retrieval;relevance	James Allan;Donna K. Harman;Evangelos Kanoulas;Dan Li;Christophe Van Gysel;Ellen M. Voorhees	2017			information retrieval;data mining;computer science	Web+IR	-31.5916843511107	-62.65758592721049	75484
45faaebfebd3be1ce475382a91c5440ec5e9f5fc	automatically mining parallel corpora for minority languages from web pages	minority languages;extracting content;text analysis;data mining;automatic parallel text identification automatic parallel corpora mining minority languages web pages multilingual natural language processing web bilingual corpora mining bilingual web sites heuristic information extraction chinese mongolian language pair world wide web;support vector machines data mining html web pages feature extraction natural language processing;identifying parallel pairs web mining minority languages parallel corpora extracting content;web sites;web mining;web sites data mining linguistics natural language processing text analysis;parallel corpora;natural language processing;identifying parallel pairs;linguistics	Parallel corpora are indispensable resources for a variety of multilingual natural language processing. This paper describes a system, which mines automatically parallel corpora from web pages. It attempts to overcome the shortage of parallel corpora in minority languages. Learning from the existing technology of mining web bilingual corpora, and combining with the characteristics of minority languages bilingual websites, a method, mining parallel corpora in minority languages based on heuristic information extracted from content, is proposed. Experiments, carried out on the Chinese-Mongolian language pair, show that the system is successful in automatically identifying a significant amount of parallel texts from the World Wide Web.	algorithm;html;heuristic;natural language processing;parallel text;text corpus;time complexity;web page;world wide web	Zede Zhu;Miao Li;Lei Chen;Weihui Zeng	2012	2012 International Conference on Asian Language Processing	10.1109/IALP.2012.29	natural language processing;web mining;text mining;computer science;data mining;world wide web;information retrieval	ML	-24.84433954948269	-66.09111176649981	75544
523db58f1354025dfc149fa333c618ce0ff7b920	a method for detecting local events using the spatiotemporal locality of microblog posts	web media;web mining;web search and information extraction	Purpose – The purpose of this paper is to propose a method to detect local events in real time using Twitter, an online microblogging platform. The authors especially aim at detecting local events regardless of the type and scale. Design/methodology/approach – The method is based on the observation that relevant tweets (Twitter posts) are simultaneously posted from the place where a local event is happening. Specifically, the method first extracts the place where and the time when multiple tweets are posted using a hierarchical clustering technique. It next detects the co-occurrences of key terms in each spatiotemporal cluster to find local events. To determine key terms, it computes the term frequency-inverse document frequency (TFIDF) scores based on the spatiotemporal locality of tweets. Findings – From the experimental results using geotagged tweet data between 9 a.m. and 3 p.m. on October 9, 2011, the method significantly improved the precision of between 50 and 100 per cent at the same recall compared to a baseline method. Originality/value – In contrast to existing work, the method described in this paper can detect various types of small-scale local events as well as large-scale ones by incorporating the spatiotemporal feature of tweet postings and the text relevance of tweets. The findings will be useful to researchers who are interested in real-time event detection using microblogs.	baseline (configuration management);cluster analysis;computer cluster;digraphs and trigraphs;field electron emission;geotagging;hierarchical clustering;locality of reference;locality-sensitive hashing;nl (complexity);numerical aperture;real-time locating system;relevance;sensor;tf–idf	Takuya Sugitani;Masumi Shirakawa;Takahiro Hara;Shojiro Nishio	2015	IJWIS	10.1108/IJWIS-04-2014-0017	web mining;computer science;data mining;internet privacy;world wide web;information retrieval	Web+IR	-24.286371903517193	-53.2813000922555	75605
ad11b33b4315942b722ff981ed738b0909591836	collaborative filtering ensemble	ensemble learning;collaborative filtering;kdd cup;rating prediction;music ratings	This paper provides the solution of the team “commendo” on the Track1 dataset of the KDD Cup 2011 Dror et al.. Yahoo Labs provides a snapshot of their music-rating database as dataset for the competition. We get approximately 260 million ratings from 1 million users on 600k items. Timestamp and taxonomy information are added to the ratings. The goal of the competition was to predict unknown ratings on a testset with RMSE as error measure. Our final submission is a blend of different collaborative filtering algorithms. The algorithms are trained consecutively and they are blended together with a neural network.	algorithm;artificial neural network;collaborative filtering;epoch (reference date);interaction;k-nearest neighbors algorithm;netflix prize;randomized algorithm;singular value decomposition;snapshot (computer storage);sorting;spatial variability;stochastic gradient descent;taxonomy (general)	Michael Jahrer;Andreas Töscher	2012			computer science;data science;collaborative filtering;machine learning;data mining;ensemble learning;world wide web	ML	-19.444020747050796	-53.748171567975454	75663
a00e36f95f0afc5ff5f3914bc5c1b49da43e9377	search and graphical visualization of concepts in document collections using taxonomies	document handling;query processing;taxonomy materials aluminum visualization engines vegetation;trees mathematics;classification;data visualisation;document ranking concept searching concept graphical visualization document collections semantic concepts general taxonomy trees;trees mathematics classification data visualisation document handling query processing	The main idea of this paper is the search for semantic concepts in documents using taxonomies. The concepts to discover are represented by general taxonomy trees which can be combined to express more sophisticated concepts. The proposed algorithm allows the ranking of documents according to the relevance of the queried concepts as well as a graphical representation of the detected concepts inside a document based on a quantified version of the taxonomy trees.	algorithm;computer graphics;graphical user interface;relevance;taxonomy (general)	Andreas Schmidt;Daniel Kimmig;Markus Dickerhof	2013	2013 46th Hawaii International Conference on System Sciences	10.1109/HICSS.2013.472	biological classification;computer science;data mining;database;information retrieval;data visualization;statistics	DB	-28.78829166131862	-53.70893508230043	75684
48c26d5edecb484ec1c34c2b148a1c843ab24327	textual resource acquisition and engineering	encyclopedias electronic publishing resource management dictionaries search problems error analysis information analysis text mining	and engineering J. Chu-Carroll J. Fan N. Schlaefer W. Zadrozny A key requirement for high-performing question-answering (QA) systems is access to high-quality reference corpora from which answers to questions can be hypothesized and evaluated. However, the topic of source acquisition and engineering has received very little attention so far. This is because most existing systems were developed under organized evaluation efforts that included reference corpora as part of the task specification. The task of answering Jeopardy!i questions, on the other hand, does not come with such a well-circumscribed set of relevant resources. Therefore, it became part of the IBM Watsoni effort to develop a set of well-defined procedures to acquire high-quality resources that can effectively support a high-performing QA system. To this end, we developed three procedures, i.e., source acquisition, source transformation, and source expansion. Source acquisition is an iterative development process of acquiring new collections to cover salient topics deemed to be gaps in existing resources based on principled error analysis. Source transformation refers to the process in which information is extracted from existing sources, either as a whole or in part, and is represented in a form that the system can most easily use. Finally, source expansion attempts to increase the coverage in the content of each known topic by adding new information as well as lexical and syntactic variations of existing information extracted from external large collections. In this paper, we discuss the methodology that we developed for IBM Watson for performing acquisition, transformation, and expansion of textual resources. We demonstrate the effectiveness of each technique through its impact on candidate recall and on end-to-end QA performance.	2.5d;carroll morgan (computer scientist);dictionary;domain analysis;domain-specific language;end-to-end principle;error analysis (mathematics);general-purpose markup language;information;iteration;iterative and incremental development;iterative method;mac os x 10.4 tiger;question answering;software quality assurance;source transformation;spatial reference system;text corpus;thomas j. watson research center;watson (computer);web content;wikipedia	Jennifer Chu-Carroll;James Fan;Nico Schlaefer;Wlodek Zadrozny	2012	IBM Journal of Research and Development	10.1147/JRD.2012.2185901	computer science;engineering;data mining;database;programming language;information retrieval	NLP	-31.236954249462674	-65.84673350433592	75699
daca6d3c5630be9d4ca2e1babad55765807b14d2	are you a compatible user? - compatibility of a microblog user with a news article		A novel concept of compatibility between a news article and an online microblog user is introduced, and a framework embodying the concept is proposed. The framework currently proposes to match two factors – user’s interest and user’s sentiment as reflected in the user’s microblog texts – to determine the compatibility. Using Twitter as an example, the framework is instantiated using the RAKE algorithm for topic keyword (for interest) matching and the VADER model-based sentiment scoring algorithm for sentiment matching. Gold standard tests show that considering both interest match and sentiment match improves the accuracy of compatibility decision significantly and that filtering topic keywords based on co-occurrence semantics helps to disambiguate the user’s sentiment match, hence the compatibility decision.	algorithm;f1 score;geolocation;ground truth;hashtag;iteration;match report;rake;sentiment analysis;social media;timeline	Sang-Pil Kim;Byung Suk Lee	2017		10.1007/978-3-319-56541-5_20	microblogging;world wide web;social media;internet privacy;computer science	NLP	-24.92310113731474	-52.36306589867926	75731
28efec4b41b0816051665b2f445882f57e7203b8	online portfolio selection based on the posts of winners and losers in stock microblogs		Online portfolio selection is an application of online learning in the machine learning literature to the financial problem of portfolio selection. The objective of online portfolio selection is to maximize the cumulative return over sequential multiple periods, where two types of online portfolio selection approaches have been proposed, Follow-the-Winner and Follow-the-Loser. Although the former approaches were well studied so far, the latter approaches have been found to outperform the former empirically in recent years. Thus, we propose a new type of Follow-the-Loser portfolio strategy by applying a semi-supervised learning method to the posts in stock microblogs. In microblogs, each stock has a thread of posts, some of which are associated with an emotion such as bullish or bearish. Our method estimates the missing emotions in a supervised learning manner and uses them to predict the stock price.	experiment;online machine learning;semi-supervised learning;semiconductor industry;supervised learning	Shinta Koyano;Kazushi Ikeda	2017	2017 IEEE Symposium Series on Computational Intelligence (SSCI)	10.1109/SSCI.2017.8280902	data mining;supervised learning;microblogging;social media;business;portfolio	AI	-19.22186903791758	-52.50148568662993	75765
193dbe4c5c85b10728e00f32a02d12ea20087b5a	knowledge bases for web content analytics	knowledge bases;information extraction;web;ontologies	The proliferation of knowledge-sharing communities such as Wikipedia and the progress in scalable information extraction from Web and text sources has enabled the automatic construction of very large knowledge bases (KBs). Recent endeavors of this kind include academic research projects such as DBpedia, KnowItAll, Probase, ReadTheWeb, and YAGO, as well as industrial ones such as Freebase, the Google Knowledge Graph, Amazon’s Evi, Microsoft’s Satori, and related efforts at Bloomberg, Walmart, and others. These projects provide automatically constructed KBs of facts about named entities, their semantic classes, and their mutual relationships. They usually contain millions of entities and hundreds of millions of facts about them. Such world knowledge in turn enables cognitive applications and knowledge-centric services like disambiguating naturallanguage text, entity linking, deep question answering, and semantic search and analytics over entities and relations in Web and enterprise data. Prominent examples of how knowledge bases can be harnessed include the Google Knowledge Cards and the IBM Watson question answering system. This tutorial presents state-of-the-art methods, recent advances, research opportunities, and open challenges in the field of knowledge harvesting and its applications. Particular emphasis will be on the twofold role of KBs for big-data analytics: using scalable distributed algorithms for harvesting knowledge from Web and text sources, and leveraging	big data;bloomberg terminal;commonsense knowledge (artificial intelligence);dbpedia;distributed algorithm;entity linking;freebase;information extraction;knowledge graph;knowledge base;named entity;question answering;scalability;semantic search;thomas j. watson research center;watson (computer);web content;wikipedia;world wide web;yago	Johannes Hoffart;Nicoleta Preda;Fabian M. Suchanek;Gerhard Weikum	2015		10.1145/2740908.2741984	web modeling;web standards;computer science;knowledge management;ontology;social semantic web;data mining;knowledge extraction;web intelligence;world wide web;information extraction;information retrieval;semantic analytics	Web+IR	-24.195807153739402	-53.41074610478386	75792
b6f10e1844d26c4507973a7c976baaa9444e168e	high value media monitoring with machine learning		The Gorkana Group provides high quality media monitoring services to its clients. This paper describes an ongoing project aimed at increasing the amount of automation in Gorkana Group’s workflow through the application of machine learning and language processing technologies. It is important that Gorkana Group’s clients should have a very high level of confidence, that, if an article is relevant to one of their briefs, then they will be shown the article. However, delivering this high-quality media monitoring service means that humans are required to read through very large quantities of data, only a small portion of which is typically deemed relevant. The challenge being addressed by the work reported in this paper is how to efficiently achieve such high-quality media monitoring in the face of huge increases in the amount of the data that needs to be monitored. We show that, while machine learning can be applied successfully to this real world business problem, the constraints of the task give rise to a number of interesting challenges.	business requirements;data pre-processing;display resolution;feature vector;high-level programming language;learning to rank;machine learning;mathematical optimization;naive bayes classifier;offset binary;preprocessor;requirement;statistical classification;technical standard	Matti Lyra;Daoud Clarke;Hamish Morgan;Jeremy Reffin;David J. Weir	2013	KI - Künstliche Intelligenz	10.1007/s13218-013-0255-2	simulation;computer science;artificial intelligence;data mining	AI	-20.968025267835465	-53.59135343505466	75819
13faefe64ea4087ed710d53aadacbca6d8c8e527	text mining and sentiment extraction in central bank documents	text mining;semantics;big data;web sites;sentiment analysis;correlation	The deep transformation induced by the World Wide Web (WWW) revolution has thoroughly impacted a relevant part of the social interactions in our present global society. The huge amount of unstructured information available on blogs, forum and public institution web sites puts forward different challenges and opportunities. Starting from these considerations, in this paper we pursue a two-fold goal. Firstly we review some of the main methodologies employed in text mining and for the extraction of sentiment and emotions from textual sources. Secondly we provide an empirical application by considering the latest 20 issues of the Bank of Italy Governor's concluding remarks from 1996 to 2015. By taking advantage of the open source software package R, we show the following: 1) checking the word frequency distribution features of the documents; 2) extracting the evolution of the sentiment and the polarity orientation in the texts; 3) evaluating the evolution of an index for the readability and the formality level of the texts; 4) attempting to measure the popularity gained from the documents in the web. The results of the empirical analysis show the feasibility in extracting the main topics from the considered corpus. Moreover it is shown how to check for positive and negative terms in order to gauge the polarity of statements and whole documents. The evaluation of these synthetic indexes is quite relevant for increasing the transparency of the central banks' communications.	blog;interaction;open-source software;sentiment analysis;synthetic data;text corpus;text mining;www;word lists by frequency;world wide web	Giuseppe Bruno	2016	2016 IEEE International Conference on Big Data (Big Data)	10.1109/BigData.2016.7840784	computer science;data mining;world wide web;information retrieval	DB	-23.937701216409444	-56.619375941507194	75887
03d456e93c980d33205c0d5e7f8cbbc924643d1a	evaluation of text document clustering approach based on particle swarm optimization		Clustering, an extremely important technique in Data Mining is an automatic learning technique aimed at grouping a set of objects into subsets or clusters. The goal is to create clusters that are coherent internally, but substantially different from each other. Text Document Clustering refers to the clustering of related text documents into groups based upon their content. It is a fundamental operation used in unsupervised document organization, text data mining, automatic topic extraction, and information retrieval. Fast and high-quality document clustering algorithms play an important role in effectively navigating, summarizing, and organizing information. The documents to be clustered can be web news articles, abstracts of research papers etc. This paper proposes two techniques for efficient document clustering involving the application of soft computing approach as an intelligent hybrid approach PSO algorithm. The proposed approach involves partitioning Fuzzy C-Means algorithm and K-Means algorithm each hybridized with Particle Swarm Optimization (PSO). The performance of these hybrid algorithms has been evaluated against traditional partitioning techniques (K-Means and Fuzzy C Means).	algorithm;cluster analysis;coefficient;coherence (physics);data mining;data structure;emoticon;external validity;f1 score;information retrieval;k-means clustering;mathematical optimization;mutual information;organizing (structure);particle swarm optimization;phase-shift oscillator;pure function;recommender system;soft computing;swarm intelligence;text corpus;unsupervised learning;web search engine;web search query	Stuti Karol;Veenu Mangat	2013	Central European Journal of Computer Science	10.2478/s13537-013-0104-2	correlation clustering;constrained clustering;data stream clustering;text mining;document clustering;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;cure data clustering algorithm;data mining;cluster analysis;brown clustering;information retrieval;affinity propagation;clustering high-dimensional data;conceptual clustering	Web+IR	-27.1315762988059	-60.154772905179485	75982
8535490d478a075f7bdb396c41488d130fc7a4ab	an approach for cross-media retrieval with cross-reference graph and pagerank	cross reference graph;relevance feedback content based retrieval multimedia databases;cross media retrieval;pagerank;information retrieval content based retrieval feedback software libraries learning systems data mining educational institutions modems information technology technology management;multimedia databases;multimodal data;relevance feedback cross media retrieval cross reference graph pagerank multimodal data;relevance feedback;content based retrieval	In this paper, we propose a novel cross-media retrieval method. The most important feature of it is to integrate the multi-modal data seamlessly via a cross-reference graph, and then based on the graph, it is able to use improved personalized PageRank to calculate how close the media object associates with the query on semantic and content level. It is also able to adjust the cross-reference graph according to user's relevance feedback, which refines the semantic relationship between the media objects, so as to improve the retrieval accuracy progressively. As demonstrated by the experiments, our method achieves satisfactory retrieval efficiency on multi-modal datasets	cross-reference;experiment;media object server;modal logic;multi-core processor;pagerank;personalization;relevance feedback	Yueting Zhuang;Hanhuai Shan;Fei Wu	2006	2006 12th International Multi-Media Modelling Conference	10.1109/MMMC.2006.1651316	relevance;computer science;multimedia;world wide web;information retrieval;human–computer information retrieval	Web+IR	-28.899714341569187	-52.38710088243599	76049
b5822891063aa91d1e3c3a7c6e769a1cd6e45586	one way to cluster documents by meaning using semantic-ball	computers;pattern clustering;document handling;statistical analysis document handling pattern clustering set theory;emergence self organization semantic ball clustering documents by meaning specificity fp growth algorithm without any definition;uncertainty;frequency domain analysis;semantics;semantics clustering algorithms hidden markov models statistical analysis computers frequency domain analysis uncertainty;set theory;hidden markov models;semantic ball;statistical analysis;without any definition;clustering documents by meaning;fp growth algorithm;clustering algorithms;word set sensing document clustering semantic ball meaning processing self organization attribution group relationship statistical analysis fp growth algorithm frequency pattern growth algorithm bottom up method automatic computing ability;emergence self organization;specificity	Semantic technology dealing with meaning is difficult to process meaning as intended by people due to it is flexibility, instability, and even uncertainty. Moreover, there is no explicit definition of how to sense meaning and expression by computer. To overcome these barriers, this paper suggests Semantic-ball, which is a co-occurrence of different word-set, and based on assumptions that Semantic-ball has emergence self-organization attribution, group relationship not pair relationship, and specificity. Therefore this paper focuses on finding Semantic-ball to solve semantic problems. Approach to find Semantic-ball is based on conventional statistical ways, the FP Growth algorithm(Frequency Pattern Growth algorithm)[1], and the bottom-up method. While the previous researches have been based on the definition which used to pin down meaning to solve semantic problems, this paper does not bring any definition. Moreover, Semantic-ball is made only using computing ability automatically. Thus, Semantic-ball helps to sense word and cluster documents by meaning.	algorithm;bottom-up proteomics;emergence;instability;self-organization;sensitivity and specificity	Chang-Yeol Seo	2012	2012 IEEE 13th International Conference on Information Reuse & Integration (IRI)	10.1109/IRI.2012.6303082	natural language processing;uncertainty;computer science;artificial intelligence;machine learning;data mining;database;semantics;semantic property;cluster analysis;world wide web;frequency domain;statistics;set theory	Robotics	-24.19356817101915	-63.83682690929207	76221
0678c73ff855c93f238538735fcecc1ee6bc8379	graph-based ranking algorithms for sentence extraction, applied to text summarization	text summarization task;automatic sentence extraction;graph-based ranking algorithm;innovative unsupervised method;established benchmarks;natural language processing;ranking algorithms;text summarization	This paper presents an innovative unsupervised method for automatic sentence extraction using graphbased ranking algorithms. We evaluate the method in the context of a text summarization task, and show that the results obtained compare favorably with previously published results on established benchmarks.	algorithm;automatic summarization;entity;information;recursion;sentence extraction;text corpus;text graph;unsupervised learning	Rada Mihalcea	2004			natural language processing;text graph;computer science;automatic summarization;pattern recognition;ranking svm;information retrieval;learning to rank	NLP	-26.46059872983253	-65.4276385822723	76331
a2d7f8c00ac1aec50323def9f1ec7e55998fa63d	summarizing information by means of causal sentences through causal graphs	causal representation;causal sentences;causal summarization;causality;causal questions	The objective of this work is to propose a complete system able to extract causal sentences from a set of text documents, select the causal sentences contained, create a causal graph in base to a given concept using as source these causal sentences, and finally produce a text summary gathering all the information connected by means of this causal graph. This procedure has three main steps. The first one is focused in the extraction, filtering and selection of those causal sentences that could have relevant information for the system. The second one is focused on the composition of a suitable causal graph, removing redundant information and solving ambiguity problems. The third step is a procedure able to read the causal graph to compose a suitable answer to a proposed causal question by summarizing the information	causal filter;causal graph;content-control software	Cristina Puente;Alejandro Sobrino;José Angel Olivas;E. Garrido	2017	J. Applied Logic	10.1016/j.jal.2016.11.020	natural language processing;causality;epistemology;pattern recognition;data mining;mathematics	AI	-28.822938215667893	-64.03102339270332	76647
f0625892fb985baf625a84dfaf7c971bb7f0ba26	event photo mining from twitter using keyword bursts and image clustering	event mining;microblog;geo photo tweet;twitter;event photo mining;geotagged image	Twitter is a unique microblogging service which enables people to post and read not only short messages but also photos from anywhere. Since microblogs are different from traditional blogs in terms of timeliness and on-thespot-ness, they include much information on various events over the world. Especially, photos posted to microblogs are useful to understand what happens in the world visually and intuitively. In this paper, we propose a system to discover events and related photos from the Twitter stream. We make use of “geo-photo tweets” which are tweets including both geotags and photos in order to mine various events visually and geographically. Some works on event mining which utilize geotagged tweets have been proposed so far. However, they used no images but only textual analysis of tweet message texts. In this work, we detect events using visual information as well as textual information. In the experiments, we analyzed 17 million geo-photo tweets posted in the United States and 3 million geo-photo tweets posted in Japan with the proposed method, and evaluated the results. We show some examples of detected events and their photos such as “rainbow”, “fireworks” “Tokyo firefly festival” and “Halloween”.	application programming interface;blog;cluster analysis;earthbound;experiment;firefly (cache coherence protocol);flickr;geotagging;real-time clock;sensor	Takamu Kaneko;Keiji Yanai	2016	Neurocomputing	10.1016/j.neucom.2015.02.081	computer science;microblogging;internet privacy;world wide web	Web+IR	-24.026961619324624	-53.08969872262155	76742
2d61e8abec36120a0e0504efa90eb2b57729f5e2	a dimensionality reduction approach for semantic document classification		The curse of dimensionality is a well-recognized problem in the field of document filtering. In particular, this concerns methods where vector space models are utilized to describe the document-concept space. When performing content classification across a variety of topics, the number of different concepts (dimensions) rapidly explodes and as a result many techniques are rendered inapplicable. Furthermore the extent of information represented by each of the concepts may vary significantly. In this paper, we present a dimensionality reduction approach which approximates the user’s preferences in the form of value function and leads to a quick and efficient filtering procedure. The proposed system requires the user to provide preference information in the form of a training set in order to generate a search rule. Each document in the training set is profiled into a vector of concepts. The document profiling is accomplished by utilizing Wikipedia-articles to define the semantic information contained in words which allows them to be perceived as concepts. Once the set of concepts contained in the training set is known, a modified Wilks’ lambda approach is used for dimensionality reduction by ensuring minimal loss of semantic information.	bellman equation;curse of dimensionality;dimensionality reduction;document classification;online and offline;test set;vector graphics;wikipedia	Oskar Ahlgren;Pekka Malo;Ankur Sinha;Pekka J. Korhonen;Jyrki Wallenius	2011			computer science;machine learning;data mining;information retrieval	AI	-20.573482659489095	-61.99238003734057	76762
3a17a550f2f7e1df0b68365615a1dc07fd9f1ca2	structure-based clustering of novels		To date, document clustering by genres or authors has been performed mostly by means of stylometric and content features. With the premise that novels are societies in miniature, we build social networks from novels as a strategy to quantify their plot and structure. From each social network, we extract a vector of features which characterizes the novel. We perform clustering over the vectors obtained, and the resulting groups are contrasted in terms of author and genre.	baseline (configuration management);cluster analysis;document classification;experiment;fingerprint;principle of good enough;social network;stylometry;unsupervised learning	Mariona Coll Ardanuy;Caroline Sporleder	2014			artificial intelligence;mathematics;multimedia;communication	Web+IR	-23.16394420017056	-59.58600285695712	76957
0bee052af002eb197277cd222d62154c7de4ac8a	combating web spam with trustrank	higher-than-deserved ranking;human expert;small set;reputable seed page;web spam page;good page;combating web spam;seed page;world wide web;seed selection;good seed;indexation;web spam;digital libraries	Web spam pages use various techniques to achieve higher-than-deserved rankings in a search engine’s results. While human experts can identify spam, it is too expensive to manually evaluate a large number of pages. Instead, we propose techniques to semiautomatically separate reputable, good pages from spam. We first select a small set of seed pages to be evaluated by an expert. Once we manually identify the reputable seed pages, we use the link structure of the web to discover other pages that are likely to be good. In this paper we discuss possible ways to implement the seed selection and the discovery of good pages. We present results of experiments run on the World Wide Web indexed by AltaVista and evaluate the performance of our techniques. Our results show that we can effectively filter out spam from a significant fraction of the web, based on a good seed set of less than 200 sites.	experiment;hoc (programming language);iterative method;learning to rank;malware;software propagation;spamdexing;spamming;trustrank;web page;web search engine;world wide web	Zoltán Gyöngyi;Hector Garcia-Molina;Jan O. Pedersen	2004			forum spam;digital library;web search engine;computer science;spamdexing;spambot;data mining;database;world wide web;information retrieval	DB	-29.69824494217135	-55.36874505221725	77358
04ed4d6dd13f1a6a115c72ef296e2b8021fdc036	a comparison of document clustering algorithms	document clustering	Document clustering is a widely used strategy for information retrieval and text data mining. This paper describes the preliminary work for ongoing research of document clustering problems. A prototype of a document clustering system has been implemented and some basic aspects of document clustering problems have been studied. Our experimental results demonstrate that the average-link inter-cluster distance measure and TFIDF weighting function are good methods for the document clustering problem. Other investigators have indicated that the bisecting K-means method is the preferred method for document clustering. However, in our research we have found that, whereas the bisecting K-means method has advantages when working with large datasets, a traditional hierarchical clustering algorithm still achieves the best performance for small datasets.	algorithm;cluster analysis;data mining;hierarchical clustering;high-availability cluster;horner's method;information retrieval;k-means clustering;newton's method;prototype;text corpus;tf–idf;weight function	Yong Wang;Julia E. Hodges	2005			correlation clustering;document clustering;fuzzy clustering;flame clustering;canopy clustering algorithm;consensus clustering;cure data clustering algorithm;cluster analysis;single-linkage clustering;brown clustering;dbscan;biclustering;clustering high-dimensional data;conceptual clustering	Web+IR	-27.227076056390707	-59.188018873000104	77528
27ac084d2ba656b0bb44ecc0d868ecefa34b1630	the effectiveness of classification on information retrieval system (case study)		Large amount of unstructured designed information is difficult to deal with. Obtaining specific information is a hard mission and takes a lot of time. Information Retrieval System (IR) is a way to solve this kind of problem. IR is a good mechanism but does not give the perfect solution. Other techniques have been added to IR to develop the result. One of the techniques is text classification. Text classification task is to assign a document to one or more category. It could be done manually or algorithmically. Text classification enhances the output of this process by reducing the results. This study proved that text classification has a positive influence on Information Retrieval Systems.	algorithm;information retrieval;stemming	Maher Abdullah;Mohammed G. H. al Zamil	2018	CoRR		data mining;computer science;information retrieval	Web+IR	-25.422103611245063	-65.50353371940801	77572
8936a073b13aaf4ff2e6b6ab699e3d0b42a155e4	classifying web queries by topic and user intent	search engine;user intent;search engines;real time;web queries;web searching;query classification;web search engine;web search;information navigation	In this research, we investigate a methodology to classify automatically Web queries by topic and user intent. Taking a 20,000 plus Web query data set sectioned by topic, we manually classified each query using a three-level hierarchy of user intent. We note that significant differences in user intent across topics. Results show that user intent (informational, navigational, and transactional) varies by topic (15 to 24 percent depending on the category). We then use this manually classified data set to classify searches in a Web search engine query stream automatically, using an exact match followed by n-gram approach. These approaches have the advantage of being implementable in real time for query classification of Web searches. The implications are that a search engine can improve retrieval performance by more effectively identifying the intent underlying user queries.	n-gram;transaction processing;web query classification;web search engine;web search query	Bernard J. Jansen;Danielle L. Booth	2010		10.1145/1753846.1754140	web service;static web page;query expansion;web query classification;metasearch engine;web design;web search engine;semantic search;computer science;web navigation;web page;database;search analytics;web search query;world wide web;information retrieval;search engine	Web+IR	-32.07480447940052	-54.478368532654166	77587
a50503dc20ccf6d3cf6dd4e69e3a4eaac1f956f8	approximate keyword search in web search engines	keyword search web search search engines dictionaries los angeles council pattern matching web pages computer science query processing sun;search engine;web pages;query processing;search engines;web search engine;indexing method;longest common subsequence;search engines data structures;keyword search;data structures;indexation;approximate matching;query processing web search engines approximate keyword search similarity measurement longest common subsequence model data structure;data structure;similarity measure	We present a new index method to provide approximate keyword search in search engines. Our approximate keyword matching adopts a new similarity measurement called Listance model, which is a variation of the LCS (longest common subsequence) model. Two keywords are considered approximately matched, if their Listance is no more than a predefined parameter k. Suppose the length of keywords A and B are m and n respectively, the Listance between A and B is defined to be max(m, n) - LCS(A, B). The index method uses a new data structure called LBS index (listance bounded subsequence index), which was designed to allow for very fast approximate keyword matching. In the index phase, a collection of keywords is used as a reference dictionary. We transform keywords in the Web pages into a special form to be indexed if they match one of the keywords approximately in the reference dictionary. During the query processing, a similar keyword transformation is conducted to search the approximate index. The experimental result shows that our approach is efficient and can provide approximate keyword search capability that could be practically interesting.	approximation algorithm;data dictionary;data structure;database;location-based service;longest common subsequence problem;search algorithm;web page;web search engine;world wide web	Sun Wu;Hsien-Tsung Chang;Ting-Chao Hsu;Pei-Shin Liu	2006	2006 1st International Conference on Digital Information Management	10.1109/ICDIM.2007.369229	data structure;computer science;database;keyword density;world wide web;information retrieval;search engine	DB	-30.964385288988915	-56.13643844050093	77617
556a62492e6bdc4a5b6a2751a612401b447d6a95	statistical learning methods for profiling analysis: notebook for pan at clef 2015		Author profiling is the task to infer some information about an author by analyzing her/his writing style. It’s application in forensics, business intelligence and psychology makes this topic interesting for researching. In this notebook, we present our baseline approach using SVM and Linear Discriminant Analysis (LDA) classifiers. We analyze features obtained from LIWC dictionaries, these are frequencies of use words by categories, which gives a general view about how the author writes and what he/she is talking about. According the experimental results, those are significant features to differentiate gender, age-group and personality. Although they are relatively few (not more than 100), they allow to discriminate with an acceptable accuracy.	baseline (configuration management);computer forensics;dictionary;linear discriminant analysis;machine learning;profiling (computer programming);support vector machine	Lesly Miculicich Werlen	2015			business intelligence;writing style;support vector machine;linear discriminant analysis;profiling (computer programming);data science;clef;personality;computer science	ML	-21.49371318343184	-59.7667582398618	77674
60af8dc79a6bab13b32da13f1d1ca2d31b49f6c4	a co-training based method for chinese patent semantic annotation	semantic annotation;information extraction;co training;patent mining	Patents are public and scientific literatures protected by the law, and their abstracts highly contained valuable information. Patent's semantic annotation can effectively protect intellectual property rights and promote corporations' scientific research innovation. Currently, automatic patent annotation mainly used supervised machine learning algorithms, which required abundant expensive labeled patent data. Due to lack of enough labeled Chinese patent data, this paper adopted a semi-supervised machine learning method named co-training, which started from a little labeled data. This method combined keyword extraction with list extraction, and incrementally annotated functional clauses in patent abstract. Experiment results indicated this method can gradually improve the recall without sacrificing the precision.	algorithm;co-training;keyword extraction;machine learning;semiconductor industry;supervised learning	Xu Chen;Zhiyong Peng;Cheng Zeng	2012		10.1145/2396761.2398645	patent visualisation;computer science;data science;data mining;database;world wide web;information extraction;information retrieval	NLP	-22.34807490220507	-62.400514149966014	77750
5a7e6665dc6b8db1a131ffaddd1bf32f906b3d71	bayes-recce: a bayesian model for detecting restriction class correspondences in linked open data knowledge bases	dbpedia;linkedmdb;ontology mapping;complex correspondence;class similarity;journal article;extensional approach;correspondence patterns;restriction classes;geonames	Linked Open Data consists of a large set of structured data knowledge bases which have been linked together, typically using equivalence statements. These equivalences usually take the form of owl:sameAs statements linking individuals, but links between classes are far less common Often, the lack of linking between classes is because the relationships cannot be described as elementary one to one equivalences. Instead, complex correspondences referencing multiple entities in logical combinations are often necessary if we want to describe how the classes in one ontology are related to classes in a second ontology. In this paper we introduce a novel Bayesian Restriction Class Correspondence Estimation (Bayes-ReCCE) algorithm, an extensional approach to detecting complex correspondences between classes. Bayes-ReCCE operates by analysing features of matched individuals in the knowledge bases, and uses Bayesian inference to search for complex correspondences between the classes these individuals belong to. Bayes-ReCCE is designed to be capable of providing meaningful results even when only small amounts of matched instances are available. We demonstrate this capability empirically, showing that the complex correspondences generated by Bayes-ReCCE have a median F1 score of over 0.75 when compared against a gold standard set of complex correspondences between Linked Open Data knowledge bases covering the geographical and cinema domains. In addition we discuss how metadata produced by Bayes-ReCCE can be included in the correspondences to encourage reuse by allowing users to make more informed decisions on the meaning of the relationship described in the correspondences.	algorithm;cinema 4d;closed-world assumption;dbpedia;database schema;elementary;entity;f1 score;field (computer science);geonames;intensional logic;jaccard index;knowledge base;linked data;ontology (information science);open world;open-world assumption;semantic integration;sensor;similarity measure;simple knowledge organization system;tbox;toolchain;trinity;turing completeness;unsupervised learning;web ontology language;while	Brian Walshe;Rob Brennan;Declan O'Sullivan	2016	Int. J. Semantic Web Inf. Syst.	10.4018/IJSWIS.2016040102	semantic integration;computer science;artificial intelligence;data mining;database;linguistics;world wide web;algorithm	Web+IR	-29.30359938984412	-65.67303232762329	77786
86d16670eeda239f6f27647159fbab22d9ec0033	exploration of proximity heuristics in length normalization		Ranking functions used in information retrieval are primarily used in the search engines and they are often adopted for various language processing applications. However, features used in the construction of ranking functions should be analyzed before applying it on a data set. This paper gives guidelines on construction of generalized ranking functions with applicationdependent features. The paper prescribes a specific case of a generalized function for recommendation system using feature engineering guidelines on the given data set. The behavior of both generalized and specific functions are studied and implemented on the unstructured textual data. The proximity feature based ranking function has outperformed by 52 % from regular BM25.	feature engineering;heuristic;information retrieval;ranking (information retrieval);recommender system;text corpus;web search engine	Pranav Agrawal	2017	CoRR		ranking;computer science;machine learning;data mining;ranking svm;world wide web;information retrieval	Web+IR	-27.672386473245624	-63.061595396039664	77943
9b7b48a1e54ef71557ffe197897bd9c30112bf1a	a semi-automatic approach for detecting dataset references in social science texts		Today, full-texts of scientific articles are often stored in different locations than the used datasets. Dataset registries aim at a closer integration by making datasets citable but authors typically refer to datasets using inconsistent abbreviations and heterogeneous metadata (e.g. title, publication year). It is thus hard to reproduce research results, to access datasets for further analysis, and to determine the impact of a dataset. Manually detecting references to datasets in scientific articles is time-consuming and requires expert knowledge in the underlying research domain. We propose and evaluate a semi-automatic three-step approach for finding explicit references to datasets in social sciences articles. We first extract pre-defined special features from dataset titles in the da|ra registry, then detect references to datasets using the extracted features, and finally match the references found with corresponding dataset titles. The approach does not require a corpus of articles (avoiding the cold start problem) and performs well on a test corpus. We achieved an F-measure of 0.84 for detecting references in full-texts and an F-measure of 0.83 for finding correct matches of detected references in the da|ra dataset registry.	cold start;f1 score;scientific literature;semiconductor industry;sensor;text corpus	Behnam Ghavimi;Philipp Mayr;Christoph Lange;Sahar Vahdati;Sören Auer	2016	Inf. Services and Use	10.3233/ISU-160816	computer science;data science;data mining;world wide web;information retrieval	NLP	-29.909429942147113	-65.60671064223055	77947
b786d85d36b98d13d5f318531323fb95b4559cb1	machine learning for query formulation in question answering	universiteitsbibliotheek	Research on question answering dates back to the 1960s but has more recently been revisited as part of TREC's evaluation campaigns, where question answering is addressed as a subarea of information retrieval that focuses on specific answers to a user's information need. Whereas document retrieval systems aim to return the documents that are most relevant to a user's query, question answering systems aim to return actual answers to a users question. Despite this difference, question answering systems rely on information retrieval components to identify documents that contain an answer to a user's question. The computationally more expensive answer extraction methods are then applied only to this subset of documents that are likely to contain an answer. As information retrieval methods are used to filter the documents in the collection, the performance of this component is critical as documents that are not retrieved are not analyzed by the answer extraction component. The formulation of queries that are used for retrieving those documents has a strong impact on the effectiveness of the retrieval component. In this paper, we focus on predicting the importance of terms from the original question. We use model tree machine learning techniques in order to assign weights to query terms according to their usefulness for identifying documents that contain an answer. Term weights are learned by inspecting a large number of query formulation variations and their respective accuracy in identifying documents containing an answer. Several linguistic features are used for building the models, including part-of-speech tags, degree of connectivity in the dependency parse tree of the question, and ontological information. All of these features are extracted automatically by using several natural language processing tools. Incorporating the learned weights into a state-of-the-art retrieval system results in statistically significant improvements in identifying answer-bearing documents.	machine learning;question answering	Christof Monz	2011	Natural Language Engineering	10.1017/S1351324910000276	ranking;question answering;computer science;data mining;database;information retrieval	ML	-28.445640428123717	-64.92117695505733	77964
cdb497525327a8d33a2a3e5e0d14fad071ce1a15	text and data mining to detect phishing websites and spam emails	phishing;spam;text mining;data mining;feature selection	In this paper, we performed phishing and spam detection using textand data mining. For phishing websites detection, we extracted 17 features from the source code and URL of the websites and for spam-email detection we ap-plied text and data mining in tandem. In both studies, we achieved high sensi-tivity compared to previous studies and also provided decision rules.	data mining;email;phishing;spamming	Mayank Pandey;Vadlamani Ravi	2013		10.1007/978-3-319-03756-1_50	spam;forum spam;text mining;phishing;computer science;machine learning;data mining;internet privacy;feature selection;world wide web	ML	-20.04040921954629	-56.04971961554484	78014
573eb74c5f468f3d1c38ce88ed5c74d0d8d2241a	percolatte : a multimodal person discovery system in tv broadcast for the medieval 2015 evaluation campaign		This paper describes the PERCOLATTE participation to MediaEval 2015 task: “Multimodal Person Discovery in Broadcast TV” which requires developing algorithms for unsupervised talking face identification in broadcast news. The proposed approach relies on two identity propagation strategies both based on document chaptering and restricted overlaid names propagation rules. The primary submission shows 10% improvement of Mean Average Precision of the baseline on the INA corpus.	algorithm;baseline (configuration management);discovery system;information retrieval;multimodal interaction;software propagation	Meriem Bendris;Delphine Charlet;Grégory Senay;Minyoung Kim;Benoît Favre;Mickael Rouvier;Frédéric Béchet;Géraldine Damnati	2015			advertising;broadcast television systems;multimedia;geography	NLP	-29.962497269416076	-63.07688799829521	78215
bccf97c75a0ff4d460e24eb0756ab1a7075947c7	semantic analysis of web pages using web patterns	web design semantic analysis web page web pattern extraction;web pages;query processing;search engines;information retrieval;pattern analysis web pages computer science software design performance analysis concrete information analysis data mining tree data structures content management;ontologies artificial intelligence;web design;semantic web;web page;web design information retrieval ontologies artificial intelligence query processing search engines semantic web;semantic analysis;web pattern extraction	This paper introduces a novel method for semantic analysis of Web pages. Analysis is performed with regard to unwritten and empirically proven agreement between users and Web designers using Web patterns. This method is based on extraction of patterns which are characteristics for concrete domain. Patterns provide formalization of the agreement and allow assignment of semantics to parts of Web pages. Experimental results verify the effectives of the proposed method	algorithm;circuit complexity;dictionary;experiment;gestalt psychology;html;hyperlink;meta element;result set;scott continuity;semantic analysis (compilers);web design;web page	Milos Kudelka;Václav Snásel;Ondrej Lehecka;Eyas El-Qawasmeh	2006	2006 IEEE/WIC/ACM International Conference on Web Intelligence (WI 2006 Main Conference Proceedings)(WI'06)	10.1109/WI.2006.153	web service;web mining;static web page;web development;web modeling;data web;web mapping;web design;web standards;computer science;semantic web;web navigation;social semantic web;web page;semantic web stack;database;web intelligence;web search query;web 2.0;world wide web;website parse template;information retrieval	SE	-28.828788789336446	-54.40136960734626	78338
f7d0c0a04069e4526f1e1b668158632527e5f084	comparing overall and targeted sentiments in social media during crises		The tracking of citizens’ reactions in social media during crises has attracted an increasing level of interest in the research community. In particular, sentiment analysis over social media posts can be regarded as a particularly useful tool, enabling civil protection and law enforcement agencies to more effectively respond during this type of situation. Prior work on sentiment analysis in social media during crises has applied well-known techniques for overall sentiment detection in posts. However, we argue that sentiment analysis of the overall post might not always be suitable, as it may miss the presence of more targeted sentiments, e.g. about the people and organizations involved (which we refer to as sentiment targets). Through a crowdsourcing study, we show that there are marked differences between the overall tweet sentiment and the sentiment expressed towards the subjects mentioned in tweets related to three crises events.	crowdsourcing;sentiment analysis;social media	Saul Vargas;Richard McCreadie;Craig MacDonald;Iadh Ounis	2016			internet privacy	HCI	-21.54566551210163	-55.31038461720047	78381
4e51ffa3ee1f4142bf96d115b2754d84ee286eee	ranking distributed knowledge repositories	ranking approach;knowledge base;promising data source;knowledge repository;centralised retrieval;semantic search challenge series;centralised index;linked data;federated search system;entity-oriented search	Increasingly many knowledge bases are published as Linked Data, driving the need for effective and efficient techniques for information access. Knowledge repositories are naturally organised around objects or entities and constitute a promising data source for entity-oriented search. There is a growing body of research on the subject, however, it is almost always (implicitly) assumed that a centralised index of all data is available. In this paper, we address the task of ranking distributed knowledge repositories—a vital component of federated search systems—and present two probabilistic methods based on generative language modeling techniques. We present a benchmarking testbed based on the test suites of the Semantic Search Challenge series to evaluate our approaches. In our experiments, we show that both our ranking approaches provide competitive performance and offer a viable alternative to centralised retrieval.	centralisation;entity;experiment;federated search;index (publishing);information access;information repository;knowledge base;language model;linked data;relevance;semantic search;sharing economy;testbed	Robert Neumayer;Krisztian Balog;Kjetil Nørvåg	2012		10.1007/978-3-642-33290-6_56	computer science;data mining;database;world wide web	Web+IR	-29.720716464497144	-61.25890736202704	78621
b04290d0f301c90213dd8eed1518eb7b7412ce99	visualizing world-wide web search engine results	world wide web search engine result visualisation;popular www search engines;focus context visualization technique;search engine;visualization web search search engines crawlers information retrieval world wide web uniform resource locators laboratories national electric code reactive power;retrieved documents;keywords;search engines;search space;information retrieval;internet data visualisation search engines;data visualisation;visualization;www queries;query terms;internet;www queries world wide web search engine result visualisation popular www search engines user queries retrieved documents keywords query terms search results focus context visualization technique search space;search results;user queries;world wide web;web search;crawlers;national electric code;focus context visualization;uniform resource locators;reactive power	Most of the popular WWW search engines show the documents that match the users' queries as pages of scrolled lists. If lots of information is retrieved, this is not very user friendly. Moreover, there is no mechanism to easily determine documents linked to the retrieved documents or keywords related to the query terms. The paper presents a system that allows the user to visualize various related information for the search results. We also introduce a focus+context visualization technique for the search space of WWW queries.	web search engine;world wide web	Sougata Mukherjea;Yoshinori Hara	1999		10.1109/IV.1999.781588	ranking;computer science;database;world wide web;information retrieval;search engine	Web+IR	-29.601847231694727	-53.89603392712474	78740
10d854e3db9191beda56850e94c5249c6791318c	the latin music database	qa 76 software;computer programming	In this paper we present the Latin Music Database, a novel database of Latin musical recordings which has been developed for automatic music genre classification, but can also be used in other music information retrieval tasks. The method for assigning genres to the musical recordings is based on human expert perception and therefore capture their tacit knowledge in the genre labeling process. We also present the ethnomusicology of the genres available in the database as it might provide important information for the analysis of the results of any experiment that employs the database.	database;information retrieval;list of online music databases	Carlos Nascimento Silla;Alessandro L. Koerich;Celso A. A. Kaestner	2008			natural language processing;speech recognition;computer science;computer programming;multimedia;world wide web	DB	-24.987388607611184	-62.52340922563498	78837
9b3a7eb7dfae2739351b32468337808e31974440	large-scale hierarchical text classification based on path semantic information	path semantic information;hierarchical structure;large scale hierarchical text classification;text editing classification;semantic representation;cooccurrence probability;odp dataset large scale hierarchical text classification path semantic information error propagation classification errors occurrence probability cooccurrence probability;training;prior information;path semantic representation;classification;strontium;text classification;large scale systems text categorization computer errors bayesian methods computer science information retrieval support vector machines support vector machine classification tv intelligent structures;large scale;semantic information;hierarchical classification;error propagation;classification algorithms;classification error;error rate;support vector machine classification;tv;prior information hierarchical classification error propagation path semantic representation;classification errors;odp dataset;occurrence probability;text categorization;regulators;text editing	Although an improvement of hierarchical text classification can be achieved by using hierarchical structure information, existing hierarchical text classification methods suffer from a problem, namely error propagation (especially in large-scale deep hierarchy). In this paper, we define the concept of path-based semantic vector for the presentation of categories based on which prior information provided by training set can be employed in a classifier-independent way to reduce and further eliminate classification errors. In particular, we first propose the occurrence probability based strategy for hierarchical text classification which can help limit errors rate efficiently. Cooccurrence probability is then introduced to correct the classification errors occurred on higher levels of the hierarchy. Extensive experiments show that our hierarchical classification strategies perform well on ODP dataset, even on deep levels of the hierarchy.	chi;categorization;document classification;entity–relationship model;experiment;international conference on machine learning;propagation of uncertainty;rm-odp;sigkdd;scalability;software propagation;support vector machine;test set;top-down and bottom-up design;world wide web;yang	Feng Gao;Chengrong Wu;Naiwang Guo;Danfeng Zhao	2009	2009 International Conference on Business Intelligence and Financial Engineering	10.1109/BIFE.2009.60	computer science;machine learning;pattern recognition;data mining	Web+IR	-24.760688271201985	-64.4688254442225	78893
09b1e690ee76e1a790f362176b11bcb03dfdabc4	unveiling the political agenda of the european parliament plenary: a topical analysis	conference publication;political speeches;text mining;topic modeling;eu politics;content analysis;dynamic topic modeling;corpora;political speech	This study analyzes political interactions in the European Parliament (EP) by considering how the political agenda of the plenary sessions has evolved over time and the manner in which Members of the European Parliament (MEPs) have reacted to external and internal stimuli when making Parliamentary speeches. It does so by considering the context in which speeches are made, and the content of those speeches. To detect latent themes in legislative speeches over time, speech content is analyzed using a new dynamic topic modeling method, based on two layers of matrix factorization. This method is applied to a new corpus of all English language legislative speeches in the EP plenary from the period 1999-2014. Our findings suggest that the political agenda of the EP has evolved significantly over time, is impacted upon by the committee structure of the Parliament, and reacts to exogenous events such as EU Treaty referenda and the emergence of the Euro-crisis have a significant impact on what is being discussed in Parliament.	bubblegum crash;emergence;expectation propagation;interaction;topic model	Derek Greene;James P. Cross	2015		10.1145/2786451.2786464	natural language processing;text mining;content analysis;computer science;artificial intelligence;text corpus;topic model	NLP	-24.070093392176073	-55.40845317901693	79034
96fc602e9417cd4d72d6a4f0c4bc5c044cb86c8c	searching the world wide web: an evaluation of available tools and methodologies	search engine;information acquisition;information retrieval;world wide web;web search	Search Engines and Classified Directories have become essential tools for locating information on the World Wide Web. A consequence of increasing demand, as the volume of information on the Web has expanded, has been a vast growth in the number of tools available. Each one claims to be more comprehensive, more accurate and more intuitive to use than the last. This paper attempts to organise the available tools into a number of categories, according to their information acquisition and retrieval methods, with the intention of exposing the strengths and weaknesses of the various approaches. The importance and implications of Information Retrieval (IR) techniques are discussed. Description of the evolution of automated tools enables an insight into the aims of recent and future implementations	authentication;categorization;directory (computing);disk mirroring;information retrieval;jenkins;linker (computing);network congestion;precision and recall;relevance;web search engine;world wide web	Charlotte Jenkins;Mike Jackson;Peter Burden;Jon Wallis	1998	Information & Software Technology	10.1016/S0950-5849(97)00061-X	web modeling;web mapping;cognitive models of information retrieval;web standards;computer science;web navigation;data mining;database;web intelligence;web engineering;world wide web;information retrieval;search engine;human–computer information retrieval	Web+IR	-32.237411225802624	-53.53369251336303	79260
55f6ab5d6671f71947fda30d8019a4392d028bea	quantifying the language of schizophrenia in social media		Analyzing symptoms of schizophrenia has traditionally been challenging given the low prevalence of the condition, affecting around 1% of the U.S. population. We explore potential linguistic markers of schizophrenia using the tweets1 of self-identified schizophrenia sufferers, and describe several natural language processing (NLP) methods to analyze the language of schizophrenia. We examine how these signals compare with the widelyused LIWC categories for understanding mental health (Pennebaker et al., 2007), and provide preliminary evidence of additional linguistic signals that may aid in identifying and getting help to people suffering from schizophrenia.	grams;latent dirichlet allocation;machine learning;n-gram;natural language processing;social media;text corpus;text-based (computing)	Margaret Mitchell;Kristy Hollingshead;Glen Coppersmith	2015			cognitive psychology;mental health;natural language processing;schizophrenia;computer science;artificial intelligence;social media;population	NLP	-20.42737178066981	-59.25956508823386	79389
1d4aca5f932551bbde22ccab765d222969affb39	overview of the nlpcc 2017 shared task: single document summarization		In this paper, we give an overview for the shared task at the 6th CCF Conference on Natural Language Processing u0026 Chinese Computing (NLPCC 2017): single document summarization. Document summarization aims at conveying important information and generating significantly short summaries for original long documents. This task focused on summarizing the news articles and released a large corpus, TTNews corpus (TTNews corpus can be downloaded at https://pan.baidu.com/s/1bppQ4z1), which was collected for single document summarization in Chinese. In this paper, we will introduce the task, the corpus, the participating teams and the evaluation results.	automatic summarization;microsoft customer care framework;natural language processing;text corpus	Lifeng Hua;Xiaojun Wan;Lei Li	2017		10.1007/978-3-319-73618-1_84	information retrieval;automatic summarization;computer science	NLP	-31.271599925597265	-63.98935537484338	79483
b0a8d512169453d92fbe44bc77a96bcc9cb9c85f	a survey of techniques for event detection in twitter	twitter data stream;monitoring social media;event detection;event identification;microblogs	Twitter is among the fastest-growing microblogging and online social networking services. Messages posted on Twitter (tweets) have been reporting everything from daily life stories to the latest local and global news and events. Monitoring and analyzing this rich and continuous user-generated content can yield unprecedentedly valuable information, enabling users and organizations to acquire actionable knowledge. This article provides a survey of techniques for event detection from Twitter streams. These techniques aim at finding real-world occurrences that unfold over space and time. In contrast to conventional media, event detection from Twitter streams poses new challenges. Twitter streams contain large amounts of meaningless messages and polluted content, which negatively affect the detection performance. In addition, traditional text mining techniques are not suitable, because of the short length of tweets, the large number of spelling and grammatical errors, and the frequent use of informal and mixed language. Event detection techniques presented in literature address these issues by adapting techniques from various fields to the uniqueness of Twitter. This article classifies these techniques according to the event type, detection task, and detection method and discusses commonly used features. Finally, it highlights the need for public benchmarks to evaluate the performance of different detection approaches and various features.	benchmark (computing);categorization;domain driven data mining;fastest;open research;red;supervised learning;testbed;text mining;unsupervised learning;user-generated content	Atefeh Farzindar;Wael Khreich	2015	Computational Intelligence	10.1111/coin.12017	computer science;microblogging;data mining;internet privacy;world wide web	Web+IR	-22.859842363671795	-54.05488731123467	79498
9793f865018710e04adfdb89cc53901855ceef56	a study of application cosine similarity and hosvd for questionnaire data	hosvd;visualization;rating scale method;questionnaire analysis;cosine similarity		cosine similarity	Yosuke Watanabe;Tomohiro Yoshikawa;Takeshi Furuhashi	2015	JACIII	10.20965/jaciii.2015.p0100	visualization;computer science;data science;machine learning;data mining;vector space model;information retrieval	HCI	-24.73768117533757	-60.62240927084715	79503
e18283e6a3eef6e089ff24fe81a776e40bbff6c2	bubblenet: an innovative exploratory search and summarization interface with applicability in health social media	user interfaces directed graphs health care search engines social networking online;health portal bubblenet innovative exploratory search interface summarization interface health social media visual force directed graph query based search word clouds hierarchical directories topic graphs cardea;visualization electronic mail databases color semantics tag clouds portals	We analyse the application of various interfaces to facilitate exploratory search and summarization of documents, especially BubbleNet, an innovative interface for summarizing corpus that also allows discovery of new knowledge that the user may not have previously been looking for. BubbleNet is a visual force-directed graph that displays an interactive and dynamic network of topics, semantic relationships, and related documents based on a corpus. Our experimental results show that BubbleNet gives a better user experience and faster performance in comparison with other exploratory search and summarization interfaces such as query-based search, word clouds, hierarchical directories, and topic graphs. We also explore the applicability of BubbleNet to Cardea, a health portal under development for patients and medics.	automatic summarization;dbpedia;directed graph;directory (computing);exploratory search;force-directed graph drawing;graph (discrete mathematics);information needs;information retrieval;machine learning;microsoft word for mac;mind;prototype;social media;tag cloud;text corpus;trust metric;user experience;user interface;wikipedia;wordnet	Saeed Mohajeri;Hamman W. Samuel;Osmar R. Zaïane;Davood Rafiei	2016	2016 International Conference on Digital Economy (ICDEc)	10.1109/ICDEC.2016.7563143	computer science;multimedia;world wide web;information retrieval	HCI	-27.144816505568695	-63.57582086672266	79543
5b80e9021f8a804da876283477ef312a735ec90e	a graph based query focused multi-document summarization	query focused summary;multi document summarization;redundancy elimination;universal networking language;spreading activation;semantic graphs	A user’s information need, normally represented as a search query, can be satisfied by creating a query focused coherent and readable summary, by fusing the relevant parts of information from multiple documents. While aggregating the information from multiple documents, the quality of the summary is improved by eliminating redundant information from the document set. In this paper, we focus on removing such redundant information and identifying the essential components from multiple documents (represented as a single global semantic graph), with respect to the given query (represented as a query graph). While the redundancy elimination is carried out using various levels of graph matching which are then indicated through canonical labeling of graphs, the selection of essential components for a query focused summary is performed, through the modified spreading activation theory, where the query graph is also integrated during the spreading activation over the global graph. The proposed system shows significant improvements in generating summaries when compared to other existing summarization systems. A Graph Based Query Focused Multi-Document Summarization	automatic summarization;coherence (physics);human-readable medium;information needs;matching (graph theory);multi-document summarization;spreading activation;web search query	Balaji Jagan;T. V. Geetha;Ranjani Parthasarathi	2014	IJIIT	10.4018/ijiit.2014010102	sargable;query optimization;query expansion;web query classification;ranking;universal networking language;boolean conjunctive query;multi-document summarization;computer science;data mining;database;rdf query language;spreading activation;web search query;world wide web;information retrieval;query language	ML	-28.43820259071231	-63.2584984860167	79896
9a12ce7b5c24631d6dd6e0b87b1c8e8ca4857135	efficient image retrieval system based on differential latent semantic index	image retrieval;latent semantic indexing		image retrieval	Sharmin Nilufar;Liang Chen;A. R. Shoyeb Ahmed Siddique	2005			automatic image annotation;pattern recognition;latent semantic indexing;computer science;artificial intelligence;visual word;image retrieval;information retrieval;probabilistic latent semantic analysis;explicit semantic analysis	Vision	-25.08160956086421	-61.65734318874288	80044
d69a4c7006ae864dd98aa296bb0c2fce2d14c5ef	large scale web-content classification		Web classification is used in many security devices for preventing users to access selected web sites that are not allowed by the current security policy, as well for improving web search and for implementing contextual advertising. There are many commercial web classification services available on the market and a few publicly available web directory services. Unfortunately they mostly focus on English-speaking web sites, making them unsuitable for other languages in terms of classification reliability and coverage. This paper covers the design and implementation of a web-based classification tool for TLDs (Top Level Domain). Each domain is classified by analysing the main domain web site, and classifying it in categories according to its content. The tool has been successfully validated by classifying all the registered it. Internet domains, whose results are presented in this paper.	contextual advertising;directory service;f1 score;internet;library (computing);web application;web search engine	Luca Deri;Maurizio Martinelli;Daniele Sartiano;Loredana Sideri	2015	2015 7th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management (IC3K)		web service;web application security;support vector machine;web mining;web development;web modeling;web query classification;data web;web analytics;web mapping;web design;web standards;computer science;machine learning;web navigation;distributed web crawling;web page;data mining;semantic web stack;web intelligence;web 2.0;world wide web;information retrieval	Web+IR	-29.569680052889755	-55.5943903153449	80141
c5c5d2995a2be667e1dd605f5da6bd41d03b64fd	a phrase-based ontology enabled semantic processing system for web search		Semantic processing system (SPS) is a system that performs phrase search of web content. SPS takes a user query in natural language, converts it to a keyword query, expands the keyword query with synonyms, hypernyms, hyponyms, and meronyms, and presents the keyword query to a search engine. SPS then sifts through the search engine result pages extracting grammatical and semantic information from each page for computing the page’s relevance to the natural language query. SPS' relevance computation uses semantic matching of phrases rather than term-and-document frequency weighting—a method that is most commonly used by existing web search engines. SPS consults an ontology that is both “crowd-sourced,” i.e., built collaboratively and incrementally by the large number of users and “auto-learned,” i.e., contextually inferred from sentences containing desired words. SPS would be suitable for the areas of biomedical literature mining, legal document review and discovery, and news/RSS feed monitoring because these are laden with prose text. We implemented a prototype SPS, experimented with it and demonstrate that SPS outperforms a representative keyword based search engine. The strength of SPS stems from its exploitation of phrase semantics, which is not used in the conventional search engines.	computation;crowdsourcing;ibm 1401 symbolic programming system;list of java keywords;natural language user interface;phrase search;prototype;rss;relevance;semantic matching;web content;web search engine	Joseph Leone;Dong-Guk Shin	2013	Trans. MLDM		ontology-based data integration;semantic computing;data mining;information retrieval;search engine;computer science;semantic search;ontology inference layer;phrase search;natural language user interface;semantic web stack	Web+IR	-29.066054235326494	-63.9348149309638	80188
1a3e67cd33cc895272dcbcde595db5465dca85a4	scholargraph: a chinese knowledge graph of chinese scholars		Scholars and their academic information are widely distributed on the Web. Integrating these information and making association between them can play a catalytic role in academic evaluation and research. Since 2008, Web and Mobile Data Management Laboratory (WAMDM) in Renmin University of China began to collect Chinese literatures in more than 20 academic domains, and build a data integration system called ScholarSpace to automatically integrate the relevant chinese academic information from the chinese scholars and science. Focusing on the chinese scholars, ScholarSpace can give you an academic portrait about a chinese scholar with the form of knowledge graph. So the ScholarSpace can be tranformed into a knowledge graph called ScholarGraph. It includes the scholar information such as the affiliation, publications, teacher-student relationship, etc. ScholarGraph is a subset of the whole knowledge graph generated from the ScholarSpace and is published on the web page of WAMDM. ScholarGraph consists of more than 10,000,000 triples, including more than 9,000,000 entities and 6 relations. It can support the search and query about portrait of Chinese scholars and other relevant applications.	entity;knowledge graph;web page;world wide web	Shuo Wang;Zehui Hao;Xiaofeng Meng;Qiuyue Wang	2018			artificial intelligence;natural language processing;speech recognition;computer science;graph	AI	-26.24887625269183	-57.817356033030535	80273
412f3ad79b12ef2a9ffeccc3d14fcc9aee26aa59	ses: sentiment elicitation system for social media data	document handling;media semantics facebook algorithm design and analysis motion pictures machine learning machine learning algorithms;decision tree;mobile device;machine learning social media sentiment rule;logistic regression;social network;sentiment;machine learning;random forest;social networking online;document sentiment representation ses sentiment elicitation system social media data technological platform textual information social networks microblogging sentiment capturing sentiment summarization sentiment identification system customer reviews emoticons negation word position domain specific words machine learning model face book twitter random forest decision tree neural network logistic regression;social networking online document handling learning artificial intelligence;learning artificial intelligence;user generated content;social media;rule;domain specificity;neural network	Social Media is becoming major and popular technological platform that allows users discussing and sharing information. Information is generated and managed through either computer or mobile devices by one person and consumed by many other persons. Most of these user generated content are textual information, as Social Networks(Face book, Linked In), Microblogging(Twitter), blogs(Blogspot, Word press). Looking for valuable nuggets of knowledge, such as capturing and summarizing sentiments from these huge amount of data could help users make informed decisions. In this paper, we develop a sentiment identification system called SES which implements three different sentiment identification algorithms. We augment basic compositional semantic rules in the first algorithm. In the second algorithm, we think sentiment should not be simply classified as positive, negative, and objective but a continuous score to reflect sentiment degree. All word scores are calculated based on a large volume of customer reviews. Due to the special characteristics of social media texts, we propose a third algorithm which takes emoticons, negation word position, and domain-specific words into account. Furthermore, a machine learning model is employed on features derived from outputs of three algorithms. We conduct our experiments on user comments from Face book and tweets from twitter. The results show that utilizing Random Forest will acquire a better accuracy than decision tree, neural network, and logistic regression. We also propose a flexible way to represent document sentiment based on sentiments of each sentence contained. SES is available online.	accessible surface area;algorithm;artificial neural network;conditional random field;decision tree;emoticon;entity;experiment;logic programming;logistic regression;machine learning;microsoft word for mac;mobile device;numerical analysis;random forest;social media;user-generated content;web application	Kunpeng Zhang;Yu Cheng;Yusheng Xie;Daniel Honbo;Ankit Agrawal;Diana Palsetia;Kathy Lee;Wei-keng Liao;Alok N. Choudhary	2011	2011 IEEE 11th International Conference on Data Mining Workshops	10.1109/ICDMW.2011.153	natural language processing;random forest;social media;computer science;machine learning;decision tree;data mining;mobile device;logistic regression;user-generated content;artificial neural network;sentiment analysis;social network	Web+IR	-22.5029650719138	-57.718859732594396	80316
fd4ccc8f68c6bc08abad7becf36ce4445334fee6	effective and efficient classification of topically-enriched domain-specific text snippets: the tetsc method	text snippets;classification;enrichment;dimensionality reduction;topic models;external data sources	Due to the explosive growth in the amount of text snippets over the past few years and their sparsity of text, organizations are unable to effectively and efficiently classify them, missing out on business opportunities. This paper presents TETSC: the Topically-Enriched Text Snippet Classification method. TETSC aims to solve the classification problem for text snippets in any domain. TETSC recognizes that there are different types of text snippets and, therefore, allows for stop word removal, named-entity recognition, and topical enrichment for the different types of text snippets. TETSC has been implemented in the production systems of a personal finance organization, which resulted in a classification error reduction of over 21%.	gene ontology term enrichment;named-entity recognition;sparse matrix	Marco R. Spruit;Bas Vlug	2015	IJSDS	10.4018/IJSDS.2015070101	biological classification;computer science;machine learning;data mining;topic model;world wide web;information retrieval;dimensionality reduction	NLP	-22.208928766790486	-62.374682743552704	80394
62db931d3af45eaffdf674b9d0a9ac5e91e0c5ea	integrating the document object model with hyperlinks for enhanced topic distillation and information extraction	web pages;information extraction;quality improvement;segmentation;topic distillation;document object model;directed graph;minimum description length principle	Topic distillation is the process of finding authoritative Web pages and comprehensive “hubs” which reciprocally endorse each other and are relevant to a given query. Hyperlinkbased topic distillation has been traditionally applied to a macroscopic Web model where documents are nodes in a directed graph and hyperlinks are edges. Macroscopic models miss valuable clues such as banners, navigation panels, and template-based inclusions, which are embedded in HTML pages using markup tags. Consequently, results of macroscopic distillation algorithms have been deteriorating in quality as Web pages are becoming more complex. We propose a uniform fine-grained model for the Web in which pages are represented by their tag trees (also called their Document Object Models or DOMs) and these DOM trees are interconnected by ordinary hyperlinks. Surprisingly, macroscopic distillation algorithms do not work in the finegrained scenario. We present a new algorithm suitable for the fine-grained model. It can dis-aggregate hubs into coherent regions by segmenting their DOM trees. Mutual endorsement between hubs and authorities involve these regions, rather than single nodes representing complete hubs. Anecdotes and measurements using a 28-query, 366000-document benchmark suite, used in earlier topic distillation research, reveal two benefits from the new algorithm: distillation quality improves, and a by-product of distillation is the ability to extract relevant snippets from hubs which are only partially relevant to the query.	aggregate data;algorithm;benchmark (computing);coherence (physics);directed graph;document object model;embedded system;html;hyperlink;information extraction;limbo;markup language;tag cloud;web page;world wide web	Soumen Chakrabarti	2001		10.1145/371920.372054	document object model;directed graph;computer science;machine learning;web page;data mining;database;hits algorithm;segmentation;world wide web;information extraction;information retrieval	Web+IR	-28.156697642472846	-61.32219617641468	80447
216ea165623fa9af942c16a3ce358cf32878d35e	challenges in creating a multilingual sentiment analysis application for social media mining		of the talk In the past years, there has been an increasing amount of research done in the field of Sentiment Analysis. This was motivated by the growth in the volume of user-generated online data, the information flood in Social Media and the applications Sentiment Analysis has to different fields – Marketing, Business Intelligence, e-Law Making, Decision Support Systems, etc. Although many methods have been proposed to deal with sentiment detection and classification in diverse types of texts and languages, many challenges still arise when passing these methods from the research settings to real-life applications. In this talk, we will describe the manner in which we employed machine translation together with human-annotated data to extend a sentiment analysis system to various languages. Additionally, we will describe how a joint multilingual model that detects and classifies sentiments expressed in texts from Social Media has been developed (at this point for Twitter and Facebook) and demo its use in a real-life application: a project aimed at detecting the citizens’ attitude on Science and Technology.	decision support system;machine translation;real life;sensor;sentiment analysis;social media mining;user-generated content	Alexandra Balahur;Hristo Tanev;Erik Van der Goot	2014			natural language processing;engineering;data science;world wide web;sentiment analysis	AI	-22.80423958027504	-55.30788681933548	80802
320e842b6f8b0d278636051afec94f2b6ca5e9b0	incremental processing of vague queries in interactive retrieval systems	data structure;information retrieval	The application of information retrieval techniques in interactive environments require systems capable of efficiently processing vague queries. To reach reasonable response times, new data structures and algorithms have to be developed. In this paper we describe an approach taking advantage of the conditions of interactive usage and special access paths. To have a reference we investigate text queries and compared our algorithms to the well known Buckley/Lewit algorithm. We achieved significant improvements for the response times.	algorithm;data structure;information retrieval;vagueness;whole earth 'lectronic link	Ulrich Pfeifer;Stefan Pennekamp	1997			relevance (information retrieval);visual word;human–computer information retrieval;information retrieval;document retrieval;data mining;data retrieval;data structure;computer science	Web+IR	-32.60798285761257	-57.94507488786467	80952
496b34ab3a62fdbb8537a7e8018f38ea6c161e3e	a clustering algorithm using twitter user biography	pattern clustering;biographies;clustering;social networking online;web search;twitter;word processing	Our previous work proposed a clustering algorithm to cluster research documents automatically. It used Web hit counts of AND-search on two words as a document vector. Target documents are clustered with a result of k-means clustering method, in which cosine similarity is used to calculate a distance. This paper uses this algorithm to cluster twitter users. However, the twitter users have different characteristics from the research documents. Therefore, we investigate problems of the using our algorithm for twitter users and propose some ideas to resolve it.	algorithm;cluster analysis;cosine similarity;hit (internet);k-means clustering	Masaki Kohana;Shusuke Okamoto;Masaya Kaneko	2013	2013 16th International Conference on Network-Based Information Systems	10.1109/NBiS.2013.70	data stream clustering;document clustering;computer science;data mining;database;internet privacy;cluster analysis;world wide web;clustering high-dimensional data	DB	-26.392276192385253	-56.17503413016221	80989
d22ccf9c0a95471179b8b8550cc885ee06b1cfe1	experiments with dbpedia, wordnet and sentiwordnet as resources for sentiment analysis in micro-blogging		Sentiment Analysis in Twitter has become an important task due to the huge user-generated content published over such media. Such analysis could be useful for many domains such as Marketing, Finance, Politics, and Social. We propose to use many features in order to improve a trained classifier of Twitter messages ; these features extend the feature vector of uni-gram model by the concepts extracted from DBpedia, the verb groups and the similar adjectives extracted from WordNet, the Senti-features extracted using SentiWordNet and some useful domain specific features. We also built a dictionary for emotion icons, abbreviation and slang words in tweets which is useful before extending the tweets with different features. Adding these features has improved the f-measure accuracy 2% with SVM and 4% with NaiveBayes.	blog;dbpedia;dictionary;f1 score;feature vector;sentiment analysis;support vector machine;user-generated content;wordnet	Hussam Hamdan;Frédéric Béchet;Patrice Bellot	2013			natural language processing;computer science;world wide web;information retrieval	NLP	-23.16169501926917	-60.993052617900794	81087
5c760e015ae6cc3b152bd2ab35626979ec77f686	handling uncertainty in relation extraction: a case study on tennis tournament results extraction from tweets	semantic reconciliation;knowledge integration;machine reading	Relation extraction involves different types of uncertainty due to the imperfection of the extraction tools and the inherent ambiguity of unstructured text. In this paper, we discuss several ways of handling uncertainties in relation extraction from social media. Our study case is to extract tennis games' results for two Grand Slam tennis tournaments from tweets. Analysis has been done to find to what extent it is useful to use semantic web, domain knowledge, facts repetition, and authors' trustworthiness to improve the certainty of the extracted relations.	darpa grand challenge;relationship extraction;semantic web;social media;trust (emotion)	Jochem G. J. Verburg;Mena B. Habib;Maurice van Keulen	2015		10.1145/2815833.2816960	knowledge integration;computer science;artificial intelligence;data mining;world wide web	NLP	-23.751665740735255	-59.76385346542053	81147
528a7393a81cb23f9a4488a94f3233e2a42bc16d	visual content structures for wrapper induction in building metasearch systems	retail data processing;hypermedia markup languages;shopping websites visual content structures wrapper induction metasearch systems search engines query submission unsupervised approach html tag information;search engine;information sources;html tag information;query processing;metasearch systems;search engines;shopping websites;web sites hypermedia markup languages query processing retail data processing search engines;metasearch system;indexation;web sites;data record extraction;query submission;vcwi metasearch system wrapper induction web information extraction data record extraction;deep web;unsupervised approach;visual content structures;vcwi;wrapper induction;web information extraction	As there are more and more online sources available on the Web, it becomes very time-consuming, if not impossible, to visit and search all web sites, one by one. Many search engines has been developed to help users find information of their need. However, search engines work poor for online sources whose data are often in deep web, which is not part of surface web indexed by standard search engines. Metasearch is a very popular mechanism to search deep web. Metasearch provides the capability for users to search and access all of the information sources in one query submission. One of the fundamental problems in building metasearch systems is to learn wrappers which extract and integrate data records from query result pages returned from online sources. In this paper, develop an unsupervised approach for wrapper induction that combines visual, content and HTML tag information. Our approach first learns a visual content model that alleviates HTML tag differences among data records, and then finds a tag model from all data records that match the visual content model. Experiment shows that our approach works well for data sets collected from well-known search engines and shopping websites.	adapter pattern;deep web;experiment;html element;surface web;unsupervised learning;web search engine;world wide web;wrapper (data mining)	Jyh-Jong Tsay;Chin-Wen Tsay;Xin-Jie Wang	2010	2010 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology	10.1109/WI-IAT.2010.40	search aggregator;metasearch engine;computer science;data mining;database;web search query;world wide web;information retrieval;deep web;search engine	Web+IR	-29.788160714608235	-54.64161534877345	81260
0b78fcf1a6fd2295aff5eab01a84103dbd0c6d22	identifying modes of user engagement with online news and their relationship to information gain in text		Prior work established the benefits of server-recorded user engagement measures (e.g. clickthrough rates) for improving the results of search engines and recommendation systems. Client-side measures of post-click behavior received relatively little attention despite the fact that publishers have now the ability to measure how millions of people interact with their content at a fine resolution using client-side logging. In this study, we examine patterns of user engagement in a large, client-side log dataset of over 7.7 million page views (including both mobile and non-mobile devices) of 66,821 news articles from seven popular news publishers. For each page view we use three summary statistics: dwell time, the furthest position the user reached on the page, and the amount of interaction with the page through any form of input (touch, mouse move, etc.). We show that simple transformations on these summary statistics reveal six prototypical modes of reading that range from scanning to extensive reading and persist across sites. Furthermore, we develop a novel measure of information gain in text to capture the development of ideas within the body of articles and investigate how information gain relates to the engagement with articles. Finally, we show that our new measure of information gain is particularly useful for predicting reading of news articles before publication, and that the measure captures unique information not available otherwise.	client-side;e-book;image scanner;information gain in decision trees;kerrison predictor;kullback–leibler divergence;mixture model;mobile device;page view;population;recommender system;scalability;server (computing);web content;web search engine	Nir Grinberg	2018		10.1145/3178876.3186180	page view;data mining;extensive reading;recommender system;computer science;search engine;summary statistics;dwell time	Web+IR	-33.553182408845025	-52.397551347098364	81483
25af05944a576f43fc88576ca93c88623db384b1	sentiment analysis: what is the end user's requirement?	sentiment tracking;sentiment summarization;sentiment visualization;5w structurization;natural language;sentiment analysis;visual system	"""In this paper we address the Sentiment Analysis problem from the end user's perspective. An end user might desire an automated at-a-glance presentation of the main points made in a single review or how opinion changes time to time over multiple documents. To meet the requirement we propose a relatively generic opinion 5Ws structurization, further used for textual and visual summary and tracking. The 5W task seeks to extract the semantic constituents in a natural language sentence by distilling it into the answers to the 5W questions: Who, What, When, Where and Why. The visualization system facilitates users to generate sentiment tracking with textual summary and sentiment polarity wise graph based on any dimension or combination of dimensions as they want i.e. """"Who"""" are the actors and """"What"""" are their sentiment regarding any topic, changes in sentiment during """"When"""" and """"Where"""" and the reasons for change in sentiment as """"Why""""."""	natural language;sentiment analysis	Amitava Das;Sivaji Bandyopadhyay;Björn Gambäck	2012		10.1145/2254129.2254173	natural language processing;computer science;data mining;world wide web;sentiment analysis	Web+IR	-23.004269636994433	-60.22275660540672	81495
117481e388aa130aa7cd00020766d9be51bc9b3a	mining natural language answers from the web	web pages;natural language;indexation;document retrieval;named entity;question answering	We present a novel method for mining textual answers in Web pages using semi-structured NL questions and Google for initial document retrieval. We exploit the redundancy on the Web by weighting all identified named entities (NEs) found in the relevant document set based on their occurrences and distributions. The ranked NEs are used as our primary anchors for document indexing, paragraph selection, and answer identification. The latter is dependent on two factors: the overlap of terms at different levels (e.g., tokens and named entities) between queries and sentences, and the relevance of identified NEs corresponding to the expected answer type. The set of answer candidates is further subdivided into ranked equivalent classes from which the final answer is selected. The system has been evaluated using question-answer pairs extracted from a popular German quiz book.	document retrieval;html element;nl (complexity);named entity;named-entity recognition;natural language;nl (format);prototype;query expansion;relevance;semantics (computer science);semiconductor industry;systems design;top-down and bottom-up design;web page;world wide web	Günter Neumann;Feiyu Xu	2004	Web Intelligence and Agent Systems		document retrieval;question answering;computer science;web page;data mining;natural language;world wide web;information retrieval	Web+IR	-29.3475523634801	-64.5491064240745	81514
0d3328264571f22d9425b261454bde3efcf31ece	a classification approach with a reject option for multi-label problems	multi label classification;reject option	We investigate the implementation of multi-label classification algorithms with a reject option, as a mean to reduce the time required to human annotators and to attain a higher classification accuracy on automatically classified samples than the one which can be obtained without a reject option. Based on a recently proposed model of manual annotation time, we identify two approaches to implement a reject option, related to the two main manual annotation methods: browsing and tagging. In this paper we focus on the approach suitable to tagging, which consists in withholding either all or none of the category assignments of a given sample. We develop classification reliability measures to decide whether rejecting or not a sample, aimed at maximising classification accuracy on non-rejected ones. We finally evaluate the trade-off between classification accuracy and rejection rate that can be attained by our method, on three benchmark data sets related to text categorisation and image annotation tasks.	multi-label classification	Ignazio Pillai;Giorgio Fumera;Fabio Roli	2011		10.1007/978-3-642-24085-0_11	computer science;artificial intelligence;machine learning;data mining	AI	-20.79448061174175	-62.53354815410987	81734
e8a4e56c4da01ac65761963f3c3903f150dc4403	a probabilistic model for distributed information retrieval	distributed information retrieval;probabilistic model		information retrieval;statistical model	Christoph Baumgarten	1997		10.1145/258525.258585	statistical model;computer science;vector space model;divergence-from-randomness model	Web+IR	-25.37941826833119	-61.687037543363815	81779
7a4ea32971e67e6f95a920d28464b3d0dd2b470f	relic: entity profiling by using random forest and trustworthiness of a source - technical report		The digital revolution has brought most of the world on the world wide web. The data available on WWW has increased many folds in the past decade. Social networks, online clubs and organisations have come into existence. Information is extracted from these venues about a real world entity like a person, organisation, event, etc. However, this information may change over time, and there is a need for the sources to be up-to-date. Therefore, it is desirable to have a model to extract relevant data items from different sources and merge them to build a complete profile of an entity (entity profiling). Further, this model should be able to handle incorrect or obsolete data items. In this paper, we propose a novel method for completing a profile. We have developed a two phase method-1) The first phase (resolution phase) links records to the queries. We have proposed and observed that the use of random forest for entity resolution increases the performance of the system as this has resulted in more records getting linked to the correct entity. Also, we used trustworthiness of a source as a feature to the random forest. 2) The second phase selects the appropriate values from records to complete a profile based on our proposed selection criteria. We have used various metrics for measuring the performance of the resolution phase as well as for the overall ReLiC framework. It is established through our results that the use of biased sources has significantly improved the performance of the ReLiC framework. Experimental results show that our proposed system, ReLiC outperforms the state-of-the-art.	digital revolution;entity;random forest;trust (emotion);two-phase commit protocol;www;word2vec;world wide web	Shubham Varma;Neyshith Sameer;C. Ravindranath Chowdary	2017	CoRR		data science;machine learning;data mining;world wide web;information retrieval;statistics	AI	-24.470192353135488	-53.758320942882	81854
a6f193abed97b9b2a70548bcd7ae3094e4e3d2a3	extraction of partial xml documents using ir-based structure and contents analysis	search engine;performance evaluation;vector space model;search method;web search engine;internet technology;content analysis;xml document	As Internet technologies develop, XML is becoming widely used as a standard data/document format. Although the use of XML  documents has attracted public attention, the application of IR technologies in XML document retrieval is still in its premature  stage. We foresee that typical XML queries for end-users will be very terse, like those used with current Web search engines.  Therefore, an XML search engine should be able to search appropriate retrieval results using only a few keywords. In this  paper, we introduce a notion of context nodes. Context nodes are used to automatically extract coherent partial documents  without the knowledge of XML document structures. This method is useful because it does not require domain analysts to analyze  DTDs and specify candidate partial documents beforehand. We use the term “context search” to represent search methods which  employ the notion of context node. As an instantiation of context search methods, we have developed algorithms to identify  result partial documents in the vector space model. We made a performance evaluation to verify the effectiveness of our method.  	xml	Kenji Hatano;Hiroko Kinutani;Masatoshi Yoshikawa;Shunsuke Uemura	2001		10.1007/3-540-46140-X_26	xml validation;search engine indexing;simple api for xml;database search engine;xml;metasearch engine;web search engine;xml schema;content analysis;streaming xml;computer science;document structure description;xml framework;database;search analytics;world wide web;vector space model;information retrieval;search engine;efficient xml interchange	DB	-31.41765917437057	-57.748868269853915	81867
0af4c710a7140b1d3b36cb5fb41904e4c917e16b	sifting micro-blogging stream for events of user interest	micro blogging;news;social communication;noisy data;standard deviation;keyphrase extraction;text messaging;semantic search;twitter;bag of words;similarity measure	"""Micro-blogging is a new form of social communication that encourages users to share information about anything they are seeing or doing, the motivation facilitated by the ability to post brief text messages through a variety of devices. Twitter, the most popular micro-blogging tool, is exhibiting rapid growth [3]: up to 11% of online Americans are using Twitter by December 2008, compared to 6% in May 2008. Due to its nature, micro-blogosphere has unique features: (i) It is a source of extremely up-to-date information about what is happening in the world; (ii) It captures the wisdom of millions of people and covers a broad range of domains. These features make micro-blogosphere more than a popular medium of social communication: we believe that it has additionally become a valuable source of extremely up-to-date news on virtually any subject of user interest. Making use of micro-blogosphere in this new role we meet the following challenges: (A) Since any given subject is generally mentioned in the micro-blogging stream on the continuous basis, a method is needed for locating periods of news on this subject. (B) Additionally, even for such periods, stream filtering is required for removing noise and for extracting messages that best describe the news. To address these challenges we make and exploit the following observations: (A) For an arbitrary subject, events that catch user interest gain distinguishably more attention than the average mentioning of the subject resulting in message activity bursts for it. (B) Most of the messages in an activity burst describe common event in close variations - either rephrased or """"retweeted"""" between the users. We demonstrate TweetSieve - a system that allows obtaining news on any given subject by sifting the Twitter stream. Our work is related to frequecy-based analysis applied to blogs [1], but higher latency and lower coverage in blogs makes the analysis less effective than in case of micro-blogs. In TweetSieve demo, the user is able to express the subject of her interest by an arbitrary search string. The system shows the period of events occuring for the subject and outputs tweets that best describe each of the events. Figure 1 shows a screenshot of the system for """"Semantic search"""" as a sample subject. The underlying process consists of two steps: Identifying activity bursts. Counting the messages matching the search string in the stream over time, the frequency curve is constructed. Activity bursts in the curve are identified by taking the periods of frequency exceeding the standard deviation from the average. Selecting messages that best describe news events. For the set of all messages matching the search string in an activity burst, we apply the message-granular variation of our keyphrase extraction algorithm [2] that is specifically suited to efficiently filtering noisy data. The algorithm clusters messages with respect to their similarity to each other and chooses central messages from the most dense clusters. As the similarity measure we use Jaccard coefficient for the """"bag of words"""" representation of messages. The demonstration illustrates the potential of our approach in bringing news acquisition to a new level of promptness and coverage range."""	bag-of-words model;blog;blogosphere;coefficient;jaccard index;screenshot;semantic search;signal-to-noise ratio;similarity measure;string searching algorithm	Maxim N. Grinev;Maria P. Grineva;Alexander Boldakov;Leonid Novak;Andrey Syssoev;Dmitry Lizorkin	2009		10.1145/1571941.1572157	semantic search;news;computer science;machine learning;data mining;internet privacy;standard deviation;world wide web;information retrieval	ML	-24.26007503825058	-53.60749437905756	82122
4bd2ac345dfad9b28f07dfd1d23f6ac7bd73f278	trusting the results in cross-lingual keyword-based image retrieval	image retrieval	This paper gives a brief description of the starting points for the experiments the SICS team has performed in the 2006 interactive CLEF campaign. Our experiments attempted to coax out social behaviour out of assessing usefulness of images for the illustration of a text, but found that the experiment design was not sensitive enough to find reliable results. 1	design of experiments;experiment;image retrieval;swedish institute of computer science;trust (emotion)	Jussi Karlgren;Fredrik Olsson	2006		10.1007/978-3-540-74999-8_30	visual word;image retrieval;computer science;multimedia;world wide web;information retrieval	AI	-32.77391075127812	-62.82206416504605	82189
ce4bb6b6efe627634724ac970c7b771d2942c479	enhancing table of contents extraction by system aggregation		The OCR-ed books usually lack logical structure information, such as chapters, sections. To enrich the navigation experience of users, several approaches have been proposed to extract table of contents (ToC) from digitised books. In this paper, we introduce an aggregation-based method to enhance ToC extraction using system submissions from the ICDAR Book structure extraction competitions (2009, 2011, and 2013). Our experimental results show that the union of two best approaches outperforms the existing approaches using both the title-based and link-based evaluation measures on a dataset of more than 2000 books. By efficiently combining the results of existing systems in an unsupervised way, we consistently beat the state-of-the-art in book structure extraction, with performance improvements that are statistically significant.	book;international conference on document analysis and recognition;optical disc authoring;unsupervised learning	Thi-Tuyet-Hai Nguyen;Antoine Doucet;Mickaël Coustaty	2017	2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)	10.1109/ICDAR.2017.48	information retrieval;pattern recognition;computer science;artificial intelligence;table of contents;text mining	DB	-26.741744070358283	-63.35221752412488	82217
3ce62164260b0959d5c3f32a3c5363164bc62ee1	contextual feature selection for text classification	explotacion texto;filtering;filtrage;named entities;text mining;filtrado;knowledge extraction;fouille texte;classification;text classification;text filtering;feature extraction;extraction connaissances;extraccion conocimiento;feature selection;extraction caracteristique;classification automatique;automatic classification;clasificacion automatica;named entity	We present a simple approach for the classification of ‘‘noisy’’ documents using bigrams and named entities. The approach combines conventional feature selection with a contextual approach to filter out passages around selected features. Originally designed for call for tender documents, the method can be useful for other web collections that also contain non-topical contents. Experiments are conducted on our in-house collection as well as on the 4-Universities data set, Reuters 21578 and 20 Newsgroups. We find a significant improvement on our collection and the 4-Universities data set (10.9% and 4.1%, respectively). Although the best results are obtained by combining bigrams and named entities, the impact of the latter is not found to be significant. 2006 Published by Elsevier Ltd.	bigram;content-control software;document classification;feature selection;http 404;hidden markov model;named entity;named-entity recognition;randomness extractor;web page	François Paradis;Jian-Yun Nie	2007	Inf. Process. Manage.	10.1016/j.ipm.2006.07.006	filter;text mining;feature extraction;biological classification;computer science;machine learning;pattern recognition;data mining;knowledge extraction;feature selection;information retrieval	Web+IR	-21.98978849063467	-64.73595897132675	82264
ad7f82d041961447240a77419ea78ac43afb723e	blogbuster: a tool for extracting corpora from the blogosphere	language technology;web pages;limit set;rule based	This paper presents BlogBuster, a tool for extracting a corpus from the blogosphere. The topic of cleaning arbitrary web pages with the goal of extracting a corpus from web data, suitable for linguistic and language technology research and development, has attracted significant research interest recently. Several general purpose approaches for removing boilerplate have been presented in the literature; however the blogosphere poses additional requirements, such as a finer control over the extracted textual segments in order to accurately identify important elements, i.e. individual blog posts, titles, posting dates or comments. BlogBuster tries to provide such additional details along with boilerplate removal, following a rule-based approach. A small set of rules were manually constructed by observing a limited set of blogs from the Blogger and Wordpress hosting platforms. These rules operate on the DOM tree of an HTML page, as constructed by a popular browser, Mozilla Firefox. Evaluation results suggest that BlogBuster is very accurate when extracting corpora from blogs hosted in the Blogger and Wordpress, while exhibiting a reasonable precision when applied to blogs not hosted in these two popular blogging platforms.	blog;blogger;blogosphere;document object model;firefox;html;language technology;logic programming;plasma cleaning;requirement;text corpus;web page;wordpress	Georgios Petasis;Dimitrios Petasis	2010			language technology;rule-based system;information retrieval;blogosphere;web page;boilerplate text;small set;data mining;computer science;document object model	ML	-27.794328808524323	-56.486764982263956	82308
cc9ea6ee82fd5793e76ee0729af84464919d99b5	leveraging unstructured data to detect emerging reliability issues	reliability;text mining;reliability customer complaints emerging issues text mining;sales management data mining reliability;emerging issues;reliability text mining synthetic aperture sonar algorithm design and analysis power steering vehicles matrix decomposition;customer complaints;text mining concepts unstructured data reliability issues after sales service businesses customer complaints technician comments	Unstructured data refers to information that does not have a predefined data model or is not organized in a predefined manner [1]. Loosely speaking, unstructured data refers to text data that is generated by humans. In aftersales service businesses, there are two main sources of unstructured data: customer complaints, which generally describe symptoms, and technician comments, which outline diagnostics and treatment information. A legitimate customer complaint can eventually be tracked to a failure or a claim. However, there is a delay between the time of a customer complaint and the time of a failure or a claim. A proactive strategy aimed at analyzing customer complaints for symptoms can help service providers detect reliability problems in advance and initiate corrective actions such as recalls. This paper introduces essential text mining concepts in the context of reliability analysis and a method to detect emerging reliability issues. The application of the method is illustrated using a case study.	data model;text corpus;text mining	Deovrat Kakde;Arin Chaudhuri	2015	2015 Annual Reliability and Maintainability Symposium (RAMS)	10.1109/RAMS.2015.7105093	text mining;computer science;artificial intelligence;data science;data mining;reliability;customer intelligence;statistics	OS	-24.947404889715923	-63.573304294782496	82404
eed021867701b3b946692f6b3aea4a12e0e92c58	mining conversations of geographically changing users	conversations;inverse document frequency;text mining;communication systems;term frequency;movement pattern;geographic;kommunikationssystem;social media	In recent disaster events, social media has proven to be an effective communication tool for affected people. The corpus of generated messages contains valuable information about the situation, needs, and locations of victims. We propose an approach to extract significant aspects of user discussions to better inform responders and enable an appropriate response. The methodology combines location based division of users together with standard text mining (term frequency inverse document frequency) to identify important topics of conversation in a dynamic geographic network. We further suggest that both topics and movement patterns change during a disaster, which requires identification of new trends. When applied to an area that has suffered a disaster, this approach can provide 'sensemaking' through insights into where people are located, where they are going and what they communicate when moving.	sensemaking;social media;text corpus;text mining;tf–idf	Liam McNamara;Christian Rohner	2012		10.1145/2187980.2188176	text mining;computer science;data mining;database;tf–idf;world wide web	HCI	-23.66824343810452	-54.03894568590138	82627
3b2f16b997c74be187064e5e6cd2895b3524e18c	learning routing queries in a query zone	query zone;routing query	Word usage is domain dependent. A common word in one domain can be quite infrequent in another. In this study we exploit th~ property of word usage to improve document routing. We show that routing queries (profiles) learned only from the documents in a query domain are better than the routing profiles learned when query domains are not used. We approximate a query domain by a guerg zone. Experiments show that routing profiles learned from a query zone are 8–12~0 more effective than the profiles generated when no query zoning is used.	approximation algorithm;routing	Amit Singhal;Mandar Mitra;Chris Buckley	1997		10.1145/258525.258530	sargable;query optimization;query expansion;web query classification;web search query;query language	Web+IR	-28.956698451351823	-60.78372628168402	82790
84bcc5b64ac477468e4e75e5cf17058411aaab5f	parsumist: a persian text summarizer	computers;data mining frequency machine learning explosions computer architecture intelligent systems communication industry mining industry text mining broadcasting;automatic text summarization;information explosion;text analysis;text summarization;multi document summarization;data mining;parsumist;text analysis natural language processing;redundancy;machine learning;feature extraction;persian text summarizer;persian document;persian;ontologies;persian document parsumist persian text summarizer information explosion automatic text summarization;humans;persian automatic text summarization multi document summarization extraction lexical chains;information service;lexical chains;natural language processing;extraction	The rapid growth of online information services causes the problem of information explosion. Automatic text summarization techniques are essential for dealing with this problem. The process of compacting a source document to reduce complexity and length, retaining the most important information is called text summarization. This paper introduces PARSUMIST; a text summarization system for Persian documents. It can generate generic or topic /query-driven extract summaries for single or multiple Persian documents, using a combination of statistical, semantic and heuristic improved methods. In this paper we will first review the related works in this field and especially in Persian text summarization. Then we will present the architecture of PARSUMIST, its components and its features. The last section will evaluate the system and compare it to other existing ones.	automatic summarization;experiment;exploit (computer security);heuristic;information explosion;markov chain;multi-document summarization	Mehrnoush Shamsfard;Tara Akhavan;Mona Erfani Jourabchi	2009	2009 International Conference on Natural Language Processing and Knowledge Engineering	10.1109/NLPKE.2009.5313844	natural language processing;text graph;multi-document summarization;computer science;automatic summarization;data mining;information retrieval	AI	-24.915948261401383	-65.15026345638634	82883
c596647a882aed5c870d76cffafd599c1ca83773	stance classification by recognizing related events about targets	electronic mail;constitution;feature extraction;sentiment analysis;facebook;twitter	"""Recently, many people express their opinions using social networking services such as Twitter and Facebook. Each opinion has a stance related to something such as product, service, and politics. The task of detecting a stance is known as sentiment analysis, reputation mining, and stance detection. A popular approach for stance detection uses sentiment polarity towards a target in a text. This approach is known as targeted sentiment analysis. If a target appears in text, the detecting stance based on targeted sentiment polarity would work well. However, how can we detect stance towards an event? (e.g. """"I cannot understand why man can marry only with a woman"""", """"The problem of low birth rate becomes more severe"""" to the event """"Allowing same-sex marriage""""). To detect these stances, it is necessary to recognize a situation in which the event occurs or does not occur. To classify texts including these phenomena, we propose a classification method based on machine learning considering PRIOR-SITUATION and EFFECT."""	anomaly detection;machine learning;sensor;sentiment analysis;wikipedia	Akira Sasaki;Junta Mizuno;Naoaki Okazaki;Kentaro Inui	2016	2016 IEEE/WIC/ACM International Conference on Web Intelligence (WI)	10.1109/WI.2016.0100	feature extraction;computer science;machine learning;data mining;world wide web;sentiment analysis	NLP	-20.859794515847717	-56.30950878505668	83346
4f0996bb1bafa5060a83f857d1a20ecc350942f5	automatic taxonomy extraction using google and term dependency	databases;information systems;indexing terms;data mining;taxonomy data mining ontologies databases terminology matrix converters information systems machine intelligence semantic web pattern analysis;user profiling;clustering;machine intelligence;taxonomy;semantic web;matrix converters;mutual information;terminology;ontologies;pattern analysis;measures of information;adjacency matrix;incremental web mining	An automatic taxonomy extraction algorithm is proposed. Given a set of terms or terminology related to a subject domain, the proposed approach uses Google page count to estimate the dependency links between the terms. A taxonomic link is an asymmetric relation between two concepts. In order to extract these directed links, neither mutual information nor normalized Google distance can be employed. Using the new measure of information theoretic inclusion index, term dependency matrix, which represents the pair-wise dependencies, is obtained. Next, using a proposed algorithm, the dependency matrix is converted into an adjacency matrix, representing the taxonomy tree. In order to evaluate the performance of the proposed approach, it is applied to several domains for taxonomy extraction.	adjacency matrix;algorithm;automatic taxonomy construction;design structure matrix;information extraction;mutual information;normalized google distance;theory	Mingliang Zhu;Weiming Hu;Xi Li;Ou Wu	2007	IEEE/WIC/ACM International Conference on Web Intelligence (WI'07)	10.1109/WI.2007.26	index term;computer science;ontology;artificial intelligence;semantic web;data mining;database;cluster analysis;mutual information;terminology;world wide web;information retrieval;information system;adjacency matrix;taxonomy	Robotics	-25.97320219037857	-63.75612665280091	83373
c5ba84f89c2d0e2a2557eed980092b13bab4f338	patmedia: augmenting patent search with content-based image retrieval	content based;retrieval;patmedia;patents;image search	Recently, the intellectual property and information retrieval communities have shown increasing interest in image retrieval, which could augment the current practices of patent search. In this context, this article presents PatMedia search engine, which is capable of retrieving patent images in contentbased manner. PatMedia is evaluated both by presenting results considering information retrieval metrics, as well as realistic patent search scenarios.	content-based image retrieval;information retrieval;web search engine	Stefanos Vrochidis;Anastasia Moumtzidou;Gerard Ypma;Yiannis Kompatsiaris	2012		10.1007/978-3-642-31274-8_9	patent visualisation;visual word;image retrieval;computer science;concept search;data mining;adversarial information retrieval;world wide web;information retrieval;search engine;human–computer information retrieval	AI	-31.17070181038565	-57.002885132249006	83545
9bdb1126c795df87163fdf04b254f517edcb4c70	evaluation of vector space models for medical disorders information retrieval	vector space model;information retrieval;medical disorder;semantic vector model;query expansion	Nowadays, consumers often search online to seek medical and health care information that they need. To improve this access, the ShARe/CLEF eHealth Evaluation Lab (SHEL) organized a shared task on information retrieval for Medical Disorders in 2013. This paper describes our participation in this task. In order to detect latent semantic relevance between queries and webpages about disorders, a semantic vector model based on distributional semantics is used as the information retrieval model. Specifically, variants of random indexing are employed to generate document and term representations. In addition, to reduce the lexical lap between different clinical expressions of the same concept, query expansion is also conducted using the UMLS. A baseline information retrieval method using the vector space model (VSM) and semantic vector models with different random indexing building procedures were developed and evaluated with or without query expansion in the shared task. The best performance was achieved by VSM, with MAP of 0.1480, P@10 of 0.3700 and nDCG@10 of 0.3363. Experimental results indicate that VSM and semantic vector model are complementary, and suggest combining these methods may further improve performance.	baseline (configuration management);distributional semantics;information retrieval;query expansion;random indexing;relevance;viable system model	Yaoyun Zhang;Trevor Cohen;Min Jiang;Buzhou Tang;Hua Xu	2013			query expansion;explicit semantic analysis;computer science;pattern recognition;data mining;vector space model;information retrieval;divergence-from-randomness model	NLP	-25.76884656872259	-61.89565151442161	83718
42a1de05cdb5a0e439c418e6df5d643a8b2b75fe	exploring personal corespace for dataspace management	desktop search tools;folder explorer;dataspace management;fuzzy set theory data handling;electronic mail;web pages;personal dataspace;prototypes;resource spacemodel;data management;personal communication networks resource management prototypes information management psychology conference management knowledge management ontologies design methodology decision support systems;semantic link network;psychology;resource space model personal dataspace personal corespace;data mining;fuzzy set theory;faceted search;personal corespace;fuzzy memory clues;resource space model;objective semantic link;memory based semantic link;ontologies;user behavior;data handling;resource spacemodel personal corespace dataspace management folder explorer desktop search tools fuzzy memory clues semantic link network objective semantic link memory based semantic link;data models	With rapid increment of personal data amount, how to efficiently search Personal DataSpace(PDS) becomes an interesting and promising research topic. Popular methods include folder explorer, desktop search tools, and etc. Because these methods ignore user features, they fail to work well in some cases. For example, sometimes users expect to relocate a personal document based on some fuzzy memory clues, such as its type, access time, and so on. These queries can't be supported well by current personal data management tools.The aim of this paper is to discover effective methods to help users search Personal DataSpace. We take Semantic Link Network(SLN) to describe PDS, and divide the semantic links of PDS into two classes: Objective Semantic Link(OSL) and Memory-based Semantic Link(MSL). Base on MSL, we propose a concept \emph{Personal CoreSpace(PCS)}, which is a classification view of personal resources and is specified as a n-dimensional space based on Resource SpaceModel(RSM). Furthermore we design an ontology of PCS based on user behavior features, and propose a method to design facet search interfaces for users to explore PCS efficiently. We validate the effectiveness of our methods by implementing a prototype system for PCS exploring.		Yukun Li;Xiaofeng Meng	2009	2009 Fifth International Conference on Semantics, Knowledge and Grid	10.1109/SKG.2009.46	data modeling;data management;computer science;ontology;artificial intelligence;operating system;machine learning;group method of data handling;web page;data mining;database;distributed computing;prototype;fuzzy set;world wide web	DB	-29.599577997941427	-52.91122488854339	83819
fff4538fdbb172bd7aa4676c33043fd6cd100c2a	pinpointing locational focus in microblogs	location extraction;named entity recognition;locational focus;point of interest	Extracting the geographical location that a tweet is about is crucial for many important applications ranging from disaster management to recommendation systems. We address the problem of finding the locational focus of tweets that is geographically identifiable on a map. Because of the short, noisy nature of tweets and inherent ambiguity of locations, tweet text alone cannot provide sufficient information for disambiguating the location mentions and inferring the actual location focus being referred to in a tweet. Therefore, we present a novel algorithm that identifies all location mentions from three information sources---tweet text, hashtags, and user profile---and then uses a gazetteer database to infer the most probable locational focus of a tweet. Our novel algorithm has the ability to infer a locational focus that may not be explicitly mentioned in the tweet and determine its most appropriate granularity, e.g., city or country.	algorithm;hashtag;location (geography);recommender system;user profile;word-sense disambiguation	Jie Yin;Sarvnaz Karimi;John Lingad	2014		10.1145/2682862.2682868	point of interest;data mining;internet privacy;world wide web	ML	-24.56914238687136	-52.91245885391071	83864
5328e5dc801d1cf0f997621519f6b31d79021006	sentiment analysis using product review data	data mining and knowledge discovery;database management;mathematical applications in computer science;computational science and engineering;information storage and retrieval;communications engineering networks	Sentiment analysis or opinion mining is one of the major tasks of NLP (Natural Language Processing). Sentiment analysis has gain much attention in recent years. In this paper, we aim to tackle the problem of sentiment polarity categorization, which is one of the fundamental problems of sentiment analysis. A general process for sentiment polarity categorization is proposed with detailed process descriptions. Data used in this study are online product reviews collected from Amazon.com. Experiments for both sentence-level categorization and review-level categorization are performed with promising outcomes. At last, we also give insight into our future work on sentiment analysis.	categorization;natural language processing;sentiment analysis	Xing Fang;Justin Zhijun Zhan	2015	Journal of Big Data	10.1186/s40537-015-0015-2	computer science;data science;data mining;information retrieval;sentiment analysis	NLP	-22.56927673767221	-58.215057320158586	83967
f7f8ed5832bb62389e363636202c7db2533d8f75	using nlp for machine learning of user profiles	information retrieval;information filtering;user profile;machine learning;world wide web;generalization hierarchy;user interaction;natural language processing	As more information becomes available electronically, tools for finding information of interest to users becomes increasingly important. The goal of the research described here is to build a system for generating comprehensible user profiles that accurately capture user interest with minimum user interaction. The research focuses on the importance of a suitable generalization hierarchy and representation for learning profiles which are predictively accurate and comprehensible. In our experiments we evaluated both traditional features based on weighted term vectors as well as subject features corresponding to categories which could be drawn from a thesaurus. Our experiments, conducted in the context of a content-based profiling system for on-line newspapers on the World Wide Web (the IDD News Browser), demonstrate the importance of a generalization hierarchy and the promise of combining natural language processing techniques with machine learning (ML) to address an information retrieval (IR) problem.	experiment;inductive reasoning;information retrieval;machine learning;natural language processing;online and offline;precision and recall;space-filling curve;thesaurus;user profile;world wide web	Eric Bloedorn;Inderjeet Mani	1998	Intell. Data Anal.	10.1016/S1088-467X(98)00003-1	computer science;machine learning;data mining;world wide web;information retrieval	AI	-27.71554070845739	-55.95802212020847	84011
3a72571fd16f53024616c6af3b02eb6a900a8b5d	topic structure mining using temporal co-occurrence	text mining;topic structure mining;pagerank;topic extraction;clustering;graph based algorithms;document similarity	This paper proposes a topic structure mining method for document sets that include time stamps. Topic structure mining is a text mining method that uses the graph structure that represents the document pair similarities in the document set. This method yields not only topic extraction from documents and clustering of documents but also extracts the relationship between clusters and the meaning of each document in the cluster. Our method combines temporal co-occurrence with document similarity in constructing the graph structure. We also report evaluation results and the effectiveness of the proposed method.	cluster analysis;information extraction;structure mining;text mining	Hiroyuki Toda;Hiroyuki Kitagawa;Ko Fujimura;Ryoji Kataoka	2008		10.1145/1352793.1352843	concept mining;text mining;document clustering;computer science;machine learning;pattern recognition;data mining;cluster analysis;information retrieval	Web+IR	-26.330831603342087	-63.91660131926047	84033
f8e4aa31665bded0e297c0442855787e243a7f95	matching web tables to dbpedia - a feature utility study	004 informatik	Relational HTML tables on the Web contain data describing a multitude of entities and covering a wide range of topics. Thus, web tables are very useful for filling missing values in cross-domain knowledge bases such as DBpedia, YAGO, or the Google Knowledge Graph. Before web table data can be used to fill missing values, the tables need to be matched to the knowledge base in question. This involves three matching tasks: table-to-class matching, rowto-instance matching, and attribute-to-property matching. Various matching approaches have been proposed for each of these tasks. Unfortunately, the existing approaches are evaluated using different web table corpora. Each individual approach also only exploits a subset of the web table and knowledge base features that are potentially helpful for the matching tasks. These two shortcomings make it difficult to compare the different matching approaches and to judge the impact of each feature on the overall matching results. This paper contributes to improve the understanding of the utility of different features for web table to knowledge base matching by reimplementing different matching techniques as well as similarity score aggregation methods from literature within a single matching framework and evaluating different combinations of these techniques against a single gold standard. The gold standard consists of class-, instance-, and property correspondences between the DBpedia knowledge base and web tables from the Web Data Commons web table corpus.	dbpedia;entity;html;knowledge graph;knowledge base;missing data;text corpus;world wide web;yago	Dominique Ritze;Christian Bizer	2017		10.5441/002/edbt.2017.20	computer science;data mining;world wide web;information retrieval	Web+IR	-28.745082169764107	-64.56411137838309	84043
9fded69c5641f44ea6cd847f025a68cfb441efbe	event detection from millions of tweets related to the great east japan earthquake using feature selection technique	graph community detection great east japan earthquake feature selection technique social media latent semantic analysis latent dirichlet allocation;electronic mail;great east japan earthquake;big data analysis;event detection;earthquakes;media;time series analysis;social networking online big data data analysis feature selection;social media analysis;feature extraction;clustering algorithms;feature selection;great east japan earthquake event detection social media analysis big data analysis feature selection;feature extraction earthquakes media time series analysis clustering algorithms event detection electronic mail	Social media offers a wealth of insight into howsignificant events -- such as the Great East Japan Earthquake, the Arab Spring, and the Boston Bombing -- affect individuals. The scale of available data, however, can be intimidating: duringthe Great East Japan Earthquake, over 8 million tweets weresent each day from Japan alone. Conventional word vector-based event-detection techniques for social media that use Latent SemanticAnalysis, Latent Dirichlet Allocation, or graph communitydetection often cannot scale to such a large volume of data due to their space and time complexity. To alleviate this problem, we propose an efficient method for event detection by leveraging a fast feature selection algorithm called CWC. While we begin withword count vectors of authors and words for each time slot (inour case, every hour), we extract discriminative words from eachslot using CWC, which vastly reduces the number of features to track. We then convert these word vectors into a time series of vector distances from the initial point. The distance betweeneach time slot and the initial point remains high while an eventis happening, yet declines sharply when the event ends, offeringan accurate portrait of the span of an event. This method makes it possible to detect events from vast datasets. To demonstrateour method's effectiveness, we extract events from a dataset ofover two hundred million tweets sent in the 21 days followingthe Great East Japan Earthquake. With CWC, we can identifyevents from this dataset with great speed and accuracy.	big data;cwc mode;feature selection;latent dirichlet allocation;selection algorithm;sensor;social media;time complexity;time series;word embedding	Takako Hashimoto;Dave Shepard;Tetsuji Kuboyama;Kilho Shin	2015	2015 IEEE International Conference on Data Mining Workshop (ICDMW)	10.1109/ICDMW.2015.248	media;feature extraction;computer science;data science;machine learning;time series;data mining;cluster analysis;feature selection	DB	-23.149126748190596	-52.81219414964955	84267
d7e53e6586c3d219d68ea7b462c0ad0822c7b66a	chinese text categorization via bottom-up weighted word clustering	vector space model;knowledge management;word clustering;text categorization;expert search	Most of the researches on text categorization are focus on using bag of words. Some researches provided other methods for classification such as term phrase, Latent Semantic Indexing, and term clustering. Term clustering is an effective way for classification, and had been proved as a good method for decreasing the dimensions in term vectors. The authors used hierarchical term clustering and aggregating similar terms. In order to enhance the performance, they present a modify indexing with terms in cluster. Their test collection extracted from Chinese NETNEWS, and used the Centroid-Based classifier to deal with the problems of categorization. The results had shown that term clustering is not only reducing the dimensions but also outperform than bag of words. Thus, term clustering can be applied to text classification by using any large corpus, its objective is to save times and increase the efficiency and effectiveness. In addition to performance, these clusters can be considered as conceptual knowledge base, and kept related terms of real world. Chinese Text Categorization via Bottom-Up Weighted Word Clustering	bag-of-words model;categorization;cluster analysis;computer cluster;document classification;knowledge base;statistical classification;top-down and bottom-up design	Yu-Chieh Wu	2015	IJEIS	10.4018/ijeis.2015010104	correlation clustering;fuzzy clustering;computer science;knowledge management;machine learning;pattern recognition;data mining;cluster analysis;brown clustering;vector space model;clustering high-dimensional data;conceptual clustering	Web+IR	-24.80522172363703	-65.21115317314388	84276
44ca8fbdae8d497024cf35bea6119ef76b582e67	exploratory analysis of a terabyte scale web corpus		In this extended abstract we present a preliminary (and at this point incomplete) analysis over the largest publicly accessible web dataset: the Common Crawl Corpus. We measure nine web characteristics from two levels of granularity using MapReduce and we comment on the initial observations which will be concluded with the final version of the paper. To the best of our knowledge two of the characteristics, the language distribution and the HTML version of pages have not been analyzed in previous work, while the specific dataset has been only analyzed on page level.	exploratory testing;html;mapreduce;terabyte	Vassilis Kolias;Ioannis Anagnostopoulos;Eleftherios Kayafas	2014	CoRR		computer science;data mining;world wide web;information retrieval	NLP	-31.691625606401008	-61.50928271346089	84595
a5561ec43d6334257625b74b006e45fc30b04aa0	opshnn: ontology based personalized searching using hierarchical neural networks evidence combination	personalized searching;ontologies neural networks web pages search engines frequency computer science educational institutions aging scalability convergence;hierarchical neural networks;networks;hierarchical neural;convergence;web pages;neural networks;search engines;aging;personalized search;user profile;weighted concept hierarchy;dempster shafer theory;concept hierarchy;user profiles;ontologies;scalability;computer science;classification accuracy;frequency;dempster shafer theory personalized searching user profiles weighted concept hierarchy hierarchical neural networks	In this paper we propose a novel ontology based personalized searching method called OPSHNN which uses hierarchical neural networks to classify documents into concepts in the reference ontology. The user profile is modeled as a weighted concept hierarchy. We use weighing methods based on the user's surfing pattern to weigh the concepts in the reference ontology. The system adapts itself to the changing interests of the user by means of aging. To overcome the problem of training in cases where insufficient documents are available for a particular concept and to increase the scalability we propose to use two different hierarchical neural network classifiers, each using a different learning function. Their beliefs are combined using Dempster-Shafer theory to eliminate any weaknesses in classification into concepts in the ontology. Results show that our system has superior classification accuracy, convergence and a precision of 16%.	artificial neural network;computation;document classification;ontology learning;personalization;scalability;server (computing);upper ontology;user profile;vergence;web search engine	Thanukrishnan Srinivasan;B. Rakesh;S. Shivashankar;Vaidheeswaran Archana	2006	The Sixth IEEE International Conference on Computer and Information Technology (CIT'06)	10.1109/CIT.2006.133	scalability;convergence;dempster–shafer theory;computer science;ontology;artificial intelligence;data science;machine learning;frequency;web page;data mining;database;ontology-based data integration;process ontology;artificial neural network;computer network	Vision	-29.715801089726202	-58.20701829712335	84717
6c521fd6a9021645625a9e92de48f5644f23cb01	extraction of relevant snippets from web pages using hybrid features	silicon;content extraction;hypermedia markup languages;document handling;svm learning content extraction relevant snippets hybrid feature;web pages;support vector machines;search engines;silicon web pages html support vector machines search engines diseases correlation;object oriented programming;support vector machines document handling hypermedia markup languages internet object oriented programming;html;internet;hybrid feature;diseases;correlation;svm learning;relevant snippets;html structure feature relevant snippets extraction web pages hybrid features web page documents dom structure document object model html page dom layout patterns search engine common pattern svm learning experiment	As the amount of web pages increase, identifying and retrieving distinct contents from the web has increasingly become more and more difficult. The traditional approach for extracting data from web page documents is to analyze the DOM (Document Object Model) structure of a HTML page and find a common pattern. However, the number of possible DOM layout patterns is virtually infinite, which means that there is no common pattern that can be used for all kinds of web pages. In this paper, we focus on the pages that are linked to a search engine and aim to analyze the features of relevant and meaningful contents instead of a common pattern. Three features of relevant snippets are introduced. They are: quantity of text, correlation between snippet and query that is inputted into a search engine, and HTML structure. Nine parameters are used to describe the three features. Also, a SVM learning experiment is conducted to verify the effectiveness of the three features. The results show that the HTML structure feature is the most effective feature which can determine whether a snippet is relevant or not.	document object model;html;web page;web search engine	Jun Zeng;Qingyu Xiong;Junhao Wen;Sachio Hirokawa	2012	2012 IIAI International Conference on Advanced Applied Informatics	10.1109/IIAI-AAI.2012.50	computer science;data mining;world wide web;information retrieval	Web+IR	-28.98062593710449	-54.30347155513345	84772
f3b1c3e0d085112b1d3803f4fd6ae308ca65007f	using a medical thesaurus to predict query difficulty	state-of-the-art method;difficult query;pre-retrieval prediction method;medical domain;search engine;novel predictor;query performance;medical thesaurus;query difficulty	Estimating query performance is the task of predicting the quality of results returned by a search engine in response to a query. In this paper, we focus on pre-retrieval prediction methods for the medical domain. We propose a novel predictor that exploits a thesaurus to ascertain how difficult queries are. In our experiments, we show that our predictor outperforms the state-of-the-art methods that do not use a thesaurus.	experiment;kerrison predictor;quality of results;thesaurus;web search engine	Florian Boudin;Jian-Yun Nie;Martin Dawes	2012		10.1007/978-3-642-28997-2_46	query optimization;web query classification;computer science;data mining;database;information retrieval	NLP	-30.78167674143315	-59.177578784040655	84820
a7da9ecb05ea4abfb50b97bbb7a53bee8a594696	a review of machine learning algorithms for web page classification		The internet contains a large amount of data, to exploit it a special treatment of its content should be done, and a particular web information system should be developed. As a critical step of those systems, the web page classification is proposed. In this paper, we present the characteristics of web page classification, we reviewed the different machine learning algorithms used to categorize web pages. Finally, we track some assumptions from the studied methods.		Lassri Safae;Benlahmar El Habib;Tragha Abderrahim	2018	2018 IEEE 5th International Congress on Information Science and Technology (CiSt)	10.1109/CIST.2018.8596420	the internet;web page;support vector machine;feature extraction;categorization;statistical classification;algorithm;exploit;machine learning;web information system;computer science;artificial intelligence	Metrics	-22.703288593044476	-58.441083811220544	84871
b9b7272416ec9c4cf9e67dde8e51137c19979e0d	multi-filtering method based cross-lingual link discovery		This paper describes cross-lingual link discovery method of ISTIC used in the system evaluation task at NTCIR-9. In this year's evaluation, we participated in cross-lingual link discovery task from English to Chinese. In this paper, we mainly describe our understanding for CLLD, the key techniques of our system, and the evaluation results.		Yingfan Gao;Hongjiao Xu;Junsheng Zhang;Huilin Wang	2011			engineering;data science;data mining;world wide web	NLP	-30.49909895556653	-63.271471652591465	84934
7232c407fd9200b87acff563b67d9c8806f0d6c8	ontology-based natural query retrieval using conceptual graphs	search and retrieval;digital library;conceptual graph;natural language	As compared to the classical library model of printed materials, digital library offers a more efficient way to browse and search for scholarly information in a networked environment. Currently, the most common way of searching and retrieving information from digital libraries is still by means of keyword-based queries. Over the past many years, there have been many attempts to enhance the query formalism to allow users to retrieve information in a more effective manner. Among them, natural query that is expressed using natural language is obviously the most natural form of search requests. However, typical NLP techniques for natural query retrieval suffer from high cost of complexity. In addition, they are also not very effective when dealing with grammatically imprecise search requests. In this paper, we propose a novel ontology-based approach for natural query retrieval of scholarly information using conceptual graphs. The paper will present the proposed ontology-based approach and its experimental results. The proposed approach has achieved some promising initial results.	artificial intelligence;browsing;conceptual graph;digital library;library (computing);natural language processing;printing;semantic web;semantics (computer science)	Thanh Tho Quan;Siu Cheung Hui	2008		10.1007/978-3-540-89197-0_30	natural language processing;conceptual graph;query optimization;query expansion;web query classification;digital library;ranking;computer science;concept search;natural language;web search query;world wide web;information retrieval;query language;search engine	AI	-31.781503203468763	-58.192589374025985	85229
6480c41cd131ade4b97982de2e969158759675fd	generalized framework for syntax-based relation mining	relational data;kernel;support vector machines;support vector machines data mining learning artificial intelligence;text mining;probability density function;frame recognition;kernel methods;joints;data mining;relation extraction;proteins;machine learning;computational linguistic;supervised learning syntax based relation mining data mining generalization relational data machine learning framework automatic relation mining tree roles kernel methods structured data support vector machines text mining datasets propbank framenet;feature extraction;tree structure;semantic role labeling;data mining proteins text mining bioinformatics computational linguistics machine learning tree data structures machine learning algorithms kernel support vector machines;kernel method;frame recognition relation mining kernel methods semantic role labeling;support vector machine;relation mining;learning artificial intelligence;partial transmit sequences;natural experiment;structured data	Supervised approaches to data mining are particularly appealing as they allow for the extraction of complex relations from data objects. In order to facilitate their application in different areas, ranging from protein to protein interaction in bioinformatics to text mining in computational linguistics research, a modular and general mining framework is needed. The major constraint to the generalization process concerns the feature design for the description of relational data. In this paper, we present a machine learning framework for the automatic mining of relations, where the target objects are structurally organized in a tree. Object types are generalized by means of the use of roles, whereas the relation properties are described by means of the underlying tree structure. The latter is encoded in the learning algorithm thanks to kernel methods for structured data, which represent structures in terms of their all possible subparts. This approach can be applied to any kind of data disregarding their very nature. Experiments with support vector machines on two text mining datasets for relation extraction, i.e. the PropBank and FrameNet corpora, show both that our approach is general, and that it reaches state-of-the-art accuracy.	algorithm;bioinformatics;computational linguistics;data mining;experiment;framenet;image scaling;kernel method;machine learning;object type (object-oriented programming);parsing;propbank;relationship extraction;semantic role labeling;steiner tree problem;support vector machine;text corpus;text mining;tree structure	Bonaventura Coppola;Alessandro Moschitti;Daniele Pighin	2008	2008 Eighth IEEE International Conference on Data Mining	10.1109/ICDM.2008.153	concept mining;support vector machine;kernel method;text mining;computer science;machine learning;pattern recognition;data mining;data stream mining;molecule mining	ML	-25.267923097224077	-65.93503536422992	85242
5be6c4f6106697bd222b9c21ff9e9cf75914695b	tfd: a multi-pattern matching algorithm for large-scale url filtering	uniform resource locators pattern matching memory management heuristic algorithms arrays automata indexes;web sites finite state machines information filtering large scale systems pattern matching;matching speed;url filtering;tfd matching speed multipattern matching algorithm large scale url filtering system malicious websites high speed url filtering two phase hash finite state machine double array storage still blacklist filter performance matching speed preprocessing time memory usage large scale url pattern sets single thread;blacklist;large scale;matching speed url filtering blacklist multi pattern matching large scale;multi pattern matching	During the past decade, URL filtering systems have been widely applied to prevent people from browsing undesirable or malicious websites. However, the key method of URL filtering, such as URL blacklist filter, is more challenging due to the limited performance of existing multi-pattern matching algorithms. In this paper, we propose a multi-pattern matching algorithm named TFD for large-scale and high-speed URL filtering. TFD employs Two-phase hash, Finite state machine and Double-array storage to eliminate the performance bottleneck of blacklist filter. Experimental results show that TFD achieves better performance than existing work in terms of matching speed, preprocessing time and memory usage. Specially, on large-scale URL pattern sets (over 10 million URLs), with single thread, TFD's matching speed reaches over 100Mbps on a general x86 platform.	algorithm;finite-state machine;megabit;pattern matching;preprocessor;two-phase locking;x86	Zhenlong Yuan;Baohua Yang;Xiaoqi Ren;Yibo Xue	2013	2013 International Conference on Computing, Networking and Communications (ICNC)	10.1109/ICCNC.2013.6504109	computer science;theoretical computer science;database;world wide web	DB	-29.979968252796294	-55.59715580469932	85351
20ea46be66bbab6ba0da8372da3e49a387b90602	collaborative rating system for web page labeling	web pages;collaboration;internet			Mon-Fong Jiang;Shian-Shyong Tseng;Yao-Tsung Lin	1999			ajax;web application security;static web page;web development;site map;data web;web design;web search engine;web standards;dynamic web page;web navigation;web page;database;same-origin policy;web 2.0;world wide web;website parse template;information retrieval	ECom	-29.67732594566739	-52.76253815578771	85371
562bee6c4941d9fb7dd3f87a1f5bfaf51864adc1	content-based recommender system enriched with wordnet synsets		Content-based recommender systems can overcome many problems related to collaborative filtering systems, such as the new-item issue. However, to make accurate recommendations, content-based recommenders require an adequate amount of content, and external knowledge sources are used to augment the content. In this paper, we use Wordnet synsets to enrich a content- based joke recommender system. Experiments have shown that content-based recommenders using K-nearest neighbors perform better than collaborative filtering, particularly when synsets are used.	recommender system;synonym ring;wordnet	Haifa Alharthi;Diana Inkpen	2015		10.1007/978-3-319-18117-2_22	computer science;data mining;world wide web;information retrieval	ML	-28.49446176337927	-58.53514189035008	85404
8c6a3bf7d9cf58f856c3c096145c3f68e6fe1622	ranking opinionated blog posts using opinionfinder	information retrieval;opinion finding;information search;qa75 electronic computers computer science;blogs	The aim of an opinion finding system is not just to retrieve relevant documents, but to also retrieve documents that express an opinion towards the query target entity. In this work, we propose a way to use and integrate an opinion-identification toolkit, OpinionFinder, into the retrieval process of an Information Retrieval (IR) system, such that opinionated, relevant documents are retrieved in response to a query. In our experiments, we vary the number of top-ranked documents that must be parsed in response to a query, and investigate the effect on opinion retrieval performance and required parsing time. We find that opinion finding retrieval performance is improved by integrating OpinionFinder into the retrieval system, and that retrieval performance grows as more posts are parsed by OpinionFinder. However, the benefit eventually tails off at a deep rank, suggesting that an optimal setting for the system has been achieved.	blog;experiment;information retrieval;parsing;tails	Ben He;Craig MacDonald;Iadh Ounis	2008		10.1145/1390334.1390473	query expansion;ranking;cognitive models of information retrieval;computer science;data mining;world wide web;information retrieval;human–computer information retrieval	Web+IR	-32.45509311842338	-54.46357235934565	85493
447815b7fe1db5c1bd92632d29a0b72c071a29c2	combining query terms extension and weight correlative for expert finding	databases;query processing;information retrieval;vectors databases equations mathematical model educational institutions correlation information retrieval;expert finding;query expansion query term extension weight correlative expert finding problem c value method;tf idf;query expension expert finding c value tf idf;vectors;mathematical model;query expension;correlation;c value	This paper proposes two methods for solving the expert finding problem. In order to enhance the correctness, a C-value method is applied to these methods for query expansion. After query expansion, proposed system calculates correlation between all query terms and experts, and finally outputs a list of experts. The experiment results show that the proposed methods can provide higher precision than the baseline method from 3% to 10%.	baseline (configuration management);correctness (computer science);query expansion	Chen-Tao Chuang;Kai-Hsiang Yang;Yu-Li Lin;Jenq-Haur Wang	2014	2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)	10.1109/WI-IAT.2014.51	sargable;query optimization;query expansion;web query classification;ranking;boolean conjunctive query;computer science;data mining;database;information retrieval	AI	-29.940773856455355	-59.21523391701298	85597
92ebe2da1e1ecaa72e3feceab0feafb885ec6ca1	a faq search training method based on automatically generated questions		We propose a FAQ search method with automatically generated questions by a question generator created from community Qu0026As. In our method, a search model is trained with automatically generated questions and their corresponding FAQs. We conducted experiments on a Japanese Qu0026A dataset created from a user support service on Twitter. The proposed method showed better Mean Reciprocal Rank and Recall@1 than a FAQ ranking model trained with the same community Qu0026As.		Takuya Makino;Tomoya Noro;Hiyori Yoshikawa;Tomoya Iwakura;Satoshi Sekine;Kentaro Inui	2018		10.1007/978-3-030-03520-4_7	data mining;information retrieval;computer science;mean reciprocal rank;ranking	NLP	-24.79235764985304	-66.12078717569992	85606
29904681e7288c248d77596f03e0f00db7a909e7	content based web sampling		Web characterization methods have been studied for many years. Most of these methods focus on textbased web contents. Some of them analyze the contents of a web page by analyzing its HTML code, hyper links, and/or DOM 1 structure. Seldom, a web page is characterized based on its visual appearance. A good reason for also considering the visual appearance of a web page is because humans initially perceive a web page as an image, and only then will look in detail at text and further pictorial contents. Hence it is a more natural way of trying to analyze and classify the contents of the web pages. Moreover, as more and more new web technologies appear in recent years (JavaScript, FLASH 2 , and AJAX 3 ); analyzing the HTML code in a web page seems to be meaningless without actually parsing and interpreting it. This offers new challenges to textual web page characterization and has an impact on the efficiency of the indexing techniques. Thus, by combining the old text classification methods with our novel (visual) content based methods we offer a more promising way to characterize the web. The main idea of the project is to take snapshot for each page and uses image classification methods to categorize them.	ajax (programming);categorization;computer vision;document classification;html;hyperlink;image;javascript;parsing;snapshot (computer storage);web page	Yini Bao;Erwin M. Bakker	2010	JDCTA		artificial intelligence;snapshot (computer storage);world wide web;machine learning;hyperlink;javascript;parsing;computer science;search engine indexing;web page;ajax;contextual image classification	Web+IR	-21.893443963935766	-61.21984321788906	85626
7aa9ba84b9dc2f5b8f1f94e7e6b156536a446a85	categorisation of web pages for protection against inappropriate content in the internet		The paper outlines a framework for automated categorisation of web pages to protect against inappropriate content. The paper contains the framework overview, analysis of state-of-the-art, description of the developed prototype and its evaluation based on series of experiments. Several sources are used for the categorisation, namely text, HTML tags and URL addresses. During the categorisation, this data and other information are analysed using machine learning and data mining methods. Finally, the evaluation of the categorisation quality is performed. The categorisation system developed as a result of this work are planned to be partially implemented in F-Secure Corporation in mass production systems performing analysis of web content.	categorization;internet;web page	Igor V. Kotenko;Andrey Chechulin;Dmitry Komashinsky	2017	IJIPT	10.1504/IJIPT.2017.10003851	multimedia;internet privacy;world wide web	Metrics	-30.654636701504106	-57.43579484593139	85664
12a102e9a84d8ead1cba0e7cf679fdd3f1aa057e	data intensive review mining for sentiment classification across heterogeneous domains	cross domain sentiment classification;text analysis;data mining;classification;internet;computational complexity;sentiment analysis;learning artificial intelligence	The automatic detection of orientation and emotions in texts is becoming increasingly important in the Web 2.0 scenario. There is a considerable need for innovative techniques and tools capable of identifying and detecting the attitude of unstructured text. The paper tackles two crucial aspects of the sentiment classification problem: first, the computational complexity of the deployed framework; second, the ability of the framework itself to operate effectively in heterogeneous commercial domains. The proposed approach adopts empirical learning to implement the sentiment-classification technology, and uses a distance-based predictive model to combine computational efficiency and modularity. A suitably designed semantic-based metric is the cognitive core that measures the distance between two user reviews, according to the sentiment they communicate. The framework ultimately nullifies the training process; at the same time, it takes advantage of a classification procedure whose computational cost increases linearly when the training corpus increases. To attain an objective measurement of the actual accuracy of the sentiment classification method, a campaign of tests involved a pair of complex, real-world scoring domains; the goal was to compare the predicted sentiment scores with actual scores provided by human assessors. Experimental results confirmed that the overall approach attained satisfactory performances in terms of both cross-domain classification accuracy and computational efficiency.	algorithmic efficiency;computation;computational complexity theory;performance;sensor;user review;web 2.0;world wide web	Federica Bisio;Paolo Gastaldo;Chiara Peretti;Rodolfo Zunino;Erik Cambria	2013	2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2013)	10.1145/2492517.2500280	the internet;biological classification;computer science;artificial intelligence;data science;machine learning;data mining;computational complexity theory;world wide web;sentiment analysis	NLP	-21.590916712175495	-58.27869570214814	85690
33e325683e750767b2b2c6eab71c34db2a343cbb	reconstructing sessions from data discovery and access logs to build a semantic knowledge base for improving data discovery	crawler detection;data discovery;web usage mining;session identification and reconstruction;semantic search	Big geospatial data are archived and made available through online web discovery and access. However, finding the right data for scientific research and application development is still a challenge. This paper aims to improve the data discovery by mining the user knowledge from log files. Specifically, user web session reconstruction is focused upon in this paper as a critical step for extracting usage patterns. However, reconstructing user sessions from raw web logs has always been difficult, as a session identifier tends to be missing in most data portals. To address this problem, we propose two session identification methods, including time-clustering-based and time-referrer-based methods. We also present the workflow of session reconstruction and discuss the approach of selecting appropriate thresholds for relevant steps in the workflow. The proposed session identification methods and workflow are proven to be able to extract data access patterns for further pattern analyses of user behavior and improvement of data discovery for more relevancy data ranking, suggestion, and navigation.	archive;blog;cluster analysis;data access;data logger;identifier;knowledge base;portals;relevance;session (web analytics);session id	Yongyao Jiang;Yun Li;Chaowei Phil Yang;Edward M. Armstrong;Thomas S. Huang;David Moroni	2016	ISPRS Int. J. Geo-Information	10.3390/ijgi5050054	computer science;data mining;database;world wide web	ML	-30.553958914303077	-53.17701227398602	85888
cf730c8d4c3f49a9d1d330890efaf7c2057f3951	incremental entity fusion from linked documents	boolean functions bayes methods support vector machines databases licenses fuses silicon;document fusion incremental entity fusion government applications law enforcement interdocument references attribute similarity iterative graph traversal locality sensitive hashing iterative match merge graph clustering unique entities discover document corpus bayesian likelihood fusion support vector machines obfuscation;support vector machines bayes methods document handling file organisation graph theory iterative methods merging pattern matching sensor fusion	In many government applications, especially for intelligence and law-enforcement, we often find that information about entities, such as persons or even companies, are available in disparate data sources. For example, information distributed across passports, driving licences, bank accounts, and income tax documents that need to be resolved and fused to reveal a consolidated profile of an individual. In this paper we describe an algorithm to fuse documents that are highly likely to belong to the same entity by exploiting inter-document references in addition to attribute similarity. Our technique uses a combination of iterative graph-traversal, locality-sensitive hashing, iterative match-merge, and graph-clustering to discover unique entities based on a document corpus. Further, new sets of documents can be added incrementally while having to re-process only a small subset of a previously fused entity-document collection. We present performance and quality results via both Bayesian likelihood fusion as well as using Support Vector Machines to demonstrate benefit of using inter-document references, both to improve accuracy as well as for detecting attempts at deliberate obfuscation.	algorithm;archive;cluster analysis;entity;graph traversal;iterative method;locality of reference;locality-sensitive hashing;sensor;support vector machine;text corpus;tree traversal	Pankaj Malhotra;Puneet Agarwal;Gautam Shroff	2014	17th International Conference on Information Fusion (FUSION)		computer science;pattern recognition;data mining;database	Web+IR	-23.719414207030006	-62.54407127713925	86093
54856063089ec725089ce3c42aa820f10c6002de	intimate: a web-based movie recommender using text categorization	content management;vocabulary intimate web based movie recommender text categorization feature representation feature based movie recommender;motion pictures text categorization collaboration filtering web pages books databases internet information technology vocabulary;text analysis;internet;feature extraction;feature weighting;content based retrieval;entertainment;text categorization;content management feature extraction entertainment internet content based retrieval text analysis	This paper presents INTIMATE, a web-based movie recommender that makes suggestions by using text categorization (TC) to learn from movie synopses The performance of various feature representations, feature selectors, feature weighting mechanisms and classifiers is evaluated and discussed. INTIMATE was also compared with a feature-based movie recommender. One key finding of this study is a heuristic that indicates when one recommender performs better than the other does. The results show that the text-based approach outperforms the feature-based if the ratio of the number of user ratings to the vocabulary size is high.	categorization;document classification;heuristic;recommender system;text-based (computing);vocabulary;web application	Harry Mak;Irena Koprinska;Josiah Poon	2003		10.1109/WI.2003.1241277	text mining;entertainment;the internet;feature extraction;content management;computer science;multimedia;world wide web;information retrieval	Web+IR	-28.270948986118714	-52.868765441902376	86260
9a71a20d04f64b2f66545f293dce1bce6d8f1e0a	conceptual voice based querying support model for relevant document retrieval		To increase the precision and recall values of retrieved documents while using a voice information retrieval system for documents (VIRD), is to develop a model that best captures users’ objectives in query speech. An approach to achieving this objective is by establishing conceptual relationship between keywords in user’s query before search. Here, the research proposed a Universal Fuzzy Concept Network Language (UFCNL) that represents’ the users speech with a declarative formal language, and establishes an associated degree of relationship between conceptual relations (auxiliaries and determiners) and concepts. The essence is to produce new sets of conceptual queries with potentials to retrieve documents which are more relevant to the information seeker unlike the ordinary keyword extraction by spoken term detection in voice based retrieval or the conventional text based retrieval system.	document retrieval	Olufade F. W. Onifade;Ayodeji O. J. Ibitoye	2015		10.1007/978-3-319-28031-8_43	natural language processing;data mining;information retrieval	Web+IR	-33.60788349576241	-64.04048051124319	86354
f6ad7555f96a9e09a7efb7b2b8bcfc0bfb2c63d9	recommending anchor points in structure-preserving hypertext document retrieval	traditional www search engines;search engine;user needs;answer sets;web pages;relevant documents;hyper document;conference_paper;structure preserving hypertext document retrieval;low precision;search engines;information retrieval;browsing;huge answer set;indexing information retrieval hypermedia internet online front ends;anchor points;out of order;answer set;user query;hypermedia;online front ends;internet;indexing;indexation;anchor point indexing problem;structure preservation;computers software;world wide web;hyper link structures;hyper link structures anchor points structure preserving hypertext document retrieval traditional www search engines individual web pages relevant documents answer set search engines hyper document matching pages browsing anchor point indexing problem user query key pages huge answer set low precision;document retrieval;computer science;usability;key pages;search engines web pages computer science world wide web indexing read only memory information retrieval usability out of order;read only memory;matching pages;individual web pages	Traditional WWW search engines index and recommend individual Web pages to assist users in locating relevant documents. Users are often overwhelmed by the large answer set recommended by the search engines. The logical starting point of the hyper-document is thus hidden among the large basket of matching pages. Users need to spend a lot of effort browsing through the pages to locate the starting point, a very time consuming process. This paper studies the anchor point indexing problem. The anchor points of a given user query is a small set of key pages from which the larger set of documents that are relevant to the query can be easily reached. The use of anchor points help solve the problems of huge answer set and low precision suffered by most search engines by considering the hyper-link structures of the relevant documents, and by providing a summary view of the result set.	document retrieval;hyperlink;hypertext;prototype;result set;stable model semantics;www;web page;web search engine	Ben Kao;Joseph K. W. Lee;David Wai-Lok Cheung;Chi-Yuen Ng	1998		10.1109/CMPSAC.1998.716724	document retrieval;anchor text;computer science;database;world wide web;information retrieval;search engine	DB	-30.615397398908247	-56.05869411165751	86356
9ee7e913ff74d467bf95b1add633833dfdd63b82	evaluating social network extraction for classic and modern fiction literature		The analysis of literary works has experienced a surge in computer-assisted processing. To obtain insights into the community structures and social interactions portrayed in novels the creation of social networks from novels has gained popularity. Many methods rely on identifying named entities and relations for the construction of these networks, but many of these tools are not specifically created for the literary domain. Furthermore, many of the studies on information extraction from literature typically focus on 19th century source material. Because of this, it is unclear if these techniques are as suitable to modern-day science fiction and fantasy literature as they are to those 19th century classics. We present a study to compare classic literature to modern literature in terms of performance of natural language processing tools for the automatic extraction of social networks as well as their network structure. We find that there are no significant differences between the two sets of novels but that both are subject to a high amount of variance. Furthermore, we identify several issues that complicate named entity recognition in modern novels and we present methods to remedy these.		Niels Dekker;Tobias Kuhn;Marieke van Erp	2018	PeerJ PrePrints	10.7287/peerj.preprints.27263v1		DB	-24.544201192936594	-56.24277183606908	86358
87cd8ce1563aafa6199628086f8578b072d66f86	conversation intention perception based on knowledge base	high temperature;voice communications;intelligence humans;conferencing communications;recognition;internet;perception;knowledge based systems	Web Intelligence is gaining its growth in a rapid speed. The notion of wisdom, which is considered as the next paradigm shift of WI, has become a hot research topic in recent years. The basic application of wisdom is making a short conversation in an interactive and understandable way based on the huge web resources. However, current conversation system normally applies the recognition of semantic similarities in the prepared database, neglecting the true intention hiding in the expression. In this paper, we present a model based on the medical Q&A knowledge base to overcome this challenge. The knowledge base includes three parts: disease entity, medicine, properties. A simple graph path algorithm based on words direction and relation weight adjustment is used to realize conversation intention perception. The experimental results show that this method can effectively perceive types of intention. This method can also be applied in deep understanding of other intelligent systems such as classifications and text mining.	algorithm;graph (discrete mathematics);knowledge base;programming paradigm;text mining;web intelligence;web resource	Yi-Zheng Chen;Huakang Li;Yi Liu	2014		10.1007/978-3-319-13186-3_1	the internet;computer science;knowledge management;artificial intelligence;knowledge-based systems;machine learning;data mining;database;perception	AI	-24.539011856022665	-57.56735247270368	86369
573179a567856fff9af4455119b5603870a9f3a3	finding key bloggers, one post at a time	statistical language model;social structure;user generated content	User generated content in general, and blogs in particular, form an interesting and relatively little explored domain for mining knowledge. We address the task of blog distillation: to find blogs that are principally devoted to a given topic, as opposed to blogs that merely happen to discuss the topic in passing. Working in the setting of statistical language modeling, we model the task by aggregating a blogger’s blog posts to collect evidence of relevance to the topic and persistence of interest in the topic. This approach achieves state-ofthe-art performance. On top of this baseline, we extend our model by incorporating a number of blog-specific features, concerning document structure, social structure, and temporal structure. These blogspecific features yield further improvements.	baseline (configuration management);blog;blogger;coherence (physics);email filtering;feedback;gene ontology term enrichment;language model;link analysis;persistence (computer science);relevance;social structure;spamming;user-generated content	Wouter Weerkamp;Krisztian Balog;Maarten de Rijke	2008		10.3233/978-1-58603-891-5-318	computer science;social structure;multimedia;internet privacy;user-generated content;world wide web	NLP	-22.943849428006093	-53.759842012652705	86388
d31b93a9d8b8f30043fc8fa971dbbcc6370e5034	relin: relatedness and informativeness-based centrality for entity summarization	entity summarization;informativeness;pagerank;random surfer model;distributional relatedness	Linked Data is developing towards a large, global repository for structured, interlinked descriptions of real-world entities. An emerging problem in many Web applications making use of data like Linked Data is how a lengthy description can be tailored to the task of quickly identifying the underlying entity. As a solution to this novel problem of entity summarization, we propose RELIN, a variant of the random surfer model that leverages the relatedness and informativeness of description elements for ranking. We present an implementation of this conceptual model, which captures the semantics of description elements based on linguistic and information theory concepts. In experiments involving real-world data sets and users, our approach outperforms the baselines, producing summaries that better match handcrafted ones and further, shown to be useful in a concrete task.	baseline (configuration management);centrality;entity;experiment;information theory;linked data;web application	Gong Cheng;Thanh Tran;Yuzhong Qu	2011		10.1007/978-3-642-25073-6_8	computer science;data mining;database;world wide web;information retrieval	AI	-27.43335436712899	-63.38184563838846	86398
c7c4efb146f48d28f3072d33ce46fda2845282c5	forgetting word segmentation in chinese text classification with l1-regularized logistic regression	chinese character-based n-gram;l1-regularized logistic regression;text classification;text representation	Word segmentation is commonly a preprocessing step for Chinese text representation in building a text classification system. We have found that Chinese text representation based on segmented words may lose some valuable features for classification, no matter the segmented results are correct or not. To preserve these features, we propose to use character-based N-gram to represent the Chinese text in a larger scale feature space. Considering the sparsity problem of the N-gram data, we suggest the L1-regularized logistic regression (L1-LR) model to classify Chinese text for better generalization and interpretation. The experimental results demonstrate our proposed method can get better performance than those state-of-the-art methods. Further qualitative analysis also shows that character-based N-gram representation with L1-LR is reasonable and effective for text classification. © Springer-Verlag 2013.	document classification;logistic regression;text segmentation	Qiang Fu;Xinyu Dai;Shujian Huang;Jiajun Chen	2013		10.1007/978-3-642-37456-2_21	speech recognition;machine learning;pattern recognition;logistic model tree	NLP	-20.262728325641675	-65.56527706661228	86464
45e6654c6a8c2871d6038fb9078898ffed2597a7	prime: a system for multi-lingual patent retrieval	document clustering;patent retrieval;target language;machine translation	Given the growing number of patents filed in multiple countries, users are interested in retrieving patents across languages. We propose a multi-lingual patent retrieval system, which translates a user query into the target language, searches a multilingual database for patents relevant to the query, and improves the browsing efficiency by way of machine translation and clustering. Our system also extracts new translations from patent families consisting of comparable patents, to enhance the translation dictionary.	bilingual dictionary;cluster analysis;compiler;cross-language information retrieval;machine translation;software patent	Shigeto Higuchi;Masatoshi Fukui;Atsushi Fujii;Tetsuya Ishikawa	2002	CoRR		natural language processing;document clustering;computer science;data mining;linguistics;machine translation;information retrieval	ML	-33.082196763775364	-64.89436371008053	86643
6fa60185110bdf410abcab5c8dd844ca8bcc8a91	using visual pages analysis for optimizing web archiving	change detection;web pages;visual page analysis;web archiving;web crawling;world wide web	Due to the growing importance of the World Wide Web, archiving it has become crucial for preserving useful source of information. To maintain a web archive up-to-date, crawlers harvest the web by iteratively downloading new versions of documents. However, it is frequent that crawlers retrieve pages with unimportant changes such as advertisements which are continually updated. Hence, web archive systems waste time and space for indexing and storing useless page versions. Also, querying the archive can take more time due to the large set of useless page versions stored. Thus, an effective method is required to know accurately when and how often important changes between versions occur in order to efficiently archive web pages. Our work focuses on addressing this requirement through a new web archiving approach that detects important changes between page versions. This approach consists in archiving the visual layout structure of a web page represented by semantic blocks. This work seeks to describe the proposed approach and to examine various related issues such as using the importance of changes between versions to optimize web crawl scheduling. The major interesting research questions that we would like to address in the future are introduced.	archive;download;effective method;information source;scheduling (computing);web archiving;web crawler;web page;world wide web	Myriam Ben Saad;Stéphane Gançarski	2010		10.1145/1754239.1754287	web service;static web page;web development;web modeling;site map;data web;web analytics;web mapping;web design;web standards;computer science;dynamic web page;web navigation;social semantic web;web page;semantic web stack;database;web intelligence;web 2.0;world wide web;website parse template;information retrieval;web server	Web+IR	-30.685056066468473	-53.81588328275489	86717
9613d80393c68ed50a8a82ef35034b7af7db917a	the effect of preprocessing techniques on twitter sentiment analysis	machine learning algorithms;standards;support vector machines;filtering algorithms;classification algorithms;sentiment analysis;twitter	As Twitter offers a fertile ground for expressing different thoughts and opinions, it can be seen as a valuable tool for sentiment analysis. Furthermore, properly identified reviews present a baseline of information as an input to different systems, such as e-learning systems, decision support systems etc. However, the data preprocessing is a crucial step in sentiment analysis, since selecting the appropriate preprocessing methods, the correctly classified instances can be increased. In view of the above, this research paper explains the necessary information to get preprocess the reviews in order to find sentiment and make analysis whether it is positive or negative. Extended comparison of sentiment polarity classification methods for Twitter text and the role of text preprocessing in sentiment analysis are discussed in depth. In the set of tests, possible combinations of methods and report on their efficiency were included, conducting experiments using manually annotated Twitter datasets. Finally, it is proved that feature selection and representation can affect the classification performance positively.	algorithm;baseline (configuration management);data pre-processing;decision support system;embedded system;experiment;feature extraction;feature selection;n-gram;personalization;preprocessor;sentiment analysis	Akrivi Krouska;Christos Troussas;Maria Virvou	2016	2016 7th International Conference on Information, Intelligence, Systems & Applications (IISA)	10.1109/IISA.2016.7785373	computer science;data science;machine learning;data mining;sentiment analysis	NLP	-21.656464221625207	-57.37784307501849	86756
df716f8d9f746cf672b88f44bdf195d98fe73d89	an exploration of pattern-based subtopic modeling for search result diversification	frequent pattern mining;information retrieval;pattern;information retrieval model;subtopic;diversification	Traditional information retrieval models do not necessarily provide users with optimal search experience because the top ranked documents may contain the same piece of relevant information, i.e., the same subtopic of a query. The goal of search result diversification is to return search results that not only are relevant to the query but also cover different subtopics. Therefore, the subtopic modeling is an important research topic in search result diversification. In this paper, we propose a novel pattern based method to extract subtopics from retrieved documents. The basic idea is to explicitly model a query subtopic as a semantically meaningful text unit in relevant documents. We apply a frequent pattern mining algorithm to efficiently extract these text units (patterns) from retrieved documents. We then model a query subtopic with a single pattern and rank subtopics based on their similarity with the query. These pattern based subtopics are then used to diversify search results.	algorithm;data mining;diversification (finance);information retrieval	Wei Zheng;Xuanhui Wang;Hui Fang;Hong Cheng	2011		10.1145/1998076.1998148	diversification;query expansion;computer science;pattern recognition;data mining;pattern;information retrieval	Web+IR	-29.943247418203	-60.55568828685415	86878
a920ef2561b4ee56440af4b2e2166e57f36069cd	on exploiting content and citations together to compute similarity of scientific papers	citation;scientific papers;content;similarity;authority	In computing the similarity of scientific papers, previous text-based and link-based similarity measures look at only a single side of the content and citations. In this paper, we propose a novel approach called SimCC that effectively combines the content and citation information to accurately compute the similarity of scientific papers. Unlike previous approaches, SimCC effectively represents both authority and context of a scientific paper simultaneously in computing similarities. Also, we propose SimCC+A to consider recently-published papers. The effectiveness of our proposed method is demonstrated via extensive experiments on a real-world dataset of scientific papers, with more than 100% improvement in accuracy compared with previous methods.	experiment;scientific literature;text-based (computing)	Masoud Reyhani Hamedani;Sang Wook Kim;Sang-Chul Lee;Dong-Jin Kim	2013		10.1145/2505515.2507842	authority;similarity;computer science;data science;data mining;information retrieval	AI	-26.54080423516257	-58.350968954193746	86985
7bfca247270686d44d411ad59b931b1f06d91a3b	icrc-dsedl: a film named entity discovery and linking system based on knowledge bases		Named entity discovery and linking are hot topics in text minig, which is very important for text understanding as named entities that usually presented in various formats and some of them are ambiguous. To accelerate the development of related technology, the China Conference on Knowledge Graph and Semantic Computing (CCKS) in 2016 launches a competition, which includes a task on film named entity discovery and linking (i.e., task 1). We participate this competition and develop a system for task 1 of the CCKS competition. The system consists of two individual parts for named entity discovery (NED) and entity linking (EL) respectively. The first part is a hybrid subsystem based on conditional random field (CRF) and structural support vector machine (SSVM) with rich features, and the second part is a ranking subsystem where not only the given knowledge base but also open knowledge bases are used for candidate generation and SVMrank is used for candidate ranking. On the official test dataset of Task1 of CCKS 2016 competition, our system achieves an F1-score of 77.83% on NED, an accuracy of 86.53% on EL and an overall F1-score of 67.35%. © Springer Nature Singapore Pte Ltd. 2016.	named entity	Yahui Zhao;Haodi Li;Qingcai Chen;Jianglu Hu;Guangpeng Zhang;Dong Huang;Buzhou Tang	2016		10.1007/978-981-10-3168-7_20	engineering;data science;data mining;information retrieval	NLP	-29.76601982801689	-64.89520117017322	87036
3d2868ed26c83fba3385b903750e4637179b9c5f	exploring customer reviews for music genre classification and evolutionary studies	conference publication;recommender systems	In this paper, we explore a large multimodal dataset of about 65k albums constructed from a combination of Amazon customer reviews, MusicBrainz metadata and AcousticBrainz audio descriptors. Review texts are further enriched with named entity disambiguation along with polarity information derived from an aspect-based sentiment analysis framework. This dataset constitutes the cornerstone of two main contributions: First, we perform experiments on music genre classification, exploring a variety of feature types, including semantic, sentimental and acoustic features. These experiments show that modeling semantic information contributes to outperforming strong bag-of-words baselines. Second, we provide a diachronic study of the criticism of music genres via a quantitative analysis of the polarity associated to musical aspects over time. Our analysis hints at a potential correlation between key cultural and geopolitical events and the language and evolving sentiments found in music reviews.	acoustic cryptanalysis;bag-of-words model;entity linking;experiment;multimodal interaction;sentiment analysis;word-sense disambiguation	Sergio Oramas;Luis Espinosa Anke;Aonghus Lawlor;Xavier Serra;Horacio Saggion	2016			computer science;data science;machine learning;multimedia;world wide web;recommender system	NLP	-25.41043346955214	-60.596089320735274	87065
a9ec5b5030d34d6cec60f52232999cc4ff9c63c7	nouveau-rouge: a novelty metric for update summarization	system performance;evaluation metric;overall response	An update summary should provide a fluent summarization of new information on a time-evolving topic, assuming that the reader has already reviewed older documents or summaries. In 2007 and 2008, an annual summarization evaluation included an update summarization task. Several participating systems produced update summaries indistinguishable from human-generated summaries when measured using ROUGE. However, no machine system performed near human-level performance in manual evaluations such as pyramid and overall responsiveness scoring. We present a metric called Nouveau-ROUGE that improves correlation with manual evaluation metrics and can be used to predict both the pyramid score and overall responsiveness for update summaries. Nouveau-ROUGE can serve as a less expensive surrogate for manual evaluations when comparing existing systems and when developing new ones.	angular defect;automatic summarization;responsiveness;windows update;nouveau	John M. Conroy;Judith D. Schlesinger;Dianne P. O'Leary	2011	Computational Linguistics	10.1162/coli_a_00033	multi-document summarization;computer science;data science;automatic summarization;data mining;computer performance;information retrieval	NLP	-31.010362849443226	-63.8937965228587	87080
6faa5825c55a320f8302f9600e739cb97b36f8a6	detecting online harassment in social networks	online harassment;social networks;profanity	Online Harassment is the process of sending messages for example in Social Networks to cause psychological harm to a victim. In this paper, we propose a pattern-based approach to detect such messages. Since user generated texts contain noisy language we perform a normalization step first to transform the words into their canonical forms. Additionally, we introduce a person identification module that marks phrases which relate to a person. Our results show that these preprocessing steps increase the classification performance. The pattern-based classifier uses the information provided by the preprocessing steps to detect patterns that connect a person to profane words. This technique achieves a substantial improvement compared to existing approaches. Finally, we discuss the portability of our approach to Social Networks and its possible contribution to tackle the abuse of such applications for the distribution of Online	bag-of-words model;blocking (computing);cyberbullying;database normalization;document;information systems;machine learning;noisy text;pareto efficiency;precision and recall;preprocessor;sensor;social network;software portability;sparse matrix;statistical classification;user-centered design	Uwe Bretschneider;Thomas Wöhner;Ralf Peters	2014			social science;artificial intelligence;machine learning;world wide web;computer security;social network	Web+IR	-20.955770892712504	-56.36452945878461	87132
0bb84c1d1bd84bb89df2a1cb68b17189555396fd	using wikipedia to improve precision of contextual advertising	english language;reference point;profitability	Contextual advertising is an important part of the Web economy today. Profit is linked to the interest that users find in the ads presented to them. The problem is for contextual advertising platforms to select the most relevant ads. Simple keyword matching techniques for matching ads to page content give poor accuracy. Problems such as homonymy, polysemy, limited intersection between content and selection keywords as well as context mismatch can significantly degrade the precision of ads selection. In this paper, we propose a method for improving the relevance of contextual ads based on “Wikipedia matching”. It is a technique that uses Wikipedia articles as “reference points” for ads selection. In our research, we worked on English language, but it is possible to port the algorithm to other languages.	algorithm;communications of the acm;contextual advertising;document classification;e-commerce;experiment;information retrieval;loose coupling;machine learning;machine translation;relevance;wikipedia;world wide web;yang	Alexander Pak	2009		10.1007/978-3-642-20095-3_49	computer science;advertising;world wide web;information retrieval	Web+IR	-30.86663315858135	-59.81385349965813	87157
1526f2fd608b76f26970b453a51f4e75f32568b8	query translation based on visual information		In the field of cross language information retrieval, how to translate the query into the target language, namely query translation, is a fundamental problem. Because of the ambiguity phenomenon, query translation is always a challenge. Existing researches always rely on mining the text information, such as the contextual relationship or word occurrence. Different from existing research efforts, in this paper, we address the query translation issue by mining the visual information of images, and a new query translation method based on visual information (QTVI) is proposed. QTVI has three steps: image search, image set denoising, and translation candidate selection. In step 1, the query and candidate translation are associated with corresponding image set via image search. Since the resulted image sets from step 1 may be unclean, in step 2, we de-noise the image sets via clustering strategy. Finally, in step 3, the final translation is selected from candidates by constructing multi-class classifier based on cleaned image sets. Empirical experiments show that QTVI outperforms Baidu Translation and Google Translation for the query translation task.	cluster analysis;compiler;cross-language information retrieval;experiment;google translate;image retrieval;noise reduction	Jiao Zhang;Yonggang Huang;Qingzhao Jiang;Wenpeng Lu;Hualei Shen	2018	2018 Tenth International Conference on Advanced Computational Intelligence (ICACI)	10.1109/ICACI.2018.8377521	feature extraction;semantics;cluster analysis;ambiguity;cross-language information retrieval;visualization;classifier (linguistics);computer science;pattern recognition;artificial intelligence;phenomenon	Vision	-25.912850453219214	-63.033110883930725	87212
a79d975f2a0546fe865453f58568f7272876d81d	sentiment polarity classification using statistical data compression models	opinion mining;statistical analysis computational linguistics data compression data mining knowledge based systems learning artificial intelligence pattern classification pattern matching;data compression;support vector machines;compression algorithms;training;frequency measurement;data mining;text classification;prediction by partial matching sentiment analysis opinion mining text classification data compression;accuracy;computational modeling;accuracy training entropy support vector machines compression algorithms frequency measurement computational modeling;statistical analysis;corpora sentiment polarity classification user generated content sentiment analysis knowledge based method machine learning adaptive statistical data compression model classification performance evaluation lossless compression algorithm prediction by partial matching compression based measure ppm character n gram frequency statistics;pattern matching;sentiment analysis;pattern classification;entropy;computational linguistics;learning artificial intelligence;knowledge based systems;prediction by partial matching	With growing availability and popularity of user generated content, the discipline of sentiment analysis has come to the attention of many researchers. Existing work has mainly focused on either knowledge based methods or standard machine learning techniques. In this paper we investigate sentiment polarity classification based on adaptive statistical data compression models. We evaluate the classification performance of the loss less compression algorithm Prediction by Partial Matching (PPM) as well as compression based measures using PPM-like character n-gram frequency statistics. Comprehensive experiments on three corpora show that compression based methods are efficient, easy to apply and can compete with the accuracy of sophisticated classifiers such as support vector machines.	algorithm;data compression;experiment;machine learning;n-gram;prediction by partial matching;sentiment analysis;statistical classification;support vector machine;text corpus;user-generated content	Dominique Ziegelmayer;Rainer Schrader	2012	2012 IEEE 12th International Conference on Data Mining Workshops	10.1109/ICDMW.2012.43	data compression;computer science;machine learning;pattern recognition;data mining;sentiment analysis;statistics	DB	-21.656622051075736	-65.162669203637	87249
165cfe06ae807572478aa37ecd3d58cc4c0160c2	link-based web spam detection using weight properties	weight properties;host level;link spam;journal article;web spam;adversarial information retrieval	Link spam is created with the intention of boosting one target’s rank in exchange of business profit. This unethical way of deceiving Web search engines is known as Web spam. Since then many anti-link spam detection techniques have constantly being proposed. Web spam detection is a crucial task due to its devastation towards Web search engines and global cost of billion dollars annually. In this paper, we proposed a novel technique by incorporating weight properties to enhance the Web spam detection algorithms. Weight properties can be defined as the influences of one Web node towards another Web node. We modified existing Web spam detection algorithms with our novel technique to evaluate the performances on a large public Web spam dataset – WEBSPAM-UK2007. The overall performance have shown that the modified algorithms outperform the benchmark algorithms up to 30.5 % improvement at host level and 6.11 % improvement at page level.	algorithm;anti-aliasing filter;anti-spam techniques;automatic target recognition;backlink;baseline (configuration management);benchmark (computing);distrust;machine learning;performance;software propagation;spamdexing;spamming;transformers: devastation;trustrank;web search engine;world wide web	Alex Goh Kwang Leng;Ravi Kumar Patchmuthu;Ashutosh Kumar Singh	2014	Journal of Intelligent Information Systems	10.1007/s10844-014-0310-y	content farm;forum spam;web threat;computer science;spamdexing;spambot;spam and open relay blocking system;data mining;adversarial information retrieval;internet privacy;world wide web	Web+IR	-19.36681709111934	-57.969375835940596	87423
d2b845e1a038b48692a841a0ef10b15e8f107574	ordinal classification/regression for analyzing the influence of superstars on spectators in cinema marketing	cinema marketing;machine learning;ordinal classification;ordinal regression	This paper studies the influence of superstars on spectators in cinema marketing. Casting superstars is a common risk-mitigation strategy in the cinema industry. Anecdotal evidence suggests that the presence of superstars is not always a guarantee of success and hence, a deeper study is required to analyze the potencial audience of a movie. In this sense, knowledge, attitudes and emotions of spectators towards stars are analyzed as potencial factors of influencing the intention of seeing a movie with stars in its cast. This analysis is performed through machine learning techniques. In particular, the problem is stated as an ordinal classification/regression task rather than a traditional classification or regression task, since the intention of watching a movie is measured in a graded scale, hence, its values exhibit an order. Several methods are discussed for this purpose, but Support Vector Ordinal Regression shows its superiority over other ordinal classification/regression techniques. Moreover, exhaustive experiments carried out confirm that the formulation of the problem as an ordinal classification/regression is a success, since powerful traditional classifiers and regressors show worse performance. The study also confirms that talent and popularity expressed by means of knowledge, attitude and emotions satisfactorily explain superstar persuasion. Finally, the impact of these three components is also checked.	cinema 4d;experiment;feature selection;level of measurement;machine learning;ordinal data;ordinal regression;peripheral;social network;statistical classification	Elena Montañés;Ana Suárez-Vázquez;José Ramón Quevedo	2014	Expert Syst. Appl.	10.1016/j.eswa.2014.07.011	ordinal regression;computer science;artificial intelligence;machine learning;data mining;statistics	Web+IR	-19.578425902162973	-52.71926529080407	87462
1ed1ed53a9f863479f8470fe4e4cb273f050955a	query polyrepresentation for ranking retrieval systems without relevance judgments	document retrieval;data fusion;information retrieval;information need	Ranking information retrieval (IR) systems with respect to their effectiveness is a crucial operation during IR evaluation, as well as during data fusion. This paper offers a novel method of approaching the system ranking problem, based on the widely studied idea of polyrepresentation. The principle of polyrepresentation suggests that a single information need can be represented by many query articulations–what we call query aspects. By skimming the top k (where k is small) documents retrieved by a single system for multiple query aspects, we collect a set of documents that are likely to be relevant to a given test topic. Labeling these skimmed documents as putatively relevant lets us build pseudo-relevance judgments without undue human intervention. We report experiments where using these pseudo-relevance judgments delivers a rank ordering of IR systems that correlates highly with rankings based on human relevance judgments.	conjunctive query;experiment;information needs;information retrieval;manifest (transportation);relevance;text retrieval conference;theory;type conversion	Miles Efron;Megan A. Winget	2010	JASIST	10.1002/asi.21310	document retrieval;information needs;query expansion;ranking;relevance;ranking;computer science;concept search;data mining;database;sensor fusion;world wide web;information retrieval;query language;human–computer information retrieval	Web+IR	-31.905483453749405	-60.818653351156705	87471
0fd12fadf8f2203772b44f1e17b16b87e0f72923	feature selection using improved mutual information for text classification	busqueda informacion;bayes estimation;dimensionalidad;analisis contenido;evaluation function;model based reasoning;evaluation performance;text;raisonnement base sur modele;evaluation fonction;donnee textuelle;high dimensionality;analisis estadistico;performance evaluation;dato textual;information retrieval;evaluacion prestacion;naive bayes;informacion mutual;dimensionality;metodo secuencial;texte;sequential method;text classification;estimacion bayes;content analysis;information mutuelle;statistical analysis;function evaluation;recherche information;dimensionnalite;analyse statistique;textual data;palabra;pattern recognition;methode sequentielle;mutual information;feature selection;word;modele donnee;sequential forward selection;reconnaissance forme;analyse contenu;reconocimiento patron;texto;information gain;mot;data models;estimation bayes	A major characteristic of text document classification problem is extremely high dimensionality of text data. In this paper we present two algorithms for feature (word) selection for the purpose of text classification. We used sequential forward selection methods based on improved mutual information introduced by Battiti [1] and Kwak and Choi [6] for non-textual data. These feature evaluation functions take into consideration how features work together. The performance of these evaluation functions compared to the information gain which evaluate features individually is discussed. We present experimental results using naive Bayes classifier based on multinomial model on the Reuters data set. Finally, we analyze the experimental results from various perspectives, including F1-measure, precision and recall. Preliminary experimental results indicate the effectiveness of the proposed feature selection algorithms in a text classification problem.	academy;document classification;evaluation function;f1 score;feature selection;information gain in decision trees;k-nearest neighbors algorithm;kullback–leibler divergence;multinomial logistic regression;mutual information;naive bayes classifier;precision and recall;stepwise regression;support vector machine;test set;text corpus	Jana Novovicová;Antonín Malík;Pavel Pudil	2004		10.1007/978-3-540-27868-9_111	data modeling;naive bayes classifier;curse of dimensionality;content analysis;computer science;artificial intelligence;model-based reasoning;machine learning;evaluation function;pattern recognition;word;data mining;kullback–leibler divergence;mutual information;feature selection;statistics	ML	-21.452434651425516	-64.26099775180509	87550
56e12020e7e9d6ba7152ba8b1b789876da365de3	detecting fake news in social networks via crowdsourcing		Our work considers leveraging crowd signals for detecting fake news and is motivated by tools recently introduced by Facebook that enable users to flag fake news. By aggregating users’ flags, our goal is to select a small subset of news every day, send them to an expert (e.g., via a third-party factchecking organization), and stop the spread of news identified as fake by an expert. The main objective of our work is to minimize the spread of misinformation by stopping the propagation of fake news in the network. It is especially challenging to achieve this objective as it requires detecting fake news with high-confidence as quickly as possible. We show that in order to leverage users’ flags efficiently, it is crucial to learn about users’ flagging accuracy. We develop a novel algorithm, DETECTIVE, that performs Bayesian inference for detecting fake news and jointly learns about users’ flagging accuracy over time. Our algorithm employs posterior sampling to actively trade off exploitation (selecting news that directly maximize the objective value at a given epoch) and exploration (selecting news that maximize the value of information towards learning about users’ flagging accuracy). We demonstrate the effectiveness of our approach via extensive experiments and show the power of leveraging community signals.	algorithm;crowdsourcing;experiment;sampling (signal processing);sensor;social network;software propagation;spamming	Sebastian Tschiatschek;Adish Singla;Manuel Gomez-Rodriguez;Arpit Merchant;Andreas Krause	2017	CoRR		data mining;flagging;internet privacy;misinformation;fake news;value of information;trade-off;social network;computer science;bayesian inference;crowdsourcing	Web+IR	-19.50459286023365	-54.64712469462307	87748
f04a46c1820c1f6e84040ee7d8ff28290b751da6	on statistical measures for selecting pertinent formal concepts to discover production rules from data	pattern classification data mining learning artificial intelligence;supervised learning;pertinence measures statistical measures production rules association rules classification rules data mining formal concept analysis data sets supervised learning entropy measures;data mining;production association rules data mining sequences supervised learning entropy learning systems lattices dna gain measurement;learning system;association rule;classification rules;pattern classification;learning artificial intelligence;production rule;formal concept analysis	The discovery of production rules (association rules and/or classification rules) is one of the most important tasks of data mining. The discovered knowledge is intelligible and comprehensible by experts in any field. In previous works, the authors used formal concept analysis to discover classification rules and association rules embedded in data sets. One of the difficulties the authors found is to measure the pertinence of the discovered rules. In supervised learning of classification rules, the authors used the known entropy measure. In un-supervised learning of association rules, they used the known support measure. However, some recent works have proven the insufficiency of these measures and have introduced other ones. In this paper, the authors present a bibliographic summary of many existing pertinence measures. Then, the authors present an experimental study of the behavior of these measures in order to help the users of our learning system, choosing the appropriate measure	association rule learning;convergence insufficiency;data mining;embedded system;experiment;formal concept analysis;production (computer science);relevance;supervised learning	Mondher Maddouri;Fatma Kaâbi	2006	Sixth IEEE International Conference on Data Mining - Workshops (ICDMW'06)	10.1109/ICDMW.2006.124	association rule learning;computer science;formal concept analysis;machine learning;pattern recognition;data mining;mathematics;supervised learning	ML	-22.363459352616548	-63.38825305035058	87784
352a415a9b5e621c27a0df7885cca67d9fff22fe	what ignites a reply?: characterizing conversations in microblogs		Nowadays, microblog platforms provide a medium to share content and interact with other users. With the large-scale data generated on these platforms, the origin and reasons of users engagement in conversations has attracted the attention of the research community. In this paper, we analyze the factors that might spark conversations in Twitter, for the English and Spanish languages. Using a corpus of 2.7 million tweets, we reconstruct existing conversations, then extract several contextual and content features. Based on the features extracted, we train and evaluate several predictive models to identify tweets that will spark a conversation. Our findings show that conversations are more likely to be initiated by users with high activity level and popularity. For less popular users, the type of content generated is a more important factor. Experimental results shows that the best predictive model is able obtain an average score $F1=0.80$. We made available the dataset scripts and code used in this paper to the research community via Github.	f1 score;machine learning;predictive modelling;principal component analysis;relevance;signal-to-noise ratio;text corpus	Johnny Torres;Carmen Vaca;Cristina L. Abad	2017		10.1145/3148055.3148071	microblogging;conversation;data mining;computer science;social computing;big data;spark (mathematics);popularity;scripting language;social media	Web+IR	-22.20380235285636	-53.816883019182356	87932
2ad8ea57885ebb379af3f5ddc0d21b5949c36cea	personality recognition applying machine learning techniques on source code metrics		Source code has become a data source of interest in the recent years. In the software industry is common the extraction of source code metrics, mainly for quality assurance purposes. In this paper source code metrics are used to consolidate programmers profiles with the purpose to identify different personality traits using machine learning algorithms. This work was done as part of the Personality Recognition in SOurce COde (PR-SOCO) shared task in the Forum for Information Retrieval Evaluation 2016 (FIRE 2016). CCS Concepts •Information systems → Content analysis and feature selection; •Computing methodologies → Supervised learning by regression; Cluster analysis; •General and reference → Metrics; •Software and its engineering → Parsers;	algorithm;cluster analysis;feature selection;information retrieval;machine learning;programmer;software industry;software metric;supervised learning	Hugo A. Castellanos	2016			source code;machine learning;data mining;personality;artificial intelligence;computer science	SE	-19.896835972765203	-60.97832975387526	87959
3aa90173b1cfc66cdd1d84ba2cf059d5e2854007	authoritative prediction of website based on deep learning		Website authoritativeness is generally measured by external links, the more high-quality external links, the higher the authority of the site. Algorithms such as PageRank, etc. are com-monly used in the evaluation of website authoritative, However, this kind of algorithm is selective to evaluate the authoritativeness of websites, which makes this method have some deficiencies. This paper uses deep learning method to evaluate the authority of different websites under a certain search query by mapping the search query and the corresponding title of the website as a vector and calculating the similarity between two vectors. Websites of high results are referred to as authoritative sites under the search query, thus providing a new perspective to measure website authoritativeness. By comparing the three model experiments with Word2vec, CNN and LSTM, the experimental results on open datasets show that it is effective to use these three models, of which the LSTM model works best.	deep learning;dynamic programming;experiment;long short-term memory;pagerank;root-finding algorithm;word2vec	Haihua Yang;Yangde Feng	2018	2018 IEEE Fourth International Conference on Big Data Computing Service and Applications (BigDataService)	10.1109/BigDataService.2018.00038	web search query;computer science;deep learning;data mining;word2vec;big data;pagerank;artificial intelligence	SE	-27.259043408176197	-55.270777222057994	88005
a78f360910393aee148942a95b3d7d0ad2959aca	hierarchical transductive classification from textual data with relevant example selection		In many textual repositories, documents are organized in a hierarchy of categories to support a thematic search by browsing topics of interests. In this paper we present a novel approach for automatic classification of documents into a hierarchy of categories that works in the transductive setting and exploits relevant example selection. While the transductive learning setting permits to classify repositories where only few examples are labelled by exploiting information potentially conveyed by unlabelled data, relevant example selection permits to tame the complexity of the task and increase the rate of learning by focusing only on informative examples. Results on real world datasets are reported.	algorithm;cluster analysis;computational complexity theory;experiment;information;learning classifier system;statistical classification;tame;text corpus;transduction (machine learning)	Michelangelo Ceci;Pasqua Fabiana Lanotte	2010			transduction (machine learning);machine learning;computer science;pattern recognition;artificial intelligence	NLP	-20.662988945742043	-62.46420967116592	88151
b7a6674ee2d01143a05e704b4a82be818a0339bc	discovering opinion leaders for medical topics using news articles	health research;uk clinical guidelines;biological patents;europe pubmed central;citation search;data mining and knowledge discovery;computational biology bioinformatics;uk phd theses thesis;life sciences;algorithms;combinatorial libraries;uk research reports;medical journals;computer appl in life sciences;europe pmc;biomedical research;bioinformatics	BACKGROUND Rapid identification of subject experts for medical topics helps in improving the implementation of discoveries by speeding the time to market drugs and aiding in clinical trial recruitment, etc. Identifying such people who influence opinion through social network analysis is gaining prominence. In this work, we explore how to combine named entity recognition from unstructured news articles with social network analysis to discover opinion leaders for a given medical topic.   METHODS We employed a Conditional Random Field algorithm to extract three categories of entities from health-related new articles: Person, Organization and Location. We used the latter two to disambiguate polysemy and synonymy for the person names, used simple rules to identify the subject experts, and then applied social network analysis techniques to discover the opinion leaders among them based on their media presence. A network was created by linking each pair of subject experts who are mentioned together in an article. The social network analysis metrics (including centrality metrics such as Betweenness, Closeness, Degree and Eigenvector) are used for ranking the subject experts based on their power in information flow.   RESULTS We extracted 734,204 person mentions from 147,528 news articles related to obesity from January 1, 2007 through July 22, 2010. Of these, 147,879 mentions have been marked as subject experts. The F-score of extracting person names is 88.5%. More than 80% of the subject experts who rank among top 20 in at least one of the metrics could be considered as opinion leaders in obesity.   CONCLUSION The analysis of the network of subject experts with media presence revealed that an opinion leader might have fewer mentions in the news articles, but a high network centrality measure and vice-versa. Betweenness, Closeness and Degree centrality measures were shown to supplement frequency counts in the task of finding subject experts. Further, opinion leaders missed in scientific publication network analysis could be retrieved from news articles.	algorithm;betweenness;categories;centrality;conditional random field;eighty;entity;extraction;name;named-entity recognition;obesity;rule (guideline);scientific literature;social network analysis;societies, scientific	Siddhartha Jonnalagadda;Ryan Peeler;Philip Topham	2012		10.1186/2041-1480-3-2	medical research;computer science;bioinformatics;data science;data mining;algorithm	Web+IR	-25.501252265449278	-58.447659305531154	88241
3943a523b7dde93a37d2f25f1be82b58545e09f5	termpicker: enabling the reuse of vocabulary terms by exploiting data from the linked open data cloud - an extended technical report		Deciding which vocabulary terms to use when modeling data as Linked Open Data (LOD) is far from trivial. Choosing too general vocabulary terms, or terms from vocabularies that are n ot used by other LOD datasets, is likely to lead to a data representat io , which will be harder to understand by humans and to be consume d by Linked data applications. In this technical report, we pr opose TermPicker : a novel approach for vocabulary reuse by recommending RDF types and properties based on exploiting the informa tion on how other data providers on the LOD cloud use RDF types and properties to describe their data. To this end, we introd uce the notion of so-calledschema-level patterns (SLPs). They capture how sets of RDF types are connected via sets of properties wit hin some data collection, e.g., within a dataset on the LOD cloud . TermPicker uses such SLPs and generates a ranked list of voca bulary terms for reuse. The lists of recommended terms are orde red by a ranking model which is computed using the machine learning approach Learning To Rank (L2R). TermPicker is evaluated base d on the recommendation quality that is measured using the Mean A verage Precision (MAP) and the Mean Reciprocal Rank at the first fi ve positions (MRR@5). Our results illustrate an improvement o f the recommendation quality by 29− 36% when using SLPs compared to the beforehand investigated baselines of recommending s olely popular vocabulary terms or terms from the same vocabulary. The overall best results are achieved using SLPs in conjunction w th the Learning To Rank algorithmRandom Forests .	learning to rank;linked data;machine learning;mean squared error;speech-generating device;tag cloud;vocabulary	Johann Schaible;Thomas Gottron;Ansgar Scherp	2016		10.1007/978-3-319-34129-3_7	computer science;data mining;database;world wide web;information retrieval	Web+IR	-29.844732792568735	-61.491958863698905	88246
745c59e9f5b0f2c3122b6a5801f24fef970cc571	parallel topic model and its application on document clustering		This paper presents PLDACOL, our parallel implementation on LDACOL model, to effectively cluster large-scale documents. Since phrases contain more semantic information than the sum of its individual word, we use topic model LDACOL for phrase discovery, and use Gibbs sampling for parameter inference. PLDACOL overcomes the high computation time cost in parameter inference by the distributed computing framework based on Hadoop. We show that our PLDACOL can be applied to the clustering of large-scale documents in different size and produces significant improvements on both effectiveness and efficiency compared with other related traditional algorithms.		Lidong Wang;Yuhuai Wang;Shihua Cao;Yun Zhang;Kang An	2017	IJICT	10.1504/IJICT.2017.10008317	topic model;machine learning;real-time computing;cluster analysis;computer science;computation;inference;document clustering;gibbs sampling;phrase;artificial intelligence	NLP	-22.99890097277134	-62.4363412903344	88250
9dace54b21b1423ddad36e991df1a3b6d5e39236	diachronic semantic cohesion for topic segmentation of tv broadcast news		This paper proposes a new way to integrate semantic relations into a topic segmentation process by defining the notion of semantic cohesion. In the context of a sliding window based automatic topic segmentation algorithm, semantic relations are incorporated in the similarity measure between adjacent blocs. Additionaly, in the context of TV Brodcast News topic segmentation, we propose a new protocole to gather relevant data for semantic relations computation, showing that a small set of diachronic data can be more relevant for the task than using a large amount of general or asynchronous data. Experiments on a corpus of 86 various French TV Broadcast News shows recorded during one week, in conjunction with text articles collected through the Google News homepage at the same period for semantic relation estimation show significant improvement in topic segmentation performance.	algorithm;computation;google news;ontology components;similarity measure;text corpus;text segmentation	Abdessalam Bouchekif;Géraldine Damnati;Yannick Estève;Delphine Charlet;Nathalie Camelin	2015			speech recognition;natural language processing;broadcast television systems;computer science;artificial intelligence;segmentation	NLP	-26.826024833683213	-65.42723939618215	88497
2fe3db1fa9c89c0c68272640fb82ce64d7ad722b	healthcare data analytics challenge	databases;semantics;diabetes;distributional semantics;frequency measurement;cosine similarity health care data analytics topic modelling distributional semantics term frequency inverse document frequency tf idf vector representation;tf idf;cosine similarity topic models distributional semantics tf idf;electronic publishing;topic models;encyclopedias;semantics databases diabetes frequency measurement encyclopedias electronic publishing;vectors data analysis health care semantic networks;cosine similarity	Online patient/caregiver support forums such as, cancer compass, ehealthforums, and patientslikeme, allow patients and caregivers to post health-related questions. In many of these forums, there is a significant volume of repetitive questions. One possible reason for this repetition could be that as forums grow longer, patients and caregivers do not have the time or patience to read through previous questions before posting their own question. The challenge here is to design and implement a system that, for a new question q, identifies a maximum of three existing questions that are most similar to q. In this challenge, we experimented with a variety of methods and representations to address this task, including approaches that leveraged topic modeling, distributional semantics (word2vec), and term frequency-inverse document frequencies (TF-IDF) to induce the vector representation of questions. For similarity measures, we used cosine similarity and the rescaled dot product over these feature spaces. Despite our experimentation with more recent methods, we found that simple TF-IDF with stemming using cosine similarity seemed to result in the best performance.	cosine similarity;distributional semantics;stemming;tf–idf;topic model;word2vec	Zhiguo Yu;Byron C. Wallace;Todd R. Johnson	2015	2015 International Conference on Healthcare Informatics	10.1109/ICHI.2015.96	computer science;data mining;database;information retrieval	NLP	-25.219365470476433	-57.96187839834691	88565
258104bb1dd5b4d0fd4e563df075154e128bd266	suggestion contextuelle composite	suggestion contextuelle;theorie de l information;diversite;grappe;recherche composite;recherche d information	Contextual suggestion aims to provide a user with venues that are relevant to his/her preferences and context. Most existing approaches only take into account those two features in order to build their suggestion list. However, research in recommender systems recently stressed the importance of diversified suggestions. This paper introduces a novel model for contextual suggestion based on composite retrieval, which consists of regrouping suggestions into different, topically cohesive bundles. The evaluation conducted through the TREC 2013 and 2014 Contextual Suggestion track shows that our approach is competitive and improves diversity within suggestions without degrading relevance. MOTS-CLÉS : Suggestion contextuelle, Recherche composite, Grappe, Diversité.	recommender system;relevance	Thibaut Thonet;Romain Deveaud;Iadh Ounis;Craig MacDonald	2015		10.24348/coria.2015.33	art history	Web+IR	-26.179277590723334	-60.42943413903409	88587
67d150402d60e8cfa8bf89d699f1c787e5f56c6a	information retrieval through the web and semantic knowledge-driven automatic question answering system		The rising popularity of the Information Retrieval (IR) field has created a high demand for the services which facilitates the web users to rapidly and reliably retrieve the most pertinent information. Question Answering (QA) system is one of the services which provide the adequate sentences as answers to the specific natural language questions. Despite its importance, it lacks in providing the accurate answer along with the adequate, significant information while increasing the degree of ambiguity in the candidate answers. It encompasses three phases to enhance the performance of QA system using the web as well as the semantic knowledge. The WAD approach defines the context-aware candidate sentences by using the query expansion technique and entity linking method, second, Ranks the sentences by exploiting the conditional probability between the query and candidate sentences and the automated system, third, identifies the precise answer including the reasonable, adequate information by optimal answer type identification and validation using conditional probability and ontology structure. The WAD methodology provides an answer to a posted query with maximum accuracy than baseline method.	information retrieval;question answering;world wide web	Ananthi Sheshasaayee;S. Jayalakshmi	2016		10.1007/978-981-10-3156-4_66	data web;question answering;image retrieval;semantic web;social semantic web;semantic web stack;web intelligence;information retrieval;human–computer information retrieval	NLP	-30.694732903239224	-60.56813595375458	88643
3151ebda4e88d19663a3aea81702f27396639416	cyberbullying detection using time series modeling	support vector machines time series analysis feature extraction media data models heuristic algorithms large scale integration;dynamic time warping algorithm cyber bullying detection time series modeling communication technologies internet cell phones personal digital assistants bullying problem online bullying sequential data modelling singular value decomposition representation cyber bullying attack classification methods lingustic style cyber predator questions feature weighting dimensionality reduction neural network svd time series data;support vector machines;media;dynamic time warping cyberbullying time series analysis singular value decomposition svm feature selection;large scale integration;time series analysis;heuristic algorithms;feature extraction;time series behavioural sciences computing internet singular value decomposition social aspects of automation support vector machines;data models	Cyber bullying is a new phenomenon resulting from the advance of new communication technologies including the Internet, cell phones and Personal Digital Assistants. It is a challenging bullying problem occurring in a new territory. Online bullying can be particularly damaging and upsetting because it's usually anonymous or hard to trace. In this paper, the proposed method is utilizing a dataset of real world conversations (i.e. Pairs of questions and answers between cyber predator and the victim), in which each predator question is manually annotated in terms of severity using a numeric label. We approach the issue as a sequential data modelling approach, in which the predator's questions are formulated using a Singular Value Decomposition representation. The motivation of this procedure is to study the accuracy of predicting the level of cyber bullying attack using classification methods and also to examine potential patterns between the lingustic style of each predator. More specifically, unlike previous approaches that consider a fixed window of a cyber-predator's questions within a dialogue, we exploit the whole question set and model it as a signal, whose magnitude depends on the degree of bullying content. Using feature weighting and dimensionality reduction techniques, each signal is straightforwardly parsed by a neural network that forecasts the level of insult within a question given a window between two and three previous questions. Throughout the time series modeling experiments, an interesting discovery was made. By applying SVD on the time series data and taking into account the second dimension (since the first is usually modeling trivial dependencies between instances and attributes) we observed that its plot was very similar to the plot of the class attribute. By applying a Dynamic Time Warping algorithm, the similarity of the aforementioned signals was proved to exist, providing an immediate indicator for the severity of cyber bullying within a given dialogue.	algorithm;artificial neural network;cyberbullying;data modeling;dimensionality reduction;dynamic time warping;experiment;google questions and answers;html attribute;internet;mobile phone;parsing;personal digital assistant;singular value decomposition;time series	Nektaria Potha;Manolis Maragoudakis	2014	2014 IEEE International Conference on Data Mining Workshop	10.1109/ICDMW.2014.170	data modeling;support vector machine;simulation;media;feature extraction;computer science;machine learning;time series;data mining;statistics	DB	-20.129939427576165	-55.79227683866585	88740
df9c13a5b912cc33a6dd541236f17fb29cfe79fa	integrating correlation clustering and agglomerative hierarchical clustering for holistic schema matching	correlation clustering;schema integration;agglomerative hierar chical clustering;holistic schema matching	Corresponding Author: Basel Alshaikhdeeb Faculty of Information Science and Technology, National University of Malaysia, Bangi, Malaysia Email: shaikhdeeb@gmail.com  Abstract: Holistic schema matching is the process of carrying off several number of schemas as an input and outputs the corre sp ndences among them. Treating large number of schemas may consume longer tim with poor quality. Therefore, several clustering approaches h ave been proposed in order to reduce the search space by partitioning the data into smaller portions which can facilitate the matching process. However, there is still a demand for improving the partitioning mechanism by avoiding th e random initial solutions (centroids) re-sulted from the clustering process. Such random solutions have a significant impact on the matching results. This study aims to integrate correlation clustering and agglomerati ve hierarchical clustering toward improving the effectiveness of holistic sche ma matching. The proposed integrated method avoids the random initia l so-lutions and the predefined number of centroids. Several preprocessi ng teps have been performed with using auxiliary information (domain dictionary). The experiments have been carried out on Airfare, Auto and Book datasets from UIUC Web Integration Repository. The proposed metho d as been compared with K-means and K-medoids clustering methods. As a results the proposed method has outperformed K-means and K-medoids by ac hieving 0.9, 0.93 and 0.9 of accuracy for Airfare, Auto and Book respectively.	1:1 pixel mapping;cluster analysis;column (database);correlation clustering;data dictionary;database schema;email;experiment;hierarchical clustering;holism;information science;k-means clustering;k-medoids;medoid	Basel Alshaikhdeeb;Kamsuriah Ahmad	2015	JCS	10.3844/jcssp.2015.484.489	correlation clustering;constrained clustering;data stream clustering;fuzzy clustering;computer science;canopy clustering algorithm;machine learning;pattern recognition;cure data clustering algorithm;data mining;cluster analysis;single-linkage clustering;brown clustering;hierarchical clustering of networks;clustering high-dimensional data	ML	-27.92424431316988	-60.30992806201972	88881
6ef0a23ffe225dea9377d4a495318a557c1f3d3f	semantic text summarization based on syntactic patterns	text summarization;word sense disambiguation;syntactic analysis;syntactic patterns;semantic analysis	Text summarization is machine based generation of a shortened version of a text. The summary should be a non-redundant extract from the original text. Most researches of text summarization use sentence extraction instead of abstraction to produce a summary. Extraction is depending mainly on sentences that already contained in the original input, which makes it more accurate and more concise. When all input articles are surrounding a particular event, extracting similar sentences would result in producing a highly repetitive summary. In this paper, a novel model for text summarization is proposed based on removing the non-effective sentences in producing an extract from the text. The model utilizes semantic analysis by evaluating sentences similarity. This similarity is provided by evaluating individual words similarity as well as syntactic relationships between neighboring words. These relationships addressed throughout the model as syntactic patterns. Word senses and the correlating part of speech for the word within context are provided in the semantic processing of matched patterns. The introduction of syntactic patterns knowledge supports text reduction by mapping the matched patterns into summarized ones. In addition, syntactic patterns make use of sentence relatedness evaluation in defining which sentences to keep and which to drop. Experiments proved that the model presented throughout the paper is well performing in results evaluation of compression rate, accuracy, recall and other human criteria like correctness, novelty, fluency and usefulness. Semantic Text Summarization Based on Syntactic Patterns	automatic summarization;correctness (computer science);experiment;semantic analysis (compilers);sentence extraction;word sense	Mohamed H. Haggag	2013	IJIRR	10.4018/ijirr.2013100102	natural language processing;text graph;speech recognition;computer science;automatic summarization;linguistics	NLP	-26.439169314091487	-65.99384739184052	88956
349ea40ebec934fe4522b18a3a28538ee3a9ce3f	akshaya: a framework for mining general knowledge semantics from unstructured text	text mining;topical anchors;topic expansion;topical markers;text semantics;semantic siblings;general knowledge semantics;analytical semantics	We report a tool called Akshaya, which implements a framework to mine four types of “general knowledge semantics” (analytical semantics) from unstructured text. The semantics being mined are semantic siblings, topical anchors, topic expansion and topical markers. The framework provides options to embed more such general knowledge semantic mining algorithms into it. We use a term co-occurrence graph representation of unstructured text corpora to mine these semantics relations between terms. The semantic mining algorithms use different graph algorithms like random walk, graph clustering and so on to mine semantic relations. The tool can currently read plain text documents and generate a term co-occurrence graph and perform semantic association mining on it.	algorithm;association rule learning;cluster analysis;graph (abstract data type);graph theory;mined;text corpus	Sumant Kulkarni;Srinath Srinivasa;Priyanka Shukla	2014			natural language processing;statistical semantics;semantic computing;text mining;computer science;data mining;semantic compression;database;information retrieval	Web+IR	-27.251602118993425	-63.974996407545675	89045
d78127425471574d742615d394eaa95a02d44f5f	cisc: clustered image search by conceptualization	conceptualization;clustering;image search	Clustering of images from search results can improve the user experience of image search. Most of the existing systems use both visual features and surrounding texts as signals for clustering while this paper demonstrates the use of an external knowledge base to make better sense out of the text signals in a prototype system called CISC. Once we understand the semantics of the text better, the result of the clustering is significantly improved. In addition to clustering the images by their semantic entities, our system can also conceptualize each image cluster into a set of concepts to represent the meaning of the cluster.	cluster analysis;conceptualization (information science);entity;image retrieval;knowledge base;prototype;user experience	Kaiqi Zhao;Enxun Wei;Qingyu Sui;Kenny Q. Zhu;Eric Lo	2013		10.1145/2452376.2452471	conceptualization;fuzzy clustering;computer science;data science;data mining;cluster analysis;information retrieval;clustering high-dimensional data	Web+IR	-27.6296918243325	-58.87539099605784	89047
c568a0fc287881a1c3d60646a5ff9e6ee5641cf3	semantic based chinese sentence sentiment analysis	ontologies natural language processing syntactics semantics tagging compounds algorithm design and analysis;text analysis computational linguistics natural language processing ontologies artificial intelligence semantic networks;text analysis;semantic networks;ontologies artificial intelligence;ontology sentiment analysis natural language processing;sentiment analysis;computational linguistics;domain ontology semantic based chinese sentence sentiment analysis sa method natural language processing technique nlp technique interdependence relationship syntax components;domain ontology;algorithm design;natural language processing	A sentiment analysis (SA) method for Chinese is presented in this paper. First, natural language processing (NLP) technique is used to analyze the interdependence relationship between components of the sentence, then the syntax components are detected which affect the sentence sentiment. Finally, a sentence sentiment is calculated. The sentiment of some terms is variable. To improve the precision and recall of our method, domain ontology is used to analyze the sentiment of those terms.	computation;computational linguistics;interdependence;np-completeness;natural language processing;ontology (information science);precision and recall;principle of good enough;sentiment analysis;text corpus;vocabulary	Chengxiang Yuan;Yanping Zhuang;Haohong Li	2011	2011 Eighth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)	10.1109/FSKD.2011.6019960	natural language processing;algorithm design;computer science;computational linguistics;semantic network;information retrieval;sentiment analysis	NLP	-25.166828112400076	-65.2653298692668	89120
a41d460d4afe4ecedb04d3bd4a9a96e0f5ea4152	smu-sis at tac 2010 - kbp track entity linking	query reformulation;information extraction;system performance;user experience;information system;smu;natural language processing;named entity;knowledge base	Entity linking task is a process of linking the named entity within the unstructured text, to the entity in the Knowledge Base. Entity liking to the relevant knowledge is useful in various information extraction and natural language processing applications that improve the user experiences such as search, summarization and so on. We propose the two way entity linking approach to reformulate query, disambiguate the entity and link to the relevant KB repository. This paper describes the details of our participation in TAC 2010 Knowledge Base Population track. We provided an innovative approach to disambiguate the entity by query reformulation using the query context and Wikipedia knowledge through heuristic approach. We participated in Entity-Linking task using KB text and Entity-Linking task without using KB text tasks. We developed several entity linking engines to evaluate our solution. We compared our methods with a baseline approach and analyzed the experimental results. For both the tasks our system performance is competitive with 76% and 75% mean average scores respectively.	algorithm;baseline (configuration management);entity linking;heuristic;information extraction;knowledge base;named entity;natural language processing;system management unit;wikipedia;word-sense disambiguation	Swapna Gottipati;Jing Jiang	2010			natural language processing;knowledge base;computer science;data mining;entity linking;database;information extraction;information retrieval;information system	NLP	-30.077531310120822	-62.787828628259376	89462
77d6fea87b89f9db8b4f57db3b543b293de7b91d	a bottom-up approach of web data extraction based on entity recognition and integration	web data extraction;bottom up;web pages;top down;information retrieval;text sequences bottom up approach web data extraction entity recognition entity integration web pages;data mining;arrays;html;internet;redundancy;web pages html data mining arrays labeling context redundancy;internet information retrieval;context;labeling	Nowadays, most popular methods for web data extraction (WDE) are top-down ones depending on structure. However, these techniques are not scalable enough when coming to complex pages. Consequently, we put forward a bottom-up approach for WDE based on entity recognition and integration to avoid over dependency to structure of web pages. The approach proposed focuses on primary text sequences labeling first and also gives consideration to repetitive patterns of them as well. We propose a Two-Level extraction model for entity recognition and repetitive pattern extraction algorithm for entity integration. Our approach can effectively reduce the attribute labeling mistakes. Also, we demonstrate our approach by scientifically experimental results. The conclusion is that our approach perform better than the traditional extraction techniques, especially on complex Web pages. Keywords-web data extraction; entity recognition; entity integration; bottom-up	algorithm;bottom-up parsing;entity;memory data register;pattern recognition;primary source;scalability;top-down and bottom-up design;vii;web page	Derong Shen;Jing Shan;Tiezheng Nie;Yue Kou	2011	2011 Eighth Web Information Systems and Applications Conference	10.1109/WISA.2011.37	web service;web mining;static web page;web modeling;site map;data web;web mapping;html;web design;computer science;semantic web;web navigation;top-down and bottom-up design;web page;data mining;semantic web stack;web intelligence;world wide web;website parse template;information retrieval;web server	DB	-25.07250470514276	-64.31222131876432	89625
5770bd05aca1502a2d8130098dec4a3376d704a4	using wikipedia categories and links in entity ranking	categories;search engine;wikipedia;entity ranking;xml retrieval;linkrank;xml	This paper describes the participation of the INRIA group in the INEX 2007 XML entity ranking and ad hoc tracks. We developed a system for ranking Wikipedia entities in answer to a query. Our approach utilises the known categories, the link structure of Wikipedia, as well as the link co-occurrences with the examples (when provided) to improve the effectiveness of entity ranking. Our experiments on the training data set demonstrate that the use of categories and the link structure of Wikipedia, together with entity examples, can significantly improve entity retrieval effectiveness. We also use our system for the ad hoc tasks by inferring target categories from the title of the query. The results were worse than when using a full-text search engine, which confirms our hypothesis that ad hoc retrieval and entity retrieval are two different tasks.	archive;entity;experiment;hoc (programming language);information retrieval;numeric character reference;test set;web search engine;wikipedia;xml;xml namespace	Anne-Marie Vercoustre;Jovan Pehcevski;James A. Thom	2007		10.1007/978-3-540-85902-4_28	computer science;data mining;world wide web;information retrieval	Web+IR	-29.628052405901773	-63.7764786220117	89693
7e685545efd833b54ea7366aa4cc6d157d171d96	timeline generation through evolutionary trans-temporal summarization	news timelines;component summary;trans-temporal characteristic;news evolution;timeline generation;rouge metrics;trans-temporal correlation;trans-temporal information;evolutionary trans-temporal summarization;challenging problem;facilitates fast news browsing	We investigate an important and challenging problem in summary generation, i.e., Evolutionary Trans-Temporal Summarization (ETTS), which generates news timelines from massive data on the Internet. ETTS greatly facilitates fast news browsing and knowledge comprehension, and hence is a necessity. Given the collection of time-stamped web documents related to the evolving news, ETTS aims to return news evolution along the timeline, consisting of individual but correlated summaries on each date. Existing summarization algorithms fail to utilize trans-temporal characteristics among these component summaries. We propose to model trans-temporal correlations among component summaries for timelines, using inter-date and intra-date sentence dependencies, and present a novel combination. We develop experimental systems to compare 5 rival algorithms on 6 instinctively different datasets which amount to 10251 documents. Evaluation results in ROUGE metrics indicate the effectiveness of the proposed approach based on trans-temporal information.	affinity analysis;algorithm;automatic summarization;experiment;experimental system;global optimization;mathematical optimization;source-to-source compiler;timeline;web mining;web page	Rui Yan;Liang Kong;Congrui Huang;Xiaojun Wan;Xiaoming Li;Yan Zhang	2011			natural language processing;computer science;data mining;world wide web;information retrieval	NLP	-26.301576339858983	-59.297083443888816	89726
31f0c27d27349a0b8cc728a14a2504aeeead7868	ranking approaches for gir	word frequency;information retrieval system;hybrid approach;geographic information retrieval	"""At the core of any information retrieval system is its method for ranking results in response to a user's query. Geographic Information Retrieval (GIR) systems have an added complexity for this task since the information to be included in the ranking process goes beyond text and word frequency information to encompass geographic proximity, containment and other spatial operations. The need to combine both geographic and text components into GIR systems has led to some interesting hybrid approaches in addition to the """"pure"""" spatial ranking methods based on spatial similarity. In this short survey I will look at some of the methods that have been reported in the literature and used in GIR evaluations including GeoCLEF and NTCIR GeoTime."""	geotime;information retrieval;word lists by frequency	Ray R. Larson	2011	SIGSPATIAL Special	10.1145/2047296.2047305	ranking;computer science;data mining;database;word lists by frequency;information retrieval	Web+IR	-30.76320483395443	-59.8681893867275	89878
206e56b9055a4ab00a1801a2a20ea5d47ee34f8c	overview of the mediaeval 2012 tagging task		The MediaEval 2012 Genre Tagging Task is a follow-up task of the MediaEval 2011 Genre Tagging Task and the MediaEval 2010 Wild Wild Web Tagging Task to test and evaluate retrieval techniques for video content as it occurs on the Internet, i.e., for semi-professional user generated content that is associated with annotations existing on the Social Web. The task uses the MediaEval 2012 Tagging Task (ME12TT) dataset which is based on the whole blip10,000 collection, in contrast to the MediaEval 2010 Wild Wild Web (ME10WWW) set used in previous tasks. In this task overview paper, we describe the principal characteristics of the dataset, the task itself, and the evaluation metrics used to test the particpants’ results.	digital video;semiconductor industry;tag (metadata);user-generated content	Sebastian Schmiedeke;Christoph Kofler;Isabelle Ferrané	2012			computer science	NLP	-30.61362057789452	-62.62455253129003	90070
c29fc13ceb79741fd753b032e76a90550e0d57d8	a unified approach to mapping and clustering of bibliometric networks	bibliometric map;digital library;informing science;data analysis;clustering;unified approach;mapping	In the analysis of bibliometric networks, researchers often use mapping and clustering techniques in a combined fashion. Typically, however, mapping and clustering techniques that are used together rely on very different ideas and assumptions. We propose a unified approach to mapping and clustering of bibliometric networks. We show that the VOS mapping technique and a weighted and parameterized variant of modularity-based clustering can both be derived from the same underlying principle. We illustrate our proposed approach by producing a combined mapping and clustering of the most frequently cited publications that appeared in the field of information science in the period 1999–2008.	bibliometrics;cluster analysis;information science	Ludo Waltman;Nees Jan van Eck;Ed C. M. Noyons	2010	J. Informetrics	10.1016/j.joi.2010.07.002	digital library;computer science;bioinformatics;data science;data mining;cluster analysis;data analysis;brown clustering;world wide web;clustering high-dimensional data	AI	-25.475686079804117	-59.32272192215456	90164
08985bd452c700bed5aa6f9317605d6d3c77645d	webpage sentiment analysis using common ontology wordnet		Research in Sentiment Analysis has shown rapid progress since late 90s. It is an important research area as analyzing user’s feedback is useful for business analysis, product comparison, counter intelligence, and poll prediction. Despite the rapid surge of Sentiment Analysis research, many unresolved research questions remain. One of the biggest concerns is the Semantic Gap, which involves translating machine understandable form to human understandable form. Though research has been carried out for machine to understand human language, it is still not capable to address the problem mentioned as human languages are diverse and complex. WordNet, for example, attempt to address this issue by incorporating large lexical database for English, with various functionalities to manipulate this database. Recently, WordNet provides multilingual support, which is very helpful to address the diverse human languages. In this paper, we propose a novel multilingual common ontology tool to analyze user’s feedback and opinion. Unlike other existing state of the art tools, our tool is capable of handling multi languages regardless of the webpage layout. Experimental results show that our tool is highly efficient in analyzing opinion from social networking sites.	sentiment analysis;web page;wordnet	Huey Jing Toh;Jer Lang Hong	2014	Austr. J. Intelligent Information Processing Systems		natural language processing;computer science;data mining;world wide web	NLP	-23.27515017710566	-63.65691659092063	90217
0f1671297c0d0a1d7a70a3a9680a2f309f29eb0a	news across languages - cross-lingual document similarity and event tracking		In today’s world, we follow news which is distributed globally. Significant events are reported by different sources and in different languages. In this work, we address the problem of tracking of events in a large multilingual stream. Within a recently developed system Event Registry we examine two aspects of this problem: how to compare articles in different languages and how to link collections of articles in different languages which refer to the same event. Building on previous work, we show there are methods which scale well and can compute a meaningful similarity between articles from languages with little or no direct overlap in the training data. Using this capability, we then propose an approach to link clusters of articles across languages which represent the same event1.		Jan Rupnik;Andrej Muhic;Gregor Leban;Primoz Skraba;Blaz Fortuna;Marko Grobelnik	2017		10.24963/ijcai.2017/720	computer science;natural language processing;information retrieval;artificial intelligence	Web+IR	-29.449285557687688	-63.50986059264329	90230
38ccc39d5494b379598423e9be6ab9ab2f4c0bd8	word co-occurrence features for text classification	classification algorithm;text mining;classification;text classification;feature extraction;bag of words	In this article we propose a data treatment strategy to generate new discriminative features, called compound-features (or c-features), for the sake of text classification. These c-features are composed by terms that co-occur in documents without any restrictions on order or distance between terms within a document. This strategy c-features. The idea is that, when c-features are used in conjunction with singlefeatures, the ambiguity and noise inherent to their bag-of-words representation are reduced. We use c-features composed of two terms in order to make their usage computationally feasible while improving the classifier effectiveness. We test this approach with several classification algorithms and single-label multi-class text collections. Experimental results demonstrated gains in almost all evaluated scenarios, from the simplest algorithms such as kNN (13% gain in micro-average F1 in the 20 Newsgroups collection) to the most complex one, the state-of-the-art SVM (10% gain in macro-average F1 in the collection OHSUMED). & 2011 Elsevier B.V. All rights reserved.	algorithm;bag-of-words model;binary classification;document classification;dominance drawing;experiment;feature extraction;heaps' law;linear function;null character;vocabulary	Fábio Figueiredo;Leonardo C. da Rocha;Thierson Couto;Thiago Salles;Marcos André Gonçalves;Wagner Meira	2011	Inf. Syst.	10.1016/j.is.2011.02.002	text mining;feature extraction;biological classification;computer science;bag-of-words model;machine learning;pattern recognition;data mining;one-class classification	Web+IR	-20.799250757826446	-64.71605846951378	90566
be3fb2a7a300f609c4f165227bb43e8e5a471705	mining the blogosphere for sociological inferences		The blogosphere, which is the name given to the universe of all blog sites, is now a collection of a tremendous amount of user generated data. The ease & simplicity of creating blog posts and their free form and unedited nature have made the blogosphere a rich and unique source of data, which has attracted people and companies across disciplines to exploit it for varied purposes. The large volume of data requires developing appropriate automated techniques for searching and mining useful inferences from the blogosphere. The valuable data contained in posts from a large number of users across geographic, demographic and cultural boundaries provide a rich opportunity for not only commercial exploitation but also for cross-cultural psychological & sociological research. This paper tries to present the broader picture in and around this theme, chart the required academic and technological framework for the purpose and presents initial results of an experimental work to demonstrate the plausibility of the idea.	blog;blogosphere;plausibility structure	Vivek Kumar Singh	2010		10.1007/978-3-642-14834-7_51	data science;data mining	HCI	-24.384916511376282	-55.73253451087394	90623
41069b73bbb40776d393bdeedf0fff57c67b96cf	deep neural networks understand investors better		Abstract Studies that seek to examine the impact of sentiment in financial markets have been affected by inaccurate sentiment measurement and the use of inappropriate data. This study applies state-of-the-art techniques from the domain-general sentiment analysis literature to construct a more accurate decision support system that generates demonstrable improvement in investor sentiment classification performance compared with previous studies. The inclusion of emojis is shown significantly improve sentiment classification in traditional algorithms. Moreover, deep neural networks with domain-specific word embeddings outperform the traditional approaches for the classification of investor sentiment. The approach to sentiment classification outlined in this paper can be applied in future empirical tests that examine the impact of investor sentiment on financial markets.	artificial neural network;deep learning	Nader Mahmoudi;Paul Docherty;Pablo Moscato	2018	Decision Support Systems	10.1016/j.dss.2018.06.002	data mining;sentiment analysis;artificial neural network;financial market;computer science;decision support system	ML	-21.319466615369464	-58.175146323424435	90662
a9a520ab9aef3b1bd78346a8a1db923d750882df	network attack scenarios extraction and categorization by mining ids alert streams	ddos attack;vector space model;network security;semantic network;intrusion detection;first order;mutual information;semantic space;text categorization;first order logic;intrusion detection system;alert correlation	The past few years have witnessed significant increase in DDoS attacks on Internet, prompting network security as a great concern. With the attacks getting more sophisticated, automatically reasoning the attack scenarios in real time and categorizing those scenarios become a critical challenge. However,the overwhelming flow of events generated by Intrusion Detection System (IDS) sensors make it hard for security administrators to uncover hidden attack plans. This paper presents a semantic vector space model to extract and categorize attack scenarios based on First-order Logics (FOL) and linguistics. The modified Case Grammar is introduced to formalize the heterogeneous IDS alerts into uniform structured alert streams. The attack resolution is then used to generate attack semantic network. Afterwards, mutual information is used to determine the alert semantic context range. Based on the attack ontology and alert contexts, attack scenarios are extracted and the alerts are represented as attack semantic space vectors. Finally text categorization technique are used to categorize the intrusion stages. The preliminary results show our model has better performance than the traditional alert correlations.	alert correlation;categorization;denial-of-service attack;document classification;first-order logic;first-order predicate;human-readable medium;intrusion detection system;mutual information;network security;semantic network;sensor;simulation	Wei Yan	2005	J. UCS	10.3217/jucs-011-08-1367	intrusion detection system;computer science;network security;first-order logic;data mining;application layer ddos attack;world wide web;computer security	Security	-19.32232773314335	-59.137395350961846	90761
8e5f1448c34cfd78b85ef2601ddc2fda84fde101	categorizing web queries according to geographical locality	search engine;web pages;real estate;search engines;information retrieval;query classification;web search engine;machine learning;query modification;web search;san francisco	Web pages (and resources, in general) can be characterized according to their geographical locality. For example, a web page with general information about wildflowers could be considered a global page, likely to be of interest to a geographically broad audience. In contrast, a web page with listings on houses for sale in a specific city could be regarded as a local page, likely to be of interest only to an audience in a relatively narrow region. Similarly, some search engine queries (implicitly) target global pages, while other queries are after local pages. For example, the best results for query [wildflowers] are probably global pages about wildflowers such as the one discussed above. However, local pages that are relevant to, say, San Francisco are likely to be good matches for a query [houses for sale] that was issued by a San Francisco resident or by somebody moving to that city. Unfortunately, search engines do not analyze the geographical locality of queries and users, and hence often produce sub-optimal results. Thus query [wildflowers] might return pages that discuss wildflowers in specific U.S. states (and not general information about wildflowers), while query [houses for sale] might return pages with real estate listings for locations other than that of interest to the person who issued the query. Deciding whether an unseen query should produce mostly local or global pages---without placing this burden on the search engine users---is an important and challenging problem, because queries are often ambiguous or underspecify the information they are after. In this paper, we address this problem by first defining how to categorize queries according to their (often implicit) geographical locality. We then introduce several alternatives for automatically and efficiently categorizing queries in our scheme, using a variety of state-of-the-art machine learning tools. We report a thorough evaluation of our classifiers using a large sample of queries from a real web search engine, and conclude by discussing how our query categorization approach can help improve query result quality.	categorization;locality of reference;machine learning;web 2.0;web page;web query classification;web search engine;web search query	Luis Gravano;Vasileios Hatzivassiloglou;Richard Lichtenstein	2003		10.1145/956863.956925	query expansion;web query classification;web search engine;computer science;artificial intelligence;machine learning;data mining;database;web search query;range query;world wide web;information retrieval;query language;search engine	Web+IR	-31.488799232878858	-53.12250057348901	90849
562b7b9aed057cdffa338070fd0c6b7e7c6bdfe1	gloss: text-source discovery over the internet	search and retrieval;digital library;digital libraries;vector space;distributed information retrieval;text database;internet search and retrieval;retrieval model;text databases	The dramatic growth of the Internet has created a new problem for users: location of the relevant sources of documents. This article presents a framework for (and experimentally analyzes a solution to) this problem, which we call the text-source discovery problem. Our approach consists of two phases. First, each text source exports its contents to a centralized service. Second, users present queries to the service, which returns an ordered list of promising text sources. This article describes GlOSS, Glossary of Servers Server, with two versions: bGlOSS, which provides a Boolean query retrieval model, and vGlOSS, which provides a vector-space retrieval model. We also present hGlOSS, which provides a decentralized version of the system. We extensively describe the methodology for measuring the retrieval effectiveness of these systems and provide experimental evidence, based on actual data, that all three systems are highly effective in determining promising text sources for a given query.	centralized computing;experiment;gloss (annotation);glossary;information retrieval;internet	Luis Gravano;Hector Garcia-Molina;Anthony Tomasic	1999	ACM Trans. Database Syst.	10.1145/320248.320252	document retrieval;query expansion;digital library;cognitive models of information retrieval;vector space;computer science;concept search;data mining;database;world wide web;vector space model;data retrieval;information retrieval;human–computer information retrieval	DB	-31.111005894675053	-56.43668461621293	90869
26d39a69ee53e83f54ef680a4cae87043201e3b1	crawling bug tracker for semantic bug search	bug tracking system;search engine;tracking system;data model;semi structured data;bug crawler;semantic search	The Web has become an important knowledge source for resolving system installation problems and for working around software bugs. In particular, web-based bug tracking systems offer large archives of useful troubleshooting advice. However, searching bug tracking systems can be time consuming since generic search engines do not take advantage of the semi-structured knowledge recorded in bug tracking systems. We present work towards a semantics-based bug search system which tries to take advantage of the semi-structured data found in many widely used bug tracking systems. We present a study of bug tracking systems and we describe how to crawl them in order to extract semi-structured data. We describe a unified data model to store bug tracking data. The model has been derived from the analysis of the most popular systems. Finally, we describe how the crawled data can be fed into a semantic search engine to facilitate semantic search.	archive;bug tracking system;data model;semantic search;semi-structured data;semiconductor industry;software bug;web application;web search engine	Ha Manh Tran;Georgi Chulkov;Jürgen Schönwälder	2008		10.1007/978-3-540-87353-2_5	semi-structured data;tracking system;semantic search;data model;computer science;data mining;database;world wide web;information retrieval;search engine	AI	-29.80482642032501	-54.34143234609553	91067
296247bad16a06df446dbc6897c9872056ed59de	rough set based approach to text classification	text analysis knowledge acquisition rough set theory;decision making machine learning text classification feature selection rough set;rough set theory;text analysis;text classification;machine learning;knowledge acquisition;feature selection;text classification rough set decision technique rough set decision making approach knowledge usage knowledge extraction feature selection methods information management information organisation textual document set;rough set;approximation methods training uncertainty vectors support vector machines feature extraction decision making	Textual document set has become an important and rapidly growing information source in the web. Text classification is one of the crucial technologies for information organisation and management. Text classification has become more and more important and attracted wide attention of researchers from different research fields. In this paper, many feature selection methods, the implement algorithms and applications of text classification are introduced firstly. However, because there are much noise in the knowledge extracted by current data-mining techniques for text classification, it leads to much uncertainty in the process of text classification which is produced from both the knowledge extraction and knowledge usage, therefore, more innovative techniques and methods are needed to improve the performance of text classification. It has been a critical step with great challenge to further improve the process of knowledge extraction and effectively utilization of the extracted knowledge. Rough Set decision making approach is proposed to use Rough Set decision techniques to more precisely classify the textual documents which are difficult to separate by the classic text classification methods. The purpose of this paper is to give an overview of existing text classification technologies, to demonstrate the Rough Set concepts and the decision making approach based on Rough Set theory for building more reliable and effective text classification framework with higher precision, to set up an innovative evaluation metric named CEI which is very effective for the performance assessment of the similar research, and to propose a promising research direction for addressing the challenging problems in text classification, text mining and other relative fields.	algorithm;data mining;document classification;feature selection;information source;rough set;set theory;statistical classification;text mining	Libiao Zhang;Yuefeng Li;Chao Sun;Wanvimol Nadee	2013	2013 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)	10.1109/WI-IAT.2013.190	text graph;text mining;computer science;machine learning;pattern recognition;data mining;one-class classification	Web+IR	-23.213190219506505	-58.15506438866059	91155
36dec5f23a63bc701fee46610ee68b81080878cd	domain-independent sentence type classification: examining the scenarios of scientific abstracts and scrum protocols	information extraction;text classification;knowledge discovery	The amount of available textual information in everybody's daily environment is increasing steadily. To satisfy a user's information needs, the user has to examine numerous documents until the required information has been found. Additionally, the relevant information is often contained in only short sections of the considered documents. This leads to a high amount of irrelevant text the user has to read what could be solved by filtering relevant information within textual documents automatically. In this article we present our findings on the classification of sentences according to the type of information contained. Our evaluation has been conducted on documents from the field of abstracts of scientific publications and protocols of Scrum retrospective meetings. The results show the feasibility of our approach for finding a higher percentage of relevant information within textual documents and hence reducing the information overload for the users.	document;information needs;information overload;relevance;scientific literature;scrum (software development)	Sebastian Schmidt;Steffen Schnitzer;Christoph Rensing	2014		10.1145/2637748.2638409	computer science;information filtering system;data mining;world wide web;information retrieval	Web+IR	-29.417998013334014	-59.838330451655686	91361
ebd823d7738132b998dd148fe4219978f140b09d	social summarization method for feedback comments in online auction	automatic summarization	Online auctions provide feedback comments for sellers written by buyers. Users select which seller to buy a product from by making reference to these comments. However, these feedback comments include a lot of courtesies in addition to being vast in quantity, and so comparing several sellers represents a lot of work for users. To address this problem, in this research the authors propose a method (the Social Summarization method) to summarize seller feedback comments by using social relationships in online auctions. In this method the authors focus one by one on buyers who wrote feedback comments for sellers in order to eliminate courtesy phrases that are not useful for representing the character of a seller. They then compare the feedback comments that a buyer writes for other sellers with the feedback comments that that buyer writes for the target seller. The authors investigate to what extent phrases summarized using their proposed method and using a general method that performs summarization without attention to sellers are different by using the feedback comments from a real online auction. © 2006 Wiley Periodicals, Inc. Syst Comp Jpn, 37(8): 38–55, 2006; Published online in Wiley InterScience (www.interscience.wiley.com). DOI 10.1002/scj.20502	automatic summarization;comment (computer programming);digi-comp i;feedback;john d. wiley	Hanako Ohno;Yukitaka Kusumura;Yoshinori Hijikata;Shogo Nishida	2006	Systems and Computers in Japan	10.1002/scj.20502	computer science;artificial intelligence;automatic summarization	AI	-27.065197700193597	-55.52108224956449	91408
e239f6059d70b6e538fef9930538240989f854bd	finding a web community by maximum flow algorithm with hits score based capacity	graph theory;maximum flow;web pages;performance evaluation;hits score based capacity;web community;authority scores;average precision;information retrieval;graphs;web pages bipartite graph performance evaluation database systems;internet;database systems;graph theory internet web sites information retrieval;web sites;experiments;web archives web community maximum flow algorithm hits score based capacity edge capacity authority scores hub scores web pages experiments graphs world wide web;edge capacity;world wide web;web archives;bipartite graph;maximum flow algorithm;hub scores	"""We propose an edge capacity based on hub and authority scores, and examine the effects of using the edge capacity on the method for extracting Web communities using maximum flow algorithm proposed by G. Flake et al. (2000). A Web community is a collection of Web pages in which a common (or related) topic is taken up. In recent years, various methods for finding Web communities have been proposed. G. Flake et al.'s method, which is based on maximum flow algorithm, has a big advantages: """"topic drift"""" does not easily occur. On the other hand, it sets the edge capacity to a fixed value for every edge, which is one of the major cause of failing to obtain a proper Web community. Our approach, which is using HITS score based edge capacity, effectively extracts Web pages retaining well-balanced in both global and local relations to the given seed node. We examined the effects by the experiments for randomly selected 20 topics using Web archives in Japan crawled in 2002. The result confirmed that the average precision rose approximately 20%."""	algorithm;archive;bridge (graph theory);experiment;failure;flake (software);information retrieval;maximum flow problem;n-flake;randomness;usb hub;web page;world wide web	Noriko Imafuji;Masaru Kitsuregawa	2003	Eighth International Conference on Database Systems for Advanced Applications, 2003. (DASFAA 2003). Proceedings.	10.1109/DASFAA.2003.1192373	maximum flow problem;computer science;graph theory;data mining;database;world wide web;information retrieval	ML	-29.393358146364047	-55.63500165303995	91607
49cb1e067527038ace91f2fe6f4fbdcd01189b1d	mobile-based intelligent transportation for bus commuters based on twitter analytics	electronic mail;training;testing;roads;computer science;twitter	In this paper, we present an intelligent, state-of-the-art, mobile-based transportation system called SAFAR (Safe and Fast around the Road), which provides dynamic information to Karachi bus commuters concerning any type of violence incident which has occurred farther ahead from their current location on the current bus route. Using named entity recognition techniques, we have trained SAFAR to recognize the location and method of violence incident along with the casualty information (if available) from live Twitter news feeds. Using the well-known A* heuristic search algorithm, SAFAR also recommends alternative routes to reach the destination in case of any violence up ahead. SAFAR has a competitive violence detection accuracy of 80% on a test corpus as well as in an online evaluation with real users. Finally, a subjective evaluation of these users reveals satisfactory performance of SAFAR across several dimensions.	a* search algorithm;deep learning;expert system;finite-state machine;graphical user interface;heuristic;microsoft windows;named entity;named-entity recognition;online and offline;whole earth 'lectronic link;windows phone	Tariq Mahmood;Ghulam Mujtaba Shaikh;Nor Liyana Mohd Shuib;Nikkishah Zulfiqar Ali;Amir Bawa;Saima Karim	2016	2016 International Conference on Frontiers of Information Technology (FIT)	10.1109/FIT.2016.048	simulation;engineering;advertising;computer security	Robotics	-21.14208379240908	-56.658965391241665	91745
1fee98c45f8723d1b500464a1cf2cf497ec58663	evaluation measures for hierarchical classification: a unified view and novel approaches	evaluation measures;dag structured class hierarchies;hierarchical classification;tree structured class hierarchies;evaluation	Hierarchical classification addresses the problem of classifying items into a hierarchy of classes. An important issue in hierarchical classification is the evaluation of different classification algorithms, an issue which is complicated by the hierarchical relations among the classes. Several evaluation measures have been proposed for hierarchical classification using the hierarchy in different ways without however providing a unified view of the problem. This paper studies the problem of evaluation in hierarchical classification by analysing and abstracting the key components of the existing performance measures. It also proposes two alternative generic views of hierarchical evaluation and introduces two corresponding novel measures. The proposed measures, along with the state-of-the-art ones, are empirically tested on three large datasets from the domain of text classification. The empirical results illustrate the undesirable behaviour of existing approaches and how the proposed methods overcome most of these problems across a range of cases.	algorithm;document classification;hierarchical clustering	Aris Kosmopoulos;Ioannis Partalas;Éric Gaussier;Georgios Paliouras;Ion Androutsopoulos	2014	Data Mining and Knowledge Discovery	10.1007/s10618-014-0382-x	evaluation;machine learning;pattern recognition;data mining;mathematics	Web+IR	-20.06487149156296	-62.36717836839128	91950
30e9300e9393975258a8b5d892c84522d336bd0b	text mining of business news for forecasting	market research;text mining;business news content;business news classification;market trend forecasting;text analysis;data mining;classification;text mining business economic forecasting computer science chemical technology data mining engines databases consumer electronics humans;text analysis business data processing classification data mining demand forecasting market research;business data processing;market trend forecasting text mining business news forecasting business news content business news classification;business news forecasting;demand forecasting	In this paper, we analyze the relation between the content of business news and long-term market trends. We describe cleansing and classification of business news, we investigate how much similarity good news and bad news have, and how their ratio behaves in context of long-terms market trends. We have processed more than 400 thousand business news coming from the years 1999 to 2005. We present results of our experiments and their possible impact on forecasting of long-term market trends	experiment;text mining	Petr Kroha;Ricardo A. Baeza-Yates;Bjorn Krellner	2006	17th International Workshop on Database and Expert Systems Applications (DEXA'06)	10.1109/DEXA.2006.135	text mining;demand forecasting;biological classification;computer science;data science;data mining	ML	-21.04254005539415	-52.15199973181215	92027
69b065de54cb7e47777fe6d23f919db5e36f133b	multiple queries as bandit arms	query pooling;multi armed bandits	Existing retrieval systems rely on a single active query to pull documents from the index. Relevance feedback may be used to iteratively refine the query, but only one query is active at a time. If the user's information need has multiple aspects, the query must represent the union of these aspects. We consider a new paradigm of retrieval where multiple queries are kept ``active'' simultaneously. In the presence of rate limits, the active queries take turns accessing the index to retrieve another ``page'' of results. Turns are assigned by a multi-armed bandit based on user feedback. This allows the system to explore which queries return more relevant results and to exploit the best ones. In empirical tests, query pools outperform solo, combined queries. Significant improvement is observed both when the subtopic queries are known in advance and when the queries are generated in a user-interactive process.	coat of arms;information needs;multi-armed bandit;programming paradigm;relevance feedback;structure of observed learning outcome	Cheng Li;Paul Resnick;Qiaozhu Mei	2016		10.1145/2983323.2983816	sargable;query optimization;query expansion;web query classification;ranking;computer science;machine learning;data mining;database;web search query;range query;world wide web;information retrieval;spatial query	DB	-32.58717791081988	-52.55489124728019	92107
695deca3e6d1bbc290a81294280e4517ef2bb4a0	federated entity search using on-the-fly consolidation		Nowadays, search on the Web goes beyond the retrieval of textual Web sites and increasingly takes advantage of the growing amount of structured data. Of particular interest is entity search, where the units of retrieval are structured entities instead of textual documents. These entities reside in different sources, which may provide only limited information about their content and are therefore called “uncooperative”. Further, these sources capture complementary but also redundant information about entities. In this environment of uncooperative data sources, we study the problem of federated entity search, where redundant information about entities is reduced on-the-fly through entity consolidation performed at query time. We propose a novel method for entity consolidation that is based on using language models and completely unsupervised, hence more suitable for this on-the-fly uncooperative setting than state-of-the-art methods that require training data. Further, we apply the same language model technique to deal with the federated search problem of ranking results returned from different sources. Particular novel are the mechanisms we propose to incorporate consolidation results into this ranking. We perform experiments using real Web queries and data sources. Our experiments show that our approach for federated entity search with on-the-fly consolidation improves upon the performance of a state-of-the-art preference aggregation baseline and also benefits from consolidation.	application programming interface;baseline (configuration management);entity;experiment;federated search;federation (information technology);language model;preference learning;search problem;semiconductor consolidation;unsupervised learning;world wide web	Daniel M. Herzig;Peter Mika;Roi Blanco;Thanh Tran	2013		10.1007/978-3-642-41335-3_11	computer science;data mining;database;world wide web;information retrieval	Web+IR	-28.80231134290029	-59.70597433255954	92116
52f807da2e1ab9ad126a04d4d088a6211cfd3bad	recherche de microblogs : quels critères pour raffiner les résultats des moteurs usuels de ri ?		The last few years saw the advent of microblogging platforms, such as Twitter. Such an appeal may be due to platforms’ features allowing to easily and quickly share information in real-time. Microbloggers, while posting microblogs, search for fresh information related to their interests. Finding good results concerning the given subjects needs to consider the characteristics of microblogs, such as short length of messages, poor syntax and credibility of the sender. In this paper, we evaluate the impact of some microblog features on search result quality. We consider three groups of features: content-based, hypertext-based, and author-based. Our experiments on the TREC Tweets 2011 collection show the interest of hypertext-based features. MOTS-CLÉS : Microblog, Twitter, moteur de recherche	experiment;hypertext;rs-232;real-time computing;real-time web;text retrieval conference	Firas Damak;Karen Pinel-Sauvagnat;Guillaume Cabanac	2012		10.24348/coria.2012.317	history;performance art	Web+IR	-25.246080701532662	-55.62667167593172	92129
398fda0709349ec3645e730657ae6cad088c70d8	detecting censorable content on sina weibo: a pilot study		This study provides preliminary insights into the linguistic features that contribute to Internet censorship in mainland China. We collected a corpus of 344 censored and uncensored microblog posts that were published on Sina Weibo and built a Naive Bayes classifier based on the linguistic, topic-independent, features. The classifier achieves a 79.34% accuracy in predicting whether a blog post would be censored on Sina Weibo.	blog;call of duty: black ops;content-control software;german parliamentary committee investigating the nsa spying scandal;naive bayes classifier;social media	Kei Yin Ng;Anna Feldman;Christopher S. Leberknight	2018		10.1145/3200947.3201037	mainland china;naive bayes classifier;data mining;microblogging;computer science;social media;classifier (linguistics);internet censorship	NLP	-20.18325198971652	-53.66815939552116	92154
1c1dbf54d9e31a040641f6164bf8a8220ae895ae	analysis of the relation between stock price returns and headline news using text categorization	text mining;asset management;naive bayes;stock price return;stock price;classification accuracy;text categorization;headline news	In this paper, we analyze about the relation between stock price returns and Headline News. Headline News is very important sources of information in asset management, and is sent in large quantities every day. We study the effect of more than 13,000 Headline News sent from JIJI PRESS. We classify Headline News using Text Categorization and analyze the reaction of a stock price return for every type of News. From our research, we figure out following issues; 1) we make the Text Categorization System that has about 80% of classification accuracy, 2) this system can extract effective information to stock price returns from Headline News.	algorithm;categorization;document classification;naive bayes classifier	Satoru Takahashi;Masakazu Takahashi;Hiroshi Takahashi;Kazuhiko Tsuda	2007		10.1007/978-3-540-74827-4_167	marketing;advertising;business;commerce	NLP	-20.287354450245843	-53.13676939226206	92316
0d1746f5e43f13aeecdee15a77e6411a31c0e9b2	managing email overload with an automatic nonparametric clustering approach	knowledge management;swinburne;email;automatic generation;cluster system;text clustering;cognitive load;quality measures;overload	Email overload is a recent problem that there is increasingly difficulty people have faced to process the large number of emails received daily. Currently this problem becomes more and more serious and it has already affected the normal usage of email as a knowledge management tool. It has been recognized that categorizing emails into meaningful groups can greatly save cognitive load to process emails and thus this is an effective way to manage email overload problem. However, most current approaches still require significant human input when categorizing emails. In this paper we develop an automatic email clustering system, underpinned by a new nonparametric text clustering algorithm. This system does not require any predefined input parameters and can automatically generate meaningful email clusters. Experiments show our new algorithm outperforms existing text clustering algorithms with higher efficiency in terms of computational time and clustering quality measured by different gauges.	algorithm;categorization;cluster analysis;computation;computer cluster;email;experiment;jaccard index;knowledge management;simple matching coefficient;time complexity	Yang Xiang;Wanlei Zhou;Jinjun Chen	2007		10.1007/978-3-540-74784-0_9	document clustering;computer science;data science;machine learning;data mining;database;cognitive load;world wide web	Web+IR	-27.256164453372374	-59.06962388327367	92344
2633f0beaeead498af4e2b2a33a40bf4ffd6f805	topic tracking using chronological term ranking		Topic tracking (TT) is an important component of topic detection and tracking (TDT) applications. TT algorithms aim to determine all subsequent stories of a certain topic based on a small number of initial sample stories. We propose an alternative similarity measure based on chronological term ranking (CTR) concept to quantify the relatedness among news articles for topic tracking. The CTR approach is based on the fact that in general important issues are presented at the beginning of news articles. By following this observation we modify the traditional Okapi BM25 similarity measure using the CTR concept. Using a large standard test collection we show that our method provides a statistically significantly improvement with respect to the Okapi BM25 measure. The highly successful performance indicates that the approach can be used in real applications.	algorithm;blog;coefficient;cosine similarity;information filtering system;okapi bm25;portals;real life;semantic similarity;similarity measure;time-domain reflectometry	Bilge Acun;Alper Baspinar;Ekin Oguz;M. Ilker Saraç;Fazli Can	2012		10.1007/978-1-4471-4594-3_36	information retrieval	Web+IR	-26.632108399899987	-52.88202962003932	92379
2ca9c01f26493917ce497465adf50e1056d6165f	domain taxonomy learning from text: the subsumption method versus hierarchical clustering	text mining;association rules;classification;clustering;ontologies	This paper proposes a framework to automatically construct taxonomies from a corpus of text documents. This framework first extracts terms from documents using a part-of-speech parser. These terms are then filtered using domain pertinence, domain consensus, lexical cohesion, and structural relevance. The remaining terms represent concepts in the taxonomy. These concepts are arranged in a hierarchy with either the extended subsumption method that accounts for concept ancestors in determining the parent of a concept or a hierarchical clustering algorithm that uses various text-based window and document scopes for concept co-occurrences. Our evaluation in the field of management and economics indicates that a trade-off between taxonomy quality and depth must be made when choosing one of these methods. The subsumption method is preferable for shallow taxonomies, whereas the hierarchical clustering algorithm is recommended for deep taxonomies.	algorithm;automatic taxonomy construction;cluster analysis;cohesion (computer science);hierarchical clustering;relevance;subsumption architecture;taxonomy (general);text corpus;text-based (computing)	Jeroen de Knijff;Flavius Frasincar;Frederik Hogenboom	2013	Data Knowl. Eng.	10.1016/j.datak.2012.10.002	natural language processing;text mining;association rule learning;biological classification;computer science;ontology;data mining;database;cluster analysis;information retrieval	AI	-25.72951925783338	-65.3677196385221	92611
fa28286391fa4fca3cd83e5e4b24cc96f9accceb	graph based tweet entity linking using dbpedia	graph centrality graph based tweet entity linking dbpedia twitter named entity linking information extraction knowledge base system;joining processes semantics knowledge based systems context information services electronic publishing internet;semantics;information services;internet;social networking online graph theory knowledge based systems natural language processing;joining processes;electronic publishing;tweet annotation semantic web named entity linking named entity recognition linked open data natural language processing dbpedia centrality algorithm;context;knowledge based systems	Twitter has became an invaluable source of information, due to his dynamic nature with more than 400 million tweets posted per day. Determining what an individual post is about can be a non trivial task because his high contextualization and his informal nature. Named Entity Linking (NEL) is a subtask of information extraction that aims to ground entity mentions to their corresponding node in a Knowledge Base (KB), which requires a disambiguation step, because many resources can be matched to the same entity that lead to synonymy and polysemy problems. To overcome these problems, especially in the context of short text, we present a novel system for tweet entity linking based on graph centrality and DBpedia as knowledge base. Our approach relies on the assumption that related entities tend to appear in the same tweet as tweets are topic specific. Also, we address the problem of irregular name mentions. Finally, to show the effectiveness of our system we evaluate it using a real twitter dataset and compare it to a well known state-of-the-art named entity linking system for short text.	centrality;dbpedia;entity linking;hashtag;information extraction;information source;knowledge base;named entity;run time (program lifecycle phase);word-sense disambiguation	Fahd Kalloubi;El Habib Nfaoui;Omar El Beqqali	2014	2014 IEEE/ACS 11th International Conference on Computer Systems and Applications (AICCSA)	10.1109/AICCSA.2014.7073240	natural language processing;the internet;computer science;data mining;entity linking;database;semantics;electronic publishing;world wide web;information system	NLP	-27.620230561815454	-65.20477561609125	92720
f9e9b172bac09c910a17957cf6e0bf1ffc6a4c80	modeling geographic, temporal, and proximity contexts for improving geotemporal search	information retrieval;search strategies	Traditional information retrieval (IR) systems show significant limitations on returning relevant documents that satisfy the user’s information needs. In particular, to answer geographic and temporal user queries, the IR task becomes a nonstraightforward process where the available geographic and temporal information is often unstructured. In this article, we propose a geotemporal search approach that consists of modeling and exploiting geographic and temporal query context evidence that refers to implicit multivarying geographic and temporal intents behind the query. Modeling geographic and temporal query contexts is based on extracting and ranking geographic and temporal keywords found in pseudo-relevant feedback (PRF) documents for a given query. Our geotemporal search approach is based on exploiting the geographic and temporal query contexts separately into a probabilistic ranking model and jointly into a proximity ranking model. Our hypothesis is based on the concept that geographic and temporal expressions tend to co-occur within the document where the closer they are in the document, the more relevant the document is. Finally, geographic, temporal, and proximity scores are combined according to a linear combination formula. An extensive experimental evaluation conducted on a portion of the New York Times news collection and the TREC 2004 robust retrieval track collection shows that our geotemporal approach outperforms significantly a well-known baseline search and the best known geotemporal search approaches in the domain. Finally, an in-depth analysis shows a positive correlation between the geographic and temporal query sensitivity and the retrieval performance. Also, we find that geotemporal distance has a positive impact on retrieval performance generally.	baseline (configuration management);information needs;information retrieval;primitive recursive function;temporal expressions;the new york times	Mariam Daoud;Xiangji Huang	2013	JASIST	10.1002/asi.22648	query expansion;ranking;computer science;data mining;database;world wide web;information retrieval	Web+IR	-30.73728857238005	-59.85358168196207	92821
6dd08e28b6fa81f51b8a029a774614976d87d413	web design based on user browsing patterns	web design	It is hard to organize a website such that pages are located where users expect to find them. Consider a visitor to an e-Commerce website in searching for a scanner. There are two ways he could find information he is looking for. One is to use the search function provided by the website. The other one is to follow the links on the website. This chapter focuses on the second case. Will he click on the link “Electronics” or “Computers” to find the scanner? For the website designer, should the scanner page be put under Electronics, Computers or both? This problem occurs across all kinds of websites, including B2C shops, B2B marketplaces, corporate web-sites and content websites. Through web usages mining, we can automatically discover pages in a website whose location is different from where users expect to find them. This problem of matching website organization with user expectations is pervasive across most websites. Since web users are heterogeneous, the question is essentially how to design a website so that majority of the users find it easy to navigate. Here, we focus on the problem of browsing within a single domain/web site (search engines are not involved since it’s a totally different way of finding information on a web site.) There are numerous reasons why users fail to find the information they are looking for when browse on a web site. Here in this chapter, we focus on the following reason. Users follow links when browsing online. Information scent guides them to select certain links to follow in search for information. If the content is not located where the users expect it to be, the users will fail to find it. How we analyze web navigation data to identify such user browsing patterns and use them to improve web design is an important task.	b2b e-commerce;browsing;information foraging;matching (graph theory);pervasive informatics;web design;web navigation;web search engine	Yinghui Yang	2009		10.4018/978-1-60566-010-3.ch317	web design program;web modeling;world wide web;web service;web page;web navigation;semantic web stack;client-side scripting;computer science;web design	Web+IR	-31.907365208792434	-53.23744977465101	92930
d204a4e2b5144f890dd155f2f86e3b1f0253e471	random walk based rank aggregation to improving web search	search engine;web pages;search strategy;pairwise majority contest;school of engineering and science;journal article;empirical evidence;query suggestion;8902 computer software and services;random walk;rank aggregation;web search;respubid23055;0806 information systems;pairwise contest;concurrent process	In Web search, with the aid of related query recommendation, Web users can revise their initial queries in several serial rounds in pursuit of finding needed Web pages. In this paper, we address the Web search problem on aggregating search results of related queries to improve the retrieval quality. Given an initial query and the suggested related queries, our search system concurrently processes their search result lists from an existing search engine and then forms a single list aggregated by all the retrieved lists. We specifically propose a generic rank aggregation framework which consists of three steps. First we build a so-called Win/Loss graph of Web pages according to a competition rule, and then apply the random walk mechanism on the Win/Loss graph. Last we sort these Web pages by their ranks using a PageRank-like rank mechanism. The proposed framework considers not only the number of wins that an item won in competitions, but also the quality of its competitor items in calculating the ranking of Web page items. Experimental results show that our search system can clearly improve the retrieval quality in a parallel manner over the traditional search strategy that serially returns result lists. Moreover, we also provide empirical evidences as to demonstrate how different rank aggregation methods affect the retrieval quality.		Lin Li;Guandong Xu;Yanchun Zhang;Masaru Kitsuregawa	2011	Knowl.-Based Syst.	10.1016/j.knosys.2011.04.001	beam search;web query classification;empirical evidence;semantic search;computer science;artificial intelligence;machine learning;web page;jump search;data mining;search analytics;web search query;world wide web;random walk;information retrieval;search engine;statistics	ML	-32.41354422445749	-53.445024316013004	93147
05782066178a447d2929840b49a4fe70f10ac3d4	an improved information gain algorithm based on relative document frequency distribution		Feature selection algorithm plays an important role in text categorization. Considering some drawbacks proposed from traditional and recently improved information gain(IG) approach, an improved IG feature selection method based on relative document frequency distribution is proposed, which combines reducing the impact of unbalanced data sets and low-frequency characteristics, the frequency distribution of features within category and the relative frequency document distribution of features among different categories. The experimental results of NLPCC-ICCPOL 2016 stance detection in Chinese microblogs show that the performance of the improved method is better than traditional IG approach and another improved method in feature selection.	categorization;document classification;feature selection;information gain in decision trees;information theory;kullback–leibler divergence;selection algorithm;test set;unbalanced circuit	Jian Peng;Xiao-Hua Yang;Chun-Ping Ouyang;Yong-Bin Liu	2016		10.1007/978-3-319-50496-4_49	information gain ratio;pattern recognition;data mining;information retrieval	NLP	-21.31758664600405	-64.14167130821676	93209
2f3a6728b87283ccf0f8822f7a60bca8280f0957	learning to aggregate vertical results into web search results	learning algorithm;learning model;aggregated search;query intent;federated search;web search;learning to rank	Aggregated search is the task of integrating results from potentially multiple specialized search services, or verticals, into the Web search results. The task requires predicting not only which verticals to present (the focus of most prior research), but also predicting where in the Web results to present them (i.e., above or below the Web results, or somewhere in between). Learning models to aggregate results from multiple verticals is associated with two major challenges. First, because verticals retrieve different types of results and address different search tasks, results from different verticals are associated with different types of predictive evidence (or features). Second, even when a feature is common across verticals, its predictiveness may be vertical-specific. Therefore, approaches to aggregating vertical results require handling an inconsistent feature representation across verticals, and, potentially, a vertical-specific relationship between features and relevance. We present 3 general approaches that address these challenges in different ways and compare their results across a set of 13 verticals and 1070 queries. We show that the best approaches are those that allow the learning algorithm to learn a vertical-specific relationship between features and relevance.	aggregate data;algorithm;machine learning;relevance;web search engine;world wide web	Jaime Arguello;Fernando Diaz;James P. Callan	2011		10.1145/2063576.2063611	computer science;machine learning;data mining;database;world wide web;information retrieval;learning to rank	Web+IR	-27.21417520595809	-54.132883655656414	93248
1562dab365726524a216840b58e6ec866a15b323	classification of small datasets: why using class-based weighting measures?	u10 methodes mathematiques et statistiques;c30 documentation et information	Abstract. In text classification, providing an efficient classifier even if the number of documents involved in the learning step is small remains an important issue. In this paper we evaluate the performance of traditional classification methods to better evaluate their limitation in the learning phase when dealing with small amount of documents. We thus propose a new way for weighting features which are used for classifying. These features have been integrated in two well known classifiers: Class-Feature-Centroid and Naı̈ve Bayes, and evaluations have been performed on two real datasets. We have also investigated the influence on parameters such as number of classes, documents or words in the classification. Experiments have shown the efficiency of our proposal relatively to state of the art classification methods. Either with a very few amount of data or with a small number of features that can be extracted from poor content documents, we show that our approach performs well.	centrality;document classification;experiment;inner class;naive bayes classifier;okapi bm25;statistical classification;supervised learning;verification and validation	Flavien Bouillot;Pascal Poncelet;Mathieu Roche	2014		10.1007/978-3-319-08326-1_35	computer science;machine learning;pattern recognition;data mining;information retrieval	Web+IR	-20.530166375339846	-64.37338111998426	93372
747fb3abae56dbc1607d9626fbfcf9528151ee7a	combining linked data and statistical information retrieval - next generation information systems		Being a part of the Information Age, users are challenged with a tremendously growing amount of Web data which generates a need for more sophisticated information retrieval systems. The Semantic Web provides necessary procedures to augment the highly unstructured Web with suitable metadata in order to leverage search quality and user experience. In this article, we will outline an approach for creating a webscale, precise and efficient information system capable of understanding keyword, entity and natural language queries. By using Semantic Web methods and Linked Data the doctoral work will present how the underlying knowledge is created and elaborated searches can be performed on top.	information retrieval;information system;linked data;natural language;scalability;semantic web;user experience	Ricardo Usbeck	2014		10.1007/978-3-319-07443-6_58	information retrieval	Web+IR	-32.503272412925554	-56.38469982616107	93563
19aa0d66e87fa6ffcf5b182bda7164dff2af9549	automated retrieval of information in the internet by using thesauri and gazetteers as knowledge sources	search engine;controlled vocabulary;user interface;information retrieval;knowledge worker;region of interest;text retrieval;foreign language	There is an immense number of information resources on the Internet that can be utilized free of charge. So many knowledge workers try to make use of this information in their daily tasks. Nevertheless, it is very hard to find the relevant information in the Internet by using the full-text retrieval techniques which are offered by most existing search engines. This paper demonstrates that Thesauri, which have been used in established online retrieval systems for a long time, also open up new methods for the automated search for information in the Internet. In addition, thesaurus-like structures known as Gazetteers allow handling geographical references of information resources in a very effective way. The knowledge represented in thesauri and gazetteers can be used to process a variety of thematic and geographical queries and to retrieve the information of interest from the Internet. Comfortable ways of specifying queries can be offered to the users, e.g., by navigating in a hierarchical tree of descriptors, by using synonymous, related or foreign-language terms rather than fixed elements of a controlled vocabulary, or by indicating a geographical region of interest on a cartographic map. In addition to the general principles, examples of powerful query processors and advanced user interfaces are presented which demonstrate the effective usage of the knowledge stored in thesauri and gazetteers. The implemented solutions turn out to be considerably more comfortable than the “black box search” offered by most existing library catalogs and Internet search engines.	black box;cartography;central processing unit;controlled vocabulary;document retrieval;internet;location-based service;microsoft outlook for mac;region of interest;theme (computing);thesaurus (information retrieval);user interface;web search engine	Wolf-Fritz Riekert	2002	J. UCS	10.3217/jucs-008-06-0581	foreign language;controlled vocabulary;computer science;multimedia;user interface;world wide web;information retrieval;search engine;region of interest	Web+IR	-32.71620932977971	-56.760153232868895	93610
355187ad3b83abc5728239a556807ab3e614ecae	a new email retrieval ranking approach	score function;information retrieval;experimental evaluation	Email Retrieval task has recently taken much attention to help the user retrieve the email(s) related to the submitted query. Up to our knowledge, existing email retrieval ranking approaches sort the retrieved emails based on some heuristic rules, which are either search clues or some predefined user criteria rooted in email fields. Unfortunately, the user usually does not know the effective rule that acquires best ranking related to his query. This paper presents a new email retrieval ranking approach to tackle this problem. It ranks the retrieved emails based on a scoring function that depends on crucial email fields, namely subject, content, and sender. The paper also proposes an architecture to allow every user in a network/group of users to be able, if permissible, to know the most important network senders who are interested in his submitted query words. The experimental evaluation on Enron corpus prove that our approach outperforms known email retrieval ranking approaches.	content-based image retrieval;email;enron corpus;heuristic (computer science);real life;scoring functions for docking;user profile	Samir E. AbdelRahman;Basma Hassan;Reem Bahgat	2010	CoRR	10.5121/ijcsit.2010.2504	ranking;html email;computer science;data mining;score;world wide web;information retrieval	Web+IR	-27.985685843524628	-54.594449620018814	93613
e077f1a235306538acb8cb7f11646a017887e43a	consolidating identities of authors through egonet structure		Individuals often appear with multiple names when considering large data-sets collected from different sources, giving rise to the name ambiguities. Name ambiguity comes in two flavors: a single individual appearing with more than one name (synonym problem); a single name being used to refer to more than one individual (homonym problem). This works focuses on the synonym problem and explores structural patterns of the collaboration network. We consider a scenario where the individual has a specific profile page that lists its bibliographic records. Despite more restrictive, this scenario has become quite common among digital libraries such as DBLP1 and Google Scholar2. Such profiles often have name ambiguities as the profile owner appears with different names within the bibliographic records (e.g., different name spellings, typos, etc). This paper tackle these name ambiguities without resorting to any content information!	bibliographic record;digital library;embnet.journal;library (computing);structural pattern	Janaína Gomide;Hugo Kling;Daniel Ratton Figueiredo	2017		10.1145/3091478.3098862	name resolution;world wide web;synonym (database);digital library;synonym;ambiguity;information retrieval;homonym;computer science	ML	-32.51908230672822	-55.72798286838089	93643
81963afb84cb1946ab77cdd24aa696558d5096ec	more than just noise? examining the information content of stock microblogs on financial markets		Scholars and practitioners alike increasingly recognize the importance of stock microblogs as they capture the market discussion and have predictive value for financial markets. This paper examines the extent to which stock microblog messages are related to financial market indicators and the mechanism leading to efficient aggregation of information. In particular, this paper investigates the information content of stock microblogs with respect to individual stocks and explores the effects of social influences on an interday and intraday basis. We collected more than 1.2 million stock-related messages (i.e., tweets) related to S&P 100 companies over a period of 7 months. Using methods from computational linguistics, we went through an elaborate process of message feature reduction, spam detection, language detection, and slang removal, which has led to an increase in classification accuracy for sentiment analysis. We analyzed the data on both a daily and a 15-min basis and found that the sentiment of messages is positively affected with contemporaneous daily abnormal stock returns and that message volume predicts 15-min follow-up returns, trading volume, and volatility. Disagreement in microblog messages positively influences stock features, both in interday and intraday analysis. Notably, if we give a greater share of voice to microblog messages depending on the social influence of microbloggers, this amplifies the relationship between bullishness and abnormal returns, market volume, and volatility. Following knowledgeable investors advice results in more power in explaining changes in market features. This offers an explanation for the efficient aggregation of information on microblogging platforms. Furthermore, we simulated a set of trading strategies using microblog features and the results suggest that it is possible to exploit market inefficiencies even when transaction costs are included. To our knowledge, this is the first study to comprehensively examine the association between the information content of stock microblogs and intraday stock market features. The insights from the study permit scholars and professionals to reliably identify stock microblog features, which may serve as valuable proxies for market sentiment and permit individual investors to make better investment decisions.		Ting Li;Jan van Dalen;Pieter Jan van Rees	2018	JIT	10.1057/s41265-016-0034-2	trading strategy;sentiment analysis;computer science;microblogging;stock market;financial market;marketing;social media;investment decisions;share of voice;market sentiment	NLP	-20.229156755900043	-53.10479070638904	93663
8f47789c420d580dca90208005f5a1ebffd4176b	content-based publish/subscribe system for web syndication	web syndication;zeinab hmedeh harry kourdounakis vassilis christophides cedric du mouza michel scholl nicolas travers 发布 订阅 聚合技术 基于内容 web 系统 更新信息 部分匹配 流行方式 content based publish subscribe system for web syndication;subscription indexing;partial matching;pub sub;scalability	Content syndication has become a popular way for timely delivery of frequently updated information on the Web. Today, web syndication technologies such as RSS or Atom are used in a wide variety of applications spreading from large-scale news broadcasting to medium-scale information sharing in scientific and professional communities. However, they exhibit serious limitations for dealing with information overload in Web 2.0. There is a vital need for efficient real-time filtering methods across feeds, to allow users to effectively follow personally interesting information. We investigate in this paper three indexing techniques for users’ subscriptions based on inverted lists or on an ordered trie for exact and partial matching. We present analytical models for memory requirements and matching time and we conduct a thorough experimental evaluation to exhibit the impact of critical parameters of realistic web syndication workloads.	atom (standard);information overload;rss;real-time web;requirement;trie;web 2.0;web syndication;world wide web	Zeinab Hmedeh;Harris Kourdounakis;Vassilis Christophides;Cédric du Mouza;Michel Scholl;Nicolas Travers	2016	Journal of Computer Science and Technology	10.1007/s11390-016-1632-8	scalability;computer science;web page;database;web syndication;internet privacy;publish–subscribe pattern;web 2.0;world wide web	Web+IR	-31.68308216424949	-54.795463257487604	93814
c44ef20fe403f96ce2d39c6f3d2d72d5565194f1	a survey of extractive and abstractive text summarization techniques	market research;bio inspired algorithms;text mining;text analysis;text summarization;text summarization bio inspired algorithms text mining;internet;bio inspired methods extractive automatic text summarization techniques abstractive automatic text summarization techniques world wide web information explosion lengthy text documents digital documents;text analysis internet	The existence of the World Wide Web has caused an information explosion. Readers are overloaded with lengthy text documents where a shorter version would suffice. All computer users, be it professionals or novice users, are particularly affected by this predicament. There exists an urgent need for the discovery of knowledge embedded in digital documents. This paper intends to investigate techniques and methods used by researchers for automatic text summarization. Special attention is paid to Bio-inspired methods for text summarization.	automatic summarization;embedded system;information explosion;user (computing);world wide web	Vipul Dalal;Latesh G. Malik	2013	2013 6th International Conference on Emerging Trends in Engineering and Technology	10.1109/ICETET.2013.31	text graph;text mining;multi-document summarization;computer science;automatic summarization;noisy text analytics;data mining;world wide web;information retrieval	DB	-27.53527569313573	-56.9548937366654	93899
85f303f623d7c1d5287a7fbc82ed27c30e738e09	extracting domain-relevant term using wikipedia based on random walk model	graph theory;wikipedia;information retrieval;link graph domain relevant concepts markov chain random walk wikipedia;biological system modeling;semantics;domain relevant concepts;weighted wikipedia link graph domain relevant term extraction domain relevant concept automatic identification domain relevant entity automatic identification markov random walk algorithm;encyclopedias internet electronic publishing semantics ontologies biological system modeling;internet;random walk;web sites;ontologies;electronic publishing;markov processes;link graph;encyclopedias;web sites graph theory information retrieval markov processes;markov chain	In this paper we present a new approach for the automatic identification of domain-relevant concepts and entities of a given domain using the category and page structures of the Wikipedia in a language independent way. By applying Markov random walk algorithm on the weighted Wikipedia link graph, our approach can identify large quantities of domain-relevant concepts and entities with very little human effort. Experimental results show that our method achieves high accuracy and acceptable efficiency in domain-relevant term extraction.	algorithm;automatic identification and data capture;bigraph;entity;markov chain;terminology extraction;wikipedia	Wenjuan Wu;Tao Liu;He Hu;Xiaoyong Du	2012	2012 Seventh ChinaGrid Annual Conference	10.1109/ChinaGrid.2012.20	computer science;machine learning;data mining;information retrieval	NLP	-26.193098567502116	-63.51051610318217	94106
481e2d03a0b8b8bb66fafc6f1bce21b8440a8d7d	leveraging social networks for effective spam filtering	social network services;unsolicited electronic mail;bayesian spam filters;unsolicited e mail bayes methods information filtering performance evaluation social networking online trusted computing;bayes methods;simple object access protocol social network services bayes methods unsolicited electronic mail;training;accuracy;spam filtering;social networks;distributed overlays;social networks trace data facebook soap prototype attack resilience performance evaluation friend notification adaptive trust management social interest based spam filtering social closeness based spam filtering bayesian filter email correspondents parsing keywords overlay links social network links distributed overlay social friends social network aided personalization user friendly spam filter spam detection user feedback spam keywords clever spammers spam emails static keyword based spam filters bayesian spam filters spam filter techniques unsolicited e mails;simple object access protocol;bayesian spam filters distributed overlays spam filtering social networks	The explosive growth of unsolicited e-mails has prompted the development of numerous spam filter techniques. Bayesian spam filters are superior to static keyword-based spam filters in that they can continuously evolve to tackle new spam by learning keywords in new spam emails. However, Bayesian spam filters are easily poisoned by clever spammers who avoid spam keywords and add many innocuous words in their emails. Also, Bayesian spam filters need a significant amount of time to adapt to a new spam based on user feedback. Moreover, few current spam filters exploit social networks to assist in spam detection. In order to develop an accurate and user-friendly spam filter, we propose a SOcial network Aided Personalized and effective spam filter (SOAP) in this paper. In SOAP, each node connects to its social friends; i.e., nodes form a distributed overlay by directly using social network links as overlay links. Each node uses SOAP to collect information and check spam autonomously in a distributed manner. Unlike previous spam filters that focus on parsing keywords (e.g., Bayesian filters) or building blacklists, SOAP exploits the social relationships among email correspondents and their (dis)interests to detect spam adaptively and automatically. In each node, SOAP integrates four components into the basic Bayesian filter: social closeness-based spam filtering, social interest-based spam filtering, adaptive trust management, and friend notification. We have evaluated the performance of SOAP using simulation based on trace data from Facebook. We also have implemented a SOAP prototype for real-world experiments. Experimental results show that SOAP can greatly improve the performance of Bayesian spam filters in terms of accuracy, attack-resilience, and efficiency of spam detection. The performance of the Bayesian spam filter is SOAP's lower bound.	anti-spam techniques;centrality;email filtering;experiment;naive bayes spam filtering;parsing;prototype;soap;simulation;social network;spamming;static (keyword);trust management (information system);usability;gift	Haiying Shen;Ze Li	2014	IEEE Transactions on Computers	10.1109/TC.2013.152	forum spam;computer science;bag-of-words model;spamming;spambot;soap;spam and open relay blocking system;accuracy and precision;internet privacy;world wide web;computer security;statistics;social network	Web+IR	-19.682739347563253	-55.20915207983953	94150
9f0b6ef5e0ddf659a9ff273239117eefad941f6a	towards a librarian of the web	text processing;text similarity;librarian;text clustering;web search;centroid term;co occurrence graph	If the World Wide Web (WWW) is considered to be a huge library, it would need a librarian, too. Google and other web search engines are more or less just keyword databases and cannot fulfil this person's tasks in a sufficient manner. Therefore, an approach to improve cataloguing and classifying documents in the WWW is introduced and its efficiency demonstrated in first simulations.	algorithm;cluster analysis;computer cluster;database;display resolution;librarian;reverse index;simulation;www;web page;web search engine;world wide web	Mario M. Kubek;Herwig Unger	2016		10.1145/3018009.3018031	web search engine;computer science;web page;data mining;world wide web;information retrieval	DB	-28.725362982581355	-56.70424535654894	94210
d9cd87a2c28a1f5be012539b325969c5eb574263	extractive single document summarization using multi-objective optimization: exploring self-organized differential evolution, grey wolf optimizer and water cycle algorithm		Abstract Text summarization techniques become paramount in extracting relevant information from large databa-ses. Current paper attempts to build some extractive single document text summarization (ESDS) systems using multi-objective optimization (MOO) frameworks. Three techniques are proposed: (1) first is an integration of self-organizing map (SOM) and multi-objective differential evolution (MODE) (named as ESDS_SMODE) (2) second is based on multi-objective grey wolf optimizer (ESDS_MGWO) and (3) third is based on multi-objective water cycle algorithm (ESDS_MWCA). The sentences present in the document are first clustered utilizing the concept of multi-objective clustering. Two objective functions measuring compactness and separation of the sentence clusters in two different ways are optimized simultaneously using MOO framework. The proposed approach is able to automatically detect the number of sentence clusters present in a document and then representative sentences are selected from different clusters using some sentence scoring features to generate the summary. The experiments were conducted on two benchmark datasets, DUC2001, and DUC2002, and the obtained results are compared with various state-of-the-art techniques using ROUGE measures. Results illustrate the superiority of our approach in comparison to state-of-the-art techniques in terms of ROUGE − 2 score for both datasets. Code of the developed approach ESDS_SMODE is available online at https://drive.google.com/open?id=1WagTeIDLgphttPrKHpnF_eO7QHWJxXxK .		Naveen Saini;Sriparna Saha;Anubhav Jangra;Pushpak Bhattacharyya	2019	Knowl.-Based Syst.	10.1016/j.knosys.2018.10.021	automatic summarization;machine learning;differential evolution;rouge;multi-objective optimization;artificial intelligence;cluster analysis;algorithm;computer science;sentence	Vision	-26.549273255791775	-61.69133952862497	94308
f7a4ee575b038ae1ff626ecf9d7a80f80caf26ae	emergency-affected population identification and notification by using online social networks	online social network;user extraction;emergency warning system	Natural disasters have been a major cause of huge losses for both people’s life and property. There is no doubt that the importance of Emergency Warning System (EWS) has been considered more seriously than ever. Unfortunately, most EWSs do not provide acceptable service to identify people who might be affected by a certain disasters. In this project, we propose an approach to identify possibly affected users of a target disaster by using online social networks. The proposed method consists of three phases. First of all, we collect location information from social network websites, such as Twitter. Then, we propose a social network analysis algorithm to identify potential victims and communities. Finally, we conduct an experiment to test the accuracy and efficiency of the approach. Based on the result, we claim that the approach can facilitate identifying potential victims effectively based on data from social networking systems.		Huong Pho;Soyeon Caren Han;Byeong Ho Kang	2011		10.1007/978-3-642-27207-3_59	engineering;data mining;internet privacy;computer security	ECom	-21.427110183022574	-55.17759893917423	94416
523909cad90a0d5de1a10815cdb32bfd15e13dc3	ranking web objects from multiple communities	search engine;web community;user study;test bed;ranking;web objects;detection algorithm;image search	Vertical search is a promising direction as it leverages domain-specific knowledge and can provide more precise information for users. In this paper, we study the Web object-ranking problem, one of the key issues in building a vertical search engine. More specifically, we focus on this problem in cases when objects lack relationships between different Web communities, and take high-quality photo search as the test bed for this investigation. We proposed two score fusion methods that can automatically integrate as many Web communities (Web forums) with rating information as possible. The proposed fusion methods leverage the hidden links discovered by a duplicate photo detection algorithm, and aims at minimizing score differences of duplicate photos in different forums. Both intermediate results and user studies show the proposed fusion methods are practical and efficient solutions to Web object ranking in cases we have described. Though the experiments were conducted on high-quality photo ranking, the proposed algorithms are also applicable to other ranking problems, such as movie ranking and music ranking.	algorithm;domain-specific language;experiment;testbed;web search engine;world wide web	Le Chen;Lei Zhang;Feng Jing;Kefeng Deng;Wei-Ying Ma	2006		10.1145/1183614.1183670	ranking;ranking;computer science;data mining;database;web search query;ranking svm;world wide web;information retrieval;search engine;testbed	Web+IR	-29.263668507912776	-59.68551007689872	94488
2dc1590075a1af8a4d760e853f3ca87121c47af2	stance detection in russian: a feature selection and machine learning based approach		The huge scale and constant increase of the data volume in social media has led to a high demand for automatic means of such content analysis, specifically stance detection. This term stands for the task of assigning stance labels (“for” and “against”) with respect to a discussion topic. In the paper we tackle stance detection for Russian texts from social network “VKontakte” with the use of machine learning methods – the support vector machine, k-nearest neighbors, Naïve Bayes, AdaBoost, and decision trees. Also we apply the Recursive Feature Elimination (RFE) algorithm for feature selection and explore the impact of morphological analysis on the quality of the task solution. The best results (F1=84.3%) are achieved by using of the SVM and vector model with relatively small set of normalized words chosen by RFE.	adaboost;conditional random field;decision tree;deep learning;display resolution;document classification;feature selection;k-nearest neighbors algorithm;machine learning;multinomial logistic regression;naive bayes classifier;radial basis function kernel;recursion (computer science);social media;social network;support vector machine	Sergey Vychegzhanin;Evgeny V. Kotelnikov	2017				ML	-20.568533978718254	-65.75063761192996	94490
462173d08465899f8166ce0d312ee05a98ed625a	computing sharp lower and upper bounds for the minimum latency problem	content based filtering;groupware;004 informatik;user preferences;statistical model;user profile;recommender system;machine learning;linguistic knowledge hybrid content collaborative recommender system electronic performance support system collaborative filtering content based filtering statistical model machine learning techniques;recommender systems collaboration filtering machine learning knowledge management hybrid intelligent systems informatics information management electronic commerce software libraries;learning artificial intelligence;information filters;content based retrieval;learning artificial intelligence content based retrieval groupware information filters;electronic performance support system	The minimum latency problem, also known as traveling repairman problem, the Deliveryman problem and the traveling salesman problem with cumulative costs is a variant of the Traveling Salesman Problem in which a repairman is required to visit customers located on each node of a graph in such a way that the overall waiting times of these customers is minimized. In the present work, an algorithm based on tight different linear programming lower- bounds and a specialized GRASP procedure are presented. The linear programming based lower-bounds are based on the Quadratic Assignment Problem with the aid of side constraints. Instances from 10 up to 60 nodes are solved very close to optimality in reasonable time.	algorithm;grasp;interrupt latency;linear programming;quadratic assignment problem;travelling salesman problem	Joao F. M. Sarubbi;Henrique Pacca Loureiro Luna;Gilberto de Miranda;Ricardo Saraiva de Camargo	2007	7th International Conference on Hybrid Intelligent Systems (HIS 2007)	10.1109/HIS.2007.30	computer science;collaborative filtering;information filtering system;multimedia;world wide web;information retrieval;recommender system	EDA	-31.422799869605974	-52.34801105693427	94521
d04517c6551ebfb33b395e355f421c74f8a4cb32	answering pico clinical questions: a semantic graph-based approach	theorie de l information;recherche d information	In this paper, we tackle the issue related to the retrieval of the best evidence that fits with a PICO (Population, Intervention, Comparison and Outcome) question. We propose a new document ranking algorithm that relies on semantic based query expansion bounded by the local search context to better discard irrelevant documents. Experiments using a standard dataset including 423 PICO questions and more than 1, 2 million of documents, show that our aproach is promising.	experiment;fits;genetic algorithm;local search (optimization);pico;population;query expansion;ranking (information retrieval);relevance;software propagation	Eya Znaidi;Lynda Tamine;Cherif Chiraz Latiri	2015		10.1007/978-3-319-19551-3_30	computer science;data science;data mining;information retrieval	NLP	-29.395866801222773	-62.07516063257292	94574
d7c58e4f16504500329315e06eeba700c4b7abca	document summarization based on data reconstruction	document summarization;data reconstruction;linear reconstruction;会议论文;optimization problems;real world application;objective functions;search results;reconstruction error;linear combinations;benchmark data	Document summarization is of great value to many real world applications, such as snippets generation for search results and news headlines generation. Traditionally, document summarization is implemented by extracting sentences that cover the main topics of a document with a minimum redundancy. In this paper, we take a different perspective from data reconstruction and propose a novel framework named Document Summarization based on Data Reconstruction (DSDR). Specifically, our approach generates a summary which consist of those sentences that can best reconstruct the original document. To model the relationship among sentences, we introduce two objective functions: (1) linear reconstruction, which approximates the document by linear combinations of the selected sentences; (2) nonnegative linear reconstruction, which allows only additive, not subtractive, linear combinations. In this framework, the reconstruction error becomes a natural criterion for measuring the quality of the summary. For each objective function, we develop an efficient algorithm to solve the corresponding optimization problem. Extensive experiments on summarization benchmark data sets DUC 2006 and DUC 2007 demonstrate the effectiveness of our proposed approach. Introduction With the explosive growth of the Internet, people are overwhelmed by a large number of accessible documents. Summarization can represent the document with a short piece of text covering the main topics, and help users sift through the Internet, catch the most relevant document, and filter out redundant information. So document summarization has become one of the most important research topics in the natural language processing and information retrieval communities. In recent years, automatic summarization has been applied broadly in varied domains. For example, search engines can provide users with snippets as the previews of the document contents (Turpin et al. 2007; Huang, Liu, and Chen 2008; Cai et al. 2004; He et al. 2007). News sites usually describe hot news topics in concise headlines to facilitate browsing. Both the snippets and headlines are specific forms of document summary in practical applications. Copyright c © 2012, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Most of the existing generic summarization approaches use a ranking model to select sentences from a candidate set (Brin and Page 1998; Kleinberg 1999; Wan and Yang 2007). These methods suffer from a severe problem that top ranked sentences usually share much redundant information. Although there are some methods (Conroy and O’leary 2001; Park et al. 2007; Shen et al. 2007) trying to reduce the redundancy, selecting sentences which have both good coverage and minimum redundancy is a non-trivial task. In this paper, we propose a novel summarization method from the perspective of data reconstruction. As far as we know, our approach is the first to treat the document summarization as a data reconstruction problem. We argue that a good summary should consist of those sentences that can best reconstruct the original document. Therefore, the reconstruction error becomes a natural criterion for measuring the quality of summary. We propose a novel framework called Document Summarization based on Data Reconstruction (DSDR) which finds the summary sentences by minimizing the reconstruction error. DSDR firstly learns a reconstruction function for each candidate sentence of an input document and then obtains the error formula by that function. Finally it obtains an optimal summary by minimizing the reconstruction error. From the geometric interpretation, DSDR tends to select sentences that span the intrinsic subspace of candidate sentence space so that it is able to cover the core information of the document. To model the relationship among sentences, we discuss two kinds of reconstruction. The first one is linear reconstruction, which approximates the document by linear combinations of the selected sentences. Optimizing the corresponding objective function is achieved through a greedy method which extracts sentences sequentially. The second one is non-negative linear reconstruction, which allows only additive, not subtractive, combinations among the selected sentences. Previous studies have shown that there is psychological and physiological evidence for parts-based representation in the human brain (Palmer 1977; Wachsmuth, Oram, and Perrett 1994; Cai et al. 2011). Naturally, a document summary should consist of the parts of sentences. With the nonnegative constraints, our method leads to parts-based reconstruction so that no redundant information needs to be subtracted from the combination. We formulate the nonnegative linear reconstruction as a convex optimization problem Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence	artificial intelligence;automatic summarization;benchmark (computing);convex optimization;entity–relationship model;experiment;greedy algorithm;information needs;information retrieval;internet;ipke wachsmuth;linear programming;mathematical optimization;natural language processing;optimization problem;optimizing compiler;reconstruction conjecture;upsampling;utility functions on indivisible goods;web search engine;yang	Zhanying He;Chun Chen;Jiajun Bu;Can Wang;Lijun Zhang;Deng Cai;Xiaofei He	2012			optimization problem;linear combination;computer science;data science;automatic summarization;machine learning;data mining;information retrieval	AI	-26.36159653318137	-61.98401171326067	94651
8a7ebe71e9a4d251c8deb6b8195f82ef6b9add14	a prototype crowdsourcing approach for document summarization service		This paper proposes a crowdsourcing approach for informative document summarization service. It first captures the task of summarizing a lengthy document as a bi-objective combinatorial optimization problem. One objective function to be minimized is the time to comprehend the summary, and the other one to be maximized is the amount of information content remaining in it. The solution space of the problem is composed of various combinations of candidate condensed elements covering the whole document as a set. Since it is not easy for a computer algorithm to create condensed elements of different lengths which are natural and easy for a human to comprehend, as well as to evaluate the two objective functions for any possible summary, these sub-tasks are crowdsourced to human contributors. The rest of the approach is handled by a computer algorithm. How the approach functions is tested by a laboratory experiment using a pilot system implemented as a web application.	automatic summarization;crowdsourcing;prototype	Hajime Mizuyama;Keishi Yamashita;Kenji Hitomi;Michiko Anse	2013		10.1007/978-3-642-41263-9_54	computer science;automatic summarization;data mining;world wide web;information retrieval	Web+IR	-27.219647150280682	-60.82343685773651	94687
d461161aa76fbfe38858fa07e212d60c8a74c839	information retrieval and visualization using sentinel.	information retrieval	Harris Corporation focuses on information retrieval support for various Government agencies. Time constraints and interest-level limit our user to reviewing the top documents before determining if the results of a query are accurate and satisfactory. In such cases, retrieval times and precision accuracy are at a premium, with recall potentially being compromised. To meet user demands our system, called SENTINEL, was designed to yield efficient, high precision retrieval.	harris affine region detector;information retrieval	Margaret M. Knepper;Robert A. Killam;Kevin L. Fox;Ophir Frieder	1998			computer science;information retrieval	Web+IR	-33.27603780439313	-58.49600400571092	94728
3a4de5c55367f50bf81596c400bf1778f844a86f	graph-of-word and tw-idf: new approach to ad hoc ir	ir theory;graph based term weighting;graph of word;tw idf;scoring functions;graph representation of document	In this paper, we introduce novel document representation (graph-of-word) and retrieval model (TW-IDF) for ad hoc IR. Questioning the term independence assumption behind the traditional bag-of-word model, we propose a different representation of a document that captures the relationships between the terms using an unweighted directed graph of terms. From this graph, we extract at indexing time meaningful term weights (TW) that replace traditional term frequencies (TF) and from which we define a novel scoring function, namely TW-IDF, by analogy with TF-IDF. This approach leads to a retrieval model that consistently and significantly outperforms BM25 and in some cases its extension BM25+ on various standard TREC datasets. In particular, experiments show that counting the number of different contexts in which a term occurs inside a document is more effective and relevant to search than considering an overall concave term frequency in the context of ad hoc IR.	concave function;directed graph;experiment;hoc (programming language);mahdiyar;okapi bm25;scoring functions for docking;text retrieval conference;tf–idf	François Rousseau;Michalis Vazirgiannis	2013		10.1145/2505515.2505671	international relations theory;machine learning;pattern recognition;data mining;database;information retrieval	Web+IR	-27.30933775959856	-62.5515340369364	95110
ffb5d6e0f42995e076077a3ef62e5880632a1540	automatic detection of online recruitment frauds: characteristics, methods, and a public dataset	employment scam;online recruitment;data mining;machine learning;job scam;natural language processing;fraud detection;dataset	The critical process of hiring has relatively recently been ported to the cloud. Specifically, the automated systems responsible for completing the recruitment of new employees in an online fashion, aim to make the hiring process more immediate, accurate and cost-efficient. However, the online exposure of such traditional business procedures has introduced new points of failure that may lead to privacy loss for applicants and harm the reputation of organizations. So far, the most common case of Online Recruitment Frauds (ORF), is employment scam. Unlike relevant online fraud problems, the tackling of ORF has not yet received the proper attention, remaining largely unexplored until now. Responding to this need, the work at hand defines and describes the characteristics of this severe and timely novel cyber security research topic. At the same time, it contributes and evaluates the first to our knowledge publicly available dataset of 17,880 annotated job ads, retrieved from the use of a real-life system.	ats;algorithm;computer security;cost efficiency;cyberbullying;email;internet troll;open reading frame;phishing;real life;reliability engineering;spamming;text mining;wikipedia	Sokratis Vidros;Constantinos Kolias;Georgios Kambourakis;Leman Akoglu	2017	Future Internet	10.3390/fi9010006	computer science;data mining;management;world wide web;computer security;data set	ML	-20.810377329885803	-54.42422921370358	95149
6c3ec78aa222debde6154cab89683244a5f8224e	multidisciplinary information retrieval		We give an overview of the highly multilingual news analysis system Europe Media Monitor (EMM), which gathers an average of 175,000 online news articles per day in tens of languages, categorises the news items and extracts named entities and various other information from them. We explain how users benefit from media monitoring and why it is so important to monitor the news in many different languages. We also describe the challenge of developing text mining tools for tens of languages and in particular that of dealing with highly inflected languages, such as those of the Balto-Slavonic and Finno-Ugric language families.	information retrieval;named entity;text mining	Bissan Audeh;Philippe Beaune;Michel Beigbeder	2013		10.1007/978-3-642-41057-4	data mining;information retrieval;computer science;multidisciplinary approach	Web+IR	-24.081440981776005	-55.387204367576786	95254
9e281a4365d378b4bd1d41618210970412e358d8	a framework for titled document categorization with modified multinomial naivebayes classifier	document categorization;document classification	Titled Documents (TD) are short text documents that are segmented into two parts: Heading Part and Excerpt Part. With the development of the Internet, TDs are widely used as papers, news, messages, etc. In this paper we discuss the problem of automatic TDs categorization. Unlike traditional text documents, TDs have short headings which have less useless words comparing to their excerpts. Though headings are usually short, their words are more important than other words. Based on this observation we propose a titled document classification framework using the widely used MNB classifier. This framework puts higher weight on the heading words at the cost of some excerpt words. By this means heading words play more important roles in classification than the traditional method. According to our experiments on four datasets that cover three types of documents, the performance of the classifier is improved by our approach.	categorization;document classification;multinomial logistic regression	Lizhu Zhou	2007		10.1007/978-3-540-73871-8_31	speech recognition;computer science;machine learning;data mining;information retrieval	ML	-23.79168074228541	-65.89850289495384	95269
0e64e869c4d7ccab0ba6807cc44d6c611c3c2b4a	internet-scale collection of human-reviewed data	data collection;data processing;manual review;machine learning;human data;question answering	Enterprise and web data processing and content aggregation systems often require extensive use of human-reviewed data (e.g. for training and monitoring machine learning-based applications). Today these needs are often met by in-house efforts or out-sourced offshore contracting. Emerging applications attempt to provide automated collection of human-reviewed data at Internet-scale. We conduct extensive experiments to study the effectiveness of one such application. We also study the feasibility of using Yahoo! Answers, a general question-answering forum, for human-reviewed data collection.	aggregate data;experiment;internet;machine learning;outsourcing;question answering	Qi Su;Dmitry Pavlov;Jyh-Herng Chow;Wendell C. Baker	2007		10.1145/1242572.1242604	question answering;data processing;computer science;data science;machine learning;data mining;database;world wide web;information retrieval;statistics;data collection	DB	-21.032907061556546	-53.540221820261586	95572
f0a507b5665ff61bc6a62845c1c6bfcba917bac2	an approach to content extraction from scientific articles using case-based reasoning		In this paper, we present an efficient approach for content extraction of scientific papers from web pages. The approach uses an artificial intelligence method, Case-Based Reasoning(CBR), that relies on the idea that similar problems have similar solutions and hence reuses past experiences to solve new problems or tasks. The key task of content extraction is the classification of HTML tag sequences where the sequences representing navigation links, advertisements and, other non-informative content are not of interest when the goal is to extract scientific contributions. Our method learns from each experience with the tag sequence classification episode and stores these in the case base. When a new tag sequence needs to be classified, the system checks its case base to see whether a similar tag was experienced before in order to reuse it for content extraction. If the tag sequence is completely new, then it uses the proposed algorithm that relies on two assumptions related to the distribution of various tag sequences occurring in the page, and the similarity of the tag sequences with respect to their structure in terms of levels of the tags. Experimental results show that the proposed approach efficiently extracts content information from scientific articles.	algorithm;artificial intelligence;case-based reasoning;html;heuristic;information;scientific literature;web page	R. Rajendra Prasath;Pinar Öztürk	2016	Research in Computing Science		data science;data mining;case-based reasoning;computer science	AI	-29.082157593117728	-60.18140462063254	95593
f1c26416226b89ebb4282bd7d8017ce3df97300c	bjut at trec 2015 contextual suggestion track		In this paper we described our efforts for TREC contextual suggestion task. Our goal of this year is to evaluate the effectiveness of: (1) predict user preferences of each scenic spot based on non-negtive matrix factorization, (2) automatic summarization method that leverages the information from multiple resources to generate the description for each candidate scenic spots; and (3) hybrid recommendation method that combing a variety of factors to construct a system of hybrid recommendation system. Finally, we conduct extensive experiments to evaluate the proposed framework on TREC 2015 Contextual Suggestion data set, and, as would be expected, the results demonstrate its generality and superior performance.	automatic summarization;experiment;recommender system;user (computing)	Weitong Chen;Hanchen Li;Zhen Yang	2015			data management;computer science;data mining;world wide web;information retrieval;information system	Web+IR	-26.230541872343977	-52.324571143207	95750
8e82157cbe2ce15b6359009e0f1264918c8e63bc	a nearest neighbor approach to contextual suggestion		The School of Information and Library Science at the University of North Carolina at Chapel Hill (UNCSILS) submitted two runs to the Contextual Suggestion Track. Given a geographical context, both our runs (UNCSILS BASE and UNCSILS PARAM) scored venues from the same candidate set gathered using the Yelp API. Our baseline run (UNCSILS BASE) followed a nearest neighbor approach. For a given profile/context pair, the candidate venues were scored using the weighted average rating associated with the venues in the profile. The weighting was implemented based on the cosine similarity between the candidate venue and the profile venue using TF.IDF term weighting. The goal of this approach was to score each candidate venue based on the rating associated with the most similar venues in the profile. Our experimental run (UNCSILS PARAM) boosted the contribution from the profile venue with the greatest similarity with the candidate venue and rating. The experimental run (UNCSILS PARAM) outperformed the baseline run (UNCSILS BASE) by a small, but statistically significant margin.	baseline (configuration management);chapel;cosine similarity;library science;param;tf–idf;venue (sound system)	Sandeep Avula;John O'Connor;Jaime Arguello	2013			param;information retrieval;computer science;k-nearest neighbors algorithm;cosine similarity;weighted arithmetic mean;weighting	Web+IR	-31.11408365762352	-63.13962333280388	95754
06dd1ef2e6c788c31fe1be9451d0e4185f3ddc22	effective summarization method of text documents	data mining information technology genetic algorithms internet web pages world wide web functional analysis html information retrieval graph theory;genetic algorithm text document text summarization sentence extraction relevance score cosine measure;vector space model;geometry;text analysis;geometry text analysis feature extraction genetic algorithms;feature extraction;genetic algorithm;genetic algorithms;document classification;information gain;semi supervised document clustering;em	In this paper, we propose text summarization method that creates text summary by definition of the relevance score of each sentence and extracting sentences from the original documents. While summarization this method takes into account weight of each sentence in the document. The essence of the method suggested is in preliminary identification of every sentence in the document with characteristic vector of words, which appear in the document, and calculation of relevance score for each sentence. The relevance score of sentence is determined through its comparison with all the other sentences in the document and with the document title by cosine measure. Prior to application of this method the scope of features is defined and then the weight of each word in the sentence is calculated with account of those features. The weights of features, influencing relevance of words, are determined using genetic algorithms.	automatic summarization;genetic algorithm;relevance	Rasim M. Alguliyev;Ramiz M. Aliguliyev	2005	The 2005 IEEE/WIC/ACM International Conference on Web Intelligence (WI'05)	10.1109/WI.2005.57	text mining;genetic algorithm;multi-document summarization;computer science;automatic summarization;machine learning;pattern recognition;data mining;tf–idf;information retrieval	NLP	-25.710552737410314	-64.13034638585442	95811
943ff6e61fb717b5b9a2cb62f2943ff3929f437d	an inference approach to basic level of categorization	conceptualization;semantic network;会议论文;basic level of categorization	Humans understand the world by classifying objects into an appropriate level of categories. This process is often automatic and subconscious. Psychologists and linguists call it as Basic-level Categorization (BLC). BLC can benefit lots of applications such as knowledge panel, advertising and recommendation. However, how to quantify basic-level concepts is still an open problem. Recently, much work focuses on constructing knowledge bases or semantic networks from web scale text corpora, which makes it possible for the first time to analyze computational approaches for deriving BLC. In this paper, we introduce a method based on typicality and PMI for BLC. We compare it with a few existing measures such as NPMI and commute time to understand its essence, and conduct extensive experiments to show the effectiveness of our approach. We also give a real application example to show how BLC can help sponsored search.	categorization;experiment;humans;knowledge graph;knowledge base;scalability;search engine marketing;semantic network;text corpus	Zhongyuan Wang;Haixun Wang;Ji-Rong Wen;Yanghua Xiao	2015		10.1145/2806416.2806533	natural language processing;conceptualization;computer science;artificial intelligence;data science;machine learning;data mining;database;semantic network;world wide web;information retrieval	AI	-24.691868378892455	-58.81610615588116	96168
744e0508062043fcfcf89f0762f17eb10b7fce17	complementary information for wikipedia by comparing multilingual articles		Information of many articles is lacking in Wikipedia because users can create and edit the information freely. We specifically examined the multilinguality of Wikipedia and proposed a method to complement information of articles which lack information based on comparing different language articles that have similar contents. However, much non-complementary information is unrelated to a user’s browsing article in the results. Herein, we propose improvement of the comparison area based on the classified complementary target.	wikipedia	Yuya Fujiwara;Yu Suzuki;Yukio Konishi;Akiyo Nadamoto	2013		10.1007/978-3-642-37401-2_27	natural language processing;world wide web;information retrieval	NLP	-29.459480636354026	-63.47030682885022	96306
afdee392c887aae5fc529c815ec88cabaeba6bcf	abductive matching in question answering.		We study question-answering over semi-structured data. We introduce a new way to apply the technique of semantic parsing by applying machine learning only to provide annotations that the system infers to be missing; all the other parsing logic is in the form of manually authored rules. In effect, the machine learning is used to provide non-syntactic matches, a step that is ill-suited to manual rules. The advantage of this approach is in its debuggability and in its transparency to the end-user. We demonstrate the effectiveness of the approach by achieving state-of-the-art performance of 40.42% on a standard benchmark dataset over tables from Wikipedia.	abductive reasoning;benchmark (computing);machine learning;parsing;question answering;semantic analysis (machine learning);semi-structured data;semiconductor industry;wikipedia	Kedar Dhamdhere;Kevin S. McCurley;Mukund Sundararajan;Ankur Taly	2017	CoRR		machine learning;artificial intelligence;natural language processing;computer science;data mining;information retrieval;parsing;question answering;transparency (graphic)	NLP	-27.643104768131096	-65.54092441657706	96436
f6a391e5b58a66d8c6ee9b81a3264ea4d28971c7	bilingual keyword extraction and its educational application		We introduce a method that extracts keywords in a language with the help of the other. The method involves estimating preferences for topical keywords and fusing language-specific word statistics. At run-time, we transform parallel articles into word graphs, build crosslingual edges for word statistics integration, and exploit PageRank with word keyness information for keyword extraction. We apply our method to keyword analysis and language learning. Evaluation shows that keyword extraction benefits from cross-language information and language learners benefit from our keywords in reading comprehension test.	bridging (networking);experiment;keyness;keyword extraction;natural language processing;online and offline;pagerank;search engine optimization	Chung-Chi Huang;Mei-hua Chen;Ping-Che Yang	2015		10.18653/v1/W15-4407	natural language processing;speech recognition;computer science;information retrieval	NLP	-25.37840600413985	-61.95593236477675	96452
038d4fb53c77aa87cd4762bea62ba314327ab8d4	improving link analysis through considering hosts and blocks	host page relationship;web graph;web pages;web pages world wide web information analysis algorithm design and analysis merging information retrieval measurement standards navigation mathematics linear algebra;data mining;link analysis;internet;internet data mining;web structure mining;host layer;web page;page block relationship;page block relationship link analysis web structure mining web page web graph host layer web page layer host page relationship;web page layer	Link analysis has shown great potential in Web structure mining. Most existing link analysis methods treat a Web page as a node, a hyperlink as an edge in the Web graph. However, in reality, a Web page's importance is also affected by its semantic factor, the structure of WWW, and other factors. Therefore, traditional link analysis methods may not be so precise in reality. It is observed that the WWW has three layers: host layer, Web page layer and host layer. Based on this concept, it is considered merging the host-page relationship and the page-block relationship to improve the result of link analysis. Experiment results demonstrate the effectiveness of our approach	algorithm;anchor text;experiment;hyperlink;link analysis;structure mining;the superficial;www;web page;webgraph;world wide web	Qiang Wang;Yan Liu;Jun Yong Luo;Jing Ning;Qing Yao	2006	2006 IEEE/WIC/ACM International Conference on Web Intelligence (WI 2006 Main Conference Proceedings)(WI'06)	10.1109/WI.2006.97	web service;ajax;backlink;web mining;static web page;web development;web modeling;site map;data web;web design;page view;computer science;dynamic web page;web navigation;web page;printer-friendly;semantic web stack;database;same-origin policy;world wide web;website parse template;information retrieval	DB	-29.54520596863459	-53.31822842804292	96509
4af53c24474f277cfff96e1aeefa2116d515c38f	widit: integrated approach to hard topic search	busqueda informacion;integrated approach;noun phrase;information retrieval;book chapter;user feedback;information discovery;recherche information;keyword extraction;text retrieval;term extraction;automatic query expansion;query expansion;sliding window	Web Information Discovery Tool (WIDIT) Laboratory at the Indiana University School of Library, whose basic approach to combine multiple methods as well as to leverage multiple sources of evidence, participated in 2005 Text Retrieval Conference’s Hard track (HARD-2005) to investigate methods of effectively dealing with HARD topics by exploring a variety of query expansion strategies, the results of which were combined via an automatic fusion optimization process. We hypothesized that the “difficulty” of topics is often due to the lack of appropriate query terms and/or misguided emphasis on non-pivotal query terms by the system. Thus, our first-tier solution was to devise a wide range of query expansion methods that can not only enrich the query with useful term additions but also identify important query terms. Our automatic query expansion included such techniques as noun phrase extraction, synonym identification, definition term extraction, keyword extraction by overlapping sliding window, and Web query expansion. The results of automatic expansion were used in soliciting user feedback, which was utilized in a post-retrieval reranking process. The paper describes our participation in HARD-2005 and is organized as follows. Section 2 gives an overview of HARD track, section 3 describes the WIDIT approach to HARD-2005, and section 4 discusses the results and implications, followed by the concluding remarks in section 5.		Kiduk Yang;Ning Yu;Hui Zhang;Shahrier Akram;Ivan Record	2006		10.1007/11880592_58	natural language processing;sliding window protocol;sargable;query optimization;noun phrase;query expansion;web query classification;ranking;computer science;query by example;machine learning;concept search;data mining;rdf query language;web search query;world wide web;information retrieval;query language	NLP	-32.13692136215666	-62.41117455147782	96524
ebdc4b7580846433c1394f8ef7b7f0b60291402f	a framework to collect and extract publication lists of a given researcher from the web		Researchers usually publish their publication lists on the web. Collecting and extracting them can be of great value to research funding agencies and to applications such as academic network analysis and ranking systems. Because of the wide variety of citation styles and different web page formats, it is not straightforward to develop an automatic system to collect and extract researchersu0027 publication lists. In this paper, we describe the method used by our framework to collect and extract publication lists. It is composed of two tools, named Raposa - Citation Extractor, and Tucano - Publication Lists Collector. Raposa uses a method that identifies regions in the web page containing citations and the delimiters separating them. Tucano collects publication lists by submitting queries to a web search engine. Experimental results show that our framework obtains 93.5% of F1 measure for collecting publication lists, which is a better value when compared to Google Scholar.	world wide web	Cristiano Mesquita Garcia;Armando Honorio Pereira;Denilson Alves Pereira	2017	Int. J. Web Eng. Technol.	10.1504/IJWET.2017.088391	world wide web;web page;data mining;computer science;citation;publication;web search engine;extractor;ranking;delimiter	Web+IR	-30.76907304546396	-64.734580825073	96559
0de7c83b3faa48e21e43aab4a18b84c662f0dd30	ranking right-wing extremist social media profiles by similarity to democratic and extremist groups		Social media are used by an increasing number of political actors. A small subset of these is interested in pursuing extremist motives such as mobilization, recruiting or radicalization activities. In order to counteract these trends, online providers and state institutions reinforce their monitoring efforts, mostly relying on manual workflows. We propose a machine learning approach to support manual attempts towards identifying right-wing extremist content in German Twitter profiles. Based on a fine-grained conceptualization of rightwing extremism, we frame the task as ranking each individual profile on a continuum spanning different degrees of right-wing extremism, based on a nearest neighbour approach. A quantitative evaluation reveals that our ranking model yields robust performance (up to 0.81 F1 score) when being used for predicting discrete class labels. At the same time, the model provides plausible continuous ranking scores for a small sample of borderline cases at the division of right-wing extremism and New Right political movements.	binary classification;categorization;conceptualization (information science);f1 score;file spanning;learning to rank;machine learning;microsoft outlook for mac;natural language;right-wing authoritarianism;social media;triune continuum paradigm	Matthias Hartung;Roman Klinger;Franziska Schmidtke;Lars Vogel	2017			machine learning;artificial intelligence;public relations;computer science;radicalization;mobilization;politics;conceptualization;data mining;new right;workflow;social media;ranking	AI	-20.11490821097171	-60.30021223392951	96562
1ed5291b6746a6420db79e3706c6b70aa819dd8f	using bag-of-concepts to improve the performance of support vector machines in text categorization	support vector machine;viable supplement;standard text categorization collection;support vector machine classifier;concept-based text representation;certain category;text categorization;new approach;concept-based representation	This paper investigates the use of conceptbased representations for text categorization. We introduce a new approach to create concept-based text representations, and apply it to a standard text categorization collection. The representations are used as input to a Support Vector Machine classifier, and the results show that there are certain categories for which concept-based representations constitute a viable supplement to word-based ones. We also demonstrate how the performance of the Support Vector Machine can be improved by combining representations.	bag-of-words model;categorization;document classification;support vector machine	Magnus Sahlgren;Rickard Cöster	2004			support vector machine;computer science;machine learning;pattern recognition;data mining;categorization	HCI	-22.972665389838454	-66.04672775570914	96745
0512caeef7dfd7a7ccda9f77d417ddaf6f4daf31	ontology mining for personalizedweb information gathering	history;search engines;information retrieval;natural languages;data mining;information presentation;html;natural language;web crawling;web search information retrieval html search engines content based retrieval natural languages intelligent structures data mining history statistics;statistics;web search;intelligent structures;content based retrieval;question answering	Implicitly structured content on the Web such as HTML tables and lists can be extremely valuable for web search, question answering, and information retrieval, as the implicit structure in a page often reflects the underlying semantics of the data. Unfortunately, exploiting this information presents significant challenges due to the immense amount of implicitly structured content on the web, lack of schema information, and unknown source quality. We present TQA, a web-scale system for automatic question answering that is often able to find answers to real natural language questions from the implicitly structured content on the web. Our experiments over more than 200 million structures extracted from a partial web crawl demonstrate the promise of our approach.	experiment;html;information retrieval;natural language;question answering;structured content;web search engine;world wide web	Xiaohui Tao;Yuefeng Li;Ning Zhong;Richi Nayak	2007		10.1109/WI.2007.88	web mining;web modeling;data web;question answering;html;web design;web standards;computer science;database;web intelligence;natural language;web search query;world wide web;information retrieval;search engine;human–computer information retrieval	Web+IR	-30.059975361158983	-56.578221989149185	96761
9216497a3e87ca357dbd69c3cca6aaf258892a28	retrieving patents with inverse patent category frequency	databases;pattern clustering information retrieval law administration patents;patents heuristic algorithms databases search problems law tagging;law;patents;heuristic algorithms;search problems;dynamic interactive retrieval inverse patent category frequency dynamic ranking algorithms patent searching method;tagging	Patent has currently been captured strong attention as a key enabler for the knowledge and information centric companies and institutes. The higher the patent capability required, the more important an effective and efficient patent retrieval system needed. The conventional patent retrieval systems, however, have produced unsatisfactory results for the patent queries, since the inherent search systems would have come from the traditional keyword based models so that it has been inevitable to result in too many unrelated items. This has made the patent experts keep spending a lot of time to refine the results manually. We propose two dynamic ranking algorithms specialized patent-searching method, in which the dynamic interactive retrieval can be achieved. In the real USPTO dataset experiment, the dynamic ranking method shows substantial improvements with respect to time and cost over conventional static ranking approaches.	algorithm;cluster analysis;data point;peer-to-patent;relevance feedback;sensitivity and specificity;software patent;text-based (computing);truncation;web search engine	Justin JongSu Song;Wookey Lee;Jafar Afshar	2016	2016 International Conference on Big Data and Smart Computing (BigComp)	10.1109/BIGCOMP.2016.7425808	patent visualisation;computer science;data science;data mining;information retrieval	DB	-28.806561079630896	-59.42745078903944	96962
fd7004b22d4a80aac38c5b5d28b4621841887a64	unsupervised query segmentation using clickthrough for information retrieval	qslm;information retrieval;language modeling;probabilistic model;expectation maximization algorithm;em algorithm;clickthrough data;language model;query segmentation	Query segmentation is an important task toward understanding queries accurately, which is essential for improving search results. Existing segmentation models either use labeled data to predict the segmentation boundaries, for which the training data is expensive to collect, or employ unsupervised strategy based on a large text corpus, which might be inaccurate because of the lack of relevant information. In this paper, we propose a probabilistic model to exploit clickthrough data for query segmentation, where the model parameters are estimated via an efficient EM algorithm. We further study how to properly interpret the segmentation results and utilize them to improve retrieval accuracy. Specifically, we propose an integrated language model based on the standard bigram language model to exploit the probabilistic structure obtained through query segmentation. Experiment results on two datasets show that our segmentation model outperforms existing segmentation models. Furthermore, extensive experiments on a large retrieval dataset reveals that the results of query segmentation can be leveraged to improve retrieval relevance by using the proposed integrated language model.	bigram;consistency model;expectation–maximization algorithm;experiment;information retrieval;language model;query optimization;relevance;statistical model;text corpus;unsupervised learning	Yanen Li;Bo-June Paul Hsu;ChengXiang Zhai;Kuansan Wang	2011		10.1145/2009916.2009957	query expansion;expectation–maximization algorithm;computer science;machine learning;segmentation-based object categorization;pattern recognition;data mining;scale-space segmentation;information retrieval;statistics;language model;divergence-from-randomness model	Web+IR	-27.901394185133878	-62.101288350044776	97188
86ff3ec005973bd853a83581d2ee70793a6ff146	bag-of-concepts document representation for textual news classification		Automatic classification of news articles is a relevant problem due to the large amount of news generated every day, so it is crucial that these news are classified to allow for users to access to information of interest quickly and effectively. Traditional classification systems represent documents as bag-of-words (BoW), which are oblivious to two problems of language: synonymy and polysemy. This paper shows the advantages of using a bag-of-concepts (BoC) representation of documents, which tackles synonymy and polysemy, in text news classification – using a Support Vector Machines algorithm. In order to create BoC representations, a Wikipedia-based semantic annotator is used. To evaluate the proposal we used a purpose-built corpus and the Reuters 21578 corpus. Results show that the efficiency of the BoC approach is very dependent on the performance of the semantic annotator in extracting concepts, which depends heavily on the characteristics of particular corpora, reaching performance increases up to 29.65%.	algorithm;bag-of-words model in computer vision;experiment;freedom of information laws by country;machine learning;regular expression;statistical classification;support vector machine;text corpus;text mining;wikipedia	Marcos Mouriño-García;Roberto Pérez-Rodríguez;Luis E. Anido-Rifón	2015	Int. J. Comput. Linguistics Appl.		information retrieval;natural language processing;artificial intelligence;computer science	Web+IR	-25.088010266040126	-66.01511944141996	97229
20ec89b5457bdb598a8a5ab4b16d84d3bce78c81	overview of the inex 2009 link the wiki track	anchor to bep;wikipedia;focused link discovery;080704 information retrieval and web search;link discovery;evaluation;information retrieval link discovery;assessment	Wikipedia is becoming ever more popular. Linking between documents is typically provided in similar environments in order to achieve collaborative knowledge sharing. However, this functionality in Wikipedia is not integrated into the document creation process and the quality of automatically generated links has never been quantified. The Link the Wiki (LTW) track at INEX in 2007 aimed at producing a standard procedure, metrics and a discussion forum for the evaluation of link discovery. The tasks offered by the LTW track as well as its evaluation present considerable research challenges. This paper briefly described the LTW task and the procedure of evaluation used at LTW track in 2007. Automated link discovery methods used by participants are outlined. An overview of the evaluation results is concisely presented and further experiments are reported.	experiment;link building;wiki;wikipedia	Wei Che Huang;Yue Xu;Andrew Trotman;Shlomo Geva	2007		10.1007/978-3-540-85902-4_32	computer science;data mining;world wide web;information retrieval	Web+IR	-30.30166863798691	-62.66158073501674	97311
3d0c5940e0fff02112e6249ce87d6a4c55ed8807	evaluating quality of chinese product reviews based on fuzzy logic	fuzzy logic;opinion ming;product review;text ming	The prevalence of web2.0 makes e-Commerce an increasingly popular trend. Consumers can post their product reviews about product after buying products on the web. These product reviews can help online user make sensible decisions and enable business enterprises to improve their business strategies. To cope with the information overload problem, opinion mining is needed to extract useful expression and summarize important opinion for users. But the quality of product reviews at websites varies greatly. In this paper, we propose a method based fuzzy logic to evaluate the quality of Chinese product reviews. We define three fuzzy sets to represent the different types of product reviews. We determine the quality of product reviews under the proximity membership principle based the three fuzzy sets. Experiments based on an expert-composed product reviews corpus show that our method can achieve promising performance. © 2011 Springer-Verlag.	fuzzy logic	Wei Wei;Yang Xiang;Qian Chen;Xingjie Guo	2011		10.1007/978-3-642-24273-1_45	computer science;data mining	NLP	-23.74355185299789	-57.35080307982259	97424
56ca11e113a7ff5ba9876f48a8a879dc2aeca1a1	on choosing an effective automatic evaluation metric for microblog summarisation	summarisation;metrics;evaluation;social media	Popular microblogging services, such as Twitter, are engaging millions of users who constantly post and share information about news and current events each day, resulting in millions of messages discussing what is happening in the world. To help users obtain an overview of microblog content relating to topics and events that they are interested in, classical summarisation techniques from the newswire domain have been successfully applied and extended for use on microblogs. However, much of the current literature on microblog summarisation assumes that the summarisation evaluation measures that have been shown to be effective on newswire, are still appropriate for evaluating microblog summarisation. Hence, in this paper, we aim to determine whether the traditional automatic newswire summarisation evaluation metrics generalise to the task of microblog summarisation. In particular, using three microblog summarisation datasets, we determine a ranking of summarisation systems under three automatic summarisation evaluation metrics from the literature. We then compare and contrast this ranking of systems produced under each metric to system rankings produced through a qualitative user evaluation, with the aim of determining which metric best simulates human summarisation preferences. Our results indicate that, for the automatic evaluation metrics we investigate, they do not always concur with each other. Further, we find that Fraction of Topic Words better agrees with what users tell us about the quality and effectiveness of microblog summaries than the ROUGE-1 measure that is most commonly reported in the literature.	automatic summarization;evaluation function;evaluation of machine translation;lazy evaluation	Stuart Mackie;Richard McCreadie;Craig MacDonald;Iadh Ounis	2014		10.1145/2637002.2637017	social media;computer science;evaluation;data mining;world wide web;metrics;information retrieval	NLP	-24.142188538406	-58.43984780173852	97522
9f38787d67e3d461a0cb3e54cd1356b9a2223f46	webssql - a query language for multimedia web documents	directed graphs;information resources;query language;web documents;search engine;document handling;web pages;multimedia;software libraries;search engines;sql;information retrieval webssql query language multimedia web documents web pages structured data text data image data directed graph similarity based meaning search needs;information retrieval;data mining;image data;multimedia computing;html;similarity based meaning;internet;search needs;airplanes;directed graph;database languages web pages search engines uniform resource locators information retrieval data mining airplanes software libraries data models html;webssql;uniform resource locators;multimedia web documents;database languages;information retrieval sql information resources internet document handling multimedia computing directed graphs;text data;data models;structured data	We describe an SQL-like query language-WebSSQL-for retrieving desired Web pages. WebSSQL has several unique features. First, WebSSQL assumes that each Web page is a multimedia document consisting of structured data, text data and possibly image data. Second, WebSSQL treats each page as a node in a directed graph composed of many Web pages and links among them. Third, WebSSQL is similarity-based meaning that the retrieved Web pages will be ranked based on their closeness to a given query. The traditional SQL does not support similarity-based retrieval and ranking. With WebSSQL, users can specify their search needs more precisely, leading to more accurate retrieval of useful information.	query language;web page	Changqing Zhang;Weiyi Meng;Zonghuan Wu;Zhongfei Zhang	2000		10.1109/ADL.2000.848370	web service;backlink;static web page;query expansion;web modeling;site map;web query classification;ranking;web mapping;computer science;web page;database;same-origin policy;hits algorithm;web search query;world wide web;information retrieval	DB	-29.660127011783768	-54.054292819783356	97648
bbc1f40ce678c06737a0542ae0ddfa7c79e62b52	discovery and cataloging of deep web sources	google;manuals;search engines indexing information retrieval internet;search engines;information retrieval;information index deep web source cataloging deep web source discovery internet search engines google bing;html;indexes;internet;indexing;web sites;crawlers;web sites crawlers indexes html search engines manuals google	With more and more information goes online, extracting and managing the information from the Internet is becoming increasingly important. While the surface Web's information is relatively easy to obtain thanks to search engines such as Google and Bing, collecting the information from the deep Web is still a challenging task and these search engines do not index information located inside the deep Web. Compared to the surface Web, the deep Web contains vast more information. In particular, building a generalized search engine that can index deep Web across all domains remains a difficult research problem. In this paper, we highlight these challenges and demonstrate via prototype implementation of a generalized deep Web discovery framework that can achieve high precision.	automatic identification and data capture;cognitive dimensions of notations;deep web;deep web;google person finder;indirection;information source;input/output;internet;machine learning;prototype;surface web;web search engine;web server	Chelsea Hicks;Matthew Scheffer;Anne H. H. Ngu;Quan Z. Sheng	2012	2012 IEEE 13th International Conference on Information Reuse & Integration (IRI)	10.1109/IRI.2012.6303014	web application security;database index;search engine indexing;web development;web modeling;site map;the internet;data web;web mapping;html;web design;web search engine;web standards;computer science;web crawler;web navigation;distributed web crawling;web page;database;web intelligence;web search query;web 2.0;world wide web;information retrieval;search engine	DB	-29.912886133825964	-54.352549601671	97754
4c8fca4836f3e3639a946d12e5d4bbb943be26d9	chinese latent relational search based on relational similarity		Latent relational search is a new way of searching information in an unknown domain according to the knowledge of a known domain from the Web. By analyzing the analogous relationship between word pairs, the latent relational search engine can tell us the accurate information that we need. Given a Chinese query, {(A, B), (C, ?)}, our aim is to get D which is the answer of “?”. In this paper, we propose an approach to Chinese Latent relational search for the first time. Moreover, we classify the relation mappings between two word pairs into three categories according to the number of relations and the number of target words corresponding to each relation. Our approach firstly extracts relation-representing words by using the preprocessing modular and two clustering algorithms, and then the candidate word set of D corresponding to each relation-representing word can be obtained. The proposed method achieves an MRR of 0.563, which is comparable to the existing methods.	algorithm;artificial intelligence;cluster analysis;preprocessor;web search engine;world wide web	Chao Liang;Zhao Lu	2012		10.1007/978-3-642-34679-8_12	pattern recognition;data mining;database;information retrieval	AI	-26.251022525762735	-66.02136045988101	97825
10923f7afd290c98c7e64748228b95e442523b9a	conceptual language models for domain-specific retrieval	busqueda informacion;conceptual knowledge;relevance model;information retrieval system;pseudo relevance feedback;information retrieval;language modeling;internal structure;conceptual language model;sistema de recuperacion de informacion;universiteitsbibliotheek;query modeling;article letter to editor;systeme de recherche d information;recherche information;evaluation;informatics;evaluacion;mean average precision;relevance feedback;domain specificity;language model;test collection;meta language	Over the years, various meta-languages have been used to manually enrich documents with conceptual knowledge of some kind. Examples include keyword assignment to citations or, more recently, tags to websites. In this paper we propose generative concept models as an extension to query modeling within the language modeling framework, which leverages these conceptual annotations to improve retrieval. By means of relevance feedback the original query is translated into a conceptual representation, which is subsequently used to update the query model. Extensive experimental work on five test collections in two domains shows that our approach gives significant improvements in terms of recall, initial precision and mean average precision with respect to a baseline without relevance feedback. On one test collection, it is also able to outperform a text-based pseudo-relevance feedback approach based on relevance models. On the other test collections it performs similarly to relevance models. Overall, conceptual language models have the added advantage of offering query and browsing suggestions in the form of conceptual annotations. In addition, the internal structure of the meta-language can be exploited to add related terms. Our contributions are threefold. First, an extensive study is conducted on how to effectively translate a textual query into a conceptual representation. Second, we propose a method for updating a textual query model using the concepts in conceptual representation. Finally, we provide an extensive analysis of when and how this conceptual feedback improves retrieval. 2009 Elsevier Ltd. All rights reserved.	baseline (configuration management);consistency model;domain-specific language;information retrieval;language model;relevance feedback;tag (metadata);text-based (computing)	Edgar Meij;Dolf Trieschnigg;Maarten de Rijke;Wessel Kraaij	2010	Inf. Process. Manage.	10.1016/j.ipm.2009.09.005	natural language processing;conceptual model;query expansion;ranking;metalanguage;computer science;evaluation;machine learning;data mining;informatics;world wide web;information retrieval;query language;language model	Web+IR	-32.35845939357095	-61.325288730026934	98160
08bf9d8e0fbcf53dbc4ec78d052e1a1d8fd59076	a system for online news recommendations in real-time with apache mahout		With the ubiquitous access to the internet, news portals have become heavily consumed online services. The huge amount of published news makes it difficult for users to find relevant articles. Recommender systems have been developed for supporting users in finding the most interesting items in vast collections of available items. In contrast to traditional recommender systems, news recommender systems must address additional challenges. These challenges include the continuous changes in the set of items and the highly contextually dependent relevance of items as well as tight time constraints for providing recommendations and scalability requirements. In this work, we present our recommender system built based on APACHE MAHOUT tailored to the needs of news recommender systems. Two algorithms are combined to ensure highly precise recommendations and a high reliability. The system is evaluated in the CLEF NEWSREEL challenge. We discuss the performance of different tested algorithms and configurations. The evaluation shows that the developed system provides high quality results and fulfills the requirements of stream-based recommender scenarios.	algorithm;apache mahout;areal density (computer storage);circular buffer;consistency model;display resolution;e-services;mathematical optimization;optimizing compiler;portals;real-time transcription;recommender system;relevance;replay attack;requirement;response time (technology);sampling (signal processing);scalability;streaming media	Paul David Beck;Manuel Blaser;Adrian Michalke;Andreas Lommatzsch	2017			computer science	Web+IR	-32.44075282190443	-56.35433619474775	98247
34410651619eacffb05da28985b550a79a8d11d4	multi-tweet summarization of real-time events	graph theory;text mining tweet summarization twitter search social network analysis;text mining;search engines;information retrieval;text analysis;text analysis boolean algebra graph theory information retrieval search engines social networking online;tweet summarization;twitter hidden markov models real time systems vectors clustering algorithms search engines linear programming;boolean algebra;topical clustering multitweet summarization real time public event twitter search engine social search engine boolean query graph based retrieval algorithm topical diversity;social networking online;social network analysis;twitter search	Popular real-time public events often cause upsurge of traffic in Twitter while the event is taking place. These posts range from real-time update of the event's occurrences highlights of important moments thus far, personal comments and so on. A large user group has evolved who seeks these live updates to get a brief summary of the important moments of the event so far. However, major social search engines including Twitter still present the tweets satisfying the Boolean query in reverse chronological order, resulting in thousands of low quality matches agglomerated in a prosaic manner. To get an overview of the happenings of the event, a user is forced to read scores of uninformative tweets causing frustration. In this paper, we propose a method for multi-tweet summarization of an event. It allows the search users to quickly get an overview about the important moments of the event. We have proposed a graph-based retrieval algorithm that identifies tweets with popular discussion points among the set of tweets returned by Twitter search engine in response to a query comprising the event related keywords. To ensure maximum coverage of topical diversity, we perform topical clustering of the tweets before applying the retrieval algorithm. Evaluation performed by summarizing the important moments of a real-world event revealed that the proposed method could summarize the proceeding of different segments of the event with up to 81.6% precision and up to 80% recall.	algorithm;cluster analysis;real-time clock;real-time locating system;real-time web;social search;web search engine	Muhammad Asif Hossain Khan;Danushka Bollegala;Guangwen Liu;Kaoru Sezaki	2013	2013 International Conference on Social Computing	10.1109/SocialCom.2013.26	boolean algebra;text mining;social network analysis;social science;computer science;machine learning;data mining;world wide web;information retrieval	AI	-25.19089177110612	-54.35239789669422	98280
f608de3c97c3de6a2feea3872216499de9663b78	weighted argumentation for analysis of discussions in twitter	info eu repo semantics article;semantic attacks;weighted arguments;abstract argumentation;discussions in twitter	Twitter has become a widely used social network to discuss ideas about many domains. This leads to a growing interest in understanding what are the major accepted or rejected opinions in different domains by social network users. At the same time, checking what are the topics that produce the most controversial discussions among users can be a good tool to discover topics that can be divisive, what can be useful, e.g., for policy makers. With the aim to automatically discover such information from Twitter discussions, we present an analysis system based on Valued Abstract Argumentation to model and reason about the accepted and rejected opinions. We consider different schemes to weight the opinions of Twitter users, such that we can tune the relevance of opinions considering different information sources from the social network. Towards having a fully automatic system, we also design a relation labeling system for discovering the relation between opinions. Regarding the underlying acceptability semantics, we use ideal semantics to compute accepted/rejected opinions. We define two measures over sets of accepted and rejected opinions to quantify the most controversial discussions. In order to validate our system, we analyze different real Twitter discussions from the political domain. The results show that different weighting schemes produce different sets of socially accepted opinions and that the controversy measures can reveal significant differences between discussions.	algorithm;argumentation framework;bookmark (world wide web);cobham's thesis;denotational semantics;directed acyclic graph;emoticon;ontology components;rp (complexity);reasoning system;relevance;social network;support vector machine;treewidth	Teresa Alsinet;Josep Argelich;Ramón Béjar;Cèsar Fernández;Carles Mateu;Jordi Planes	2017	Int. J. Approx. Reasoning	10.1016/j.ijar.2017.02.004	computer science;knowledge management;artificial intelligence;machine learning;data mining;world wide web	ML	-23.97599246567476	-58.44570353376559	98588
a39b66c1ca910f91e42a031cb4432a570e358784	a new suffix tree similarity measure for document clustering	document clustering;cluster algorithm;web documents;document model;vector space model;hierarchical agglomerative clustering;suffix tree;agglomerative hierarchical clustering;term weighting;similarity measure	In this paper, we propose a new similarity measure to compute the pairwise similarity of text-based documents based on suffix tree document model. By applying the new suffix tree similarity measure in Group-average Agglomerative Hierarchical Clustering (GAHC) algorithm, we developed a new suffix tree document clustering algorithm (NSTC). Experimental results on two standard document clustering benchmark corpus OHSUMED and RCV1 indicate that the new clustering algorithm is a very effective document clustering algorithm. Comparing with the results of traditional word term weight tf-idf similarity measure in the same GAHC algorithm, NSTC achieved an improvement of 51% on the average of F-measure score. Furthermore, we apply the new clustering algorithm in analyzing the Web documents in online forum communities. A topic oriented clustering algorithm is developed to help people in assessing, classifying and searching the the Web documents in a large forum community.	algorithm;benchmark (computing);cluster analysis;f1 score;hierarchical clustering;similarity measure;suffix tree;text-based (computing);tf–idf;web page;world wide web	Hung Chim;Xiaotie Deng	2007		10.1145/1242572.1242590	correlation clustering;data stream clustering;document clustering;k-medians clustering;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;cluster analysis;single-linkage clustering;brown clustering;world wide web;vector space model;biclustering;dendrogram;hierarchical clustering of networks;clustering high-dimensional data;conceptual clustering	Web+IR	-26.601384551424108	-56.63398602204393	98654
460d641449b7d250c24e1e159cb48ea49b87565a	setting up a competition framework for the evaluation of structure extraction from ocr-ed books	information structure;hipertexto;navegacion informacion;structure information;information retrieval;navigation information;optical character recognition;information browsing;estructura informacion;table of contents;user assistance;reconnaissance caractere;assistance utilisateur;realite terrain;research evaluation;asistencia usuario;reconocimento optico de caracteres;realidad terreno;ground truth;character recognition;hypertexte;hypertext;reconocimiento caracter;reconnaissance optique caractere	This paper describes the setup of the Book Structure Extraction competition run at ICDAR 2009. The goal of the competition was to evaluate and compare automatic techniques for deriving structure information from digitized books, which could then be used to aid navigation inside the books. More specifically, the task that participants faced was to construct hyperlinked tables of contents for a collection of 1,000 digitized books. This paper describes the setup of the competition and its challenges. It introduces and discusses the book collection used in the task, the collaborative construction of the ground truth, the evaluation measures, and the evaluation results. The paper also introduces a data set to be used freely for research evaluation purposes.	book;ground truth;hyperlink;international conference on document analysis and recognition	Antoine Doucet;Gabriella Kazai;Bodin Dresevic;Aleksandar Uzelac;Bogdan Radakovic;Nikola Todic	2010	International Journal on Document Analysis and Recognition (IJDAR)	10.1007/s10032-010-0127-3	computer vision;simulation;speech recognition;hypertext;ground truth;table of contents;computer science;artificial intelligence;optical character recognition;world wide web	NLP	-32.57175608138948	-62.1236003649864	98688
5521c129360fd25475fc520c2aa4338a1241f998	a research case study: difficulties and recommendations when using a textual data mining tool	sic;privacy statement;corporate websites;legal statements;terms of use;clustering;naics;textual data mining;policy statements;industry classification	Although many interesting results have been reported by researchers using numeric data mining methods, there are still questions that need answering before textual data mining tools will be considered generally useful due to the effort needed to learn and use them. In 2011, we generated a dataset from the legal statements (mainly privacy policy and terms of use) on the websites of 475 of the US Fortune 500 Companies and used it as input to see what we could detect about the organizational relationships between the companies by using a textual data mining tool. We hoped to find that the tool would cluster similar corporations into the same industrial sector, as validated by the company’s self-reported North American Industry Classification System code (NAICS). Unfortunately, this proved only marginally successful, leading us to ask why and to pose our research question: What problems occur when a data-mining tool is used to analyze large textual datasets that are unstructured, complex, duplicative, and contain many homonyms and synonyms? In analyzing our large dataset we learned a great deal about the problem and fortunately, after significant effort, determined how to ‘‘massage’’ the raw dataset to improve the process and learn how the tool can be better used in research situations. We also found that NAICS, as self-reported by companies, are of dubious value to a researcher—a matter briefly discussed. 2013 Elsevier B.V. All rights reserved. * Corresponding author. E-mail addresses: abeer@cba.edu.kw (A.A. Al-Hassan), fal-shameri@howard.edu (F. Alshameri), esibley@gmu.edu (E.H. Sibley). 1 These impose a legal obligation between the corporation and the website user and vice versa. They may appear in more than one section on different corporations’ websites. Attachments or section such as a privacy statement and terms of use are the most common. But others, such as terms and conditions, forward statements, and links, cover the same topics but in no predefined order.		Abeer A. Al-Hassan;Faleh Alshameri;Edgar H. Sibley	2013	Information & Management	10.1016/j.im.2013.05.010	public relations;privacy policy;economics;industry classification;computer science;engineering;data science;marketing;data mining;north american industry classification system;cluster analysis;management;world wide web	AI	-23.407148404291277	-57.35045365363169	98759
189f83eb09f0896afefdbd577a5ce60df6090a0f	rumor detection on twitter	earthquake rumor detection twitter disaster;social networking online disasters earthquakes	Twitter is useful in a situation of disaster for communication, announcement, request for rescue and so on. On the other hand, it causes a negative by-product, spreading rumors. This paper describe how rumors have spread after a disaster of earthquake, and discuss how can we deal with them. We first investigated actual instances of rumor after the disaster. And then we attempted to disclose characteristics of those rumors. Based on the investigation we developed a system which detects candidates of rumor from twitter and then evaluated it. The result of experiment shows the proposed algorithm can find rumors with acceptable accuracy.	algorithm;burst transmission;dot-com bubble;named entity;named-entity recognition;online and offline;side effect (computer science)	Tetsuro Takahashi;Nobuyuki Igata	2012	The 6th International Conference on Soft Computing and Intelligent Systems, and The 13th International Symposium on Advanced Intelligence Systems	10.1109/SCIS-ISIS.2012.6505254	computer science;internet privacy;computer security	Robotics	-21.43688950267301	-54.96836276738525	98789
173fd7e31dcc29b7971585acf52d734882487283	temporal query log profiling to improve web search ranking	log analysis;temporal information;search logs analysis;web search;temporal data mining;query logs;search spam	Temporal information can be leveraged and incorporated to improve web search ranking. In this work, we propose a method to improve the ranking of search results by identifying the fundamental properties of temporal behavior of low-quality hosts and spam-prone queries in search logs and modeling those properties as quantifiable features. In particular, we introduce the concepts of host churn, a measure of changes in host visibility for user queries, and query volatility, a measure of semantic instability of query results, and propose the methods for construction of temporal profiles from search query logs that can be used for estimation of a set of features based on the introduced concepts. The utility of the proposed concepts has been experimentally demonstrated for two language-independent search tasks: the regression-based ranking of search results and a novel classification problem of detecting spam-prone queries introduced in this work.	experiment;information retrieval;instability;language-independent specification;search algorithm;sensor;spamming;volatility;web search engine	Alexander Kotov;Pranam Kolari;Lei Duan;Yi Chang	2010		10.1145/1871437.1871583	query expansion;web query classification;ranking;semantic search;computer science;spamdexing;concept search;data mining;database;search analytics;web search query;world wide web;information retrieval;search engine	Web+IR	-31.837096495939388	-54.74907638520895	98896
268857f94994b908446d3aa74de32161c4bcf80d	support or oppose? classifying positions in online debates from reply activities and opinion expressions		We propose a method for the task of identifying the general positions of users in online debates, i.e., support or oppose the main topic of an online debate, by exploiting local information in their remarks within the debate. An online debate is a forum where each user post an opinion on a particular topic while other users state their positions by posting their remarks within the debate. The supporting or opposing remarks are made by directly replying to the opinion, or indirectly to other remarks (to express local agreement or disagreement), which makes the task of identifying users’ general positions difficult. A prior study has shown that a linkbased method, which completely ignores the content of the remarks, can achieve higher accuracy for the identification task than methods based solely on the contents of the remarks. In this paper, we show that utilizing the textual content of the remarks into the link-based method can yield higher accuracy in the identification task.	horner's method	Akiko Murakami;Raymond H. Putra	2010			data mining	NLP	-23.354125287148754	-59.425032744080866	99146
5f39e48ea21412da169b847a8a24b6ca61763ae3	hawkes processes for continuous time sequence classification: an application to rumour stance classification in twitter		Classification of temporal textual data sequences is a common task in various domains such as social media and the Web. In this paper we propose to use Hawkes Processes for classifying sequences of temporal textual data, which exploit both temporal and textual information. Our experiments on rumour stance classification on four Twitter datasets show the importance of using the temporal information of tweets along with the textual content.	benchmark (computing);experiment;information;language model;multinomial logistic regression;social media;statistical classification;text corpus;world wide web	Michal Lukasik;P. K. Srijith;Duy Vu;Kalina Bontcheva;Arkaitz Zubiaga;Trevor Cohn	2016			artificial intelligence;natural language processing;computer science;machine learning;social media;data mining;exploit	NLP	-23.613892702842787	-55.553509493989466	99218
1cb045321cc3b2eb452b1c67bb0ea236152288e1	hspam14: a collection of 14 million tweets for hashtag-oriented spam research	spam;tweets;hashtag;twitter	Hashtag facilitates information diffusion in Twitter by creating dynamic and virtual communities for information aggregation from all Twitter users. Because hashtags serve as additional channels for one's tweets to be potentially accessed by other users than her own followers, hashtags are targeted for spamming purposes (e.g., hashtag hijacking), particularly the popular and trending hashtags. Although much effort has been devoted to fighting against email/web spam, limited studies are on hashtag-oriented spam in tweets. In this paper, we collected 14 million tweets that matched some trending hashtags in two months' time and then conducted systematic annotation of the tweets being spam and ham (i.e., non-spam). We name the annotated dataset HSpam14. Our annotation process includes four major steps: (i) heuristic-based selection to search for tweets that are more likely to be spam, (ii) near-duplicate cluster based annotation to firstly group similar tweets into clusters and then label the clusters, (iii) reliable ham tweets detection to label tweets that are non-spam, and (iv) Expectation-Maximization (EM)-based label prediction to predict the labels of remaining unlabeled tweets. One major contribution of this work is the creation of HSpam14 dataset, which can be used for hashtag-oriented spam research in tweets. Another contribution is the observations made from the preliminary analysis of the HSpam14 dataset.	email;expectation–maximization algorithm;hashtag;heuristic;spamdexing;spamming;virtual community	Surendra Sedhai;Aixin Sun	2015		10.1145/2766462.2767701	spam;data mining;internet privacy;world wide web	Web+IR	-22.555220144442625	-54.11883590693026	99311
2d3c2d399a003461e4fd4cf05e4cd0822e0598a8	ranking complex relationships on the semantic web	busqueda informacion;search engine;ontologie;buscador;architecture systeme;red www;metadata;search engines;bepress selected works;information retrieval;semantic web complex relationships ranking information retrieval semantic metadata document ranking search engines;sistema informatico;web semantique;reseau web;computer system;semantic web metadata semantic web technology semantic metadata ontology semantic association semantic discovery user defined context semantic analytics relationship based querying semantic ranking graph traversal relationship ranking test bed semantic relationships relationships search;test bed;graph traversal;user defined context;semantic web resource description framework data mining search engines ontologies large scale systems industrial relations information retrieval information analysis cancer;relationship based querying;semantic web technology;meta data semantic web search engines information retrieval;semantic metadata;web information retrieval;relationships search;relationship ranking;recherche information;web semantica;metadonnee;semantic relationships;xml;semantic web;world wide web;ontologia;arquitectura sistema;meta data;systeme informatique;metadatos;moteur recherche;system architecture;semantic discovery;empirical evaluation;ontology;information search and retrieval;semantic ranking;semantic analytics;rdf;xsl;semantic association;rdf metadata xml xsl	Industry and academia are both focusing their attention on information retrieval over semantic metadata extracted from the Web, and it is increasingly possible to analyze such metadata to discover interesting relationships. However, just as document ranking is a critical component in today's search engines, the ranking of complex relationships would be an important component in tomorrow's semantic Web engines. This article presents a flexible ranking approach to identify interesting and relevant relationships in the semantic Web. The authors demonstrate the scheme's effectiveness through an empirical evaluation over a real-world data set.	deep web;domain driven data mining;ibm notes;information retrieval;next-generation network;ranking (information retrieval);relevance;semantic web;web page;web search engine;world wide web	Boanerges Aleman-Meza;Christian Halaschek-Wiener;Ismailcem Budak Arpinar;Cartic Ramakrishnan;Amit P. Sheth	2005	IEEE Internet Computing	10.1109/MIC.2005.63	ranking;data web;semantic search;semantic grid;computer science;semantic web;ontology;social semantic web;semantic web stack;database;metadata;world wide web;information retrieval;semantic analytics;search engine	Web+IR	-32.14058736237826	-56.680254350673195	99376
5580d2afa9e9dab24912500a93bd035845fa29b9	robust and distributed web-scale near-dup document conflation in microsoft academic service	entity conflation near duplicate detection shingling algorithm n gram;resource management;n gram;algorithm design and analysis noise measurement resource management data models robustness computational modeling proteins;noise measurement;computational modeling;proteins;robustness;distributed web scale near dup document conflation scalable shingling algorithm gbdt model training microsoft academic service dataset noisy data conflation data quality;learning artificial intelligence document handling internet;entity conflation;near duplicate detection;algorithm design and analysis;shingling algorithm;data models	In modern web-scale applications that collect data from different sources, entity conflation is a challenging task due to various data quality issues. In this paper, we propose a robust and distributed framework to perform conflation on noisy data in the Microsoft Academic Service dataset. Our framework contains two major components. In the offline component, we train a GBDT model to determine whether two papers from different sources should be conflated to the same paper entity. In the online component, we propose a scalable shingling algorithm that can apply our offline model to over 100 million instances. The result shows that our algorithm can conflate noisy data robustly and efficiently.	andrei broder;approximation;banff world media festival;baseline (configuration management);dup;data mining;data quality;entity;gradient boosting;greedy algorithm;lu decomposition;local interconnect network;moses charikar;online and offline;pattern matching;rounding;scalability;signal-to-noise ratio;spatial database;symposium on theory of computing;w-shingling;web crawler;word-sense disambiguation;world wide web;yang	Chieh-Han Wu;Yang Song	2015	2015 IEEE International Conference on Big Data (Big Data)	10.1109/BigData.2015.7364059	computer science;data science;data mining;information retrieval	DB	-24.126111033351084	-63.70088302189364	99381
c5922a5964e6dc0d25449b2ee694b72615ec0306	an empirical study of embedding features in learning to rank		This paper explores the possibility of using neural embedding features for enhancing the effectiveness of ad hoc document ranking based on learning to rank models. We have extensively introduced and investigated the effectiveness of features learnt based on word and document embeddings to represent both queries and documents. We employ several learning to rank methods for document ranking using embedding-based features, keyword-based features as well as the interpolation of the embedding-based features with keyword-based features. The results show that embedding features have a synergistic impact on keyword based features and are able to provide statistically significant improvement on harder queries.	hoc (programming language);interpolation;learning to rank;ranking (information retrieval);synergy	Faezeh Ensan;Ebrahim Bagheri;Amal Zouaq;Alexandre Kouznetsov	2017		10.1145/3132847.3133138	interpolation;empirical research;learning to rank;computer science;embedding;machine learning;ranking;artificial intelligence;pattern recognition	Web+IR	-19.620886105323233	-62.81108364442386	99387
381969513efb5198279354c2ba59b9ca015cfb22	mining test oracles of web search engines	search engine;search engines;search engines data mining internet;search engines itemsets association rules google engines;data mining;web search engine;internet;association rule;data mining test oracle web search engine frequent association rules web data;test oracle;everyday life	Web search engines have major impact in people's everyday life. It is of great importance to test the retrieval effectiveness of search engines. However, it is labor-intensive to judge the relevance of search results for a large number of queries, and these relevance judgments may not be reusable since the Web data change all the time. In this work, we propose to mine test oracles of Web search engines from existing search results. The main idea is to mine implicit relationships between queries and search results, e.g., some queries may have fixed top 1 result while some may not, and some Web domains may appear together in top 10 results. We define a set of items of queries and search results, and mine frequent association rules between these items as test oracles. Experiments on major search engines show that our approach mines many high-confidence rules that help understand search engines and detect suspicious search results.	association rule learning;oracle machine;relevance;web search engine;web server;world wide web	Wujie Zheng;Hao Ma;Michael R. Lyu;Tao Xie;Irwin King	2011	2011 26th IEEE/ACM International Conference on Automated Software Engineering (ASE 2011)	10.1109/ASE.2011.6100085	search engine indexing;database search engine;organic search;metasearch engine;web search engine;semantic search;search engine optimization;proximity search;computer science;engineering;spamdexing;phrase search;web crawler;online search;data mining;search analytics;web search query;world wide web;information retrieval;search engine	SE	-30.414712562031866	-54.946995567536966	99757
7122f5e7b892d017ec96d606fa3498e4fb7b7c87	employing emotion keywords to improve cross-domain sentiment classification	cross domain sentiment classification;label propagation algorithm;emotion keywords;bipartite graph	Cross-domain classification is a challenging problem in the research of sentiment classification. In this study, we propose a novel approach to cross-domain sentiment classification by exploiting the classification knowledge from some emotion keywords. First, our approach uses some emotion keywords to extract the automatically-labeled samples with a high precision from the target area. Then, both the automatically-labeled samples from the target domain and the real labeled samples from the source domain are combined to be a new labeled data set. Third, all the labeled data and the unlabeled data in the target domain are used to perform cross-domain sentiment classification with a standard label-propagation algorithm. The empirical results demonstrate the effectiveness of our approach.		Zhu Zhu;Daming Dai;Yaxing Ding;Jianbin Qian;Shoushan Li	2012		10.1007/978-3-642-36337-5_8	computer science;pattern recognition;data mining;information retrieval	NLP	-19.841272620838392	-65.81810708667979	99905
9bf086d6e3127b0a98aacde4bc10b6c1eb3d7206	a new chinese text feature selection method in centroid-based classifier	centroid based classifier;entropy ratio chinese text feature selection method centroid based classifier;training;text analysis;natural languages;text analysis classification feature extraction natural languages;classification;entropy ratio;automatic text classification text feature selection centroid based classifier;text classification;computational modeling;chinese text feature selection method;text feature selection;feature extraction;automatic text classification;assessment methods;frequency text categorization support vector machines support vector machine classification information security standardization classification tree analysis bayesian methods information processing educational institutions;support vector machine classification;agriculture;feature selection;entropy;text categorization	Feature selection method based on text study is a mainstream method currently, whose research key lies in finding out one suitable feature assessment method, which can reduce the numbers of the words to be processed as less as possible in the situation of not decreasing classification precision, to improve the speed and the efficiency of classification. A new feature assessment method entropy ratio is proposed in this paper on the base of researching the classical feature assessment methods in the existing literature. This method not only considered feature classification ability, but also the feature generalization ability. It is a new and better choice to apply the centroid-based classifier to improve the effect of classification. Experimental results show that the effect obtained by using this method to select features is obviously superior to the one obtained by other methods, especially when the feature selected is less.	feature selection;statistical classification	Yijun Gu;Rong Wang;Jianhua Wang;Jiangde Yu	2008	2008 International Symposiums on Information Processing	10.1109/ISIP.2008.108	computer science;machine learning;linear classifier;pattern recognition;data mining;feature	Web+IR	-22.49615004182236	-64.44816932407758	100279
e87f97ebcfc960f45a07090aca27ce00fa9f193d	attentional image retweet modeling via multi-faceted ranking network learning			faceted classification	Zhou Zhao;Lingtao Meng;Jun Xiao;Min Yang;Fei Wu;Deng Cai;Xiaofei He;Yueting Zhuang	2018		10.24963/ijcai.2018/442	machine learning;artificial intelligence;computer science;ranking	AI	-24.055339851726842	-61.07745612172723	100324
50143e73030809335b29d04bdac9922f99dfd25d	trust-based improved recommendation of it-related web resources	specialized communities trust based improved recommendation it related web resources world wide web search engines specialized documents metadata trust based computing topic specific trust community members individual resources evaluation it specific faceted keyword based search tool yarquen informatics graduate students;context recommender systems engines measurement color computer science search engines;trusted computing document handling information technology internet search engines	The explosive growth of the Web makes increasingly harder to identify relevant resources among large result sets yield by search engines. Researcher looking for specialized documents have little guidance beyond the documents' own metadata. This article describes the use of trust-based computing to improve ranking and relevance of results by combining topic-specific trust among community members and individual's resources evaluation. The approach has been prototyped with Antu, built over the IT-specific faceted keyword-based search tool Yarquen. An experimental study was conducted with informatics graduate students, and found improved relevance of the suggested documents. This result suggests that trust-based approaches have huge potential to improve recommendations in specialized communities.	algorithm;baseline (configuration management);computer science;experiment;faceted classification;informatics;precision and recall;quality of results;recommender system;relevance;scalability;web resource;web search engine;world wide web	Pablo Cruz;Oscar Cornejo;Hernán Astudillo	2014	2014 XL Latin American Computing Conference (CLEI)	10.1109/CLEI.2014.6965185	computer science;data mining;world wide web;information retrieval	Web+IR	-29.9241315067387	-55.51970845996719	100329
2e11e025b298712e257e236941b3c704467d33a0	query expansion based on equi-width and equi-frequency partition		Query Expansion has been widely used to improve the effectiveness of conceptual search. In this paper pseudo relevance feedback is used along with equi-width and equi-frequency partition technique. The proposed method effectively uses the position and frequency of the query terms for identifying a region within the retrieved documents, which is expected to contain expansion terms. This region is an intersecting region obtained by partitioning the retrieved documents using equi-width and equi-frequency partition techniques. Initial results indicate that words falling in the intersecting region contain good candidate terms for query expansion. The experiments are performed on FIRE 2011’s Ad-hoc Hindi and English Data using Terrier as the retrieval engine. The initial experiments show an improvement in average precision of 12-14% in case of English data and 12.75% in case of Hindi data set.	frequency partition of a graph;query expansion	Rekha Vaidyanathan;Sujoy Das;Namita Srivastava	2011		10.1007/978-3-642-40087-2_2	query optimization;partition refinement	NLP	-32.88313677381269	-63.99180483695613	100757
2d6e268e48af1b96dede7cccb0cb75714c39a64a	comparative topic analysis of japanese and chinese bloggers	community;cultural difference between japan and china;overview;nihon blog mura;blogs medical treatment communities color educational institutions internet;color;blogger cultures;blog;blog posts;blogger community service comparative topic analysis japanese bloggers chinese bloggers blog posts bloggers classification blogger concerns blogger opinions blogger cultures sina blog nihon blog mura;bloggers classification;sina blog;comparative topic analysis;internet;web sites;chinese bloggers;pattern classification;cultural difference between japan and china blog community overview topic model;blogger community service;blogger opinions;blogger concerns;communities;medical treatment;japanese bloggers;information analysis;blogs;web sites information analysis pattern classification;topic model	"""This paper first studies how to apply a topic model to Chinese and Japanese blog posts collected from a few hundred Chinese and Japanese bloggers and then to classify bloggers into topics. The estimated topics are exploited in the task of over viewing the Chinese and Japanese bloggers' concerns, opinions, and cultures. Those topics are also quite helpful when comparing them between Chinese and Japanese in order to discover differences in the concerns, opinions, and cultures of the two languages. In the evaluation, we collect a few hundred bloggers from a well-known Sina blog host bloggers categories in China, and an also well-known blogger community service Nihon Blog Mura in Japan. As case studies, we focus on the """"health"""", """"military"""", and """"nursing care"""" categories in the services of both languages, and generate topics based on a topic model, and then overview and compare them between Chinese and Japanese. We actually discover certain differences in bloggers' topics between Chinese and Japanese."""	blog;blogger;chinese room;topic model	Liyi Zheng;Tian Nie;Ichiro Moriya;Yusuke Inoue;Takakazu Imada;Takehito Utsuro;Yasuhide Kawada;Noriko Kando	2014	2014 28th International Conference on Advanced Information Networking and Applications Workshops	10.1109/WAINA.2014.107	community;the internet;computer science;internet privacy;topic model;data analysis;world wide web	NLP	-21.18741340968865	-56.784390765589286	100844
191271738d391f3406f08821f0a7913d628be821	struap: a tool for bundling linguistic trees through structure-based abstract pattern		We present a tool for developing tree structure patterns that makes it easy to define the relations among textual phrases and create a search index for these newly defined relations. By using the proposed tool, users develop tree structure patterns through abstracting syntax trees. The tool features (1) intuitive pattern syntax, (2) unique functions such as recursive call of patterns and lexicon reference, and (3) whole workflow support for relation development and validation. We report the current implementation of the tool and its effectiveness.	frequent subtree mining;lexicon;recursion (computer science);search engine indexing;tree (data structure);tree structure	Kohsuke Yanai;Misa Sato;Toshihiko Yanase;Kenzo Kurotsuchi;Yuta Koreeda;Yoshiki Niwa	2017			computer science;artificial intelligence;natural language processing;machine learning	NLP	-28.18560034117989	-64.33075648384346	101004
789f8f9cd3a372a808f09585e7b88a7c1c4a0ea7	visual analysis of documents with semantic graphs	semantic representation;text mining;data mining;automatic generation;summarization;triplet;machine learning;directed graph;visual analysis;visual analytics;natural language processing;document visualization;named entity;exploratory data analysis;semantic graph	In this paper, we present a technique for visual analysis of documents based on the semantic representation of text in the form of a directed graph, referred to as semantic graph. This approach can aid data mining tasks, such as exploratory data analysis, data description and summarization. In order to derive the semantic graph, we take advantage of natural language processing, and carry out a series of operations comprising a pipeline, as follows. Firstly, named entities are identified and co-reference resolution is performed; moreover, pronominal anaphors are resolved for a subset of pronouns. Secondly, subject -- predicate -- object triplets are automatically extracted from the Penn Treebank parse tree obtained for each sentence in the document. The triplets are further enhanced by linking them to their corresponding co-referenced named entity, as well as attaching the associated WordNet synset, where available. Thus we obtain a semantic directed graph composed of connected triplets. The document's semantic graph is a starting point for automatically generating the document summary. The model for summary generation is obtained by machine learning, where the features are extracted from the semantic graph structure and content. The summary also has an associated semantic representation. The size of the semantic graph, as well as the summary length can be manually adjusted for an enhanced visual analysis. We also show how to employ the proposed technique for the Visual Analytics challenge.	data mining;directed graph;machine learning;named entity;natural language processing;parse tree;parsing;synonym ring;treebank;visual analytics;wordnet	Delia Rusu;Blaz Fortuna;Dunja Mladenic;Marko Grobelnik;Ruben Sipos	2009		10.1145/1562849.1562857	natural language processing;text graph;semantic similarity;semantic computing;visual analytics;explicit semantic analysis;directed graph;semantic grid;computer science;pattern recognition;abstract semantic graph;data mining;semantic compression;semantic equivalence;semantic technology;probabilistic latent semantic analysis;exploratory data analysis;graph database;information retrieval	NLP	-27.934486487298386	-64.28162168111561	101218
7e1169cf3cfad3ba86e00b3caa8e7b3fc91c727c	a bipartite graph-based ranking approach to query subtopics diversification focused on word embedding features		Web search queries are usually vague, ambiguous, or tend to have multiple intents. Users have different search intents while issuing the same query. Understanding the intents through mining subtopics underlying a query has gained much interest in recent years. Query suggestions provided by search engines hold some intents of the original query, however, suggested queries are often noisy and contain a group of alternative queries with similar meaning. Therefore, identifying the subtopics covering possible intents behind a query is a formidable task. Moreover, both the query and subtopics are short in length, it is challenging to estimate the similarity between a pair of short texts and rank them accordingly. In this paper, we propose a method for mining and ranking subtopics where we introduce multiple semantic and content-aware features, a bipartite graph-based ranking (BGR) method, and a similarity function for short texts. Given a query, we aggregate the suggested queries from search engines as candidate subtopics and estimate the relevance of them with the given query based on word embedding and content-aware features by modeling a bipartite graph. To estimate the similarity between two short texts, we propose a Jensen-Shannon divergence based similarity function through the probability distributions of the terms in the top retrieved documents from a search engine. A diversified ranked list of subtopics covering possible intents of a query is assembled by balancing the relevance and novelty. We experimented and evaluated our method on the NTCIR-10 INTENT-2 and NTCIR-12 IMINE-2 subtopic mining test collections. Our proposed method outperforms the baselines, known related methods, and the official participants of the INTENT-2 and IMINE-2 competitions. key words: subtopic mining, query intent, diversification, word embedding, bipartite graph	aggregate data;ambiguous grammar;baseline (configuration management);diversification (finance);information retrieval;jensen's inequality;relevance;shannon (unit);similarity measure;subpixel rendering;vagueness;web search engine;web search query;word embedding	Md Zia Ullah;Masaki Aono	2016	IEICE Transactions		natural language processing;diversification;bipartite graph;machine learning;pattern recognition;mathematics	Web+IR	-27.6307844865404	-62.791131284878986	101261
a9accff869fdd5501b2e7a65b6fc73f3557626b6	a new suffix tree similarity measure and labeling for web search results clustering	trees mathematics information retrieval internet pattern clustering search engines;pattern clustering;search engine;search engines;information retrieval;trees mathematics;web search engine;suffix tree;labeling web search search engines clustering algorithms navigation information retrieval merging educational institutions size measurement organizing;internet;web search;web search engines suffix tree similarity measure web search results clustering user queries document references;similarity measure	Due to the enormous size of the web and low precision of user queries, finding the right information from the web can be difficult if not impossible. One approach that tries to solve this problem is using clustering techniques for grouping similar documents together in order to facilitate presentation of results in more compact form and enable thematic browsing of the results set. Web Search Results Clustering is an attempt to apply the idea of clustering to document references (snippets) returned by a search engine in response to a query. Thus, it can be perceived as a way of organizing the snippets into set of meaningful thematic groups. This paper introduces a new similarity criterion for merging which is evaluated for search results returned from actual web search engines.	cluster analysis;organizing (structure);similarity measure;suffix tree;web search engine	Archana Pritam Kale;Ujwala Bharambe;M. SashiKumar	2009	2009 Second International Conference on Emerging Trends in Engineering & Technology	10.1109/ICETET.2009.13	beam search;search engine indexing;query expansion;web query classification;metasearch engine;document clustering;semantic search;computer science;spamdexing;phrase search;web crawler;data mining;cluster analysis;search analytics;web search query;world wide web;information retrieval;search engine;human–computer information retrieval	DB	-30.20945967801019	-56.28872017507122	101272
9694957c27c9227925771a48db1c915edc85e7a3	venue appropriateness prediction for contextual suggestion		is technical report presents the work of Università della Svizzera italiana (USI) at TREC 2016 Contextual Suggestion track. e goal of the Contextual Suggestion track is to develop systems that could make suggestions for venues that a user will potentially like. Our proposed method aempts to model the users’ behavior and opinion by training a SVM classier for each user. It then enriches the basic model using additional data sources such as venue categories and taste keywords to model users’ interest. For predicting the contextual appropriateness of a venue to a user’s context, we modeled the problem as a binary classication one. Furthermore, we built two datasets using crowdsourcing that are used to train a SVM classier to predict the contextual appropriateness of venues. Finally, we show how to incorporate the multimodal scores in our model to produce the nal ranking. e experimental results illustrate that our proposed method performed very well in terms of all the evaluation metrics used in TREC.	crowdsourcing;list of code lyoko characters;multimodal interaction;norm (social);text retrieval conference;venue (sound system)	Mohammad Aliannejadi;Ida Mele;Fabio Crestani	2016			data mining;computer science	Web+IR	-25.880387633904302	-60.59079469223831	101425
7d629f189fa8d295feb3a12539af684d7cc2b6ea	spam classification: a comparative analysis of different boosted decision tree approaches		PurposernrnrnrnrnEmail spam classification is now becoming a challenging area in the domain of text classification. Precise and robust classifiers are not only judged by classification accuracy but also by sensitivity (correctly classified legitimate emails) and specificity (correctly classified unsolicited emails) towards the accurate classification, captured by both false positive and false negative rates. This paper aims to present a comparative study between various decision tree classifiers (such as AD tree, decision stump and REP tree) with/without different boosting algorithms (bagging, boosting with re-sample and AdaBoost).rnrnrnrnrnDesign/methodology/approachrnrnrnrnrnArtificial intelligence and text mining approaches have been incorporated in this study. Each decision tree classifier in this study is tested on informative words/features selected from the two publically available data sets (SpamAssassin and LingSpam) using a greedy step-wise feature search method.rnrnrnrnrnFindingsrnrnrnrnrnOutcomes of this study show that without boosting, the REP tree provides high performance accuracy with the AD tree ranking as the second-best performer. Decision stump is found to be the under-performing classifier of this study. However, with boosting, the combination of REP tree and AdaBoost compares favourably with other classification models. If the metrics false positive rate and performance accuracy are taken together, AD tree and REP tree with AdaBoost were both found to carry out an effective classification task. Greedy stepwise has proven its worth in this study by selecting a subset of valuable features to identify the correct class of emails.rnrnrnrnrnResearch limitations/implicationsrnrnrnrnrnThis research is focussed on the classification of those email spams that are written in the English language only. The proposed models work with content (words/features) of email data that is mostly found in the body of the mail. Image spam has not been included in this study. Other messages such as short message service or multi-media messaging service were not included in this study.rnrnrnrnrnPractical implicationsrnrnrnrnrnIn this research, a boosted decision tree approach has been proposed and used to classify email spam and ham files; this is found to be a highly effective approach in comparison with other state-of-the-art modes used in other studies. This classifier may be tested for different applications and may provide new insights for developers and researchers.rnrnrnrnrnOriginality/valuernrnrnrnrnA comparison of decision tree classifiers with/without ensemble has been presented for spam classification.		Shrawan Kumar Trivedi;Prabin Kumar Panigrahi	2018	J. Systems and IT	10.1108/JSIT-11-2017-0105	knowledge management;adaboost;boosting (machine learning);image spam;gradient boosting;email spam;decision tree;decision tree learning;computer science;decision stump;machine learning;artificial intelligence	ML	-20.45007229631591	-60.78617498315214	101441
a825161e1a9a415e37be4ef24b30a6fe40c26b49	ambiguity of queries and the challenges for query language detection	query language	In this paper, a sample set of 510 simple searches from the TEL action log 2009 is analyzed for query content and query language. More than half of the queries are for named entities, which has consequences for query language disambiguation. A manual identification of query language finds that often a definite language cannot be determined, because many named entities are not translated. Problems and challenges for query category and language identification are discussed. Further analysis shows that IP address and interface language are not very strong indicators for determining the query	data logger;language identification;named entity;named-entity recognition;query language;relevance;the european library;word-sense disambiguation	Juliane Stiller;Maria Gäde;Vivien Petras	2010			natural language processing;language identification;online aggregation;sargable;query optimization;.ql;query expansion;web query classification;universal networking language;boolean conjunctive query;data manipulation language;data control language;computer science;query by example;database;rdf query language;web search query;range query;information retrieval;query language;object query language;spatial query	DB	-33.1048415317598	-65.07688220705928	101551
b84faf201c04017c5cf63161346897905db1a664	wust system at ntcir-12 short text conversation task		Our WUST team has participated in the Chinese subtask of the NTCIR-12 STC (Short Text Conversation) Task. This paper describes our approach to the STC and discusses the official results of our system. Our system constructs the model to find the appropriate comments for the query derived from the given post. In our system, we hold the hypothesis that the relevant posts tend to have the common comments. Given the query q, the topic words firstly are extracted from q, and the initial set of post-comment pairs retrieved, and then used to match and rank to produce the final ranked list. The core of the system is to calculate the similarity between the responses and the given query q. The experimental results using the NTCIREVAL tool suggest that our system should be improved by combining with related knowledge and features.	comment (computer programming)	Maofu Liu;Yifan Guo;Yang Wu;Limin Wang;Han Ren	2016			conversation;natural language processing;speech recognition;artificial intelligence;computer science	NLP	-30.721575320192564	-64.09861741724701	101616
30e9d83a28939e4bdc83858f35740b1bd3b48e10	modeling and visualization of media in arabic	media quantization;arabic text segmentation;visualization	In this paper, a novel method for analyzing media in Arabic using new quantitative characteristics is proposed. A sequence of newspaper daily issues is represented as histograms of occurrences of informative terms. The histograms closeness is evaluated via a rank correlation coefficient by treating the terms as ordinal data consistent with their frequencies. A new characteristic is introduced to quantify the relationship of an issue with numerous earlier ones. A newspaper is imaged as a time series of this characteristic values affected by the current social situation. The change points of this process may indicate fluctuations in the social behavior of the corresponding society as is evident from changes in the linguistic content. Moreover, the similarity measure created by means of this characteristic makes it possible to accurately derive the groups of homogeneous issues without any additional information. The methodology is evaluated on sequential issues of an Egyptian newspaper, “Al-Ahraam”, and a Lebanese newspaper, “Al-Akhbaar”. The results exhibit the high ability of the proposed approach to expose changes in the linguistic content and to connect them with changes in the structure of society and the relationships in it. The method can be suitably extended to every alphabetic language media.		Zeev Volkovich;Oleg N. Granichin;Oleg Redkin;Olga Bernikova	2016	J. Informetrics	10.1016/j.joi.2016.02.008	speech recognition;visualization;computer science;artificial intelligence;machine learning;data mining;world wide web;statistics	Visualization	-20.991225366671017	-59.015524540005245	101883
5d4af01b2f2e898d56e93151da11ef8af1426627	a joint information model for n-best ranking	n-best list;n-best ranking;small set;possible semantic property;joint model;joint information;baseline independent model;information theoretic objective function;novel task;joint ranking model;joint information model;empirical evidence;information model;objective function	In this paper, we present a method for modeling joint information when generating n-best lists. We apply the method to a novel task of characterizing the similarity of a group of terms where only a small set of many possible semantic properties may be displayed to a user. We demonstrate that considering the results jointly, by accounting for the information overlap between results, generates better n-best lists than considering them independently. We propose an information theoretic objective function for modeling the joint information in an n-best list and show empirical evidence that humans prefer the result sets produced by our joint model. Our results show with 95% confidence that the n-best lists generated by our joint ranking model are significantly different from a baseline independent model 50.0% ± 3.1% of the time, out of which they are preferred 76.6% ± 5.2% of the time.	approximation algorithm;baseline (configuration management);heuristic;information model;information theory;latent semantic analysis;loss function;optimization problem;recommender system;search algorithm	Patrick Pantel;Vishnu Vyas	2008			empirical evidence;information model;computer science;machine learning;data mining;information retrieval	NLP	-26.76566773189844	-62.31825322229116	101987
3b9ec8d97c610bdc7e29b03405e5bde91d09353c	jabot: a multilingual java-based intelligent agent for web sites	web site page;intelligent agent;information retrieval;novel type;multilingual natural language interface;lexical semantic map;common web site problem;associated linguistic knowledge;web site	This paper presents a novel type of intelligent agent with a multilingual natural language interface, which retrieves information from within a Web site. This agent, named JaBot after the fact that it is a bot which has been programmed in Java, has been designed and developed by the authors in an attempt to solve common Web site problems related to information retrieval. JaBot runs quickly and efficiently, and rather than running directly on the Web site pages, it is connected to a lexical semantic map. This map is based upon the contents of the Web site in question together with other associated linguistic knowledge. Introduction Java was launched by Sun Microsystems in the early '90s as a simple, robust, dynamic, multithreaded, general-purpose, object-oriented, platform independent programming language! Its strengths can be split into four key issues, namely, portability, security, robustness and ease of usage, and distributed operation across the Web (Read et al., 1997). These benefits make Java an ideal programming language for constructing Web-based computational linguistic applications and agents (Ritchey, 1995; Sommers, 1997). Some applications of this type are beginning to appear on the Web, such as the English learning tools developed in Java by the authors as part of the UNED Profesor Virtual (UPV) research project 1 (Read & B~ircena, in prep.). l Although JaBot and the rest of the modules that make up the UPV are fully functional and have been operational for some time now locally on our departmental Web pages, they cannot be accessed yet on the Internet because our Web site is in the f'mal stages of construction. Access to the vast amounts of information contained on the Web still highlights some problems, such as that o f cataloguing or indexing all that information. The sheer size of the Web and the ever changing nature of its contents means that the process of charting it is closer to mapping a large cavern with only the aid of a small torch than to the construction of a library catalogue. Bot or agent technology is playing an increasingly important role in this mapping process, as will be seen next. 1 Bots and the Web Bots are distinguished from other commonly used programs in that they act as if they have some degree of intelligence and independence (Thompson, 1998). Born in the '60s, nowadays bots should be viewed as part of the wider move towards distributed object-based systems (Weber, 1997). Instead of having massive programs, the tendency is to use networked computer systems made of a large number of co-operating taskspecific components. Some of these components will act when told to; others, bots, will be more autonomous, making the on-line experience more pleasant and productive. Internet search engines have a reputation of being unfriendly and unhelpful, despite the fact that some of them offer basic natural language interaction. The problem arises exactly at the point when the user connects to a specific Web site in search of some information that s/he believes to be contained there. I f the site is large and there is no search engine, finding a particular item can be very difficult and time consuming, especially over a slow connection. Even if a search engine does exist, the current basis of search technology on the use of 'wild card'-based literal strings means that, unless the user knows a keyword which will be part of the entry s/he wants, the results of the search may well be zero links or a large list of	autonomous robot;computer security;distributed object;general-purpose modeling;information retrieval;intelligent agent;java;literal (computer programming);natural language user interface;object-based language;online and offline;programming language;robustness (computer science);software portability;surface web;thread (computing);torch;video game bot;web page;web search engine;world wide web	Timothy Read;Elena Bárcena	1998			natural language processing;lexical semantics;natural language user interface;computer science;linguistics;world wide web;intelligent agent;information retrieval	AI	-31.655244407581225	-56.00737967810295	102023
d219d9663841a2d765c21726f94fd9866bd579ad	improving summarization quality with topic modeling	maximal coverage;topic modeling;summarization;lda;polytope model;linear programming;genetic algorithm;optimization	The problem of extractive text summarization for a collection of documents is defined as the problem of selecting a small subset of sentences so that the contents and meaning of the original document set are preserved in the best possible way. In this paper we describe different applications of topic modeling as it relates to summarization. We consider two summarization models - supervised and unsupervised - enhanced by integrating the topic knowledge into their standard lexicon-based models. Both summarizers strive to cover as much information of the input documents as possible when generating the summaries. The supervised summarizer operates in a standard rank-and-select-sentences manner of extractive summarization, where the best linear combination of multiple sentence features is learned by a genetic algorithm (GA). The unsupervised summarizer models the summarization task as an optimization problem. As is the case with most existing summarization approaches, both original models measure information coverage by lexical units and were enriched by topic knowledge, providing a new measure for the informaition coverage. The experimental results show that utilizing topic knowledge improves the summarization quality.	automatic summarization;genetic algorithm;lexicon;mathematical optimization;optimization problem;topic model	Marina Litvak;Natalia Vanetik;Chunlei Liu;Lemin Xiao;Onur Savas	2015		10.1145/2809936.2809944	multi-document summarization;computer science;submodular set function;automatic summarization;machine learning;pattern recognition;data mining	NLP	-26.301366958726728	-62.70376444340637	102048
9ea5ef780bafba5ff9b03c78a5bbc1d0798ca014	measuring, predicting and visualizing short-term change in word representation and usage in vkontakte social network		Language in social media is extremely dynamic: new words emerge, trend and disappear, while the meaning of existing words can fluctuate over time. This work addresses several important tasks of visualizing and predicting short term text representation shift, i.e. the change in a word’s contextual semantics. We study the relationship between short-term concept drift and representation shift on a large social media corpus – VKontakte collected during the Russia-Ukraine crisis in 2014 – 2015. We visualize short-term representation shift for example keywords and build predictive models to forecast short-term shifts in meaning from previous meaning as well as from concept drift. We show that short-term representation shift can be accurately predicted up to several weeks in advance and that visualization provides insight into meaning change. Our approach can be used to explore and characterize specific aspects of the streaming corpus during crisis events and potentially improve other downstream classification tasks including real-time event forecasting in social media.	concept drift;downstream (software development);futures studies;long short-term memory;predictive modelling;protologism;real-time locating system;signal-to-noise ratio;social media;social network;streaming media;word embedding	Ian Stewart;Dustin Arendt;Eric Bell;Svitlana Volkova	2017			artificial intelligence;streams;machine learning;natural language processing;concept drift;social media;social network;semantics;computer science	NLP	-22.173843028277517	-53.70599764310645	102270
5a85cd78a8f6223e676c185267aefca1412611ea	meta-classifiers easily improve commercial sentiment detection tools		In this paper, we analyze the quality of several commercial tools for sentiment detection. All tools are tested on nearly 30,000 short texts from various sources, such as tweets, news, reviews etc. The best commercial tools have average accuracy of 60%. We then apply machine learning techniques (Random Forests) to combine all tools, and show that this results in a meta-classifier that improves the overall performance significantly.	machine learning;random forest;tree-meta	Mark Cieliebak;Oliver Dürr;Fatih Uzdilli	2014			sentiment analysis;artificial intelligence;natural language processing;random forest;computer science	Metrics	-21.135983422640997	-61.20600975232814	102600
86fb521bfc75e1509a23daaf61c921ac0b77764a	discovering and characterizing places of interest using flickr and twitter	databases;web;qa75 electronic computers computer science;ibcn;places of interest;technology and engineering;geographic information retrieval;discovering;social media	Databases of places have become increasingly popular to identify places of a given type that are close to a user-specified location. As it is important for these systems to use an up-to-date database with a broad coverage, there is a need for techniques that are capable of expanding place databases in an automated way. In this paper the authors discuss how geographically annotated information obtained from social media can be used to discover new places. In particular, the authors first determine potential places of interest by clustering the locations where Flickr photos have been taken. The tags from the Flickr photos and the terms of the Twitter messages posted in the vicinity of the obtained candidate places of interest are then used to rank them based on the likelihood that they belong to a given type. For several place types, their methodology finds places that are not yet contained in the databases used by Foursquare, Google, LinkedGeoData and Geonames. Furthermore, the authors’ experimental results show that the proposed method can successfully identify errors in existing place databases such as Foursquare.	cluster analysis;database;flickr;geonames;social media	Steven Van Canneyt;Steven Schockaert;Bart Dhoedt	2013	Int. J. Semantic Web Inf. Syst.	10.4018/ijswis.2013070105	social media;computer science;data mining;internet privacy;world wide web	Web+IR	-25.127195332903174	-52.30939916502789	102603
60dff9feae346922f2da75e747310b8f653baeb9	a hybrid system using pso and data mining for determining the ranking of a new participant in eurovision	social modeling;voting behavior;pso;data mining;hybrid system	The intention of the present work is to apply data mining and PSO to propose the solution of a specific problem about society modelling. We analyze the voting behavior and ratings of judges in a popular song contest held every year in Europe. The dataset makes it possible to analyze the determinants of success, and gives a rare opportunity to run a direct test of vote trading from logrolling. We show that they are rather driven by linguistic and cultural proximities between singers and voting countries. With this information it is possible to predict the final rank of a new country in the contest.	data mining;hybrid system;particle swarm optimization	Carlos Alberto Ochoa Ortíz Zezzatti;Angel Eduardo Muñoz Zavala;Arturo Hernández Aguirre	2008		10.1145/1389095.1389423	voting behavior;artificial intelligence;data science;machine learning;data mining;hybrid system	ML	-20.536411722189513	-52.17802760617322	102713
5b5db1d182c2186f249794c1bd49a964568f05dc	summarizing email conversations with clue words	mobile device;text mining;email summarization	Accessing an ever increasing number of emails, possibly on small mobile devices, has become a major problem for many users. Email summarization is a promising way to solve this problem. In this paper, we propose a new framework for email summarization. One novelty is to use a fragment quotation graph to try to capture an email conversation. The second novelty is to use clue words to measure the importance of sentences in conversation summarization. Based on clue words and their scores, we propose a method called CWS, which is capable of producing a summary of any length as requested by the user. We provide a comprehensive comparison of CWS with various existing methods on the Enron data set. Preliminary results suggest that CWS provides better summaries than existing methods.	chrome web store;email;mobile device	Giuseppe Carenini;Raymond T. Ng;Xiaodong Zhou	2007		10.1145/1242572.1242586	text mining;computer science;automatic summarization;data mining;mobile device;database;internet privacy;world wide web	Web+IR	-27.70696753782076	-54.418005911927054	102747
254da68cf883690476d899c74e44a80afce87535	exploring traits of adjectives to predict polarity opinion in blogs and semantic filters in genomics	language model;query expansion;query language	This paper presents the results of our team in the Genomics and Blog tracks in TREC 2007. We used the language model implementation provided by Indri for both tracks. For the BLOG track we explored the use of adjectives with in a post as a way to predict opinion polarity. Our work in the Genomics track explores two approaches to generate queries from the original topics. The first approach performs automatic term expansion using UMLS to generate a structured query that can be submitted using Indri’s query language. The second approach uses a query expansion and re-ranking method based on identification of semantic relatives. This approach tries to capture the semantic of the potential answer, key terms in the topic and detection of gene/protein terms mentioned in the topic.	blog;language model;query expansion;query language	Miguel E. Ruiz;Ying Sun;Jianqiang Wang;Hongfang Liu	2007			light spectrum;information retrieval;data mining;photoresist;photomultiplier;focal length;computer science;wafer;laser;planar	NLP	-29.104876201593505	-65.12397946731545	102784
dcef39723aa0a1e2a1474775407e56b6f916f1a4	exploiting extended search sessions for recommending search experiences in the social web	conference publication;community recommendation;social search	HeyStaks is a case-based social search system that allows users to create and share case bases of search experiences (called staks) and uses these staks as the basis for result recommendations at search time. These recommendations are added to conventional results from Google and Bing so that searchers can benefit from more focused results from people they trust on topics that matter to them. An important point of friction in HeyStaks is the need for searchers to select their search context (that is, their active stak) at search time. In this paper we extend previous work that attempts to eliminate this friction by automatically recommending an active stak based on the searchers context (query terms, Google results, etc.) and demonstrate significant improvements in stak recommendation accuracy.		Zurina Saaya;Markus Schaal;Maurice Coyle;Peter Briggs;Barry Smyth	2012		10.1007/978-3-642-32986-9_28	data mining;world wide web;information retrieval	Web+IR	-33.568920329777455	-53.11905682332425	102900
2db0b31b2945f40cb06130d45011833e5bc3cc0d	modelling on chinese subject-term extracting algorithm	unlisted terms;paragraph analysis;text modelling;chinese segmentation;lexicon;subject term extraction;synonymous terms;chinese language;semantic analysis	With the rapid development of computer science and technology, information is increasing dramatically. How to analyse and extract useful knowledge effectively from the huge data is becoming more and more important. As the weighted subject-terms can be considered as the condensed versions of documents, on the basis of the statistics and computational linguistics, this paper presents a novel Chinese subject-term extraction algorithm based on paragraph analysing, Chinese segmentation, synonymous and unlisted-term processing. The proposed algorithm can remedy the shortage of pure statistics method and avoid the lower efficiency on semantic analysing. The experimental results and the analysis show the feasible of the approach, and the existing problems and further works are also present in the end.	algorithm;computational linguistics;computer science;feature selection;terminology extraction;text corpus;thesaurus	Kai Gao;Yang-Jie Li	2011	IJMIC	10.1504/IJMIC.2011.041309	natural language processing;speech recognition;computer science;data mining;chinese	AI	-24.79158837987049	-65.54844400862406	103049
1a1f598c8657f4eb9daf5d9a507641ea55861320	beyond the web graph: mining the information architecture of the www with navigation structure graphs	graph theory;information architecture;web graph;cascading style sheets;data mining;web structure mining web graph information architecture mining www navigation structure graph web sites content organization link analysis navigation system mining;navigation;visualization;web sites data mining graph theory;navigation humans organizations information architecture cascading style sheets data mining visualization;hierarchy extraction;web sites;web structure mining;humans;organizations;hierarchy extraction web structure mining web graph	Large Web sites contain a plethora of different menus and navigation aids, which implement systems of content organization as hierarchies, linear structures or matrices. Humans are able to decode the fine-grained content organization because they are aware of the different access methods provided by navigation systems and understand the higher-level information architecture. In contrast, current methods of link analysis cannot extract such a detailed model of the information architecture and are not able to recognize site boundaries and content hierarchies the way humans do. In this paper present a new approach of mining navigation systems that increases the precision of Web structure mining. Instead of analyzing the complete Web graph spanned by pages and hyperlinks, sub graphs called Navigation Structure Graphs (NSGs) are analyzed. A NSG represents the hyperlinks belonging to a certain navigation system. We demonstrate the capabilities of NSGs for analyzing the organization of Web sites and present our research on mining NSGs.	humans;hyperlink;information architecture;link analysis;structure mining;www;webgraph;world wide web	Matthias Keller;Martin Nussbaumer	2011	2011 International Conference on Emerging Intelligent Data and Web Technologies	10.1109/EIDWT.2011.23	computer science;web navigation;data mining;printer-friendly;database;world wide web	DB	-28.946412941270477	-53.24394083418334	103133
a40590007b45a161497d9b6b6badb5be983fa154	a quality evaluation of combined search on a knowledge base and text		We provide a quality evaluation of KB+Text search, a deep integration of knowledge base search and standard full-text search. A knowledge base (KB) is a set of subject–predicate–object triples with a common naming scheme. The standard query language is SPARQL, where queries are essentially lists of triples with variables. KB+Text search extends this by a special occurs-with predicate, which can be used to express the co-occurrence of words in the text with mentions of entities from the knowledge base. Both pure KB search and standard full-text search are included as special cases. We evaluate the result quality of KB+Text search on three different query sets. The corpus is the full version of the English Wikipedia (2.4 billion word occurrences) combined with the YAGO knowledge base (26 million triples). We provide a web application to reproduce our evaluation, which is accessible via http://ad.informatik.uni-freiburg.de/publications.	entity;error analysis (mathematics);freebase;knowledge base;query language;sparql;web application;wikipedia;yago	Hannah Bast;Björn Buchhold;Elmar Haussmann	2017	KI - Künstliche Intelligenz	10.1007/s13218-017-0513-9	web application;web search query;data mining;information retrieval;sparql;predicate (grammar);full text search;semantic search;query language;knowledge base;computer science	Web+IR	-30.883061937501537	-65.94451305619852	103134
1bb1c47d56850adbb8db2f41f84d4db3f7996aa8	detecting spammers in community question answering		As the popularity of Community Question Answering(CQA) increases, spamming activities also picked up in numbers and variety. On CQA sites, spammers often pretend to ask questions, and select answers which were published by their partners or themselves as the best answers. These fake best answers cannot be easily detected by neither existing methods nor common users. In this paper, we address the issue of detecting spammers on CQA sites. We formulate the task as an optimization problem. Social information is incorporated by adding graph regularization constraints to the text-based predictor. To evaluate the proposed approach, we crawled a data set from a CQA portal. Experimental results demonstrate that the proposed method can achieve better performance than some state-of-the-art methods.	kerrison predictor;mathematical optimization;optimization problem;question answering;sensor;social network;spamming;text-based (computing);yahoo! answers	Zhuoye Ding;Yeyun Gong;Yaqian Zhou;Qi Zhang;Xuanjing Huang	2013			computer science;data mining;world wide web;information retrieval	AI	-23.466598638970805	-52.69750171090759	103609
75946e8e9d2f93463f297eb544dc48be1d12a20e	consumer health information system		World Wide Web acts as one of the major sources of information for health related questions. However, often, there are multiple conflicting answers to a single question and it is hard to come up with “a single best correct answer”. Therefore, it is highly desirable to identify conflicting perspectives about a particular question (or topic). In this paper, we have described our participation in Consumer Health Information System(CHIS) task at FIRE 2016. There were two sub-tasks in this contest. The first sub-task deals with identifying if a particular answer is relevant to a given question. The second sub-task deals with detecting if a particular answer agrees or refuses the claim posed in a given question. We pose both these tasks as supervised pair classification tasks. We report our results for various document representations and classification algorithms.	algorithm;information system;sensor;world wide web	Raksha Sanjay Jalan;Pattisapu Nikhil Priyatam;Vasudeva Varma	2016			marketing;health informatics;business	AI	-23.50976372875743	-64.68787649828157	103705
5e61b5d8c67aea61f86a8887edf6457489e9ac40	phrase-representation summarization method and its evaluation	phrase-represented summary;phrase-representation summarization;at-a-glance;indicative;task-based evaluation	We have developed a summarization method that creates a summary suitable for the process of sifting information retrieval results. Unlike conventional methods that extract important sentences, this method constructs short phrases to reduce the burden of reading long sentences. We developed an improved task-based evaluation method and applied to prove the effectiveness of phrase-represented summary. In the task-based evaluation in TSC, phrase-represented summary provided the fastest judgement among systems that achieved almost the same accuracy. However, the experiment method has some problems, and we must try to design tasks closer to the real world IR in the future TSC.	fastest;information retrieval;remote desktop services	Mamiko Oka;Yoshihiro Ueda	2001			automatic summarization;artificial intelligence;pattern recognition;phrase;computer science	Web+IR	-26.871460954883275	-65.20353380621916	103790
ccec2e1708450d9514c42513deacd920d5c761dd	using topic models to analyze concepts for new mesh terms.				Zhiguo Yu;Todd R. Johnson	2016			topic model;theoretical computer science;computer science	Theory	-24.526234916976083	-61.44816520334253	103794
01b830f1d1d4a0df9ed145293cc80a9d9ceea4bf	sias: suicidal intentions alerting system		In this paper, we present an alerting system based on an efficient classification model for detecting suicidal people using natural language processing and data mining techniques. The model uses linguistic features which are derived from an analysis of handwritten and electronic messages/notes. The model was trained and validated with fully anonymised real data provided by the Cyber Crime Division of Greek Police as well as available suicidal notes from social media. The alerting system is intended as a prevention, management tool for automatic detection of suicidal intentions.	data mining;feature extraction;natural language processing;sensor;social media;social network;text mining	Georgios Domalis;Christos Makris;Pantelis Vikatos;Anastasios Papathanasiou;Efterpi Paraskevoulakou;Manos Sfakianakis	2017		10.5220/0006297402910297	computer science;world wide web;internet privacy	AI	-21.136450034052306	-56.61601386973501	103967
209b21a43441be90fd31267151d6a0df9dd6476b	hybrid mappings of complex questions over an integrated semantic space	semantic similarity;ontology mapping;text analysis;integrated semantic space;ontologies artificial intelligence;ontology mapping hybrid mapping integrated semantic space latent semantic analysis uncertainty reasoning;uncertainty reasoning;semantic web;ontologies frequency particle measurements natural languages semantic web information management functional analysis organizing taxonomy terminology;semantic space;hybrid mapping;semantic relations;latent semantic analysis;text analysis ontologies artificial intelligence semantic web	We address the issue of measuring semantic similarity between ontologies and text by means of applying latent semantic analysis. This method allows ranking of vector representations describing semantic relations according to their cosine similarity with a particular query. Our work is expected to make contributions including the introduction of reasoning about uncertainty when mapping between ontologies, an algorithm that can perform automatic mapping between concepts or relations derived from text and concepts or relations belonging to different ontologies, and the capability to infer implicit similarity between concepts or relations	algorithm;cosine similarity;latent semantic analysis;ontology (information science);semantic similarity	Gaston Burek;Anne N. De Roeck;Zdenek Zdráhal	2005	16th International Workshop on Database and Expert Systems Applications (DEXA'05)	10.1109/DEXA.2005.110	semantic data model;natural language processing;semantic interoperability;semantic similarity;semantic computing;text mining;latent semantic indexing;semantic integration;semantic web rule language;explicit semantic analysis;latent semantic analysis;semantic search;semantic grid;computer science;semantic web;social semantic web;data mining;semantic web stack;semantic compression;semantic equivalence;semantic technology;probabilistic latent semantic analysis;information retrieval;semantic analytics	Web+IR	-25.436226315912524	-63.66504576329667	104286
136da2a9a78b82cbecd80b803233c993468641de	concept chaining utilizing meronyms in text characterization	digital library;digital libraries;document representation;text characterization;ease of use;concept extraction;machine learning;clustering;term weighting;background knowledge;access method;bag of words;ontology;natural language processing	For most, the web is the first source to answer a question formulated by curiosity, need, or research reasons. This phenomenon is due to the internet's ubiquitous access, ease of use, and the extensive and ever expanding content. The problem is no longer the need to acquire content to encourage use, but to provide organizational tools to support content categorization that will facilitate improved access methods. This paper presents the results of a new text characterization algorithm that combines semantic and linguistic techniques utilizing domain-based ontology background knowledge. It explores the combination of meronym, synonym, and hypernym linguistic relationships to create a set of concept chains used to represent concepts found in a document. The experiments show improved accuracy over bag-of-words based term weighting methods and reveal characteristics of the meronym contribution to document representation.	algorithm;bag-of-words model in computer vision;categorization;experiment;usability	Lori Watrous-deVersterre;Chong Wang;Min Song	2012		10.1145/2232817.2232862	natural language processing;digital library;usability;computer science;bag-of-words model;data mining;cluster analysis;access method;world wide web;information retrieval	Web+IR	-25.319227833559722	-58.64752447867491	104322
3463e8935c8fbfd09bf64a1387efff4fa642cdef	a common theory of information fusion from multiple text sources step one: cross-document structure	summary length;user preference;common theory;multi-document summarization;cross-document structure theory;cross-document relationship;rhetorical structure;information provenance;related textual document;information fusion;multiple text sources step;cross-source agreement;multi-document analysis	We introduce CST (cross-document slructure theory), a paradigm for multidocument analysis. CST takes into aceount the rhetorical structure o f clusters of related textual documents. We present a taxonomy of cross-document relationships. We argue that CST can be the basis for multidocument summarization guided by user preferences for summary length, information provenance, cross-source agreement, and chronological ordering o f facts. 1 I n t r o d u c t i o n The Topic Detection and Tracking model (TDT) [Allan et al. 98] describes news events as they are reflected in news sources. First, many sources write on the same event and, second, the same source typically produces a number of accounts o f the event over a period o f time. Sixteen news stories related to the same event from six news sources over a two-hour time period are represented in Figure 1. i II I I I _ I ! I I, _ 1 I I _ I I I1! I i 06:30 06:45 07:00 07:15 07:30 07:45 08:00 08:15 08:30 Figure 1 : Time distribution of related documents from multiple sources A careful analysis of related news articles shows that they exhibit some interesting properties [Radev & McKeown 98]. In some cases, different sourees agree with each other, at other times, the information presented in them is contradictory. The same source sometimes adds new information as it becomes available and puts it in the context o f what has already been discussed earlier. In other eases, to get a full picture o f an event, one has to read stories from multiple sources as neither o f them presents all relevant information. All these examples point to the existence o f cross-document structure that is waiting to be exploited. Figure 2 illustrates how the same story can be told in several different ways. The six extracts are from news stories about the same event: the declaration by Bill Clinton at a press conference that millions o f dollars will be handed out to low income people affected by recent surges in oil pFices. In this paper we introduce CST, a theory o f cross-document structure. CST assigns labels	automatic summarization;declaration (computer programming);document;inferring horizontal gene transfer;modality (human–computer interaction);motorola i1;nick mckeown;parse tree;programming paradigm;taxonomy (general);theory;time-domain reflectometry;user (computing)	Dragomir R. Radev	2000			natural language processing;computer science;data mining;algorithm	ML	-24.703557719476645	-56.46608368649493	104480
28a84e4df86f1fe7f73a7faa4440fe2bb86b69b4	an agent based intelligent meta search engine	会议论文;personalized retrieval;agent;meta search engine	Addressing the problems that available search engines seldom consider the personalized needs of users with low precision rate and the discrete retrieval results, an Agent-based intelligent meta-search engine model is proposed. Agent technology is used, which makes the system more intelligent. In order to achieve personalized retrieval analysis, the model uses a four-tuple user interest model and improved text classification model. A retrieval result synthesis strategy is proposed based on the factors of initial positions, related degree of retrieval queries and abstracts, and weight of individual search engines. And result consistence sorting is also realized. The experimental results show that the proposed model has a preferably performance.		Qingshan Li;Yingcheng Sun	2012		10.1007/978-3-642-33469-6_71	search engine indexing;metasearch engine;computer science;data mining;database;world wide web;information retrieval;search engine;law of agency;human–computer information retrieval	Robotics	-29.57219960914928	-52.189371386765	104527
47a8a83b78dae41af1536eed2a3ee29704d643df	supporting the curation of twitter user lists	information network;network analysis;computers and society	Media outlets can now break or cover stories as they evolve by leveraging the content produced by users of social media sites (e.g. videos, photographs, tweets). However, significant issues arise when trying to (a) identify content around a breaking news story in a timely manner, (b) monitor the proliferation of content on a certain news event over a period of time, and (c) ensure that this content is reliable and accurate. Storyful1 is a social media news agency established in 2009 with the aim of filtering news, or newsworthy content, from the vast quantities of noisy data on social networks. To this end, Storyful invests considerable time into the manual curation of content on networks such as Twitter and YouTube. In some cases this involves identifying “gatekeepers” who are prolific in their ability to locate, filter, and monitor news from eyewitnesses.	clique graph;digital curation;emoticon;signal-to-noise ratio;social media;social network	Derek Greene;Fergal Reid;Gavin Sheridan;Padraig Cunningham	2011	CoRR		network analysis;computer science;data science;data mining;world wide web	Web+IR	-21.610264981472948	-54.04322662867034	104637
90a57281f71f2b70a46164e412165fa8a25ea4a9	incorporating paragraph embeddings and density peaks clustering for spoken document summarization	embedding;spoken document;pattern clustering document handling learning artificial intelligence natural language processing;redundancy spoken document summarization embedding relevance;summarization;redundancy;relevance;redundancy training context modeling predictive models context artificial neural networks;redundancy information paragraph embeddings density peaks clustering spoken document summarization representation learning machine learning application word embedding natural language processing extractive text speech summarization extractive summarization relevance information	Representation learning has emerged as a newly active research subject in many machine learning applications because of its excellent performance. As an instantiation, word embedding has been widely used in the natural language processing area. However, as far as we are aware, there are relatively few studies investigating paragraph embedding methods in extractive text or speech summarization. Extractive summarization aims at selecting a set of indicative sentences from a source document to express the most important theme of the document. There is a general consensus that relevance and redundancy are both critical issues for users in a realistic summarization scenario. However, most of the existing methods focus on determining only the relevance degree between sentences and a given document, while the redundancy degree is calculated by a post-processing step. Based on these observations, three contributions are proposed in this paper. First, we comprehensively compare the word and paragraph embedding methods for spoken document summarization. Next, we propose a novel summarization framework which can take both relevance and redundancy information into account simultaneously. Consequently, a set of representative sentences can be automatically selected through a one-pass process. Third, we further plug in paragraph embedding methods into the proposed framework to enhance the summarization performance. Experimental results demonstrate the effectiveness of our proposed methods, compared to existing state-of-the-art methods.	automatic summarization;cluster analysis;machine learning;natural language processing;part-of-speech tagging;redundancy (engineering);relevance;universal instantiation;video post-processing;word embedding	Kuan-Yu Chen;Kai-Wun Shih;Shih-Hung Liu;Berlin Chen;Hsin-Min Wang	2015	2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)	10.1109/ASRU.2015.7404796	natural language processing;speech recognition;relevance;multi-document summarization;computer science;automatic summarization;pattern recognition;embedding;redundancy	NLP	-25.410504599811794	-64.94210306004925	104654
a5a562c82b705ad2cb6a045e552de4479d62a857	a path-based approach for web page retrieval	navigation path;web information retrieval;web search	Use of links to enhance page ranking has been widely studied. The underlying assumption is that links convey recommendations. Although this technique has been used successfully in global web search, it produces poor results for website search, because the majority of the links in a website are used to organize information and convey no recommendations. By distinguishing these two kinds of links, respectively for recommendation and information organization, this paper describes a path-based method for web page ranking. We define the Hierarchical Navigation Path (HNP) as a new resource for improving web search. HNP is composed of multi-step navigation information in visitors’ website browsing. It provides indications of the content of the destination page. We first classify the links inside a website. Then, the links for web page organization are exploited to construct the HNPs for each page. Finally, the PathRank algorithm is described for web page retrieval. The experiments show that our approach results in significant improvements over existing solutions.	algorithm;anchor text;deep web;display resolution;experiment;knowledge organization;link analysis;pagerank;relevance;text retrieval conference;usb on-the-go;web page;web search engine	Jianqiang Li;Yu Zhao;Hector Garcia-Molina	2011	World Wide Web	10.1007/s11280-011-0133-5	backlink;deep linking;static web page;web development;web modeling;site map;web analytics;web design;web search engine;page view;computer science;web navigation;web page;data mining;printer-friendly;database;same-origin policy;world wide web;website parse template;information retrieval	Web+IR	-31.423649115047823	-52.76946395739891	104776
98991d3e7af7053d4eea7c11bfd0acc1ff1e6df6	collaboratively patching linked data		Today’s Web of Data is noisy. Linked Data often needs extensive preprocessing to enable efficient use of heterogeneous resources. While consistent and valid data provides the key to efficient data processing and aggregation we are facing two main challenges: (1st) Identification of erroneous facts and tracking their origins in dynamically connected datasets is a difficult task, and (2nd) efforts in the curation of deficient facts in Linked Data are exchanged rather rarely. Since erroneous data is often duplicated and (re-)distributed by mashup applications it is not only the responsibility of a few original publishers to keep their data tidy, but progresses to become a mission for all distributers and consumers of Linked Data, too. We present a new approach to expose and to reuse patches on erroneous data to enhance and to add quality information to the Web of Data. The feasibility of our approach is demonstrated in the example of a collaborative game that patches statements in DBpedia data and provides notifications for relevant changes.	dbpedia;digital curation;html tidy;linked data;mashup (web application hybrid);preprocessor;world wide web	Magnus Knuth;Johannes Hercher;Harald Sack	2012	CoRR		computer science;data mining;database;world wide web;information retrieval	ML	-30.595033127011927	-53.20091770793229	104868
80662b96957fa1108c396e20cf19dc41e0ed59c5	generative modeling of persons and documents for expert search	generic model;mixture of experts;probabilistic algorithm;expert finding;enterprise search;expertise;e mail;language model	In this paper we address the task of automatically finding an expert within the organization, known as the expert search problem. We present the theoretically-based probabilistic algorithm which models retrieved documents as mixtures of expert candidate language models. Experiments show that our approach outperforms existing theoretically sound solutions.	generative modelling language;language model;randomized algorithm;search problem	Pavel Serdyukov;Djoerd Hiemstra;Maarten M. Fokkinga;Peter M. G. Apers	2007		10.1145/1277741.1277929	natural language processing;legal expert system;computer science;data science;machine learning;data mining;randomized algorithm;language model	NLP	-29.823120318730464	-63.93854660951813	104955
26bcfffe1740d3fd8c88e60760e139d1f489af99	risk-reward trade-offs in rank fusion		Rank fusion is a powerful technique that merges multiple system runs to produce a single top-k list that often has much higher effectiveness than any single system can produce. Recently, there has been renewed interest in rank fusion in the IR community as these techniques can also be combined with query variations to produce highly effective runs. In this work, we comprehensively evaluate several state-of-the-art fusion algorithms in the context of risk. Like many re-ranking algorithms, there is a risk-reward trade-off in rank fusion, where improving the retrieval effectiveness for most queries often comes at the expense of others. Since system performance is usually compared using only aggregate scores for an evaluation metric, the risk is potentially obscured. In this work, we explore the use of the risk-based evaluation metrics over deep and shallow evaluation goals, and show that the risk-reward payoff in keyword queries can in fact be significantly improved when careful combinations of system and query variations are fused into a single run.	aggregate data;algorithm;elegant degradation;faceted classification;learning to rank;the australian;unsupervised learning	Rodger Benham;J. Shane Culpepper	2017		10.1145/3166072.3166084	data mining;computer science;machine learning;fusion;stochastic game;artificial intelligence;data structure	Web+IR	-26.56568543410038	-55.056660281874564	105010
496ce577a302c33bfc413f30c072b926be6447a6	characterizing and predicting bursty events: the buzz case study on twitter		The prediction of bursty events on the Internet is a challenging task. Difficulties are due to the diversity of information sources, the size of the Internet, dynamics of popularity, user behaviors. . . On the other hand, Twitter is a structured and limited space. In this paper, we present a new method for predicting bursty events using content-related indices. Prediction is performed by a neural network that combines three features in order to predict the number of retweets of a tweet on the Twitter platform. The indices are related to popularity, expressivity and singularity. Popularity index is based on the analysis of RSS streams. Expressivity uses a dictionary that contains words annotated in terms of expressivity load. Singularity represents outlying topic association estimated via a Latent Dirichlet Allocation (LDA) model. Experiments demonstrate the effectiveness of the proposal with a 72% F-measure prediction score for the tweets that have been forwarded at least 60 times.	artificial neural network;dictionary;expressive power (computer science);information source;internet;latent dirichlet allocation;rss;singularity project	Mohamed Morchid;Georges Linarès;Richard Dufour	2014			internet privacy;world wide web	Web+IR	-23.719109318108522	-52.409792650460055	105043
c5138a33de8949ccb23c8c1d75200cf1494c4eda	is my ontology matching system similar to yours?		The quality of the mappings computed by an ontology matching system in the Ontology Alignment Evaluation Initiative (OAEI) [2, 1] is typicallymeasured in terms of precision and recall with respect to a reference set of mappings. A dditionally, the OAEI also evaluates the coherence of the computed mappings [1]. However, the differences and similarities among the mappin gs computed by different systems have often been neglected in the OAEI. 1 In this paper we provide a more fine-grained comparison among the matching systems partici pa ng in the OAEI 2012 Large BioMed track; 2 concretely(i) we have harmonised (i.e. voted) the computed mapping sets, and(ii) we provide a graphical representation of the similarity of t hese sets.	ontology alignment;precision and recall	Ernesto Jiménez-Ruiz;Bernardo Cuenca Grau;Ian Horrocks	2013				AI	-32.193210722404004	-65.78292729119254	105050
a47f0b8bbeb34734ca4584ead35a96832d698435	identifying authoritative sources of multimedia content: mining specificity and expertise from large-scale multimedia databases	multimedia retrieval;source ranking;large scale;photo sharing;directed graph;multimedia database;image citation;image retrieval	"""We present a framework for identifying authoritative sources (such as web sites or individual users) that are likely to produce high-quality or interesting images. We construct a directed graph across sources based on the propensity of one source to """"cite"""" the content from another. A graph-centrality measure scores the authority for each source, which could then be applied for retrieval purposes. We apply this method to web image retrieval, where web sites are the sources, and citations are found via copy detection; and on a photo sharing site, where individuals are the sources and citations are users' favorites. We are able to identify primary or influential sources of media while avoiding the computational cost of other approaches."""	algorithmic efficiency;bookmark (world wide web);centrality;computation;database;directed graph;image retrieval;sensitivity and specificity	Lyndon Kennedy;Malcolm Slaney	2011		10.1145/2072298.2071975	directed graph;image retrieval;computer science;pattern recognition;multimedia;world wide web;information retrieval	Web+IR	-28.215958773968108	-54.5817043017528	105311
8800c032596302253486b2612cf87caaf004126d	a comparative evaluation of korean text categorization based on knn learning			categorization;document classification;k-nearest neighbors algorithm	Heui-Seok Lim	2002			natural language processing;categorization;computer science;artificial intelligence;pattern recognition	NLP	-21.278217688140835	-65.0250227678124	105312
b630ed5d9cacfafde2a36f1219adc403a1c14797	multi-session re-search: in pursuit of repetition and diversification	re search;query log analysis;web search	Search engine users regularly re-issue queries that are the same or similar to ones they have previously issued. In this paper we study this act of query re-issuing, called re-search, focusing on multi session re-searching from an information seeking perspective. By focusing on the series of repeat or similar queries where the user shows a continued interest, new patterns of behavior not previously seen arise. We find that the well-studied re-finding behavior is only a piece of the re-search puzzle, and that even amidst repeated re-findings users exhibit diversification and novelty seeking behaviours for many re-search queries. This suggests diversity and re-finding behaviors should be jointly modelled and captured in evaluation measures, instead of being studied as two separate problems as is seen in many previous approaches.	diversification (finance);information seeking;web search engine;web search query;whole earth 'lectronic link	Sarah K. Tyler;Yi Zhang	2012		10.1145/2396761.2398571	computer science;data mining;world wide web	Web+IR	-33.18732262988924	-53.28714354024958	105543
643d13fe4c84fb679487f4f4cb206e2bd77de848	clustering tagged documents with labeled and unlabeled documents	document clustering;text mining;tagged document clustering;semi supervised clustering	This study employs our proposed semi-supervised clustering method called Constrained-PLSA to cluster tagged documents with a small amount of labeled documents and uses two data sets for system performance evaluations. The first data set is a document set whose boundaries among the clusters are not clear; while the second one has clear boundaries among clusters. This study employs abstracts of papers and the tags annotated by users to cluster documents. Four combinations of tags and words are used for feature representations. The experimental results indicate that almost all of the methods can benefit from tags. However, unsupervised learning methods fail to function properly in the data set with noisy information, but Constrained-PLSA functions properly. In many real applications , background knowledge is ready, making it appropriate to employ background knowledge in the clustering process to make the learning more fast and effective. In the past few years, there has been an exponential growth in the number of social networking sites and the popularity of these web sites can be attributed to the changes brought by Web 2.0 in the way users interact with the Web. One of the important features of Web 2.0 is tagging, which allows users to annotate resources with descriptive words. The widespread use of social networking sites and tags have given rise to many interesting and challenging problems to the research community. One of the research topics is how to use tags to improve document analysis. Intuitively, social tags can provide more information than the words appearing in the content, since social tags are provided by users and are similar to the keywords of documents. Moreover, clustering can automatically group documents into meaningful categories, so clustering tagged documents has seen increasing attention recently This study focuses on tagged document clustering problem and discovers how to utilize tags in document analysis tasks. showed that tags by themselves are weak at partitioning blog data, since there are no techniques for specifying ''meaning'', inducing a hierarchy or inferring relationships between tags. This study analyzes and presents how tags improve document clustering by employing various combinations of tags and content words. Document clustering plays an important role in providing better document retrieval, since a good document clustering method can help machines automatically organize a document corpus into a meaningful cluster hierarchy, which enables an efficient browsing and navigation of the corpus. For instance, clustering can automatically build an ontology like …	blog;cluster analysis;document retrieval;performance;probabilistic latent semantic analysis;seeds (cellular automaton);semi-supervised learning;semiconductor industry;tag (metadata);text corpus;time complexity;unsupervised learning;web 2.0;world wide web	Chien-Liang Liu;Wen-Hoar Hsaio;Chia-Hoang Lee;Chun-Hsien Chen	2013	Inf. Process. Manage.	10.1016/j.ipm.2012.12.004	data stream clustering;text mining;document clustering;fuzzy clustering;flame clustering;computer science;pattern recognition;data mining;cluster analysis;world wide web;information retrieval;clustering high-dimensional data;conceptual clustering	Web+IR	-25.531642715745427	-58.59436693711555	105562
40a566a2581a3483301e2061bf86e168ef0e7831	bnaic 2016: artificial intelligence		We propose a system that assigns topical labels to automatically detected events in the Twitter stream. The automatic detection and labeling of events in social media streams is challenging due to the large number and variety of messages that are posted. The early detection of future social events, specifically those associated with civil unrest, has a wide applicability in areas such as security, e-governance, and journalism. We used machine learning algorithms and encoded the social media data using a wide range of features. Experiments show a high-precision (but low-recall) performance in the first step. We designed a second step that exploits classification probabilities, boosting the recall of our category of interest, social action events.	algorithm;artificial intelligence;categorization;computer performance;domain-specific language;e-governance;experiment;f1 score;lexicon;machine learning;naive bayes classifier;precision and recall;social media;unrest;web search engine	Tibor Bosse;Bert Bredeweg	2016		10.1007/978-3-319-67468-1	artificial intelligence;computer science	AI	-20.71162023478639	-58.36005461980775	105590
a045122129ec97b7e26add8e0d237a74e9370884	word similarity based model for tweet stream prospective notification		The prospective notification on tweet streams is a challenge task in which the user wishes to receive timely, relevant, and nonredundant update notification to remain up-to-date. To be effective the system attempts to optimize the aforementioned properties (timeliness, relevance, novelty and redundancy) and find a trade-off between pushing too many and pushing too few tweets. We propose an adaptation of the extended Boolean model based on word similarity to estimate the relevance score of tweets. We take advantage of the word2vec model to capture the similarity between query terms and tweet terms. Experiments on the TREC MB RTF 2015 dataset show that our approach outperforms all considered baselines.	megabyte;microsoft word for mac;norm (social);prospective search;relevance;streaming media;thresholding (image processing);word2vec	Abdelhamid Chellal;Mohand Boughanem;Bernard Dousset	2017		10.1007/978-3-319-56608-5_62	novelty;word2vec;redundancy (engineering);information retrieval;streams;data mining;computer science;extended boolean model	Web+IR	-26.39187512193538	-54.162631525786324	105607
b32a8a5e792f0c2aa3374c34580c2c4a99f36fd6	on the automatic identification of music for common activities		In this paper, we address the challenge of identifying music suitable to accompany typical daily activities. We first derive a list of common activities by analyzing social media data. Then, an automatic approach is proposed to find music for these activities. Our approach is inspired by our experimentally acquired findings (a) that genre and instrument information, i.e., as appearing in the textual metadata, are not sufficient to distinguish music appropriate for different types of activities, and (b) that existing content-based approaches in the music information retrieval community do not overcome this insufficiency. The main contributions of our work are (a) our analysis of the properties of activity-related music that inspire our use of novel high-level features, e.g., drop-like events, and (b) our approach's novel method of extracting and combining low-level features, and, in particular, the joint optimization of the time window for feature aggregation and the number of features to be used. The effectiveness of the approach method is demonstrated in a comprehensive experimental study including failure analysis.	automatic identification and data capture;convergence insufficiency;emoticon;experiment;failure analysis;feature extraction;high- and low-level;information retrieval;information source;linear programming relaxation;mathematical optimization;microsoft outlook for mac;pa-risc;population;recommender system;social media;timeline;upload;usability testing	Karthik Yadati;Cynthia C. S. Liem;Martha Larson;Alan Hanjalic	2017		10.1145/3078971.3078997	pop music automation;computer science;music information retrieval;metadata;multimedia;activities of daily living;social media	Web+IR	-24.815759115192925	-54.340697159095335	105661
305330594c912c27b2940c247bbc354d8d4b7fea	a topic-sensitive model for salient entity linking		In recent years, the amount of entities in large knowledge bases available on the Web has been increasing rapidly. Such entities can be used to bridge textual data with knowledge bases and thus help with many tasks, such as text understanding, word sense disambiguation and information retrieval. The key issue is to link the entity mentions in documents with the corresponding entities in knowledge bases, referred to as entity linking. In addition, for many entity-centric applications, entity salience for a document has become a very important factor. This raises an impending need to identify a set of salient entities that are central to the input document. In this paper, we introduce a new task of salient entity linking and propose a graph-based disambiguation solution, which integrates several features, especially a topic-sensitive model based on Wikipedia categories. Experimental results show that our method significantly outperforms the state-of-the-art entity linking methods in terms of precision, recall and F-measure.	baseline (configuration management);entity linking;experiment;f1 score;information retrieval;knowledge base;text corpus;wikipedia;word sense;word-sense disambiguation;world wide web	Lei Zhang;Cong Liu;Achim Rettinger	2015			weak entity	NLP	-27.356123209637392	-65.46162712004681	105825
7bc3d2fa8b8d9f9befd804475b3ba5371eb2be58	coin: a network analysis for document triage	ncku 成功大學 成大 圖書館 機構典藏;data mining;dissertations and theses journal referred papers conference papers nsc reserach report patent nckur ir ncku institutional repostiory 博碩士論文 期刊論文 國科會研究報告 專利 成大機構典藏;user computer interface;databases factual;peer review research;pubmed;documentation	In recent years, there was a rapid increase in the number of medical articles. The number of articles in PubMed has increased exponentially. Thus, the workload for biocurators has also increased exponentially. Under these circumstances, a system that can automatically determine in advance which article has a higher priority for curation can effectively reduce the workload of biocurators. Determining how to effectively find the articles required by biocurators has become an important task. In the triage task of BioCreative 2012, we proposed the Co-occurrence Interaction Nexus (CoIN) for learning and exploring relations in articles. We constructed a co-occurrence analysis system, which is applicable to PubMed articles and suitable for gene, chemical and disease queries. CoIN uses co-occurrence features and their network centralities to assess the influence of curatable articles from the Comparative Toxicogenomics Database. The experimental results show that our network-based approach combined with co-occurrence features can effectively classify curatable and non-curatable articles. CoIN also allows biocurators to survey the ranking lists for specific queries without reviewing meaningless information. At BioCreative 2012, CoIN achieved a 0.778 mean average precision in the triage task, thus finishing in second place out of all participants. Database URL: http://ikmbio.csie.ncku.edu.tw/coin/home.php.	automatic identification and data capture;biocreative;biocurator;co-occurrence networks;comparative toxicogenomics database (ctd);digital curation;experiment;genetic heterogeneity;information retrieval;name;named entity;named-entity recognition;nexus (resin cement);pubmed;review [publication type];social network analysis;supervised learning;test data;triage;uniform resource locator	Yi-Yu Hsu;Hung-Yu Kao	2013		10.1093/database/bat076	documentation;computer science;bioinformatics;data science;data mining;database;world wide web;information retrieval	Web+IR	-27.445895729668152	-59.63700633433202	105884
17ba9019ec7eb1f03d64f4d80e47376a4f6f8583	learning from the past: answering new questions with past answers	automatic question answering;query performance prediction;statistical model;data extraction;community based question answering;success rate;natural language processing;question answering	"""Community-based Question Answering sites, such as Yahoo! Answers or Baidu Zhidao, allow users to get answers to complex, detailed and personal questions from other users. However, since answering a question depends on the ability and willingness of users to address the asker's needs, a significant fraction of the questions remain unanswered. We measured that in Yahoo! Answers, this fraction represents 15% of all incoming English questions. At the same time, we discovered that around 25% of questions in certain categories are recurrent, at least at the question-title level, over a period of one year.  We attempt to reduce the rate of unanswered questions in Yahoo! Answers by reusing the large repository of past resolved questions, openly available on the site. More specifically, we estimate the probability whether certain new questions can be satisfactorily answered by a best answer from the past, using a statistical model specifically trained for this task. We leverage concepts and methods from query-performance prediction and natural language processing in order to extract a wide range of features for our model. The key challenge here is to achieve a level of quality similar to the one provided by the best human answerers.  We evaluated our algorithm on offline data extracted from Yahoo! Answers, but more interestingly, also on online data by using three """"live"""" answering robots that automatically provide past answers to new questions when a certain degree of confidence is reached. We report the success rate of these robots in three active Yahoo! Answers categories in terms of both accuracy, coverage and askers' satisfaction. This work presents a first attempt, to the best of our knowledge, of automatic question answering to questions of social nature, by reusing past answers of high quality."""	algorithm;display resolution;natural language processing;online and offline;performance prediction;question answering;recurrent neural network;robot;statistical model;yahoo! answers	Anna Shtok;Gideon Dror;Yoelle Maarek;Idan Szpektor	2012		10.1145/2187836.2187939	statistical model;question answering;computer science;data mining;world wide web;information retrieval	NLP	-31.85881205752311	-59.389907252116124	105897
ee44cec3c3408ea1077a9cbf877f9b03955f197b	semantic clustering-based deep hypergraph model for online reviews semantic classification in cyber-physical-social systems		Sentiment classification of online reviews is playing an increasingly important role for both consumers and businesses in cyber-physical-social systems. However, existing works ignore the semantic correlation among different reviews, causing the ineffectiveness for sentiment classification. In this paper, a word embedding clustering-based deep hypergraph model (ECDHG) is proposed for the sentiment analysis of online reviews. The ECDHG introduces external knowledge by employing the pre-training word embeddings to express reviews. Then, semantic units are detected under the supervision of semantic cliques discovered by an improved hierarchical fast clustering algorithm. Convolutional neural networks are connected to extract the high-order textual and semantic features of reviews. Finally, the hypergraph can be constructed based on high-order relations of samples for the sentiment classification of reviews. Experiments are performed on five-domain data sets including movie, book, DVD, kitchen, and electronic to assess the performance of the proposed model compared with other seven models. The results validate that our model outperforms the compared methods in classification accuracy.	algorithm;artificial neural network;cluster analysis;convolutional neural network;sentiment analysis;social system;word embedding	Xu Wei Yuan;Mingyang Sun;Zhikui Chen;Jing Gao;Peng Li	2018	IEEE Access	10.1109/ACCESS.2018.2813419	word embedding;convolutional neural network;feature extraction;machine learning;sentiment analysis;semantics;computer science;cluster analysis;statistical classification;distributed computing;cyber-physical system;artificial intelligence	NLP	-20.259802839431288	-57.87909578296607	105943
4c48f49b56cfd6c8e916191dd0bb24f4092625c3	research on semantic label extraction of domain entity relation based on crf and rules	conditional random fields;entity relation;feature template;semantic label	For the vast amounts of data on the Web, this paper presents an extraction method of semantic label of entity relation in the tourism domain based on the conditional random fields and rules. In this method, firstly making use of the ideas of classification in named entity recognition, semantic items reflecting entity relations are seen as semantic labels in the contextual information to be labeled, and identify the semantic label with CRF, then respectively according to the relative location information of the two entities and semantic label and rules, the semantic labels are assigned to the associated entities. The experimental results on the corpus in the field of tourism show that this method can reach the F-measure of 73.68%, indicating that the method is feasible and effective for semantic label extraction of entity relation.		Jianyi Guo;Jun Zhao;Zhengtao Yu;Lei Su;Nianshu Jiang	2012		10.1007/978-3-642-29426-6_19	semantic similarity;computer science;machine learning;pattern recognition;data mining;entity linking;semantic equivalence;conditional random field;weak entity;information retrieval	NLP	-27.640700285036267	-65.91178958042013	106001
4f2b319cb159eb2d38081d6b2b5151c3860212cf	detecting the correlation between sentiment and user-level as well as text-level meta-data from benchmark corpora		Do tweets from users with similar Twitter characteristics have similar sentiments? What meta-data features of tweets and users correlate with tweet sentiment? In this paper, we address these two questions by analyzing six popular benchmark datasets where tweets are annotated with sentiment labels. We consider user-level as well as tweet-level meta-data features, and identify patterns and correlations of these feature with the log-odds for sentiment classes. We further strengthen our analysis by replicating this set of experiments on recent tweets from users present in our datasets; finding that most of the patterns are consistent across our analysis. Finally, we use our identified meta-data features as features for a sentiment classification algorithm, which results in around 2% increase in F1 score for sentiment classification, compared to text-only classifiers, along with a significant drop in KL-divergence. These results have potential to improve sentiment analysis applications on social media data.	algorithm;benchmark (computing);experiment;f1 score;kullback–leibler divergence;sensor;sentiment analysis;social media;text corpus;text-based user interface;user space	Shubhanshu Mishra;Jana Diesner	2018		10.1145/3209542.3209562	sentiment analysis;data mining;computer science;world wide web;metadata;f1 score;social media	NLP	-20.911881679000388	-58.02799259440431	106186
433c25b12357c81956545d4b231a9fce7c45c72a	high level event identification in social media	google;vocabulary;data mining;integrated circuit modeling;facebook;mathematical model	The rapid growth of information technology along with variety of digital data generation provides an opportunity to better understand human dynamics. However, knowing and obtaining complete information about events and activities that happen, becomes a complicated task in Natural Language Processing (NLP) and location based social networks. In this research, we introduce a new approach to recognize events in social media. On the basis of the approach, we demonstrate an event explorer system which models event topics using a latent Dirichlet allocation (LDA) and classifies the events by various features. The preliminary result is introduced on certain web-sources. The study aims at improving automatic event recognition task from social media.	digital data;human dynamics;latent dirichlet allocation;natural language processing;social media;social network	Zolzaya Dashdorj;Battushig Tsogtbaatar;Altangerel Tumurchudur;Erdenebaatar Altangerel	2016	2016 12th International Conference on Semantics, Knowledge and Grids (SKG)	10.1109/SKG.2016.026	computer science;artificial intelligence;data science;complex event processing;machine learning;mathematical model;data mining;database;world wide web	NLP	-22.394357020954004	-56.8686627166659	106212
1c4cd0f51127b51315ee1b75f743b41a084a0285	tag recommendation for large-scale ontology-based information systems	tag recommendation;novel technique;traditional setting;available online;large-scale ontology-based information system;latent dirichlet allocation;idf weighting;traditional ir technique;ontological distance;best tag;sciencewise portal	We tackle the problem of improving the relevance of automatically selected tags in large-scale ontology-based information systems. Contrary to traditional settings where tags can be chosen arbitrarily, we focus on the problem of recommending tags (e.g., concepts) directly from a collaborative, user-driven ontology. We compare the effectiveness of a series of approaches to select the best tags ranging from traditional IR techniques such as TF/IDF weighting to novel techniques based on ontological distances and latent Dirichlet allocation. All our experiments are run against a real corpus of tags and documents extracted from the ScienceWise portal, which is connected to ArXiv.org and is currently used by growing number of researchers. The datasets for the experiments are made available online for reproducibility purposes.	experiment;information system;latent dirichlet allocation;relevance;scientific literature;tf–idf;word-sense disambiguation	Roman Prokofyev;Alexey Boyarsky;Oleg Ruchayskiy;Karl Aberer;Gianluca Demartini;Philippe Cudré-Mauroux	2012		10.1007/978-3-642-35173-0_22	computer science;data mining;database;world wide web;information retrieval	Web+IR	-29.654741189848036	-60.91847578968234	106334
47be99dc6be520fb825e3145a668ba87b0d6c50a	nonlinear evidence fusion and propagation for hyponymy relation mining	hyponymy relation mining;performance improvement;propagation algorithm;hypernym label;evidence combination;million web page;result quality;open-domain web document;nonlinear evidence fusion;nonlinear probabilistic model	This paper focuses on mining the hyponymy (or is-a) relation from large-scale, open-domain web documents. A nonlinear probabilistic model is exploited to model the correlation between sentences in the aggregation of pattern matching results. Based on the model, we design a set of evidence combination and propagation algorithms. These significantly improve the result quality of existing approaches. Experimental results conducted on 500 million web pages and hypernym labels for 300 terms show over 20% performance improvement in terms of P@5, MAP and R-Precision.	approximation algorithm;data quality;information extraction;is-a;lexico;map;nonlinear system;pattern matching;software propagation;statistical model;web page	Fan Zhang;Shuming Shi;Jing Liu;Shu-Qi Sun;Chin-Yew Lin	2011			data science;machine learning;data mining;information retrieval	ML	-27.949458735023292	-62.215923323573215	106369
0829a361bf8701abf585ea7115bc254653359bb4	automatic document classification part ii. additional experiments	information retrieval;data processing;classification;factor analysis;document classification;automation	This study reports the results of a series of experiments in the techniques of automatic document classification. Two different classification schedules are compared along with two methods of automatically classifying documents into categories. It is concluded that, while there is no significant difference in the predictive efficiency between the Bayesian and the Factor Score methods, automatic document classification is enhanced by the use of a factor-analytically-derived classification schedule. Approximately 55 percent of the document were automatically and correctly classified.	document classification;experiment;library classification	Harold Borko;Myrna Bernick	1964	J. ACM	10.1145/321217.321219	document clustering;data processing;biological classification;computer science;automation;pattern recognition;data mining;factor analysis;one-class classification;information retrieval	Web+IR	-21.201177130915614	-64.19322333518326	106517
30593ad852efa6b0f7b21c0413041f9ae9d06e51	the effect of wording on message propagation: topic- and author-controlled natural experiments on twitter		Consider a person trying to spread an important message on a social network. He/she can spend hours trying to craft the message. Does it actually matter? While there has been extensive prior work looking into predicting popularity of socialmedia content, the effect of wording per se has rarely been studied since it is often confounded with the popularity of the author and the topic. To control for these confounding factors, we take advantage of the surprising fact that there are many pairs of tweets containing the same url and written by the same user but employing different wording. Given such pairs, we ask: which version attracts more retweets? This turns out to be a more difficult task than predicting popular topics. Still, humans can answer this question better than chance (but far from perfectly), and the computational methods we develop can do better than both an average human and a strong competing method trained on noncontrolled data.	algorithm;align (company);computation;experiment;heuristic (computer science);human reliability;hypertext transfer protocol;social network;software propagation	Chenhao Tan;Lillian Lee;Bo Pang	2014			computer science;world wide web	NLP	-20.54400179186337	-54.076395560021005	106531
9e4e313d6f10610f958c21e6838156c7353d8039	examining the information retrieval process from an inductive perspective	empirical analysis;information retrieval;information retrieval model;term weighting;indexation;models;peak effect;qa76 computer software	Term-weighting functions derived from various models of retrieval aim to model human notions of relevance more accurately. However, there is a lack of analysis of the sources of evidence from which important features of these term weighting schemes originate. In general, features pertaining to these term-weighting schemes can be collected from (1) the document, (2) the entire collection and (3) the query. In this work, we perform an empirical analysis to determine the increase in effectiveness as information from these three different sources becomes more accurate.  First, we determine the number of documents to be indexed to accurately estimate collection-wide features to obtain near optimal effectiveness for a range of a term-weighting functions. Similarly, we determine the amount of a document and query that must be sampled to achieve near-peak effectiveness. This analysis also allows us to determine the factors that contribute most to the performance of a term-weighting function (i.e. the document, the collection or the query).  We use our framework to construct a new model of weighting where we discard the 'bag of words' model and aim to retrieve documents based on the initial physical representation of a document using some basic axioms of retrieval. We show that this is a good first step towards incorporating some more interesting features into a term-weighting function	bag-of-words model;document;information retrieval;relevance;weight function	Ronan Cummins;Mounia Lalmas;Colm O'Riordan	2010		10.1145/1871437.1871453	query expansion;ranking;computer science;data mining;database;tf–idf;world wide web;vector space model;information retrieval	Web+IR	-31.914338337584343	-60.24875717486052	106667
2707eec441c4fcbb5ef098e0ab478cb2bc0ffa83	ontocrawler: a focused crawler with ontology-supported website models for information agents	search engine;website models;information agent shell;focused crawlers;information agent	This paper proposed the use of ontology-supported website models to provide a semantic level solution for an information agent so that it can provide fast, precise, and stable query results. We have based on the technique to develop a focused crawler, namely, OntoCrawler which can benefit both user requests and domain semantics. The technique in this research has practically applied on Google and Yahoo searching engines to actively search for webpages of related information, and the experiment outcomes indicated that this technique could definitely up-rise precision rate and recall rate of webpage query. Equipped with this technique, we have developed an ontology-supported information agent shell in Scholar domain which manifests the following interesting features: ontology-supported construction of website models, website models-supported website model expansion, website models-supported webpage retrieval, high-level outcomes of information recommendation, and accordingly proved the feasibility of the related techniques proposed in this paper. 2010 Elsevier Ltd. All rights reserved.	application programming interface;categorization;chen–ho encoding;database;entity–relationship model;experiment;focused crawler;high- and low-level;intelligent user interface;java class library;list of java apis;ontology (information science);portals;precision and recall;sensitivity and specificity;sourceforge;stemming;tf–idf;web crawler;web page;web search engine;word lists by frequency;wordnet	Sheng-Yuan Yang	2010	Expert Syst. Appl.	10.1016/j.eswa.2010.01.018	website architecture;computer science;internet privacy;world wide web;information retrieval;search engine	AI	-30.621591413364566	-55.07475654893333	106803
aeabfa4a073c4b1b0e99f08fe6fd7111bbed2fd1	use of dl for shallow information analysis for texts	information analysis			Nicolas Capponi	1997			information retrieval;computer science	NLP	-31.074245761724224	-57.941384106765966	106861
9e1fff42d2fd700f6b7e215fb4542f3250e06a5c	boosting a semantic search engine by named entities	search engine;information retrieval;semantic search;bag of words;named entity;entity relationship	Traditional Information Retrieval (IR) systems are based on bag-of-words representation. This approach retrieves relevant documents by lexical matching between query and document terms. Due to synonymy and polysemy, lexical methods produce imprecise or incomplete results. In this paper we present SENSE (SEmantic N-levels Search Engine), an IR system that tries to overcome the limitations of the ranked keyword approach, by introducing semantic levels which integrate (and not simply replace) the lexical level represented by keywords. Semantic levels provide information about word meanings, as described in a reference dictionary, and named entities. This paper focuses on the named entity level. Our aim is to prove that named entities are useful to improve retrieval performance. We exploit a model able to capture entity relationships, although they are not explicit in documents text. Experiments on CLEF dataset prove the effectiveness of our hypothesis.	bag-of-words model;boosting (machine learning);dictionary;experiment;information retrieval;named entity;relevance feedback;semantic search;sense;synonym ring;web services for devices;web search engine	Annalina Caputo;Pierpaolo Basile;Giovanni Semeraro	2009		10.1007/978-3-642-04125-9_27	natural language processing;entity–relationship model;semantic search;computer science;bag-of-words model;concept search;data mining;entity linking;information retrieval;search engine	Web+IR	-27.69446080198231	-65.76422137553305	106955
619e9486c8769f6c80ae60ee52b15da154787fb9	a study on liwc categories for opinion mining in spanish reviews	opinion mining;intelligence artificielle;logique en informatique;apprentissage;machine learning;informatique et langage;sentiment analysis;natural language processing with liwc	With the exponential growth of social media i.e. blogs and social networks, organizations and individual persons are increasingly using the number of reviews of these media for decision making about a product or service. Opinion mining detects whether the emotions of an opinion expressed by a user on Web platforms in natural language, is positive or negative. This paper presents extensive experiments to study the effectiveness of the classification of Spanish opinions in five categories: highly positive, highly negative, positive, negative and neutral, using the combination of the psychological and linguistic features of LIWC. LIWC is a text analysis software that enables the extraction of different psychological and linguistic features from natural language text. For this study, two corpora have been used, one about movies and one about technological products. Furthermore, we have conducted a comparative assessment of the performance of various classification techniques: J48, SMO and BayesNet, using precision, recall and F-measure metrics. All in all, findings have revealed that the positive and negative categories provide better results than the other categories. Finally, experiments on both corpora indicated that SMO produces better results than BayesNet and J48 algorithms, obtaining an F-measure of 90.4% and 87.2% in each domain.	algorithm;blog;experiment;f1 score;information science;lexicon;natural language;ontology (information science);probabilistic latent semantic analysis;sentiment analysis;sequential minimal optimization;social media;social network;symantec endpoint protection;text corpus;time complexity;transistor;vocabulary;weka;word-sense disambiguation	María del Pilar Salas-Zárate;Estanislao López-López;Rafael Valencia-García;Nathalie Aussenac-Gilles;Ángela Almela;Giner Alor-Hernández	2014	J. Information Science	10.1177/0165551514547842	natural language processing;speech recognition;computer science;data mining;database;linguistics;world wide web;information retrieval;sentiment analysis	NLP	-22.1935889778936	-65.16154649164717	106957
a1b415d88e67a3416d784742bb4f095c97ffc125	a critical review of migrating parallel web crawler		The size of the internet is very large and it has grown enormously, search engines are the tools for World Wide Web navigation. In order to provide powerful search facilities, search engines maintain comprehensive indices for documents and their contents on the Web by continuously downloading Web pages for processing, known as web crawling. In this paper we reviewed various web crawlers and their performance attributes. We study mobile and parallel web crawling approach that makes web crawling system more effective and efficient. The major advantage of the mobile approach is that the analysis portion of the crawling process is done locally where the data resides rather than remotely inside the Web search engine. This can significantly reduce net- work load which, in turn, can improve the performance of the crawling process. The major advantage of parallel crawling is that as the size of the Web grows, it becomes imperative to parallelize a crawling process, in order to finish downloading pages in a reasonable amount of time. We identify fundamental issues related to migrating parallel crawling and also propose metrics to evaluate a migrating parallel crawler. Lastly, we summarize the web crawlers and their performance attributes that effects the process of web crawling.	web crawler	Md. Faizan Farooqui;Md. Rizwan Beg;Md. Qasim Rafiq	2012		10.1007/978-3-642-31552-7_63	database;distributed computing;world wide web	DB	-30.07545825143573	-54.61497077392078	107045
cd0e4671b75326369164932f594e968c67641437	patent evaluation based on technological trajectory revealed in relevant prior patents		It is a challenging task for firms to assess the importance of a patent and identify valuable patents as early as possible. Counting the number of citations received is a widely used method to assess the value of a patent. However, recently granted patents have few citations received, which makes the use of citation counts infeasible. In this paper, we propose a novel idea to evaluate the value of new or recently granted patents using recommended relevant prior patents. Our approach is to exploit trends in temporal patterns of relevant prior patents, which are highly related to patent values. We evaluate the proposed approach using two patent value evaluation tasks with a large-scale collection of U.S. patents. Experimental results show that the models created based on our idea significantly enhance those using the baseline features or patent backward citations.		Sooyoung Oh;Zhen Lei;Wang-Chien Lee;John Yen	2014		10.1007/978-3-319-06608-0_45	data mining	HCI	-32.80931149930486	-58.46817071647075	107070
ed45052ab4ab9c63cdffab08e64c0cf60bce86f6	extracting knowledge about cognitive style	analytical models;cognitive style;testing out of order context analytical models;text mining;user sensory preference knowledge extraction user cognitive style sensory vocabulary text mining approach psycholinguistic research internet natural language processing sensory expression;behavioural sciences computing;user interfaces behavioural sciences computing cognition data mining knowledge acquisition text analysis;text analysis;testing;data mining;user modeling;out of order;lexical analysis cognitive style sensory preference user modeling text mining;knowledge acquisition;cognition;sensory system;user interfaces;natural language processing;context;lexical analysis;analytical model;sensory preference;user model	According to psycholinguistic research, any text contains a lot of implicit information about its writer. The Internet provides an incredible amount of text produced by users. The information potential of texts that can be directly linked to a user (which is especially the case for forum and blog posts) is not yet sufficiently examined. Natural language processing techniques are in some cases used for getting information on the user's personality, moods, affects and sentiments, but not yet for getting information on the user's cognitive style concerning sensory preference. This paper explores the potential of such an idea. Therefore, a corpus with more than 1.000.000 forum posts was analyzed for the occurrence of expressions that are directly linked to a sensory system. We found that users differ significantly in their use of sensory expressions and that most users have preferred patterns for the use of sensory expressions. Furthermore we found a correlation between the sensory vocabulary of a post and the sensory preference of the users who answered this post.	blog;data acquisition;information processing;internet;microsoft outlook for mac;natural language processing;regular expression;s-expression;text corpus;user modeling;vocabulary	Gudrun Kellner;Bettina Berendt	2011	2011 7th International Conference on Natural Language Processing and Knowledge Engineering	10.1109/NLPKE.2011.6138172	natural language processing;computer science;multimedia;communication	HCI	-23.093127954679108	-60.09895478689198	107103
f319a754f071aae0d35711670b534de43cf5e226	sentiment analysis of sub-events extracted out of an event using word2vec		Word2vec is an assortment of related models specially employed to yield word embeddings. By its application to a relatively large dataset that corresponds to a given event coming about at a given point of time at a given location, we can break down the event into sub-events, and study them further. Investigating sub-events in the right direction can help us in countless ways. It can enable us to decipher their local yet inevitable impacts which might otherwise have gone missing in the sea of the whole event altogether. In our paper, we have broken down the event (of the happenings of 'Kashmir') into sub-events and pulled out a few randomly. We have then applied sentiment-analysis to each one of them instead of applying it on to the whole event all at once. The rise and fall of the sentiment with respect to each sub-event is plotted and the variation is visualised in the end. The procedure is not just limited to our domain of interest but can be adopted to study any event.	randomness;sentiment analysis;unrest;volume rendering;word embedding;word2vec	Bettahally N. Keshavamurthy;Shashank Prakash Srivastava;Jaseel Haris;Ankush Kumar;Seema Wazarkar	2018	2018 IEEE 18th International Conference on Advanced Learning Technologies (ICALT)	10.1109/ICALT.2018.00105	multimedia;data mining;word2vec;visualization;sentiment analysis;semantics;computer science;context model	SE	-23.29328013110147	-55.88740131842093	107765
c044dee8c973151408ea5d4a00639498c5ba4ca7	on the evaluation of tweet timeline generation task		Tweet Timeline Generation (TTG) task aims to generate a timeline of relevant but novel tweets that summarizes the development of a given topic. A typical TTG system first retrieves tweets then detects novel tweets among them to form a timeline. In this paper, we examine the dependency of TTG on retrieval quality, and its effect on having biased evaluation. Our study showed a considerable dependency, however, ranking systems is not highly affected if a common retrieval run is used.	timeline	Walid Magdy;Tamer Elsayed;Maram Hasanain	2016		10.1007/978-3-319-30671-1_48	multimedia;internet privacy;world wide web	AI	-26.70881480148809	-53.300879718574215	107769
05c7e7c2b907864dbb2ebca85052f266dae13110	analyzing social media content for security informatics	content management;graph theory;classification algorithms security media training algorithm design and analysis standards accuracy;population health social media content analysis security informatics public opinion sentiment estimation emotion estimation web based analysis text classification documents bipartite graph word bipartite graph sentiment emotion classifier semisupervised learning graph transduction sentiment analysis online consumer product reviews suicide bombing bombing event frequency vaccination;security informatics;text analysis;sentiment;internet;machine learning;emotion;social networking online;pattern classification;learning artificial intelligence;security;social media;text analysis content management graph theory internet learning artificial intelligence pattern classification security social networking online;security informatics text analysis social media sentiment emotion machine learning	Inferring public opinion regarding an issue or event by analyzing social media content is of great interest to security analysts but is also technically challenging to accomplish. This paper presents a new method for estimating sentiment and/or emotion expressed in social media which addresses the challenges associated with Web-based analysis. We formulate the problem as one of text classification, model the data as a bipartite graph of documents and words, and construct the sentiment/emotion classifier through a combination of semi-supervised learning and graph transduction. Interestingly, the proposed approach requires no labeled training documents and is able to provides accurate text classification using only a small lexicon of words of known sentiment/ emotion. The classification algorithm is shown to outperform state of the art methods on a benchmark task involving sentiment analysis of online consumer product reviews. We illustrate the utility of the approach for security informatics through two case studies, one examining the possibility that online sentiment about suicide bombing predicts bombing event frequency, and one investigating public sentiment about vaccination and its implications for population health and security.	algorithm;benchmark (computing);document classification;informatics;lexicon;semi-supervised learning;semiconductor industry;sentiment analysis;social media;supervised learning;transduction (machine learning)	Richard Colbaugh;Kristin Glass	2013	2013 European Intelligence and Security Informatics Conference	10.1109/EISIC.2013.14	social media;emotion;content management;computer science;artificial intelligence;information security;graph theory;data science;machine learning;data mining;world wide web;computer security;sentiment analysis	Web+IR	-20.87052147587911	-55.98368303274175	107790
8e1ad45cb441399758fdc45896c63be25f1f964f	comparing events coverage in online news and social media: the case of climate change	news values;mainstream media;news coverage;climate change;content analysis;online news;news bias;social media	Social media is becoming more and more integrated in the distribution and consumption of news. How is news in social media different from mainstream news? This paper presents a comparative analysis covering a span of 17 months and hundreds of news events, using a method that combines automatic and manual annotations. We focus on climate change, a topic that is frequently present in the news through a number of arguments, from current practices and causes (e.g. fracking, CO2 emissions) to consequences and solutions (e.g. extreme weather, electric cars). The coverage that these different aspects receive is often dependent on how they are framed—typically by mainstream media. Yet, evidence suggests an existing gap between what the news media publishes online and what the general public shares in social media. Through the analysis of a series of events, including awareness campaigns, natural disasters, governmental meetings and publications, among others, we uncover differences in terms of the triggers, actions, and news values that are prevalent in both types of media. This methodology can be extended to other important topics present in the news.	database trigger;qualitative comparative analysis;social media	Alexandra Olteanu;Carlos Castillo;Nicholas Diakopoulos;Karl Aberer	2015			media relations;social media;content analysis;computer science;internet privacy;news values;climate change	Web+IR	-23.246773458426997	-54.419707029787794	107794
402b6851cd0cfda73ab8b1fcf1f2e21431b81b34	word sense disambiguation method based on improved mutual information with wikipedia extend	wikipedia;dependency parse tree;semantics;context feature extraction encyclopedias electronic publishing internet semantics;improved mutual information wsd dependency parse tree rules wikipedia;internet;feature extraction;chinese word sense disambiguation ambiguous word sense disambiguation method improved mutual information wikipedia extend dependency parse tree feature word selection feature word expansion ambiguous context words;wsd;electronic publishing;encyclopedias;web sites feature selection grammars natural language processing trees mathematics;improved mutual information;context;rules	In view of word sense disambiguation shortcomings of the previous methods, they generally do not consider on word distance for computing semantic correlation of the influence of context, as well as the context is limited for ambiguous word sense disambiguation, and the use of part ambiguous context words make word senses more ambiguous. Therefore, this paper proposes the use of dependency parse tree and rules for feature word selection, then the ambiguous word is mapped to the Wikipedia pages to expand the feature words of the ambiguous word. Feature words expansion of word sense and feature words will eliminate the limitation of context words, by calculating improved mutual information between feature words of ambiguous context words senses and feature word of ambiguous word senses, then finally obtain the most suitable items of ambiguous word with context words in this sentence. Experimental results show that the proposed method compares with the previous method improves the accuracy of Chinese word sense disambiguation of 11.4%, with good scalability and practicality.	dependency grammar;mutual information;parse tree;parsing;scalability;wikipedia;word sense;word-sense disambiguation	Feiyue Ye;Yulong Zhu	2015	2015 IEEE 12th International Conference on e-Business Engineering	10.1109/ICEBE.2015.22	natural language processing;the internet;semeval;feature extraction;computer science;pattern recognition;brand;semantics;word lists by frequency;electronic publishing;law;information retrieval;encyclopedia	NLP	-25.73610482576431	-64.92036692544916	107937
0b6fd9ce0528e2c662e36b46c143fb94733eb349	yoda system for wmt16 shared task: bilingual document alignment		In this paper, we address the task of automatically aligning/detecting the bilingual documents that are translations of each other from a single web-domain as part of WMT 2016. 1 Given the large amounts of data available in each web-domain, a brute force approach like finding similarities between every possible pair is a computationally expensive operation. Therefore, we start with a simple approach on matching just the web page urls after some pre-processing to reduce the number of possible pairings to a small extent. This simple approach obtained a recall of 50% and the exact matches from this approach are removed from further consideration. We built on top of this using an n-gram based approach that uses the partial English translations of French web pages and achieved a recall of 93.71% on the training pairs provided. We also outline an IR-based approach that uses both content and the meta data of each web page url, thereby obtaining a recall of 56.31%. Our final submission to this shared task using n-gram based approach achieved a recall of 93.92%.	analysis of algorithms;brute-force search;html;heuristic (computer science);lexical analysis;n-gram;preprocessor;sensor;web page	Aswarth Abhilash Dara;Yiu-Chang Lin	2016			computer science;database;world wide web;information retrieval	NLP	-31.420004249726976	-65.20811060683582	107954
e8e82fed794b8090bd1ec47c5f9562152171cd07	semantically annotating ceur-ws workshop proceedings with rml	mappings;semantic publishing challenge;ceur ws;technology and engineering;rml	In this paper, we present our solution for the first task of the second Semantic Publishing Challenge. The task requires extracting and semantically annotating information regarding CEUR-WS workshops, their chairs and conference affiliations, as well as their papers and their authors, from a set of HTML-encoded workshop proceedings volumes. Our solution builds on last year's submission, while we address a number of shortcomings, assess the generated dataset for its quality and publish the queries as SPARQL query templates. This is accomplished using the RDF Mapping Language (RML) to define the mappings, the RMLPROCESSOR to execute them, the RDFUnit to both validate the mapping documents and assess the generated dataset's quality, and the DATATANK to publish the SPARQL query templates. This results in an overall improved quality of the generated dataset that is reflected in the query results.		Pieter Heyvaert;Anastasia Dimou;Ruben Verborgh;Erik Mannens;Rik Van de Walle	2015		10.1007/978-3-319-25518-7_14	computer science;sparql;data mining;database;information retrieval	HPC	-30.87813973849829	-63.58606033352901	107993
012dce7d112fdf07b784d19180d04186d5328ef5	predicting email recipients	community detection;method of moments;electronic mail;history;connectors;postal services;business;multiplex network analysis;multiplex network metrics;computer science;community evaluation	"""The ability to accurately predict recipients of an email, while it is being composed, is of great practical importance for two reasons. First, prediction of recipients allows for effective """"auto-complete"""" of this field, thereby improving user experience and reducing the overhead of manual typing of the recipient. Second, this capability allows the system to alert the user when she has typed unlikely recipients. Such alerts can help avoid human error that might result in forgetting relevant recipients, or, even worse, disclosure of personal or classified information.  In this demonstration, a system that effectively predicts email recipients, given an email history, will be exhibited. The system takes into consideration a variety of email related features to achieve high accuracy. Extensive experimentation on diverse email corpora has shown that our system adapts well to a variety of domains (such as business, personal and political email). Conference participants will be able to view real emails sent, and to observe how well our system predicted the recipients. In addition, they will be able to """"impersonate"""" users whose email history is already available to the system, to compose a new email, and to view the recipient predictions."""	email;human error;overhead (computing);text corpus;user experience	Zvi Sofershtein;Sara Cohen	2015	2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)	10.1145/2808797.2808805	method of moments;html email;computer science;data mining;mathematics;internet privacy;opt-in email;world wide web;computer security;email authentication;statistics	Visualization	-20.8008580751855	-54.379454565912	108000
06705021064cf3dbf54cce17f921eb6182190535	journal cross-citation analysis for validation and improvement of journal-based subject classification in bibliometric research	cluster algorithm;citation analysis;journal;reference systems;cluster analysis;community networks;subject classification;mapping of science;cognitive structure;sista;journal cross citation	The objective of this study is to use a clustering algorithm based on journal cross-citation to validate and to improve the journal-based subject classification schemes. The cognitive structure based on the clustering is visualized by the journal cross-citation network and three kinds of representative journals in each cluster among the communication network have been detected and analyzed. As an existing reference system the 15-field subject classification by Glänzel and Schubert (Scientometrics 56:55–73, 2003) has been compared with the clustering structure.	algorithm;bibliometrics;citation analysis;citation network;cluster analysis;journal citation reports;scientometrics;telecommunications network	Lin Zhang;Frizo A. L. Janssens;Liming Liang;Wolfgang Glänzel	2010	Scientometrics	10.1007/s11192-010-0180-1	computer science;data science;data mining;cluster analysis;citation analysis;world wide web;information retrieval	Web+IR	-26.038432169479446	-58.084733211761524	108003
b0804992277822ca1038bd0ef9647f3197ac5734	ontology based comparative sentence and relation mining for sentiment classification		Due to the rapid expansion of the internet, business through e-commerce has become popular. Many products are being sold on the internet and the merchants selling the products ask their customers to write reviews about the products that they have purchased. Opinion mining and sentiment classification are not only technically challenging because of the need for natural language processing, but also very useful in practice. In this study, ontology based compararive sentence and relation mining for sentiment classification in mobile phone (product) reviews are studied. POS taggers are used to tag sentiment words in the input sentences. In this study, Naive Bayes classifier is also used for sentiment classification. Moreover, the comparison between with ontology and without ontology are aiso described. This study is very useful for manufacturers and customers in E-commerce Sites, Review Sites, Blog etc.		Myat Su Wai;May Aye Chan Aung;Than Nwe Aung	2015		10.1007/978-3-319-23207-2_45	pattern recognition;data mining;information retrieval	NLP	-23.200743037328305	-57.20103594667313	108044
39ee1023420bdaa877484bb5a49367bb7257f9d2	temporal recommendation, long- and short-term preferences, session-based temporal graph, time weight content-based graph, time-averaged hit ratio, pagerank, injected preference fusion			pagerank	Armel Jacques Nzekon Nzeko'o;Maurice Tchuente;Matthieu Latapy	2017		10.5220/0006288202680275	multimedia;world wide web;information retrieval	AI	-26.634137482638604	-54.09583432767915	108063
c1867cace312aa54c5b089570289d4edc4c2cbd2	a decision support system: automated crime report analysis and classification for e-government	measurement;e government;classification;similarity measures;algorithms;natural language processing	Abstract This paper investigates how text analysis and classification techniques can be used to enhance e-government, typically law enforcement agenciesu0027 efficiency and effectiveness by analyzing text reports automatically and provide timely supporting information to decision makers. With an increasing number of anonymous crime reports being filed and digitized, it is generally difficult for crime analysts to process and analyze crime reports efficiently. Complicating the problem is that the information has not been filtered or guided in a detective-led interview resulting in much irrelevant information. We are developing a decision support system (DSS), combining natural language processing (NLP) techniques, similarity measures, and machine learning, i.e., a Naive Bayesu0027 classifier, to support crime analysis and classify which crime reports discuss the same and different crime. We report on an algorithm essential to the DSS and its evaluations. Two studies with small and big datasets were conducted to compare the system with a human expertu0027s performance. The first study includes 10 sets of crime reports discussing 2 to 5 crimes. The highest algorithm accuracy was found by using binary logistic regression (89%) while Naive Bayesu0027 classifier was only slightly lower (87%). The expert achieved still better performance (96%) when given sufficient time. The second study includes two datasets with 40 and 60 crime reports discussing 16 different types of crimes for each dataset. The results show that our system achieved the highest classification accuracy (94.82%), while the crime analystu0027s classification accuracy (93.74%) is slightly lower.	decision support system;e-government	Chih Hao Ku;Gondy Leroy	2014	Government Information Quarterly	10.1016/j.giq.2014.08.003	e-government;biological classification;computer science;data science;data mining;world wide web;computer security;measurement;statistics	SE	-20.894894473225843	-57.25993092225704	108135
0262a9834fe2999c27aee7a86aa03841f070e913	a layout-independent web news article contents extraction method based on relevance analysis	content extraction;search engine;rss feeds;news extraction	The traditional Web news article contents extraction methods are time-costly and need much maintenance because they analyze the layout of news pages to generate the wrappers manually or automatically. In this paper, we propose a relevance-based analysis method to extract the news article contents from the news pages without the analysis of news page layouts before extraction. This method is applicable to the general news pages and we give the implementations of news extraction from different kinds of news sources.		Hao Han;Takehiro Tokuda	2009		10.1007/978-3-642-02818-2_37	computer science;rss;multimedia;world wide web;information retrieval;search engine;news aggregator	NLP	-28.852829012449682	-56.00095640091386	108139
faf6de40c6ba0a08b630da85447a206ae8f9b579	umass at trec 2003: hard and qa	information retrieval;words language;symposia;accuracy;mathematical models;heuristic methods;clustering;passage retrieval;documents;entropy;query expansion;search theory	In the final analysis, all runs using metadata or clarification forms failed to outperform our best baseline run. We interpret this as an indictment of the track and of our effort. As with most new TREC tracks, the HARD track was slow to get started, had problems being clearly defined, and had poor training data. In addition, several engineering bottlenecks delayed our initial work and prevented us from moving as rapidly as we had originally intended. The rest of this section discusses what we did. We first describe the baseline runs that we generated for comparison. The same section presents the mechanism behind our passage retrieval runs. In Section 1.3 we discuss the types of clarification forms that we used and, in Section 1.4, how we used the responses. We outline how we used query and document metadata in Section 1.5 and how it was incorporated into the ranking in Section 1.6. We discuss our results in Section 1.8.	baseline (configuration management);software quality assurance	Nasreen Abdul Jaleel;Andrés Corrada-Emmanuel;Qi Li;Xiaoyong Liu;Courtney Wade;James Allan	2003			search theory;natural language processing;entropy;query expansion;computer science;mathematical model;data mining;database;accuracy and precision;cluster analysis;world wide web;information retrieval	NLP	-31.259006738382716	-63.33794972138484	108229
d7aa2c04b42e18ff9002d7a2deb1e009137eee78	sqr: a semantic query rating scheme	query interpretation;logic;semantic search	We introduce a query rating scheme that identifies the possible interpretations which can be assigned to a semantic query. The interpretations range from the traditional bag-of-words interpretation to more context- and semantic-aware interpretations. The aims of this scheme are to communicate the extent of semantics that is being interpreted for a query and to assign suitable query processing methods for each level of interpretation accordingly.	bag-of-words model;database;query language;sqr;semantic query	Hany Azzam;Thomas Roelleke	2010		10.1145/1871962.1871976	natural language processing;sargable;query optimization;query expansion;web query classification;boolean conjunctive query;computer science;query by example;database;rdf query language;web search query;information retrieval;query language	DB	-32.81345306660476	-60.69197893090698	108350
cea93ac31c2976daf59dbf91476b46a9caaabc09	how joe and jane tweet about their health: mining for personal health information on twitter		With 19%–28% of Internet users participating in online health discussions, it became imperative to be able to detect and analyze posted personal health information (PHI). In this work we introduce two semantic-based methods for mining PHI on social networks which will warn the users about potential privacy breaches. One method uses WordNet as a source of health-related knowledge, another an ontology of personal relations. We use Twitter data to empirically evaluate our methods. We also apply Machine Learning to demonstrate advantages of our extraction procedure when tweets containing PHI have to be automatically identified among other tweets.	approximation;baseline (configuration management);imperative programming;jane (software);joe's own editor;machine learning;programming in the large and programming in the small;social network;wordnet	Marina Sokolova;Stan Matwin;Yasser Jafer;David Schramm	2013			computer science;data mining;internet privacy;world wide web	NLP	-21.234254686961844	-56.13571480372213	108468
e26169ae5032e668c94640e2a96470b663781a1d	tutorial: identifying malicious actors on social media		Online social media platforms are severely compromised by the existence of malicious actors such as bots on Twitter, vandals on Wikipedia, fake accounts on Facebook, trolls on Twitter and Slashdot, and spammers who seem to be omnipresent. This tutorial presents methods to identify malicious actors in at least 4 settings: Twitter, Facebook, Slashdot, and Wikipedia. We will look at 4 broad categories of methods: (i) network based techniques where the structure of the social network is used, (ii) text based methods where the linguistic content of posts is examined, (iii) behavior-based methods which study actions of users, and (iv) real-time processes which enable defenders of social media to keep a step ahead of malicious actors. The tutorial will identify commonly used features for classifying actors into malicious vs. benign and will give a brief explanation of different algorithms both specific to social platforms and general algorithms that are platform neutral.	algorithm;real-time web;slashdot;social media;social network;spamming;text-based (computing);wikipedia	Srijan Kumar;Francesca Spezzano;V. S. Subrahmanian	2016			internet privacy;computer science;social media	Web+IR	-20.18227524824694	-55.02511468495184	108664
4334f7dad2d6d1753e24affde1360470c3014e2a	"""lca-based keyword search for effectively retrieving """"information unit"""" from web pages"""	unsolicited electronic mail;pediatrics;web pages;search engines;information unit retrieval;information retrieval;conference management;tree data structures;web search engine;internet technology;lca;internet;keyword search;data structures;information unit;web sites;next generation;clustering algorithms;web search;lca based keyword search;relational databases;subtrees;web search engines;algorithm design and analysis;structured data	"""With the rapid development of the Internet technology, the structured data are more and more prevalent in the Internet. Moreover, most Web sites organize their data systematically and relevant data may be separated into different pages but linked through hyperlinks. However, the existing Web search engines cannot integrate information from multiple interrelated pages to answer keyword queries meaningfully. Next-generation web search engines require link-awareness, or more generally, the capability of integrating correlative information items that are linked through hyperlinks. In this paper, we study the problems of identifying the """"information unit"""" of relevant pages containing all the input keywords as the answer. We model a set of most related Web pages as a tree, where the nodes in the tree are the web pages and the edges are the links between the Web pages. We retrieve the """"information unit"""" of the most related and connected subtrees instead of single Web page as the answer. To improve the search efficiency, we propose an effective LCA-based algorithm to identify those subtrees which are most related to the given input keywords. We have conducted a set of extensive experiments on the proposed algorithm. The experimental results show that our method achieves high search performance and outperforms the existing alternative methods significantly."""	algorithm;experiment;hyperlink;internet;link awareness;tree (data structure);web page;web search engine;world wide web	Xiaoming Song;Jianhua Feng;Guoliang Li;Qin Hong	2008	2008 The Ninth International Conference on Web-Age Information Management	10.1109/WAIM.2008.15	algorithm design;static web page;site map;web query classification;the internet;web mapping;web search engine;data model;relational database;computer science;web crawler;web page;data mining;printer-friendly;database;cluster analysis;tree;hits algorithm;world wide web;information retrieval;search engine	DB	-29.710411079054282	-53.259939320638374	108668
c6b0c2adc98e2bd672f053dbd409e373b7a70d62	a new method of parameter estimation for multinomial naive bayes text classifiers	document model;development;naive bayes;text classification;naive bayes classifier;visualisation;analysis;parameter estimation;topic tracking	Multinomial naive Bayes classifiers have been widely used for the probabilistic text classification. However, their parameter estimation method sometimes generates inappropriate probabilities. In this paper, we propose a topic document model approach for naive Bayes text classification, where their parameters are estimated with an expectation from the training documents. Experiments are conducted on Reuters 21578 and 20 Newsgroup collection, and our proposed approach obtained a significant improvement in performace over the conventional approach.	document classification;estimation theory;experiment;naive bayes classifier;newton's method	Sang-Bum Kim;Hae-Chang Rim;Heui-Seok Lim	2002		10.1145/564376.564459	bayes classifier;naive bayes classifier;bayesian programming;computer science;machine learning;pattern recognition;data mining;bayes error rate	Web+IR	-21.248663510329482	-63.52453669455705	108701
ebfa6ac2b9b78263d1576c02c783b7a7777f0925	titan: a system for effective web service discovery	web service description language;search engine;web service discovery;wsdl;web service;tag;clustering	With the increase of web services and user demand's diversity, effective web service discovery is becoming a big challenge. Clustering web services would greatly boost the ability of web service search engine to retrieve relevant ones. In this paper, we propose a web service search engine Titan which contains 15,969 web services crawled from the Internet. In Titan, two main technologies, i.e., web service clustering and tag recommendation, are employed to improve the effectiveness of web service discovery. Specifically, both WSDL (Web Service Description Language) documents and tags of web services are utilized for clustering, while tag recommendation is adopted to handle some inherent problems of tagging data, e.g., uneven tag distribution and noise tags.	cluster analysis;service discovery;titan;web services description language;web search engine;web service	Jian Wu;Liang Chen;Yanan Xie;Zibin Zheng	2012		10.1145/2187980.2188069	web service;web application security;static web page;web development;web modeling;data web;web analytics;web mapping;web design;web search engine;web standards;computer science;ws-policy;web navigation;web page;database;cluster analysis;ws-i basic profile;web 2.0;law;world wide web;information retrieval;universal description discovery and integration;search engine;web coverage service	Web+IR	-28.622905985655077	-57.3738680916814	108705
68c16c43a7ba55faa0d142d7aea9cd8518ad783e	learning to rank for information retrieval (lr4ir 2009)	information retrieval;machine learning;learning to rank		information retrieval;learning to rank	Hang Li;Tie-Yan Liu;ChengXiang Zhai	2009	SIGIR Forum	10.1145/1670564.1670571		Web+IR	-25.15020996946725	-61.52201438781462	108849
c21fb11bd1e61edd64732740dfd130a27d8570c5	a comparison of centralized and distributed information retrieval approaches	optimal quality;electronic mail;dir;approximation algorithms;search engines;information retrieval;federated search distributed information retrieval;usa councils;distributed information retrieval;internet;optimal quality centralized information retrieval distributed information retrieval dir www world wide web;merging;federated search;world wide web;internet information retrieval;centralized information retrieval;merging search engines information retrieval usa councils partitioning algorithms approximation algorithms electronic mail;www;partitioning algorithms	Distributed Information Retrieval (DIR) has been suggested to offer a prospective solution to a number of issues concerning information retrieval in the WWW. On the other hand, previous studies have indicated that centralized approaches offer the best solution for optimal quality of result (i.e. effectiveness). In this paper, we revisit those claims and investigate if and under which conditions can DIR offer a new paradigm for both efficient and effective information retrieval.	centralized computing;download;experiment;information retrieval;programming paradigm;prospective search;selection algorithm;www	Georgios Paltoglou;Michail Salampasis;Maria Satratzemi	2008	2008 Panhellenic Conference on Informatics	10.1109/PCI.2008.18	the internet;computer science;data mining;database;world wide web;approximation algorithm;information retrieval;search engine;human–computer information retrieval	Web+IR	-30.34263366051379	-56.11754464935288	108998
b82dccf3709e1e761e2581ed4a85b7202e4d280a	enterprise information access and the user experience	web sites internet query processing search engines;web search system enterprise information access user experience digitally stored information information retrieval document retrieval query processing;query processing;search engines;information retrieval;query refinement;information access;aggregation;faceted search;dynamic clustering;visualization;internet;enterprise information access;visualization enterprise information access query refinement faceted search dynamic clustering aggregation treemap;information retrieval lifting equipment project management libraries content management pressing web search feedback control systems natural languages;user experience;treemap;web sites;web search	Today, digitally stored information isn't only ubiquitous, it's also increasing in volume at an exponential rate. And not only is the volume increasing, but so is the variety, as well as the ways of combining information from different sources to derive insights. Not surprisingly, our most pressing technological and business problem is finding what we need in this sea of information. The dominant paradigm for addressing this problem is information retrieval (Modem Information Retrieval, Ricardo Baeza-Yates and Berthier Ribeiro-Neto, ACM Press, 1999). In this paradigm, the user enters a query (typically a few words typed into a search box), and the system retrieves documents matching the query, ranking the matches based on an estimate of their relevancy to the query. If the system finds many matches, the user sees only the highest-ranked matches. The popularity of Web search systems such as Google shows that the information retrieval paradigm can be effective. An information access framework empowers users by explicitly focusing on the interaction between users and the system. The key problem for information access systems isn't guessing which matching document is most relevant, but establishing a dialogue in which users progressively communicate their information goals while the system provides immediate, incremental feedback that guides users in the pursuit of those goals	enterprise information access;information retrieval;modem;programming paradigm;relevance;time complexity;user experience;web search engine	Frederick Knabe;Daniel Tunkelang	2007	IT Professional	10.1109/MITP.2007.8	treemapping;user experience design;query expansion;the internet;ranking;visualization;cognitive models of information retrieval;computer science;database;world wide web;information retrieval;human–computer information retrieval	Web+IR	-31.055428632443572	-53.061673011666336	109226
b71a458ef1058dee187411941289fca22c84d158	topicdsdr: combining topic decomposition and data reconstruction for summarization	k l divergence;data reconstruction;multi document summarization;lda	Multi-document summarization attempts to select the most important information to generate a compressed summary from a collection of documents. From the perspective of data reconstruction, a good summary may also well reconstruct the original documents. A document generally contains a variety of information centered around a main topic and covers different aspects of the main topic. In this paper we propose a novel model that combines data reconstruction and topic decomposition to summarize the documents, named TopicDSDR, which can not only best reconstruct the original documents but also capture the semantic similarity and main topics. We discuss two kinds of reconstructions: linear reconstruction and nonnegative reconstruction. We use the generalized Kullback-Leibler(KL) divergence as the loss function to evaluate the quality of summary for linear and nonnegative reconstruction and develop two new algorithms respectively. We conduct experiments on DUC2006 and DUC2007 summarization data sets, the experimental results demonstrate the effectiveness of our proposed methods.	algorithm;automatic summarization;experiment;kl-one;kullback–leibler divergence;loss function;mathematical optimization;multi-document summarization;semantic similarity	Zhiming Zhang;Hongjie Li;Lian'en Huang	2013		10.1007/978-3-642-38562-9_35	multi-document summarization;computer science;automatic summarization;pattern recognition;data mining;information retrieval	AI	-26.20330604944211	-62.49721446807815	109348
04110329013c84bd8af787424b11fb765dcda976	cross-lingual topic discovery from multilingual search engine query log	probabilistic topic model;search engine;query log	Today, major commercial search engines are operating in a multinational fashion to provide web search services for millions of users who compose search queries by different languages. Hence, the search engine query log, which serves as the backbone of many search engine applications, records millions of users’ search history in a wide spectrum of human languages and demonstrates a strong multilingual phenomenon. However, with its salience, the multilingual nature of a search engine query log is usually ignored by existing works, which usually consider query log entries of different languages as being orthogonal and independent. This kind of oversimplified assumption heavily distorts the underlying structure of web search data. In this article, we pioneer in recognition of the multilingual nature of a query log and make the first attempt to cross the language barrier in query logs. We propose a novel model named Cross-Lingual Query Log Topic Model (CL-QLTM) to analyze query logs from a cross-lingual perspective and derive the latent topics of web search data. The CL-QLTM comprehensively integrates web search data in different languages by collectively utilizing cross-lingual dictionaries, as well as the co-occurrence relations in the query log. In order to relieve the efficiency bottleneck of applying the CL-QLTM on voluminous query logs, we propose an efficient parameter inference algorithm based on the MapReduce computing paradigm. Both qualitative and quantitative experimental results show that the CL-QLTM is able to effectively derive cross-lingual topics from multilingual query logs and spawn a wide spectrum of new search engine applications.	acm transactions on information systems;algorithm;clozure cl;dictionary;distortion;downstream (software development);estimation theory;internet backbone;lol;mapreduce;programming paradigm;real life;spawn (computing);topic model;vocabulary;web search engine;web search query;world wide web	Di Jiang;Yongxin Tong;Yuanfeng Song	2016	ACM Trans. Inf. Syst.	10.1145/2956235	search-oriented architecture;sargable;query optimization;query expansion;web query classification;ranking;computer science;database;rdf query language;search analytics;web search query;world wide web;information retrieval;query language;search engine	Web+IR	-23.610857280662426	-62.55988131028241	109376
4f79cd402c11a5bbfd1c55be69a667fad5fad7cc	exna: an efficient search pattern for search engines		As the Web enters Big Data age, users and search engines may find it more and more difficult to effectively use and manage such big data. To solve this problem, this paper presents a novel search pattern named ExNa by defining its model and basic operations in detail, which can help to design new type of search engines. To validate the ExNa search pattern, we develop a prototype news search engine named KNOWLE, which shows that KNOWLE equipped with ExNa can improve the efficiency of search system.	learning to rank;web search engine	Xiao Wei;Xiangfeng Luo;Qing Li;Jun Zhang	2014		10.1007/978-3-319-08010-9_74	beam search;search engine indexing;semantic search;data mining;incremental heuristic search;search analytics;world wide web;information retrieval;search engine	Crypto	-29.62165229874917	-54.56125784155235	109389
8eb76629b9e9e6189dd31c94b67fd4af35b1767b	an enhanced em method of semi-supervised classification based on naive bayesian	unlabeled data;category information;classification algorithm;feature terms;algorithm efficiency;iteration process;intermediate classifier;bayes methods;naive bayesian;supervised classification;bayesian methods;text analysis;semi supervised learning;bayesian method;maximum posterior category probability;feature vector;accuracy;machine learning;expectation maximization;automatic text classification;feature selection function;automatic text classification expectation maximization method enhancement semisupervised learning naive bayesian feature selection function feature vector intermediate classifier maximum posterior category probability iteration process macro average accuracy algorithm efficiency category information feature terms;classification algorithms;text analysis bayes methods expectation maximisation algorithm learning artificial intelligence;mathematical model;enhanced em;feature selection;learning artificial intelligence;macro average accuracy;classification accuracy;semi supervised classification;em algorithm;expectation maximization method enhancement;bayesian methods classification algorithms accuracy machine learning text categorization educational institutions mathematical model;text categorization;semisupervised learning;naive bayesian semi supervised classification feature selection enhanced em;expectation maximisation algorithm	Semi-supervised learning (SSL) based on Naïve Bayesian and Expectation Maximization (EM) combines small limited numbers of labeled data with a large amount of unlabeled data to help train classifier and increase classification accuracy. With the aim of improving the efficiency problem of the basic EM algorithm, an enhanced EM method is proposed. Firstly, a feature selection function of strong category information is constructed to control the dimension of feature vector and preserve useful feature terms. Secondly, an intermediate classifier gradually transfers unlabeled documents of maximum posterior category probability to labeled collection during each iteration process of the EM algorithm. The iteration number of the enhanced EM is obviously less than the basic EM. Finally, experiments shows that the improved method obtains very effective performance in terms of macro average accuracy and algorithm efficiency.	algorithmic efficiency;expectation–maximization algorithm;experiment;feature selection;feature vector;information theory;iteration;naive bayes classifier;semi-supervised learning;semiconductor industry;statistical classification;supervised learning	Han Wen;Nanfeng Xiao;Zhao Li	2011	2011 Eighth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)	10.1109/FSKD.2011.6019690	expectation–maximization algorithm;bayesian probability;computer science;machine learning;pattern recognition;data mining;feature selection	ML	-20.164142794314692	-64.33407334533865	109619
012e692b248065d7456b721340b86a1ed4c702a3	who is the barbecue king of texas?: a geo-spatial approach to finding local experts on twitter	expert finding;local expert;social tagging;twitter;crowdsourcing	This paper addresses the problem of identifying local experts in social media systems like Twitter. Local experts -- in contrast to general topic experts -- have specialized knowledge focused around a particular location, and are important for many applications including answering local information needs and interacting with community experts. And yet identifying these experts is difficult. Hence in this paper, we propose a geo-spatial-driven approach for identifying local experts that leverages the fine-grained GPS coordinates of millions of Twitter users. We propose a local expertise framework that integrates both users' topical expertise and their local authority. Concretely, we estimate a user's local authority via a novel spatial proximity expertise approach that leverages over 15 million geo-tagged Twitter lists. We estimate a user's topical expertise based on expertise propagation over 600 million geo-tagged social connections on Twitter. We evaluate the proposed approach across 56 queries coupled with over 11,000 individual judgments from Amazon Mechanical Turk. We find significant improvement over both general (non-local) expert approaches and comparable local expert finding approaches.	amazon mechanical turk;embedded system;geographic coordinate system;global positioning system;ibm notes;information needs;information retrieval;interaction;prototype;smartphone;social media;software propagation;spatial reference system;the turk;time complexity	Zhiyuan Cheng;James Caverlee;Himanshu Barthwal;Vandana Bachani	2014		10.1145/2600428.2609580	computer science;data science;data mining;world wide web;crowdsourcing	Web+IR	-24.535885823476256	-52.966034519097086	109889
5e0032a9587af824c8473c4448f95e52fd9f131c	introduction to the special issue on bibliometric-enhanced information retrieval and natural language processing for digital libraries (birndl)		The large scale of scholarly publications poses a challenge for scholars in information seeking and sensemaking. Bibliometric, information retrieval (IR), text mining, and natural language processing techniques can assist to address this challenge, but have yet to be widely used in digital libraries (DL). This special issue on bibliometric-enhanced information retrieval and natural language processing for digital libraries (BIRNDL) was compiled after the first joint BIRNDL workshop that was held at the joint conference on digital libraries (JCDL 2016) in Newark, New Jersey, USA. It brought together IR and DL researchers and professionals to elaborate on new approaches in natural language processing, information retrieval, scientometric, and recommendation techniques that can advance the state of the art in scholarly document understanding, analysis, and retrieval at scale. This special issue includes 14 papers: four extended papers originating from the first BIRNDL workshop 2016 and the BIR workshop at ECIR 2016, four extended system reports of the CL-SciSumm Shared Task 2016 and six original research papers submitted via the open call for papers.	bibliometrics;compiler;digital library;european conference on information retrieval;information seeking;joint conference on digital libraries;library (computing);natural language processing;scientometrics;sensemaking;text mining	Philipp Mayr;Ingo Frommholz;Guillaume Cabanac;Muthu Kumar Chandrasekaran;Kokil Jaidka;Min-Yen Kan;Dietmar Wolfram	2017	International Journal on Digital Libraries	10.1007/s00799-017-0230-x	information extraction;universal networking language;human–computer information retrieval;computer science;information retrieval;computational linguistics;natural language processing;digital library;question answering;information seeking;artificial intelligence;text mining	Web+IR	-31.861817132261063	-63.286263865356986	109930
c3f5b688661ffe6845a92e932cd3f4f414393f77	using learned browsing behavior models to recommend relevant web pages	web pages;behavior modeling;user study;satisfiability;machine learning;web mining;information need	We introduce our research on learning browsing behavior models for inferring a user’s information need (corresponding to a set of words) based on the actions he has taken during his current web session. This information is then used to find relevant pages, from essentially anywhere on the web. The models, learned from over one hundred users during a fiveweek user study, are session-specific but independent of both the user and website. Our empirical results suggest that these models can identify and satisfy the current information needs of users, even if they browse previously unseen pages containing unfamiliar words.	baseline (configuration management);browsing;c4.5 algorithm;information needs;personalization;recommender system;usability testing;web search engine;world wide web	Tingshao Zhu;Russell Greiner;Gerald Häubl;Kevin Jewell;Robert Price	2005			behavioral modeling;web service;information needs;web mining;static web page;web modeling;computer science;machine learning;web navigation;web page;data mining;client-side scripting;world wide web;website parse template;information retrieval;web server;satisfiability	Web+IR	-33.31878418090064	-52.16977224806306	110299
109f6f045ee61582f8e02622e64ae8285faf927c	are popular documents more likely to be relevant? a dive into the aclia ir4qa pools		The ACLIA IR4QA Task at NTCIR-7 is an ad hoc document retrieval task involving three document languages. Although IR4QA used pooling for collecting relevance assessments, it was unique in that the pooled documents were sorted before presenting them to the assessors, based on the assumption that “popular” documents are more likely to be relevant than others. We show that this assumption is indeed valid for the IR4QA test collections.	document retrieval;hoc (programming language);relevance	Tetsuya Sakai;Noriko Kando	2008			computer science;data mining;database;information retrieval	Web+IR	-30.69688680438069	-62.47581710980482	110446
998615a1692eaebb1c3416fe051e12c4d7a23592	exploiting sequential relationships for familial classification	market research;crf;classification;family history;machine learning;genealogy;natural language processing	The pervasive nature of the internet has caused a significant transformation in the field of genealogical research. This has impacted not only how research is conducted, but has also dramatically increased the number of people discovering their family history. Recent market research (Maritz Marketing 2000, Harris Interactive 2009) indicates that general interest in the United States has increased from 45% in 1996, to 60% in 2000, and 87% in 2009. Increased popularity has caused a dramatic need for improvements in algorithms related to extracting, accessing, and processing genealogical data for use in building family trees.  This paper presents one approach to algorithmic improvement in the family history domain, where we infer the familial relationships of households found in human transcribed United States census data. By applying advances made in natural language processing, exploiting the sequential nature of the census, and using state of the art machine learning algorithms, we were able to decrease the error by 35% over a hand coded baseline system. The resulting system is immediately applicable to hundreds of millions of other genealogical records where families are represented, but the familial relationships are missing.	algorithm;baseline (configuration management);family tree;harris affine region detector;interactivity;internet;machine learning;natural language processing;pervasive informatics	Lee S. Jensen;James G. Shanahan	2010		10.1145/1871437.1871759	market research;natural language processing;biological classification;computer science;artificial intelligence;machine learning;data mining;database;world wide web;statistics	ML	-20.42615079713951	-59.10163751202624	110559
4b996490d8aaeab15b2cafe668494f11d50bb0c7	keyphrase extraction with sequential pattern mining		Existing studies show that extracting a complete keyphrase candidate set is the first and crucial step to extract high quality keyphrases from documents. Based on a common sense that words do not repeatedly appear in an effective keyphrase, we propose a novel algorithm named KCSP for document-specific keyphrase candidate search using sequential pattern mining with gap constraints, which only needs to scan a document once and automatically specifies appropriate gap constraints for words without users’ participation. The experimental results confirm that it helps improve the quality of keyphrase extraction.	algorithm;data mining;display resolution;keyword extraction;sequential pattern mining	Qingren Wang;Victor S. Sheng;Xindong Wu	2017			computer science;artificial intelligence;machine learning;sequential pattern mining	AI	-27.27983637198931	-64.62811785686358	110641
e0d3df110f4b1fb68e69ac52fd54327eb27b6d67	aligning wordnet synsets and wikipedia articles	wikipedia;knowledge bases;ontologies;wordnet	This paper examines the problem of finding articles in Wikipedia to match noun synsets in WordNet. The motivation is that these articles enrich the synsets with much more information than is already present in WordNet. Two methods are used. The first is title matching, following redirects and disambiguation links. The second is information retrieval over the set of articles. The methods are evaluated over a random sample set of 200 noun synsets which were manually annotated. With 10 candidate articles retrieved for each noun synset, the methods achieve recall of 93%. The manually annotated data set and the automatically generated candidate article sets are available online for research purposes.	information retrieval;synonym ring;wikipedia;word-sense disambiguation;wordnet	Samuel Fernando;Mark Stevenson	2010			natural language processing;wordnet;extended wordnet;computer science;ontology;artificial intelligence;data mining;brand;information retrieval	NLP	-29.236868121297665	-64.74730075502455	110684
13ef953226069cb009f48f90c93a3a2e2ce28629	topic cube: topic modeling for olap on multidimensional text databases	data cube;heuristic method;data model;text database;em algorithm;structured data	As the amount of textual information grows explosively in various kinds of business systems, it becomes more and more desirable to analyze both structured data records and unstructured text data simultaneously. While online analytical processing (OLAP) techniques have been proven very useful for analyzing and mining structured data, they face challenges in handling text data. On the other hand, probabilistic topic models are among the most effective approaches to latent topic analysis and mining on text data. In this paper, we propose a new data model called topic cube to combine OLAP with probabilistic topic modeling and enable OLAP on the dimension of text data in a multidimensional text database. Topic cube extends the traditional data cube to cope with a topic hierarchy and store probabilistic content measures of text documents learned through a probabilistic topic model. To materialize topic cubes efficiently, we propose a heuristic method to speed up the iterative EM algorithm for estimating topic models by leveraging the models learned on component data cells to choose a good starting point for iteration. Experiment results show that this heuristic method is much faster than the baseline method of computing each topic cube from scratch. We also discuss potential uses of topic cube and show sample experimental results.	baseline (configuration management);data cube;data mining;data model;expectation–maximization algorithm;heuristic;heuristic (computer science);iteration;olap cube;off topic;online analytical processing;outline (list);probabilistic latent semantic analysis;text corpus;text mining;topic model	Duo Zhang;ChengXiang Zhai;Jiawei Han	2009		10.1137/1.9781611972795.96	data model;online analytical processing;computer science;data mining;database;information retrieval	DB	-22.823862386908807	-61.950669949010745	110775
12e1805ff484ce6d18d0d0212294353b15611ef5	the emotion recognition from uyghur sentences based on combination of class discriminating words and sentiment dictionary	class discriminating word;emotion recognition;sentiment dictionary;uyghur;sentence sentiment	Recently, emotion recognition has become a hot research topic in Natural Language Processing. Feature selection (FS) is a key process in Uyghur text emotion recognition, which will directly affect the accuracy of text emotion recognition. In this paper presents a recognition method for Uyghur sentence sentiments, such as angry, happy, sadness and wonder, based on combining Class-Discriminating Words (CDW) and sentiment dictionary. First, based on the characteristics of sentiment expression in Uyghur sentence text, features are extracted by using CDW feature selection method, and are used to emotion recognition. Second, some emotional words are collected by manually, and are built a sentiment dictionary, then combined with CDW feature words re-used to emotion recognition. The experimental results show that the method is effective in Uyghur text sentence emotion recognition. Therefore, it verifies the validity of this method.	dictionary;emotion recognition;feature selection;natural language processing;sadness	Abdusalam Dawut;Hussein Yusuf;Askar Hamdulla	2014	The 9th International Symposium on Chinese Spoken Language Processing	10.1109/ISCSLP.2014.6936643	natural language processing;speech recognition;pattern recognition	NLP	-23.2104393778138	-66.07404399015789	111058
76ed1b12f2ca36255a729e6c69e7d36e04b2b1d7	template extraction from candidate template set generation: a structure and content approach	template extraction;structure similarity;table informativeness;levels of abstraction;candidate template set;structural similarity	This paper introduces a new approach of webpage template extraction. Unlike traditional methods which concern only content information, this paper considers both structure and content similarity. It uses natural table structure as content units instead of text blocks or pagelets. This paper novelly and formally defines the templates and other concepts. It introduces a new concept, candidate template, which is an intermediate level of abstract table structure. A candidate template only covers the most informative tables, and abstracts a large page set with similar structures. This paper proposes a novel approach of template extraction by solving three sub problems surrounding candidate template set. The involving of candidate template set solves the accuracy and efficiency problems of traditional approaches. This paper also introduces a new model for structural similarity, and for table informativeness based on six heuristics.	heuristic (computer science);information;page (computer memory);structural similarity;web page	Hang Su;Qiaozhu Mei	2005		10.1145/1167253.1167303	template method pattern;computer science;bioinformatics;data mining;database	AI	-29.494884951726032	-65.89986420367102	111063
ca3856fa3bc3bf0b1ccf01b65d0e90faa739badb	exploiting redundancy in natural language to penetrate bayesian spam filters	different spam probability;bayesian filter;similar spam probability;different word;spam message;bayesian spam filter;spam mail;different user;similar token;natural language;exploiting redundancy;similar term	Today’s attacks against Bayesian spam filters attempt to keep the content of spam mails visible to humans, but obscured to filters. A common technique is to fool filters by appending additional words to a spam mail. Because these words appear very rarely in spam mails, filters are inclined to classify the mail as legitimate. The idea we present in this paper leverages the fact that natural language typically contains synonyms. Synonyms are different words that describe similar terms and concepts. Such words often have significantly different spam probabilities. Thus, an attacker might be able to penetrate Bayesian filters by replacing suspicious words by innocuous terms with the same meaning. A precondition for the success of such an attack is that Bayesian spam filters of different users assign similar spam probabilities to similar tokens. We first examine whether this precondition is met; afterwards, we measure the effectivity of an automated substitution attack by creating a test set of spam messages that are tested against SpamAssassin, DSPAM, and Gmail.	bayesian network;dspam;email filtering;gmail;natural language;precondition;spamassassin;spamming;test set	Christoph Karlberger;Günther Bayler;Christopher Krügel;Engin Kirda	2007			computer science;bag-of-words model;data mining;internet privacy;world wide web	Security	-19.48979147275544	-58.41339434545856	111264
55394564381e29326b3adf858a78416f12d2b25c	on performance of topical opinion retrieval	settore inf 01 informatica;monte carlo sampling;classification;relevance ranking;evaluation measure;sentiment analysis;opinion retrieval	We investigate the effectiveness of both the standard evaluation measures and the opinion component for topical opinion retrieval. We analyze how relevance is affected by opinions by perturbing relevance ranking by the outcomes of opinion-only classifiers built by Monte Carlo sampling. Topical opinion rankings are obtained by either re-ranking or filtering the documents of a first-pass retrieval of topic relevance.  The proposed approach establishes the correlation between the accuracy and the precision of the classifier and the performance of the topical opinion retrieval. Among other results, it is possible to assess the effectiveness of the opinion component by comparing the effectiveness of the relevance baseline with the topical opinion ranking.	baseline (configuration management);content-control software;monte carlo method;relevance;sampling (signal processing)	Gianni Amati;Giuseppe Amodeo;Valerio Capozio;Carlo Gaibisso;Giorgio Gambosi	2010		10.1145/1835449.1835611	biological classification;computer science;data science;data mining;information retrieval;sentiment analysis;statistics;monte carlo method	Web+IR	-26.200771652791076	-60.976115954925746	111401
a41785c506acac5612efc67592f1bf4eac41ad6d	supporting temporal analytics for health-related events in microblogs	event detection;time series analysis;twitter;disease outbreaks	Microblogging services, such as Twitter, are gaining interests as a means of sharing information in social networks. Numerous works have shown the potential of using Twitter posts (or tweets) in order to infer the existence and magnitude of real-world events. In the medical domain, there has been a surge in detecting public health related tweets for early warning so that a rapid response from health authorities can take place. In this paper, we present a temporal analytics tool for supporting a comparative, temporal analysis of disease outbreaks between Twitter and official sources, such as, World Health Organization (WHO) and ProMED-mail. We automatically extract and aggregate outbreak events from official outbreak reports, producing time series data. Our tool can support a correlation analysis and an understanding of the temporal developments of outbreak mentions in Twitter, based on comparisons with official sources.	aggregate data;sensor;social network;time series	Nattiya Kanhabua;Sara Romano;Avare Stewart;Wolfgang Nejdl	2012		10.1145/2396761.2398726	data science;time series;data mining;world wide web;statistics	HCI	-22.903772884533954	-54.412417157136105	111470
6982754ee2da4248f99dfd29f44dc4446817441a	predicting political donations using twitter hashtags and character n-grams	predictive analytics;political science;customer segmentation;data mining;twitter profiling;business analytics;social media	We describe a novel approach for predicting politicaldonations and performing psychographic segmentation basedon social data linked to election donation records. The role ofmicroblogs in enterprise informatics, specifically in relation tocustomer relationship systems is highlighted. Algorithms trainedon social data can be used to interpret and detect prospects' psychographicinformation. Contrasted with past approaches whichfocused exclusively on a single source of social data, the methodbeing presented allows us to use an objective gold standard bylinking Twitter and election records. Two experiments were conductedusing data collected from 438 Twitter users, half of whichare linked with donation event records collected from the UnitedStates Federal Election Commission. Probabilistic, entropy andkernel approaches were tested for predictive accuracy, while theCNG technique is explored as an alternative. The CNG algorithmwas found to predict political affiliation 17 percentage pointsabove the majority classifier, exceeding benchmarks suggestedby the literature. A NaïveBayes word n-gram approach wasfound to outperform CNG at predicting donations by predictingpolitical donations. Insufficient performance and poor reliabilityof standard word n-gram techniques in opinion detection revealskepticism about past work on political affiliation analysis fromsocial data alone. This suggests that prospecting systems maybenefit from constructing algorithms using data linked to external sources.	affinity analysis;algorithm;artificial neural network;data mining;experiment;grams;hashtag;informatics;n-gram;null character;scalability;social media	Colin Conrad;Vlado Keselj	2016	2016 IEEE 18th Conference on Business Informatics (CBI)	10.1109/CBI.2016.42	engineering;data science;data mining;world wide web	ML	-22.835555315386483	-54.617922968980636	111566
6c2abcd48ef2de5d6c1570d6589d030481ae159f	bridging semantic gap: learning and integrating semantics for content-based retrieval			bridging (networking)	Joo-Hwee Lim	2004				AI	-25.014510525402503	-61.47570242607544	111574
bc1101c884054b46868338526fe41561294faddd	effective and scalable authorship attribution using function words	busqueda informacion;estensibilidad;business information management incl records;and intelligence;information retrieval;intelligence artificielle;machine learning;recherche information;artificial intelligence;knowledge and information management;analyse information;feature selection;extensibilite;scalability;inteligencia artificial;information analysis	Techniques for identifying the author of an unattributed document can be applied to problems in information analysis and in academic scholarship. A range of methods have been proposed in the research literature, using a variety of features and machine learning approaches, but the methods have been tested on very different data and the results cannot be compared. It is not even clear whether the differences in performance are due to feature selection or other variables. In this paper we examine the use of a large publicly available collection of newswire articles as a benchmark for comparing authorship attribution methods. To demonstrate the value of having a benchmark, we experimentally compare several recent feature-based techniques for authorship attribution, and test how well these methods perform as the volume of data is increased. We show that the benchmark is able to clearly distinguish between different approaches, and that the scalability of the best methods based on using function words features is acceptable, with only moderate decline as the difficulty of the problem is increased.	benchmark (computing);bernstein polynomial;data compression;decision tree;experiment;feature selection;machine learning;natural language processing;norm (social);phil bernstein;scalability;scientific literature;stylometry;the australian	Ying Zhao;Justin Zobel	2005		10.1007/11562382_14	scalability;computer science;artificial intelligence;data science;machine learning;data mining;database;data analysis;feature selection	NLP	-19.829253343054333	-63.180698583139026	111613
0ae464c769bdefbfdaa793cc70bfc149bf866496	entropy based feature selection for text categorization	weighted argumentation frameworks;soft constraint satisfaction problems;coalition formation;indexation;comparative study;xml document;mutual information;feature selection;odd ratio;information gain;text categorization;document frequency	In text categorization, feature selection can be essential not only for reducing the index size but also for improving the performance of the classifier. In this article, we propose a feature selection criterion, called Entropy based Category Coverage Difference (ECCD). On the one hand, this criterion is based on the distribution of the documents containing the term in the categories, but on the other hand, it takes into account its entropy. ECCD compares favorably with usual feature selection methods based on document frequency (DF), information gain (IG), mutual information (IM), χ2, odd ratio and GSS on a large collection of XML documents from Wikipedia encyclopedia. Moreover, this comparative study confirms the effectiveness of selection feature techniques derived from the χ2 statistics.	categorization;direction finding;document classification;entropy (information theory);feature selection;information gain in decision trees;kullback–leibler divergence;mutual information;wikipedia;xml	Christine Largeron;Christophe Moulin;Mathias Géry	2011		10.1145/1982185.1982389	xml;computer science;artificial intelligence;machine learning;comparative research;pattern recognition;data mining;database;kullback–leibler divergence;mutual information;feature selection;world wide web;information retrieval;statistics;odds ratio	Web+IR	-22.35736002132753	-63.86651366658862	111754
adf9452d853d39d7578967011f2d5d2cc5697ab2	textual information retrieval systems test: the point of view of an organizer and corpuses provider		Amaryllis is an evaluation programme for text retrieval systems which has been carried out as two test campaigns. The second Amaryllis campaign took place in 1998/1999. Corpuses of documents, topics, and the corresponding responses were first sent to each of the participating teams for system learning purposes. Corpuses of new documents and a set of new topics were then supplied for evaluation purposes. Two optional tracks were added for Internet and interlingual track. The first track of these contained a test via the Internet. INIST sent topics to the system and collected responses directly, thus reducing the need for conceptor manipulations. The second contained tests in different European Community language pairs. The corpuses of documents consisted of records of questions and answers from the European Commission, in parallel official language versions. Participants could use any language pair for their tests. The aim of this paper is to give the point of view of an organizer and corpus provider (INIST) on the organization of an operation of this sort. In particular, it will describe the difficulties encountered during the tests (corpus construction, translation of topics and systems evaluation ), and will suggest avenues to explore for future tests.	document retrieval;google questions and answers;image organizer;information retrieval;internet;point of view (computer hardware company);programming language;text retrieval conference	Patrick Kremer;Laurent Schmitt	2000			the internet;official language;data mining;information retrieval;computer science	Web+IR	-32.063405196295385	-64.97333507730252	111828
37286271ab58d72152e95c0b757b4a961a536fda	a web text filter based on rough set weighted bayesian	bayesian theory;information filtering information filters bayesian methods web pages filtering theory internet set theory uniform resource locators probability feature extraction;web pages;inverse document frequency;bayes methods;rough set theory;information filtering;web text filter;bayesian methods;text analysis;html;coefficient weighted method;internet;feature selection methods;rough set weighted bayesian theory;text analysis bayes methods information filtering internet rough set theory;feature selection;baysian theory web text filter rough set;text content anaysis;rough set;coefficient weighted method web text filter rough set weighted bayesian theory internet web pages text content anaysis feature selection methods inverse document frequency;information filters;young people;filtering theory;baysian theory	With the deep penetration of the Internet, uncontrolled flood of information has become one of the most serious problems to Internet users. Harmful contents about pornography, violence and other illegal messages, etc have posed serious influence to the whole society, especially to the young people. In this paper, a novel web text filter based on rough set and Bayesian theory is proposed to analysis text content of web pages to filter harmful pages. Some of current feature selection methods such as Inverse document frequency (IDF) does not take the classification information into account. To avoid this shortcoming rough set is used to reduce original feature terms. Meanwhile, a novel coefficient weighted method based on rough set is proposed and introduced into Bayesian formula, which will greatly improve filtering performance. In the final experiment, this paper compared the novel method with other weighted methods applied in Bayesian formula, such as Tf, IDF and TFIDF. The results demonstrate that this novel filter works efficiently.	coefficient;data pre-processing;decision tree;feature selection;html element;information overload;platform for internet content selection;rough set;sports rating system;tf–idf;uncontrolled format string;web page	Yu Wu;Kun She;William Zhu;Xiaojun Yue;Huiqiong Luo	2009	2009 Eighth IEEE International Conference on Dependable, Autonomic and Secure Computing	10.1109/DASC.2009.38	rough set;bayesian probability;computer science;machine learning;pattern recognition;data mining;feature selection;information retrieval;statistics	Vision	-20.2397994138626	-57.04691223060621	111845
0b74af5233b693226c86736ff796b91b3e60f50b	relational summarization for corpus analysis		This work introduces a new problem, relational summarization, in which the goal is to generate a natural language summary of the relationship between two lexical items in a corpus, without reference to a knowledge base. Motivated by the needs of novel user interfaces, we define the task and give examples of its application. We also present a new queryfocused method for finding natural language sentences which express relationships. Our method allows for summarization of more than two times more query pairs than baseline relation extractors, while returning measurably more readable output. Finally, to help guide future work, we analyze the challenges of relational summarization using both a news and a social media corpus.	automatic summarization;baseline (configuration management);human-readable medium;knowledge base;natural language processing;relationship extraction;social media;user interface	Abram Handler;Brendan T. O'Connor	2018			automatic summarization;natural language processing;computer science;artificial intelligence	NLP	-27.914769578538255	-65.72682449848848	111878
dec6fcf599a4006b07a996b87cec2f50a57c67ce	minimal interaction search in recommender systems	human computer interaction;active learning;interactive search;generalized binary search;recommender systems	While numerous works study algorithms for predicting item ratings in recommender systems, the area of the user-recommender interaction remains largely under-explored. In this work, we look into user interaction with the recommendation list, aiming to devise a method that allows users to discover items of interest in a minimal number of interactions. We propose generalized linear search (GLS), a combination of linear and generalized searches that brings together the benefits of both approaches. We prove that GLS performs at least as well as generalized search and compare our method to several baselines and heuristics. Our evaluation shows that GLS is liked by the users and achieves the shortest interactions.	algorithm;baseline (configuration management);generalized least squares;heuristic (computer science);interaction;linear search;recommender system	Branislav Kveton;Shlomo Berkovsky	2015		10.1145/2678025.2701367	human–computer interaction;computer science;machine learning;data mining;active learning;world wide web;information retrieval;recommender system	ML	-31.90825833793086	-52.405193198949284	111917
71b16be4d30be41d636b542077ced4fd182fce9f	an effective term-ranking function for query expansion based on information foraging assessment		With the exponential growth of information on the Internet and the significant increase in the number of pages published each day have led to the emergence of new words in the Internet. Owning to the difficulty of achieving the meaning of these new terms, it becomes important to give more weight to subjects and sites where these new words appear, or rather, to give value to the words that occur frequently with them. For this reason, in this work, we propose an effective term-ranking function for query expansion based on the co-occurrence and proximity of words for retrieval effectiveness enhancement. A novel efficiency/effectiveness measure based on the principle of optimal information forager is also proposed in order to evaluate the quality of the obtained results. Our experiments were conducted using the OHSUMED test collection and show significant performance improvement over the state-of-the-art.		Ilyes Khennak;Habiba Drias;Hadia Mosteghanemi	2014		10.1007/978-3-319-13817-6_1	machine learning;data mining;information retrieval	HCI	-30.736835110470636	-58.46849321972689	112221
e38ec87bb91952360754f5b0eaf98fdee3c940bc	effective online knowledge graph fusion		Recently, Web search engines have empowered their search with knowledge graphs to satisfy increasing demands of complex information needs about entities. Each engine offers an online knowledge graph service to display highly relevant information about the query entity in form of a structured summary called knowledge card. The cards from different engines might be complementary. Therefore, it is necessary to fuse knowledge cards from these engines to get a comprehensive view. Such a problem can be considered as a new branch of ontology alignment, which is actually an on-the-fly online data fusion based on the users’ needs. In this paper, we present the first effort to work on knowledge cards fusion. We propose a novel probabilistic scoring algorithm for card disambiguation to select the most likely entity a card should refer to. We then design a learning-based method to align properties from cards representing the same entity. Finally, we perform value deduplication to group equivalent values of the aligned properties as value clusters. The experimental results show that our approach outperforms the state of the art ontology alignment algorithms in terms of precision and recall.	algorithm;align (company);data deduplication;entity;information needs;knowledge graph;ontology alignment;precision and recall;value (computer science);web search engine;word-sense disambiguation	Haofen Wang;Zhijia Fang;Le Zhang;Jeff Z. Pan;Tong Ruan	2015		10.1007/978-3-319-25007-6_17	computer science;artificial intelligence;data mining;database;world wide web;information retrieval	Web+IR	-28.705327834490927	-59.71440491779658	112306
102f1083abc1a28ebce6724de9a3a3943bfd8923	sketch the storyline with charcoal: a non-parametric approach		Generating a coherent synopsis and revealing the development threads for news stories from the increasing amounts of news content remains a formidable challenge. In this paper, we proposed a hddCRP (hybird distant-dependent Chinese Restaurant Process) based HierARChical tOpic model for news Article cLustering, abbreviated as CHARCOAL. Given a bunch of news articles, the outcome of CHARCOAL is threefold: 1) it aggregates relevant new articles into clusters (i.e., stories); 2) it disentangles the chain links (i.e., storyline) between articles in their describing story; 3) it discerns the topics that each story is assigned (e.g.,Malaysia Airlines Flight 370 story belongs to the aircraft accident topic and U.S presidential election stories belong to the politics topic). CHARCOAL completes this task by utilizing a hddCRP as prior, and the entities (e.g., names of persons, organizations, or locations) that appear in news articles as clues. Moveover, the adaptation of non-parametric nature in CHARCOAL makes our model can adaptively learn the appropriate number of stories and topics from news corpus. The experimental analysis and results demonstrate both interpretability and superiority of the proposed approach.	automatic summarization;client–server model;coherence (physics);definition;entity;information overload;layer (electronics);server (computing);sketch;statistical model;topic model;video synopsis	Siliang Tang;Fei Wu;Si Li;Weiming Lu;Zhongfei Zhang;Yueting Zhuang	2015			computer science;artificial intelligence	AI	-24.310903490732528	-59.25838398300346	112331
7678a6f3a3d1beb618dceae409da26b2ab8b5901	query disambiguation based on novelty and similarity user's feedback	inf;search engine;ing inf 05 sistemi di elaborazione delle informazioni;document retrieval	In this paper we propose a query disambiguation mechanism for query context focalization in a meta-search environment. Our methods start from a set of documents retrieved executing a query over a search engine and applies clustering in order to generate distinct homogeneous groups. Then, the following step is to compute for each cluster a disambiguated query that highlights its main contents. The disambiguated queries are suggestions for possible new focalized searches. The ranking of the clusters from which the queries are derived is provided based on a balance of the novelty of cluster contents, and their overall similarity with respect to the query.	word-sense disambiguation	Gloria Bordogna;Alessandro Campi;Giuseppe Psaila;Stefania Ronchi	2009		10.1007/978-3-642-04957-6_16	document retrieval;sargable;query optimization;query expansion;web query classification;ranking;boolean conjunctive query;computer science;data mining;database;web search query;world wide web;information retrieval;query language;search engine;spatial query	HCI	-32.20027333766727	-59.611194548905644	112337
67c42a30220edacf848e58c29a624234a0c37a6e	wise-integrator: an automatic integrator of web search interfaces for e-commerce	web interface;automatic integration;automatic integrator;high-quality integrated search interface;web search interface;web interfaces;different search interface;e-commerce search engine;multiple e-commerce search engine;e-commerce site;search interface;form-based search interface;human interaction;web accessibility;search engine;e commerce	More and more databases are becoming Web accessible through form-based search interfaces, and many of these sources are E-commerce sites. Providing a unified access to multiple Ecommerce search engines selling similar products is of great importance in allowing users to search and compare products from multiple sites with ease. One key task for providing such a capability is to integrate the Web interfaces of these Ecommerce search engines so that user queries can be submitted against the integrated interface. Currently, integrating such search interfaces is carried out either manually or semi-automatically, which is inefficient and difficult to maintain. In this paper, we present WISE-Integrator a tool that performs automatic integration of Web Interfaces of Search Engines. WISE-Integrator employs sophisticated techniques to identify matching attributes from different search interfaces for integration. It also resolves domain differences of matching attributes. Our experimental results based on 20 and 50 interfaces in two different domains indicate that WISEIntegrator can achieve high attribute matching accuracy and can produce high-quality integrated search interfaces without human interactions.	database;e-commerce payment system;interaction;semiconductor industry;user interface;web search engine;world wide web	Hai He;Weiyi Meng;Clement T. Yu;Zonghuan Wu	2003			e-commerce;interpersonal relationship;web modeling;metasearch engine;semantic search;computer science;spamdexing;web accessibility;data mining;database;search analytics;user interface;web search query;world wide web;information retrieval;search engine	DB	-30.885278707341048	-55.38090824181862	112388
aefba58a525cccd2d8839ae222b5a81f730faa85	measuring emotion bifurcation points for individuals in social media	emotion bifurcation point;social network services;computers;sina weibo;pediatrics;bifurcation;media ontologies bifurcation dictionaries pediatrics computers social network services;social networking online psychology;media;ontology social media emotion bifurcation point sentiment analysis sina weibo;dictionaries;sentiment analysis;emotion element ontology emotion bifurcation points measurement social media emotion expression textual messages emotional event social networks online individual emotion complex system theory tweets emotion perception ability chinese emotion dictionaries sina weibo corpus;ontologies;social media;ontology	Social media has been a new platform for emotion expression of individuals or groups in recent years. Millions of textual messages are constantly being generated. People with different emotion perceptions have different reactions to the same emotional event occurring in real life. However, it is hard to measure individual's emotion perception ability in both real world and social networks. This paper deals with online individual's emotion in view of complex system theory, and explores the emotion expression mechanism behind tweets. An concept of emotion bifurcation point is defined to denote the emotion perception ability and a methodological framework is proposed to measure it. Under the fundamental integration of the recognized Chinese emotion dictionaries (25,651 words included in total after reconciliation), new-born emotion words (458 in total) trained from a Sina-weibo corpus with 17 million tweets and commonly used emoticons (298 in total) as full-scale as possible, an emotion element ontology is constructed. Experimental evaluation on several certificated figures on Sina-weibo are implemented and the obtained results illustrate the reliability and validity of the proposed method.	bifurcation theory;complex system;dictionary;emoticon;full scale;machine perception;real life;social media;social network;systems theory	Jiandong Zhou;Yanping Zhao;Huaping Zhang;Tianming Wang	2016	2016 49th Hawaii International Conference on System Sciences (HICSS)	10.1109/HICSS.2016.246	media;social media;computer science;ontology;artificial intelligence;ontology;multimedia;world wide web;sentiment analysis	AI	-21.878172876204015	-53.08209818288821	112504
8a0956e2096a587d49d0d2fd158e142c7865a287	lexnet: a graphical environment for graph-based nlp	passage retrieval;lexical similarity;graph-based nlp;lexical graph;different nlp object;binary classification;text summarization;nlp task;graphical environment;interactive presentation	This interactive presentation describes LexNet, a graphical environment for graph-based NLP developed at the University of Michigan. LexNet includes LexRank (for text summarization), biased LexRank (for passage retrieval), and TUMBL (for binary classification). All tools in the collection are based on random walks on lexical graphs, that is graphs where different NLP objects (e.g., sentences or phrases) are represented as nodes linked by edges proportional to the lexical similarity between the two nodes. We will demonstrate these tools on a variety of NLP tasks including summarization, question answering, and prepositional phrase attachment.	attachments;automatic summarization;binary classification;graphical user interface;natural language processing;question answering	Dragomir R. Radev;Günes Erkan;Anthony Fader;Patrick Jordan;Siwei Shen;James P. Sweeney	2006			natural language processing;binary classification;computer science;automatic summarization;machine learning;pattern recognition;information retrieval	NLP	-27.09108541414004	-64.23427371243184	112536
4bf4e6be57ddebab13d04c65a5bcfe520c39e6c7	news video search with fuzzy event clustering using high-level features	pseudo relevance feedback;video retrieval;fuzzy clustering;event based clustering;retrieval model;automated speech recognition;information search and retrieval;video search	Precise automated video search is gaining in importance as the amount of multimedia information is increasing at exponential rates. One of the drawbacks that make video retrieval difficult is the lack of available semantics. In this paper, we propose to supplement the semantic knowledge for retrieval by providing useful semantic clusters derived from event entities present in the news video. These entities include the output from keywords derived from the automated speech recognition (ASR) and event-related High-level Features (HLF) extracted from the news video at the pseudo story level. Fuzzy clustering is then carried out to group similar stories together to form semantic clusters. The retrieval system utilizes these clusters to refine the re-ranking process in the Pseudo Relevance Feedback (PRF) step. Initial experiments performed on video search task using the TRECVID 2005 dataset show that the proposed approach can improve the search performance significantly.	automated system recovery;cluster analysis;entity;experiment;fuzzy clustering;high- and low-level;primitive recursive function;relevance feedback;speech recognition;time complexity	Shi-Yong Neo;Yantao Zheng;Tat-Seng Chua;Qi Tian	2006		10.1145/1180639.1180687	fuzzy clustering;computer science;machine learning;video tracking;concept search;pattern recognition;data mining;multimedia;world wide web;information retrieval;human–computer information retrieval	Web+IR	-25.53765712652621	-62.31572987647519	112540
ab72c6929994736339300dfa78dcc029e4e78f30	hierarchical comments-based clustering	real time;user study;contextual information;side effect;human computation games;term extraction;image tagging	"""Information resources on the Web like videos, images, and documents are increasingly becoming more """"social"""" through user engagement via commenting systems. These commenting systems provide a forum for users to discuss the resources but have the side effect of providing valuable editorial and contextual information about the resources. In this paper, we explore a comments-driven clustering framework for organizing Web resources according to this user-based perspective. Concretely, we propose a hierarchical comment clustering approach that relies on two key features: (i) comment term normalization and key term extraction for distilling noisy comments for effective clustering; and (ii) a real-time insertion component for incrementally updating the comments-based hierarchy so that resources can be efficiently placed in the hierarchy as comments arise and without the need to re-generate the (potentially) expensive hierarchy. We study the clustering approach over the popular video sharing site YouTube. YouTube is a challenging and difficult environment, notorious for its extremely short, ill-formed, and often unintelligible user-contributed comments. Through extensive experimental study, we find that the proposed approach can lead to effective and efficient comments-based video organizing even in a YouTube-like environment."""	cluster analysis;experiment;organizing (structure);real-time transcription;terminology extraction;web resource;world wide web	Chiao-Fang Hsu;James Caverlee;Elham Khabiri	2011		10.1145/1982185.1982434	computer science;artificial intelligence;machine learning;data mining;database;multimedia;programming language;world wide web;computer security;side effect	Web+IR	-26.164529725507318	-54.36297094661781	112548
ef5cb5d49716bcd754b971a9a7303d7da2cd1036	from greedy selection to exploratory decision-making: diverse ranking with policy-value networks		The goal of search result diversification is to select a subset of documents from the candidate set to satisfy as many different subtopics as possible. In general, it is a problem of subset selection and selecting an optimal subset of documents is NP-hard. Existing methods usually formalize the problem as ranking the documents with greedy sequential document selection. At each of the ranking position the document that can provide the largest amount of additional information is selected. It is obvious that the greedy selections inevitably produce suboptimal rankings. In this paper we propose to partially alleviate the problem with a Monte Carlo tree search (MCTS) enhanced Markov decision process (MDP), referred to as M$^2$Div. In M$^2$Div, the construction of diverse ranking is formalized as an MDP process where each action corresponds to selecting a document for one ranking position. Given an MDP state which consists of the query, selected documents, and candidates, a recurrent neural network is utilized to produce the policy function for guiding the document selection and the value function for predicting the whole ranking quality. The produced raw policy and value are then strengthened with MCTS through exploring the possible rankings at the subsequent positions, achieving a better search policy for decision-making. Experimental results based on the TREC benchmarks showed that M$^2$Div can significantly outperform the state-of-the-art baselines based on greedy sequential document selection, indicating the effectiveness of the exploratory decision-making mechanism in M$^2$Div.	artificial neural network;bellman equation;benchmark (computing);diversification (finance);exploratory testing;greedy algorithm;heuristic;local optimum;markov chain;markov decision process;monte carlo method;monte carlo tree search;multi-master replication;np-hardness;parsing;recurrent neural network;reinforcement learning;value network	Yue Feng;Jun Xu;Yanyan Lan;Jiafeng Guo;Wei Zeng;Xueqi Cheng	2018		10.1145/3209978.3209979	computer science;data mining;baseline (configuration management);recurrent neural network;artificial intelligence;bellman equation;monte carlo tree search;ranking;markov decision process;pattern recognition;diversification (marketing strategy);value network	Web+IR	-27.07038247014213	-54.5627580554941	112676
57b33c94273e4ba4c6b8c98905a3fdd54fdcedb9	method of enriching queries by contextual information to approve of information retrieval system in arabic	pragmatics;evaluations metrics informationretrieval arabic nlp query enrichment weighting function;search engines;semantics;document corpus query enrichment method information retrieval system irs arabics significant terms identification descriptive list generation statistical treatment similarity method salton tf idf weighting function salton tf ief weighting function;semantics pragmatics search engines labeling statistical analysis air pollution;statistical analysis;statistical analysis document handling information retrieval systems natural language processing query processing;air pollution;labeling	In this paper, we propose a method is to improve the performance of information retrieval systems (IRS) by increasing the selectivity of relevant documents on the web. Indeed, a significant number of relevant documents on the web are not returned by an IRS (specifically a search engine), because of the richness of natural language Arabics. For this purpose the search engine does not reach high performance and does not meet the needs of users. To remedy this problem, we propose a method of enrichment of the query. This method relies on many steps. First, identification of significant terms (simple and composed) present in the query. Then, generation of a descriptive list and its assignment to each term that has been identified as significant in the query. A descriptive list is a set of linguistic knowledge of different types (morphological, syntactic and semantic). In this paper we are interested in the statistical treatment, based on the similarity method. This method exploits the weighting functions of Salton TF-IDF and TF-IEF on the list generated in the previous step. TF-IDF function identifies relevant documents, while the TF-IEF's role is to identify the relevant sentence. The terms of high weight (which are terms which may be correlated to the context of the response) are incorporated into the original query. The application of this method is based on a corpus of documents belonging to a closed domain.	ca gen;gene ontology term enrichment;information retrieval;map;natural language;selectivity (electronic);text corpus;tf–idf;web search engine	Souheyl Mallat;Houssem Abdellaoui;Mohsen Maraoui;Mounir Zrigui	2015	2015 5th International Conference on Information & Communication Technology and Accessibility (ICTA)	10.1109/ICTA.2015.7426926	labeling theory;query expansion;web query classification;ranking;computer science;machine learning;data mining;database;semantics;linguistics;web search query;world wide web;information retrieval;query language;search engine;pragmatics;air pollution	Web+IR	-30.048631491794584	-65.29678923410533	112804
60118fe6060335415ef35e7c50e9b523905521f1	diagnosing editorial strategies of chilean media on twitter using an automatic news classifier		In Chile, does not exist an independent entity that publishes quantitative or qualitative surveys to   understand   the   traditional   media   environment   and   its   adaptation   on   the   Social   Web. Nowadays, Chilean newsreaders are increasingly using social web platforms as their primary source of   information,  among which Twitter  plays  a central   role.  Historical  media and pure players  are developing  different  strategies   to   increase  their  audience  and  influence on  this platform. In this article, we propose a methodology based on data mining techniques to provide a first level of analysis of the new Chilean media environment. We use a crawling technique to mine news streams of  37 different  Chilean media actively  presents on Twitter  and propose several  indicators  to compare  them. We analyze their  volumes of production,   their  potential audience, and using NLP techniques, we explore the content of their production: their editorial line and their geographic coverage.	data mining;information source;natural language processing;primary source;social media	Matthieu Vernier;Luis Cárcamo;Eliana Scheihing	2016	CoRR		multimedia;world wide web	Web+IR	-23.871804348440023	-55.05067512284395	112839
3f466e2ffefa360c6b987e94a657a3b3ea5eed91	multiple evidence combination in web site search based on users' access histories	search engine;web pages;retrieval model	Despite the success of global search engines, web site search is still problematic in its retrieval accuracy. In this study, we propose to extract terms based on users' access histories to build web page representations, and then use multiple evidence combination to combine these log-based terms with text-based and anchor-based terms. We test different combination approaches and baseline retrieval models. Our experimental results show that the server log, when used in multiple evidence combination, can improve the effectiveness of the web site search, whereas the impact on different models is different.		Chen Ding;Jin Zhou	2007		10.1007/978-3-540-73078-1_54	site map;semantic search;computer science;web crawler;data mining;web search query;world wide web;information retrieval;search engine	Web+IR	-30.416899441795085	-55.50695049669265	112878
cc189fd13eccd7ebbc2534433b007bd90ab6600e	evaluation of user search in a web-database	search and retrieval;information resources;databases information resources internet electronic mail information retrieval statistics government information science data security organizational aspects;information use;web databases;information retrieval;statistical databases;topic initiation user search evaluation world wide web databases information resource development information resource use trends national statistical database web site improvement efforts web log data registered users user sessions topic sessions session definition registered organizations task approximation information searching information retrieval request sequence;task analysis information resources statistical databases information use information retrieval;usage study;task analysis;user session;interface evaluation;web database	The number of Web-databases has exploded during the last years. In order to justify the development of new information resources, it is essential to know if the use of existing resources has followed a similar trend. This paper presents an analysis of the use of a national statistical webdatabase made to support Web site improvement efforts. The study is based on log data taken during 2 time periods in 1999 and 2000. In this period, the number of registered users increased 5-fold and the number of sessions more than double. During September 2000, active users spent 4,320 hours on the Web-DB and initiated 14,998 topic-sessions giving an average of 7 hours and 25 sessions per user. Definition of a session has proved difficult since the log data available is based on registered organizations, rather than on tasks or individual persons. Ideally, a session should be defined as a search and retrieval for the information required for a task. We have used a topic-session, defined as the sequence of requests from topic initiation to retrieval of data from this topic, as a task approximation.	approximation;internet;period-doubling bifurcation;statistical model;web sql database;world wide web	Joan C. Nordbotten;Svein Nordbotten	2002		10.1109/HICSS.2002.994099	relevance;cognitive models of information retrieval;computer science;data mining;task analysis;database;adversarial information retrieval;world wide web;information retrieval;human–computer information retrieval	Web+IR	-33.477987736776974	-53.860754601910834	112930
a0a38449c493bfbc2e9bdef2090d1a02a8e611be	a robust discriminative term weighting based linear discriminant method for text classification	robustness text categorization electronic mail application software web pages information filtering hybrid power systems data mining computer science data engineering;electronic mail;probability density function;generative discriminative algorithm text classification term weighting;text analysis;data mining;feature space;classification;text classification;empirical evidence;weight measurement;hybrid power systems;linear discriminant function;term weighting;text analysis classification;generative discriminative algorithm;robustness;two dimensional feature space robust discriminative term weighting based linear discriminant method supervised text classification method discrimination information pooling documents linear opinion pool;entropy	Text classification is widely used in applications ranging from e-mail filtering to review classification. Many of these applications demand that the classification method be efficient and robust, yet produce accurate categorizations by using the terms in the documents only. We present a supervised text classification method based on discriminative term weighting, discrimination information pooling, and linear discrimination. Terms in the documents are assigned weights according to the discrimination information they provide for one category over the others. These weights also serve to partition the terms into two sets. A linear opinion pool is adopted for combining the discrimination information provided by each set of terms yielding a two-dimensional feature space. Subsequently, a linear discriminant function is learned to categorize the documents in the feature space. We provide intuitive and empirical evidence of the robustness of our method with three term weighting strategies. Experimental results are presented for data sets from three different application areas. The results show that our method's accuracy is higher than other popular methods, especially when there is a distribution shift from training to testing sets. Moreover, our method is simple yet robust to different application domains and small training set sizes.	algorithm;anti-spam techniques;application domain;categorization;discriminative model;document classification;email filtering;feature vector;iterative method;kullback–leibler divergence;linear discriminant analysis;mathematical optimization;naive bayes classifier;personalization;principle of maximum entropy;robustness (computer science);scalability;statistical model;test set	Khurum Nazir Junejo;Asim Karim	2008	2008 Eighth IEEE International Conference on Data Mining	10.1109/ICDM.2008.26	entropy;probability density function;text mining;empirical evidence;feature vector;biological classification;computer science;machine learning;pattern recognition;data mining;statistics;robustness	DB	-21.16944402435886	-63.014089103206196	113297
7016776a8105db0c2d442d9bffbbd2828fdf2bf9	a study of measures for document relatedness evaluation	informational search;semantic similarity;semantics encyclopedias electronic publishing internet taxonomy measurement;document handling;document relatedness;measurement;information retrieval;information retrieval document handling;document appropriateness document relatedness evaluation measurement information retrieval;semantics;internet;taxonomy;semantic measures;electronic publishing;encyclopedias;information retrieval informational search document relatedness semantic similarity semantic measures	In this review paper we classified and described measures and approaches for document relatedness evaluation. For the reviewed measures we pointed out the reasons of their construction and usage limitations. We concluded this research with a discourse on challenges of the day in estimating document appropriateness in the domain of information retrieval.	feedback;information retrieval;norm (social);relevance;the new york times	Evgeny Pyshkin;Vitaly Klyuev	2012	2012 Federated Conference on Computer Science and Information Systems (FedCSIS)		semantic similarity;the internet;computer science;data mining;database;semantics;linguistics;electronic publishing;information retrieval;encyclopedia;taxonomy;measurement	Web+IR	-29.551438412107917	-62.513025479971084	113409
9234042eb4f40d849cf9ab5609511a74604f510b	learning to classify biomedical terms through literature mining and genetic algorithms	terminologie;expresion regular;terminologia;analisis estadistico;analisis datos;genie biomedical;intelligence artificielle;algoritmo genetico;data mining;classification;data analysis;biomedical engineering;statistical analysis;fouille donnee;analyse statistique;algorithme genetique;pattern recognition;expression reguliere;artificial intelligence;analyse donnee;genetic algorithm;terminology;ingenieria biomedica;inteligencia artificial;reconnaissance forme;reconocimiento patron;busca dato;clasificacion;regular expression	We present an approach to classification of biomedical terms based on the information acquired automatically from the corpus of relevant literature. The learning phase consists of two stages: acquisition of terminologically relevant contextual patterns (CPs) and selection of classes that apply to terms used with these patterns. CPs represent a generalisation of similar term contexts in the form of regular expressions containing lexical, syntactic and terminological information. The most probable classes for the training terms co-occurring with the statistically relevant CP are learned by a genetic algorithm. Term classification is based on the learnt results. First, each term is associated with the most frequently co-occurring CP. Classes attached to such CP are initially suggested as the term’s potential classes. Then, the term is finally mapped to the most similar suggested class.	genetic algorithm;regular expression	Irena Spasić;Goran Nenadic;Sophia Ananiadou	2004		10.1007/978-3-540-28651-6_51	genetic algorithm;biological classification;computer science;artificial intelligence;data mining;data analysis;terminology;regular expression;algorithm;statistics	AI	-22.20256877334768	-64.12291469784623	113560
52aa034a1c1413e03e7895dab0a0b67549cf92e1	information re-retrieval: repeat queries in yahoo's logs	repeat queries;search engine;query log analysis;automatic detection;re finding;web search	People often repeat Web searches, both to find new information on topics they have previously explored and to re-find information they have seen in the past. The query associated with a repeat search may differ from the initial query but can nonetheless lead to clicks on the same results. This paper explores repeat search behavior through the analysis of a one-year Web query log of 114 anonymous users and a separate controlled survey of an additional 119 volunteers. Our study demonstrates that as many as 40% of all queries are re-finding queries. Re-finding appears to be an important behavior for search engines to explicitly support, and we explore how this can be done. We demonstrate that changes to search engine results can hinder re-finding, and provide a way to automatically detect repeat searches and predict repeat clicks.	internet;iteration;tracing (software);web search engine	Jaime Teevan;Eytan Adar;Rosie Jones;Michael A. S. Potts	2007		10.1145/1277741.1277770	query expansion;web query classification;computer science;data mining;web search query;world wide web;information retrieval;search engine	Web+IR	-33.56345917898934	-53.15922444526958	113944
dde6d2c360fd402ad211e1381ec4c4d8d3dbd757	generating page clippings from web search results using a dynamically terminated genetic algorithm	search engine;page clipping synthesis;computacion informatica;information retrieval;search method;ciencias basicas y experimentales;cost effectiveness;genetic algorithm;termination criteria;web search;grupo a	We present a page clipping synthesis (PCS) search method to extract relevant paragraphs from other web search results. The PCS search method applies a dynamically terminated genetic algorithm to generate a set of best-of-run page clippings in a controlled amount of time. These page clippings provide users the information they are most interested in and therefore save the users time and trouble in browsing lots of hyperlinks. We justify that the dynamically terminated genetic algorithm yields cost-effective solutions compared with solutions reached by conventional genetic algorithms. Meanwhile, effectiveness measure confirmed that PCS performs better than general search engines.	genetic algorithm;web search engine	Lin-Chih Chen;Cheng-Jye Luh;Chichang Jou	2005	Inf. Syst.	10.1016/j.is.2004.04.002	beam search;genetic algorithm;cost-effectiveness analysis;computer science;artificial intelligence;theoretical computer science;operating system;data mining;database;best-first search;world wide web;search engine	Web+IR	-31.185047390411412	-54.09349120937116	113962
8b98e582c4a2a1adfa1a43cc681c12d4446102d4	random walk with wait and restart on document co-citation network for similar document search		One of the latest algorithms for computing similarities between nodes in a graph is Random Walk with Restart (RWR). However, on a document co-citation network for similar document search, computing transition probabilities remains difficult. To solve the problem, this paper proposes a Random Walk with Wait and Restart (RWWR) algorithm, which contains a new technique for adjusting the transition probability by incorporating a “selfreturning” edge into the normalization. To evaluate its effectiveness empirically, the search performance of two retrieval methods using RWWR was compared to a method using the standard RWR; the performance was measured by average precision and nDCG. The experiment was conducted on a test collection created from the Open Access Subset of PubMed Central, and the results indicated that the RWWR methods tend to outperform the standard RWR method.	algorithm;citation network;co-citation;information retrieval;markov chain;pubmed central;running with rifles	Masaki Eto	2014			data mining;database;world wide web	Web+IR	-28.320813411120316	-62.084345883074405	113978
50ddb7d0e03632d6d1d1748edc385ecfca8dc99a	graph-based semi-supervised learning for text classification		In this paper, we propose a graph-based representation of document collections in which both documents and features are represented by nodes. The nodes are connected with weights based on word order, context similarity and word frequency. Graph-based representations can overcome the limitations of bag-of-words based representations that suffer from sparseness for collections with short documents. In a series of experiments, we evaluate multiple types of graph-based text features in the context of semi-supervised text classification, and investigate the effect of the number of labeled documents in the collection. We find that graph-based semi-supervised learning outperforms bag-of-words semi-supervised learning but not bag-of-words supervised learning in 20-class text categorization. A large asset of graph-based representations is that they are flexible in the types of nodes and relations that are included.	archive;bag-of-words model;baseline (configuration management);categorization;document classification;experiment;neural coding;semi-supervised learning;semiconductor industry;supervised learning;word lists by frequency	Natalie Widmann;Suzan Verberne	2017		10.1145/3121050.3121055	graph (abstract data type);word order;semi-supervised learning;supervised learning;categorization;graph database;machine learning;text graph;artificial intelligence;pattern recognition;word lists by frequency;computer science	NLP	-25.669417831739775	-65.27589634931908	114007
39633e93996c5369af23c7f7fa3eac053bcc1637	accessing the deep web using ontology	databases;attribute attribute ontology deep web information web pages ontology based search attribute value ontology;search engine;web pages;query processing;hidden web;knowledge extraction;semantics;data mining;ontologies artificial intelligence;web crawler;internet;deep web hidden web semantic web;indexation;semantic web;ontologies;deep web;ontologies databases web pages data mining semantic web semantics;domain ontology;query processing internet ontologies artificial intelligence	Ontologies act like a bridge between user expressions and raw data. Hence, they can play an important role in assisting the users in their search for Web pages. Different users use different queries according to their knowledge and intuition to find the results. The numbers of relevant Web pages returned to users differ depending on the terms entered into the search box of traditional search engines. Many Web pages returned to users may be completely irrelevant, and it takes too long for users to identify the relevant Web pages by going through too many results. It is necessary to develop a methodology such that the number of returned Web pages becomes smaller while the overall number of relevant Web pages becomes bigger. This paper proposes a novel approach that combines Deep Web information, which consists of dynamically generated Web pages and cannot be indexed by the existing automated Web crawlers, with ontologies built from the knowledge extracted from Deep web sources. Here, Ontology based search is divided into different modules. The first module constructs attribute-value ontology. Second module constructs the attribute-attribute ontology. Third module formulate the user query, fills the search interface using domain ontology, extract results by looking into the index database.	attribute grammar;deep web;dynamic web page;ontology (information science);relevance;web crawler;web search engine	Anuradha;A. K. Sharma	2010	2010 3rd International Conference on Emerging Trends in Engineering and Technology	10.1109/ICETET.2010.35	web service;static web page;web development;web modeling;site map;web query classification;data web;web mapping;web design;web search engine;web standards;computer science;web navigation;social semantic web;web page;semantic web stack;database;web search query;web 2.0;world wide web;owl-s;website parse template;information retrieval;web server	Web+IR	-29.775364008158657	-54.2621197647666	114221
63bd0d2183c7bfed911f638ec8d44e88e327480e	cross lingual question answering using cindi_qa for qa@clef 2007	french;questions beyond factoids;english;bilingual;question answering	This article presents the first participation of the CINDI group in the Multiple Language Question Answering Cross Language Evaluation Forum (QA@CLEF). We participated in a track using French as the source language and English as the target language. CINDI_QA first uses an online translation tool to convert the French input question into an English sentence. Second, a Natural Language Parser extracts keywords such as verbs, nouns, adjectives and capitalized entites from the query. Third, synonyms of those keywords are generated thanks to a Lexical Reference module. Fourth, our integrated Searching and Indexing component localises the answer candidates from the QA@CLEF data collection provided to us. Finally, the candidates are matched against our existing set of templates to decide on the best answer to return to the user. Two runs were submitted. Having missed using a large part of the corpora, CINDI_QA was only able to generate correct and exact answers for 13% of the questions it received.	compiler;natural language;parser;question answering;text corpus;yahoo! answers	Chedid Haddad;Bipin C. Desai	2007			noun;information retrieval;data collection;parsing;question answering;search engine indexing;natural language;synonym;computer science;sentence	NLP	-31.331810224506334	-65.01709020865204	114382
6f7f5b88d783c3887486e75a2926d3eeeb0d538f	n-gram graphs: representing documents and document sets in summary system evaluation		Within this article, we present the application of the AutoSummENG method within the TAC 2009 AESOP challenge. We further offer an alternative to the original AutoSummENG method, which uses an additional operator of the n-gram graph framework to represent a set of documents with a single, merged graph. This alternative shows promising effectiveness and suggests that n-gram graphs and their operators can constitute an effective and updatable text representation method.	experiment;graph (abstract data type);n-gram;preprocessor;responsiveness;text corpus	George Giannakopoulos;Vangelis Karkaletsis	2009			computer science;artificial intelligence;machine learning;data mining	NLP	-27.5113893933119	-64.2684666193433	114403
9710d30f6f1e554bf9cba5c626887640dc1ee285	improving query suggestion by utilizing user intent	search logs;user intent;users search context;search engines;query formulation;information needs;data mining;inspection;suggestions online;consecutive query;indexes;query suggestion;internet;mathematical model;current user search pattern query suggestion users search context search logs consecutive query reformulation patterns suggestions online user intent search intents customized suggestions;search engines information needs internet query formulation;customized suggestions;current user search pattern;indexes context equations search engines mathematical model data mining inspection;reformulation patterns;context;similarity search;search intents	In this paper, we introduce a query suggestion approach to reuse users' search context and search logs. For a given search log, we integrate two pieces of wisdom embedded in the search context: consecutive queries and reformulation patterns between consecutive queries. When providing suggestions online, we extract concepts that represent the user's intent and associate these concepts with wisdom attained from past users who had similar search intents. Finally, customized suggestions are provided according to the current user's search pattern. The experimental results demonstrate that the proposed approach outperforms existing query suggestion methods and effectively provides users with more accurate suggestions to help them get required information faster.	embedded system;pattern recognition;text mining	Shuo-En Tsai;Yi-Shin Chen;Chia-Yu Tsai;Shih-Wei Tu	2010	2010 IEEE International Conference on Information Reuse & Integration	10.1109/IRI.2010.5558971	database index;information needs;web query classification;the internet;inspection;computer science;mathematical model;data mining;database;web search query;world wide web;information retrieval;statistics	Robotics	-30.087357549039215	-53.94726089515597	114428
1ab2582c077ba051b02990f65f84fbba431742b8	automatically generating descriptions for resources by tag modeling	tagging system;generative model;tag generation	We have been witnessing an increasing number of social tagging systems on the web. Tags help users understand a resource readily and accurately. In a social tagging system, however, there are typically a fairly large number of resources each associated with a long list of tags. When browsing resources, users are reluctant to read these tags one by one. Instead, users prefer a shorter list of tags as a compact description of a resource. Such a tag description facilitates users to understand the resource accurately and effortlessly. This calls for a generator for a tag description, which selects a set of high-quality tags for a given resource. The tag description condenses the original tag list by retaining the most important tags of the long list.  We propose that a good generator should go beyond pure tag popularity and towards diversifying a tag description. In this paper, we present a general framework of selecting a set of k tags as the description for a given resource. In addition, a generative model BTM is proposed to model users' tagging process. The experimental results on real-world tagging data confirm the effectiveness of the proposed approach in social tagging systems, showing significant improvement over the other baselines.	description logic;folksonomy;generative model;mathematical optimization;optimization problem;tag cloud	Bin Bi;Junghoo Cho	2013		10.1145/2505515.2505632	tag system;computer science;machine learning;data mining;generative model;world wide web;information retrieval	AI	-30.996805381661158	-53.56574443688072	114476
bb9b8a86842b8dcb2698b50a6d55a064edef3bb2	a comparison of automatic search query enhancement algorithms that utilise wikipedia as a source of a priori knowledge		This paper describes the benchmarking and analysis of five Automatic Search Query Enhancement (ASQE) algorithms that utilise Wikipedia as the sole source for a priori knowledge. The contributions of this paper include: 1) A comprehensive review into current ASQE algorithms that utilise Wikipedia as the sole source for a priori knowledge; 2) benchmarking of five existing ASQE algorithms using the TREC-9 Web Topics on the ClueWeb12 data set and 3) analysis of the results from the benchmarking process to identify the strengths and weaknesses each algorithm.  During the benchmarking process, 2,500 relevance assessments were performed. Results of these tests are analysed using the Average Precision @10 per query and Mean Average Precision @10 per algorithm.  From this analysis we show that the scope of a priori knowledge utilised during enhancement and the available term weighting methods available from Wikipedia can further aid the ASQE process. Although approaches taken by the algorithms are still relevant, an over dependence on weighting schemes and data sources used can easily impact results of an ASQE algorithm.	algorithm;analysis of algorithms;arbitrary-precision arithmetic;average-case complexity;genetic algorithm;information retrieval;mean squared error;naivety;programming language;python;ranking (information retrieval);relevance;video post-processing;web search query;wikipedia	Kyle Goslin;Markus Hans Hofmann	2017		10.1145/3158354.3158356	computer science;data mining;a priori and a posteriori;algorithm;benchmarking;web search query;strengths and weaknesses;weighting	Web+IR	-33.30845661232185	-62.75215595340103	114734
3d511bafe41dfcd03c7da971d07bf0dee946228b	document summarization using dictionary learning		Document summarization is a strategy, intended to extract information from multiple documents, deliberating the same subject. Many software applications handle document summarization, helping people grab the main thought, from a collection of documents, within a short time. Automatic summaries present information algorithmically extracted from multiple sources, without any impressionistic human intervention mediation. Experiments have resulted in ingenious algorithms, surmount the task of creating a short and salient summary. One such technique suggested in this paper is Dictionary Learning. This paper focuses on Document summarization, using dictionary learning and sparse coding techniques, considering the ordering of sentences and redundancy of documents. We use Singular Value Decomposition(SVD) for dictionary learning and Orthogonal Matching Pursuit(OMP) for sparse coding. The application of SVD augments the semantics of the generated summary. The order of sparsity in the final sparse code is used in ordering the sentences in the final summary. Verification of our proposed methodology have shown 75% precision.	algorithm;automatic summarization;column (database);data compression;dictionary;experiment;machine learning;neural coding;singular value decomposition;sparse matrix	Remya R. K. Menon;N. Aswathy	2017	2017 International Conference on Advances in Computing, Communications and Informatics (ICACCI)	10.1109/ICACCI.2017.8125914	information retrieval;control engineering;matching pursuit;computer science;automatic summarization;redundancy (engineering);software;encoding (memory);sparse matrix;semantics;singular value decomposition	NLP	-25.397234561891594	-63.388672642854274	114949
2b9a3bf6ec5bf0744a142c347f33d6f9f20ea43c	distributed and collaborative web change detection system	crawling systems and search engines;content refresh;incremental crawling	Search engines use crawlers to traverse the Web in order to download web pages and build their indexes. Maintaining these indexes up-to-date is an essential task to ensure the quality of search results. However, changes in web pages are unpredictable. Identifying the moment when a web page changes as soon as possible and with minimal computational cost is a major challenge. In this article we present the Web Change Detection system that, in a best case scenario, is capable to detect, almost in real time, when a web page changes. In a worst case scenario, it will require, on average, 12 minutes to detect a change on a low PageRank web site and about one minute on a web site with high PageRank. Meanwhile, current search engines require more than a day, on average, to detect a modification in a web page (in both cases).	algorithmic efficiency;apache nutch;best, worst and average case;distributed computing;download;experiment;heritrix;learning to rank;md5;mapreduce;pagerank;push technology;scalability;sensor;surface web;systems architecture;traverse;the void (virtual reality);web crawler;web page;web search engine;webmaster;world wide web;worst-case scenario	Víctor M. Prieto;Manuel Álvarez;Victor Carneiro;Fidel Cacheda	2015	Comput. Sci. Inf. Syst.	10.2298/CSIS131120081P	site map;web analytics;computer science;web crawler;distributed web crawling;data mining;database;web search query;world wide web;information retrieval;mashup	Web+IR	-30.427205106595693	-54.58412835152874	114987
39776d135c25e16217ef25e19a2ecdda496bb556	toward the development of a psycholinguistic-based measure of insider threat risk focusing on core word categories used in social media	psycholinguistic analysis;insider threat;five factor model;dark triad;liwc	The development of a psycholinguistic measure of insider threat risk faces many challenges. Prior research suggests that psycholinguistic analysis has potential for identifying individuals who are high risks for insider/criminal activity, but testing and validation of proposed methods is hampered by lack of available data. For example, analytic tools can discriminate text samples authored by known criminals from e-mail text produced in an organization, but a robust risk assessment tool must be able to handle heterogeneous text sources that come from a diverse population with wide differences in word use across age and other demographic variables. The current study seeks to advance the state of psycholinguistic insider threat research by among other things, augmenting word analysis dictionaries used in current practice and accommodating linguistic characteristics of social media, focusing on a core set of differentially-weighted word categories that represent an intersection of critical personality traits related to behaviors of concern.	dark web;dictionary;email;ground truth;insider threat;risk assessment;social media;threat (computer);word lists by frequency	Christopher R. Brown;Frank L. Greitzer;Alison Watkins	2013			psychology;speech recognition;communication;social psychology	HCI	-21.472218511037923	-59.90974832010285	115119
513628abe11c55701c2e419ce422c7bce9fb9573	automatic indexing for research papers using references	information retrieval;genetic algorithm	An effective way to reveal the contents of research papers is assigning a group of terms against a controlled vocabulary. To the best of our knowledge, a variety of automatic indexing techniques have been studied to enhance the effectiveness and the efficiency. However, the current approaches depended on the content of a research paper, such as title, abstract, etc., which suffering from limitations on the automatic indexing performance. In this paper, we propose a new approach of automatic indexing for single research paper with its references based on Genetic Algorithm (GA). The extensive experiments on four subjects show the effectiveness of the proposed approach.	controlled vocabulary;experiment;genetic algorithm	Wei Liu	2014	JSW		genetic algorithm;computer science;data mining;database;information retrieval	DB	-31.08749206674665	-59.17779125559423	115134
6876cd49ff90161e5ad6413fe3aa96dcf93d05b6	a supervised learning algorithm for binary domain classification of web queries using serps	google;search engines learning artificial intelligence pattern classification query processing;search engines;web queries;internet;query understanding;feature extraction;google supervised learning algorithm binary domain classification web queries serp general purpose search engines local dl aol queries nasa technical report server;query understanding search engines web queries;electronic publishing;encyclopedias;google feature extraction search engines encyclopedias electronic publishing internet	"""General purpose Search Engines (SEs) crawl all domains (e.g., Sports, News, Entertainment) of the Web, but sometimes the informational need of a query is restricted to a particular domain (e.g., Medical). We leverage the work of SEs as part of our effort to route domain specific queries to local Digital Libraries (DLs). SEs are often used even if they are not the """"best"""" source for certain types of queries. Rather than tell users to """"use this DL for this kind of query"""", we intend to automatically detect when a query could be better served by a local DL (such as a private, access-controlled DL that is not crawlable via SEs). This is not an easy task because Web queries are short, ambiguous, and there is lack of quality labeled training data (or it is expensive to create). To detect queries that should be routed to local, specialized DLs, we first send the queries to Google and then examine the features in the resulting Search Engine Result Pages. Using 400,000 AOL queries for the """"non-scholar"""" domain and 400,000 queries from the NASA Technical Report Server for the """"scholar"""" domain, our classifier achieved a precision of 0.809 and F-measure of 0.805."""	algorithm;digital library;f1 score;learning to rank;routing;search engine results page;supervised learning;world wide web	Alexander C. Nwala;Michael L. Nelson	2016	2016 IEEE/ACM Joint Conference on Digital Libraries (JCDL)	10.1145/2910896.2925449	query expansion;web query classification;the internet;metasearch engine;semantic search;feature extraction;computer science;data mining;electronic publishing;web search query;world wide web;information retrieval;encyclopedia;spatial query	Web+IR	-29.23955876212458	-55.26892193318876	115248
5d82ba0280ee2b96b112d169b5385ea6d5a62dd6	emotion tracking on blogs - a case study for bengali	bengali wordnet affect;blog;emotion;tracking	Rapid growth of blogs in the Web 2.0 and the handshaking between multilingual search and sentiment analysis motivate us to develop a blog based emotion analysis system for Bengali. The present paper describes the identification, visualization and tracking of bloggers' emotions with respect to time from Bengali blog documents. A simple pre-processing technique has been employed to retrieve and store the bloggers' comments on specific topics. The assignment of Ekman's six basic emotions to the bloggers' comments is carried out at word, sentence and paragraph level granularities using the Bengali WordNet AffectLists. The evaluation produces the precision, recall and F-Score of 59.36%, 64.98% and 62.17% respectively for 1100 emotional comments retrieved from 20 blog documents. Each of the bloggers' emotions with respect to different timestamps is visualized by an emotion graph. The emotion graphs of 20 bloggers demonstrate that the system performs satisfactorily in case of emotion tracking.		Dipankar Das;Sagnik Roy;Sivaji Bandyopadhyay	2012		10.1007/978-3-642-31087-4_47	speech recognition;emotion;tracking;multimedia	NLP	-25.085659244028903	-62.558053408766526	115321
72b501fb2058dd9fe2270d6ae82a1ac20949fc4e	scalable entity resolution using probabilistic signatures on parallel databases		Accurate and efficient entity resolution is an open challenge of particular relevance to intelligence organisations that collect large datasets from disparate sources with differing levels of quality and standard. Starting from a first-principles formulation of entity resolution, this paper presents a novel entity resolution algorithm that introduces a data-driven blocking and record linkage technique based on the probabilistic identification of entity signatures in data. The scalability and accuracy of the proposed algorithm are evaluated using benchmark datasets and shown to achieve state-of-the-art results. The proposed algorithm can be implemented simply on modern parallel databases, which we have done in the financial intelligence domain with tens of Terabytes of noisy data.		Yuhang Zhang;Kee Siong Ng;Tania Churchill;Peter Christen	2018		10.1145/3269206.3272016	record linkage;name resolution;computer science;data mining;noisy data;database;scalability;probabilistic logic;connected component;terabyte	DB	-22.951130550993266	-62.47199304624474	115326
de4f513b42da9afec89c2bdb03b3bb7e5f8b8524	wise-cluster: clustering e-commerce search engines automatically	document clustering;search engine;web pages;e commerce;keyword search	In this paper, we propose a new approach to automatically clustering e-commerce search engines (ESEs) on the Web such that ESEs in the same cluster sell similar products. This allows an e-commerce metasearch engine (comparison shopping system) to be built over the ESEs for each cluster. Our approach performs the clustering based on the features available on the interface page (i.e. the Web page containing the search form or interface) of each ESE. Special features that are utilized include the number of links, the number of images, terms appearing in the search form and normalized price terms. Our experimental results based on nearly 300 ESEs indicate that this approach can achieve good results.	categorization;cluster analysis;comparison shopping website;e-commerce;experiment;extensible storage engine;web page;web search engine;world wide web	Qian Peng;Weiyi Meng;Hai He;Clement T. Yu	2004		10.1145/1031453.1031473	document clustering;computer science;web page;data mining;database;world wide web;information retrieval;search engine	Web+IR	-30.107528418412002	-55.29473170168516	115356
eedf0f3bbfc4789933551366a8d9036e96b78caa	evaluation of the music ontology framework	music ontology framework;bbc music website;related description framework;real-world music-related user need;editorial data;music-related data;ontology evaluation framework;music ontology;real-world user need;dbtune project;ontological framework	The Music Ontology provides a framework for publishing structured music-related data on the Web, ranging from editorial data to temporal annotations of audio signals. It has been used extensively, for example in the DBTune project and on the BBC Music website. Until now it hasn’t been systematically evaluated and compared to other frameworks for handling music-related data. In this article, we design a ‘query-driven’ ontology evaluation framework capturing the intended use of this ontology. We aggregate a large set of real-world music-related user needs, and evaluate how much of it is expressible within our ontological framework. This gives us a quantitative measure of how well our ontology could support a system addressing these real-world user needs. We also provide some statistical insights in terms of lexical coverage for comparison with related description frameworks and identify areas within the ontology that could be improved.	aggregate data;knowledge representation and reasoning;ontology (information science);subject-matter expert;web ontology language;world wide web	Yves Raimond;Mark B. Sandler	2012		10.1007/978-3-642-30284-8_24	upper ontology;ontology alignment;bibliographic ontology;computer science;ontology;data mining;database;ontology-based data integration;world wide web;owl-s;information retrieval;process ontology;suggested upper merged ontology	Web+IR	-31.093606103026428	-60.21926273372384	115403
12fe7255ffc0221abf2571c8911a596784749963	extracting positive attributions from scientific papers	extraction information;learning algorithm;information extraction;base connaissance;algorithme apprentissage;acquisition connaissances;apprentissage machine;machine learning;knowledge acquisition;base conocimiento;adquisicion de conocimientos;algoritmo aprendizaje;extraccion informacion;knowledge base	The aim of our work is to provide support for reading (or skimming) scientific papers. In this paper we report on the task to identify concepts or terms with positive attributions in scientific papers. This task is challenging as it requires the analysis of the relationship between a concept or term and its sentiment expression. Furthermore, the context of the expression needs to be inspected. We propose an incremental knowledge acquisition framework to tackle these challenges. With our framework we could rapidly (within 2 days of an expert’s time) develop a prototype system to identify positive attributions in scientific papers. The resulting system achieves high precision (above 74%) and high recall rates (above 88%) in our initial experiments on corpora of scientific papers. It also drastically outperforms baseline machine learning algorithms trained on the same data.	algorithm;baseline (configuration management);document classification;experiment;information extraction;knowledge acquisition;knowledge base;machine learning;parsing expression grammar;precision and recall;prototype;scientific literature;supervised learning;text corpus;text mining	Son Bao Pham;Achim G. Hoffmann	2004		10.1007/978-3-540-30214-8_13	knowledge base;computer science;artificial intelligence;machine learning;data mining;database;information extraction	NLP	-22.853804703442982	-63.577410307043586	115551
fe11e37f0f735dbcf7f4c77e643f650229453c4e	semantic search at yahoo!	semantics;entity search;semantic web;web search;semantic search	Semantic search refers to a broad array of methods that aim to improve retrieval by interpreting queries beyond the traditional weighted bag of words model of document retrieval. In this talk, we will focus on the subset of these methods that rely on explicit semantic annotations, i.e. linking queries and text to items in a Knowledge Base. We will discuss techniques of entity linking on queries and documents, and the potential impact of these methods on improving performance on the classical ad-hoc document retrieval task. We will also discuss some novel tasks, including entity retrieval and related entity recommendations, and their implementation in Yahoo Search. We will close by considering some of the challenges that are specific to developing search services in a mobile context.	bag-of-words model;document retrieval;entity linking;hoc (programming language);knowledge base;semantic search	Peter Mika	2014		10.1145/2663712.2666198	explicit semantic analysis;semantic search;computer science;concept search;data mining;semantic web stack;database;information retrieval	Web+IR	-29.788840847520717	-61.31599087971014	115560
63dbd41886646d796ba5e72a08aaf8254147e745	overview of the author profiling task at pan 2013	articulo	This overview presents the framework and results for the Author Profiling task at PAN 2013. We describe in detail the corpus and its characteristics, and the evaluation framework we used to measure the participants performance to solve the problem of identifying age and gender from anonymous texts. Finally, the approaches of the 21 participants and their results are described.	bag-of-words model;binary prefix;blog;campus;chat log;collocation;dictionary;grams;html;multimodal interaction;n-gram;nec shun-ei;named entity;profiling (computer programming);robustness (computer science);skolem normal form;text corpus;vlc media player	Francisco M. Rangel Pardo;Paolo Rosso;Moshe Koppel;Efstathios Stamatatos;Giacomo Inches	2013			data mining;profiling (computer programming);computer science	NLP	-21.30806722943642	-60.3882502488575	115815
2abf9a01844a52fe4685e5eabbf4a5d5be644ee1	investigating segment-based query expansion for user-generated spoken content retrieval	discourse segmentation segment based query expansion user generated social multimedia spoken content retrieval standard information retrieval problem ir qe methods speech segment based methods semantic segmentation;thyristors speech multimedia communication standards semantics media streaming media;thyristors;standards;speech recognition content based retrieval document handling multimedia computing;semantics;speech;media;streaming media;multimedia communication	The very rapid growth in user-generated social multimedia content on online platforms is creating new challenges for search technologies. A significant issue for search of this type of content is its highly variable form and quality. This is compounded by the standard information retrieval (IR) problem of mismatch between search queries and target items. Query Expansion (QE) has been shown to be an effect technique to improve IR effectiveness for multiple search tasks. In QE, words from a number of relevant or assumed relevant top ranked documents from an initial search are added to the initial search query to enrich it before carrying out a further search operation. In this work, we investigate the application of QE methods for searching social multimedia content. In particular we focus on social multimedia content where the information is primarily in the audio stream. To address the challenge of content variability, we introduce three speech segment-based methods for QE using: Semantic segmentation, Discourse segmentation and Window-Based. Our experimental investigation illustrates the superiority of these segment-based methods in comparison to a standard full document QE method for a version of the MediaEval 2012 Search task newly extended as an adhoc search task.	c99;experiment;heuristic (computer science);image noise;information retrieval;internet;query expansion;spatial variability;streaming media;user-generated content;web search query	Ahmad Khwileh;Gareth J. F. Jones	2016	2016 14th International Workshop on Content-Based Multimedia Indexing (CBMI)	10.1109/CBMI.2016.7500268	thyristor;query expansion;media;computer science;speech;semantics;multimedia;world wide web;information retrieval	Web+IR	-25.094276297590383	-62.42988018896222	116092
95ede21fcf2ef56e8d8ac24a16145fc68049f48a	a personalized url re-ranking methodology using user's browsing behavior	search engine;web pages;user preferences;computational method;personalization;personalized search;search api;anchor text;re ranking	This paper proposes a personalized re-ranking of URLs returned by a search engine using user's browsing behaviors. Our personalization method constructs an index of the anchor text retrieved from the web pages that the user has clicked during his/her past searches. We propose a weight computation method that assigns different values to anchor texts according to user's browsing behaviors such as 'clicking' or 'downloading'. Experiment results show that our method can be practical for saving surfing time and effort to find users' preferred web pages.	browsing;vanity domain	Harshit Kumar;Sungjoon Park;Sanggil Kang	2008		10.1007/978-3-540-78582-8_22	anchor text;computer science;web page;personalization;internet privacy;world wide web;information retrieval;search engine	HCI	-30.208537535716136	-52.469165018463855	116110
413f142d6b31cec15df3e5a92dc16d95306623c9	reprint of: computational approaches for mining user's opinions on the web 2.0	opinion mining;text preprocessing;noisy text;data mining;user generated content	"""We carry out an empirical analysis to determine characteristics of social media channels.User generated content is """"noisy"""" and contains mistakes, emoticons, etc.We evaluate text preprocessing algorithms regarding user generated content.Discussion of improvements to opinion mining process. The emerging research area of opinion mining deals with computational methods in order to find, extract and systematically analyze people's opinions, attitudes and emotions towards certain topics. While providing interesting market research information, the user generated content existing on the Web 2.0 presents numerous challenges regarding systematic analysis, the differences and unique characteristics of the various social media channels being one of them. This article reports on the determination of such particularities, and deduces their impact on text preprocessing and opinion mining algorithms. The effectiveness of different algorithms is evaluated in order to determine their applicability to the various social media channels. Our research shows that text preprocessing algorithms are mandatory for mining opinions on the Web 2.0 and that part of these algorithms are sensitive to errors and mistakes contained in the user generated content."""	web 2.0;world wide web	Gerald Petz;Michal Karpowicz;Harald Fürschuß;Andreas Auinger;Václav Stríteský;Andreas Holzinger	2015	Inf. Process. Manage.	10.1016/j.ipm.2014.07.011	text mining;computer science;data science;machine learning;data mining;user-generated content;world wide web;information retrieval;sentiment analysis	DB	-22.74809303355509	-56.17605108161706	116112
24aaa4eab8ac32bc4533b2fd74734c3ddc910e89	an application for detecting network related problems from call center text data		A Network Service Provider can receive different network related problems and complaints from various communication channels regarding their service activity in certain regions. One of the major and most important communication channels is customer call center. Detecting network related problems that customers are notifying during these calls are significant in order to provide solutions and increase customer satisfaction. However, due to sheer volume of the call records that are converted to text, it is quite difficult to analyze whole data using traditional approaches. In this paper, we study a topic modeling approach for detecting network related problems from call center text data. The analysis results demonstrate that for a major broadband service providers' personal Internet at home tariff, most of calls received in a customer call center is related to information, whereas the second majority of all calls are related to faults that are network related issues. These results signify the existence of network and service related issues in service providers' infrastructure.	apache spark;big data;deep learning;internet;network service provider;sensor;text corpus;topic model;unsupervised learning	Ibrahim Onuralp Yigit;Engin Zeydan;Ahmet Feyzi Ates	2017	2017 IEEE International Black Sea Conference on Communications and Networking (BlackSeaCom)	10.1109/BlackSeaCom.2017.8277710	the internet;topic model;network service;service provider;customer satisfaction;computer network;broadband;business;tariff;text mining	HPC	-20.818919426512412	-54.385723848154996	116259
9135d61e77345935ee7f6586f4f39483a2bbb740	integrating the optimal classifier set for sentiment analysis		Automatic identification of users’ sentiment is important for many Web applications, such as recommender systems and business intelligence. Sentiment analysis can be treated as a classification task, which tries to identify the user’s overall sentiment expressed in documents. But it is difficult for users to select a classifier for a special analyzed domain, since each classifier would achieve various performances in different domains. Thus, we proposed a three phase solution of multiple classifiers for sentiment analysis, in which an optimal set of classifiers is selected and integrated automatically. An approximate algorithm is designed to tackle the Combinatorial Explosion Problem of classifier set selection, which can be proven to be 2-approximation. At last, extensive experiments carried out on real-world datasets show that the proposed solution outperforms not only the best single classifier methods, but also the state-of-art competitors of ensemble learning.	approximation algorithm;automatic identification and data capture;ensemble learning;experiment;feature vector;greedy algorithm;n-gram;performance;point of sale;recommender system;sentiment analysis;statistical classification;user-generated content;world wide web	Yuming Lin;Xiaoling Wang;You Li;Aoying Zhou	2015	Social Network Analysis and Mining	10.1007/s13278-015-0295-8	margin classifier;quadratic classifier;computer science;machine learning;pattern recognition;data mining;sentiment analysis	Web+IR	-19.540468304930283	-65.16733106007526	116381
0312a5d98b42d6427ebb2ead94b0a8b0823236c3	opinion detection in blogs: what is still missing?	social network services;blogosphere;information retrieval opinion detection opinion prediction blogs blogosphere social networking polarity detection subjectivity;polarity detection;social networking;opinion detection;information retrieval;lexical based approaches;major element;opinion prediction;social network;estimation;machine learning;social networking online;lexical based approaches opinion detection social network blogosphere machine learning;face;learning artificial intelligence;subjectivity;social networking online learning artificial intelligence;blogs;time frequency analysis;context;blogs social network services context estimation machine learning time frequency analysis face	In recent years, a lot of work has been done in the field of Opinion Detection in blogs but most of the research is based on machine learning or lexical based approaches. The objective of this paper is to focus on Social Network based evidences that can be exploited for the task of Opinion Detection. We propose a framework that makes use of the major elements of the blogosphere for extracting opinions from blogs. Besides this, we highlight the tasks of opinion prediction and multidimensional ranking. In addition, we also discuss the challenges that researchers might face while realizing the proposed framework. At the end, we demonstrate the importance of social networking evidences by performing experimentation.	blog;blogosphere;machine learning;social network	Malik Muhammad Saad Missen;Mohand Boughanem;Guillaume Cabanac	2010	2010 International Conference on Advances in Social Networks Analysis and Mining	10.1109/ASONAM.2010.59	social science;computer science;data science;data mining;mathematics;world wide web;social network	ML	-22.500055376490092	-57.3316726682093	116471
5dd0ae971c88a817bb46160d1afc8af3c09fa69d	identifying, indexing, and ranking chemical formulae and chemical names in digital documents	search and retrieval;digital documents;search engine;support vector machines;conditional random fields;ranking;chemical name;query models;indexation;hierarchical text segmentation;conditional random field;index pruning;entity extraction;support vector machine;similarity search;text segmentation;independent frequent subsequence;chemical formula	End-users utilize chemical search engines to search for chemical formulae and chemical names. Chemical search engines identify and index chemical formulae and chemical names appearing in text documents to support efficient search and retrieval in the future. Identifying chemical formulae and chemical names in text automatically has been a hard problem that has met with varying degrees of success in the past. We propose algorithms for chemical formula and chemical name tagging using Conditional Random Fields (CRFs) and Support Vector Machines (SVMs) that achieve higher accuracy than existing (published) methods. After chemical entities have been identified in text documents, they must be indexed. In order to support user-provided search queries that require a partial match between the chemical name segment used as a keyword or a partial chemical formula, all possible (or a significant number of) subformulae of formulae that appear in any document and all possible subterms (e.g., “methyl”) of chemical names (e.g., “methylethyl ketone”) must be indexed. Indexing all possible subformulae and subterms results in an exponential increase in the storage and memory requirements as well as the time taken to process the indices. We propose techniques to prune the indices significantly without reducing the quality of the returned results significantly. Finally, we propose multiple query semantics to allow users to pose different types of partial search queries for chemical entities. We demonstrate empirically that our search engines improve the relevance of the returned results for search queries involving chemical entities.	algorithm;approximate string matching;chemical database;conditional random field;entity;experiment;fuzzy logic;gate;high-throughput satellite;indexed grammar;prune and search;query expansion;relevance;requirement;response time (technology);substring;support vector machine;time complexity;web search engine;web search query;webserver directory index	Bingjun Sun;Prasenjit Mitra;C. Lee Giles;Karl T. Mueller	2011	ACM Trans. Inf. Syst.	10.1145/1961209.1961215	support vector machine;computer science;machine learning;pattern recognition;data mining;database;world wide web;conditional random field;information retrieval	Web+IR	-31.607453715867795	-58.41254439427839	116675
09f54d4af38d2c47632bf379860d5ca1ba996083	an investigation into the interaction between feature selection and discretization: learning how and when to read numbers	naive bayes;machine learning;feature selection	Pre-processing is an important part of machine learning, and has been shown to significantly improve the performance of classifiers. In this paper, we take a selection of pre-processing methods—focusing specifically on discretization and feature selection—and empirically examine their combined effect on classifier performance. In our experiments, we take 11 standard datasets and a selection of standard machine learning algorithms, namely one-R, ID3, naive Bayes, and IB1, and explore the impact of different forms of preprocessing on each combination of dataset and algorithm. We find that in general the combination of wrapper-based forward selection and naive supervised methods of discretization yield consistently above-baseline results.	baseline (configuration management);discretization;experiment;feature selection;id3 algorithm;machine learning;naive bayes classifier;preprocessor;stepwise regression;supervised learning	Sumukh Ghodke;Timothy Baldwin	2007		10.1007/978-3-540-76928-6_7	naive bayes classifier;computer science;machine learning;pattern recognition;data mining;supervised learning;feature selection	NLP	-20.056601891623863	-66.01952078933789	116775
d27f68bee69f592cf49ed440ca88d80e18d3206b	a comparison of classifiers and features for authorship authentication of social networking messages	social networking;classification;machine learning;stylometry;cybersecurity;authorship authentication	SummaryrnThis paper develops algorithms and investigates various classifiers to determine the authenticity of short social network postings, an average of 20.6 words, from Facebook. This paper presents and discusses several experiments using a variety of classifiers. The goal of this research is to determine the degree to which such postings can be authenticated as coming from the purported user and not from an intruder. Various sets of stylometry and ad hoc social networking features were developed to categorize 9259 posts from 30 Facebook authors as authentic or non-authentic. An algorithm to utilize machine-learning classifiers for investigating this problem is described, and an additional voting algorithm that combines three classifiers is investigated. This research is one of the first works that focused on authorship authentication in short messages, such as postings on social network sites. The challenges of applying traditional stylometry techniques on short messages are discussed. Experimental results demonstrate an average accuracy rate of 79.6% among 30 users. Further empirical analyses evaluate the effect of sample size, feature selection, user writing style, and classification method on authorship authentication, indicating varying degrees of success compared with previous studies. Copyright © 2016 John Wiley u0026 Sons, Ltd.	authentication	Jenny S. Li;Li-Chiou Chen;John V. Monaco;Pranjal Singh;Charles C. Tappert	2017	Concurrency and Computation: Practice and Experience	10.1002/cpe.3918	distributed computing;computer science;categorization;feature selection;data mining;voting;stylometry;writing style;social network;authentication	OS	-19.927974490545864	-57.238763020447614	116782
dfa6a1cb801fd09f958289e48b0695755cf44b06	unsupervised learning for lexicon-based classification		In lexicon-based classification, documents are assigned labels by comparing the number of words that appear from two opposed lexicons, such as positive and negative sentiment. Creating such words lists is often easier than labeling instances, and they can be debugged by non-experts if classification performance is unsatisfactory. However, there is little analysis or justification of this classification heuristic. This paper describes a set of assumptions that can be used to derive a probabilistic justification for lexicon-based classification, as well as an analysis of its expected accuracy. One key assumption behind lexicon-based classification is that all words in each lexicon are equally predictive. This is rarely true in practice, which is why lexicon-based approaches are usually outperformed by supervised classifiers that learn distinct weights on each word from labeled instances. This paper shows that it is possible to learn such weights without labeled data, by leveraging co-occurrence statistics across the lexicons. This offers the best of both worlds: light supervision in the form of lexicons, and data-driven classification with higher accuracy than traditional word-counting heuristics. Introduction Lexicon-based classification refers to a classification rule in which documents are assigned labels based on the count of words from lexicons associated with each label (Taboada et al. 2011). For example, suppose that we have opposed labels Y ∈ {0, 1}, and we have associated lexiconsW0 and W1. Then for a document with a vector of word counts x, the lexicon-based decision rule is,	binary classification;debugging;heuristic (computer science);horner's method;lexicon;machine learning;multiclass classification;statistical classification;supervised learning;unsupervised learning;word lists by frequency	Jacob Eisenstein	2017			natural language processing;computer science;machine learning;pattern recognition	AI	-19.637130369648755	-65.7166573290876	116864
1bbe7a0297c1829ba96827e218b19696cae57f20	weakly-supervised acquisition of open-domain classes and class attributes from web documents and query logs	web documents;information extraction;large scale;query logs	A new approach to large-scale information extraction exploits both Web documents and query logs to acquire thousands of opendomain classes of instances, along with relevant sets of open-domain class attributes at precision levels previously obtained only on small-scale, manually-assembled classes.	information extraction;web page;world wide web	Marius Pasca;Benjamin Van Durme	2008			natural language processing;query expansion;web query classification;ranking;computer science;database;web search query;world wide web;information extraction;information retrieval	NLP	-28.21275204310791	-64.16527727576742	116994
21fc2613d529170271106e3bddef4d811f7b0f31	irit at imageclef 2011: medical retrieval task		In this paper, we reported some experiments conducted by our members in the SIG team at the IRIT laboratory in the University of Toulouse within the context of the medical information retrieval (IR) task. As in our previous participation in ImageCLEF, in 2011, our research focuses on the case-based retrieval task. We compared the performance of different state-of-the-art term weighting models for retrieving patient cases that might best suit the clinical information need. Furthermore, we also combined term scores obtained by two state-of-the-art weighting models using a particular data fusion technique. Finally, a state-of-the-art query expansion (QE) technique is used for improving biomedical IR performance.	experiment;information needs;information retrieval;query expansion	Duy Dinh;Lynda Tamine	2011			computer science;data mining;multimedia;information retrieval	Web+IR	-32.302590941584654	-63.47869722251078	117218
0fbc5f27a37f851361a4e9d8275aaafb10920d5f	quality flaw prediction in spanish wikipedia: a case of study with verifiability flaws		Abstract In this work, we present the first quality flaw prediction study for articles containing the two most frequent verifiability flaws in Spanish Wikipedia: articles which do not cite any references or sources at all (denominated Unreferenced ) and articles that need additional citations for verification (so-called Refimprove ). Based on the underlying characteristics of each flaw, different state-of-the-art approaches were evaluated. For articles not citing any references, a well-established rule-based approach was evaluated and interesting findings show that some of them suffer from Refimprove flaw instead. Likewise, for articles that need additional citations for verification, the well-known PU learning and one-class classification approaches were evaluated. Besides, new methods were compared and a new feature was also proposed to model this latter flaw. The results showed that new methods such as under-bagged decision trees with sum or majority voting rules, biased-SVM, and centroid-based balanced SVM, perform best in comparison with the ones previously published.		Edgardo Ferretti;Leticia Cecilia Cagnina;Viviana Paiz;Sebastián Delle Donne;Rodrigo Zacagnini;Marcelo Luis Errecalde	2018	Inf. Process. Manage.	10.1016/j.ipm.2018.08.003	support vector machine;information retrieval;data mining;pu learning;decision tree;computer science;majority rule	NLP	-20.103545754963875	-63.61087619743179	117460
2ed7fb49cdafbe4cf93951f93ca7ed9763ddb0b2	mining user-generated comments	databases;nested comments extraction user generated comment mining social media web sites newspapers blogs forums user generated comment generation user generated comment exchange opinion mining descriptive annotations information extraction user generated comments html template html document unsupervised extraction nested answer extraction commentsminer unsupervised users comments extraction theoretical framework frequent subtree mining data extraction comment mining task constrained closed induced subtree mining problem learning to rank problem plain comment extraction;social networking online data mining hypermedia markup languages learning artificial intelligence;data mining;companies;vegetation;html;feature extraction;data mining vegetation feature extraction html user generated content databases companies;user generated content	Social-media websites, such as newspapers, blogs, and forums, are the main places of generation and exchange of user-generated comments. These comments are viable sources for opinion mining, descriptive annotations and information extraction. User-generated comments are formatted using a HTML template, they are therefore entwined with the other information in the HTML document. Their unsupervised extraction is thus a taxing issue - even greater when considering the extraction of nested answers by different users. This paper presents a novel technique (CommentsMiner) for unsupervised users comments extraction. Our approach uses both the theoretical framework of frequent subtree mining and data extraction techniques. We demonstrate that the comment mining task can be modelled as a constrained closed induced subtree mining problem followed by a learning-to-rank problem. Our experimental evaluations show that CommentsMiner solves the plain comments and nested comments extraction problems for 84% of a representative and accessible dataset, while outperforming existing baselines techniques.	algorithm;blog;comment (computer programming);frequent subtree mining;html;information extraction;learning to rank;sensitivity and specificity;surrogate model;tree (data structure);user-generated content	Julien Subercaze;Christophe Gravier;Frédérique Laforest	2015	2015 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)	10.1109/WI-IAT.2015.138	html;feature extraction;computer science;machine learning;data mining;knowledge extraction;user-generated content;world wide web;information retrieval;vegetation	NLP	-25.333397695802603	-54.49502868719934	117622
7fd060836c30607f1e1e5eee17ad262c031d6a16	exploiting geotagged resources for spatial clustering on social network services	naive bayes;geotagged resources;classification;multiple svm;spatial folksonomy	Nowadays, it has become common for users to geotag resources on many online social networking services. However, a large amount of data exists on social network services without annotations of their geographical location. Thus, it would be useful to tag these resources with geotags. This paper proposes a method to predict the location of unlabeled resources on social networking services. We use the Naive Bayes and support vector machine methods to classify the resources that are collected by using the term frequency of the tags in each class. In addition, we improve the calculation for these methods by using the values of the term frequency, and we invert the class frequency to optimize the input data. These results can be applied to tag unlabeled resources on social networking services. Copyright © 2015 John Wiley & Sons, Ltd.	cluster analysis;geotagging;social network	Jason J. Jung	2016	Concurrency and Computation: Practice and Experience	10.1002/cpe.3634	naive bayes classifier;biological classification;computer science;data science;machine learning;data mining;database	DB	-23.200258825194993	-52.2132625951549	117689
17c0ba6f509fedb9486209e019ff397f787e4487	visualizing large-scale human collaboration in wikipedia	co authoring;wikipedia;category;information visualization;visualization of collaborative processes applications	Volunteer-driven large-scale human-to-human collaboration has become common in the Web 2.0 era. Wikipedia is one of the foremost examples of such large-scale collaboration, involving millions of authors writing millions of articles on a wide range of subjects. The collaboration on some popular articles numbers hundreds or even thousands of co-authors. We have analysed the co-authoring across entire Wikipedias in different languages and have found it to follow a geometric distribution in all the language editions we studied. In order to better understand the distribution of co-author counts across different topics, we have aggregated content by category and visualized it in a form resembling a geographic map. The visualizations produced show that there are significant differences of co-author counts across different topics in all the Wikipedia language editions we visualized. In this article we describe our analysis and visualization method and present the results of applying our method to the English, German, Chinese, Swedish and Danish Wikipedias. We have evaluated our visualization against textual data and found it to be superior in usability, accuracy, speed and user preference.	big data;cloud computing;exabyte;foremost;server (computing);terabyte;text corpus;usability;web 2.0;wiki;wikidata;wikipedia;world wide web	Robert P. Biuk-Aghai;Patrick Cheong-Iao Pang;Yain-Whar Si	2014	Future Generation Comp. Syst.	10.1016/j.future.2013.04.001	information visualization;category;computer science;data science;brand;database;world wide web;information retrieval	HCI	-24.71803895196161	-55.993092537385145	117746
85b1bb5356ef303f4c3d3e1df32caeeb8a7c7a8d	unsupervised learning of general-specific noun relations from the web	unsupervised learning;noun;weighted graph;quantitative evaluation	In this paper, we propose a new methodology based on directed weighted graphs and the TextRank algorithm to automatically induce general-specific noun relations from web corpora frequency counts. Different asymmetric association measures are implemented to build the graphs upon which the TextRank algorithm is applied and produces an ordered list of nouns from the most general to the most specific. Experiments are conducted based on the WordNet noun hierarchy and a quantitative evaluation is proposed.	algorithm;experiment;text corpus;unsupervised learning;wordnet;world wide web	Gaël Dias;Raycho Mukelov;Guillaume Cleuziou	2008			natural language processing;unsupervised learning;noun;computer science;machine learning;pattern recognition	NLP	-26.307659979025658	-64.99825400536113	117837
24eb97792a7e2c7cdffa01db3c1414667833b52d	an evaluation corpus for temporal summarization	fine-grained level;topic tracking;standard evaluation;user judgment;multi-document summary;main event;entire collection;large number;evaluation corpus;temporal summarization;recent year	In recent years, a lot of work has been done in the eld of Topic Tracking. The focus of this work has been on identifying stories belonging to the same topic. This might result in a very large number of stories being reported to the user. It might be more useful to a user if a summary of the main events in the topic rather than the entire collection of stories related to the topic were presented. Though work on such a ne-grained level has been started, there is currently no standard evaluation testbed available to measure the accuracy of such techniques. We describe a scheme for developing a testbed of user judgments which can be used to evaluate the above mentioned techniques. The corpus that we have created can also be used to evaluate single or multi-document summaries.	testbed;text corpus	Vikas Khandelwal;Rahul Gupta;James Allan	2001			computer science;data mining;multimedia;information retrieval	Web+IR	-24.927754449875298	-59.89740321718618	118102
6e851882830e6835e671e331a6ef7b0084320efd	an effective semantic event matching system in the internet of things (iot) environment	iot;event matching;publish/subscribe;semantic	IoT sensors use the publish/subscribe model for communication to benefit from its decoupled nature with respect to space, time, and synchronization. Because of the heterogeneity of communicating parties, semantic decoupling is added as a fourth dimension. The added semantic decoupling complicates the matching process and reduces its efficiency. Our proposed algorithm clusters subscriptions and events according to topic and performs the matching process within these clusters, which increases the throughput by reducing the matching time from the range of 16-18 ms to 2-4 ms. Moreover, the accuracy of matching is improved when subscriptions must be fully approximated, as demonstrated by an over 40% increase in F-score results. This work shows the benefit of clustering, as well as the improvement in the matching accuracy and efficiency achieved using this approach.	approximation algorithm;cluster analysis;coupling (computer programming);dijkstra's algorithm;f1 score;internet of things;matching;publish–subscribe pattern;throughput;sensor (device);statistical cluster	Noura Alhakbani;Mohammad Mehedi Hassan;Mourad Ykhlef	2017		10.3390/s17092014	throughput;decoupling (cosmology);cluster analysis;publication;data mining;synchronization;fourth dimension;internet of things;computer science	Mobile	-22.793317983942238	-61.95102377907164	118125
168a5763720cb283155205d76edca4357595de73	semantic matching in app search	app store;tag;app search;semantic matching;topic model	Past years, with the growth of smart-phones and applications, APP market has become an important mobile internet portal. As an important function in application market, APP search gains lots of attentions.However, mismatch between queries and APP is the most critical problem in APP search because of less text within term matching search engine. In this talk, we describe a semantic matching architecture in APP search--which mining topics and tags in big data. It enriches query and APP representations with topics and tags to achieve semantic matching in search. Some challenge must be considered: 1) How to extract tag-APP relationship from large web text. 2) How to use machine learning technologies to process de-noising and computing confidence. 3) How to hybrid ranking apps retrieved by different matching method. These will be introduced in some of our related works and as examples to describe how semantic matching is used in Tencent MyApp, an application market which serving hundreds of millions of users.	big data;machine learning;semantic matching;smartphone;tencent qq;web search engine	Juchao Zhuo;Zeqian Huang;Yunfeng Liu;Zhanhui Kang;Xun Cao;Mingzhi Li;Long Jin	2015		10.1145/2684822.2697046	semantic search;computer science;machine learning;data mining;topic model;world wide web;information retrieval	ML	-23.588104681098173	-56.89426096067378	118166
fc59db54a148b902426b03d1266d9404d2681668	on partitioning for ontology alignment.		Ontology Alignment (OA) is the process of determining the mappings between two ontologies. A number of systems currently exists and many of them are participating in the annual Ontology Alignment Evaluation Initiative (OAEI).3 Ontology alignment for two very large ontologies becomes time consuming and memory intensive. For example, the largebio track in the OAEI campaign still poses serious challenges to participants and only 4 out of 11 systems managed to complete the largest largebio task. A general approach to address these challenges is to partition each ontology into cohesive blocks. The matching task is then divided into smaller tasks involving only relevant pair of blocks (i.e., partitions). Ontology partitioning brings new challenges: how best to partition each ontology into blocks and whether the partitioning process on each ontology should be independent of each other. Three main strategies exist: (i) totally independent partitioning of both ontologies using various clustering algorithms, (ii) independent partitioning of the better structured ontology and then use its partitioning to direct the partitioning of the other, and (iii) dependent partitioning between the two using a quick and efficient initial mapping of the two and then this mapping directs their partitioning. A preliminary study of these three partitioning strategies and their effects on ontology alignment is presented. The objective of this preliminary work is to determine the suitability of these strategies to improve the performance of OA systems when dealing with large ontologies, especially those unable to cope with the largest tasks.	algorithm;cluster analysis;ontology (information science);ontology alignment	Sunny Pereira;Valerie V. Cross;Ernesto Jiménez-Ruiz	2017			database;data mining;ontology alignment;computer science	AI	-29.624694054333514	-62.27141707031534	118277
d79a19b72b58b6787f67e9b4c77dcb16d150e1fb	towards monitoring of novel statements in the news	novelty detection;statement extraction;semantic novelty measures	In media monitoring users have a clearly defined information need to find so far unknown statements regarding certain entities or relations mentioned in natural-language text. However, commonly used keyword-based search technologies are focused on finding relevant documents and cannot judge the novelty of statements contained in the text. In this work, we propose a new semantic novelty measure that allows to retrieve statements, which are both novel and relevant, from natural-language sentences in news articles. Relevance is defined by a semantic query of the user, while novelty is ensured by checking whether the extracted statements are related, but non-existing in a knowledge base containing the currently known facts. Our evaluation performed on English news texts and on CrunchBase as the knowledge base demonstrates the effectiveness, unique capabilities and future challenges of this novel approach to novelty.	entity;information needs;knowledge base;natural language;relevance;semantic query;techcrunch	Michael Färber;Achim Rettinger;Andreas Harth	2016		10.1007/978-3-319-34129-3_18	computer science;data mining;world wide web;information retrieval	Web+IR	-29.007774801585143	-63.921021798664235	118378
7be0297e89c652627bd5b6766d0d720acc2a2697	multi-document text summarization in e-learning system for operating system domain		The query answering in E-learning systems generally mean retrieving relevant answer for the user query. In general the conventional E-learning systems retrieve answers from their inbuilt knowledge base. This leads to the limitation that the system cannot work out of its bound i.e. it does not answer for a query whose contents are not in the knowledge base. The proposed system overcomes this limitation by passing the query online and carrying out multi-document summarization on online documents. The proposed system is a complete E-learning system for the domain Operating systems. The system avoids the need to maintain the knowledge base thus reducing the space complexity. A similarity check followed by multi-document summarization leads to a non-redundant answer. The queries are classified into simple and complex types. Brief answers are retrieved for simple queries whereas detailed answers are retrieved for complex queries.		S. Saraswathi;M. Hemamalini;S. Janani;Veeramani Priyadharshini	2011		10.1007/978-3-642-22726-4_19	text graph;speech recognition;automatic summarization;world wide web;information retrieval	AI	-33.14609360035851	-60.06370643604994	118580
b99e0fdcfd3c8cc6f34e4d3ad667f963e9e917fd	estimating the credibility of examples in automatic document classification	genetic program;classification algorithm;genetic programming;document classification;bayes classifier;credibility;automatic document classification	Classification algorithms usually assume that any example in the training set should contribute equally to the classification model being generated. However, this is not always the case. This paper shows that the contribution of an example to the classification model varies according to many factors, which are application dependent, and can be estimated using what we call a credibility function. The credibility of an entity reflects how much value it aggregates to a task being performed, and here we investigate it in Automatic Document Classification, where the credibility of a document relates to its terms, authors, citations, venues, time of publication, among others. After introducing the concept of credibility in classification, we investigate how to estimate a credibility function using information regarding documents content, citations and authorship using mainly metrics previously defined in the literature. As the credibility of the content of a document can be easily mapped to any other classification problem, in a second phase we focus on content-based credibility functions. We propose a genetic programming algorithm to estimate this function based on a large set of metrics generally used to measure the strength of term-class relationship. The proposed and evolved credibility functions are then incorporated to the Näıve Bayes classifier, and applied to four text collections, namely ACM-DL, Reuters, Ohsumed, and 20 Newsgroup. The results obtained showed significant improvements in both microF1 and macro-F1, with gains up to 21% in Ohsumed when compared to the traditional Näıve Bayes.		João R. M. Palotti;Thiago Salles;Gisele Lobo Pappa;Filipe de Lima Arcanjo;Marcos André Gonçalves;Wagner Meira	2010	JIDM		computer science;pattern recognition;data mining;information retrieval	Web+IR	-21.49169071369511	-62.534698775815194	118712
76e1352af80fd9ca3b6ebd0cc86c2eb120ec800c	a :) is worth a thousand words: how people attach sentiment to emoticons and words in tweets	sentiment classification;text analysis;social networking online;pattern classification;emoticons;twitter	Emoticons are widely used to express positive or negative sentiment on Twitter. We report on a study with live users to determine whether emoticons are used to merely emphasize the sentiment of tweets, or whether they are the main elements carrying the sentiment. We found that the sentiment of an emoticon is in substantial agreement with the sentiment of the entire tweet. Thus, emoticons are useful as predictors of tweet sentiment and should not be ignored in sentiment classification. However, the sentiment expressed by an emoticon agrees with the sentiment of the accompanying text only slightly better than random. Thus, using the text accompanying emoticons to train sentiment models is not likely to produce the best results, a fact that we show by comparing lexicons generated using emoticons with others generated using simple textual features.	algorithm;emoticon;emotion markup language;ground truth;lexicon;sentiment analysis;usability testing	Marina Boia;Boi Faltings;Claudiu Cristian Musat;Pearl Pu	2013	2013 International Conference on Social Computing	10.1109/SocialCom.2013.54	text mining;computer science;internet privacy;world wide web;sentiment analysis	NLP	-22.130887524487818	-59.53575092011673	118748
4d742ea01c7826313a2e494e7efc94b5ad33a6d8	instance filtering for entity recognition	text mining;supervised classification;text analysis;learning system;comparative study;prediction accuracy;natural language processing;named entity	In this paper we propose Instance Filtering as preprocessing step for supervised classification-based learning systems for entity recognition. The goal of Instance Filtering is to reduce both the skewed class distribution and the data set size by eliminating negative instances, while preserving positive ones as much as possible. This process is performed on both the training and test set, with the effect of reducing the learning and classification time, while maintaining or improving the prediction accuracy. We performed a comparative study on a class of Instance Filtering techniques, called Stop Word Filters, that simply remove all the tokens belonging to a list of stop words. We evaluated our approach on three different entity recognition tasks (i.e. Named Entity, Bio-Entity and Temporal Expression Recognition) in English and Dutch, showing that both the skewness and the data set size are drastically reduced. Consequently, we reported an impressive reduction of the computation time required for training and classification, while maintaining (and sometimes improving) the prediction accuracy.	computation;machine learning;named entity;preprocessor;software maintenance;supervised learning;test set;time complexity	Alfio Massimiliano Gliozzo;Claudio Giuliano;Raffaella Rinaldi	2005	SIGKDD Explorations	10.1145/1089815.1089818	text mining;computer science;machine learning;comparative research;pattern recognition;data mining	AI	-20.19790053818358	-64.82668198636323	118856
837b071d4c83e0f90d3e6b6edc1a2d4ca35711fb	tagme: on-the-fly annotation of short text fragments (by wikipedia entities)	search engine;semantic annotation;wikipedia;text mining;word sense disambiguation;on the fly;bag of words	We designed and implemented TAGME, a system that is able to efficiently and judiciously augment a plain-text with pertinent hyperlinks to Wikipedia pages. The specialty of TAGME with respect to known systems [5,8] is that it may annotate texts which are short and poorly composed, such as snippets of search-engine results, tweets, news, etc.. This annotation is extremely informative, so any task that is currently addressed using the bag-of-words paradigm could benefit from using this annotation to draw upon (the millions of) Wikipedia pages and their inter-relations.	bag-of-words model;entity;hyperlink;information;programming paradigm;relevance;web search engine;wikipedia	Paolo Ferragina;Ugo Scaiella	2010		10.1145/1871437.1871689	natural language processing;text mining;computer science;bag-of-words model;brand;world wide web;information retrieval;search engine	Web+IR	-28.54183554596765	-63.59124932673345	119018
4e0695048b4d3c2104bc1813e53d00fe62b30e98	can twitter predict royal baby's name ?		In this paper, we analyze the existence of possible correlation between public opinion of twitter users and the decision-making of persons who are influential in the society. We carry out this analysis on the example of the discussion of probable name of the British crown baby, born in July, 2013. In our study, we use the methods of quantitative processing of natural language, the theory of frequent sets, the algorithms of visual displaying of users' communities. We also analyzed the time dynamics of keyword frequencies. The analysis showed that the main predictable name was dominating in the spectrum of names before the official announcement. Using the theories of frequent sets, we showed that the full name consisting of three component names was the part of top 5 by the value of support. It was revealed that the structure of dynamically formed users' communities participating in the discussion is determined by only a few leaders who influence significantly the viewpoints of other users.	algorithm;association rule learning;blog;crown group;frequency band;natural language processing;prince;social network	Bohdan Pavlyshenko	2013	CoRR		computer science;data mining;world wide web	ML	-22.852799694814316	-52.93484804736906	119225
01984b611f8478f7ab5ba021389450aa4c3508b1	spoken document retrieval using extended query model and web documents		This paper proposes a novel approach for spoken document retrieval. In our method, a query model which is one of the probabilistic language models is adopted, in order to computes a probability to generate a given query from each document. We employ not only a “static” document collection consisting of targeted documents but also a “dynamic” document collection including web documents related with queries. We expand the query model so as to incorporate probabilities obtained from “static” and “dynamic” language models using the Dirichlet smoothing. Furthermore, in order to improve retrieved results, we develop a weighting method for web documents. Experiments using NTCIR-9 SpokenDoc Dry-run and NTCIR-10 SpokenDoc-2 Formal-run were conducted, and it is found our proposed scheme has enough performance compared with conventional methods.	archive;dirichlet kernel;document retrieval;dry run (testing);language model;smoothing;web page	Kiichi Hasegawa;Masanori Takehara;Satoshi Tamura;Satoru Hayamizu	2013			query expansion;web query classification;ranking (information retrieval);web search query;information retrieval;query language;language model;document retrieval;document clustering;computer science	Web+IR	-28.59730414829676	-61.551759232384356	119247
3ee3f714046a5f4e75e777f40a6679ce8f714455	applying multiple characteristics and techniques to obtain high levels of performance in information retrieval at ntcir-4	information retrieval system;average precision;information retrieval;statistical test;poisson model	"""Our information retrieval systemtakes advantage of numerouscharacteristicsof theinformationandappliesnumeroussophisticatedtechniques.Robertson’ s 2-PoissonmodelandRocchio’sformula,bothof which areknownto beeffective, havebeenappliedin thesystem.Characteristicsof newspaper ssuch aslocational informationwere applied.We presentour application of Fujita’smethod,where longer termsare usedin retrieval by thesystembut de-emphasized relativeto the emphasison the shortestterms; this allows us to use bothcompoundandsingle-word terms.Thestatistical testusedin expandingqueriesthroughan automatic feedback processis described. The methodgivesus termswhich havebeenstatisticallyconfirmedto berelated to the top-rankeddocumentsthat were obtained in the first retrieval. We also useda numericalterm QIDF, which is anIDF termfor queries.It hasa function to decreasethe scoresfor stopwords that occur in manyqueries.It canbeveryusefulfor foreignlanguagesfor which we cannotexaminestopwords. We participatedin threetasks(Korean,Japanese , andEnglish) of monolingualinformationretrieval at NTCIR 4. We obtainedrelativelyhigherprecisionsin all the tasksin which we participated. In particular, we obtainedthebestprecisionin Koreandescription-based monolingualinformationretrieval. Keywords: Monolingual IR, Locational information, De-emphasisof longer terms, Statistical test, QIDF 1 Introduction Our information retrieval system has taken advantageof numerouscharacteristicsof the information and appliednumeroussophisticatedtechniques. Robertson’ s 2-PoissonmodelandRocchio’s formula, both of which are known to be very effective, have beenappliedin the system.We usedsuchcharacter istics of newspapersas locational information. This methodis very effective in retrieval from collections of newspaperarticles, suchas the documentset for NTCIR 4. We appliedFujita’s method,wherelonger termsareusedin retrieval by the systembut areassignedlower weightsthanthe shortestterms;this allowsusto usecompoundtermsaswell assingle-word terms. We also useda statisticaltest in expanding queriesthroughan automaticfeedbackprocess.This methodgives us termswhich have beenstatistically confirmedto be relatedto the top-ranked documents thatwereobtainedin the first retrieval. We alsoused a numerical term QIDF, which is an IDF term for queries. It hasa function to decreasethe scoresfor stopwords that occursin many queries. We applied thesystemto thethreetasksof monolingualinformation retrieval at NTCIR 4, referredto asJJ,KK, and EE. Oursystemobtainedrelatively higherprecisions in all thetasksin which we participated.In particular, we obtainedthebestprecisionin Koreandescriptionbasedmonolingualinformationretrieval. JJ meansJapanesemonolingual information retrieval, KK meansKoreanmonolingualinformationretrieval,andEEmeansEnglish monolingualinformationretrieval. 2 Outline of our system Our systemusesRobertson’ s 2-Poissonmodel[6], which is a probabilistic approach. In Robertson’ s method,eachdocument’ sscoreis calculatedby using the following equation. The documentsthat obtain high scoresarethenoutputasthe resultsof retrieval. below is thescoreof adocument against aquery . ! """" #%$ term & in ! '( &*) + & """" &*) + & """"-,/.10 2 4365 &87 + """" 9 : 2 ;5=< ) & """" : &*)4> +!1 & """" &*);> !1 & """" ,?. !-@ (1) where A indicatesa term that appearsin a query. A4B A is thefrequency of A in adocument , A4BDC * A is thefrequency of A in a query , B A is thenumber of thedocumentsin which A appears, and E is thetotal numberof documents, F HGJI A4K 6 is thelengthof a document , and L is theaveragelengthof thedocuments.M N and M C areconstantswhicharesetaccording to theresultsof experiments. In this equation,we call &*) & """" &*) & """"-,O. 0 2 4365 &87 + """" 9 the TF term, (abbr. PRQ * S A ), 2 5UT V+WHX 0ZY the IDF term, (abbr. [6\OQ A ), and 0 W^] X >`_ 0ZY 0 W^]`X >`_ 0ZY aSb > the TFC term (abbr. PcQ C A ). In oursystem,severaltermsareaddedto extendthis equation,andthemethodfor doingthisisexpressedby thefollowing equation. d + e ! """" #gfhhh i hhhj $ term & in ! lknmo + & """" :qp r mo & """" : ksm > +! & """" :Jtvu w4x y 0+z w4{ + & """" :|t V } 0 y z u~: 2 5< ! ! ) & """"1 b; ] @ , 2 ;3-5 &87 """" 2 4365 &87 + """"6, 9 (2) TheTF, IDF andTFC termsin thisequationareidentical to thosein Eq. (1). Thevalueof theterm u } {1 0Z u } {  0Zda  increaseswith the lengthof thedocument.This term is introducedbecause, in a casewhereall of theother informationis exactly thesame,a longerdocumentis morelikely to includecontentthat is relevantasa responseto thequery. E is thetotalnumberof queries and B A is thenumberof queriesin which A occurs. Thosetermswhich occurmore frequentlyin queries aremorelikely to besuchasbunsho”document”and mono”thing”. We use F HI C C4D N8 to decreasethescores for stop words. We refer to this numericalterm as QIDF, becauseit is an IDF term for queries. It has  Thisequationis BM11,whichcorrespondsto BM25 in thecase where # [7]. a functionto decreasethescoresfor wordsthatoccurs in many queries(i.e. stopwords).It canbeveryuseful for foreign languagesfor which we cannotexamine stop words. O;^ N8 ; and   N    are extendednumericaltermsthatareintroducedto improvetheprecisionof results./ ^ N8 4 usesthelocationof theterm within the document. If the term is in the title or at thebeginningof thebodyof thedocument,it is given a higherweighting.   N    usesinformationsuchas whetherthetermis a propernounand/ora stopword. In thenext section,we explain theseextendednumerical termsin detail. 3 Extended numerical terms Weusethetwo extendednumericalterms/ ^ N8 ; and   N    in Eq. (2). In this section,they areexplainedin detail. 1. Locationalinformation(   ^ N8 4 )  Thetitle or first sentenceof thebodyof a documentin a newspaperwill generallyindicatethe subject. So, precisionin information retrieval canbeimprovedby assigninggreaterweightto termsfrom theselocations.This is achievedby  ;^ N8 ; , which is usedto adjusttheweightof a termaccordingto whetheror not it appearsat thebeginningof adocument.A termin thetitle or at the beginning of the body of a document, is assignedahigherweight.A termelsewhereis givena lower weight. / ^ N8 4 is expressedas follows: t u w4x y 0+z w4{ & """" # fhhhh i hhhhj . u w xy 0+z w4{ _ (whena term & occursin thetitle of adocument ), J,?. u w xy 0 z w4{ _  2 4365 &87 + """" ¡ £¢¥¤o + & """" """" 2 ;3-5 &87 """" (otherwise) (3) ¦ A is the locationof a term A in the document . Whena term appearsmore thanonce in a document,the locationin which it first appearsis usedto set this parameter . M  ^ N8 4~§ and M  ^ N8 4 § areconstantsto whichvaluesare assignedaccordingto theresultsof experiments. 2. Otherinformation(  ̈  N    )  ̈  N    is a moredetailednumericalterm that usesdifferent information, suchas whetheror nota termis apropernounandwhetheror not it is a stopword suchasbunsho”document”and mono”thing”. If a term is a propernoun, it is assignedahighweight. If a termis astopword, © This methodwasdevelopedby Murataet. al. [3]. it is assigneda low weight.    N    is expressed in thefollowingwayfor simplicity; thevariables for the documentandterm, and A , have been omitted: t V;} 0 y z u # t V;}«a x¬ ­ t ̄® ¬ w ® } ¬ ­ t {H° ± (4) Thetermsin thisequationareexplainedbelow. 2   43 « ́ Whena term is obtainedfrom the title of a query, i.e. DESCRIPTION,    3 « ́ = M   3 « ́ μ·¶ . Otherwise,   34« ́ = 1. This is becausewe canassumethat termsobtained from the descriptionof the query areimportant. 2 1 ̧  ́`  ̧ ^ ́ When a term is a proper noun, 1 ̧  ́   ̧ ^ ́ = Mo ̧  ́   ̧ ^ ́ 4μ»¶H . Otherwise1 ̧  ́   ̧ ^ ́ = 1. Thisisbecausetermsthatarepropernouns areimportant. 2  D1⁄4H1⁄2 Whenatermis numeric, D1⁄4H1⁄2 = M  1⁄4 1⁄2 3⁄4 ¶ . Otherwise,  1⁄4 1⁄2 = 1. A term which consistssolely of numeralswill not contain much relevant information, and thus lacksimportancefor thequery. 4 How terms are extracted We areonly ableto useEq. (2) in informationretrieval after we have extractedtermsfrom the query. This sectiondescribeshow this is achieved. We consideredseveralmethodsof termextractionaslistedbelow. 1. Usingonly theshortesterms This is thesimplestmethod.In this method,the querysentenceis dividedinto shorttermsby using amorphologicalanalyzerorsimilartool. All of the shorttermsareusedin the retrieval process.Themethodusedto divide thequerysentenceinto shorttermsis describedin Section5. 2. Usingall termpatterns The first methodproducesterms that are too short. For example,”enterprise”and”amalgamation” would beusedseparatelywhile “enterpriseamalgamation”wouldnotbeused.Wefelt that ”enterpriseamalgamation”shouldbe used with thetwo shortterms.Therefore,wedecided to usebothshortandlongterms.Wecall thisthe ”all term-patternsmethod”.For example,when enterprise amalgamation materialization enterprise amalgamation enterprise amalgamation materialization amalgamation materialization Figure 1. An example of a lattice structure ”enterpriseamalgamationrealization”¿ wasinput,weused”enterprise”,”amalgamation”,”realization”, ”enterpriseamalgamation”,”amalgamationrealization”,and”enterpriseamalgamationrealization”astermsin informationretrieval. We felt that this methodwould be effectivebecauseit makesuseof all termpatterns. Wealsofelt, however, thathaving only thethree terms”enterprise”,”amalgamation”,and”realization” derived from ”... enterprise... amalgamation... realization...”, while six termsare derived from ”enterpriseamalgamationrealization” would lack balance.We examinedseveral methodsof normalizationin preliminaryexperiments,thendecidedto dividetheweightof each term by À   Á  , where G is the numberof successi ve words. For example,in the caseof ”enterpriseamalgamationrealization”, GÂÄÃ ."""	cinema 4d;crystal structure;experiment;information retrieval;numerical analysis;tag (game);tf–idf;the 3-d battles of worldrunner;the final cartridge iii	Masaki Murata;Qing Ma;Hitoshi Isahara	2002			document retrieval;computer science;theoretical computer science;data mining;vector space model;data retrieval;information retrieval	Web+IR	-32.83754962284333	-60.25959791392557	119258
156c7ce2b4cb6f7e2689e2644f0afb5cc4efe365	cooperation schemes between a web server and a web search engine	search engine;search engines meta data internet web sites;meta data search engine web crawler web server polling;search engines;web search engine;web crawler;internet;web sites;meta data;web server web search search engines crawlers web pages proposals protocols computer science robots humans	Search engines provide search results based on a large repository of pages downloaded by a web crawler from several servers. To provide best results, this repository must be kept as fresh as possible, but this can be difficult due to the large volume of pages involved and to the fact that polling is the only method for detecting changes. In this paper, we explore and compare several alternatives for keeping fresh repositories that involve some degree of cooperation from servers.	polling (computer science);sensor;server (computing);web crawler;web search engine;web server	Carlos Castillo	2003		10.1109/LAWEB.2003.1250301	web service;static web page;web development;web modeling;site map;data web;web design;web search engine;web standards;computer science;spamdexing;web crawler;web api;web page;database;focused crawler;search analytics;web search query;world wide web;rewrite engine;information retrieval;web server;search engine	Web+IR	-30.34298792426018	-54.4046624027913	119293
d526959340b51b4c0fae1c01ae299cbf0685d23c	a fuzzy based approach to stylometric analysis of blogger's age and gender	blog representation fuzzy based approach stylometric analysis blogger age bloggergender fuzzy logic blog analysis feature words blogger style age group gender group normalized word frequency membership value fuzzy c means algorithm fcm algorithm;pattern clustering;clustering fuzzy logic stylometrics blog age gender fuzzy c means;text analysis;fuzzy set theory;fuzzy logic;web sites;web sites fuzzy logic fuzzy set theory pattern clustering text analysis;decision support systems hybrid intelligent systems helium mercury metals rail to rail outputs	Fuzzy logic deals with partial truth. A fuzzy based approach to blog analysis, on the basis of various feature words, allows us to determine the degree to which a blogger's style belongs to a particular age or gender group. Each blog was represented by a set of normalized word frequencies of selected feature words in it. Using membership values obtained from applying Fuzzy C-Means (FCM) algorithm to these blog representations, we can call the blogger's style to belong weakly, fairly, strongly or very strongly to a particular class. The advantage of using fuzzy logic for this problem is that a weak belonging to a particular class means that there is a decent belonging to the other class (es). Hence when a search or query is carried out, no useful blog will be left out of the results for that other class (es).	algorithm;blog;blogger;fuzzy cognitive map;fuzzy logic;stylometry;word lists by frequency	Sumit Goswami;Mayank Singh Shishodia	2012	2012 12th International Conference on Hybrid Intelligent Systems (HIS)	10.1109/HIS.2012.6421307	fuzzy logic;fuzzy cognitive map;membership function;defuzzification;fuzzy clustering;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;data mining;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations	Robotics	-19.37900821835959	-60.35993535460424	119321
217ea28513d0ea5b0f7ca0645a9803f70891320d	a probabilistic ranking model for audio stream retrieval	audio stream retrieval;probabilistic ranking;hmm	In Audio Stream Retrieval (ASR) systems, clients periodically query an audio database with an audio segment taken from the input audio stream to keep track of the flow of the stream in the original content sources or to compare two differently edited streams. We recently developed a series of ASR applications such as broadcast monitoring systems, automatic caption fetching systems, and automatic media edit tracking systems. Based on this experience, we propose a probabilistic ranking model designed for ASR systems. In order to train and test the model, we create a new set of audio streams and make it publicly available. Our experiments with these new streams confirm that the proposed ranking model works effectively with the retrieved results and reduces the errors when used in various ASR applications.	automated system recovery;automatic media;experiment;streaming media;tracking system;user-generated content	YoungHoon Jung;Jaehwan Koo;Karl Stratos;Luca P. Carloni	2016		10.1145/2927006.2927013	audio mining;speech recognition;computer science;world wide web;information retrieval	Web+IR	-26.219197179575428	-54.340610954239125	119381
8a34e5523d911b62ad3a4c9c6266292e318d264c	an unsupervised sentiment classifier on summarized or full reviews	empirical study;anaphora resolution	These days web users searching for opinions expressed by others on a particular product or service PS can turn to review repositories, such as Epinions.com or Imdb.com. While these repositories often provide a high quantity of reviews on PS, browsing through archived reviews to locate different opinions expressed on PS is a time-consuming and tedious task, and in most cases, a very labor-intensive process. To simplify the task of identifying reviews expressing positive, negative, and neutral opinions on PS, we introduce a simple, yet effective sentiment classifier, denoted SentiClass, which categorizes reviews on PS using the semantic, syntactic, and sentiment content of the reviews. To speed up the classification process, SentiClass summarizes each review to be classified using eSummar, a single-document, extractive, sentiment summarizer proposed in this paper, based on various sentence scores and anaphora resolution. SentiClass (eSummar, respectively) is domain and structure independent and does not require any training for performing the classification (summarization, respectively) task. Empirical studies conducted on two widely-used datasets, Movie Reviews and Game Reviews, in addition to a collection of Epinions.com reviews, show that SentiClass (i) is highly accurate in classifying summarized or full reviews and (ii) outperforms well-known classifiers in categorizing reviews.	anaphora (linguistics);archive;browsing;categorization;internet movie database (imdb);naive bayes classifier;ps (unix);sentiment analysis;social search	Maria Soledad Pera;Rani Qumsiyeh;Yiu-Kai Ng	2010		10.1007/978-3-642-17616-6_14	computer science;artificial intelligence;data science;data mining;database;empirical research;world wide web;information retrieval	NLP	-23.9086286742508	-58.515822430947004	119502
6b8576dc52e51c97690554da6845ad7113cbef1c	structure-based crawling in the hidden web	hidden web;tree edit distance;web crawling;dynamic content	The number of applications that need to crawl the Web to gather data is growing at an ever increasing pace. In some cases, the criterion to determine what pages must be included in a collection is based on theirs contents; in others, it would be wiser to use a structure-based criterion. In this article, we present a proposal to build structure-based crawlers that just requires a few examples of the pages to be crawled and an entry point to the target web site. Our crawlers can deal with form-based web sites. Contrarily to other proposals, ours does not require a sample database to fill in the forms, and does not require the user to interact heavily. Our experiments prove that our precision is 100% in seventeen real-world web sites, with both static and dynamic content, and that our recall is 95% in the eleven static web sites examined.	data-intensive computing;deep web;dynamic web page;entry point;experiment;form (html);granular computing;precision and recall;web crawler;word lists by frequency;world wide web	Márcio L. A. Vidal;Altigran Soares da Silva;Edleno Silva de Moura;João M. B. Cavalcanti	2008	J. UCS	10.3217/jucs-014-11-1857	spider trap;site map;web analytics;computer science;web crawler;dynamic web page;distributed web crawling;data mining;database;focused crawler;world wide web;website parse template;deep web;mashup	Web+IR	-29.7822460629485	-54.373447145631	119540
d63a71e91433d8044dc4bf5761419534c726a09d	emotionvis: designing an emotion text inference tool for visual analytics		With increasingly high volumes of conversations across social media, the rapid detection of emotions is of significant strategic value to industry practitioners. Summarizing large volumes of text with computational linguistics and visual analytics allows for several new possibilities from general trend detection to specific applications in marketing practice, such as monitoring product launches, campaigns and public relations milestones. After collecting 1.6 million user-tagged feelings from 12 million online posts that mention emotions, we utilized machine learning techniques towards building an automatic 'feelings meter'; a tool for both researchers and practitioners to automatically detect emotional dimensions from text. Following several iterations, the test version has now taken shape as emotionVis, a dashboard prototype for inferring emotions from text while presenting the results for visual analysis.		Christopher J. Zimmerman;Mari-Klara Stein;Daniel Hardt;Christian Danielsen;Ravikiran Vatrapu	2016		10.1007/978-3-319-39294-3_22	computer science;data science;data mining;multimedia	HCI	-22.738328297889023	-55.54053487267325	119615
01078381b337053bf357ea579024b4aea6195a56	hmm-based address parsing: efficiently parsing billions of addresses on mapreduce	large scale data;address parsing;record linkage	Record linkage is the task of identifying which records in one or more data collections refer to the same entity, and address is one of the most commonly used fields in databases. Hence, segmentation of the raw addresses into a set of semantic fields is the primary step in this task. In this paper, we present a probabilistic address parsing system based on the Hidden Markov Model. We also introduce several novel approaches to build models for noisy real-world addresses, obtaining 95.6% F-measure. Furthermore, we demonstrate the viability and efficiency of this system for large-scale data by scaling it up to parse billions of addresses with Hadoop.	apache hadoop;database;expectation–maximization algorithm;experiment;hidden markov model;image scaling;linkage (software);mapreduce;markov chain;parsing;signal-to-noise ratio	Xiang Li;Hakan Kardes;Xin Wang;Ang Sun	2014		10.1145/2666310.2666471	record linkage;computer science;bottom-up parsing;data mining;database;world wide web	NLP	-23.018378057619636	-62.55445069025149	119733
fb4860ca58e01245b24b310c7acadb7cc714b2ad	word embedding based approaches for information retrieval			information retrieval;word embedding	Dwaipayan Roy	2017		10.14236/ewic/FDIA2017.9	human–computer information retrieval;word embedding;visual word;term discrimination;information retrieval;computer science	Web+IR	-25.168102085845263	-61.59150211275921	119834
366f85f29c2e89cf40dc1f489266ff6665d8f7eb	recommendation systems in mathematical character recognition		In handwritten text there are usually several accepted styles for forming each character. We hypothesize that in the handwriting of individuals there is a correlation among the styles used for characters, and that these correlations may be used to anticipate which styles particular writers will use for symbols that have not yet been seen. This approach may prove useful in the setting of mathematical handwriting recognition, where there are many symbols and it would be onerous to require writers to provide samples of every one in order to personalize handwriting recognition. We describe preliminary experiments using ideas from the area of recommendation systems to predict which styles writers will likely use for symbols they have not yet written. The experiments demonstrate that writers tend to use only a small fraction of the possible combinations of character writing styles, and there are correlations among the styles used for symbols.	experiment;handwriting recognition;optical character recognition;personalization;recommender system;statistical classification	Vadim Mazalov;Stephen M. Watt	2013			recommender system;writing style;handwriting;handwriting recognition;natural language processing;intelligent character recognition;artificial intelligence;computer science	AI	-21.2184123966874	-60.047710375515074	119848
2d88125e4859259d943c067ca74a4a9696f3455d	a data-stream-based abnormal data mining in web texts environment		The stability and self-adaption for combination texts must be processed in Web Texts Environment. Therefore a language and technology method for self-adapting environment of web texts is needed. To do this, we have built an adaptive data-stream method in which the abnormal data mining process is started. The resource consumption of abnormal data in a text includes the resource consumption of error text and the total resource consumptions of relating with the previously executed texts which are dependent on the error text. In this paper an adaptive data-stream method is applied to implement the Abnormal Data Mining in Web Texts Environment. Proved by simulation verification, we proposed this adaptive data-stream method is efficient for solving the problem of abnormal data mining in web texts environment.	data mining	Jin-Yun Wang;Yezheng Liu;Jinkun Wang	2016	J. Comput. Meth. in Science and Engineering	10.3233/JCM-160623	computer science;data science;machine learning;data mining;world wide web	Theory	-23.90492078699904	-54.884784536307095	119888
127fa0bca843cf7751ea6c91fdfc842be6da38d1	predicting web searcher satisfaction with existing community-based answers	search engine;query question match;answer quality;satisfiability;web search engine;query clarity;indexation;searcher satisfaction;web search;community question answering;question answering	Community-based Question Answering (CQA) sites, such as Yahoo! Answers, Baidu Knows, Naver, and Quora, have been rapidly growing in popularity. The resulting archives of posted answers to questions, in Yahoo! Answers alone, already exceed in size 1 billion, and are aggressively indexed by web search engines. In fact, a large number of search engine users benefit from these archives, by finding existing answers that address their own queries. This scenario poses new challenges and opportunities for both search engines and CQA sites. To this end, we formulate a new problem of predicting the satisfaction of web searchers with CQA answers. We analyze a large number of web searches that result in a visit to a popular CQA site, and identify unique characteristics of searcher satisfaction in this setting, namely, the effects of query clarity, query-to-question match, and answer quality. We then propose and evaluate several approaches to predicting searcher satisfaction that exploit these characteristics. To the best of our knowledge, this is the first attempt to predict and validate the usefulness of CQA archives for external searchers, rather than for the original askers. Our results suggest promising directions for improving and exploiting community question answering services in pursuit of satisfying even more Web search queries.	archive;question answering;web search engine;web search query	Qiaoling Liu;Eugene Agichtein;Gideon Dror;Evgeniy Gabrilovich;Yoelle Maarek;Dan Pelleg;Idan Szpektor	2011		10.1145/2009916.2009974	question answering;web search engine;computer science;data mining;web search query;world wide web;information retrieval;search engine;satisfiability	Web+IR	-32.1591894476305	-54.15203626379618	119911
f6d34568bbab083e3f83c64f752046b6f377205d	computing the semantic relatedness of music genre using semantic web data		Computing the semantic relatedness between two entities has many applications domains. In this paper, we show a new way to compute the semantic relatedness between two resources using semantic web data. Moreover, we show how this measure can be used to compute the semantic relatedness between music genres which can be used for music recommendation systems. We first describe how to build a vector representations for resources in an ontology. Subsequently we show how these vector representations can be used to compute the semantic relatedness of two resources. Finally, as an application, we show that our measure can be used to compute the semantic relatedness of music genres. CCS Concepts •Information systems → Similarity measures; Language models;	entity;ontology (information science);recommender system;semantic web;semantic similarity	Dennis Diefenbach;Pierre-René Lhérisson;Fabrice Muhlenbach;Pierre Maret	2016			semantic similarity;semantic computing;social semantic web;information retrieval;semantic search;semantic equivalence;semantic technology;natural language processing;semantic analytics;semantic web stack;artificial intelligence;computer science	Web+IR	-29.00770602102732	-58.634722958249995	119985
4f2a20d6b0ea699d67a1aa05ec0a3cecb5c69df0	on the predictive analysis of behavioral massive job data using embedded clustering and deep recurrent neural networks		Abstract The recent proliferation of social networks as a main source of information and interaction has led to a huge expansion of automatic e-recruitment systems and by consequence the multiplication of web channels (job boards) that are dedicated to job offers disseminating. In a strategic and economic context where cost control is fundamental, it has become necessary to identify the relevant job board for a given new job offer has become necessary. The purpose of this work is to present the recent results that we have obtained on a new job board recommendation system that is a decision-making tool intended to guide recruiters while they are posting a job on the Internet. Firstly, the Doc2Vec embedded representation is used to analyse the textual content of the job offers, then the job applicant clickstreams history on various job boards are stored in a large learning database, and then represented as time series. Secondly, a deep neural network architecture is used to predict future values of the clicks on the job boards. Third, and in parallel, dimensionality reduction techniques are used to transform the clicks numerical time series into temporal symbolic sequences. Forecasting algorithms are then used to predict future symbols for each sequence. Finally, a list of top ranked job boards are kept by maximizing the clickstreams forecasting in both representations. Our experiments are tested on a real dataset, coming from a job-posting database of an industrial partner. The promising results have shown that using deep learning, the recommendation system outperforms standard multivariate models.	artificial neural network;cluster analysis;embedded system;recurrent neural network	Sidahmed Benabderrahmane;Nedra Mellouli;Myriam Lamolle	2018	Knowl.-Based Syst.	10.1016/j.knosys.2018.03.025	machine learning;the internet;recommender system;dimensionality reduction;artificial intelligence;cluster analysis;artificial neural network;deep learning;recurrent neural network;computer science;ranking	ML	-21.150403362530714	-52.49047586632384	120052
026fdf1de952403bf31ee2038f5be9cf722d76db	using social media to find places of interest: a case study	qa75 electronic computers computer science;ibcn;technology and engineering;hm sociology;geographic information retrieval;social media;detecting places of interest	In this paper, we show how the large amount of geographically annotated data in social media can be used to complement existing place databases. After explaining our method, we illustrate how this approach can be used to discover new instances of a given semantic type, using London as a case study. In particular, for several place types, our method finds places in London that are not yet contained in the databases used by Foursquare, Google, LinkedGeoData and Geonames. Encouraged by these results, we briefly sketch how similar techniques could potentially be used to identify likely errors in existing databases, to estimate the spatial extent of places, to discover semantic relationships between place types, and to recommend tags to users who are uploading photos.	database;geonames;social media;upload	Steven Van Canneyt;Olivier Van Laere;Steven Schockaert;Bart Dhoedt	2012		10.1145/2442952.2442954	computer science;data science;data mining;world wide web	Web+IR	-25.170702135333347	-52.3022261793419	120195
b2588c8ab4112182b345f34370da25198aa0c6e9	research topics variation analysis and prediction based on faro and neural networks	market research;neural networks;data mining;mathematical model;predictive models;ontologies;conferences	Given the explosive growth of scientific information and the fast advancement of research fields, researchers may not be able to find the most promising topics to combine with their current research and may be trapped in a few familiar research topics without creative ideas. Many studies of recommendation system make the effort to address the above problem, but they ignore the different styles of users and generate the recommendation results based on a common strategy. In this paper, we propose a framework to generate the adaptive recommendation results according to the research styles of users. Our framework contains 3 main parts, the research topic ontology construction, trend prediction and recommendation. First of all, the Fun of Academic Research Ontology (FARO), which has the capacity of describing dynamic and static research features and building a social network, is constructed to organize entities about academic research. Secondly, this paper predicts the popularity variation of research topics with the neural network model. Finally, some adaptive topics are recommended to specific researchers according to the evaluation of their research styles. Basically, this paper is inspired by the associative thinking of human brain to combine the advantages of Web knowledge representation language and the neural network to execute the prediction and recommendation. We test our results based on the publication data of IEEE and Springer. The experimental results demonstrate that our prediction model has a good generalization performance. A questionnaire survey is carried out to assess the recommendation results, and the result shows the feasibility of our method.	artificial neural network;entity;knowledge representation and reasoning;network model;recommender system;social media;social network;springer (tank);variable rules analysis;world wide web	Hongyin Zhu;Yi Zeng;Yiping Yang	2016	2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2016.7844357	market research;computer science;ontology;artificial intelligence;data science;machine learning;mathematical model;data mining;predictive modelling;artificial neural network	DB	-24.342157496468964	-57.2912254482612	120263
fd01885259776b74eaa569ce20005086b9f72d7c	indonesian-english transitive translation for cross-language information retrieval	cross language evaluation forum;language resources;query expansion;cross language information retrieval;machine translation	This is a report on our evaluation of using some language resources for the Indonesian-English bilingual task of the 2007 Cross-Language Evaluation Forum (CLEF). We chose to translate an Indonesian query set into English using machine translation, transitive translation, and parallel corpus-based techniques. We also made an attempt to improve the retrieval effectiveness using a query expansion technique. The result shows that the best retrieval performance was achieved by combining the machine translation technique and the query expansion technique.	cross-language information retrieval;query expansion;statistical machine translation	Mirna Adriani;Herika Hayurani;Syandra Sari	2007		10.1007/978-3-540-85760-0_16	computer-assisted translation;natural language processing;query expansion;speech recognition;universal networking language;example-based machine translation;computer science;evaluation of machine translation;linguistics;rdf query language;machine translation;rule-based machine translation;machine translation software usability;information retrieval;query language;language model	NLP	-33.087492754192255	-64.29977591454444	120355
8c6e55ff824cdaab65af0a6331c0648d55222fa7	evaluating the potential of explicit phrases for retrieval quality	query performance;retrieval quality;trec terabyte benchmark;explicit phrase;standard score;optimal choice;potential impact;proximity-aware score;system-identified phrase;case study	This paper evaluates the potential impact of explicit phrases on retrieval quality through a case study with the TREC Terabyte benchmark. It compares the performance of userand system-identified phrases with a standard score and a proximity-aware score, and shows that an optimal choice of phrases, including term permutations, can significantly improve query performance.	benchmark (computing);terabyte	Andreas Broschart;Klaus Berberich;Ralf Schenkel	2010		10.1007/978-3-642-12275-0_62	computer science;data mining;database;information retrieval	Web+IR	-33.328492808222194	-63.64994404175144	120384
76dac731beb8b6a94b0f1559a62410ff648d8ed9	performance improvement of drug effects extraction system from japanese blogs	drugs;opinion mining;information extraction;text mining;drugs data mining blogs diseases dictionaries accuracy;text analysis;data mining;grammars;web sites data mining drugs electronic health records grammars natural language processing text analysis;web sites;medication usage information text mining opinion mining information extraction;medication usage information;medication usage information drug effects extraction system japanese blogs patients blogs drug name illness survival blogs free natural language parsing;natural language processing;electronic health records	Information disclosed to the public by patients is very important for people who are suffering from same illness because such information can be a source of knowledge and encouragement. Our aim is to make a system that extracts, organizes and visually represents information from patients' blogs. As the first step, the purpose of this paper is to extract descriptions of the effects caused by taking drugs as a triplet of expressions - drug name, object of change, and its effect - from illness survival blogs. However, conventional extraction methods are not suitable since these blogs are written in free natural language. Therefore, this paper proposes a method to extract the triplets using specific clue words and parsing the results. An evaluation experiment confirmed that medication usage information can be extracted with high accuracy using our proposed method, in comparison to existing methods. Moreover, recall was improved by combining our proposed method and a baseline system.	baseline (configuration management);blog;natural language;norm (social);parsing;triplet state	Shiho Kitajima;Rafał Rzepka;Kenji Araki	2013	2013 IEEE Seventh International Conference on Semantic Computing	10.1109/ICSC.2013.71	natural language processing;text mining;computer science;data mining;database;rule-based machine translation;world wide web;information retrieval	NLP	-21.530311766909822	-56.32067466293207	120392
ef6ae97944d49b1c929fbbd7d929a36c9ce789d9	sense based organization of descriptive data	ontologies data mining humans data analysis current measurement psychology algorithm design and analysis computational linguistics information retrieval multidimensional systems;text analysis;ontologies artificial intelligence;context driven space sense based organization descriptive data document keywords wordnet ontology document repository data driven dimensions;similarity measure;text analysis ontologies artificial intelligence	In this paper we propose a new technique allowing to map documents' keywords into relative distance space, which is based primarily on senses of these terms. We use WordNet ontology to retrieve multiple senses of keywords with the aim of generating multidimensional space for our data. The focus of this work is mainly on transferring the available ontology of keywords (and their senses) into multiple dimensions, where each dimension reflects approximation of a single, but broad context of important keywords appearing in our document repository. We have concentrated on discovery of appropriate similarity measurements and construction of data-driven dimensions. It benefits quality of generated dimensions and provides a clearer view of the whole document repository in low-dimensional, context-driven space.	approximation;dimensionality reduction;experiment;ontology (information science);organizing (structure);software testing controversies;synonym ring;text corpus;wordnet	M. Shahriar Hossain;Monika Akbar;Rafal A. Angryk	2007	2007 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2007.4414084	text mining;computer science;data science;data mining;information retrieval	Robotics	-25.521841978971132	-63.44056116317035	120447
cf91b0cdac0b2924b1ae752a3053332520b54a5b	detecting influenza states based on hybrid model with personal emotional factors from social networks	personal emotional factors;hybrid model;social web mining;influenza detection;transition time detection	"""In this paper, we exhibit how social media data can be used to detect and analyze real-word phenomena with several data mining techniques. We investigate the real-time flu detection problem and propose a flu state detection model with personal emotional factors and semantic information (Em-Flu model). First, we extract flu-related microblog posts automatically in real-time using a hybrid model composed by Support Vector Machine with features extracted from Restricted Boltzmann Machine. In order to overcome the limitation of 140 words for posts, expect for sentiment related features, association semantic rules are also adopted as additional features, such as bag of words, negative words, degree adverbs and sentiment words dictionary. For flu state detection at specific location, we propose an unsupervised model based on personal emotional factors to figure out what state of flu in specific place. For comparison, a supervised model is also built by adopting Conditional Random Fields to decide whether a poster has """"really"""" catch flu and what influenza stage the poster is in. Some statistic methods and prior rules are adopted in supervised model to get the flu state of specific locations by counting the number of microblog posts in different flu states. By considering personal emotional factors, spatial features and temporal patterns of influenza, the performance of unsupervised and supervised models are both improved. The system could tell when and where influenza epidemic is more likely to occur at certain time in specific locations. In different experiments results, the hybrid models show robustness and effectiveness than state-of-the-art unsupervised and supervised model only considering the number of posts."""	sensor;social network	Xiao Sun;Jia-qi Ye;Fuji Ren	2016	Neurocomputing	10.1016/j.neucom.2016.01.107	speech recognition;machine learning;data mining	AI	-20.11108185103484	-57.575237751570405	120454
f624888aa484238383513a406accb2a958ed90d9	the impact of data preprocessing on the performance of a naive bayes classifier	text classifier;electronic mail training feature extraction data preprocessing text categorization filtering noise measurement;naive bayes classifier;text classifier naive bayes classifier spam filter preprocessing map reduce;spam filter;map reduce;unsolicited e mail bayes methods data mining pattern classification;data mining process data preprocessing impact naive bayes classifier text mining document classification dataset preprocessing spam emails;preprocessing	In the research of text mining, document classification is a growing field. Even though we have many existing classifying approaches, Naïve Bayes Classifier is simple and effective at classification. Data preprocessing is the important step in the data mining process. It prepares the raw data for the further process. The aim of this paper is to identify the impact of preprocessing the dataset on the performance of a Naïve Bayes Classifier. The Naïve Bayes Classifier is suggested as the most effective method to identify the spam emails. The Impact of preprocessing phase on the performance of the Naïve Bayes classifier is analyzed by comparing the output of both the preprocessed dataset result and non-preprocessed dataset result. The test results show that combining Naïve Bayes classification with the proper data preprocessing can improve the prediction accuracy.	data mining;data pre-processing;document classification;effective method;email;naive bayes classifier;preprocessor;spamming;text mining	Priyanga Chandrasekar;Kai Qian	2016	2016 IEEE 40th Annual Computer Software and Applications Conference (COMPSAC)	10.1109/COMPSAC.2016.205	bayes classifier;naive bayes classifier;computer science;machine learning;pattern recognition;data mining;bayes error rate;programming language;preprocessor	ML	-20.90673396033427	-61.672061546571435	120464
b5eb3f1c7bd5d6cc6bcbcb95feb91be209734bbd	thematic clustering of text documents using an em-based approach	health research;uk clinical guidelines;biological patents;europe pubmed central;citation search;data mining and knowledge discovery;computational biology bioinformatics;uk phd theses thesis;life sciences;algorithms;combinatorial libraries;uk research reports;medical journals;computer appl in life sciences;europe pmc;biomedical research;bioinformatics	Clustering textual contents is an important step in mining useful information on the web or other text-based resources. The common task in text clustering is to handle text in a multi-dimensional space, and to partition documents into groups, where each group contains documents that are similar to each other. However, this strategy lacks a comprehensive view for humans in general since it cannot explain the main subject of each cluster. Utilizing semantic information can solve this problem, but it needs a well-defined ontology or pre-labeled gold standard set. In this paper, we present a thematic clustering algorithm for text documents. Given text, subject terms are extracted and used for clustering documents in a probabilistic framework. An EM approach is used to ensure documents are assigned to correct subjects, hence it converges to a locally optimal solution. The proposed method is distinctive because its results are sufficiently explanatory for human understanding as well as efficient for clustering performance. The experimental results show that the proposed method provides a competitive performance compared to other state-of-the-art approaches. We also show that the extracted themes from the MEDLINE® dataset represent the subjects of clusters reasonably well.	cluster analysis;electron microscopy;extraction;local optimum;medline;numerous;ontology;personnameuse - assigned;silo (dataset);text-based (computing);tracer;algorithm;contents - htmllinktype;explanation;statistical cluster	Sun Kim;W. John Wilbur	2012		10.1186/2041-1480-3-S3-S6	correlation clustering;constrained clustering;data stream clustering;medical research;fuzzy clustering;computer science;bioinformatics;data science;data mining;cluster analysis;brown clustering;information retrieval;algorithm;clustering high-dimensional data	Web+IR	-26.489973025216877	-60.01181029429693	120482
de41e1cb1edcdba1ca9e961d48522ef02b788767	efs: expert finding system based on wikipedia link pattern analysis	wikipedia expert finding automatic term recognition;wikipedia pattern analysis proposals ontologies computer science electronic mail databases social network services information science application software;academic environment;wikipedia;electronic mail;web pages;automatic term recognition;information retrieval;expert finding;web sites electronic publishing information analysis information retrieval;information services;data mining;internet;journal publication;web sites;pattern analysis;electronic publishing;wikipedia link pattern analysis;web site wikipedia link pattern analysis expert finding system academic environment journal publication;encyclopedias;proposals;information analysis;expert finding system;web site	Building an expert finding system is very important for many applications especially in the academic environment. Previous work uses e-mails or Web pages as corpus to analyze the expertise for each expert. In this paper, we present an Expert Finding System, abbreviated as EFS to build experts' profiles by using their journal publications. For a given proposal, the EFS first looks up the Wikipedia Web site to get relative link information, and then list and rank all associated experts by using those information. In our experiments, we use a real-world dataset which comprises of 882 people and 13,654 papers, and are categorized into 9 expertise domains. Our experimental results show that the EFS works well on several expertise domains like ldquoArtificial Intelligencerdquo and ldquoImage & Pattern Recognitionrdquo etc.	categorization;email;encrypting file system;experiment;pattern recognition;text corpus;web page;wikipedia	Kai-Hsiang Yang;Chun-Yu Chen;Hahn-Ming Lee;Jan-Ming Ho	2008	2008 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2008.4811348	the internet;computer science;web page;data mining;brand;electronic publishing;data analysis;world wide web;information retrieval;information system;encyclopedia	DB	-25.53812201807673	-56.84430787180011	120529
347ca07e749f01f1e32bdeae41b8acdfce3cba0c	relevant term suggestion based on pseudo relevance feedback from web contexts		Most search engines rely on query logs to give query suggestions. By mining potential relevant terms surrounding the query from Web resources, we aim at improving query formulation and retrieval effectiveness without query logs. In this paper, we propose a relevant term suggestion approach based on pseudo relevance feedback from Web contexts. Expansion term candidates are extracted and filtered by contextual relevance as calculated by mutual information and Web n-gram language model. Experimental results show a good performance in relevant term suggestion.	relevance feedback	Jenq-Haur Wang;Meng-Han Shih	2012		10.1007/978-3-642-34752-8_39	data mining;world wide web;information retrieval	Web+IR	-29.74258398877478	-59.24218445865644	120531
3737d41b258d8c363aae6046540506d33e17d991	a similarity search system based on the hamming distance of social profiles	similarity search systems social profiles data retrieval similarity level social media social network vector space model vsm users profiles random hyper plane hashing rhh function similarity searches hamming similarity reference profiles queries frequency distribution hamming distances;query formulation;similarity search hamming distance rhh;hamming distance;vectors hamming distance correlation databases measurement social network services prototypes;social networking online;social networking online query formulation;similarity search;rhh	The goal of a similarity search system is to allow users to retrieve data that presents a required similarity level in a certain dataset. For example, such dataset may be applied in the social media scenario, where huge amounts of data represent users in a social network. This paper uses a Vector Space Model (VSM) to represent users' profiles and the Random Hyper plane Hashing (RHH) function to create indexes for them. Both VSM and RHH compose an alternative to address the challenge of performing similarity searches over the huge amount of data present in the social media scenario: the Hamming similarity. In order to evaluate the effectiveness of our proposal, this paper brings examples of reference profiles, used for performing queries, and presents results regarding the correlation between cosine and Hamming similarity and the frequency distribution of Hamming distances among identifiers of users' profiles. In short, the results indicate that Hamming similarity can be useful for the development of similarity search systems for social media.	curse of dimensionality;hamming distance;identifier;personalization;semantic similarity;similarity search;social media;social network;viable system model;window function	Rodolfo da Silva Villaça;Luciano Bernardes de Paula;Rafael Pasquini;Maurício F. Magalhães	2013	2013 IEEE Seventh International Conference on Semantic Computing	10.1109/ICSC.2013.24	hamming distance;computer science;theoretical computer science;machine learning;data mining;database;world wide web;information retrieval;statistics	DB	-29.059390776365618	-57.80784833122161	120656
3caa8d7ee82377405e60760747957ebedd8f3ba2	text classification based on nonlinear dimensionality reduction techniques and support vector machines	text analysis natural language processing support vector machines;nonlinear dimensionality reduction techniques;classification algorithm;high dimensionality;text documents;vector space model;support vector machines;text categorization methods;text analysis;text classification;multi class classifier;nonlinear dimensionality reduction;multi class classifier text classification nonlinear dimensionality reduction techniques support vector machines natural language processing text documents text categorization methods;text categorization support vector machines support vector machine classification principal component analysis space technology natural language processing information filtering feature extraction computer science helium;support vector machine;classification accuracy;natural language processing;text categorization	Text classification is an important task in the field of natural language processing. The dimension of the text data is huge for the text documents are usually represented with the vector space model. Thus, it is greatly time-consuming to perform existed text categorization methods. Moreover, it is almost unimaginable to store and enquire high-dimensional text data. To improve the executing efficiency of classification methods, we present a classification algorithm based on nonlinear dimensionality reduction techniques and support vector machines. In the procedure, the ISOMAP algorithm is firstly executed to reduce the dimension of the high-dimensional text data. Then the low-dimensional data are classified with a multi-class classifier based single-class SVM. Experimental results demonstrate that the executing efficiency of categorization methods is greatly improved after decreasing the dimension of the text data without loss of the classification accuracy.	algorithm;categorization;document classification;enquire;isomap;natural language processing;nonlinear dimensionality reduction;support vector machine;text corpus	Lukui Shi;Jun Zhang;Enhai Liu;Pilian He	2007	Third International Conference on Natural Computation (ICNC 2007)	10.1109/ICNC.2007.706	computer science;machine learning;pattern recognition;data mining	ML	-20.76706864535335	-64.08339434760714	120837
c1c2c04b0f6c5bb90431b51ba7645ec38b739df5	the categorisation of hidden web databases through concept specificity and coverage	content management;databases;document handling;web pages;content summary;information retrieval;hidden web;document handling information retrieval systems information retrieval content based retrieval classification content management;information search;data mining;classification;content summary hidden web databases concept specificity concept coverage information search document database;document database;hidden web databases;concept specificity;information retrieval systems;concept coverage;sampling methods;frequency;content based retrieval;databases information retrieval sampling methods frequency data mining web pages	Hidden Web databases maintain a collection of specialised documents, which are dynamically generated in response to users' queries. The categorisation of such databases into a set of predefined categories has been widely employed to assist users in their information searches. In this paper we present a technique that automatically categorises a document database through its content summary and concepts described by their specificity and coverage. Experimental results show that our approach categorises databases with a larger number of relevant categories.	categorization;code coverage;dark web;document-oriented database;sensitivity and specificity	Yih-Ling Hedley;Muhammad Younas;Anne E. James	2005	19th International Conference on Advanced Information Networking and Applications (AINA'05) Volume 1 (AINA papers)	10.1109/AINA.2005.323	sampling;biological classification;content management;computer science;frequency;web page;data mining;database;information retrieval;deep web	DB	-31.703978325372397	-56.80976115266211	120871
86ded2a6d4897603dccb71c2c653d0af62609dc3	clef experiments at the university of maryland: statistical stemming and back-off translation strategies		The University of Maryland participated in the CLEF 2000 multilingual task, submitting three o cial runs that explored the impact of applying language-independent stemming techniques to dictionary-based cross-language information retrieval. The paper begins by describing a cross-language information retrieval architecture based on balanced document translation. A four-stage backo strategy for improving the coverage of dictionary-based translation techniques is then introduced, and an implementation based on automatically trained statistical stemming is presented. Results indicate that competitive performance can be achieved using these techniques in conjunction with freely available bilingual dictionaries.	bilingual dictionary;cross-language information retrieval;data dictionary;language-independent specification;query expansion;query language;relevance feedback;stemming;venue (sound system)	Douglas W. Oard;Gina-Anne Levow;Clara I. Cabezas	2000			natural language processing;information retrieval;architecture;artificial intelligence;clef;computer science;speech recognition	Web+IR	-32.85691958527154	-64.4237883826736	121143
52683056c24bf6f63ef1b21e8cc443fabf883e79	demographic context in web search re-ranking	personalization;web search;context aware ranking;click models	In this paper we study usefulness of user's demographical context for improving ranking of ambiguous queries. Context-aware relevance model is learnt from implicit user behaviour by using a simple yet general modification of a state-of-art click model which is capable to catch dependences from the search context. After that the machine learned click model is used in an offline re-ranking experiment and it is demonstrated that the demographical context ranking features provide improvements in ranking quality. Further, we perform a study to investigate the impact of different facets of demographical features (gender, age, and income) on search ranking performance and manually analyse queries which exhibit strong context dependences to get an additional understanding of the model behaviour.	online and offline;relevance;search algorithm;web search engine	Eugene Kharitonov;Pavel Serdyukov	2012		10.1145/2396761.2398690	computer science;data mining;personalization;multimedia;world wide web	Web+IR	-33.44174688101103	-52.30971665987842	121170
4e09ba81a829ecaddea4f696a8be8525ac8f7514	semantic similarity calculation method using information contents-based edge weighting.		In this paper, we propose Semantic Similarity calculation measurement using INformation contents on EdGEs of ontology (SSINEGE) which is a hybrid edgeand information contents-based methodology. SSINEGE is devised to solve the limitation of the applying the same weighted edges by edgebased similarity. So, SSINEGE adopts information-contents theory to calculate the varied weights of edges. The varied weighted edges by SSINEGE can also solve a problem with the same degree of similarity for all pairs of concepts that are sharing a same Least Common Subsumer (LCS). To minimize the overlapped information-contents on the weighted, SSINEGE adopts the conceptual path between concepts instead of depths of the ontology. To verify the superiority of SSINEGE, we compared SSINEGE with widely used four similarity measurements including Leacock and Chodorow. We conducted two kinds of evaluations: first is calculation of similarity using the varied edge-weighting and second is for the discriminative capability using conceptual distances between comparative concepts. To verify the superiority of SSINEGE, we compared the calculated similarities of SSINEGE with Leacock and Chodorow. As the results, we verified that the calculated similarity of SSINEGE is significantly increased than the other comparatives.	futures studies;ontology (information science);semantic similarity;wordnet	Sunghwan Jeong;Jun Hyeok Yim;Hyun Jung Lee;Mye M. Sohn	2017	J. Internet Serv. Inf. Secur.		pattern recognition;data mining;information retrieval	AI	-28.207768303889296	-59.07246031974703	121343
fee2f069fd6f1ef2a5e662855a2e23b54a687928	keywords similarity based topic identification for indonesian news documents	information retrieval;indonesian text documents;text analysis electronic publishing information retrieval natural language processing pattern classification;text analysis;bracewell algorithm topic identification indonesian text documents information retrieval news domain;news domain;article keyword testing keyword similarity based topic identification textual documents online news tid indonesian corpus indonesian news documents japanese corpora english corpora bracewell s keywords similarity algorithm top n keywords selection method indonesian news documents bracewell s performance classification training dataset;accuracy vectors training databases clustering algorithms equations mathematical model;pattern classification;topic identification;electronic publishing;natural language processing;bracewell algorithm	Topic identification (TID) is a technique associated with labelling a set of textual documents with a meaningful label representing its content. TID for online news presents different problems from TID for other corpora, such as the large data volume and the frequently updated topic. Moreover, the number of developing methods for Indonesian corpus is rather small. Brace well's algorithm has been proven effective in identifying topics in English and Japanese corpora with high accuracy. This paper implements a method for TID based on Brace well's keywords similarity algorithm and the top-n keywords selection for Indonesian news documents. The top-n method is utilized to improve Brace well's performance within Indonesian corpus, and to reduce the dimension of dataset during training. The combination is aimed to reduce the heavy computation problem and to explore the possibility of a new emerging topic which possibly has to be created. The method consists of two stages: training and classification. It studies the keywords of the training dataset then calculates the similarity between testing and training articles' keywords. The algorithm produced accuracy as high as 95.22% on onlineand95.26% on offline environment, 84% against human evaluation, and an average of 2.96 seconds computational time.	algorithm;cluster analysis;computation;computational problem;cosine similarity;experiment;online and offline;statistical classification;text corpus;thresholding (image processing);time complexity	Aini Fuddoly;Jafreezal Jaafar;Norshuhani Zamin	2013	2013 European Modelling Symposium	10.1109/EMS.2013.3	natural language processing;speech recognition;computer science;information retrieval	NLP	-23.183253906731657	-64.61649903150523	121355
afb025b770d946e4fe8c9d3052bced514cd92c9a	postal address detection fromweb documents	image recognition;web documents;postal address detection;application software;information retrieval;grammars postal address detection web documents text blocks text recognition;web sites document image processing grammars image recognition text analysis;false alarm rate;text analysis;text recognition statistical analysis partial response channels application software data mining information retrieval computer science html computer applications machine learning;data mining;computer applications;html;grammars;text blocks;statistical analysis;machine learning;web sites;partial response channels;document image processing;computer science;text recognition	An approach to postal address detection from Web pages is proposed. The Web pages are first segmented into text blocks based on their visual similarity. The text content in each block undergoes the recognition process, which employs a syntactic approach. The grammars of almost all possible patterns of postal addresses are built for this purpose. The results of our preliminary experiments on 44 Web pages with 56 true addresses show that our approach can detect the postal addresses with a high precision (89.3%) and a low false alarms rate (3.8%)	experiment;postal;syntactic pattern recognition;telephone number;test data;web page	Lin Can;Zhang Qian;Xiaofeng Meng;Wenyin Lin	2005	International Workshop on Challenges in Web Information Retrieval and Integration	10.1109/WIRI.2005.28	computer science;data mining;world wide web;information retrieval	Web+IR	-23.773073856733152	-63.24175748375932	121365
bbc89c3f65e42eda579668135ce6fa64a058416c	improving interpretations of topic modeling in microblogs		Topic models were proposed to detect the underlying semantic structure of large collections of text documents in order to facilitate the process of browsing and accessing documents with similar ideas and topics. Applying topic models to short text documents to extract meaningful topics is challenging. The problem becomes even more complicated when dealing with short and noisy micro-posts in Twitter that are about one general topic. In such a case, the goal of applying topic models is to extract subtopics. This results in topics represented by similar sets of keywords, which in turn makes the process of topic interpretation more confusing. In this paper we propose a new method that incorporates Twitter-LDA, WordNet, and hashtags to enhance the keyword labels that represent each topic. We emphasize the importance of different keywords to different topics based on the semantic relationships and the co-occurrences of keywords in hashtags. We also propose a method to find the best number of topics to represent the text document collection. Experiments on two real-life Twitter data sets on fashion suggest that our method performs better 1 The official version is published in the Journal of the Association for Information Science and Technology (JASIST) http://dx.doi.org/10.1002/asi.23980 than the original Twitter-LDA in terms of perplexity, topic coherence and the quality of keywords for topic labeling.	archive;hashtag;journal of the association for information science and technology;linear discriminant analysis;micro isv;perplexity;real life;topic model;wordnet	Sarah A. Alkhodair;Benjamin C. M. Fung;Osmud Rahman;Patrick C. K. Hung	2018	JASIST	10.1002/asi.23980	computer science;microblogging;data mining;topic model;social media	ML	-25.55896719805053	-59.37184850789694	121504
75c37243f438e0e73d7a5a1705e63e08b7ebd522	modeling extractive sentence intersection via subtree entailment		Sentence intersection captures the semantic overlap of two texts, generalizing over paradigms such as textual entailment and semantic text similarity. Despite its modeling power, it has received little attention because it is difficult for non-experts to annotate. We analyze 200 pairs of similar sentences and identify several underlying properties of sentence intersection. We leverage these insights to design an algorithm that decomposes the sentence intersection task into several simpler annotation tasks, facilitating the construction of a high quality dataset via crowdsourcing. We implement this approach and provide an annotated dataset of 1,764 sentence intersections.	algorithm;crowdsourcing;display resolution;original chip set;textual entailment	Omer Levy;Ido Dagan;Gabriel Stanovsky;Judith Eckle-Kohler;Iryna Gurevych	2016			natural language processing;artificial intelligence;computer science;tree (data structure);pattern recognition;sentence;logical consequence	NLP	-27.3553634369303	-65.46574441583769	121682
839068a103bb481c364e9f9329e83c81ae1db1d4	high-dimensional vector semantics		"""In this paper we explore the """"vector semantics"""" problem from the perspective of """"almost orthogonal"""" property of high-dimensional random vectors. We show that this intriguing property can be used to """"memorize"""" random vectors by simply adding them, and we provide an efficient probabilistic solution to the set membership problem. Also, we discuss several applications to word context vector embeddings, document sentences similarity, and spam filtering."""	anti-spam techniques;email filtering;machine learning;sentiment analysis	M. Andrecut	2018	CoRR	10.1142/S0129183118500158	discrete mathematics;mathematical analysis;mathematics;filter (signal processing);probabilistic logic;semantics;memorization	ML	-19.300241827237723	-61.55429134950997	121796
4237dec2175fc2588aa447befd9971d0686e2688	automatic chatbot knowledge acquisition from online forum via rough set and ensemble learning	ensemble learning;bagging;automatic chatbot knowledge acquisition;rough set theory;training;data mining;machine learning;feature extraction;knowledge acquisition;knowledge acquisition artificial intelligence support vector machines support vector machine classification knowledge management impedance matching discussion forums internet parallel processing process control;classification model;pattern classification;artificial intelligence;online forum;rough set classifier automatic chatbot knowledge acquisition online forum ensemble learning classification model;rough set classifier;learning artificial intelligence;rough set;child care;rough set theory knowledge acquisition learning artificial intelligence natural language processing pattern classification;natural language processing;knowledge based systems;rough set chatbot ensemble learning knowledge acquisition;knowledge base;chatbot	Existing chatbot knowledge bases are mostly hand-constructed, which is time consuming and difficult to adapt to new domains. Automatic chatbot knowledge acquisition method from online forums is presented in this paper. It includes a classification model based on rough set, and the theory of ensemble learning is combined to make a decision. Given a forum, multiple rough set classifiers are constructed and trained first. Then all replies are classified with these classifiers. The final recognition results are drawn by voting to the output of these classifiers. Finally, the related replies are selected as chatbot knowledge. Relevant experiments on a child-care forum prove that the method based on rough set has high recognition efficiency to related replies and the combination of ensemble learning improves the results.	ensemble learning;experiment;knowledge acquisition;rough set	Yu Wu;Gongxiao Wang;Weisheng Li;Zhijun Li	2008	2008 IFIP International Conference on Network and Parallel Computing	10.1109/NPC.2008.24	knowledge base;rough set;computer science;machine learning;pattern recognition;data mining;ensemble learning	AI	-19.184688267630737	-61.03662932869128	121829
0facf2520318af4b29b8505846626f66c7769117	on low overlap among search results of academic search engines		Number of published scholarly articles is growing exponentially. To tackle this information overload, researchers are increasingly depending on niche academic search engines. Recent works have shown that two major general web search engines: Google and Bing, have high level of agreement in their top search results. In contrast, we show that various academic search engines have low degree of agreement among themselves. We performed experiments using 2500 queries over four academic search engines. We observe that overlap in search result sets of any combination of academic search engines is significantly low and in most of the cases the search result sets are mutually exclusive. We also discuss implications of this low overlap.	academic search;experiment;high-level programming language;information overload;list of academic databases and search engines;niche blogging;web search engine	Anasua Mitra;Amit Awekar	2017		10.1145/3041021.3054265	computer science;data mining;world wide web;information retrieval	Web+IR	-32.616883222888596	-54.070627065432895	121892
415b54ce5599f5995ab59644204672d0b10a67d1	online social networks event detection: a survey		Today online social network services are challenging state-of-the-art social media mining algorithms and techniques due to its real-time nature, scale and amount of unstructured data generated. The continuous interactions between online social network participants generate streams of unbounded text content and evolutionary network structures within the social streams that make classical text mining and network analysis techniques obsolete and not suitable to deal with such new challenges. Performing event detection on online social networks is no exception, state-of-the-art algorithms rely on text mining techniques applied to pre-known datasets that are being processed with no restrictions on the computational complexity and required execution time per document analysis. Moreover, network analysis algorithms used to extract knowledge from users relations and interactions were not designed to handle evolutionary networks of such order of magnitude in terms of the number of nodes and edges. This specific problem of event detection becomes even more serious due to the real-time nature of online social networks. New or unforeseen events need to be identified and tracked on a real-time basis providing accurate results as quick as possible. It makes no sense to have an algorithm that provides detected event results a few hours after being announced by traditional newswire.		Mário Cordeiro;João Gama	2016		10.1007/978-3-319-41706-6_1	internet privacy	AI	-23.682651076862726	-53.64691774120308	121921
d487e75235ec7147d5c56ff024f4b595016e3322	simulation of within-session query variations using a text segmentation approach	information retrieval;ad hoc queries	We propose a generative model for automatic query reformulations from an initial query using the underlying subtopic structure of top ranked retrieved documents. We address three types of query reformulations a) specialization; b) generalization; and c) drift. To test our model we generate the three reformulation variants starting with selected fields from the TREC-8 topics as the initial queries. We use manual judgments from multiple assessors to calculate the accuracy of the reformulated query variants and observe accuracies of 65%, 82% and 69% respectively for specialization, generalization and drift reformulations.	concept drift;drift plus penalty;emoticon;generative model;partial template specialization;real life;simulation;text segmentation	Debasis Ganguly;Johannes Leveling;Gareth J. F. Jones	2011		10.1007/978-3-642-23708-9_11	natural language processing;query optimization;query expansion;web query classification;computer science;machine learning;data mining;web search query;information retrieval;query language	Web+IR	-28.280807712236022	-62.649835115001395	122025
0d36da77b017b6fea8d167fa86493339f6ca66bf	social tagging is no substitute for controlled indexing: a comparison of medical subject headings and citeulike tags assigned to 231, 388 papers	automatic taxonomy generation;automatic indexing;knowledge representation	Social tagging and controlled indexing both facilitate access to information resources. Given the increasing popularity of social tagging and the limitations of controlled indexing (primarily cost and scalability), it is reasonable to investigate to what degree social tagging could substitute for controlled indexing. In this study, we compared CiteULike tags to Medical Subject Headings (MeSH) terms for 231,388 citations indexed in MEDLINE. In addition to descriptive analyses of the data sets, we present a paper-by-paper analysis of tags and MeSH terms: the number of common annotations, Jaccard similarity, and coverage ratio. In the analysis, we apply three increasingly progressive levels of text processing, ranging from normalization to stemming, to reduce the impact of lexical differences. Annotations of our corpus consisted of over 76,968 distinct tags and 21,129 distinct MeSH terms. The top 20 tags/MeSH terms showed little direct overlap. On a paper-by-paper basis, the number of common annotations ranged from 0.29 to 0.5 and the Jaccard similarity from 2.12% to 3.3% using increased levels of text processing. At most, 77,834 citations (33.6%) shared at least one annotation. Our results show that CiteULike tags and MeSH terms are quite distinct lexically, reflecting different viewpoints/processes between social tagging and controlled indexing. © 2012 Wiley Periodicals, Inc.	folksonomy	Danielle H. Lee;Titus K Schleyer	2012	JASIST	10.1002/asi.22653	knowledge representation and reasoning;computer science;data mining;world wide web;information retrieval	Vision	-33.47628249644596	-61.57376630647489	122058
4860aa6b00100975bb9b8bda2805cb199a42bb85	discriminating between closely related languages on twitter		In this paper we tackle the problem of discriminating Twitter users by the language they tweet in, taking into account very similar South-Slavic languages – Bosnian, Croatian, Montenegrin and Serbian. We apply the supervised machine learning approach by annotating a subset of 500 users from an existing Twitter collection by the language the users primarily tweet in. We show that by using a simple bag-ofwords model, univariate feature selection, 320 strongest features and a standard classifier, we reach user classification accuracy of∼98%. Annotating the whole 63,160 users strong Twitter collection with the best performing classifier and visualizing it on a map via tweet geo-information, we produce a Twitter language map which clearly depicts the robustness of the classifier.	bag-of-words model;british informatics olympiad;definition;feature selection;geolocation;information source;machine learning;naive bayes classifier;statistical classification;supervised learning;telecentre;test set;user (computing)	Nikola Ljubesic;Denis Kranjcic	2015	Informatica (Slovenia)		natural language processing;computer science;machine learning;data mining;world wide web	NLP	-20.426110285808956	-60.01680249442818	122070
94ade3870ec0656679478fb50c36b942a5df2513	adapting support vector machines for f-term-based classification of patents	task performance;f term classification;learning algorithm;support vector machines;patent processing;process support;support vector machine;document classification;structured documents	Support Vector Machines (SVM) have obtained state-of-the-art results on many applications including document classification. However, previous works on applying SVMs to the F-term patent classification task did not obtain as good results as other learning algorithms such as kNN. This is due to the fact that F-term patent classification is different from conventional document classification in several aspects, mainly because it is a multiclass, multilabel classification problem with semi-structured documents and multi-faceted hierarchical categories.  This article describes our SVM-based system and several techniques we developed successfully to adapt SVM for the specific features of the F-term patent classification task. We evaluate the techniques using the NTCIR-6 F-term classification terms assigned to Japanese patents. Moreover, our system participated in the NTCIR-6 patent classification evaluation and obtained the best results according to two of the three metrics used for task performance evaluation. Following the NTCIR-6 participation, we developed two new techniques, which achieved even better scores using all three NTCIR-6 metrics, effectively outperforming all participating systems. This article presents this new work and the experimental results that demonstrate the benefits of the latest approach.	algorithm;faceted classification;machine learning;performance evaluation;semiconductor industry;support vector machine	Yaoyong Li;Kalina Bontcheva	2008	ACM Trans. Asian Lang. Inf. Process.	10.1145/1362782.1362786	support vector machine;web query classification;computer science;machine learning;linear classifier;multiclass classification;pattern recognition;data mining;relevance vector machine;structured support vector machine;one-class classification	Web+IR	-21.082197300019104	-64.21692798018022	122078
638608ea0d887eb1f3aba0cd2b87e61f276a404a	contextualization using hyperlinks and internal hierarchical structure of wikipedia documents	schema agnostic search;contextualization;structural indices;xml retrieval;random walks;semi structured data;re weighting	Context surrounding hyperlinked semi-structured documents, externally in the form of citations and internally in the form of hierarchical structure, contains a wealth of useful but implicit evidence about a document's relevance. These rich sources of information should be exploited as contextual evidence. This paper proposes various methods of accumulating evidence from the context, and measures the effect of contextual evidence on retrieval effectiveness for document and focused retrieval of hyperlinked semi-structured documents.  We propose a re-weighting model to contextualize (a) evidence from citations in a query-independent and query-dependent fashion (based on Markovian random walks) and (b) evidence accumulated from the internal tree structure of documents. The in-links and out-links of a node in the citation graph are used as external context, while the internal document structure provides internal, within-document context. We hypothesize that documents in a good context (having strong contextual evidence) should be good candidates to be relevant to the posed query, and vice versa.  We tested several variants of contextualization and verified notable improvements in comparison with the baseline system and gold standards in the retrieval of full documents and focused elements.	baseline (configuration management);citation graph;document;emoticon;hyperlink;relevance;semiconductor industry;tree structure;wikipedia	Muhammad Ali Norozi;Paavo Arvola;Arjen P. de Vries	2012		10.1145/2396761.2396855	semi-structured data;computer science;data mining;database;world wide web;random walk;information retrieval;statistics	Web+IR	-28.211197864602088	-62.21202221633877	122098
4e2dd75c0a0d88e487659b4ca3d4c0c8ee3f2d88	design and evaluation of an ir-benchmark for sparql queries with fulltext conditions	linked data;sparql with fulltext search;rdf	In this paper, we describe our goals in introducing a new, annotated benchmark collection, with which we aim to bridge the gap between the fundamentally different aspects that are involved in querying both structured and unstructured data. This semantically rich collection, captured in a unified XML format, combines components (unstructured text, semistructured infoboxes, and category structure) from 3.1 Million Wikipedia articles with highly structured RDF properties from both DBpedia and YAGO2. The new collection serves as the basis of the INEX 2012 Ad-hoc, Faceted Search, and Jeopardy retrieval tasks. With a focus on the new Jeopardy task, we particularly motivate the usage of the collection for question-answering (QA) style retrieval settings, which we also exemplify by introducing a set of 90 QA-style benchmark queries which come shipped in a SPARQL-based query format that has been extended by fulltext filter conditions.	benchmark (computing);crostata;dbpedia;exemplification;faceted classification;hoc (programming language);question answering;sparql;wikipedia;xml	Arunav Mishra;Sairam Gurajada;Martin Theobald	2012		10.1145/2390148.2390154	computer science;sparql;data mining;database;information retrieval	Web+IR	-33.29602066206282	-66.1059509482977	122259
3fd30ba7efc379c06923a26b993f70cfa34ae30a	information theoretical and statistical features for intrinsic plagiarism detection		In this paper we present some information theoretical and statistical features including function word skip n-grams for detecting plagiarism intrinsically. We train a binary classifier with different feature sets and observe their performances. Basically, we propose a set of 36 features for classifying plagiarized and non-plagiarized texts in suspicious documents. Our experiment finds that entropy, relative entropy and correlation coefficient of function word skip n-gram frequency profiles are very effective features. The proposed feature set achieves F-Score of 85.10%.	binary classification;coefficient;f1 score;grams;information theory;interpupillary distance;item response theory;kullback–leibler divergence;n-gram;performance;sensor;text corpus	Rashedur Rahman	2015			data science;machine learning;pattern recognition	ML	-19.66692750924385	-61.71768442835238	122371
043ed82643699cad19cb8976dd1d828e27b7b9c4	large-scale evaluation infrastructure for information access technologies enhancement and creativity	cross lingual efforts;text mining;information retrieval;large scale evaluations;nii test collection;large scale evaluation infrastructure;social infrastructures;text summarization;information access;large scale systems information retrieval informatics testing text mining information technology internet research and development solids humans;large scale;research and development;large scale infrastructures;internet;reliable large scale evaluation large scale evaluation infrastructure information access nii test collection information retrieval text summarization question answering text mining cross lingual efforts large scale evaluations internet social infrastructures large scale infrastructures;reliable large scale evaluation;internet information retrieval;it evaluation;cumulant;test collection;question answering	"""This paper introduces the NTCIR (NII Test Collection for Information Retrieval and access technologies) Project and its evaluation workshop series called NTCIR Workshops, which are designed to enhance research in information access technologies, such as information retrieval, text summarization, question answering, text mining, and their cross-lingual efforts by providing infrastructure for large-scale evaluations. With prosperity of the Internet, information access technologies have become one of the very fundamental social infrastructures and its importance has been increased tremendously. Research and development of information access technologies require solid evidence based on experiments to show the superiority of the newly proposed system and/or strategy over previous ones. NTCIR has provided large-scale infrastructures usable for such testing and evaluation. Conducting meaningful and reliable large-scale evaluation is not easy - The success criteria for information access are human judgments of """"relevance"""", which are not consistent across assessors and over time. Difficulty of the information access varies according to the document sets, users' search requests, and users' tasks or situation. Under such circumstances, how to perform more reliable, stable and sensible evaluation are always challenging. We have cumulated insights on these matters working together with participants and other evaluation projects of the world like TREC, CLEF and DUC. We hope that the NTCIR Workshops serve for those areas and that wide-ranging insights have been produced as results To conclude, some thoughts on future directions are suggested."""	automatic summarization;experiment;information access;information retrieval;internet;national information infrastructure;question answering;relevance;text retrieval conference;text mining;upsampling	Noriko Kando	2007	7th IEEE International Conference on Computer and Information Technology (CIT 2007)	10.1109/CIT.2007.189	text mining;the internet;question answering;computer science;data mining;database;world wide web;information retrieval;cumulant	Web+IR	-31.584944744262593	-62.12588626069738	122407
1e94fa530df3403b3afb4112640ecfa90d833d5a	the lailaps search engine: relevance ranking in life science databases.	search engine;relevance ranking;life sciences	Search engines and retrieval systems are popular tools at a life science desktop. The manual inspection of hundreds of database entries, that reflect a life science concept or fact, is a time intensive daily work. Hereby, not the number of query results matters, but the relevance does. In this paper, we present the LAILAPS search engine for life science databases. The concept is to combine a novel feature model for relevance ranking, a machine learning approach to model user relevance profiles, ranking improvement by user feedback tracking and an intuitive and slim web user interface, that estimates relevance rank by tracking user interactions. Queries are formulated as simple keyword lists and will be expanded by synonyms. Supporting a flexible text index and a simple data import format, LAILAPS can easily be used both as search engine for comprehensive integrated life science databases and for small in-house project databases. With a set of features, extracted from each database hit in combination with user relevance preferences, a neural network predicts user specific relevance scores. Using expert knowledge as training data for a predefined neural network or using users own relevance training sets, a reliable relevance ranking of database hits has been implemented. In this paper, we present the LAILAPS system, the concepts, benchmarks and use cases. LAILAPS is public available for SWISSPROT data at http://lailaps.ipk-gatersleben.de.	artificial neural network;biological neural networks;biological science disciplines;desktop computer;estimated;extraction;feature model;interaction;keyword;learning to rank;life support systems;machine learning;published database;relevance;thrombocytopenia;uniprot;user interface device component;web search engine;search a word	Matthias Lange;Karl Spies;Joachim Bargsten;Gregor Haberhauer;Matthias Klapperstück;Michael Leps;Christian Weinel;Röbbe Wünschiers;Mandy Weißbach;Jens Stein;Uwe Scholz	2010	Journal of integrative bioinformatics	10.2390/biecoll-jib-2010-110	computer science;data science;data mining;search analytics;information retrieval;search engine	Web+IR	-32.55858868533747	-56.352241726654164	122427
52dc3248a034f6b496ec63bb00cd1e2ab02f09df	implicit queries for email	logistic regression;logistic regression model;search engine	Implicit query systems examine a document and automatically conduct searches for the most relevant information. In this paper, we offer three contributions to implicit query research. First, we show how to use query logs from a search engine: by constraining results to commonly issued queries, we can get dramatic improvements. Second, we describe a method for optimizing parameters for an implicit query system, by using logistic regression training. The method is designed to estimate the probability that any particular suggested query is a good one. Third, we show which features beyond standard TF-IDF features are most helpful in our logistic regression model: query frequency information, capitalization information, subject line information, and message length information. Using the optimization method and the additional features, we are able to produce a system with up to 6 times better results on top-1 score than a simple TF-IDF system.	computer-mediated communication;email;information retrieval;logistic regression;mathematical optimization;tf–idf;web search engine	Joshua Goodman;Vitor R. Carvalho	2005			logistic model tree;data mining;logistic regression;search engine;computer science	Web+IR	-32.619275027869925	-54.0707830125045	122713
17c4a09a0a1d2bed8353c1901f50755c56be4723	hht-svm: an online method for detecting profile injection attacks in collaborative recommender systems	hilbert huang transform;collaborative recommendation;online attack detection;support vector machine;profile injection attacks	Collaborative recommender systems are known to be particularly vulnerable to profile injection attacks, in which malicious users insert fake profiles into the rating database in order to bias the systems’ output. To reduce this risk, a number of methods have been proposed to detect such attacks. However, almost all of them operate in batch mode, i.e., they require examining and processing the entire rating database to detect the attacks. With this problem in mind, we propose an online method (called HHT–SVM) to detect profile injection attacks by combining Hilbert–Huang transform (HHT) and support vector machine (SVM), which can operate incrementally. The underpinning idea of HHT–SVM is the feature extraction method based on an individual user profile. In this paper, we first construct rating series for each user profile based on the novelty and popularity of items. Then, by introducing HHT we use the empirical mode decomposition (EMD) approach to decompose each rating series and extract Hilbert spectrum based features to characterize the profile injection attacks. Finally, we exploit SVM to detect profile injection attacks based on the proposed features. We conduct experiments on the MovieLens 1M dataset and compare the performance of HHT–SVM with PCA-VarSelect and Batch-SVM to demonstrate the effectiveness of the proposed approach.	recommender system;sensor;support vector machine	Fuzhi Zhang;Quanqiang Zhou	2014	Knowl.-Based Syst.	10.1016/j.knosys.2014.04.020	support vector machine;computer science;hilbert–huang transform;machine learning;data mining;world wide web;computer security	AI	-19.260493600891934	-54.89520909545766	122748
89093102985796abf9ee9e94110e6d86bc2d0a06	the termolator: terminology recognition based on chunking, statistical and search-based scores		The Termolator is an open-source high-performing terminology extraction system, available on Github. The Termolator combines several different approaches to get superior coverage and precision. The in-line term component identifies potential instances of terminology using a chunking procedure, similar to noun group chunking, but favoring chunks that contain out-of-vocabulary words, nominalizations, technical adjectives, and other specialized word classes. The distributional component ranks such term chunks according to several metrics including: (a) a set of metrics that favors term chunks that are relatively more frequent in a “foreground” corpus about a single topic than they are in a “background” or multi-topic corpus; (b) a well-formedness score based on linguistic features; and (c) a relevance score which measures how often terms appear in articles and patents in a Yahoo web search. We analyse the contributions made by each of these components and show that all modules contribute to the system’s performance, both in terms of the number and quality of terms identified. This paper expands upon previous publications about this research and includes descriptions of some of the improvements made since its initial release. This study also includes a comparison with another terminology extraction system available on-line, Termostat (Drouin, 2003). We found that the systems get comparable results when applied to small amounts of data: about 50% precision for a single foreground file (Einstein’s Theory of Relativity). However, when running the system with 500 patent files as foreground, Termolator performed significantly better than Termostat. For 500 refrigeration patents, Termolator got 70% precision vs. Termostat’s 52%. For 500 semiconductor patents, Termolator got 79% precision vs. Termostat’s 51%.	computer performance;emoticon;numerical relativity;online and offline;open-source software;relevance;semiconductor;shallow parsing;terminology extraction;vocabulary;web search engine	Adam Meyers;Yifan He;Zachary Glass;Olga Babko-Malaya	2015		10.3389/frma.2018.00019	data mining;nominalization;information extraction;terminology;noun;computer science;technology forecasting;chunking (psychology);terminology extraction	NLP	-31.617172393583694	-65.87665732142062	122858
8802c94f84ad5144094f5c8f3ebd546d5adbd64f	sentiment analysis of wine aroma	svm wine aroma sentiment analysis machine classification wine reviews support vector machine;feature selection sentiment analysis svm;support vector machines marketing data processing pattern classification reviews sentiment analysis;sentiment analysis;svm;feature selection;support vector machines wheels manuals sentiment analysis feature extraction information science electrical engineering	"""It has been easy for us to send information thanks to the growth of the Internet. Information includes reputation. Analysis of the large amount of reputation and making summary of the analysis is very helpful for consumers to decide which goods they should buy. They are also helpful for companies in order to make marketing decisions. Many researches try to classify documents on whether they have positive or negative sentiment. In this paper, we focus on more complex sentiment. We will show a result of machine classification of wine reviews from the point of view of """"aroma"""" using Support Vector Machine (SVM)."""	internet;sentiment analysis;support vector machine	Nao Wariishi;Brendan Flanagan;Takahiko Suzuki;Sachio Hirokawa	2015	2015 IIAI 4th International Congress on Advanced Applied Informatics	10.1109/IIAI-AAI.2015.253	engineering;machine learning;pattern recognition;data mining;sentiment analysis	ML	-22.40748462121941	-57.17068594132327	122897
bf026eb7195aba2547b29a7d8f999781d704ca7e	towards a new reading experience via semantic fusion of text and music	bag of words model;in vocabulary;digital library;canonical correlation analysis;latent semantic analysis;bag of words;semantic association	In CADAL, there preserve a lot of Chinese classical literatures, including graceful prose and verse. These works written in ancient Chinese comparatively are concise in vocabulary and sentence patterns. But they express rich feelings and convey a wealth of information. Although can be explained in modern Chinese, the aesthetic sense in those works disappears. So we aim to illustrate the feeling in these works using Chinese traditional music which is also another part of Chinese culture. This is an interesting and challenging work. In this paper, the correlation between the text and music is studied. A novel approach is proposed to model the latent semantic association underlying the two medium. Based on the correlation model we learned from training data, we can associate a literary work (mainly verse and prose in our digital library) with a few music pieces automatically. When a reader is appreciating a literary work, a piece of background music is playing meanwhile, the information and emotion implied by the work and music blend together. The reader may be immersed into the emotion and obtain aesthetic enjoyment intensively. We implement the proposed method and design experiments to evaluate the performance of it. The experimental result substantiates the feasibility of the proposed approach in this paper.	digital library;experiment;verse protocol;vocabulary	Ling Zhuang;Zhenchao Ye;Jiangqin Wu;Feng Zhou;Jian Shao	2011		10.1145/1998076.1998105	natural language processing;digital library;computer science;bag-of-words model;multimedia;world wide web	AI	-23.006369000873004	-59.05373928374377	122902
a7d9de4f5907b78741cc4aef8cf0193802db2150	on textual documents classification using fourier domain scoring	textual document classification;information retrieval;fourier domain scoring;text analysis;classification;textual document ranking;text classification;fourier discrete transformation;spatial information information retrieval text classification text representation word signal discrete transforms;discrete transforms;word signal;discrete cosine transforms;text representation;information retrieval textual document classification textual document ranking fourier domain scoring fourier discrete transformation cosine discrete transformation;cosine discrete transformation;document classification;kernel frequency performance evaluation computer science information retrieval text categorization discrete transforms data mining computational efficiency shape;text analysis classification discrete cosine transforms discrete fourier transforms information retrieval;discrete fourier transforms;spatial information	Recently, Fourier and cosine discrete transformations have been proposed for textual document ranking. The advantage of the methods is that not only the count of a frequency term within documents used; the spatial information about presence of the term is also considered. Here, this novel approach is used to improve performance of classifiers	ranking (information retrieval)	Michal Pryczek;Piotr S. Szczepaniak	2006	2006 IEEE/WIC/ACM International Conference on Web Intelligence (WI 2006 Main Conference Proceedings)(WI'06)	10.1109/WI.2006.125	text mining;speech recognition;biological classification;computer science;pattern recognition;spatial analysis;information retrieval	DB	-22.240373854096326	-63.456989525642314	122974
582c19761984bfe3df071cb8e126845e5f492abf	semantic clustering of web documents: an ontology based approach using swarm intelligence	semantic similarity;k means;clustering;particle swarm optimization;ontology	With the massive growth and large volume of the web it is very difficult to recover results based on the user preferences. The next generation web architecture, semantic web reduces the burden of the user by performing search based on semantics instead of keywords. Even in the context of semantic technologies optimization problem occurs but rarely considered. In this paper document clustering is applied to recover relevant documents. The authors propose an ontology based clustering algorithm using semantic similarity measure and Particle Swarm Optimization (PSO), which is applied to the annotated documents for optimizing the result. The proposed method uses Jena API and GATE tool API and the documents can be recovered based on their annotation features and relations. A preliminary experiment comparing the proposed method with K-Means shows that the proposed method is feasible and performs better than K-Means. DOI: 10.4018/jitwe.2012100102 International Journal of Information Technology and Web Engineering, 7(4), 20-33, October-December 2012 21 Copyright © 2012, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited. Salton et al. (1989) shows that most of the document clustering approaches use Vector Space Model (VSM) for document representation. But using VSM ignores the semantic relatedness among documents. For example, having “fruits” in one document and “Apple” in another document does not contribute to similarity measurement unless semantic relatedness is considered. Semantic relationship is not included in most of the clustering approaches. According to Maedche, Staab, Stojanovic, Studer, and Sure (2003) use of ontology provides a good background knowledge and improves document clustering. Recent works has shown that ontology is useful to improve the performance of text clustering in these situations. Currently a challenge when querying information using semantics offered by ontology is how to extract information from ontology more efficiently (Aleman-Meza, Halaschek, Arpinar, & Sheth, 2003). Semantic annotation is about assigning to the entities in the text links to their semantic description (Maedche, Staab, Stojanovic, Studer, & Sure, 2003). Annotation provides additional information about web contents so that better decision on content can be made. Annotation of ontology tells us what kind of property and value types should be used in describing a resource. The usage of domain ontologies are used for annotation. The manual annotation of document is of high cost and error prone task. However there is still some work to do achieve a complete automation of annotation. The classical model is incapable of supporting logical inference. In this paper we propose an ontology based information retrieval model which uses PSO to improve clustering of web documents. Ontology similarity is used to identify the importance of concepts in the document. Particle Swarm Optimisation is used to cluster the documents since it is effective for global search in finding solutions to nondeterministic problems. Moreover Particle Swarm Optimisation method enhances adaptability of meta searching. Performance of PSO based clustering is evaluated using K-Means algorithm. The proposed model uses semantics and relationship available in the knowledge base to improve the relevancy of documents. The rest of the paper is organized as follows: Section 2 highlights the previous research in the related area. Section 3 describes the document representation and the methodology used for similarity calculation. Section 4 describes the clustering approach. Section 5 describes experimental results and discussion. Finally, we offer concluding remarks and describe future directions of our research work.	algorithm;apache jena semantic web framework;application programming interface;artificial intelligence;cluster analysis;cognitive dimensions of notations;entity;gate;information retrieval;international journal of information technology;k-means clustering;knowledge base;mathematical optimization;ontology (information science);optimization problem;particle swarm optimization;relevance;semantic similarity;similarity measure;swarm intelligence;user (computing);viable system model;web engineering;web page	J. Avanija;K. Ramar	2012	IJITWE	10.4018/jitwe.2012100102	semantic similarity;computer science;machine learning;ontology;social semantic web;data mining;database;cluster analysis;particle swarm optimization;world wide web;information retrieval;k-means clustering	AI	-28.295775437086593	-59.55590577659381	123063
cfebcec840747ec4d25c5b2d92de6db537e5eef0	kecir question answering system at ntcir7 cclqa		At the NTCIR-7 CCLQA (Complex Cross-Language Question Answering) task, we participated in the Chinese-Chinese (C-C) and English-Chinese (E-C) QA (Question Answering) subtasks. In this paper, we describe our QA system, which includes modules for question analysis, document retrieval, information extraction and answer generation. Besides, we used an online MT (Machine Translation) system to deal with question translation in our E-C task. An overall analysis and a detailed module-by-module analysis are presented. Since document retrieval is an essential part of CLQA, we also did experiments and submit QA results using IR4QA results in order to find out which IR technique would help CCLQA.	alessandro vespignani;call of duty: black ops;chua's circuit;computation;computational linguistics;document retrieval;entity–relationship model;experiment;hideki imai;information access;information extraction;kaisa nyberg;latent semantic analysis;machine translation;named entity;question answering;signal-to-noise ratio;software quality assurance;steven anson coons;www;yang;eric	Yu Bai;Li Guo;Lei Liu;Dongfeng Cai;Bo Zhou	2008			computer science;data mining;world wide web;information retrieval	NLP	-30.86465879838945	-64.79686411846954	123371
91eab3d9d584cb8b1c5b846a75e1e0cfde924018	developing an efficient fuzzy model for phishing identification	google;membership functions fuzzy model phishing identification internet commerce uniform resource locator address url address if then rule sets;uniform resource locators feature extraction google testing accuracy training heuristic algorithms;fuzzy system phishing url based;training;testing;accuracy;heuristic algorithms;feature extraction;fuzzy set theory computer crime fuzzy logic;uniform resource locators	The explosive growth of Internet commerce has made phishers who may attempt to create phishing sites aimed to steal personal information such as password, banking account and credit card account details, etc. Most of these phishing pages look similar to the real pages in terms of interface and uniform resource locator (URL) address. Many techniques have been proposed to identify phishing sites. However, the numbers of victims have been increasing due to inefficient protection technique. In this paper, we develop a fuzzy model for phishing identification efficiently. The model eliminates the subjective factors to improve efficiency such as if-then rule sets, the parameters of membership functions, etc. Moreover, the efficiency features for identifying phishing were used for the fuzzy model. The proposed technique is evaluated with the datasets of 11,660 phishing sites and 5,000 legitimate sites. The results show that the proposed technique can identify over 99% phishing sites.	artificial neural network;e-commerce;heuristic (computer science);online locator service;pagerank;password;personally identifiable information;phishing;subject (philosophy);uniform resource identifier	Luong Anh Tuan Nguyen;Huu Khuong Nguyen	2015	2015 10th Asian Control Conference (ASCC)	10.1109/ASCC.2015.7244834	engineering;data mining;world wide web;computer security	Web+IR	-27.03691035414679	-55.77472595529841	123377
ee3f04eb4dfcaea88040d45f78d6c47d1e4e3ece	introduction to special issue on machine learning for business applications	machine learning	"""In recent years we have witnessed a dramatic increase in novel uses of machine learning for business applications. These applications cover a wide range of traditional as well as new business activities from intelligent customer segmentation for direct marketing to intelligent stock market analysis and the analysis of the long tail in the new Web-based economy. Innovative machine-learning techniques that make use of different aspects of newly emerged business data, such as customer feedback and behavior data and social network data are under development. To further foster the advances of the most recent and exciting work on machine learning for business applications, we initiated this special issue. The response to our call for papers was very strong. After careful reviews of the submissions by international experts in the area, we have selected six articles for this special issue. Since the theme of the special issue is business applications, which relates to money, three of the six articles are about applying machine learning and data mining to the financial market. In """" Prediction in Financial Markets: The Case for Small Disjuncts """" , Vasant Dhar proposes that a counterintuitive method of learning many small disjuncts instead of a single model can provide a credible model for financial market prediction, a problem with a high degree of noise. His results suggest that for problems characterized by a high degree of noise and lack of a stable target concept, which are widespread in many real-world applications, constructing sets of small rules and then reconstructing them periodically is a promising approach. In """" A Learning-Based Contrarian Trading Strategy via a Dual-Classifier Model """" , Szu-Hao Huang et al. first discuss a return anomaly proposed by behavioral financial experts called overreaction. Overreaction means that the past winning and losing stocks will have return reversal after a holding period of certain lengths. This phenomenon can be exploited in automatic trading approaches including the contrarian trading strategy. The authors develop a novel dual-classification method for the con-trarian trading strategy to improve the reliability of predicting stocks with the return reversal to form better portfolios. Various experiments showed that their system can increase the returns of portfolios impressively. However, their method does not consider the transaction cost in stock trading. In """" CORN : Correlation-Driven Nonparametric Learning Approach for Portfolio Selection """" , Li et al. present a statistical correlation-based method named CORN to select the best stocks …"""	anomaly detection;data mining;experiment;long tail;machine learning;naruto shippuden: clash of ninja revolution 3;social network	Charles X. Ling	2011	ACM TIST	10.1145/1961189.1961190	robot learning;computer science;machine learning;hyper-heuristic	ML	-20.213624923174113	-53.015460337477926	123537
3314541bbff07f337628ba7a1175148e1ccc6ba3	keyword generation for search engine advertising using semantic similarity between terms	semantic similarity;search engine;sponsored search;kernel function;keyword generation;search engine optimization	An important problem in search engine advertising is key-word1 generation. In the past, advertisers have preferred to bid for keywords that tend to have high search volumes and hence are more expensive. An alternate strategy involves bidding for several related but low volume, inexpensive terms that generate the same amount of traffic cumulatively but are much cheaper. This paper seeks to establish a mathematical formulation of this problem and suggests a method for generation of several terms from a seed keyword. This approach uses a web based kernel function to establish semantic similarity between terms. The similarity graph is then traversed to generate keywords that are related but cheaper.	contextual advertising;dictionary;relevance;semantic similarity;text corpus;web search engine;wordnet	Vibhanshu Abhishek;Kartik Hosanagar	2007		10.1145/1282100.1282119	semantic search;computer science;data mining;keyword density;search analytics;world wide web;information retrieval;search engine	Web+IR	-30.84631456504256	-54.82346984738357	123692
bcc6589005edf8d80842c2c492cba87f63e44d8b	dynamic structuring of web information for access visualization	document clustering;web documents;wireless channels;web pages;information extraction;global information infrastructure;dom;three dimensional;visualization;mobile web;xml;vrml;web document structure;proxy server	The Internet has led to the formation of a global information infrastructure. To explore a web site, a site map would be useful as a short cut for a user to locate for the target information in a structured and efficient manner, rather than drilling into the web site following hyperlinks, reading possibly irrelevant information. Useless information impacts a mobile web environment, where mobile clients are only connected with unreliable wireless channels of limited bandwidth. Structured web page organization at the web server proxy is an important issue to resolve to provide efficient browsing experience for web clients, while minimizing the browsing of unrelated pages or sites. In this paper, we adopt the Document Information Extraction mechanism to construct a document cluster dynamically and intelligently with respect to a requested root web page. The document cluster works like a dynamic site map, spanning across several web sites. The clusters are generated and stored in XML format at proxy server so that it can potentially benefit a large number of mobile clients. Clients process the XML clusters and transform them to be visualized through VRML or DOM. For VRML, a transformer is built at the client-side to support a three-dimensional modeling view. For DOM, JavaScript is used for accessing the parsed XML data to produce a two-dimensional tree output.	client-side;dimensional modeling;document object model;file spanning;hyperlink;information extraction;internet;javascript;parsing;proxy server;relevance;server (computing);site map;transformer;vrml;web page;web server;xml	Jess Y. S. Mak;Hong Va Leong;Alvin T. S. Chan	2002		10.1145/508791.508942	web service;ajax;three-dimensional space;document object model;static web page;web development;web modeling;site map;xml;data web;vrml;web analytics;mobile web;visualization;web mapping;document clustering;clickstream;web design;web standards;computer science;web api;operating system;dynamic web page;web navigation;web page;data mining;database;web 2.0;world wide web;information extraction;web server;mashup	Web+IR	-30.00493675211485	-53.31461994495027	123902
c66e99f45d045e9a57f7fa37479409bcb210a300	extracting user behavior-related words and phrases using temporal patterns of sequential pattern evaluation indices		The growth of social media sites, such as Twitter, which can provide a visual record of the daily interests and concerns of people in the form of their tweets and tweeting behaviors, has led to an increasing demand among enterprise users, to be able to identify those users who are interested in the services and products that these enterprises offer. However, accurately determining whether people who receive information, such as tweets, from enterprise users have a genuine interest in it can be difficult. In this study, a method for extracting feature words and phrases from the past users’ tweets using temporal patterns of sequential pattern evaluation indices and phrase importance evaluation indices is developed. In this method, a variety of the followers interests are first analyzed using the feature words and phrases retweeted by the followers. Next, the temporal patterns of each evaluation index that are created based on the usage frequencies of feature words and phrases obtained from the historical followers’ tweeting behaviors are extracted. An experimental result has shown that this method successfully extracted the sets of words and phrases based on the followers’ tweeting behaviors as the temporal patterns for each evaluation index and the following retailer’s account. These sets of words and phrases lead to understand the variety of the followers’ interests with more clues.	predictive modelling;reblogging;retransmission (data networks);social media	Hidenao Abe	2016	Vietnam Journal of Computer Science	10.1007/s40595-016-0084-y	computer science;data mining;multimedia;world wide web	DB	-21.977046893718423	-52.5931452597277	123985
4e3d04ab1135faf27cf3fa4d866a80c16c90d9ec	leveraging web intelligence for finding interesting research datasets	unsupervised learning;context search engines standards google semantics measurement artificial intelligence;query processing;search engines;unsupervised learning internet meta data pattern matching query processing recommender systems search engines;web knowledge;internet;pattern matching;web intelligence research datasets user interest interest item matching recommendation systems search engines metadata information algorithmic approach missing item context unsupervised algorithm item ranking dataset recommendation performance evaluation user queries dataset finding algorithm;meta data;web knowledge search engines recommender systems interest item matching;recommender systems;interest item matching	The problem of user's interest to item matching is at the core of recommendation systems and search engines. This problem is well studied in different contexts such as item, document, music and movie recommendations. For the purpose of recommendation these systems store the context or the meta-data information about the item of interest (e.g. user rating for books, tags, price etc). However, the general approaches for finding relevant items for recommendation cannot be directly applied in the case when the context or meta-data information about the item of interest is missing. In this paper we describe an algorithmic approach to handle this problem of missing context for items. In the proposed approach we have extended the context of user's interest and developed an unsupervised algorithm to find the items of interest for the user. Finally the items are ranked based on their relevance to the user's interest. We study this problem in the domain of dataset recommendation where the meta-data information about the datasets is missing due to lack of coherent and complete repository for the research datasets. We evaluate the performance of the proposed framework with real world dataset consisting of 20 user queries. We find that the proposed framework can recommend datasets for user queries with a recall of 90% in the top-4 recommendations. We also compared the performance of the dataset finding algorithm with the state of art supervised classification approach. We get a significant improvement of 36% using the proposed algorithm.	algorithm;book;coherence (physics);machine learning;recommender system;relevance;supervised learning;web intelligence;web search engine	Ayush Singhal;Ravindra Kasturi;Vidyashankar Sivakumar;Jaideep Srivastava	2013	2013 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)	10.1109/WI-IAT.2013.46	unsupervised learning;the internet;computer science;machine learning;pattern matching;data mining;metadata;world wide web;information retrieval	Web+IR	-27.555672241185636	-53.5372461608838	124138
eb4f3b681d1802ab4ebccb7b586b8ec20c1fdb2c	annotating attribution relations and their features	information extraction;quotations;language technology;attribution relations	Attribution has attracted increasing attention in recent years following the development of Opinion Analysis and Information Extraction applications. Given a piece of information or an opinion, being able to correctly identify its source (either a specific entity e.g. President Obama or a class thereof e.g. experts, official sources, rumours) and determine how different sources and features of attribution affect the way information is perceived would be extremely beneficial. It would in fact enhance opinion-oriented applications of Language Technology and revolutionise the way we can select information, e.g. on the basis of source expertise and reliability. However, approaches to the automatic extraction of attribution relations have achieved only partial results to date and are therefore not sufficient to enable the development of reliable applications. In addition, there has been little or no attempt to discern the impact of different sources and attribution strategies on the perception and interpretation of the attributed material. Grounded on previous analysis of this relation in Italian [2] and English [4] news corpora, this study addresses several of the attribution strategies identified to develop a more comprehensive system for the extraction of attributions and their relevant features from news texts.	information extraction;language technology;news aggregator;text corpus	Silvia Pareti	2011		10.1145/2064713.2064725	psychology;artificial intelligence;data mining;social psychology	NLP	-22.489574167914906	-59.7487906619594	124413
d0d5ae2b7af60bde970e67540d60e33d3ac5190b	matching and integration across heterogeneous data sources	equivalence class detection;database alignment;environmental protection agency;mutual information;information theory;heterogeneous data sources	A sea of undifferentiated information is forming from the body of data that is collected by people and organizations, across government, for different purposes, at different times, and using different methodologies. The resulting massive data heterogeneity requires automatic methods for data alignment, matching and/or merging. In this poster, we describe two systems, Guspin™ and Sift™, for automatically identifying equivalence classes and for aligning data across databases. Our technology, based on principles of information theory, measures the relative importance of data, leveraging them to quantify the similarity between entities. These systems have been applied to solve real problems faced by the Environmental Protection Agency and its counterparts at the state and local government level.	data structure alignment;database;entity;information theory;scale-invariant feature transform;turing completeness	Patrick Pantel;Andrew Philpot;Eduard H. Hovy	2006		10.1145/1146598.1146738	computer science;data science;data mining;information retrieval	ML	-23.614623310193114	-62.42893075641291	124559
75ae52cdedb47e645bbc292659c8fd1ef03a83f3	computing entity semantic similarity by features ranking		This article presents a novel approach to estimate semantic entity similarity using entity features available as Linked Data. The key idea is to exploit ranked lists of features, extracted from Linked Data sources, as a representation of the entities to be compared. The similarity between two entities is then estimated by comparing their ranked lists of features. The article describes experiments with museum data from DBpedia, with datasets from a LOD catalog, and with computer science conferences from the DBLP repository. The experiments demonstrate that entity similarity, computed using ranked lists of features, achieves better accuracy than state-of-the-art measures.	computer science;dbpedia;entity;experiment;linked data;semantic similarity	Lívia Ruback;Claudio Lucchese;Alexander Arturo Mera Caraballo;Grettel Monteagudo Garc'ia;Marco A. Casanova;Chiara Renso	2018	CoRR			NLP	-28.876568595487264	-62.63110374202983	124720
e19a5dfe34a973b414d9be8b2f78a251d3452316	investigating the relevance of sponsored results for web ecommerce queries	search engine;sponsored search;web searching;web search engine;business model;sponsored links;sponsored results;web search;web search engines;ecommerce searching	Are sponsored links, the primary business model for Web search engines, providing Web consumers with relevant results? This research addresses this issue by investigating the relevance of sponsored and non-sponsored links for ecommerce queries from the major search engines. The results show that average relevance ratings for sponsored and non-sponsored links are virtually the same, although the relevance ratings for sponsored links are statistically higher. We used 108 ecommerce queries and 8,256 retrieved links for these queries from three major Web search engines, Google, MSN, and Yahoo!. We present the implications for Web search engines and sponsored search as a long-term business model as well as a mechanism for finding relevant information for searchers.	e-commerce;relevance;search engine marketing;web search engine;world wide web	Bernard J. Jansen	2007		10.1145/1277741.1277944	business model;backlink;metasearch engine;web search engine;semantic search;computer science;web crawler;data mining;web search query;world wide web;information retrieval;search engine	Web+IR	-32.50651768737055	-54.196470829398606	124752
4c2258fc5c960c716326431b21a046c427fba43a	discovering word associations in news media via feature selection and sparse classification	classification algorithm;logistic regression;regularization;text classification;human subjects;difference scheme;sparsity;news media;evaluation;feature selection;new media;logistic regression model	"""We analyze the """"image"""" of a given query word in a given corpus of text news by producing a short list of other words with which this query is strongly associated. We use a number of feature selection schemes for text classification to help in this task. We apply these classification techniques using indicators of the query word's appearance in each document used as the document """"labels"""" and the indicators for all other words as document predictors/features. The features selected by any scheme is then considered the list of words comprising the query word's """"image"""".  To be easily understandable, a list should be extremely short with respect to the dictionary of terms present in the corpus. The approach thus requires aggressive feature (word) selection in order to single out at most a few tens of terms in a universe of hundreds of thousands or more. In addition, a word imaging scheme should scale well with the size of data (number and size of documents, size of dictionary).  We produce one scheme for feature selection through a sparse classification model. A standard classification algorithm assigns one weight per term in the predictor dictionary, in order to maximize the capacity to successfully predict the labels of document units. By imposing a sparsity constraint on the weight vector, we single out the few words that are most able to predict the presence or absence of a query word in any document. This paper compares this and several other schemes that are potentially well suited to the task of word imaging, each method presenting a different manner of feature selection.  We present two evaluations of these schemes. One evaluates the predictive classification performance of a logistic regression model trained over the corpus using only a scheme's selected features. The other is based on the judgement of human readers: a pair of word lists generated by different schemes operating on identical queries are presented to a human subject alongside a trio of document units (paragraphs) containing the query word. This subject then chooses the list which in his/her estimation is the best summary of the document units.  We apply these schemes to study the images of frequently covered countries and regions in recent news articles from the International section of the New York Times. Our preliminary experiments indicate that, while most methods perform similarly well on our data in terms of predictive performance, human-based evaluations appear to favor features selected by training of sparse logistic regression, a penalized variant of logistic regression that encourages sparse classifiers. This indicates that classification metrics based on pure predictive performance, while useful as a indicator for pre-selecting algorithms, are not enough to predict human assessment of word association algorithms."""	algorithm;data dictionary;dictionary attack;document classification;experiment;feature selection;kerrison predictor;logistic regression;microsoft word for mac;s3 trio;sparse matrix;text corpus;the new york times	Brian Gawalt;Jinzhu Jia;Luke Miratrix;Laurent El Ghaoui;Bin Yu;Sophie Clavier	2010		10.1145/1743384.1743421	web query classification;computer science;machine learning;pattern recognition;data mining;logistic regression;feature selection;tf–idf;world wide web;information retrieval;statistics	Web+IR	-23.118419219174864	-63.39373611131218	124861
00081d576cb772cc680d2621a4278ea2fce48f31	text summarization based on conceptual data classification	galois connection;text summarization;classification;conceptual data classification;conceptual learning;data classification	In this article, we present an original approach for text summarization using conceptual data classification. We show how a given text can be summarized without losing meaningful knowledge and without using any semantic or grammatical concepts. In fact, concept date classification is used to extract the most interacting sentences from the main text and ignoring the other meaningless sentences in order to generate the text summary. The approach is tested on Arabic and English texts with different sizes and different topics and the obtained results are satisfactory. The system may be incorporated with the indexers of search engines over the Internet in order to find key words and other pertinent information of the new deployed Web pages that would be stored in databases for quick search.		Jihad Mohamad Jaam;Ali Jaoua;Ahmad Hasnah;F. Hassan;H. Mohamed;T. Mosaid;H. Saleh;F. Abdullah;Habib Cherif	2006	IJITWE	10.4018/jitwe.2006100102	concept learning;biological classification;computer science;automatic summarization;machine learning;pattern recognition;data mining;information retrieval;conceptual clustering	Web+IR	-26.058943478621124	-65.64866091340319	125044
12ca0368e1b63b27f3d8d8b227fb4a4bddfd907a	discovery of dependency tree patterns for relation extraction	conference paper	Relation extraction is to identify the relations between pairs of named entities. In this paper, we try to solve the problem of relation extraction by discovering dependency tree patterns (a pattern is an embedded sub dependency tree indicating a relation instance). Our approach is to find an optimal rule (pattern) set automatically based on the proposed dependency tree pattern mining algorithm. The experimental results show that the extracted patterns can achieve a high precision and a reasonable recall rate when used as rules to extract relation instances. Furthermore, an additional experiment shows that other machine learning based relation extraction methods can also benefit from the extracted patterns by using them as features.	algorithm;data mining;embedded system;machine learning;named entity;relationship extraction;sensitivity and specificity	Hongzhi Xu;Changjian Hu;Guoyang Shen	2009			machine learning;pattern recognition;data mining;mathematics	NLP	-26.292923785995196	-65.44090432080495	125294
c520c1bfcdd746139c91abf09d3bd59c4e649a8f	qa@inex track 2011: question expansion and reformulation using the reg summarization system		In this paper, our strategy and results for the INEX@QA 2011 question-answering task are presented. In this task, a set of 50 documents is provided by the search engine Indri, using some queries. The initial queries are titles associated with tweets. Reformulation of these queries is carried out using terminological and named entities information. To design the queries, the full process is divided into 2 steps: a) both titles and tweets are POS tagged, and b) queries are expanded or reformulated, using: terms and named entities included in the title, terms and named entities found in the tweet related to those ones, and Wikipedia redirected terms and named entities from those ones included in the title. In our work, the automatic summarization system REG is used to summarize the 50 documents obtained with these queries. The algorithm models a document as a graph to obtain weighted sentences. A single document is generated and it is considered the answer of the query. This strategy, combining summarization and question reformulation, obtains good results regarding informativeness and readability.		Jorge Vivaldi;Iria da Cunha	2011		10.1007/978-3-642-35734-3_24	computer science;theoretical computer science;engineering drawing;algorithm	Vision	-29.600758059282217	-64.44386981725134	125393
29c4e2033fbacd632aede5ca40bc09f66303f74d	the universal recommender	internet protocol;semantic representation;information retrieval;optimization problem;recommender system;machine learning;domain specificity	We describe the Universal Recommender, a recommender system for semantic datasets that generalizes domain-specific recommenders such as content-based, collaborative, social, bibliographic, lexicographic, hybrid and other recommenders. In contrast to existing recommender systems, the Universal Recommender applies to any dataset that allows a semantic representation. We describe the scalable three-stage architecture of the Universal Recommender and its application to Internet Protocol Television (IPTV). To achieve good recommendation accuracy, several novel machine learning and optimization problems are identified. We finally give a brief argument supporting the need for machine learning recommenders.	algorithm;entity;hard coding;iptv;lexicography;machine learning;mathematical optimization;recommender system;scalability	Jérôme Kunegis;Alan Said;Winfried Umbrath	2009	CoRR		internet protocol;optimization problem;computer science;machine learning;data mining;world wide web;information retrieval;recommender system	ML	-21.16650194501263	-61.98915962503563	125455
65b5cbeb55c926901ae11fcfeda818465654cb6b	cluster analysis of scientific citation context		Investigation of related research is a very important task for researchers. In recent years, databases of academic papers have been developed, and researchers can search for related research using keywords and so on. However, it is a very time-consuming task to discover appropriate papers exhaustively from a large number of academic papers, classify them, and understand their contents. We have been researching methods that can properly extract papers of related research from an academic paper database. After finding those papers, researchers need to understand the relationship between those papers. We believe that there are several types of relations between papers that appear in citation expressions of related papers. In this paper, automatic classification of citation expressions is performed as the first step in the analysis of citation expressions, and the analysis results of each cluster are reported.	cluster analysis;database;regular expression;scientific citation	Tetsuya Nakatoh;Kenta Nagatani;Kumiko Kanekawa;Takahiro Suzuki;Sachio Hirokawa	2017		10.1145/3151759.3151811	data mining;bibliometrics;scientific citation;citation;computer science;expression (mathematics)	ML	-27.429695104820645	-59.37244022709106	125464
42be4fdbf445f403054b7deccc89e67b9ad16830	consensus ontologies: reconciling the semantics of web pages and agents	information resources;information sources;web pages;multi agent system;bepress selected works;information retrieval;software agent;information needs;information needs information resources information retrieval multi agent systems software agents;ontologies web pages bridges terminology semantic web search engines information retrieval internet web search information systems;software agents;multi agent systems;computational science and engineering;distributed information sources consensus ontologies web pages web site development world wide web searching information organization document semantics dynamically generated information retrieval tailored information user needs local annotation web sources semantic distance measures user preferences software agents semantic misconceptions agent interoperation;information need;information needs information resources software agents multi agent systems information retrieval	I n an old joke, a drunk is on his hands and knees searching for his keys underneath a lamppost. “Is this where you dropped them?” he is asked. “No, I dropped them over there, but the light is better here.” As you build a Web site, it is worthwhile to ask a similar question: “Should you put your information where it belongs or where people are most likely to look for it?” Our recent research to improve search through ontologies is providing some interesting results for answering this question.	interoperation;ontology (information science);pointer (computer programming);request for information;software agent;web page	Larry M. Stephens;Michael N. Huhns	2001	IEEE Internet Computing	10.1109/4236.957901	information needs;computer science;artificial intelligence;software agent;multi-agent system;data mining;database;world wide web;information retrieval	Web+IR	-30.788690448635737	-54.19774943061886	125529
d4f4809e2656397c676ee52508d9cb207a81fae0	cv-pcr: a context-guided value-driven framework for patent citation recommendation	patent;citation;recommendation;heterogeneous network	Patent citation recommendation and prior patent search, critical for patent filing and patent examination, have become increasingly difficult due to the rapidly growing number of patents. Unlike paper citations that focus on reference comprehensiveness, patent citations tend to be more parsimonious and refer only to those prior patents bearing significant technological and/or economic value, as they define the scope of the citing patent and thus have significant legal and economic implications. Based on the insight that patent citations are important information reflecting the value of cited patents to the citing patent, we propose a heterogeneous patent citation-bibliographic network that combines patent citations (reflecting value relation) and bibliographic information (reflecting similarity relation) together. From this network, we extract various features that reflect the value of a prior patent to a query patent with regard to the context of the query patent such as its assignee, classifications, etc. We then propose a two-stage framework for patent citation recommendation. Our idea is that by exploiting those context-specific value measures of candidate patents to the query patent, the proposed framework is able to make effective patent citation recommendations. We evaluate the proposed context-guided value-driven framework using a collection of 1.8M U.S. patents. Experimental results validate our ideas and show that those value-driven features are very effective and significantly outperform two state-of-the-art methods in terms of both the precision and recall rates.	occam's razor;peer-to-patent;precision and recall	Sooyoung Oh;Zhen Lei;Wang-Chien Lee;Prasenjit Mitra;John Yen	2013		10.1145/2505515.2505659	patent visualisation;heterogeneous network;computer science;data science;data mining;information retrieval	AI	-26.391251318894508	-58.46812504521411	125662
62460c6991d7e017dd0e8df6783e3ad1c21d7f4d	the effect of combining different feature selection methods on arabic text classification	classification accuracy arabic text classification feature selection feature representation classification algorithms;text analysis classification natural language processing;text analysis;classification;ltc feature selection methods arabic text classification saudi press agency dataset representative subset chi ig gss ngl rs intersection combination union combination boolean tfidf;accuracy diversity reception text categorization classification algorithms niobium educational institutions computers;arabic text classification;classification algorithms;feature selection;feature representation;classification accuracy;natural language processing	Feature selection is one of several factors affecting text classification systems. Feature selection aims to choose a representative subset of all features to reduce the complexity of classification problems. Usually a single method is used for feature selection. For English, several attempts were reported examining the combination of different feature selection methods. To the best of our knowledge no such attempts were reported for Arabic text classification. In this study, we examined the effect of combining five feature selection methods, namely CHI, IG, GSS, NGL and RS, on Arabic text classification accuracy. Two approaches of combination were used, intersection (AND) and union (OR). The NB classification algorithm was used to classify a Saudi Press Agency dataset which comprised 6,300 texts divided evenly into six classes. Three feature representation schemas were used, namely Boolean, TFiDF and LTC. The experiments show slight improvement in classification accuracy for combining two and three feature selection methods. No improvement on classification accuracy was seen when four or all five feature selection methods were combined.	algorithm;boolean expression;chi;document classification;experiment;feature selection;generic security services application program interface;litecoin;ngl (programming language);naive bayes classifier;statistical classification;tf–idf	Abdulmohsen Al-Thubaity;Norah Abanumay;Sara Al-Jerayyed;Aljoharah Alrukban;Zarah Mannaa	2013	2013 14th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing	10.1109/SNPD.2013.89	biological classification;computer science;machine learning;linear classifier;pattern recognition;data mining;feature selection;library classification	Web+IR	-21.647125980337755	-64.86891365045909	125727
534415608291156d2e3bc78e8a27144bd65f92f7	towards a framework for detecting and managing opinion contradictions	opinion mining;noisy data;behavioural sciences computing;opinion mining sentiment analysis;statistical model;data analysis;large scale;internet;statistical analysis;opinion contradiction detection statistical model sentiment evolution synthetic datasets sentiment analysis sentiment variance argument extraction noisy data opinion contradiction management;sentiment analysis;statistical analysis behavioural sciences computing data analysis internet;aggregates data mining blogs conferences measurement noise feature extraction	Sentiment Analysis gains in interest due to the large amount of potential applications and the increasing number of opinions expressed in particular in the Web. The focus of this paper is the development of a framework on top of sentiment analysis for detecting contradictions. First, we introduce a statistical model of contradictions based on a mean value and the variance of sentiments among different posts. It can be used to analyze and track sentiment evolution over time, to identify interesting trends and patterns or even to enable argument extraction. Using synthetic datasets, we demonstrate the effectiveness of our method in capturing contradictions on noisy data. Inspired by this model, which has proven to be effective and efficient for numeric sentiments, we are trying to generalize it for arbitrary opinion data and outline a universal framework which can be efficiently used on a large scale. We discuss various problems and challenges of such a formulation and outline the scope of our future work in this direction.	aggregate data;cluster analysis;sensor;sentiment analysis;signal-to-noise ratio;statistical model;synthetic intelligence;world wide web;c-treeace	Mikalai Tsytsarau;Themis Palpanas	2011	2011 IEEE 11th International Conference on Data Mining Workshops	10.1109/ICDMW.2011.167	statistical model;the internet;computer science;data science;data mining;data analysis;world wide web;sentiment analysis;statistics	DB	-23.724224562503085	-56.25586943024367	125739
9c276de1e4b474355f2dfc5b42fb147f6c56d4a4	a cost-effective method for detecting web site replicas on search engine databases	search engine;query processing;search engines;mirror;quality improvement;site replication;data storage;cost effectiveness	Identifying replicated sites is an important task for search engines. It can reduce data storage costs, improve query processing time and remove noise that might affect the quality of the final answers given to the user. This paper introduces a new approach to detect web sites that are likely to be replicas in a search engine database. Our method uses the websites' structure and the content of their pages to identify possible replicas. As we show through experiments, such a combination improves the precision and reduces the overall costs related to the replica detection task. Our method achieves a quality improvement of 47.23% when compared to previously proposed approaches.	database;effective method;search engine optimization;sensor;web search engine	André Luiz da Costa Carvalho;Edleno Silva de Moura;Altigran Soares da Silva;Klessius Berlt;Allan José de Souza Bezerra	2007	Data Knowl. Eng.	10.1016/j.datak.2006.08.010	quality management;query expansion;computer science;spamdexing;data mining;database;search analytics;web search query;world wide web;information retrieval;search engine	DB	-31.01923636636617	-54.45131029734749	125926
a68a42c7259ce17f685c24adc6a3d94c53e5a5a7	hypergeometric distribution based semantic searching technique	optimisation;hypergeometric distribution;finite population;capture recapture method;searching;probability distribution;distributed models;semantic web;semantic search;capture recapture	"""Semantic Web is, without a doubt, gaining momentum in both industry and academia. The word """"Semantic"""" refers to """"meaning"""" -- a semantic web is a web of meaning. A web that knows what the entities on the web mean can make use of that knowledge. Why do we think that the web would be improved, if it understood the meaning of its contents? Doesn't it understand it now? Google is very good at correcting typing mistakes, figuring out what I """"meant"""" when I miss-typed a query or suggesting keyword to expand our search query. In this paper we redefine the idea of searching related text information on web. The syntactical characters of related keywords and texts are described in detail, but it doesn't involve semantics of the keywords. Hence computers are able to determine the related keywords and texts without actually understanding the meanings or relevance of the keywords. Here Hypergeometric distribution model is used which is a discrete probability distribution that describes the number of successes in a sequence of n keywords to be searched from a finite population without replacement."""	computer;entity;relevance;semantic web;semantic search	N. Thakur;S. Gupta	2011		10.1145/1980022.1980145	semantic similarity;semantic search;computer science;social semantic web;data mining;world wide web;information retrieval	Web+IR	-31.47672873080788	-57.1091119619064	125957
4648b984749234a0c5d32937df4dc7a15ca14205	cluster based detection and analysis of internet topics	pattern clustering;cluster;web pages;cluster internet topic data analysis job scheduling;pattern clustering internet pattern classification;data analysis;servers;internet;engines;internet servers data analysis engines web pages conferences data warehouses;pattern classification;data warehouses;job scheduling;internet topic;internet topic clasification cluster based detection internet topic detection intelligent information access technology average job time average waiting time;conferences	Internet topic detection and classification is an intelligent information access technology. It studies how to detect new events and classify sentiment of the content. Classical detection and analysis system of internet topics has low analysis efficiency and large process delay. The functions of cluster-based analysis system are internet data collection, real-time analysis and off-line data analysis. Experimental results show that the Average Job Time (AJT) and Average Waiting Time (AWT) for jobs in case of Service Cluster are comparatively lesser with respect to Physical Server, and the Service Cluster shortens the service failover time by 93.4%.	abstract window toolkit;assistive technology;failover;information access;internet;mike lesser;online and offline;real-time clock	Jiao Wu;Weihua Gao;Bin Zhang;Jinsong Liu;Chao Li	2011	2011 Fourth International Symposium on Computational Intelligence and Design	10.1109/ISCID.2011.195	the internet;computer science;job scheduler;data warehouse;web page;data mining;database;data analysis;world wide web;server;cluster	Metrics	-25.71796447709254	-55.24489098920873	126224
27144dbb53b2ba86e01cc7a67348dc7d753205a9	a study on the semantic relatedness of query and document terms in information retrieval	active study;lexical semantic knowledge;information retrieval;different indicator;computational method;document term;collaborative knowledge base;document relevance;semantic relatedness;lexical semantic relation;knowledge base;lexical semantics	The use of lexical semantic knowledge in information retrieval has been a field of active study for a long time. Collaborative knowledge bases like Wikipedia and Wiktionary, which have been applied in computational methods only recently, offer new possibilities to enhance information retrieval. In order to find the most beneficial way to employ these resources, we analyze the lexical semantic relations that hold among query and document terms and compare how these relations are represented by a measure for semantic relatedness. We explore the potential of different indicators of document relevance that are based on semantic relatedness and compare the characteristics and performance of the knowledge bases Wikipedia, Wiktionary and WordNet.	archive;benchmark (computing);document;experiment;information retrieval;knowledge base;lkb;query expansion;relevance;semantic similarity;vocabulary mismatch;wikipedia;word sense;word-sense disambiguation;wordnet	Christof Müller;Iryna Gurevych	2009			natural language processing;knowledge base;semantic similarity;semantic computing;lexical semantics;explicit semantic analysis;computer science;data mining;semantic web stack;linguistics;semantic technology;information retrieval	NLP	-29.81989029069265	-61.610947522897	126378
dc2ddf67ad011b6e4c1a0272f2e981a486f12a4f	r2d2 at ntcir 2 ad-hoc task: relevance-based superimposition model for ir		This paper describes our evaluation experiments for NTCIR 2 ad-hoc task. We developed a retrieval system using the Relevance-based Superimposition (RS) model, in which document vectors are modified based on the relevance of the documents. The major focus of this year is on combination of the RS model and query expansion (QE). We submitted fully automatic ad-hoc results brought by different parameter settings.	database;experiment;hoc (programming language);keyword extraction;query expansion;reed–solomon error correction;relevance	Teruhito Kanazawa;Atsuhiro Takasu;Jun Adachi	2001				NLP	-32.29498865902271	-63.61176215688913	126394
b4c98b5c3ba8d7c3a0e4a2797bc06e8299a42ead	citation-based methods for personalized search in digital libraries	search engine;digital library;personalized search	In this paper we present our work about personalized search in digital libraries. Unlike other researches which use content-based methods, we focus on citation-based methods for this purpose. We propose a practical approach to estimate the co-citation relatedness between scientific papers using the Google search engine. We conducted some experiments to evaluate performance of different citation-based methods. The experimental results show that our approach is promising and applicable for personalized search in digital libraries.	digital library;personalized search	Thanh-Trung Van;Michel Beigbeder	2007		10.1007/978-3-540-77010-7_34	digital library;computer science;multimedia;search analytics;world wide web;information retrieval;search engine	NLP	-30.765525830407146	-57.26502637422479	126436
7f05df12dff3defee495507abd4870a0a30c3590	placing images with refined language models and similarity search with pca-reduced vgg features		We describe the participation of the CERTH/CEA-LIST team in the MediaEval 2016 Placing Task. We submitted five runs to the estimation-based sub-task: one based only on text by employing a Language Model-based approach with several refinements, one based on visual content, using geospatial clustering over the most visually similar images, and three based on a hybrid scheme exploiting both visual and textual cues from the multimedia items, trained on datasets of different size and origin. The best results were obtained by a hybrid approach trained with external training data and using two publicly available gazetteers.	cluster analysis;language model;similarity search	Giorgos Kordopatis-Zilos;Adrian Popescu;Symeon Papadopoulos;Yiannis Kompatsiaris	2016			language model;pattern recognition;computer science;nearest neighbor search;artificial intelligence	Web+IR	-30.845288366840723	-62.97840922666683	126677
3a704c28a610cd42bf1e7447bd83bb44618b1125	overviewing the knowledge of a query keyword by clustering viewpoints of web search information needs	overview;web pages;search engines;search engines web search web pages training production facilities knowledge engineering search problems;training;search engine queries search engine suggest web search information need overview clustering;search engine queries;web search information need;aggregated search engine suggests query keyword viewpoints clustering web search information needs knowledge overview web pages japanese search engine search engine suggests clustering document vectors snippets;clustering;production facilities;web search;search problems;search engine suggest;web sites pattern clustering query processing search engines;knowledge engineering	In this paper, we address the issue of how to overview the knowledge of a given query keyword. We especially focus on concerns of those who search for Web pages with a given query keyword, and study how to efficiently overview the whole list of Web search information needs of a given query keyword. First, we collect Web search information needs of a given query keyword through search engine suggests. In the case of a Japanese search engine, we collect up to around 1,000 suggests given a query keyword. However, some of them are redundant in that they originate from almost the same Web search information needs. In order to aggregate such redundant search engine suggests, we take an approach of clustering search engine suggests based on document vectors generated from the snippets shown by the search engine. Evaluation result shows that the proposed clustering approach proves to be quite useful for efficiently over viewing Web search information needs of a given query keyword. We also develop an interface system for over viewing those aggregated search engine suggests of a given query keyword as well slinks to top ranked Web pages that are closely related to those aggregated search engine suggests.	aggregate data;cluster analysis;information needs;web page;web search engine	Ichiro Moriya;Yusuke Inoue;Takakazu Imada;Takehito Utsuro;Yasuhide Kawada;Noriko Kando	2015	2015 IEEE 29th International Conference on Advanced Information Networking and Applications Workshops	10.1109/WAINA.2015.46	search-oriented architecture;sargable;search engine indexing;query optimization;query expansion;database search engine;web query classification;organic search;computer science;phrase search;knowledge engineering;concept search;web page;data mining;keyword density;cluster analysis;search analytics;web search query;world wide web;information retrieval;search engine	DB	-30.146185451932322	-56.23807338220087	126682
3096dc7fc691bb68ed03e51f2e702495eb32c642	a framework to generate carrier path using semantic similarity of competencies in job position		A career path is necessary for students and workers to keep themselves in track for their career goal. However, a career path following a job standard in general is very rare. This paper presents a method to find a semantic similarity within competencies of Job positions for realising a path to relate career. By a development of Thai WordNet containing terms used in competency description, a distance of classes of WordNet structure is used to determine a semantic similarity of competencies. Paths to relate job positions are assumed for the job positions sharing similar competencies, and the more they share, the more transferrable job is viable. From the usage scenario, the proposed framework proved that semantic of words is more useful than using character based similarity in competency comparison.	semantic similarity	Wasan Na Chai;Taneth Ruangrajitpakorn;Marut Buranarach;Thepchai Supnithi	2016		10.1007/978-3-319-60675-0_8	data mining;semantic similarity;computer science;machine learning;artificial intelligence;wordnet;competence (human resources)	NLP	-30.064439292531095	-65.53783171006285	126765
6b198dafbacf62ec6fe317804399ed52e252e1f8	automatic wrapper maintenance for semi-structured web sources using results from previous queries	maintenance;web;examples;extraction;wrapper	During the last years, significant attention has been paid to the problem of building wrappers for extracting data from semistructured web sources. Nevertheless, since web sources are autonomous, they may experience changes that invalidate the wrappers. In this paper, we present new heuristics and algorithms to address the problem of automatic wrapper maintenance. Our approach is based on collecting query results during wrapper operation and using them later to generate new sets of examples that can be used to induce a new wrapper when the source changes.	algorithm;autonomous robot;heuristic (computer science);semiconductor industry;wrapper library	Juan Raposo;Alberto Pan;Manuel Álvarez;Ángel Viña	2005		10.1145/1066677.1066826	extraction;computer science;data mining;database;world wide web	DB	-30.111894136371046	-54.78364140101074	126836
4cb65d08f0c27b650ca0479ece53588437ab9417	where are you tweeting?: a context and user movement based approach	hidden markov model;geotagging;tweet	Geotagged tweets allow one to extract geo-information-trend, search local events, and identify natural disasters. In this paper, we propose a Hidden-Markov-based model to integrate tweet contents and user movements for geotagging. A language model is obtained for different locations from training datasets and movements of users among cities are analyzed. Home cities of users are considered in modeling the patterns of user movements. Evaluation on a large Twitter dataset shows that our method can significantly improve geotagging accuracy by 55% for home cities and 2% for other non-home cities as well as reduce error distances by orders of magnitude compared with pure text-based methods.	geotagging;language model;markov chain;text-based (computing)	Zhi Liu;Yan Huang	2016		10.1145/2983323.2983881	computer science;machine learning;data mining;geotagging;internet privacy;world wide web;hidden markov model	ML	-23.97030286028258	-52.803628686770665	126861
6a0648ecdf889cd0f5eb6ce329ce8fb2c9d5ea2d	comparison of two-pass algorithms for dynamic topic modeling based on matrix decompositions		In this paper we present a two-pass algorithm based on different matrix decompositions, such as LSI, PCA, ICA and NMF, which allows tracking of the evolution of topics over time. The proposed dynamic topic models as output give an easily interpreted overview of topics found in a sequentially organized set of documents that does not require further processing. Each topic is presented by a user-specified number of top-terms. Such an approach to topic modeling if applied to, for example, a news article data set, can be convenient and useful for economists, sociologists, political scientists. The proposed approach allows to achieve results comparable to those obtained using complex probabilistic models, such as LDA.		Gabriella Skitalinskaya;Mikhail Alexandrov;John Cardiff	2017		10.1007/978-3-030-02840-4_3	latent dirichlet allocation;topic model;probabilistic logic;matrix decomposition;matrix (mathematics);algorithm;computer science;dynamic topic model;non-negative matrix factorization	Web+IR	-25.03864521642014	-59.44338096976844	126904
10e1e58e63de4358a266f70bc56ff9c998726d56	x-site: a workplace search tool for software engineers	task;software engineering;enterprise search;contextual search;genre	Professionals in the workplace need high-precision search tools capable of retrieving information that is useful and appropriate to the task at hand. One approach to identifying content, which is not only relevant but also useful, is to make use of the task context of the search. We present X-Site, an enterprise search engine for the software engineering domain that exploits relationships between user's tasks and document genres in the collection to improve retrieval precision.	software engineer;web search engine	Peter C. K. Yeung;Luanne Freund;Charles L. A. Clarke	2007		10.1145/1277741.1277968	enterprise software;semantic search;computer science;knowledge management;data mining;film genre;search analytics;world wide web;contextual advertising;information retrieval;search engine	SE	-30.85117017382462	-55.37123349114125	126915
905e7fc68bb75497b69777f6b7bc579e69bcebb9	applying machine learning techniques for sentiment analysis in the case study of indian politics		In the recent era, humans have become detached from their surroundings, immediate peers and more addicted to their social media platforms and micro-blogging sites. Technology is digitalizing at a very fast pace and this has led to man being social, but only on technological forefront. Social media platforms like twitter, facebook, whatsapp, instagram are in trend. In our paper, we will concentrate on data generated through Twitter (tweets). People express their opinions, perspectives within a 140 character tweet, which is subjective. We try to analyze their emotion by tweet classification followed by sentiment analysis. On an average, with 328 million Twitter users, 6000 tweets are generated every second. This tremendous amount of data can be used to assess general public’s views in economy, politics, environment, product reviews, feedbacks etc. and so many other sectors. Here, we take into account the political data from Tweets. The data obtained can be images, videos, links, emoticons, text, etc. The results obtained could help the government function better, improve their flaws, plan out better strategies to empower the nation.	machine learning;sentiment analysis	Annapurna P. Patil;Dimple Doshi;Darshan Dalsaniya;B. S. Rashmi	2017		10.1007/978-3-319-67934-1_31	marketing;sentiment analysis;data science;politics;social media;government;political science	NLP	-22.371715019389182	-54.85276426998858	126988
4928fa868366f51fbf7c364607743582fe8b9c08	gauging ecliptic sentiment		With the Great American Eclipse of 2017, the National Aeronautics and Space Administrationu0027s (NASA) production of social media content was crucial to the general publicu0027s awareness and understanding of the event. To date, it may have been NASAu0027s largest production of an event spanning many social media platforms and hundreds of media outlets. Since NASA is government funded, the millions of dollars spent on their social media campaign of the eclipse requires evaluation. The primary goal with this paper is to understand how the public perceived the social media coverage that NASA provided, specifically in the world of Twitter, a free social networking microblogging service that allows registered members to broadcast short posts called tweets. The aforementioned goals are accomplished through sentiment analysis and the spotting of trends within Twitter data attached to the eclipse itself and to NASA. Furthermore, a framework is created that can be applied in the future to gauge the effectiveness of social media campaigns in a variety of categories in a simple and cost-effective manner.	byte;eclipse;file spanning;interaction;köppen climate classification;multimedia framework;quantifier (logic);sentiment analysis;social media	Gautam Srivastava	2018	2018 41st International Conference on Telecommunications and Signal Processing (TSP)	10.1109/TSP.2018.8441433	sentiment analysis;eclipse;computer network;public relations;big data;microblogging;computer science;social network;social media;solar eclipse;government	DB	-23.01890567328583	-54.20317483540225	126996
b0fda86510ee09eda88fe8baa29807131b153347	things and strings: improving place name disambiguation from short texts by combining entity co-occurrence with topic modeling		Place name disambiguation is the task of correctly identifying a place from a set of places sharing a common name. It contributes to tasks such as knowledge extraction, query answering, geographic information retrieval, and automatic tagging. Disambiguation quality relies on the ability to correctly identify and interpret contextual clues, complicating the task for short texts. Here we propose a novel approach to the disambiguation of place names from short texts that integrates two models: entity co-occurrence and topic modeling. The first model uses Linked Data to identify related entities to improve disambiguation quality. The second model uses topic modeling to differentiate places based on the terms used to describe them. We evaluate our approach using a corpus of short texts, determine the suitable weight between models, and demonstrate that a combined model outperforms benchmark systems such as DBpedia Spotlight and Open Calais in terms of F1-score and Mean Reciprocal Rank.	baseline (configuration management);benchmark (computing);dbpedia;entity;f1 score;ibm notes;information retrieval;linked data;literal (mathematical logic);quality engineering;string (computer science);text corpus;topic model;towns;word-sense disambiguation	Yiting Ju;Benjamin Adams;Krzysztof Janowicz;Yingjie Hu;Bo Yan;Grant McKenzie	2016		10.1007/978-3-319-49004-5_23	natural language processing;computer science;pattern recognition;information retrieval	NLP	-27.302077934313946	-65.95294085590021	127060
b7efc296d695dce995aecc67a2e79b0c478783f2	web usage mining framework for data cleaning and ip address identification		The World Wide Web is the most wide known information source that is easily available and searchable. It consists of billions of interconnected documents Web pages are authored by millions of people. Accesses made by various users to pages are recorded inside web logs. These log files exist in various formats. Because of increase in usage of web, size of web log files is increasing at a much faster rate. Web mining is application of data mining technique to these log files. It can be of three types Web usage mining, Web structure mining and Web content mining. Web Usage mining is mining of usage patterns of users which can then be used to personalize web sites and create attractive web sites. It consists of three main phases: Preprocessing, Pattern discovery and Pattern analysis. In this paper we focus on Data cleaning and IP Address identification stages of preprocessing. Methodology has been proposed for both the stages. At the end conclusion is made about number of users left after IP address identification. Keywords—Web usage; preprocessing;IP Address identification	algorithm;blog;data logger;data mining;disk sector;information source;personalization;plasma cleaning;preprocessor;structure mining;unique user;web content;web mining;web page;world wide web	Priyanka Verma;Nishtha Kesswani	2014	CoRR		web service;web application security;web mining;static web page;web development;web modeling;data web;web analytics;web mapping;web design;web standards;computer science;web navigation;web log analysis software;web page;data mining;semantic web stack;database;web intelligence;web 2.0;world wide web;web server	ML	-29.628423350814206	-52.658034360015634	127061
aa9b09927b4d80c4100087069e03749b7f7a64d6	concept-based document models using explicit semantic analysis	web sites natural language processing relevance feedback text analysis;document model;tree ad hoc track collection concept based document models explicit semantic analysis esa based retrieval models text meaning wikipedia pseudo relevance feedback concept based retrieval model language modeling framework concept mapping conceptual representations;language model document model explicit semantic analysis;text analysis;computational modeling;internet;web sites;electronic publishing;encyclopedias electronic publishing internet computational modeling;encyclopedias;relevance feedback;explicit semantic analysis;natural language processing;language model	explicit semantic analysis (ESA) is a novel method that represents the meaning of texts in a high-dimensional space of concepts derived from Wikipedia. Several different ESA-based retrieval models have been proposed. However, these approaches depended on the performance of pseudo relevance feedback. In this paper, we propose a concept-based retrieval model under the language modeling framework. By means of concept mapping using explicit semantic analysis, the original documents are translated into conceptual representations, which are subsequently used to update the document models. The concept-based document model is evaluated on the TREe Ad Hoc Track (Disks 1, 2, and 3) collections. Experiment results show significant improvements with respect to the baseline models.	baseline (configuration management);cluster analysis;concept map;document classification;esa;explicit semantic analysis;floppy disk;information retrieval;language model;relevance feedback;text retrieval conference;text mining;wikipedia	Jing Luo;Bo Meng;Xinhui Tu;Maofu Liu	2012	2012 IEEE International Conference on Granular Computing	10.1109/GrC.2012.6468647	natural language processing;semantic computing;the internet;explicit semantic analysis;computer science;data mining;semantic compression;electronic publishing;computational model;information retrieval;encyclopedia;language model	SE	-27.5175422942775	-62.15133775647342	127107
7c07a7a38de20e76538fb0dbcd7d759baf1a14ce	evaluation of background knowledge for latent semantic indexing classification	singular value decomposition;text classification;latent semantic indexing;background knowledge	This paper presents work that evaluates background knowledge for use in improving accuracy for text classification using Latent Semantic Indexing (LSI). LSI’s singular value decomposition process can be performed on a combination of training data and background knowledge. Intuitively, the closer the background knowledge is to the classification task, the more helpful it will be in terms of creating a reduced space that will be effective in performing classification. Using a variety of data sets, we evaluate sets of background knowledge in terms of how close they are to training data, and in terms of how much they improve classification.	document classification;singular value decomposition;statistical classification	Sarah Zelikovitz;Finella Marquez	2005			latent semantic indexing;computer science;pattern recognition;data mining;probabilistic latent semantic analysis;singular value decomposition;information retrieval	Web+IR	-20.343640761421664	-64.44545067277708	127125
3b98b03e8224e51c06fc6fba294b53d1eafdac7e	nlp-nitmz @ clscisumm-18		This paper report NLP-NITMZ @ CL-Scisumm 2018 system participation for the shared task task 1A, task 1B and task 2 at the BIRNDL 2018 Workshop. We developed our system based on the previous years data provided by the organizer. For task 1A and 1B, we apply various rule based approaches and trained the KNearest Neighbors Classifier (KNN) using different features identified from the input citation text and the reference Text. We achieved an overall accuracy score of 42.75 (%) and 78.75 (%) score for task 1A and task 1B. For task 2, We built our summary generation system using OpenNMT tool. We developed the model using the training and development datasets released from previous year tracks and validated the results of our system using previous years test data set. We have evaluated our system using Recall-Oriented Understudy for Gisting Evaluation, (ROUGE) score on some test data of CL-Scisumm 2017. For task 2, we achieved an overall accuracy score of 37.75 (%).	experiment;image organizer;k-nearest neighbors algorithm;naive bayes classifier;natural language processing;test data	Dipanwita Debnath;Amika Achom;Partha Pakray	2018			information retrieval;computer science;natural language processing;artificial intelligence	NLP	-31.09388436635807	-64.1823372549862	127221
f935c79404bcb8ec0c1e4779380e8c16b3895094	detection of incoherences in a document corpus based on the application of a neuro-fuzzy system	document handling;supervised classification process;economic problem;expert systems;content incoherences;technical document corpora;fuzzy rules;training;supervised classification;fuzzy neural networks law documentation legal factors fuzzy systems quality management fuzzy sets proposals text analysis systems engineering and theory;testing;decision taking system;data mining;companies;pattern classification document handling expert systems fuzzy set theory learning artificial intelligence;fuzzy set theory;law;summarization;4 tuples;neuro fuzzy system;social problems;pattern classification;social problem;expert knowledge;matching techniques;supervised classification content incoherences summarization 4 tuples matching techniques neuro fuzzy system;domain expert system;learning artificial intelligence;decision taking system neuro fuzzy system technical document corpora information coherency economic problem social problem supervised classification process domain expert system fuzzy set theory;context;information coherency;documentation	The aim of this paper is to detect incoherences in concepts, ideas, values, and others contained in technical document corpora. The way in which document collections are generated, modified or updated generates problems and mistakes in the information coherency, leading to legal, economic and social problems. A solution based on summarization, matching and neuro-fuzzy systems is proposed to dealt with this problem. For this goal, every document (from the electric domain) is summarized by its relevant information in the form of 4-tuples of terms, describing the most relevant ideas and concepts that must be free of incoherences. These representations are then matched using several well-known algorithms (Levenshtein distance and cosine similarity). The final decision about the real existence or not of an incoherence, and its relevancy, is obtained by training a neuro-fuzzy system FasArt in a supervised classification process, based on the previous knowledge of the activity area and domain experts. On the other hand, using this fuzzy approach, it is possible to extract the learnt and expert knowledge from the the neuro-fuzzy system, through a set of fuzzy rules that can support a decision taking system about this complex and non objective problem.	algorithm;cosine similarity;fuzzy control system;fuzzy logic;levenshtein distance;machine learning;matching (graph theory);neuro-fuzzy;relevance;supervised learning;technical standard;text corpus	Susana Martin-Toral;Víctor Arribas;Gregorio Ismael Sainz Palmero	2009	2009 10th International Conference on Document Analysis and Recognition	10.1109/ICDAR.2009.101	computer science;social issues;automatic summarization;machine learning;pattern recognition;data mining;expert system	DB	-24.75655434865685	-63.979705948188595	127299
1705ea43d7afc92b90ec58575a6927ccc6569c4e	integrating content and citation information for the ntcir-6 patent retrieval task		This paper describes our system participated in the Japanese and English Retrieval Subtasks at the NTCIR-6 Patent Retrieval Task. The purpose of these subtasks is the invalidity search, in which a patent application including a target claim is used to search documents that can invalidate the demand in the claim. Although we use a regular text-based retrieval method for the Japanese Retrieval Subtask, we combine text and citation information to improve the retrieval accuracy for the English Retrieval Subtask.	text-based (computing)	Atsushi Fujii	2007			patent application;human–computer information retrieval;concept search;information retrieval;relevance (information retrieval);document retrieval;citation;data mining;data retrieval;computer science	Web+IR	-32.20163186256211	-63.18482872779044	127383
baa10b06d45d1294b27cdf857351431cec39d5a3	a vector similarity measure for interval type-2 fuzzy sets	vector similarity measure;computing with words;decoding;uncertainty;bismuth;vocabulary;natural languages;shape measurement;fuzzy set theory;fuzzy sets;fuzzy set theory computational linguistics fuzzy logic;fuzzy logic;engines;vsm;it2 fs;cww engine;comparative study;type 2 fuzzy set;computational linguistics;linguistic label;similarity measure;fuzzy sets frequency selective surfaces engines bismuth vocabulary decoding shape measurement fuzzy logic natural languages uncertainty;interval type 2 fuzzy sets;linguistic label vector similarity measure vsm interval type 2 fuzzy sets it2 fs fuzzy logic computing with words cww engine;frequency selective surfaces	Fuzzy logic is frequently used in computing with words (CWW). When input words to a CWW engine are modeled by interval type-2 fuzzy sets (IT2 FSs), the CWW engine's output can also be an IT2 FS, A tilde, which needs to be mapped to a linguistic label so that it can be understood. Because each linguistic label is represented by an IT2 FS Bi, there is a need to compare the similarity of A tilde and B tildei to find the B tildei most similar to A tilde. In this paper, a vector similarity measure (VSM) is proposed for IT2 FSs, whose two elements measure the similarity in shape and proximity, respectively. A comparative study shows that the VSM gives more reasonable results than all other existing similarity measures for IT2 FSs.	computing with words and perceptions;fuzzy logic;fuzzy set;similarity measure;tilde;type-2 fuzzy sets and systems;viable system model	Dongrui Wu;Jerry M. Mendel	2007	2007 IEEE International Fuzzy Systems Conference	10.1109/FUZZY.2007.4295333	discrete mathematics;computer science;artificial intelligence;computational linguistics;machine learning;mathematics;fuzzy set;algorithm	SE	-24.520498150227304	-64.1531512962879	127499
5b5e0c8d1986719954a12a463fe8e6c6ecef636a	link-based text classification using bayesian networks	bayesian network;text classification;xml document;document classification	In this paper we propose a new methodology for link-based document classification based on probabilistic classifiers and Bayesian networks. We also report the results obtained of its application to the XML Document Mining Track of INEX’09.	bayesian network;best practice;document classification;experiment;logistic regression;naive bayes classifier;or gate;primitive recursive function;relevance;situated;support vector machine	Luis M. de Campos;Juan M. Fernández-Luna;Juan F. Huete;Andrés R. Masegosa;Alfonso E. Romero	2009		10.1007/978-3-642-14556-8_39	pattern recognition;data mining;information retrieval	Web+IR	-22.608152399432708	-62.812113075044756	127580
2c7dc6006f7e577c4e57c14dcaef2cc217449107	automatic abstracting important sentences of web articles	information resources;information structure;sentence extraction;information retrieval;heuristic rules internet web articles structural information statistical information sentence extraction;natural languages;natural languages information retrieval information resources computational linguistics;computational linguistics;data mining internet frequency explosions humans information filtering information filters production surface reconstruction statistics	Being increasingly popular, the Internet greatly changes our live. We can conveniently receive and send information via the Internet. With the information explosion in Web, it is becoming crucial to develop means to automatically extract important sentences from the Web articles. In this paper, we propose a method which uses both statistical and structural information in sentence extraction. In addition, following the analysis of human's extractions, several heuristic rules are added to filter out non-important sentences and to prevent similar sentences from being extracted. Our experimental results proved the effectiveness of these means. In particular, once the heuristic rules being added, a significant improvement has been observed.		Fuji Ren;Shigang Li;Kenji Kita	2001		10.1109/ICSMC.2001.973531	natural language processing;computer science;computational linguistics;information filtering system;machine learning;data mining;natural language;information extraction;information retrieval	NLP	-27.666662899463475	-56.50107600841935	127687
e6c5f2eff320426d795298bd59b753a66576a4ee	multi-document summarization based on atomic semantic events and their temporal relationships	multi document summarization;thesis;temporal relations;events	Automatic multi-document summarization (MDS) is the process of extracting the most important information, such as events and entities, from multiple natural language texts focused on the same topic. In this paper, we experiment with the effects of different groups of information such as events and named entities in the domain of generic and update MDS. Our generic MDS system has outperformed the best recent generic MDS systems in DUC 2004 in terms of ROUGE-1 recall and (f_1)-measure. Update summarization is a new form of MDS, where novel yet salient sentences are chosen as summary sentences based on the assumption that the user has already read a given set of documents. We present an event based update summarization where the novelty is detected based on the temporal ordering of events, and the saliency is ensured by the event and entity distribution. To our knowledge, no other study has deeply experimented with the effects of the novelty information acquired from the temporal ordering of events (assuming that a sentence contains one or more events) in the domain of update multi-document summarization. Our update MDS system has outperformed the state-of-the-art update MDS system in terms of ROUGE-2 and ROUGE-SU4 recall measures. All our MDS systems also generate quality summaries which are manually evaluated based on popular evaluation criteria.	automatic summarization;multi-document summarization	Yllias Chali;Mohsin Uddin	2016		10.1007/978-3-319-30671-1_27	multi-document summarization;computer science;data mining;database;information retrieval	NLP	-27.038179175684622	-65.11331647516808	127766
3865d02552139862bf8dcc4942782af1d9eec17c	analyzing news media coverage to acquire and structure tourism knowledge	mass media;indian ocean;term frequency;destinations;tourism;media coverage;internet;decision making process;web sites;h social sciences general;new media;natural language processing;semantic association	Destination image significantly influences a tourist’s decision-making process. The impact of news media coverage on destination image has attracted research attention and became particularly evident after catastrophic events such as the 2004 Indian Ocean earthquake that triggered a series of lethal tsunamis. Building upon previous research, this article analyzes the prevalence of tourism destinations among 162 international media sites. Term frequency captures the attention a destination receives—from a general and, after contextual filtering, from a tourism perspective. Calculating sentiment estimates positive and negative media influences on destination image at a given point in time. Identifying semantic associations with the names of countries and major cities, the results of co-occurrence analysis reveal the public profiles of destinations, and the impact of current events on media coverage. These results allow national tourism organizations to assess how their destination is covered by news media in general, and in a specific tourism context. To guide analysts and marketers in this assessment, an iterative analysis of semantic associations extracts tourism knowledge automatically, and represents this knowledge as ontological structures.	iterative method;sentiment analysis	Arno Scharl;Astrid Dickinger;Albert Weichselbraun	2008	J. of IT & Tourism	10.3727/109830508785059039	public relations;media relations;decision-making;the internet;new media;computer science;marketing;sociology;advertising;tourism;law;tf–idf;world wide web;mass media	Web+IR	-23.68299785948985	-53.99218224205695	127932
fe2725da809c585461111fbb13f328adacd33284	global vs. localized search: a comparison of database selection methods in a hierarchical environment	databases;comparative analysis;information retrieval;search strategies;relevance information retrieval;local search	Abstract#R##N##R##N#In this work, we compare standard global IR searching with more localized techniques to address the database selection problem. We conduct a series of experiments to compare the retrieval effectiveness of three separate search modes using a hierarchically structured data environment of textual databse representations. The data environment is represented as a tree-like structure containing over 15,000 unique databases and approximately 100,000 total leaf nodes. The search modes consist of varying degrees of browse and search, from a global search at the root node to a refined search at a sub-node using dynamically-calculated inverse document frequencies (idfs) to score the candidate databases for probable relevance. Our findings indicate that a browse plus search approach that relies upon localized searching from sub-nodes in this environment produces the most effective results.		Jack G. Conrad;Joanne S. Claussen;Changwen Yang	2002		10.1002/meet.1450390115	qualitative comparative analysis;full text search;relevance;computer science;local search;data science;concept search;data mining;world wide web;information retrieval;search engine	DB	-30.217521674188443	-60.68419932594364	128010
667f66f3a78f4c04d5823bad580e7b12d82885bc	p2p web search: make it light, make it fly (demo)	focused crawling;user interface;p2p;web search engine;indexation;web search;local search;query routing	We propose a live demonstration of MinervaLight, a P2P Web search engine. MinervaLight combines the (previously separate) focused crawler BINGO! (to harvest Web data), the local search engine TopX, and our P2P Web search system MINERVA under one common user interface. The crawler unattendedly downloads and indexes Web data, where the scope of the focused crawl can be tailored to the thematic interest profile of the user. The result of this process is a local search index, which is used by TopX to evaluate user queries. In the background, MinervaLight continuously computes compact statistical synopses that describe a user’s local search index and publishes that information to a conceptually global, but physically fully decentralized directory. MinervaLight offers a search interface where users can submit queries to MINERVA. Sophisticated query routing strategies are used to identify the most promising peers for each query based on the statistical synopses in the directory. The query is forwarded to those judiciously chosen peers and evaluated based on their local indexes. These results are sent back to the query initiator and merged into a single result list. We give a live demonstration of the fully functional system.	directory (computing);focused crawler;local search (optimization);peer-to-peer;query optimization;routing;scsi initiator and target;search engine indexing;user interface;web crawler;web search engine	Matthias Bender;Sebastian Michel;Josiane Xavier Parreira;Tom Crecelius	2007			web service;static web page;web modeling;site map;database search engine;web query classification;organic search;metasearch engine;web design;web search engine;semantic search;search engine optimization;computer science;local search;web crawler;web navigation;peer-to-peer;distributed web crawling;web page;database;search analytics;user interface;web search query;world wide web;information retrieval;search engine	Web+IR	-32.091912751270705	-52.812921328624704	128035
adde68f430bbef95770111dc2006b090b8957e3f	service discovery acceleration with hierarchical clustering	service matchmaking;hirerachical clustering;service discovery	This paper presents an efficient Web Service Discovery approach based on hierarchical clustering. Conventional web service discovery approaches usually organize the service repository in a list manner, therefore service matchmaking is performed with linear complexity. In this work, services in a repository are clustered using hierarchical clustering algorithms with a distance measure from an attached matchmaker. Service discovery is then performed over the resulting dendrogram (binary tree). In comparison with conventional approaches that mostly perform exhaustive search, we show that service-clustering method brings a dramatic improvement on time complexity with an acceptable loss in precision.	cluster analysis;hierarchical clustering;service discovery	Zijie Cong;Alberto Fernández;Holger Billhardt;Marin Lujak	2015	Information Systems Frontiers	10.1007/s10796-014-9525-2	computer science;data mining;database;service discovery;world wide web	ML	-28.299292169062916	-57.34296777988752	128144
9367eb880d339ed910abe9c41ff10900de52fd2a	forecasting the 2016 us presidential elections using sentiment analysis		The aim of this paper is to make a zealous effort towards true prediction of the 2016 US Presidential Elections. We propose a novel technique to predict the outcome of US presidential elections using sentiment analysis. For this data was collected from a famous social networking website (SNW) Twitter in form of tweets within a period starting from September 1, 2016 to October 31, 2016. To accomplish this mammoth task of prediction, we build a model in WEKA 3.8 using support vector machine which is a supervised machine learning algorithm. Our results showed that Donald Trump was likely to emerge winner of 2016 US Presidential Elections.	sentiment analysis	Prabhsimran Singh;Ravinder Singh Sawhney;Karanjeet Singh Kahlon	2017		10.1007/978-3-319-68557-1_36	presidential system;support vector machine;data mining;sentiment analysis;social network;computer science	AI	-20.05070817650055	-53.17331013525474	128228
e9569833de00af5b16f4d90b65a895fccd1a0444	semantic similarity definition over gene ontology by further mining of the information content	semantic similarity;information content.;gene ontology;information content	The similarity of two gene products can be used to solve many problems in information biology. Since one gene product corresponds to several GO (Gene Ontology) terms, one way to calculate the gene product similarity is to use the similarity of their GO terms. This GO term similarity can be defined as the semantic similarity on the GO graph. There are many kinds of similarity definitions of two GO terms, but the information of the GO graph is not used efficiently. This paper presents a new way to mine more information of the GO graph by regarding edge as information content and using the information of negation on the semantic graph. A simple experiment is conducted and, as a result, the accuracy increased by 8.3 percent in average, compared with the traditional method which uses node as information source.	gene ontology;information source;quantum information;self-information;semantic similarity	Yuan-Peng Li;Bao-Liang Lu	2008			upper ontology;open biomedical ontologies;ontology alignment;semantic similarity;semantic computing;self-information;computer science;ontology;ontology-based data integration;semantic technology;information retrieval;suggested upper merged ontology;dishin	AI	-28.85171067475795	-59.030981139246364	128316
019d569c88c94960543498425e285d69212adc42	a feature-based classification technique for answering multi-choice world history questions: frdc_qa at ntcir-11 qa-lab task		Our FRDC_QA team participated in the QA-Lab English subtask of the NTCIR-11. In this paper, we describe our system for solving real-world university entrance exam questions, which are related to world history. Wikipedia is used as the main external resource for our system. Since problems with choosing right/wrong sentence from multiple sentence choices account for about two-thirds of the total, we individually design a classification based model for solving this type of questions. For other types of questions, we also design some simple methods.	question answering;software quality assurance;wikipedia	Shuangyong Song;Yao Meng;Zhongguang Zheng;Jun Sun	2014	CoRR		computer science;artificial intelligence;data mining;information retrieval	NLP	-31.18497297010131	-64.44833088815743	128336
0efa1e09ee4c3164010d43e799ab107fbcde38a0	web document clustering and ranking using tf-idf based apriori approach	international journal of computer applications ijca	The dynamic web has increased exponentially over the past few years with more than thousands of documents related to a subject available to the user now. Most of the web documents are unstructured and not in an organized manner and hence user facing more difficult to find relevant documents. A more useful and efficient mechanism is combining clustering with ranking, where clustering can group the similar documents in one place and	cluster analysis;tf–idf;web page	Rajendra Kumar Roul;O. R. Devanand;Sanjay Kumar Sahay	2014	CoRR		ranking;computer science;machine learning;data mining;world wide web;information retrieval	Web+IR	-28.395927561399578	-57.084283178929994	128469
3024cecd35ffeb124a69b6947c50ce1719f9ffe9	gte: a distributional second-order co-occurrence approach to improve the identification of top relevant dates in web snippets	temporal information retrieval;query log analysis;implicit temporal queries;temporal query understanding	In this paper, we present an approach to identify top relevant dates in Web snippets with respect to a given implicit temporal query. Our approach is two-fold. First, we propose a generic temporal similarity measure called GTE, which evaluates the temporal similarity between a query and a date. Second, we propose a classification model to accurately relate relevant dates to their corresponding query terms and withdraw irrelevant ones. We suggest two different solutions: a threshold-based classification strategy and a supervised classifier based on a combination of multiple similarity measures. We evaluate both strategies over a set of real-world text queries and compare the performance of our Web snippet approach with a query log approach over the same set of queries. Experiments show that determining the most relevant dates of any given implicit temporal query can be improved with GTE combined with the second order similarity measure InfoSimba, the Dice coefficient and the threshold-based strategy compared to (1) first-order similarity measures and (2) the query log based approach.	coefficient of determination;experiment;first-order predicate;fold (higher-order function);machine learning;relevance;similarity measure;sørensen–dice coefficient	Ricardo Campos;Gaël Dias;Alípio Mário Jorge;Celia Nunes	2012		10.1145/2396761.2398567	sargable;query optimization;query expansion;web query classification;computer science;data mining;database;web search query;world wide web;information retrieval	Web+IR	-30.04431472482787	-59.330574026326495	128511
095dfca892ce6d9e3eca134fbddbb679571bdb2c	towards the taxonomy-oriented categorization of yellow pages queries	category mapping function;search engine;flexible framework;standard industrial classification;category filter;yellow pages query categorization;support vector machine	Yellow pages search is a popular service that provides a means for finding businesses close to particular locations. The efficient search of yellow pages is becoming a rapidly evolving research area. The underlying data maintained in yellow pages search engines are typically labeled according to Standard Industry Classification (SIC) categories, and users can search yellow pages with categories according to their interests. Categorizing yellow pages queries into a subset of topical categories can help to improve search experience and quality. However, yellow pages queries are usually short and ambiguous. In addition, a yellow pages query taxonomy is typically organized by a hierarchy of a fairly large number of categories. These characteristics make automatic yellow pages query categorization difficult and challenging. In this article, we propose a flexible yellow pages query categorization approach. The proposed technique is built based on a TF-IDF similarity taxonomy matching scheme that is able to provide more accurate query categorization than previous keyword-based matching schemes. To further improve the categorization performance, we design several filtering schemes. Through extensive experimentation, we demonstrate encouraging results. We obtain F1 measures of about 0.5 and 0.3 for categorizing yellow pages queries into 19 coarse categories and 244 finer categories, respectively. We investigate different components in the proposed approach and also demonstrate the superiority of our approach over a hierarchical support vector machine classifier.	ambiguous grammar;categorization;simplified instructional computer;support vector machine;tf–idf;web query classification;web search engine	Zhisheng Li;Xiangye Xiao;Meng Wang;Chong Wang;Xufa Wang;Xing Xie	2008	ACM Trans. Internet Techn.	10.1145/2109211.2109213	standard industrial classification;support vector machine;web query classification;computer science;artificial intelligence;data science;machine learning;data mining;database;world wide web;information retrieval;search engine	DB	-29.05729190135601	-56.48924382221266	128644
15156983f24dea8777f772587625e82af30901aa	query suggestions in the absence of query logs	search engine;query formulation;query log analysis;web search engine;enterprise search;personalized search;query suggestion;information need;query logs;query completion	After an end-user has partially input a query, intelligent search engines can suggest possible completions of the partial query to help end-users quickly express their information needs. All major web-search engines and most proposed methods that suggest queries rely on search engine query logs to determine possible query suggestions. However, for customized search systems in the enterprise domain, intranet search, or personalized search such as email or desktop search or for infrequent queries, query logs are either not available or the user base and the number of past user queries is too small to learn appropriate models. We propose a probabilistic mechanism for generating query suggestions from the corpus without using query logs. We utilize the document corpus to extract a set of candidate phrases. As soon as a user starts typing a query, phrases that are highly correlated with the partial user query are selected as completions of the partial query and are offered as query suggestions. Our proposed approach is tested on a variety of datasets and is compared with state-of-the-art approaches. The experimental results clearly demonstrate the effectiveness of our approach in suggesting queries with higher quality.	desktop computer;email;information needs;information retrieval;intranet;personalized search;randomized algorithm;text corpus;web search engine;web search query	Sumit Bhatia;Debapriyo Majumdar;Prasenjit Mitra	2011		10.1145/2009916.2010023	search-oriented architecture;online aggregation;information needs;sargable;query optimization;query expansion;web query classification;ranking;boolean conjunctive query;web search engine;computer science;query by example;concept search;database;rdf query language;web search query;view;range query;world wide web;information retrieval;query language;search engine;object query language;spatial query	Web+IR	-31.31006197640448	-58.839710841783585	128697
78377874112427392567f5bb028caaf46ee5d8ae	influential analysis in micro scholar social networks		Scholar citation is a basic activity in scientific community. Some academic search engines have been developed in Web such as Google Scholar and Microsoft Academic Search. Efficient flexible querying method is essential for researchers to effectively follow trends within related topics of their research field. In this paper, we propose a procedure to construct Micro Scholar Social Networks (MSSN) from Google Scholar and then develop a querying and ranking method to find the influential researchers or articles in MSSN. An extension to the Follow Model (Extended Follow Model) is proposed in this paper and applied to describe the paper citation and author-follow relationships. It is also coupled with different ranking algorithms, namely, PageRank, AuthorRank and InventorRank to study a MSSN in Air Traffic Management. The case study shows that Extended Follow Model is robust and efficient for ranking and mining a heterogeneous academic network. In spite the fact that study was done on Google Scholar, but the proposed data mining method is applicable for other academic search engines.	algorithm;data mining;google scholar;list of academic databases and search engines;microsoft academic search;pagerank;social network;web search engine	Weigang Li;Icaro Araújo Dantas;Ahmed Abdelfattah Saleh;Daniel LeZhi Li	2015			computer science;data science;data mining;world wide web	ML	-26.511238658671736	-57.24624555526312	128864
f4dc15cbb927ef296883bfc723e4456c7e9413a7	visualization of spread of topic words on twitter using stream graphs and relational graphs	stacked graph;social networking online data visualisation graph theory;twitter analysis;visualization;stream graph;data visualization games twitter market research sports equipment image color analysis time frequency analysis;relational graph;relational graph visualization twitter analysis stream graph stacked graph;relational graphs topic words visualization twitter stream graphs	In this paper, we examine occurrences, cooccurrences, and characteristics for influence and meaning of words by visualizing large amounts of data from Twitter. We classified words using morphological analysis of tweets and developed a stream graph by finding the frequency of each word. We analyzed the co-occurrence of words using quantification methods of the fourth type to find relationships and showed distances between words in a similarity graph. We present examples of the relationships found by our analysis.		Keigo Amma;Shunsuke Wada;Kanto Nakayama;Yuki Akamatsu;Yuichi Yaguchi;Keitaro Naruse	2014	2014 Joint 7th International Conference on Soft Computing and Intelligent Systems (SCIS) and 15th International Symposium on Advanced Intelligent Systems (ISIS)	10.1109/SCIS-ISIS.2014.7044759	visualization;computer science;theoretical computer science;data mining;graph;world wide web;graph database	SE	-24.916820771718875	-55.19980319092639	128892
3762a08b9f37d1f26913a196d7a8f446df1c3db7	dependency tree-based rules for concept-level aspect-based sentiment analysis		Over the last few years, the way people express their opinions has changed dramatically with the progress of social networks, web communities, blogs, wikis, and other online collaborative media. Now, people buy a product and express their opinion in social media so that other people can acquire knowledge about that product before they proceed to buy it. On the other hand, for the companies it has become necessary to keep track of the public opinions on their products to achieve customer satisfaction. Therefore, nowadays opinion mining is a routine task for every company for developing a widely acceptable product or providing satisfactory service. Concept-based opinion mining is a new area of research. The key parts of this research involve extraction of concepts from the text, determining product aspects, and identifying sentiment associated with these aspects. In this paper, we address each one of these tasks using a novel approach that takes text as input and use dependency parse tree-based rules to extract concepts and aspects and identify the associated sentiment. On the benchmark datasets, our method outperforms all existing state-of-the-art systems.	algorithm;benchmark (computing);blog;dependency grammar;natural language;parse tree;parsing;sentiment analysis;social media;social network;text mining;textual entailment;wiki	Soujanya Poria;Nir Ofek;Alexander F. Gelbukh;Amir Hussain;Lior Rokach	2014		10.1007/978-3-319-12024-9_5	parse tree;data mining;sentiment analysis;customer satisfaction;social media;social network;business	NLP	-21.658020059257975	-66.11449955807439	128902
3b9f492c9c594e77b205bd56e09f06a516f00566	hierarchical clustering of www image search results using visual, textual and link information	hierarchical clustering;web pages;ucl;page segmentation;discovery;theses;conference proceedings;image clustering;web image search;spectral clustering;link analysis;digital web resources;ucl discovery;open access;image search;visual features;ucl library;search result organization;book chapters;graph model;open access repository;spectral analysis;vision based page segmentation;ucl research	We consider the problem of clustering Web image search results. Generally, the image search results returned by an image search engine contain multiple topics. Organizing the results into different semantic clusters facilitates users' browsing. In this paper, we propose a hierarchical clustering method using visual, textual and link analysis. By using a vision-based page segmentation algorithm, a web page is partitioned into blocks, and the textual and link information of an image can be accurately extracted from the block containing that image. By using block-level link analysis techniques, an image graph can be constructed. We then apply spectral techniques to find a Euclidean embedding of the images which respects the graph structure. Thus for each image, we have three kinds of representations, i.e. visual feature based representation, textual feature based representation and graph based representation. Using spectral clustering techniques, we can cluster the search results into different semantic clusters. An image search example illustrates the potential of these techniques.	algorithm;cluster analysis;computer cluster;hierarchical clustering;image retrieval;link analysis;spectral clustering;www;web page;web search engine	Deng Cai;Xiaofei He;Zhiwei Li;Wei-Ying Ma;Ji-Rong Wen	2004		10.1145/1027527.1027747	feature detection;link analysis;computer science;machine learning;web page;data mining;hierarchical clustering;world wide web;information retrieval;spectral clustering	Web+IR	-28.72815120028676	-53.30885600606432	128954
2e754e1fa154dc51ccfa2f4349f7e3742a4d8eb3	adaptive cluster-based browsing using incrementally expanded queries and its effects (poster abstract)	document clustering;multi document summary;term co occurence;concept hierarchy;subsumption	The accuracy of document clustering depends on the user’s viewpoints. We make use of queries as the user’s viewpoints to enhance cluster-based browsing for a large amount of search results, reflecting the user’s interests. Here the queries are incrementally expanded in the process of cluster-based browsing. We present an application of the proposed method to an IR system on the WWW and evaluate its performance by basic experiments.	browsing;cluster analysis;experiment;www	Koji Eguchi	1999		10.1145/312624.312688	document clustering;computer science;data mining;database;world wide web;information retrieval	Web+IR	-29.40351318373557	-53.366820533395575	129152
910a221cf97f15fd1a6f35563fb9d111ffa85afb	query expansion based on folksonomy tag co-occurrence analysis	social service;electronic mail;web pages;query processing;folksonomy tag co occurrence analysis;information retrieval;social services;data mining;feedback thesauri computer science information resources keyword search data mining web sites online communities technical collaboration web pages text analysis;information content;internet;engines;information retrieval query expansion folksonomy tag co occurrence analysis semantic relationship social service web 2 0;sun;web 2 0;query processing internet;semantic relationship;semantic relations;query expansion	In traditional query expansion techniques, we choose the expansion terms based on their weights in the relevant documents. However, this kind of approaches does not take into account the semantic relationship between the original query terms and the expansion terms. Folksonomy is a social service in Web 2.0, which provides a large amount of social annotations. As the core of folksonomy, tags are high quality descriptors of the information contents and topics. Moreover, different tags describing the same information resource are semantically related to some extent. In this paper, we propose a query expansion method that utilizes the tag co-occurrence information to select the most appropriate expansion terms. Experimental results show that our tag co-occurrence-based query expansion technique consistently improves retrieval performance, compared with no-expansion method. This means the expansion terms we selected are semantically related to the original query, and tags of folksonomy will be the new resource of expansion terms.	display resolution;folksonomy;query expansion;web 2.0	Song Jin;Hongfei Lin;Sui Su	2009	2009 IEEE International Conference on Granular Computing	10.1109/GRC.2009.5255110	query optimization;query expansion;the internet;social work;self-information;computer science;web page;database;web search query;web 2.0;world wide web;information retrieval	SE	-29.46043048293457	-57.62131553258852	129435
6814fd30a563e5dc7d9323e0400279d9eafc70c8	applying short-term memory to social search agents	lru cache;bfs	lru cache;bfs	long short-term memory;social search	Albert Trias i Mansilla;Sam Sethserey;Josep Lluís de la Rosa i Esteva	2015		10.1016/j.procs.2015.09.237	computer science;artificial intelligence;data mining;world wide web	AI	-24.19609718156524	-61.053126201395735	129458
02e9705c3c140d595aad6c10eecb2538b043bd3b	adaptive quality control of web resources	search engines artificial intelligence accuracy mathematical model quality control automation equations;information resources;information sources;information source;search engines;information retrieval;dynamic repository;hypertext document;web resource;satisfiability;web resources;system performance;three dimensional;automatic correction adaptive quality control web resource dynamic repository human knowledge information retrieval information source hypertext document;hypermedia;accuracy;automatic correction;adaptive quality control;internet;quality control hypermedia information resources information retrieval internet;quality evaluation;partial evaluation;quality evaluation web resources;mathematical model;artificial intelligence;information need;quality control;human knowledge;automation	The web is a comprehensive and dynamic repository of information regarding most of the areas of human knowledge. However, retrieving information from the web is not an easy task. Web's characteristics place many difficulties to users willing to explore it as an information source. Information retrieved from the web is usually very extensive, composed of voluminous collections of hypertext documents. Taking advantage of the intrinsic value present in this huge resource requires to organize relevant information according to specific needs. When organizing web resources to satisfy persistent information needs, some evaluation scheme is required to measure the system performance, as perceived by users. The resource quality might be continuously measured, allowing for automatic corrections if and when significant deviations are detected. This adaptive quality control framework may also detect and follow drift in user information needs. We propose to evaluate and control resources' global quality based on a three-dimensional space with dimensions representing Automation, Efficacy and Efficiency. Each dimension in this space aggregates a set of measures, relevant to the resource's global quality. The quality of the system is permanently monitored, the system periodically measures and stores the values of its quality parameters. This quality framework has been partially evaluated on two web resources.	automation;content management system;hypertext;information needs;information source;organizing (structure);partial evaluation;semi-supervised learning;semiconductor industry;web resource;world wide web	Nuno Filipe Escudeiro;Paula Escudeiro	2010	2010 Seventh International Conference on the Quality of Information and Communications Technology	10.1109/QUATIC.2010.91	web service;web modeling;computer science;data mining;computer performance;web resource;information quality;world wide web;information retrieval	DB	-33.18049166075195	-57.98452060017863	129868
a1479413417201b22bfa179275afc89d0611e100	adjusting to specialties of search engines using metaweaver	search engine;web pages;meta search engine	In this paper, we propose MetaWeaver which can selectively utilize effective ones from registered search engines depending on queries. Traditional meta-search engines use all the registered search engines without considering which search engines are appropriate, and often return many irrelevant Web pages in the hit list. In contrast with that, MetaWeaver is able to evaluate the specialties of each search engine by analyzing the hit list for sample queries, and to utilize them for selecting adequate search engines for current queries. Also we found out our approach is promising by making preliminary experiments.	experiment;relevance;thesaurus;web page;web search engine	Mikihiko Mori;Seiji Yamada	2000			search engine indexing;database search engine;metasearch engine;spamdexing;data mining;search analytics;web search query;world wide web;information retrieval;search engine	AI	-30.72457834469418	-54.97369587513544	129880
5d9dcc11a829e21a429cf12a7c39d430083a38f0	spam review detection using ensemble machine learning		The importance of consumer reviews has evolved significantly with increasing inclination towards e-Commerce. Potential consumers exhibit sincere intents in seeking opinions of other consumers. These consumers have had a usage experience of the products they are intending to make a purchase decision on. The underlying businesses also deem it fit to ascertain common public opinions regarding the quality of their products as well as services. However, the consumer reviews have bulked over time to such an extent that it has become a highly challenging task to read all the reviews and detect their genuineness. Hence, it is crucial to manage reviews since spammers can manipulate the reviews to demote or promote wrong product. The paper proposes an algorithm for detecting the fake reviews. Since the proposed work concentrates only on text. So, n-gram (unigram + bigram) features are used. Supervised learning technique is used for reviews filtering. The proposed algorithm considers the combination of multiple learning algorithms for better predictive performance. The obtained results clearly indicate that using only simple features like n-gram, Ensemble can boost efficiency of algorithm at significant level.	machine learning	Shwet Mani;Sneha Kumari;Ayushi Jain;Prabhat Kumar	2018		10.1007/978-3-319-96133-0_15	computer science;artificial intelligence;machine learning;supervised learning;ensemble learning;bigram;simple features	ML	-20.001517825390344	-53.378262282948356	129883
1a5ca26f748d39db83a6c32cbf3971bdcac645ab	"""support for internet-based commonsense processing - causal knowledge discovery using japanese """"if"""" forms"""	search engine;web pages;information extraction;indexation;causal knowledge discovery;web mining;commonsense;human computer interface;knowledge discovery	This paper introduces our method for causal knowledge retrieval from the Internet resources, its results and evaluation of using it in utterance creation process. Our system automatically retrieves commonsensical knowledge from the Web resources by using simple web-mining and information extraction techniques. For retrieving causal knowledge the system uses three of specific several Japanese “if” forms. From the results we can conclude that Japanese web pages indexed by a common search engine spiders are enough to discover common causal relationships and this knowledge can be used for making Human-Computer Interfaces sound more natural and interesting than while using classic methods.	causal filter;causality;information extraction;internet;web crawler;web mining;web page;web resource;web search engine;world wide web	Yali Ge;Rafał Rzepka;Kenji Araki	2005		10.1007/11552451_131	computer science;data science;data mining;knowledge extraction;commonsense knowledge;information retrieval	AI	-29.94007422922663	-56.92911303157685	130048
88d7aa7fe7df65dab63697a983d1e69d3acb8b79	multi-layered graph-based multi-document summarization model		Multi-document summarization is a process of automatic generation of a compressed version of the given collection of documents. Recently, the graph-based models and ranking algorithms have been actively investigated by the extractive document summarization community. While most work to date focuses on homogeneous connecteness of sentences and heterogeneous connecteness of documents and sentences (e.g. sentence similarity weighted by document importance), in this paper we present a novel 3-layered graph model that emphasizes not only sentence and document level relations but also the influence of under sentence level relations (e.g. a part of sentence similarity).	algorithm;automatic summarization;multi-document summarization;windows update	Ercan Canhasi	2014	CoRR		natural language processing;multi-document summarization;computer science;automatic summarization;pattern recognition;information retrieval	NLP	-26.349453463967233	-63.845002429955635	130268
abf466df57679541f5df5c11d31abd6a3343f6a7	semantic based search technology for images	digital library;digital libraries;image annotation;image search;ontology engineering;semantic web;collective intelligence;digital image;image retrieval	In order to maximize the benefit of the huge repository of digital images available both publicly and in private collections, intelligent matchmaking tools are required. Unfortunately, most image search engines rely on free-text search which often returns inaccurate sets of results based on the recurrence of the search keywords in the text accompanying the images. In this paper we present a semantically-enabled image annotation and retrieval engine that relies on methodically structured ontologies for image annotation, thus allowing for more intelligent reasoning about the image content and subsequently obtaining a more accurate set of results and a richer set of alternatives matchmaking the original query.	automatic image annotation;classification tree method;digital image;image retrieval;ontology (information science);web search engine	P. Kshirsagar;V. Munde;S. Deshpande	2010		10.1145/1741906.1742031	visual word;image analysis;image retrieval;computer science;data mining;automatic image annotation;world wide web;information retrieval	AI	-30.663242074780133	-56.83764890850711	130324
b5f5afdbd3a508771d70c6dc5e92ebc4d057ce1b	systematic weighting and ranking: cutting the gordian knot	information retrieval;world wide web;algorithms;databases;relevance information retrieval	A powers-of-two algorithm is described that automatically creates discrete, well-defined, and unique result sets, displaying them in decreasing order of likely relevance. All computations are transparent, and a simple query form allows the searcher to focus on the choice of terms and their sequence-an implicit indicator of their relative importance. The program can be used with traditional databases or with search engines designed for the Word Wide Web. It also can be used with an intelligent agent to search the Web with a pushdown store, returning only those items that best reflect the searcheru0027s stated interests.		Charles H. Davis;Geoffrey W. McKim	1999	JASIS	10.1002/(SICI)1097-4571(1999)50:7%3C626::AID-ASI7%3E3.0.CO;2-A	relevance;computer science;data mining;database;weighting;world wide web;information retrieval;information system	NLP	-32.44763562071968	-53.8173453271346	130435
8cb45a5a03d2e8c9cc56030a99b9938cb2981087	tut: a statistical model for detecting trends, topics and user interests in social media	trend;user interest;modeling;topic;evolution	The rapid development of online social media sites is accompanied by the generation of tremendous web contents. Web users are shifting from data consumers to data producers. As a result, topic detection and tracking without taking users' interests into account is not enough. This paper presents a statistical model that can detect interpretable trends and topics from document streams, where each trend (short for trending story) corresponds to a series of continuing events or a storyline. A topic is represented by a cluster of words frequently co-occurred. A trend can contain multiple topics and a topic can be shared by different trends. In addition, by leveraging a Recurrent Chinese Restaurant Process (RCRP), the number of trends in our model can be determined automatically without human intervention, so that our model can better generalize to unseen data. Furthermore, our proposed model incorporates user interest to fully simulate the generation process of web contents, which offers the opportunity for personalized recommendation in online social media. Experiments on three different datasets indicated that our proposed model can capture meaningful topics and trends, monitor rise and fall of detected trends, outperform baseline approach in terms of perplexity on held-out dataset, and improve the result of user participation prediction by leveraging users' interests to different trends.	baseline (configuration management);computer cluster;perplexity;personalization;sensor;simulation;social media;statistical model;web 2.0	Xuning Tang;Christopher C. Yang	2012		10.1145/2396761.2396884	computer science;artificial intelligence;data science;machine learning;data mining;evolution;database;world wide web;information retrieval	Web+IR	-21.966295807471877	-53.24840852797023	130533
d871eaca03f2dd7c793ece983f9f3dae8c47ec0a	utilizing focused relevance feedback	focused;feedback;relevance	We present a novel study of ad hoc retrieval methods utilizing document-level relevance feedback and/or focused relevance feedback; namely, passages marked as (non-)relevant. The first method uses a novel mixture model that integrates relevant and non-relevant information at the language model level. The second method fuses retrieval scores produced by using relevant and non-relevant information separately. Empirical exploration attests to the merits of our methods, and sheds light on the effectiveness of using and integrating relevance feedback for textual units of varying granularities.	hoc (programming language);language model;mixture model;relevance feedback	Elinor Brondwine;Anna Shtok;Oren Kurland	2016		10.1145/2911451.2914695	relevance;computer science;artificial intelligence;machine learning;feedback;information retrieval	Web+IR	-27.756560396280605	-61.40283433129954	130560
dad5a9c5d8ba8c598c4c79df5b0a5a195e6cf313	the hyper media news system for multimodal and personalised fruition of informative content	content management;multimedia content management;information content;large scale;indexation;news recommendation	Despite the number of efforts, managing content for media production and distribution can be still considered a time-consuming and hard task. From here derives the need for effective data documentation and retrieval systems. Hyper Media News provides a set of integrated tools for large-scale acquisition, analysis, indexing, browsing and recommendation of news content from both television and the Web.	documentation;information;multimodal interaction;world wide web	Alberto Messina;Maurizio Montagnuolo;Riccardo Di Massa;Andrea Elia	2011		10.1145/1991996.1992060	self-information;content management;computer science;multimedia;world wide web;information retrieval	Web+IR	-31.193967497151853	-56.90196434108766	130858
82076a4b0c0e0129f5d04937b4df6f15e7343967	multi-media retrieval with semantic web: a case study in airport security inspection applications	airport security inspection;semantic web;ontology;semantic retrieval;image retrieval	Multi-media retrieval based on content meaning is an important issue. It has significant applications in the tracing of terrorists based on airport security inspection information. In this paper, we review the defects of traditional image retrieval technology. A semantic Web based image retrieval method is proposed, which is deployed into an airport security inspection information retrieval system. We present the techniques of domain ontology construction and semantic-Web annotation for the media. An image retrieval technique based on semantic annotation is suggested, and the design of a semantic information retrieval prototype system for airport security inspection application is described.	airport security;semantic web	Kai Zhang;Lejian Liao;Yuanda Cao;Liehuang Zhu	2006			image retrieval;computer science;semantic web;ontology;data mining;semantic web stack;automatic image annotation;world wide web;data retrieval;information retrieval;human–computer information retrieval	Web+IR	-30.992394576851968	-57.76457109417425	130922
1dfab8f086b4c588fd434a5f9e504ccd3340c875	ouc's participation in the 2012 inex book and linked-data tracks		In this article we describe the Oslo University College’s participation in the INEX 2012 endeavor. This year we participate in the Book Track’s ”Prove it” (as in 2011) and Social search tasks, as well as the Linked Data track’s Ad-hoc task. In 2011, the OUC submitted retrieval results for the ”Prove It” task with traditional relevance detection combined with detection of confrmation based on specificity detected through the Wordnet concept hierarchy. In line with our belief that proving or refuting facts are different semantic aware actions of speech, we have this year attempted to incorporate some semantic support based on Named entity recognition. For the Social search task, we wish to examine the utility of the MARCdata (subject heading field) in social searching for readings. For the Linked data task, we wish to explore the possibility of using links as a query expansion mechanism. 1 The Prove-it task of the Book Track In recent years large organizations like national libraries, as well as multinational organizations like Microsoft and Google have been investing labor, time and money in digitizing books. Beyond the preservation aspects of such digitization endeavors, they call on finding ways to exploit the newly available materials, and an important aspect of exploitation is book and passage retrieval. The INEX Book Track[1], which has been running since 2007, is an effort aiming to develop methods for retrieval in digitized books. One important aspect here is to test the limits of traditional methods of retrieval, designed for retrieval within ”documents” (such as news-wire), when applied to digitized books. One wishes to compare these methods to book-specific retrieval methods. One important mission of such retrieval is supporting the generation of new knowledge based on existing knowledge. The generation of new knowledge is closely related to access to – as well as faith in – existing knowledge. One important component of the latter is claims about facts. This year’s ”Prove It” task may be seen as challenging the most fundamental aspect of generating new knowledge, namely the establishment (or refutal) of factual claims encountered during research. On the surface, this may be seen as simple retrieval, but proving a fact is more than finding relevant documents. This type of retrieval requires from a passage to ”make a statement about” rather than ”be relevant to” a claim, which traditional retrieval is about. The questions we posed in 2010 were:	book;case preservation;course (navigation);crowdsourcing;experiment;fink;hoc (programming language);information retrieval;knowledge base;lecture notes in computer science;lexical database;library (computing);linked data;named-entity recognition;natural language;ora lassila;query expansion;question answering;relevance;semantic web;sensitivity and specificity;social media;social search;springer (tank);web intelligence;wikipedia;wordnet	Michael Preminger;David Massey	2012			computer science;artificial intelligence;data mining;communication	Web+IR	-32.23167213660529	-64.45887426000714	130996
3ff2e0864b78add68ef21d23735af2de282c5ee0	visual analysis of topical evolution in unstructured text: design and evaluation of topicflow		Topic models are regularly used to provide directed exploration and a high-level overview of a corpus of unstructured text. In many cases, it is important to analyze the evolution of topics over a time range. In this work, we present an application of statistical topic modeling and alignment (binned topic models) to group related documents into automatically generated topics and align the topics across a time range. Additionally, we present TopicFlow, an interactive tool to visualize the evolution of these topics. The tool was developed using an iterative design process based on feedback from expert reviewers. We demonstrate the utility of the tool with a detailed analysis of a corpus of data collected over the period of an academic conference, and demonstrate the effectiveness of this visualization for reasoning about large data by a usability study with 18 participants.	align (company);evolution;high- and low-level;iterative design;iterative method;text corpus;topic model;usability testing	Alison Smith;Sana Malik;Ben Shneiderman	2015		10.1007/978-3-319-19003-7_9	natural language processing;data science;world wide web	HCI	-32.37975452837593	-60.61258650804668	131010
5483f2011f986e91a024576d77c8b8624a068601	extracting semantic knowledge from twitter	e participation;trend analysis;machine learning;social network analysis;semantic search;twitter mining;semantic analysis;semantic patterns	Twitter is the second largest social network after Facebook and currently 140 millions Tweets are posted on average each day. Tweets are messages with a maximum number of 140 characters and cover all imaginable stories ranging from simple activity updates over news coverage to opinions on arbitrary topics. In this work we argue that Twitter is a valuable data source for e-Participation related projects and describe other domains were Twitter has already been used. We then focus on our own semantic-analysis framework based on our previously introduced Semantic Patterns concept. In order to highlight the benefits of semantic knowledge extraction for Twitter related e-Participation projects, we apply the presented technique to Tweets covering the protests in Egypt starting at January 25 and resulting in the ousting of Hosni Mubarak on February 11 2011. Based on these results and the lessons learned from previous knowledge extraction tasks, we identify key requirements for extracting semantic knowledge from Twitter.	requirement;semantic data model;social network	Peter Teufl;Stefan Kraxberger	2011		10.1007/978-3-642-23333-3_5	computer science;data science;data mining;world wide web	Web+IR	-24.004745342916326	-54.84580463817242	131086
a6c9184cf18bb4f8dd3e7bcca3f351181ec656bb	using a personalized anomaly detection approach with machine learning to detect stolen phones.		We devise an anomaly detection system that detects stolen phones. In this system, we use a mining algorithm to extract sequential patterns from a user’s past behavior to construct a personalized model. We then put forward scoring functions and threshold setting strategies to detect stealing events. We evaluate our approach with a data set from the MIT Reality Mining project. Experimental results indicate that our approach can detect 87% of simulated stealing events with an average false positive rate of 0.9%.	algorithm;anomaly detection;behavioral pattern;cross-validation (statistics);data mining;film-type patterned retarder;machine learning;mobile phone;personalization;preprocessor;scoring functions for docking;sensor;sequential pattern mining	Huizhong Hu;Philip K. Chan	2018			machine learning;artificial intelligence;anomaly detection;computer science	ML	-20.26004737498912	-54.735969949717074	131116
41bcdd8550a46f82e2b5f41d5c8b5bb15b7b7564	lip-reading based on fuzzy language model	mouth;probability;text analysis fuzzy set theory handicapped aids hidden markov models image matching image recognition natural language processing;smoothing methods;computational modeling;hidden markov models;video channel fuzzy language model lip reading system statistical language model vocabulary corpus fuzzy mathematical theory fuzzy evaluation sets fuzzy membership vectors recognition model hfm hmm language model computational linguistics research lab institute of applied linguistic ministry of education n gram language sentence accuracy syllable accuracy text flow analysis blindly text matching;mathematical model;corpus lip reading fuzzy language model hmm;hidden markov models mathematical model computational modeling mouth probability smoothing methods time frequency analysis;time frequency analysis	Applying language model in lip-reading system can greatly improve the recognition rate. But the traditional statistical language model depended on corpus excessively, so it could not be used in some special occasions with a small vocabulary corpus. In this paper, according to fuzzy mathematical theory, the fuzzy evaluation sets were firstly established. Then the frequencies of words or sentences in the corpus were represented as fuzzy membership vectors. Based on HMM, a novel recognition model HFM (HMM and Fuzzy Language Model) was proposed. Then a small lip-reading system was constructed. Based on corpus online constructed by computational linguistics research lab, institute of applied linguistic, ministry of education, a small vocabulary corpus was selected and established. Experimental results demonstrated that, compared to the lip-reading system using n-gram language model, applying HFM (did not need smoothing), syllable accuracy can be increased by 6.5%, and sentence accuracy by 22.7%. In addition, exploited language model for text flow analysis, rather than blindly text matching, in single video channel the accuracy can be up to 68.7%.	computational linguistics;cross-validation (statistics);data-flow analysis;hidden markov model;language model;n-gram;smoothing;syllable;text corpus;video blog;vocabulary	Zhenjun Yue;Chuanzhen Rong;Yuan Wang;Yu Kyung Yang	2015	2015 International Conference on Wireless Communications & Signal Processing (WCSP)	10.1109/WCSP.2015.7340989	n-gram;natural language processing;language identification;cache language model;speech recognition;computer science;pattern recognition	NLP	-23.065593667761853	-65.59101379882478	131117
666718149050d4a92465089df652938646b4e8db	detection of affectively comparable term using hierarchical knowledge and blog snippets	web intelligence;natural language processing;affective computing;knowledge discovery		blog	Ryosuke Yamanishi;Jun-ichi Fukumoto;Fumito Masui	2014	JACIII	10.20965/jaciii.2014.p0166	natural language processing;computer science;artificial intelligence;data science;affective computing;web intelligence;world wide web	NLP	-24.038857283453936	-61.087164710581796	131190
61d3a476ae0eea2a2aa7f85e78d1819de743a9ca	a genetic graph-based clustering approach to biomedical summarization	automatic summarization;summarization;clustering;genetic algorithms;natural language processing	Summarization techniques have become increasingly important over the last few years, specially in biomedical research, where information overload is major problem. Researchers of this area need a shorter version of the texts which contains all the important information while discarding irrelevant one. There are several applications which deal with this problem, however, these applications are sometimes less informative than the user needs. This work deals with this problem trying to improve a summarization graph-based process using genetic clustering techniques. Our automatic summaries are compared to those produced by several commercial and research summarizers, and demonstrate the appropriateness of using genetic techniques in automatic summarization.	automatic summarization;cluster analysis;information overload;norm (social);relevance	Héctor D. Menéndez;Laura Plaza;David Camacho	2013		10.1145/2479787.2479807	text graph;multi-document summarization;computer science;automatic summarization;pattern recognition;data mining;information retrieval	Web+IR	-27.307685440630856	-59.95862894035119	131294
94d71ebc4bf6f71dd08543093db8211083af10be	new word identification in social network text based on time series information	social network new word identification time domain;time series internet natural language processing social networking online text analysis;microblogging application new word identification social network text time series information western countries french english chinese language segmentation process segmentation targets chinese new words time domain information social network corpus text series string statistical features;social network services time domain analysis feature extraction entropy vectors tagging blogs	Different from the languages widely used in western countries such as English or French, there are no spaces between words in Chinese language, and a segmentation of the texts is necessary before other superior processes. New word identification is an important problem in the segmentation process, especially when the segmentation targets are social network texts which have more abbreviated words or other non-standard representations. Several methods have been proposed to detect Chinese new words. Most of these methods take the corpus as a static set and they don't consider the time domain information. Different from these studies, we regard our social network corpus as a text series spreading along the time line and design a new kind of features named dynamic features which can reflect the temporal variety of the string's statistical features. The experimental results on the dataset crawled from the biggest microblogging application in China show that this method can significantly improve the effect of Chinese new word identification.	protologism;social network;text corpus;text-based (computing);time series;timeline	Meng Wang;Lanfen Lin;Feng Wang	2014	Proceedings of the 2014 IEEE 18th International Conference on Computer Supported Cooperative Work in Design (CSCWD)	10.1109/CSCWD.2014.6846904	natural language processing;speech recognition;computer science;pattern recognition	NLP	-21.179413237129076	-59.035922889250756	131367
a51ba1ff82d0da55c2284458f8ef3c1b59e6678e	ca-lstm: search task identification with context attention based lstm		Search task identification aims to understand a user's information needs to improve search quality for applications such as query suggestion, personalized search, and advertisement retrieval. To properly identify the search task within long query sessions, it is important to partition these sessions into segments before further processing. In this paper, we present the first search session segmentation model that uses a long short-term memory (LSTM) network with an attention mechanism. This model considers sequential context knowledge, including temporal information, word and character, and essentially learns which parts of the input query sequence are relevant to the current segmentation task and determines the influence of these parts. This segmentation technique is also combined with an efficient clustering method using a novel query relevance metric for end-to-end search task identification. Using real-world datasets, we demonstrate that our segmentation technique improves task identification accuracy of existing clustering-based algorithms.	algorithm;cluster analysis;end-to-end principle;experiment;information needs;long short-term memory;long tail;personalized search;relevance;session (web analytics);web search engine	Cong Du;Peng Shu;Yong Li	2018		10.1145/3209978.3210087	computer science;information retrieval;information needs;machine learning;cluster analysis;personalized search;segmentation;artificial intelligence	Web+IR	-27.06236130341174	-54.49214651734689	131374
8fc37de7e6a7cf200d65a12506b5ebf0bda77c4d	bootstrapping delta: a safety net in open-set authorship attribution			stylometry	Maciej Eder	2013			safety net;data mining;delta;open set;bootstrapping;attribution;computer science	Logic	-19.51103149526816	-53.96261160504455	131383
6fef0d68a3b0cd15eb3abf6a50c630c9ca49bb8d	a probabilistic retrieval model for semistructured data	information systems not elsewhere classified;semistructured data;col;retrieval model;language model	Retrieving semistructured (XML) data typically requires either a structured query such as XPath, or a keyword query that does not take structure into account. In this paper, we infer structural information automatically from keyword queries and incorporate this into a retrieval model. More specifically, we propose the concept of a mapping probability, which maps each query word into a related field (or XML element). This mapping probability is used as a weight to combine the language models estimated from each field. Experiments on two test collections show that our retrieval model based on mapping probabilities outperforms baseline techniques significantly.	baseline (configuration management);experiment;language model;xml;xpath	Jinyoung Kim;Xiaobing Xue;W. Bruce Croft	2009		10.1007/978-3-642-00958-7_22	query expansion;computer science;data mining;database;mountain pass;information retrieval;query language;language model;divergence-from-randomness model	Web+IR	-28.332352819398498	-62.10739871237805	131387
4e35819810dc2f75a2161ce16f1308b4b37e6292	layout-tree-based approach for identifying visually similar blocks in a web page	layout tree;hypermedia markup languages;tag tree;layout tree based approach;information systems;web pages;information extraction;visually similar block identification;web mining layout tree pattern recognition visually similar information extraction;layout tree similarity;visually similar;layout;tree data structures;data mining;html;data visualisation;visualization;visible rectangular region;pattern matching;xpath;web sites;pattern recognition method;layout tree notion;ie systems;pattern recognition;web mining;particle separators;web page;finite element analysis;computer science miscellaneous;dom tree;hmtl source code;html dependent method;layout visualization particle separators web pages finite element analysis html data mining;web sites data mining data visualisation hypermedia markup languages pattern matching tree data structures;layout tree similarity layout tree based approach visually similar block identification web page information extraction ie systems hmtl source code dom tree tag tree xpath html dependent method layout tree notion pattern recognition method visible rectangular region	When extracting information from a web page, IE systems usually need to perform pattern recognition to identify the elements that have similar patterns. However, most of them are mainly based on analyzing HMTL source code, DOM tree, tag tree or Xpath of web pages. These methods are language-dependent, or more precisely, HTML-dependent. They have some insuperable limitations. In order to overcome these limitations, we propose a notion of layout-tree and a pattern recognition method to identify visual blocks with similar visual pattern using layout tree. In this paper, we call a visible rectangular region in a web page a visual block or block for short. We consider if the elements of two blocks are displayed in a similar layout, we define that the two blocks are visually similar. We first transform the layout into a layout tree. By calculating the similarity of the layout trees of two blocks, we can determine whether the two blocks are visually similar or not. The result of experiment shows that the layout tree is an effective method to identify visually similar blocks.	abstract syntax tree;address space layout randomization;data mining;document object model;effective method;html;information extraction;pattern recognition;tree structure;web page;xpath	Jun Zeng;Brendan Flanagan;Sachio Hirokawa	2013	2013 IEEE/ACIS 12th International Conference on Computer and Information Science (ICIS)	10.1109/ICIS.2013.6607818	web mining;computer science;web page;data mining;database;world wide web;information extraction	Vision	-28.789630849891466	-53.71363077099264	131394
1a50bd47d54979113619802a20351d16ccdb2875	keyword query expansion on linked data using linguistic and semantic features	linguistic fetures;semantic features query expansion linguistic fetures;query processing;question answering information retrieval;lightweight semantic features keyword query expansion linked data linguistic feature structured information textual user input query expansion method alternative query elements query expansion features semantic inference linguistic inference linked open data machine learning training dataset qald question answering benchmark linguistic semantic features;inference mechanisms;semantics pragmatics feature extraction benchmark testing support vector machines vocabulary vectors;semantic web inference mechanisms learning artificial intelligence query processing question answering information retrieval;semantic web;learning artificial intelligence;query expansion;semantic features	Effective search in structured information based on textual user input is of high importance in thousands of applications. Query expansion methods augment the original query of a user with alternative query elements with similar meaning to increase the chance of retrieving appropriate resources. In this work, we introduce a number of new query expansion features based on semantic and linguistic inferencing over Linked Open Data. We evaluate the effectiveness of each feature individually as well as their combinations employing several machine learning approaches. The evaluation is carried out on a training dataset extracted from the QALD question answering benchmark. Furthermore, we propose an optimized linear combination of linguistic and lightweight semantic features in order to predict the usefulness of each expansion candidate. Our experimental study shows a considerable improvement in precision and recall over baseline approaches.	baseline (configuration management);benchmark (computing);experiment;linked data;machine learning;precision and recall;query expansion;question answering;semantic web;semantic query;semantic search	Saeedeh Shekarpour;Konrad Höffner;Jens Lehmann;Sören Auer	2013	2013 IEEE Seventh International Conference on Semantic Computing	10.1109/ICSC.2013.41	natural language processing;sargable;query optimization;query expansion;web query classification;boolean conjunctive query;computer science;artificial intelligence;semantic web;data mining;web search query;information retrieval;query language	NLP	-28.588607172223856	-61.5679261790191	131430
c08f3659478581fffdd1256dbca8e9c001458fc3	overview of the cl-scisumm 2016 shared task		The CL-SciSumm 2016 Shared Task is the first medium-scale shared task on scientific document summarization in the computational linguistics (CL) domain. The task built off of the experience and training data set created in its namesake pilot task, which was conducted in 2014 by the same organizing committee. The track included three tasks involving: (1A) identifying relationships between citing documents and the referred document, (1B) classifying the discourse facets, and (2) generating the abstractive summary. The dataset comprised 30 annotated sets of citing and reference papers from the open access research papers in the CL domain. This overview paper describes the participation and the official results of the second CL-SciSumm Shared Task, organized as a part of the Joint Workshop onBibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2016), held in New Jersey,USA in June, 2016. The annotated dataset used for this shared task and the scripts used for evaluation can be accessed and used by the community at: https://github.com/WING-NUS/scisumm-corpus.	automatic summarization;computational linguistics;digital library;document;information retrieval;natural language processing;organizing (structure);test set	Kokil Jaidka;Muthu Kumar Chandrasekaran;Sajal Rustagi;Min-Yen Kan	2016			computer science	NLP	-31.360498500229546	-64.12481388303593	131596
209461790bf648c8c11ed4d472ec19acf2bd4e9a	linguistic generalization of slang used in mexican tweets, applied in aggressiveness detection		This paper summarizes the shared task aggressive detection in Twitter organized as part of the MEX-A3T workshop. The aim of this task is to determine whether a tweet is aggressive or not. In tasks of classifying small texts of social networks, as in many others, the use of bag of words can influence the performance of the models. In the Twitter context the formal and “slang” language are mixed increasing the number of synonyms and so the size of the bag of words. Generalizing some expressions as insults or laughs can reduce the size of the bag of words without losing the linguistic meaning or the writer intention, and provide a generalization to some unseeing words in the training set. Being that and immense bag of words for short texts is too disperse (Fang, et al. 2014), the use of reductions of components improves even more the performance of the models, as many words have a lack importance in the aggressiveness task, or appear too few times. In this paper we will develop a linguistic generalization for the common slang Mexican used in tweets to reduce the impact of the size in the bag of words. As well as a PCA implementation to improve the computational cost in the training process.	algorithmic efficiency;bag-of-words model;computation;mex file;principal component analysis;social network;test set	Sebastián Correa;Alberto Pesquera Martin	2018			slang;linguistics;psychology	AI	-20.76189814277939	-65.29428326734292	131636
162f6ad6b487ccb7acbeeaf1bce03195c1e50b66	a web recommendation system based on maximum entropy	nearest neighbor searches;maximum entropy methods;information systems;maximum entropy principle;information retrieval;web recommendation system;page level clickstream statistics;data mining;automatic generation;navigation;recommender system;machine learning;web usage mining;information filters maximum entropy methods data mining web sites statistics information retrieval;web sites;prediction accuracy;statistics;web usage mining web recommendation system maximum entropy principle user navigational behavior web site usage pattern discovery page level clickstream statistics information navigation;maximum entropy model;pattern analysis;entropy;usage pattern discovery;computer science;power system modeling;information navigation;entropy navigation data mining machine learning pattern analysis nearest neighbor searches power system modeling computer science information systems statistics;information filters;user navigational behavior;maximum entropy;web site	In this paper, we propose a Web recommendation system based on a maximum entropy model. Under the maximum entropy principle, multiple sources of knowledge about users' navigational behavior in a Web site can be seamlessly combined to discover usage patterns and to automatically generate the most effective recommendations for new users with similar profiles. In this paper we integrate the knowledge from page-level clickstream statistics about users' past navigations with the aggregate usage patterns discovered through Web usage mining. Our experiment results show that our method can achieve better prediction accuracy when compared to standard recommendation approaches, while providing a better interpretation of Web users' diverse navigational behaviors.	aggregate data;clickstream;data mining;entropy (information theory);feature selection;latent dirichlet allocation;markov chain;markov model;principle of maximum entropy;probabilistic latent semantic analysis;recommender system;web mining	Xin Jin;Bamshad Mobasher;Yanzan Zhou	2005	International Conference on Information Technology: Coding and Computing (ITCC'05) - Volume II	10.1109/ITCC.2005.53	computer science;data mining;world wide web;information retrieval	ML	-26.309554197751677	-53.37533271289772	131730
3b96f026d0340b7a5911225a5e53dea8e463b912	mumblesearch extraction of high quality web information for sme	content management;backend system;search engine;data mining information retrieval search engines content based retrieval monitoring performance analysis content management humans indexing computer architecture;software platform;search engines;information retrieval;e commerce;web service;data mining;computer architecture;real world application;quality requirement;monitoring;indexing;small and medium enterprise;performance analysis;intelligent agent;telecom corporation;humans;content based retrieval	Although search engines are playing a crucial role for the retrieval of information from the Web, they cannot guarantee the quality required for most relevant business activities as well as for many top-level research projects. In this paper we present MumbleSearch, a Web Content Monitor which is especially conceived to extract and organize topic-based information with emphasis on quality requirements. We present the architecture of the software platform and its deployment for a real-world application, involving Italian Small and Medium Enterprises (SME).	information extraction;requirement;software deployment;web content;web search engine;world wide web	Nicola Baldini;Marco Gori;Marco Maggini	2004	IEEE/WIC/ACM International Conference on Web Intelligence (WI'04)	10.1109/WI.2004.102	computer science;data mining;database;world wide web;intelligent agent;information retrieval;search engine	DB	-30.520192219038005	-57.0147085835906	131762
13281106c65139826841aa501f0c6cebb5e05068	semantic context transfer across heterogeneous sources for domain adaptive video search	context information;flickr context similarity;adaptive refinement;domain adaptive video search;heterogeneous sources;semantic context transfer;video search	Automatic video search based on semantic concept detectors has recently received significant attention. Since the number of available detectors is much smaller than the size of human vocabulary, one major challenge is to select appropriate detectors to response user queries. In this paper, we propose a novel approach that leverages heterogeneous knowledge sources for domain adaptive video search. First, instead of utilizing WordNet as most existing works, we exploit the context information associated with Flickr images to estimate query-detector similarity. The resulting measurement, named Flickr context similarity (FCS), reflects the co-occurrence statistics of words in image context rather than textual corpus. Starting from an initial detector set determined by FCS, our approach novelly transfers semantic context learned from test data domain to adaptively refine the query-detector similarity. The semantic context transfer process provides an effective means to cope with the domain shift between external knowledge source (e.g., Flickr context) and test data, which is a critical issue in video search. To the best of our knowledge, this work represents the first research aiming to tackle the challenging issue of domain change in video search. Extensive experiments on 120 textual queries over TRECVID 2005-2008 data sets demonstrate the effectiveness of semantic context transfer for domain adaptive video search. Results also show that the FCS is suitable for measuring query-detector similarity, producing better performance to various other popular measures.	data domain;experiment;flickr;sensor;test data;vocabulary;wordnet	Yu-Gang Jiang;Chong-Wah Ngo;Shih-Fu Chang	2009		10.1145/1631272.1631296	semantic similarity;semantic search;computer science;data mining;world wide web;information retrieval	Vision	-27.905601100133126	-58.66142826998304	131813
90220e305ea03da19f699622d2fb498d3be39d94	sequential patterns for text categorization	statistical approach;text mining;sequential patterns;machine learning;association rule;natural language;spac;support vector machine;sequential pattern;bag of words;text categorization;categorization;neural network	Text categorization is a well-known task based essentially on statistical approaches using neural networks, Support Vector Machines and other machine learning algorithms. Texts are generally considered as bags of words without any order. Although these approaches have proven to be efficient, they do not provide users with comprehensive and reusable rules about their data. Such rules are, however, very important for users to describe trends in the data they have to analyze. In this framework, an association-rule based approach has been proposed by Bing Liu (CBA). We propose, in this paper, to extend this approach by using sequential patterns in the SPaC method (Sequential Patterns for Classification) for text categorization. Taking order into account allows us to represent the succession of words through a document without complex and time-consuming representations and treatments such as those performed in natural language and grammatical methods. The original method we propose here consists of mining sequential patterns in order to build a classifier. We experimentally show that our proposal is relevant, and that it is very interesting compared to other methods. In particular, our method outperforms CBA and provides better results than SVM on some corpus.	algorithm;artificial neural network;bing liu;categorization;context-free grammar;data mining;document classification;experiment;machine learning;natural language;online complex processing;performance;sequential logic;sequential pattern mining;statistical classification;succession;support vector machine	Simon Jaillet;Anne Laurent;Maguelonne Teisseire	2006	Intell. Data Anal.		support vector machine;text mining;association rule learning;boosting methods for object categorization;computer science;bag-of-words model;machine learning;pattern recognition;data mining;natural language;artificial neural network;categorization	ML	-20.886315237625283	-65.4020215706594	131899
1ed80c9d83fd018cb9733f5a632a783c157abb28	current state of text sentiment analysis from opinion to emotion mining		Sentiment analysis from text consists of extracting information about opinions, sentiments, and even emotions conveyed by writers towards topics of interest. It is often equated to opinion mining, but it should also encompass emotion mining. Opinion mining involves the use of natural language processing and machine learning to determine the attitude of a writer towards a subject. Emotion mining is also using similar technologies but is concerned with detecting and classifying writers emotions toward events or topics. Textual emotion-mining methods have various applications, including gaining information about customer satisfaction, helping in selecting teaching materials in e-learning, recommending products based on users emotions, and even predicting mental-health disorders. In surveys on sentiment analysis, which are often old or incomplete, the strong link between opinion mining and emotion mining is understated. This motivates the need for a different and new perspective on the literature on sentiment analysis, with a focus on emotion mining. We present the state-of-the-art methods and propose the following contributions: (1) a taxonomy of sentiment analysis; (2) a survey on polarity classification methods and resources, especially those related to emotion mining; (3) a complete survey on emotion theories and emotion-mining research; and (4) some useful resources, including lexicons and datasets.	biconnected component;blog;categorization;email;lexicon;machine learning;natural language processing;sensor;sentiment analysis;theory	Ali Yadollahi;Ameneh Gholipour Shahraki;Osmar R. Zaïane	2017	ACM Comput. Surv.	10.1145/3057270	sentiment analysis;data mining;data science;concept mining;customer satisfaction;computer science;text mining	NLP	-22.616250453585643	-58.34051399132221	131957
028ff8659b2e88fa53d88cdec3c776238bd553c9	sistem informasi pengarsipan menggunakan algoritma levensthein string pada kecamatan seberang ulu ii	t technology general;ai indexes general	Archival information systems in government agency is one of the most used applications for daily acitivities. One feature in application management information document is searching. This feature serves to search for documents from a collection of available information based on keywords entered by the user. But some researches on a search engine (searching) concluded that the average user error in the search is quite high due to several factors. Therefore, we need a development on this feature as search suggestion. This study discusses the application of the method of approximate string matching algorithm using levenshtein distance. Levensthein distance algorithm is capable of calculating the minimum distance conversion of a string into another string to the optimum. Archiving information system using Levensthein Algorithm String is an application that will be built to address these problems, this application will help, especially in the administration to enter or save a document, locate and make a report that will be seen by government agencies. Kyewords: Archival information systems, Levensthein algorithms.	application lifecycle management;approximate string matching;levenshtein distance;management information system;string searching algorithm;user error;web search engine	Pratiwi Vidyarsih;Leon Andretti Abdillah;Ari Muzakir	2016	CoRR		telecommunications;computer science;artificial intelligence;operating system;data mining;string metric;world wide web;computer security;information retrieval	NLP	-33.529054621237236	-56.74838198070163	132078
6e0fc37f34a7d5fb613978bc02e74caf0b9300e7	whu at blog track 2007	data collection	We participate in all the sub tracks of the Blog track 2007. For the Opinionated Task, we use an excerpt list of the SentiWordNet to determine the opinionated nature of returned blog posts. For the Topic distillation Task, we test the effectiveness of cleaning work for the data collection in improvement of retrieval performance and the use of title and description part as queries. Both results show that our method does not work well.	blog;plasma cleaning	Haozhen Zhao;Zhicheng Luo;Wei Lu	2007			computer science;multimedia;world wide web;information retrieval;data collection	NLP	-31.59242265472037	-62.842286504755506	132087
9bc54ca3c844ea2ff09bca659aa5de44b2d49efd	an unsupervised technique to extract information from semi-structured web pages	unsupervised learning;web information extraction	We propose a technique that takes two or more web pages generated by the same server-side template and tries to learn a regular expression that represents it and helps extract relevant information from similar pages. Our experimental results on real-world web sites demonstrate that our technique outperforms others in terms of both effectiveness and efficiency and is not affected by HTML errors.	semiconductor industry;unsupervised learning;web page	Hassan A. Sleiman;Rafael Corchuelo	2012		10.1007/978-3-642-35063-4_46	computer science;data mining;world wide web;information retrieval	Theory	-28.317512451956304	-55.10293824671741	132148
0ce40685b9324c3e67d59591aeb980ff1133a19a	a hybrid learning algorithm for text classification	supervised learning;information retrieval;text classification;science learning;association rule;hybrid learning;genetic algorithm;bayes classifier;evolutionary computing	Text classification is the process of classifying documents into predefined categories based on their content. Existing supervised learning algorithms to automatically classify text need sufficient documents to learn accurately. This paper presents a new algorithm for text classification that requires fewer documents for training. Instead of using words, word relation i.e association rules from these words is used to derive feature set from preclassified text documents. The concept of Naïve Bayes classifier is then used on derived features and finally only a single concept of Genetic Algorithm has been added for final classification. Experimental results show that the classifier build this way is more accurate than the existing text classification systems.	association rule learning;computation;document classification;genetic algorithm;hybrid algorithm;machine learning;naive bayes classifier;randomness;supervised learning;synapomorphy;test set;time complexity	S. M. Kamruzzaman;Farhana Haider	2004	CoRR		bayes classifier;naive bayes classifier;genetic algorithm;association rule learning;computer science;machine learning;linear classifier;pattern recognition;data mining;learning classifier system;supervised learning;one-class classification;evolutionary computation	AI	-20.754243003308847	-64.81745156579542	132151
44e72db6dcd85250b3bc1d7e86753b841bb3d9cc	detecting perspectives in political debates		We explore how to detect people’s perspectives that occupy a certain proposition. We propose a Bayesian modelling approach where topics (or propositions) and their associated perspectives (or viewpoints) are modeled as latent variables. Words associated with topics or perspectives follow different generative routes. Based on the extracted perspectives, we can extract the top associated sentences from text to generate a succinct summary which allows a quick glimpse of the main viewpoints in a document. The model is evaluated on debates from the House of Commons of the UK Parliament, revealing perspectives from the debates without the use of labelled data and obtaining better results than previous related solutions under a variety of evaluations.		David Vilares;Yulan He	2017			natural language processing;computer science;artificial intelligence;politics	NLP	-24.285762162705748	-59.529838355414014	132169
118d716215ed78e96d9922a2fcecb5e6cab9b40a	sentiment computing for the news event based on the social media big data	text mining sentiment computing emotion recognition social network services classification big data data mining;social network services;text mining;sentiment computing;emotion recognition;data mining;classification;journal article;big data;big data data mining internet sentiment analysis social networking online;social media big data mining web sentiment computing word emotion association network news event sentiment computation wean word emotion computation algorithm initial word emotion standard emotion thesaurus sentence sentiment;text mining sentiment computing emotion classification social media big data	The explosive increasing of the social media data on the Web has created and promoted the development of the social media big data mining area welcomed by researchers from both academia and industry. The sentiment computing of news event is a significant component of the social media big data. It has also attracted a lot of researches, which could support many real-world applications, such as public opinion monitoring for governments and news recommendation for Websites. However, existing sentiment computing methods are mainly based on the standard emotion thesaurus or supervised methods, which are not scalable to the social media big data. Therefore, we propose an innovative method to do the sentiment computing for news events. More specially, based on the social media data (i.e., words and emoticons) of a news event, a word emotion association network (WEAN) is built to jointly express its semantic and emotion, which lays the foundation for the news event sentiment computation. Based on WEAN, a word emotion computation algorithm is proposed to obtain the initial words emotion, which are further refined through the standard emotion thesaurus. With the words emotion in hand, we can compute every sentence’s sentiment. Experimental results on real-world data sets demonstrate the excellent performance of the proposed method on the emotion computing for news events.	algorithm;big data;computation;data mining;emoticon;scalability;social media;supervised learning;thesaurus;world wide web	Dandan Jiang;Xiangfeng Luo;Junyu Xuan;Zheng Xu	2017	IEEE Access	10.1109/ACCESS.2016.2607218	text mining;big data;biological classification;computer science;data science;data mining;world wide web;sentiment analysis	AI	-22.339926412722974	-57.738525003108	132188
c697523a1de0a7bf6cfd548fad17619fb56948a3	deep learning for sentiment analysis		Lang Linguist Compass 2016; 10: 701–719 wileyo Abstract Research and industry are becoming more and more interested in finding automatically the polarised opinion of the general public regarding a specific subject. The advent of social networks has opened the possibility of having access to massive blogs, recommendations, and reviews. The challenge is to extract the polarity from these data, which is a task of opinion mining or sentiment analysis. The specific difficulties inherent in this task include issues related to subjective interpretation and linguistic phenomena that affect the polarity of words. Recently, deep learning has become a popular method of addressing this task. However, different approaches have been proposed in the literature. This article provides an overview of deep learning for sentiment analysis in order to place these approaches in context.	blog;deep learning;interpretation (logic);sentiment analysis;social network	Lina Maria Rojas-Barahona	2016	Language and Linguistics Compass	10.1111/lnc3.12228	psychology;computer science;artificial intelligence;data science;data mining;linguistics;sociology;sentiment analysis	NLP	-22.613490961335785	-58.988184671663035	132255
0d3d59e058e4da718fb6076e270028482fd818fa	using a bayesian network induction approach for text categorization	bayesian network;bayesian method	We investigate Bayesian methods for automatic document                    categorization and develop a new approach to this problem.                    Our new approach is based on a Bayesian network induction                    which does not rely on some major assumptions found in a                    previous method using the Bayesian independence classifier                    approach. The design of the new approach as well as its                    justification are presented. Experiments were conducted                    using a large scale document collection from Reuters news                    articles. The results show that our approach outperformed                    the Bayesian independence classifier as measured by a                    metric that combines precision and recall measures.	bayesian network;categorization;document classification	Wai Lam;Kon Fan Low;Chao Yang Ho	1997			machine learning;artificial intelligence;categorization;boosting methods for object categorization;bayesian network;computer science;pattern recognition;bayesian probability	AI	-21.15251301758488	-63.79838941383652	132333
7d3c2ff37d04914836f9cbd9ce54b6c97aa74a22	truth of varying shades: analyzing language in fake news and political fact-checking		We present an analytic study on the language of news media in the context of political fact-checking and fake news detection. We compare the language of real news with that of satire, hoaxes, and propaganda to find linguistic characteristics of untrustworthy text. To probe the feasibility of automatic political fact-checking, we also present a case study based on PolitiFact.com using their factuality judgments on a 6-point scale. Experiments show that while media fact-checking remains to be an open research question, stylistic cues can help determine the truthfulness of text.	computer multitasking;experiment;feedback;ibm notes;open research	Hannah Rashkin;Eunsol Choi;Jin Yea Jang;Svitlana Volkova;Yejin Choi	2017		10.18653/v1/d17-1317	artificial intelligence;natural language processing;computer science;politics;fake news	NLP	-20.73136802848356	-55.538092766692145	132346
d1501eb3105c4c6b5b916b7e4820ca37eb19b2aa	an efficient internet crawling and filtering system for the nationwide tendering information retrieval	web page internet crawling system internet filtering system nationwide tendering information retrieval central government local government web site search engine contracting information database;public information systems;search engine;web pages;information retrieval;information filtering;web sites internet information retrieval public information systems information filters tendering;local governance;internet information filtering information filters information retrieval local government search engines ieee news manufacturing databases web pages;tendering;internet;web sites;information filters	With the growth of Internet, the central government and local governments have begun to publish matters concerning the prospect of orders for public works, the announcement of tendering and the contracting information on their Web sites. However, it is time consuming and painful for bidders such as constructors and manufacturers to periodically search the above information that matches their needs. Recently, there are various search engines, e.g. Google and Yahoo!, but those general search engines are not effective for the purpose of retrieving the above information quickly enough because of their crawling interval and coverage. Then we developed a system to automate the process of gathering such information, filtering for users' needs and delivering as the tendering and contracting information database. We describe the concept of the system as well as the key techniques to realize it: (1) to efficiently retrieve only relevant Web pages, and (2) filtering to match users' needs.	information retrieval;procurement	Toshio Matsuda;Kazushige Nakamura;Norihiko Sakamoto	2003		10.1109/WI.2003.1241304	the internet;procurement;web search engine;computer science;information filtering system;distributed web crawling;web page;data mining;adversarial information retrieval;world wide web;information retrieval;search engine;human–computer information retrieval	Web+IR	-31.297873466076176	-55.336372323384616	132477
6b6ad6a0d6c7d7d946838d03abc022064934b872	webis at the trec 2010 sessions track		In this paper we provide an overview of the Webis group’s two-phase approach to the TREC 2010 Sessions track. In a preprocessing phase the queries are segmented to highlight contained concepts. In the final retrieval phase we treat Carnegie Mellon’s ClueWeb search engine as a black box and apply the MAXIMUM QUERY framework.	black box;preprocessor;two-phase commit protocol;web search engine	Matthias Hagen;Benno Stein;Michael Völske	2010				Web+IR	-31.89869424884472	-63.375950326097616	132478
5c7ab89a9a329efd3b071731344401e6d6d3a5b6	web metadata: a matter of semantics	data handling internet information retrieval;internet web metadata interoperability searching world wide web resource description framework machine understandable semantics;information retrieval;internet resource description framework application software libraries search engines web sites computer industry xml surges pain;internet;data handling	"""T he surge in popularity of the World Wide Web—and in the quantity of information it contains—is staggering. Although the Web is built on relatively simple principles, its growth has not been without substantial growing pains. The Web was built for human consumption , and although everything on the Web is machine-readable, it is not machine-understandable. This makes it very hard to automate anything on the Web and—because of the sheer volume of information—impossi-ble to manage manually. Coping with the volume of information on the Web is a real problem, as witnessed by anyone who has used one of the popular search services. Since Web documents are not designed to be understood by machines, the only real form of searching available to us is full-text search. Entering keywords into a search engine and receiving thousands of hits is not necessarily useful—the documents we seek may or may not be among those thousands. Mere words used as search keywords are subject to cross-disciplinary semantic drift. Keywords thus perform poorly in situations where a search index covers multiple subject areas, as is the case with the Web. 1 Wouldn't it be useful if other means of searching were available to us, in addition to full-text search (matching strings)? For example, we might know who wrote the document, when it was published, and what specifically it discusses (although any particular word describing that subject might not be contained in the document desired). Machines (in this case, search engines) cannot understand a natural-language document and thus cannot always extract specific information from the document, such as author, publication date, or topic. In a recent white paper, Tim Berners-Lee, director of the World Wide Web Consortium, wrote: """" Currently there is not only a large industry in applications to put information from legacy information systems onto the Web, there is also an industry in applications which surf the Web and, programmed with some idea of how the Web pages were automatically generated , retrieve the information and reconvert it into hard, well-defined machine-processable data. """" 2 It's clear that stronger, more precise means of describing documents are needed. Information such as author, publication dates, and so forth is often called metadata. Metadata is commonly defined as data about data. For example, a library catalog is metadata because it describes publications, or library data. Similarly, a file system maintains access control information about files; this information …"""	access control;consortium;document;human-readable medium;information system;natural language;search engine indexing;speeded up robust features;web page;web search engine;world wide web	Ora Lassila	1998	IEEE Internet Computing	10.1109/4236.707688	web service;web application security;web development;web modeling;the internet;web of things;web mapping;web standards;computer science;web api;semantic web;group method of data handling;web navigation;web page;semantic web stack;database;web intelligence;ws-i basic profile;web 2.0;law;world wide web;information retrieval;universal description discovery and integration	Web+IR	-31.491608879378333	-56.367219932843966	132760
e003697c219f12f85d0502146e9afa1a2adb120f	overview of the 2013 alta shared task		The 2013 ALTA shared task was the fourth in the ALTA series of shared tasks, where all participants attempt to solve the same problem using the same data. This year’s shared task was based on the problem of restoring casing and punctuation. As with last year, we used Kaggle in Class as the framework to submit the results and maintaining a leaderboard. There was a strong participation this year, with 50 teams participating, of which 21 teams submitted results that improved on the published baseline. In this overview we describe the task, the process of building the training set, and the evaluation criteria, and present the results of the submitted systems. We also comment on our experience with using Kaggle in Class.	baseline (configuration management);test set;trusted computer system evaluation criteria	Diego Mollá Aliod	2013			real-time computing;simulation;engineering;operations management	NLP	-31.797995736903673	-63.93121678861164	132803
4a548be993a09583b1939d5e8837dc5a615a88d8	towards meaningful maps of polish case law	case law;unsupervised learning;info eu repo semantics article;visualization of document collections	In this work, we analyze the utility of two dimensional document maps for exploratory analysis of Polish case law. Such maps reflect the structure of analyzed collection by grouping similar documents in a neighbouring regions of 2D space. This visual aid could be useful for browsing and searching, finding anomalous documents or quickly gaining synthetic knowledge about large corpora. We started our study by selecting two automatic methods of generating document maps. First one is a standard method using linear principal component analysis (PCA) and visualizing two most important components on a scatterplot. Second one is a modern nonlinear method of embedding multidimensional data in 2D, namely, the t-Distributed Stochastic Neighbor Embedding method (t-SNE) [1]. t-SNE is optimized to preserve local structure (i.e., very similar documents should end up close together) and to reduce the so called ,,crowding problem”. The ,,crowding problem” is undesired feature of many dimensionality reduction techniques, which essentially place many data points in a very small area of low-dimensional space (hence creating a crowd). These two features make t-SNE unique and frequently used in recent analyses of high-dimensional data. The results of the PCA and t-SNE analysis of 2000 judgments from Polish courts are presented in Figure 1. t-SNE approach forms three well separated clusters which occupy large fraction of available space. For PCA, there is no obvious structure visible, as well as most of the available space remains unused. In Figure 2, we added color-coded information about the document source (which	crowding;data point;dimensionality reduction;map;nonlinear system;principal component analysis;synthetic intelligence;t-distributed stochastic neighbor embedding;text corpus	Michal Jungiewicz;Michal Lopuszynski	2015		10.3233/978-1-61499-609-5-185	unsupervised learning;computer science;artificial intelligence;data science;common law;machine learning;data mining	ML	-23.379714436258958	-59.62138692713789	132823
05522cae9b879738abf995baef66f74bda90daef	don't use a lot when little will do: genre identification using urls	genre identification;focused crawlers;web page classification	Shih et. al. [WWW 2004] Web page classification for content recommendation and ad blocking URL tokens and the their placement in the referring page (HTML tree structure) Kan et. al. [WWW 2004] New segmentation techniques introduced using an information theoretic method. Tokens obtained using information content of other partitions over and above the nonalphabetic characters. Anastacio et al. [PAI 2009] Categorizing documents according to their implicit locational relevance. URL n-grams with assign weights according to the TF-IDF scheme. Abramson et al. [AAAI 2012] They build classifiers (Naive Bayes and SVM) using Santini and Syr7 datasets. Syntactic and semantic style features, POS tags, punctuations and special characters	blocking (computing);brown corpus;categorization;contextual advertising;grams;html;http referer;information theory;n-gram;naive bayes classifier;relevance;self-information;tf–idf;tree structure;url redirection;www;web page	Pattisapu Nikhil Priyatam;Srinivasan Iyengar;Krish Perumal;Vasudeva Varma	2013	Research in Computing Science		multimedia;world wide web;information retrieval	NLP	-24.18422198359576	-61.71526559159834	132883
171c34a28f4853aa2a198183ce03e2346f27d5cc	autosummeng and memog in evaluating guided summaries		Within this article, we present the application of the AutoSummENG and MeMoG methods within the TAC 2011 AESOP challenge. Both evaluation methods are based on n-gram graphs. The experiments indicate that both methods offer very high performance in different aspects of evaluation, without the need of deep analysis or preprocessing. The results also imply some interesting open problems and point to further directions of study, related to negative examples of good summaries.	experiment;n-gram;preprocessor;usability	George Giannakopoulos;Vangelis Karkaletsis	2011			data mining;information retrieval;computer science;preprocessor;graph	NLP	-27.95013965134509	-63.814715566001496	133135
ac1d1653ed37c2d38926a61bc59ed11a8313caad	impact of query intent and search context on clickthrough behavior in sponsored search		Implicit feedback techniques may be used for query intent detection, taking advantage of user behavior to understand their interests and preferences. In sponsored search, a primary concern is the user’s interest in purchasing or utilizing a commercial service, or what is called online commercial intent. In this paper, we develop a methodology for employing the content of search engine result pages (SERPs), along with the information obtained from query strings, to study characteristics of query intent, with a particular focus on sponsored search. Our work represents a step toward the development and evaluation of an ontology for commercial search, considering queries that reference specific products, brands, and retailers. Characteristics of query categories are studied with respect to aggregated user clickthrough behavior on advertising links. We present a model for clickthrough behavior that considers the influence of such factors as the location of ads and the rank of ads, along with query category. We evaluate our work using a large corpus of clickthrough data obtained from a major commercial search engine. In addition, the impact of query intent is studied on clickthrough rate, where a baseline model and the query intent model are compared for the purpose of calculating an expected ad clickthrough rate. Our findings suggest that query-based features, along with the content of SERPs, are effective in detecting query intent. Factors such as query category, the rank of an ad, and the total number of ads displayed on a result page relate to the context of the ad, rather than its content. We demonstrate that these context-related factors can have a major influence on expected clickthrough rate, suggesting that these factors should be taken into consideration when the performance of an ad is evaluated.	amazon mechanical turk;baseline (configuration management);mathematical optimization;purchasing;query string;search engine marketing;search engine optimization;search engine results page;sensor;text corpus;the turk;web search engine;web search query	Azin Ashkan;Charles L. A. Clarke	2012	Knowledge and Information Systems	10.1007/s10115-012-0485-x	query expansion;web query classification;computer science;data mining;database;world wide web	Web+IR	-32.509658624442366	-53.851634945125824	133902
860b8f55f2974cfca0a8f798722044c92e9faaf9	falconao: aligning ontologies with falcon.	similarity measurement;mapping;match- ing;ontology alignment;semantic web	Falcon-AO is an automatic tool for aligning ontologies. There are two matchers integrated in Falcon-AO: one is a matcher based on linguistic matching for ontologies, called LMO; the other is a matcher based on graph matching for ontologies, called GMO. In Falcon-AO, GMO takes the alignments generated by LMO as external input and outputs additional alignments. Reliable alignments are gained through LMO as well as GMO according to the concept of reliability. The reliability is obtained by observing the linguistic comparability and structural comparability of the two ontologies being compared. We have performed Falcon-AO on tests provided by OAEI 2005 campaign and got some preliminary results. In this paper, we describe the architecture and techniques of Falcon-AO in brief and present our results in more details. Finally, comments about test cases and lessons learnt from the campaign will be presented.	ambient occlusion;falcon (video game series);matching (graph theory);ontology (information science);test case	Ningsheng Jian;Wei Hu;Gong Cheng;Yuzhong Qu	2005			upper ontology;ontology alignment;semantic similarity;computer science;ontology;semantic web;data mining;semantic web stack;database;information retrieval;matching	Robotics	-29.867978168192266	-64.615032487161	133957
8622c7ec517e41703fe6fe7c3f45f2c2293c3ae7	fast categorization of web documents represented by graphs	web documents;classification algorithm;decision tree;document model;vector space model;information retrieval;vector space;document representation;hybrid approach;hybrid method;k nearest neighbor;classification accuracy;text categorization	Most text categorization methods are based on the vector-space model of information retrieval. One of the important advantages of this representation model is that it can be used by both instance-based and model-based classifiers for categorization. However, this popular method of document representation does not capture important structural information, such as the order and proximity of word occurrence or the location of a word within the document. It also makes no use of the mark-up information that is available from web document HTML tags. A recently developed graph-based representation of web documents can preserve the structural information. The new document model was shown to outperform the traditional vector representation, using the k-Nearest Neighbor (k-NN) classification algorithm. The problem, however, is that the eager (model-based) classifiers cannot work with this representation directly. In this chapter, three new, hybrid approaches to web document categorization are presented, built upon both graph and vector space representations, thus preserving the benefits and overcoming the limitations of each. The hybrid methods presented here are compared to vector-based models using two model-based classifiers (C4.5 decision-tree algorithm and probabilistic Naïve Bayes) and several benchmark web document collections. The results demonstrate that the hybrid methods outperform, in most cases, existing approaches in terms of classification accuracy, and in addition, achieve a significant increase in the categorization speed.		Alex Markov;Mark Last;Abraham Kandel	2006		10.1007/978-3-540-77485-3_4	web query classification;vector space;computer science;machine learning;decision tree;pattern recognition;k-nearest neighbors algorithm;vector space model;information retrieval	Web+IR	-27.256125650046368	-62.42182559366238	134061
0f726e8637eeb761e48294e75aaefa013f99905f	robust query rewriting using anchor data	anchor text;query rewriting	Query rewriting algorithms can be used as a form of query expansion, by combining the user's original query with automatically generated rewrites. Rewriting algorithms bring linguistic datasets to bear without the need for iterative relevance feedback, but most studies of rewriting have used proprietary datasets such as large-scale search logs. By contrast this paper uses readily available data, particularly ClueWeb09 link text with over 1.2 billion anchor phrases, to generate rewrites. To avoid overfitting, our initial analysis is performed using Million Query Track queries, leading us to identify three algorithms which perform well. We then test the algorithms on Web and newswire data. Results show good properties in terms of robustness and early precision.	algorithm;anchor text;iteration;overfitting;query expansion;relevance feedback;rewriting;robustness (computer science)	Nick Craswell;Bodo Billerbeck;Dennis Fetterly;Marc Najork	2013		10.1145/2433396.2433440	query optimization;query expansion;web query classification;anchor text;computer science;data mining;database;web search query;world wide web;information retrieval;query language	Web+IR	-30.974563360671585	-59.411257488118274	134328
85365fed8600373f72585924348210ea3566c31f	imcube @ mediaeval 2015 retrieving diverse social images task: multimodal filtering and re-ranking		This paper summarizes the participation of Imcube at the Retrieving Diverse Social Images Task of MediaEval 2015. This task addresses the problem of result diversification in the context of social photo retrieval where the results of a query should contain relevant but diverse items. Therefore, we propose a multi-modal approach for filtering and re-ranking in order to improve the relevancy and diversity of the returned list of ranked images.	diversification (finance);modal logic;multimodal interaction;relevance	Sebastian Schmiedeke;Pascal Kelm;Lutz Goldmann	2015			information retrieval;filter (signal processing);ranking;geography;diversification (marketing strategy)	Web+IR	-30.715515863916355	-61.40483823123115	134329
52be7cfb6dc2114811489c0461f327d4233caadd	recall estimation for rare topic retrieval from large corpuses	recall estimation;human evaluation recall estimation classifier evaluation social media twitter mechanical turk;estimation measurement labeling media twitter joints companies;human evaluation;classifier evaluation;social media classifiers recall estimation rare topic retrieval pertinent documents sentiment analysis clustering keyword filters classification models topical analysis classified documents unbiased sampling recall measurement procedure;pattern clustering document handling information retrieval pattern classification;twitter;social media;mechanical turk	The problem of finding documents pertaining to a particular topic finds application in a variety of scenarios. Indeed, the demand for topically pertinent documents has led to myriad companies offering services to find and deliver them (perhaps along with sentiment analysis or clustering) to customers for any topics of interest. The methodologies used to uncover relevant documents range from manually curated keyword filters to trained classification models. Any serious topical analysis requires a sound understanding of key metrics behind the retrieval process, two of the most important being precision and recall. While precision can be easily and inexpensively measured by sampling from classified documents and utilizing (paid) human computation to mark incorrectly classified instances, it is not as straightforward to use the same approach for measuring recall. With most topics occurring relatively sparsely, an unbiased sampling approach becomes prohibitively expensive. In this paper, we introduce a recall measurement procedure requiring only relatively few human judgements. The technique makes use of pairs of sufficiently independent classifiers and the paper provides a detailed discussion of how such classifier pairs can be constructed in practice, with a focus on social media classifiers. We report the performance of the proposed method with simple keyword filters as well as with classifiers of progressive levels of complexity and show that under reasonable conditions, recall can be estimated to within 0.10 absolute error and 15% relative error, and often closer with a reduction of cost by a factor of as much as 1000x as compared with unbiased sampling.	approximation error;cluster analysis;human-based computation;numerical method;precision and recall;rm-odp;relevance;sampling (signal processing);sentiment analysis;social media	Praveen Bommannavar;Alek Kolcz;Anand Rajaraman	2014	2014 IEEE International Conference on Big Data (Big Data)	10.1109/BigData.2014.7004312	computer science;pattern recognition;data mining;precision and recall;information retrieval	DB	-22.418608277827982	-52.594234345970015	134370
d8c46943b3fb59dc89592c40abdf0e3be155f783	a similarity reinforcement algorithm for heterogeneous web pages	similarity metric;web pages;vector space model;heterogeneous data;machine learning;latent semantic indexing;data mining algorithm	Many machine learning and data mining algorithms crucially rely on the similarity metrics. However, most early research works such as Vector Space Model or Latent Semantic Index only used single relationship to measure the similarity of data objects. In this paper, we first use an Intraand InterType Relationship Matrix (IITRM) to represent a set of heterogeneous data objects and their inter-relationships. Then, we propose a novel similarity-calculating algorithm over the Interand IntraType Relationship Matrix. It tries to integrate information from heterogeneous sources to serve their purposes by iteratively computing. This algorithm can help detect latent relationships among heterogeneous data objects. Our new algorithm is based on the intuition that the intrarelationship should affect the inter-relationship, and vice versa. Experimental results on the MSN logs dataset show that our algorithm outperforms the traditional Cosine similarity.	algorithm;cluster analysis;cosine similarity;data mining;digital library;iterative method;library (computing);machine learning;sequence read archive;similarity learning;web page	Ning Liu;Jun Yan;Fengshan Bai;Benyu Zhang;Wensi Xi;Weiguo Fan;Zheng Chen;Lei Ji;Chenyong Hu;Wei-Ying Ma	2005		10.1007/978-3-540-31849-1_13	semantic similarity;latent semantic indexing;computer science;artificial intelligence;data science;machine learning;web page;data mining;database;world wide web;vector space model	ML	-28.122617321112724	-60.173695201560605	134375
e0c49060c658cf928827bec44feda2f78f17b3fa	lionrank: lion algorithm-based metasearch engines for re-ranking of webpages		Due to the rapid growth of the web, the process of collecting the relevant web pages based on the user query is one of the major challenging tasks in recent days. Hence, it is very complicated for the users to know the most relevant information even though various search engines are widely employed. To deal with users’ trouble in identifying the relevant information from the web, we have proposed a meta-lion search engine to capture and analyze the ranking scores of various search engines and thereby, generate the re-ranked score results. Accordingly, LionRank, a lion algorithm-based meta-search engine is proposed for the re-ranking of the web pages. Here, different features like text based, factor based, rank based and classifier based features are used by the underlying search engines. In classifier based feature extraction, we have used the fuzzy integrated extended nearest neighbor (FENN) classifier to include the semantics in feature extraction. Moreover, an intelligent re-ranking process is proposed based on the lion algorithm to fuse the features scores optimally. Finally, the results of the proposed LionRank is analyzed with the web page database collected through four benchmark queries, and the quantitative performance are analyzed using precision, recall, and F-score. From the results, we proved that the proposed LionRank obtained the maximum F-score of 81% as compared with that of existing search engines like QuadRank, Outrank, Google, Yahoo, and Bing.	algorithm;benchmark (computing);binge eating disorder;eighty;f1 score;feature extraction;fuse device component;mathematical optimization;page (document);question (inquiry);single linkage cluster analysis;statistical classification;text-based (computing);web page;web search engine	P. Vijaya;Satish Chander	2017	Science China Information Sciences	10.1007/s11432-017-9343-5		Web+IR	-26.84989325326726	-55.14496644948429	134411
55c10ee7dd314f96e6143328ecbd4485da859fd5	the university of british columbia at tac 2008		In this paper we describe the University of British Columbia’s participation in the Text Analysis Conference 2008. This work represents our first submission to the DUC/TAC series of conferences, and we participated in both the summarization tasks: the main update task as well as the pilot task on summarizing blog opinions. We describe our systems in detail and describe our performance in the context of all submitted systems.	blog;columbia (supercomputer);dassault logiduc;email;responsiveness;sensor;unsupervised learning;upsampling	Gabriel Murray;Shafiq R. Joty;Giuseppe Carenini;Raymond T. Ng	2008			computer science;data science;world wide web	Robotics	-31.432878394634955	-63.99301819268202	134439
1d66b59c5dfcdeb1b44a31c09a4b90d98c1189e9	can we infer book classification by blurbs?		The aim of this work is to study the feasibility of an automated classification of books in the social network Zazie by means of the lexical analysis of book blurbs. A supervised learning approach is used to determine if a correlation between the characteristics of a book blurb and the emotional icons associated to the book by the Zazie’s users exists.	book;lexical analysis;library classification;social network;supervised learning	Valentina Poggioni;Valentina Franzoni;Fabiana Zollo	2014				NLP	-22.41289575068108	-58.76738330256751	134660
48aadadafa7122356815d43599ac098903076d00	twevent: segment-based event detection from tweets	tweet segmentation;event detection;drntu engineering computer science and engineering;conference paper;microblogging;twitter	Event detection from tweets is an important task to understand the current events/topics attracting a large number of common users. However, the unique characteristics of tweets (e.g. short and noisy content, diverse and fast changing topics, and large data volume) make event detection a challenging task. Most existing techniques proposed for well written documents (e.g. news articles) cannot be directly adopted. In this paper, we propose a segment-based event detection system for tweets, called Twevent. Twevent first detects bursty tweet segments as event segments and then clusters the event segments into events considering both their frequency distribution and content similarity. More specifically, each tweet is split into non-overlapping segments (i.e. phrases possibly refer to named entities or semantically meaningful information units). The bursty segments are identified within a fixed time window based on their frequency patterns, and each bursty segment is described by the set of tweets containing the segment published within that time window. The similarity between a pair of bursty segments is computed using their associated tweets. After clustering bursty segments into candidate events, Wikipedia is exploited to identify the realistic events and to derive the most newsworthy segments to describe the identified events. We evaluate Twevent and compare it with the state-of-the-art method using 4.3 million tweets published by Singapore-based users in June 2010. In our experiments, Twevent outperforms the state-of-the-art method by a large margin in terms of both precision and recall. More importantly, the events detected by Twevent can be easily interpreted with little background knowledge because of the newsworthy segments. We also show that Twevent is efficient and scalable, leading to a desirable solution for event detection from tweets.	cluster analysis;event segment;experiment;named entity;precision and recall;scalability;wikipedia	Chenliang Li;Aixin Sun;Anwitaman Datta	2012		10.1145/2396761.2396785	computer science;microblogging;data mining;internet privacy;world wide web;information retrieval	Web+IR	-24.20837757461668	-54.2072373634016	134666
4fd5e4a12becb963a1467be623818f9928f8bad8	exploring concept representations for concept drift detection		We present an approach to estimating concept drift in online news. Our method is to construct temporal concept vectors from topicannotated news articles, and to correlate the distance between the temporal concept vectors with edits to the Wikipedia entries of the concepts. We find improvements in the correlation when we split the news articles based on the amount of articles mentioning a concept, instead of calendar-based units of time.	concept drift;formal concept analysis;microsoft windows;wikipedia	Oliver Becher;Laura Hollink;Desmond Elliott	2017			concept drift;machine learning;unit of time;artificial intelligence;mathematics	ML	-23.968443128285386	-61.78562210895461	134760
a0af50580b566cc62041c81772730050f7b1e33a	domain lexicon-based query expansion for patent retrieval	semantics;patents;terminology;correlation;encyclopedias;knowledge based systems	Patent retrieval is important for technology survey and knowledge protection. Its aim is to search as many patent documents relevant to the patent document query as possible, which is considered as a recall-oriented task. However, existing methods suffer from the term mismatch problem caused by the frequent use of many non-standard technical terminologies in patents. To address the issue, we present a novel patent retrieval approach by utilizing domain lexicon-based query expansion. In particular, we present the domain lexicon construction scheme and the domain lexicon-based query expansion algorithm to augment the query with suitable expansion concepts. Experimental results on the CLEF-IP patent data set demonstrate that the proposed approach can achieve significant improvement in retrieval performance compared to several typical methods.	algorithm;jargon;knowledge base;lexicon;peer-to-patent;query expansion;software patent	Feng Wang;Lanfen Lin	2016	2016 12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)	10.1109/FSKD.2016.7603405	natural language processing;query optimization;query expansion;computer science;data mining;semantics;terminology;correlation;information retrieval;encyclopedia	AI	-30.64049647470682	-65.74596714175956	135051
2cb4329ba93d19482af9741cfdb0a7bcb472bd28	sparse lexical representation for semantic entity resolution	document handling;probability;lasso sparse signal recovery semantic entity resolution posterior probability pursuit;sparse lexical representation entity centric web information processing negative log posterior probability ppp posterior probability pursuit lasso algorithm salient lexical features sparse signal recovery web document knowledge base semantic entity resolution;semantic web;knowledge based systems semantics vectors measurement signal resolution encyclopedias;semantic web document handling knowledge based systems probability;knowledge based systems	This paper addresses the problem of semantic entity resolution (SER), which aims to determine whether some or none of the entities in a knowledge base is mentioned in a given web document. The lexical features, e.g., words and phrases, which are critical to the resolution of the semantic entities are typically of a small amount compared to all lexical features in the web document, and therefore can be modeled as sparse signals. Two techniques leveraging the principles of sparse signal recovery are proposed to identify the sparse, salient lexical features: one technique, based on the Lasso algorithm with the l2-norm distance metric, attempts to recover all the salient lexical features at once; the other technique, namely Posterior Probability Pursuit (PPP), sequentially identifies salient features one after one using the negative log posterior probability as the distance metric. Using a knowledge base consisting of about 100 million entities, we show that the proposed techniques exploiting the sparsity nature underlying SER deliver substantial performance improvement over baseline methods without sparsity consideration, demonstrating the potentials of sparse signal techniques in entity-centric web information processing.	algorithm;baseline (configuration management);detection theory;entity;information processing;knowledge base;lasso;sparse matrix;web page	Yuzhe Jin;Kuansan Wang;Emre Kiciman	2013	2013 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2013.6639339	speech recognition;computer science;knowledge-based systems;machine learning;semantic web;pattern recognition;probability;data mining;information retrieval;statistics	ML	-23.71681641076768	-62.69947496774193	135170
0835690b8aecf8d71a7e18fb0e65ca693683170d	epci: extracting potentially copyright infringement texts from the web	search engine;copy detection;web pages;information retrieval	In this paper, we propose a new system extracting potentially copyright infringement texts from the Web, called EPCI. EPCI extracts them in the following way: (1) generating a set of queries based on a given copyright reserved seed-text, (2) putting every query to search engine API, (3) gathering the search result Web pages from high ranking until the similarity between the given seed-text and the search result pages becomes less than a given threshold value, and (4) merging all the gathered pages, then re-ranking them in the order of their similarity. Our experimental result using 40 seed-texts shows that EPCI is able to extract 132 potentially copyright infringement Web pages per a given copyright reserved seed-text with 94% precision in average.	application programming interface;pci express;web page;web search engine;world wide web	Takashi Tashiro;Takanori Ueda;Taisuke Hori;Yu Hirate;Hayato Yamana	2007		10.1145/1242572.1242740	computer science;web page;data mining;database;world wide web;information retrieval;search engine	Web+IR	-29.51571603755096	-55.147978084761476	135172
91d53eb743c051a847160797d02da505200b9260	promoting ranking diversity for biomedical information retrieval using wikipedia	biomedical ir;information retrieval;ranking diversity;re ranking	Traditional Information Retrieval models assume that the relevance of a document is independent of the relevance of other documents. However, in reality, this assumption may not hold. The usefulness of retrieving a document usually depends on previous ranked documents, since a user may want to see the top ranked documents concerning different aspects of his/her information need instead of reading relevant documents that only deliver redundant information. In this talk, I will discuss how to find relevant documents that can deliver more different aspects of a query. In particular, I will discuss new models derived from the survival analysis theory for measuring aspect novelty. I will discuss how to use Wikipedia to detect aspects covered by retrieved documents. An aspect filter based on a two-stage model will be introduced and a new re-ranking method that combines the novelty and the relevance of a retrieved document at the aspect level will also be presented. Through extensive experiments on standard large-scale TREC biomedical collections, I will show that the proposed models and methods are effective in promoting ranking diversity for biomedical information retrieval.	document;experiment;information needs;information retrieval;relevance;text retrieval conference;wikipedia	Xiaoshi Yin;Xiangji Huang;Zhoujun Li	2010		10.1007/978-3-642-12275-0_43	ranking;computer science;data mining;world wide web;information retrieval	Web+IR	-26.706348048793796	-52.29686408365703	135313
97bf5339edf762ee9314d52365a9bd5535d57695	why we watch the news: a dataset for exploring sentiment in broadcast video news	person naming;video processing;audio processing;sentiment analysis;news video;multimodal processing	We present a multimodal sentiment study performed on a novel collection of videos mined from broadcast and cable television news programs. To the best of our knowledge, this is the first dataset released for studying sentiment in the domain of broadcast video news. We describe our algorithm for the processing and creation of person-specific segments from news video, yielding 929 sentence-length videos, and are annotated via Amazon Mechanical Turk. The spoken transcript and the video content itself are each annotated for their expression of positive, negative or neutral sentiment. Based on these gathered user annotations, we demonstrate for news video the importance of taking into account multimodal information for sentiment prediction, and in particular, challenging previous text-based approaches that rely solely on available transcripts. We show that as much as 21.54% of the sentiment annotations for transcripts differ from their respective sentiment annotations when the video clip itself is presented. We present audio and visual classification baselines over a three-way sentiment prediction of positive, negative and neutral, as well as person-dependent versus person-independent classification influence on performance. Finally, we release the News Rover Sentiment dataset to the greater research community.	algorithm;amazon mechanical turk;digital video;gene prediction;java annotation;mined;multimodal interaction;rover (the prisoner);text-based (computing);the turk;video clip	Joseph G. Ellis;Brendan Jou;Shih-Fu Chang	2014		10.1145/2663204.2663237	computer vision;audio signal processing;computer science;multimedia;video processing;internet privacy;world wide web;sentiment analysis	NLP	-21.00633176509954	-59.78970612465245	135518
b37ce8cf790ae5b2c1ba146eefe74c9c0e1e8c23	controversial topic discovery on members of congress with twitter	topic modeling;latent dirichlet allocation;social media mining;twitter;semantic extraction;polarizing topics	Abstract   This paper addresses how Twitter can be used for identifying conflict between communities of users. We aggregate documents by topic and by community and perform sentiment analysis, which allows us to analyze the overall opinion of each community about each topic. We rank the topics with opposing views (negative for one community and positive for the other). For illustration of the proposed methodology we chose a problem whose results can be evaluated using news articles. We look at tweets for republican and democrat congress members for the 112 th  House of Representatives from September to December 2013 and demonstrate that our approach is successful by comparing against articles in the news media.		Aleksey Panasyuk;Edmund Szu-Li Yu;Kishan G. Mehrotra	2014		10.1016/j.procs.2014.09.073	latent dirichlet allocation;computer science;machine learning;data mining;topic model;world wide web	ML	-23.058466666398907	-58.89563346794922	135613
1fcc930436f33f13cbb6e5286ca75d64bc023799	automatic wrapper induction from hidden-web sources with domain knowledge	information extraction;hidden web;form;invisible web;web service;supervised machine learning;domain knowledge;probing;deep web;wrapper induction;wrapper	We present an original approach to the automatic induction of wrappers for sources of the hidden Web that does not need any human supervision. Our approach only needs domain knowledge expressed as a set of concept names and concept instances. There are two parts in extracting valuable data from hidden-Web sources: understanding the structure of a given HTML form and relating its fields to concepts of the domain, and understanding how resulting records are represented in an HTML result page. For the former problem, we use a combination of heuristics and of probing with domain instances; for the latter, we use a supervised machine learning technique adapted to tree-like information on an automatic, imperfect, and imprecise, annotation using the domain knowledge. We show experiments that demonstrate the validity and potential of the approach.	deep web;experiment;form (html);html;heuristic (computer science);machine learning;supervised learning;wrapper (data mining)	Pierre Senellart;Avin Mittal;Daniel Muschick;Rémi Gilleron;Marc Tommasi	2008		10.1145/1458502.1458505	web service;computer science;machine learning;data mining;database;world wide web;information retrieval;domain knowledge;deep web	Web+IR	-29.03116619783218	-63.731691937685916	135672
1970dec73724a1f38491d872d084f35452b6e709	analysis on chinese quantitative stylistic features based on text mining	期刊论文	In this article, data mining was selected to examine whether some linguistic features, taking parts of speech (POS) for instance, can be used as Chinese quantitative stylistic feature. It can be also said that the purpose of this article is to explore the method to determine the Chinese quantitative stylistic features. Texts of different styles, which are news, science, official, art, TV conversation, and daily conversation styles, were selected to establish the corpus for our study. Text vectors characterized by POS were analyzed by principal component analysis and clustered by agglomerative hierarchical clustering method. The results of them indicate that POS can be used as a distinctive feature of texts. Then, support vector machine was adopted to establish classification model on training data and precision and recall rates to validate the results of text classification. Random forest was selected to compute the importance of POS, i.e. the contribution to classification, and text vectors characterized by important POS were clustered and classified consequently. The results of the experiments show that POS can be taken as Chinese quantitative stylistic feature, and the results of clustering and classification are preferably taking the 60 most important POS as the character of texts.	text mining	Renkui Hou;Minghu Jiang	2016	DSH	10.1093/llc/fqu067	natural language processing;speech recognition;computer science;pattern recognition	NLP	-23.235755663991423	-65.94639587009029	135750
2b41f545c7e5dcb1ccf556dbd3775cb468084744	data extraction and annotation for dynamic web pages	semantic annotation;semantic web web sites information retrieval;web pages;information retrieval;data extraction;data mining web pages humans html computer science information systems internet writing databases graphical user interfaces;web sites;semantic web;experimental evaluation;information system;wrapper generation data extraction dynamic web pages web sites structured data semantic annotation template;structured data	Many Web sites contain large sets of pages generated dynamically using a common template. The structured data extracted from these pages with semantic annotation are valuable for information system. We proposed a system, ADeaD, to automatically extract data values from these Web pages and annotate the data schema. Experimental evaluation on a lot of real Web page collections indicates our algorithm correctly extracted data and annotated the data schema.	dynamic web page	Hui Song;Suraj Giri;Fanyuan Ma	2004		10.1109/EEE.2004.1287353	web service;web mining;static web page;web development;web modeling;data web;web mapping;html;image retrieval;web standards;computer science;semantic web;web navigation;social semantic web;web page;semantic web stack;database;web intelligence;web 2.0;world wide web;website parse template;information retrieval;web server	NLP	-29.76220929046092	-54.39711435695948	135751
be1439273cfc48a6733726c38cf88035808c9484	microsearch: an interface for semantic search	metadata extraction;semantic search	In this paper we discuss the potential for semantic search and focus on the most immediate problem toward its realization: the problem of the sparsity and relatively low quality of embedded metadata. We suggest that a part of the solution is to expose users to embedded metadata as part of their daily activity of searching the Web. We present the publicly available microsearch system which enriches search result presentation with metadata extracted from search results and report on some of the early feedback we have received.	embedded system;semantic search;sparse matrix;world wide web	Peter Mika	2008			semantic search;semantic grid;computer science;database;world wide web;information retrieval;metadata repository	Web+IR	-31.273606059696082	-53.062509360817124	135757
0755cf8c50a190886a2f001086bc01b964a5810e	btu dbis' personal photo retrieval runs at imageclef 2013		This paper summarizes the results of the BTU DBIS research group’s participation in the Personal Photo Retrieval subtask of ImageCLEF 2013. In order to solve the subtask, a self-developed multimodal multimedia retrieval system, PythiaSearch, is used. The discussed retrieval approaches focus on two different strategies. First, two automatic approaches that combine visual features and meta data are examined. Second, a manually assisted relevance feedback approach is presented. All approaches are based on a special query language, CQQL, which supports the logical combination of different features. Considering only automatic runs without relevance feedback that have been submitted to the subtask, DBIS reached the best overall results, while the relevance feedback-assisted approach is placed second amongst all participants of the subtask.	experiment;iteration;multimodal interaction;query language;radio frequency;relevance feedback	Thomas Böttcher;David Zellhöfer;Ingo Schmitt	2013			information retrieval;metadata;query language;computer science;relevance feedback	Web+IR	-31.43010860675299	-62.67133233927471	135871
b27fd07b9a9730ccfe6051191ccbf24ceadfdd65	course opinion mining methodology for knowledge discovery, based on web social media	e learning systems;sentiment analysis;course performance evaluation;data mining algorithms;content mining	Authors propose a new educational course evaluation framework and architecture for automatic (course evaluation intelligent mining) knowledge discovery at the web social media and search meta-engines.  The proposed architecture shall provide an additional educational tool for an institution's course evaluation process, apart from the already proposed by authors educational course evaluation context mining metrics and algorithms. That is, a course sentiment analysis process which extracts an opinion regarding an academic course from web user generated data.	algorithm;sentiment analysis;social media	Sotirios Kontogiannis;Stavros Valsamidis;Ioannis Kazanidis;Alexandros Karakos	2014		10.1145/2645791.2645827	web mining;computer science;data science;data mining;database;world wide web;sentiment analysis	ML	-23.929350236902362	-57.79802867932776	135955
5d1c164b6c3b294a786227099700d8ed13568d34	isis in the eyes of the dutch		The presence of militant group Islamic State of Iraq and Syria (ISIS) is growing. Terrorist attacks in Europe and an incoming stream of refugees in the south of the continent are some of the reasons Europe is getting socially involved in the Middle-Eastern war. It might seem that the Netherlands could become a target of the organization too. But are Dutch citizens concerned about this? In this paper, we describe the reaction of the Dutch on ISIS by analyzing what they say on Twitter about the organization. With the use of text classification, topic modeling and visualization tools, we were able to retrieve Tweets about ISIS and create a network graph displaying the ten main topics about ISIS which Dutch people tweeted about, in addition to a word cloud, displaying the words which were most used in the Tweets. The visualizations are used to analyze what topics people discussed most regarding ISIS on Twitter and how they felt about these topics. Understanding the social network responses to ISIS’ attacks can give us some clues to understand its impact on the society.	document classification;isis;social network;tag cloud;topic model	Bas Hendrikse;Mena B. Habib;Maurice van Keulen	2017			geography;genealogy;advertising;cartography	Web+IR	-22.486047364128904	-54.74858101709494	136008
0b60aacb1d5e7776d8337a139bab4ac3d7788a20	i see a car crash: real-time detection of small scale incidents in microblogs	004 informatik	Microblogs are increasingly gaining attention as an important information source in emergency management. Nevertheless, it is still difficult to reuse this information source during emergency situations, because of the sheer amount of unstructured data. Especially for detecting small scale events like car crashes, there are only small bits of information, thus complicating the detection of relevant information. We present a solution for a real-time identification of small scale incidents using microblogs, thereby allowing to increase the situational awareness by harvesting additional information about incidents. Our approach is a machine learning algorithm combining text classification and semantic enrichment of microblogs. An evaluation based shows that our solution enables the identification of small scale incidents with an accuracy of 89% as well as the detection of all incidents published in real-time Linked Open Government Data.	algorithm;document classification;electromagnetically induced transparency;emergent;gene ontology term enrichment;information source;machine learning;natural language processing;real-time clock;real-time transcription;semantic web;sensor	Axel Schulz;Petar Ristoski;Heiko Paulheim	2013		10.1007/978-3-642-41242-4_3	internet privacy;computer security	NLP	-22.489789292841888	-55.12808959033434	136022
5d611e1ff18e08c9425141b7b1ffaeef432117c0	visualization and analytical support of questionnaire free-texts data based on hk graph with concepts of words	graph theory;environmental factors;concept;text analysis data mining data visualisation graph theory;concept text mining hk graph visualization questionnaire analysis;article author;text mining;visualization method;hierarchical keyword graph;aggregating words method;text analysis;aggregating words method visualization method questionnaire free text data analysis hk graph concept of words hierarchical keyword graph text mining;concept of words;questionnaire free text data analysis;earthquakes;data mining;thesauri;data visualisation;visualization;feature extraction;data visualization;hk graph;questionnaire analysis;graph visualization;earthquakes thesauri text mining data visualization feature extraction environmental factors;environmental factor	A lot of companies carry out questionnaires. These questionnaires often have questions which need respondents to answer by free description. It is, however, inefficient for an analyzer to read whole texts to get outlines or classify them, or it is difficult to correctly analyze them without subjective biases. The authors have proposed “HK Graph” (Hierarchical Keyword Graph) which is a support tool for text mining. HK Graph can visualize the relationships among attributes and words with hierarchical graph structure based on frequency of co-occurrence. However, the result of HK Graph is not enough helpful for the analyzer to grasp the outlines of the texts and extract opinions from them, because it regards divided words as different ones unless they perfectly match and that makes the visualized result complicated. This paper presents a new visualization method for the HK Graph incorporating an aggregating words method based on concepts of words. An experiment is carried out by applying the proposed method to actual questionnaire data on disasters and studies the effectiveness of the proposed method.	experiment;n-gram;text mining;thesaurus	Daisuke Kobayashi;Tomohiro Yoshikawa;Takeshi Furuhashi	2011	2011 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE 2011)	10.1109/FUZZY.2011.6007556	text mining;visualization;feature extraction;computer science;data science;machine learning;data mining;graph drawing;concept;information retrieval;data visualization	Robotics	-24.640486821559804	-57.819280212043715	136050
4a59a73d2bc85b793463bba899d45e5a894223ec	a comprehensive human computation framework: with application to image labeling	image labeling;common sense problems;humansense;distributed knowledge acquisition;large scale;knowledge acquisition;human computation;binding problem;common sense;multiple choice;video search	"""Image and video labeling is important for computers to understand images and videos and for image and video search. Manual labeling is tedious and costly. Automatically image and video labeling is yet a dream. In this paper, we adopt a Web 2.0 approach to labeling images and videos efficiently: Internet users around the world are mobilized to apply their """"common sense"""" to solve problems that are hard for today's computers, such as labeling images and videos. We first propose a general human computation framework that binds problem providers, Web sites, and Internet users together to solve large-scale common sense problems efficiently and economically. The framework addresses the technical challenges such as preventing a malicious party from attacking others, removing answers from bots, and distilling human answers to produce high-quality solutions to the problems. The framework is then applied to labeling images. Three incremental refinement stages are applied. The first stage collects candidate labels of objects in an image. The second stage refines the candidate labels using multiple choices. Synonymic labels are also correlated in this stage. To prevent bots and lazy humans from selecting all the choices, trap labels are generated automatically and intermixed with the candidate labels. Semantic distance is used to ensure that the selected trap labels would be different enough from the candidate labels so that no human users would mistakenly select the trap labels. The last stage is to ask users to locate an object given a label from a segmented image. The experimental results are also reported in this paper. They indicate that our proposed schemes can successfully remove spurious answers from bots and distill human answers to produce high-quality image labels."""	computer;ecosystem;email;human-based computation;internet;lazy evaluation;list of code lyoko episodes;malware;money;music download;refinement (computing);web 2.0	Yang Yang;Bin B. Zhu;Rui Guo;Linjun Yang;Shipeng Li;Nenghai Yu	2008		10.1145/1459359.1459423	multiple choice;binding problem;computer science;artificial intelligence;machine learning;data mining;world wide web	Vision	-29.183533923081097	-55.54115785458186	136061
b8e933f6c24c74925c85d41992aedbd4944a4d62	efficient multi-account detection on ugc sites	computers;classification multi account detection security user generated content writing style;data mining;classification;text analysis feature extraction pattern classification security of data social networking online;feature extraction;google ugc sites multiaccount user detection user generated content sites writing style based multiaccount detection one class classification based approach multiaccount behavior detection mutual similarity measurement detection precision bigram extraction trigram extraction part of speech extraction grammatical relations osn twitter facebook;writing;writing style;correlation;user generated content;security;multi account detection;writing feature extraction correlation user generated content privacy data mining computers;privacy	This work presents a novel writing style-based approach to detect multi-account users on User-Generated Content (UGC) sites. Unlike existing works which emphasize feasibility and privacy leakage, we focus on precise writing style-based multi-account detection. Specifically, we leverage a one-class classification-based approach to detect multi-account behaviors, in which a mutual similarity measurement is defined to increase detection precision. In addition to traditional features used in writing style detection, we also extract bigrams, trigrams, part-of-speech, and grammatical relations. We evaluate our methodology based on datasets crawled from 3 popular OSNs (i.e., Twitter, Facebook, and Google+). Experimental results demonstrate that compared with the most recent achievements, our method not only improves the average detection precision to almost 90%, but also increases both recall and F-measure to 90% and even better.	bigram;f1 score;google+;one-class classification;part-of-speech tagging;spectral leakage;trigram;user-generated content	Xucheng Luo;Fan Zhou;Mengjuan Liu;Yajun Liu;Chunjing Xiao	2016	2016 IEEE Symposium on Computers and Communication (ISCC)	10.1109/ISCC.2016.7543780	feature extraction;biological classification;computer science;information security;writing style;data mining;internet privacy;user-generated content;privacy;writing;world wide web;computer security;correlation	SE	-20.77099276076055	-58.26009073404053	136173
e33d0a66f5fdd8c150961181913b79b4d7115894	classification and summarization of pros and cons for customer reviews	opinion mining;electronic commerce;supervised learning;e commerce;sentence weight;testing;classification;summarization;customer review;intelligent agent mutual information electronic commerce manufacturing conferences design methodology web page design buildings testing supervised learning;web page design;manufacturing;intelligent agent;mutual information;sentence weight opinion mining classification summarization customer review;buildings;conferences;design methodology	As e-commerce is becoming more and more popular, the number of customer reviews for online products grows rapidly. For a popular product, there can be hundreds of reviews. This makes it difficult for a potential customer to read all of them in order to get as much information as possible and to make a decision on purchasing. Therefore, a summarization of product reviews would make purchase more convenient and reliable. The conventional way of summarizing a review is to select or rewrite a subset of the original sentences from the review, which is inefficient. In this paper, we propose to summarize all customers’ reviews of a product as a list of phrases named pros and cons list, which can be perceived and understood at a glance. We employ a score algorithm which considers the strength of a word towards positive or negative orientation to calculate and weigh the sentiment of a sentence. To assess our algorithm, a number of existing classifiers are also presented. Our experimental results show that our Sentence Weight classifier is more accurate and effective than those compared.	algorithm;e-commerce;human body weight;purchasing;rewrite (programming);select (sql)	Xinghua Hu;Bin Wu	2009	2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology	10.1109/WI-IAT.2009.234	e-commerce;design methods;biological classification;computer science;artificial intelligence;data science;machine learning;data mining;software testing;manufacturing;mutual information;world wide web	AI	-23.31389859435273	-57.79746090742566	136206
d56c4a01a412b2a9972a49c14baecd52422db328	mining evolutionary event patterns of web texts for text resource aggregation	information resources;text mining;text analysis information resources internet;text analysis;research method;internet;evolutionary event pattern mining;internet evolutionary event pattern mining web texts text resource aggregation;data mining internet mathematics text mining information retrieval resource management fuzzy systems hidden markov models;web texts;text resource aggregation	As an outcome of the overwhelming volume of Internet resources, it is necessary to research methods integrating distributed autonomous resources connected to the Internet to make them more effective and efficient. With the understanding of characteristics of internet resources, this paper will focus on solving the problem of text resource aggregation in open environment and its emergence showed during aggregation over time. We process these text resources both in space and time dimension through viewing them as an event stream evolving over time, and attempt to discover the evolutionary event patterns, furthermore, to mine the emergence of text content, such as revealing research trends in scientific literature. Experiments show that our methods can discover interesting evolutionary event patterns effectively.	autonomous robot;emergence;experiment;internet;scientific literature	Chengli Zhao;Dongyun Yi	2007	Fourth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD 2007)	10.1109/FSKD.2007.399	text mining;the internet;computer science;data science;data mining;world wide web	DB	-26.02652244628026	-57.07980864321478	136346
a1fbab6cc4d54d8ed4a98c663077d0f411ce420d	increasing the explanatory power of investor sentiment analysis for commodities in online media.		Online media are an important source for investor sentiment on commodities. Although there is empirical evidence for a relationship between investor sentiment from news and commodity returns, the impact of classifier design on the explanatory power of sentiment for returns has received little attention. We evaluate the explanatory power of nine classifier designs and find that (1) a positive relationship holds between more opinionated online media sentiment and commodity returns, (2) weighting dictionary terms by machine learning increases explanatory power by up to 25%, and (3) the commonly used dictionary of Loughran and McDonald is detrimental for commodity sentiment analysis.		Achim Klein;Martin Riekert;Lyubomir Kirilov;Jörg Leukel	2018		10.1007/978-3-319-93931-5_23	marketing;digital media;sentiment analysis;commodity;classifier (linguistics);computer science;empirical evidence;explanatory power;weighting	NLP	-19.489003534868733	-52.50491902150201	136378
37b682e3cc8de622cbbe67a13a8a7150201a7d34	automatic text categorization by unsupervised learning	sentence similarity measure;certain number;traditional supervised learning method;large number;training document;automatic text categorization;text categorization;supervised learning;low-cost text categorization;unsupervised learning method;unsupervised learning	The goal of text categorization is to classify documents into a certain number of predefined categories. The previous works in this area have used a large number of labeled training doculnents for supervised learning. One problem is that it is difficult to create the labeled training documents. While it is easy to collect the unlabeled documents, it is not so easy to manually categorize them for creating traiuing documents. In this paper, we propose an unsupervised learning method to overcome these difficulties. The proposed lnethod divides the documents into sentences, and categorizes each sentence using keyword lists of each category and sentence simihuity measure. And then, it uses the categorized sentences for refining. The proposed method shows a similar degree of performance, compared with the traditional supervised learning inethods. Therefore, this method can be used in areas where low-cost text categorization is needed. It also can be used for creating training documents.	categorization;document classification;supervised learning;unsupervised learning;word sense;word-sense disambiguation	Youngjoong Ko;Jungyun Seo	2000			semi-supervised learning;unsupervised learning;computer science;machine learning;pattern recognition;supervised learning;information retrieval;categorization	AI	-24.2536762180023	-65.44052059832222	136518
cbf2dfe9bb070473108a69fc58e5969bfc1e4064	learning summary content units with topic modeling	text span;candidate scus;probabilistic topic modeling approach;topic modeling approach;important approach;model summary;pyramid method;topic model;human model summary;summary content unit;semantically similar text span	In the field of multi-document summarization, the Pyramid method has become an important approach for evaluating machine-generated summaries. The method is based on the manual annotation of text spans with the same meaning in a set of human model summaries. In this paper, we present an unsupervised, probabilistic topic modeling approach for automatically identifying such semantically similar text spans. Our approach reveals some of the structure of model summaries and identifies topics that are good approximations of the Summary Content Units (SCU) used in the Pyramid method. Our results show that the topic model identifies topic-sentence associations that correspond to the contributors of SCUs, suggesting that the topic modeling approach can generate a viable set of candidate SCUs for facilitating the creation of Pyramids.	approximation;automatic summarization;bag-of-words model;latent variable;multi-document summarization;precision and recall;pyramid (geometry);pyramid (image processing);single compilation unit;statistical model;topic model;unsupervised learning	Leonhard Hennig;Ernesto William De Luca;Sahin Albayrak	2010			natural language processing;computer science;data science;data mining;information retrieval	NLP	-26.880344175990082	-64.14974907007166	136523
0121c8848da1e7f630bbfb9a77ab5d41c024c926	tweetin' in the rain: exploring societal-scale effects of weather on mood	sentiment;twitter;prediction	There has been significant recent interest in using the aggregate sentiment from social media sites to understand and predict real-world phenomena. However, the data from social media sites also offers a unique and—so far—unexplored opportunity to study the impact of external factors on aggregate sentiment, at the scale of a society. Using a Twitterspecific sentiment extraction methodology, we the explore patterns of sentiment present in a corpus of over 1.5 billion tweets. We focus primarily on the effect of the weather and time on aggregate sentiment, evaluating how clearly the wellknown individual patterns translate into population-wide patterns. Using machine learning techniques on the Twitter corpus correlated with the weather at the time and location of the tweets, we find that aggregate sentiment follows distinct climate, temporal, and seasonal patterns.	aggregate data;algorithm;amazon web services;ibm notes;machine learning;social media;whole earth 'lectronic link	Aniko Hannak;Eric Anderson;Lisa Feldman Barrett;Sune Lehmann;Alan Mislove;Mirek Riedewald	2012			prediction;data mining;world wide web	HCI	-22.937838211539773	-54.17200652147843	136639
d696ae1dfdd35e8ed203525b6f965abda4144836	blurb mining: discovering interesting excerpts from e-commerce product reviews		Product reviews on modern e-commerce websites have evolved into repositories of valuable firsthand opinions on products. Showcasing the opinions that reviewers express on a product in a succinct way can not only promote the product, but also provide an engaging experience that simplifies the shopping journey for online shoppers. In the case of traditional media such as movies and books, employingblurbs or excerpts from critic reviews for promotional purposes is an established practice among movie publicists and book editors that has proven to be an effective way of capturing attention of customers. Such excerpts can be discovered from e-commerce product reviews to highlight interesting reviewer opinions and add emotive elements to otherwise bland e-commerce product pages. While traditional movie or book blurbs are manually extracted, they must be automatically extracted from e-commerce product reviews owing to the scale of catalogues. Further, traditional blurbs are generally phrased to be very positive in tone and sometimes may take some words out of context. However, excerpts for e-commerce products must represent the true opinions of the reviewers and must capture the context in which the words were used to retain trust of users. To that end, we introduce the problem of extracting engaging excerpts from e-commerce product reviews in this paper. We present methods to automatically discover such excerpts from reviews at scale by leveraging natural language properties such as syntactic structure of sentences and sentiment, and discuss some of the underlying challenges. We further present an evaluation of the effectiveness of the proposed methods in terms of the quality of the blurbs generated and their ranking orders produced.	e-commerce payment system	Saratchandra Indrakanti;Gyanit Singh;Justin House	2018		10.1145/3184558.3191626	multimedia;e-commerce;natural language;emotive;syntax;sentiment analysis;computer science;ranking	ML	-23.06109413597156	-57.174395998836474	136778
903772853e70cd52f6e74133660221849142eb58	generating aspect-oriented multi-document summarization with event-aspect model	compression algorithm;automatic group;multi document summarization;automatic generation;random walk;aspect oriented;information system;semantic relations;integer linear program;quantitative evaluation	In this paper, we propose a novel approach to automatic generation of aspect-oriented summaries from multiple documents. We first develop an event-aspect LDA model to cluster sentences into aspects. We then use extended LexRank algorithm to rank the sentences in each cluster. We use Integer Linear Programming for sentence selection. Key features of our method include automatic grouping of semantically related sentences and sentence ranking based on extension of random walk model. Also, we implement a new sentence compression algorithm which use dependency tree instead of parser tree. We compare our method with four baseline methods. Quantitative evaluation based on Rouge metric demonstrates the effectiveness and advantages of our method.	algorithm;aspect-oriented software development;automatic summarization;baseline (configuration management);cluster analysis;data compression;integer programming;linear programming;local-density approximation;multi-document summarization;rouge (metric);test data;tree (data structure);xslt/muenchian grouping	Peng Li;Yinglin Wang;Wei Gao;Jing Jiang	2011			data compression;natural language processing;aspect-oriented programming;multi-document summarization;computer science;machine learning;pattern recognition;data mining;random walk;information system;statistics	NLP	-26.324264404695707	-64.20248826771987	136873
296913eac3ac87453692ee1d2318f06f1179fe78	detecting hate, offensive, and regular speech in short comments		The freedom of expression provided by the Internet also favors malicious groups that propagate contents of hate, recruit new members, and threaten users. In this context, we propose a new approach for hate speech identification based on Information Theory quantifiers (entropy and divergence) to represent documents. As a differential of our approach, we capture weighted information of words, rather than just their frequency in documents. The results show that our approach overperforms techniques that use data representation, such as TF-IDF and unigrams combined to text classifiers, achieving an F1-score of 86%, 84% e 96% for classifying hate, offensive, and regular speech classes, respectively. Compared to the baselines, our proposal is a win-win solution that improves efficacy (F1-score) and efficiency (by reducing the dimension of the feature vector). The proposed solution is up to 2.27 times faster than the baseline.	baseline (configuration management);data (computing);f1 score;feature vector;information theory;internet;malware;sensor;tf–idf	Thais G. Almeida;Bruno A. Souza;Fabíola Guerra Nakamura;Eduardo Freire Nakamura	2017		10.1145/3126858.3131576	the internet;information theory;supervised learning;baseline (configuration management);offensive;feature vector;external data representation;artificial intelligence;computer science;machine learning	NLP	-19.617937057267905	-61.374617300127866	136937
c2804b350138efa18baeb841f172dadaa8449157	requirements for query evaluation in weighted information retrieval	information retrieval;evaluation methods;algebra;mathematical models;query evaluation;algorithms;search strategies;relevance information retrieval	In this article, a general mathematical framework for information retrieval models is presented, giving more insight into the evaluation mechanisms that may be used in IR. In this framework a number of requirements for operators improving recall and precision are investigated. It is shown that these requirements can be satisfied by using descriptor weights and one combination operator only. Moreover, information items and queries—virtual items—can be treated in exactly the same way, reflecting the fact, that they both are descriptions of a number of concepts. Evaluation is performed by formulating similarity homomorphisms from query descriptions to retrieval status values that allow ranking items accordingly. For good comparisons, however, ranking must be done by the achieved proportion of perfect similarity rather than by similarity itself. In term independence models, this normalization process may satisfy the homomorphism requirement of algebra under certain conditions, but it contradicts the requirement in the term dependence case. Nevertheless, by demanding separability for the unnormalized part of the evaluation measure, it can be guaranteed that the query is evaluated to each item descriptor by descriptor, and the normalization value must be calculated only once for the whole query, using the query as a whole. Also, for each real item, the normalization value must have been calculated only once, using the item as a whole.	information retrieval	Martin Bärtschi	1985	Inf. Process. Manage.	10.1016/0306-4573(85)90054-8	query optimization;query expansion;ranking;relevance;computer science;theoretical computer science;evaluation;machine learning;mathematical model;data mining;mathematics;information retrieval	DB	-32.55933685109366	-60.42668006863091	137040
e46631daf0bda873304be4b39b6172139179b161	detecting and tracking the spread of astroturf memes in microblog streams	politics;twitter;memes;social media;classification;information diffusion;mi- croblogs;truthy;real time;feature extraction;public relations;web service;social interaction;supervised learning;sentiment analysis	Online social media are complementing and in some cases replacing person-to-person social interaction and redefining the diffusion of information. In particular, microblogs have become crucial grounds on which public relations, marketing, and political battles are fought. We introduce an extensible framework that will enable the real-time analysis of meme diffusion in social media by mining, visualizing, mapping, classifying, and modeling massive streams of public microblogging events. We describe a Web service that leverages this framework to track political memes in Twitter and help detect astroturfing, smear campaigns, and other misinformation in the context of U.S. political elections. We present some cases of abusive behaviors uncovered by our service. Finally, we discuss promising preliminary results on the detection of suspicious memes via supervised learning based on features extracted from the topology of the diffusion networks, sentiment analysis, and crowdsourced annotations.	crowdsourcing;definition;meme;real-time web;sensor;sentiment analysis;smear campaign;social media;supervised learning;web service	Jacob Ratkiewicz;Anchana Chanomethaporn;Mark R. Meiss;Bruno Gonçalves;Snehal Patil;Alessandro Flammini;Filippo Menczer	2010	CoRR			Web+IR	-22.42369880734745	-53.98052742206643	137052
3239d3fe4fd82e29723cdd1aa5b0f93b8bae79cc	discovering user interactions in ideological discussions		Online discussion forums are a popular platform for people to voice their opinions on any subject matter and to discuss or debate any issue of interest. In forums where users discuss social, political, or religious issues, there are often heated debates among users or participants. Existing research has studied mining of user stances or camps on certain issues, opposing perspectives, and contention points. In this paper, we focus on identifying the nature of interactions among user pairs. The central questions are: How does each pair of users interact with each other? Does the pair of users mostly agree or disagree? What is the lexicon that people often use to express agreement and disagreement? We present a topic model based approach to answer these questions. Since agreement and disagreement expressions are usually multiword phrases, we propose to employ a ranking method to identify highly relevant phrases prior to topic modeling. After modeling, we use the modeling results to classify the nature of interaction of each user pair. Our evaluation results using real-life discussion/debate posts demonstrate the effectiveness of the proposed techniques.	behavioral modeling;experiment;flaming (internet);generative model;interaction;lexicon;n-gram;real life;relevance;semiconductor industry;subject matter expert turing test;topic model	Arjun Mukherjee;Bing Liu	2013			data mining;world wide web	NLP	-22.491647252724075	-59.44898325511628	137118
52063894260602edd86abda1f0ce5e719340d59e	indexing of textual databases based on lexical resources: a case study for serbian		In this paper we describe an approach to improvement of information retrieval results for large textual databases by pre-indexing documents using bag-of-words and named entity recognition. The approach was applied on a database of geological projects financed by the Republic of Serbia for several decades now. Each document within this database is described by a summary report, consisting of metadata on the geological project, such as title, domain, keywords, abstract, and geographical location. A bag of words was produced from these metadata with the help of morphological dictionaries and transducers, while named entities were recognized using a rule-based system. Both were then used for pre-indexing documents for information retrieval purposes where ranking of retrieved documents was based on several $$tf\_idf$$ based measures. Evaluation of ranked retrieval results based on data obtained by pre-indexing were compared to results obtained by informational retrieval without pre-indexing with precision-recall curve, showing a significant improvement in terms of the mean average precision measure.		Ranka Stankovic;Cvetana Krstev;Ivan Obradovic;Olivera Kitanovic	2015		10.1007/978-3-319-27932-9_15	computer science;data mining;database;information retrieval	NLP	-32.722555254705355	-64.90559168249851	137259
7e0e6e0cf37ce4f4dde8c940ee4ce0fdb7b28656	sentiment analysis and summarization of twitter data	opinion mining;generators;support vector machines;feature extraction twitter accuracy dictionaries support vector machines classification algorithms generators;sentiment summarization;opinion mining sentiment analysis sentiment summarization;accuracy;internet;feature extraction;dictionaries;classification algorithms;social networking online;sentiment analysis;data handling;twitter;hybrid polarity detection system sentiment analysis twitter data product based sentiment summarization multidocuments informal texts sa polarity detection;social networking online data handling internet	"""Sentiment Analysis (SA) and summarization has recently become the focus of many researchers, because analysis of online text is beneficial and demanded in many different applications. One such application is product-based sentiment summarization of multi-documents with the purpose of informing users about pros and cons of various products. This paper introduces a novel solution to target-oriented (i.e. aspect-based) sentiment summarization and SA of short informal texts with a main focus on Twitter posts known as """"tweets"""". We compare different algorithms and methods for SA polarity detection and sentiment summarization. We show that our hybrid polarity detection system not only outperforms the unigram state-of-the-art baseline, but also could be an advantage over other methods when used as a part of a sentiment summarization system. Additionally, we illustrate that our SA and summarization system exhibits a high performance with various useful functionalities and features."""	algorithm;automatic summarization;baseline (configuration management);multi-document summarization;n-gram;sentiment analysis	Seyed Ali Bahrainian;Andreas Dengel	2013	2013 IEEE 16th International Conference on Computational Science and Engineering	10.1109/CSE.2013.44	support vector machine;the internet;multi-document summarization;feature extraction;computer science;automatic summarization;machine learning;group method of data handling;data mining;accuracy and precision;world wide web;information retrieval;sentiment analysis	NLP	-21.468963407713154	-61.11268114899673	137377
0059031eca3db96ddc53a0b67324933f2943fda0	evaluation of automatic text summarization methods based on rhetorical structure theory	atmospheric measurements;computational linguistics text summarization rhetorical structure theory natural language processing;particle measurements;automatic text summarization;text analysis;text summarization;extractive summarizer automatic text summarization rhetorical structure theory;distance measurement;extractive summarizer;hybrid method;rhetorical structure theory;computational linguistic;satellites;intelligent systems intelligent structures performance evaluation computational linguistics classification tree analysis natural languages area measurement statistics;text analysis computational linguistics;area measurement;humans;computational linguistics;natural language processing	Motivated by governmental, commercial and academic interests, automatic text summarization area has experienced an increasing number of researches and products, which led to a countless number of summarization methods. In this paper, we present a comprehensive comparative evaluation of the main automatic text summarization methods based on rhetorical structure theory (RST), claimed to be among the best ones. We also propose new methods and compare our results to an extractive summarizer, which belongs to a summarization paradigm with severe limitations. To the best of our knowledge, most of our results are new in the area and reveal very interesting conclusions. The simplest RST-based method is among the best ones, although all of them present comparable results. We show that all RST-based methods overcome the extractive summarizer and that hybrid methods produce worse summaries. finally, we verify that Mann and Thompson strong assumption in summarization and RST research area is not helpful in the way previously imagined.	automatic summarization;intel matrix raid;programming paradigm	Vinícius Rodrigues Uzêda;Thiago Alexandre Salgueiro Pardo;Maria das Graças Volpe Nunes	2008	2008 Eighth International Conference on Intelligent Systems Design and Applications	10.1109/ISDA.2008.289	natural language processing;speech recognition;multi-document summarization;computer science;computational linguistics;automatic summarization;information retrieval;satellite	DB	-22.77774679499424	-64.51190568839371	137518
2b7523f6cecdb970b5d0a17df48029306735fb1e	evaluation of novelty metrics for sentence-level novelty mining	performance evaluation;information retrieval;real time;prior information;novelty detection;novelty mining;novelty metric;knowledge base	This work addresses the problem of detecting novel sentences from an incoming stream of text data, by studying the performance of different novelty metrics, and proposing a mixed metric that is able to adapt to different performance requirements. Existing novelty metrics can be divided into two types, symmetric and asymmetric, based on whether the ordering of sentences is taken into account. After a comparative study of several different novelty metrics, we observe complementary behavior in the two types of metrics. This finding motivates a new framework of novelty measurement, i.e. the mixture of both symmetric and asymmetric metrics. This new framework of novelty measurement performs superiorly under different performance requirements varying from high-precision to high-recall as well as for data with different percentages of novel sentences. Because it does not require any prior information, the new metric is very suitable for real-time knowledge base applications such as novelty mining systems where no training data is available beforehand.		Flora S. Tsai;Wenyin Tang;Kap Luk Chan	2010	Inf. Sci.	10.1016/j.ins.2010.02.020	knowledge base;computer science;machine learning;pattern recognition;data mining	DB	-21.106572825573057	-62.96145927027863	137673
cb6e007164f806d559da782181be59425b988ec3	content-based retrieval for heterogeneous domains: domain adaptation by relative aggregation points	domain adaptation;spoken document retrieval;feature space;average cost;adaptive method;content based retrieval;test collection;image retrieval	We introduce the problem of domain adaptation for content-based retrieval and propose a domain adaptation method based on relative aggregation points (RAPs). Content-based retrieval including image retrieval and spoken document retrieval enables a user to input examples as a query, and retrieves relevant data based on the similarity to the examples. However, input examples and relevant data can be dissimilar, especially when domains from which the user selects examples and from which the system retrieves data are different. In content-based geographic object retrieval, for example, suppose that a user who lives in Beijing visits Kyoto, Japan, and wants to search for relatively inexpensive restaurants serving popular local dishes by means of a content-based retrieval system. Since such restaurants in Beijing and Kyoto are dissimilar due to the difference in the average cost and areas' popular dishes, it is difficult to find relevant restaurants in Kyoto based on examples selected in Beijing. We propose a solution for this problem by assuming that RAPs in different domains correspond, which may be dissimilar but play the same role. A RAP is defined as the expectation of instances in a domain that are classified into a certain class, e.g. the most expensive restaurant, average restaurant, and restaurant serving the most popular dishes. Our proposed method constructs a new feature space based on RAPs estimated in each domain and bridges the domain difference for improving content-based retrieval in heterogeneous domains. To verify the effectiveness of our proposed method, we evaluated various methods with a test collection developed for content-based geographic object retrieval. Experimental results show that our proposed method achieved significant improvements over baseline methods. Moreover, we observed that the search performance of content-based retrieval in heterogeneous domains was significantly lower than that in homogeneous domains. This finding suggests that relevant data for the same search intent depend on the search context, that is, the location where the user searches and the domain from which the system retrieves data.	baseline (configuration management);document retrieval;domain adaptation;feature vector;image retrieval;rapid refresh	Makoto P. Kato;Hiroaki Ohshima;Katsumi Tanaka	2012		10.1145/2348283.2348392	visual word;feature vector;image retrieval;computer science;machine learning;data mining;world wide web;information retrieval	Web+IR	-28.211231495653536	-52.81309605580928	137722
4fd0fda073554ca2f568bf63f0465a86a9d3cb4a	research on text-reducing method based on the improved knn algorithm	similarity degree;text reducing;text reducing method;training;sparse matrices independent component analysis polymers space technology statistical analysis principal component analysis fuzzy systems information science knowledge engineering internet;text classify text reducing method improved knn algorithm text vector space vector polymer theory feature selection methods feature words;vector space;text classify;text analysis;testing;feature words;redundancy;vector polymerization;feature selection methods;feature extraction;text analysis feature extraction pattern classification;classification algorithms;pattern classification;clustering algorithms;support vector machine classification;feature selection;similarity degree text reducing feature selection vector polymerization;vector polymer theory;text vector space;sparse matrices;improved knn algorithm	There are relevance and redundancy of the feature words in the text vector space, so we proposed a text-reducing method based on the improved KNN algorithm in this paper. Vector polymer theory and feature selection methods were used to reducing the dimension of vector space. Feature words would have more ability to represent categories after feature selection. Experiments proved, the improved KNN algorithm were used in text-reducing not only can reducing the dimension of vector space more effectively, but also can improving the speed and accuracy of the text classify.	experiment;feature selection;genetic algorithm;k-nearest neighbors algorithm;polymer;relevance	Peiyu Liu;Ye Qiu;Lina Zhao	2009	2009 Sixth International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2009.616	feature vector;sparse matrix;vector space;feature extraction;computer science;machine learning;pattern recognition;data mining;mathematics;software testing;cluster analysis;redundancy;feature selection	Robotics	-21.651878666578945	-63.91260619387181	137871
1f430553ee231db65ae1ee47a8838b1c2f25c601	deep text: using text analytics to conquer information overload, get real value from social media, and add big(ger) text to big data			big data;information overload;social media;text mining	Behrooz Bayat	2017	The Electronic Library	10.1108/EL-09-2017-0188	computer science;world wide web;data science;big data;data mining;information overload;social media;text mining	AI	-23.54480376711797	-56.38585944065283	137892
7d5339a582aa054298b8ff3f536cf00fcb522807	automatic medical coding of patient records via weighted ridge regression	semantic similarity;document clustering;pattern clustering;search engine;vector space model;text analysis;turkish document clustering;web site turkish document clustering world wide web search engine single term semantic similarity measure text analysis vector space model;web sites natural language processing pattern clustering text analysis;single term semantic similarity measure;web sites;world wide web;similarity measure;natural language processing;similarity search;web site;taxonomy design engineering web sites world wide web search engines functional analysis frequency thesauri machine learning application software	In this paper, we apply weighted ridge regression to tackle the highly unbalanced data issue in automatic large-scale ICD-9 coding of medical patient records. Since most of the ICD-9 codes are unevenly represented in the medical records, a weighted scheme is employed to balance positive and negative examples. The weights turn out to be associated with the instance priors from a probabilistic interpretation, and an efficient EM algorithm is developed to automatically update both the weights and the regularization parameter. Experiments on a large-scale real patient database suggest that the weighted ridge regression outperforms the conventional ridge regression and linear support vector machines (SVM).	code;computation;cross-validation (statistics);expectation–maximization algorithm;experiment;f1 score;support vector machine;unbalanced circuit	Jian-Wu Xu;Shipeng Yu;Jinbo Bi;Lucian Vlad Lita;Radu Stefan Niculescu;R. Bharat Rao	2007	Sixth International Conference on Machine Learning and Applications (ICMLA 2007)	10.1109/ICMLA.2007.32	semantic similarity;ranking;document clustering;computer science;data mining;world wide web;vector space model;information retrieval;search engine;similarity heuristic	ML	-22.65610638816384	-63.70796385297873	138178
94cbdf61b435ba216580d95f16e5b1b361a7779d	bridging the terminology gap in web archive search		Web archives play an important role in preserving our cultural heritage for future generations. When searching them, a serious problem arises from the fact that terminology evolves constantly. Since today’s users formulate queries using current terminology, old but relevant documents are often not retrieved. The query saint petersburg museum, for instance, does not retrieve documents from the 1970s about museums in Leningrad (the former name of Saint Petersburg). We address this problem by determining query reformulations that paraphrase the user’s information need using terminology prevalent in the past. A measure of across-time semantic similarity that assesses the degree of relatedness between two terms when used at different times is proposed. Using this measure as a crucial building block, we propose a novel query reformulation technique based on a hidden Markov model (HMM). Experiments on twenty years worth of New York Times articles demonstrate the usefulness and efficiency of our approach.	bridging (networking);hidden markov model;information needs;markov chain;semantic similarity;the new york times;web archive	Klaus Berberich;Srikanta J. Bedathur;Mauro Sozio;Gerhard Weikum	2009			semantic similarity;web search query;data mining;database;war;information retrieval;terminology;computer science;information needs;paraphrase;cultural heritage;semantic web stack	Web+IR	-31.488088856484765	-57.00479700776571	138233
5dab1e6a93b56c80e895aded3199ec4e52c5866d	image tag clarity: in search of visual-representative tags for social images	flickr;information sources;image annotation;visual representative tag;image tag clarity;langugage model;social media;language model;image retrieval	Tags associated with images in various social media sharing web sites are valuable information source for superior image retrieval experiences. Due to the nature of tagging, many tags associated with images are not visually descriptive. In this paper, we propose Normalized Image Tag Clarity (NITC) to evaluate the effectiveness of a tag in describing the visual content of its annotated images. It is measured by computing the zero-mean normalized distance between the tag language model estimated from the images annotated by the tag and the collection language model. The visual-representative tags that are commonly used to annotate visually similar images are given high tag clarity scores. Evaluated on a large real-world dataset containing more than 269K images and their associated tags, we show that NITC score can effectively identify the visual-representative tags from all tags contributed by users. We also demonstrate through experiments that most popular tags are indeed visually representative.	experiment;flickr;image retrieval;information source;language model;rank (j programming language);social media;tag (metadata)	Aixin Sun;Sourav S. Bhowmick	2009		10.1145/1631144.1631150	computer science;data mining;world wide web;tag cloud;information retrieval	NLP	-25.694239171168807	-60.041494332032144	138387
0a501e40157531a292443bcbe3decbd52d7b3fc9	bibliographic attributes extraction with layer-upon-layer tagging	author tagging;bibliographic attribute;high degree;layer-upon-layer tagging;llt method;experimental evaluation;whole reference tagging;mantic tagging;bibliographic attributes extraction;rule-based method;high accuracy;rule based;computer and information science;digital libraries;knowledge based systems;digital library	Bibliographic attributes extraction is an important research topic for digital libraries. In this paper we propose a rule-based method for bibliographic attributes extraction with Layer-upon-Layer Tagging (LLT). The method analyzes bibliographic attributes' appearances and punctuations to perform format and semantic taggings on two defined parsing layers. The method also resolves to specifically constructed lexicons to achieve high accuracy of semantic tagging. In the experimental evaluation on 1,000 reference strings, the accuracy of author tagging reaches to 96.8% and the accuracy of whole reference tagging is 82.9%. The experimental results demonstrate that the proposed LLT method can tag bibliographic attributes in reference strings with high degree of accuracy.	digital library;lexicon;library (computing);logic programming;parsing;statistical model;tag (metadata)	Wei Wei;Irwin King;Jimmy Ho-Man Lee	2007	Ninth International Conference on Document Analysis and Recognition (ICDAR 2007)	10.1109/ICDAR.2007.72	digital library;computer science;data mining;world wide web;information retrieval;information and computer science	NLP	-30.04209591474886	-65.64225505097181	138433
e98eacb6be571eb9eca0df96630c9d75a0375f96	discovering key moments in social media streams	domain specific baseline key moments discovery social media streams laburst machine learning temporal patterns twitter classifier burst detection techniques event detection algorithms language agnostic algorithm sporting competitions time series analysis baseline;social networking online data analysis learning artificial intelligence;twitter media event detection earthquakes time frequency analysis conferences electronic mail	This paper introduces a general technique, called LABurst, for identifying key moments, or moments of high impact, in social media streams without the need for domain-specific information or seed keywords. We leverage machine learning to model temporal patterns around bursts in Twitter's unfiltered public sample stream and build a classifier to identify tokens experiencing these bursts. We show LABurst performs competitively with existing burst detection techniques while simultaneously providing insight into and detection of unanticipated moments. To demonstrate our approach's potential, we compare two baseline event-detection algorithms with our language-agnostic algorithm to detect key moments across three major sporting competitions: 2013 World Series, 2014 Super Bowl, and 2014 World Cup. Our results show LABurst outperforms a time series analysis baseline and is competitive with a domain-specific baseline even though we operate without any domain knowledge. We then go further by transferring LABurst's models learned in the sports domain to the task of identifying earthquakes in Japan and show our method detects large spikes in earthquake-related tokens within two minutes of the actual event.	algorithm;baseline (configuration management);domain-specific language;language-independent specification;machine learning;social media;software token;statistical classification;time series	Cody Buntain;Jimmy J. Lin;Jennifer Golbeck	2016	2016 13th IEEE Annual Consumer Communications & Networking Conference (CCNC)	10.1109/CCNC.2016.7444808	simulation;telecommunications;operating system;data mining;world wide web;computer security	DB	-23.673693736795926	-55.09565053755939	138443
d86c54b41e4f872c323b7bdda287c54c11d06d57	the chinese duplicate web pages detection algorithm based on edit distance	edit distance;feature string of web page;near-replicas detection;the largest number of common character	On one hand, redundant pages could increase searching burden of the search engine. On the other hand, they would lower the user’s experience. So it is necessary to deal with the pages. To achieve near-replicas detection, most of the algorithms depend on web page content extraction currently. But the cost of content extraction is large and it is difficult. What’s more, it becomes much harder to extract web content properly. This paper addresses these issues through the following ways: it gets the definition of the largest number of common character by taking antisense concept of edit distance; it suggests that the feature string of web page built by a previous Chinese character of period in simple processing text; and it utilizes the largest number of common character to calculate the overlap factor between the feature strings of web page. As a consequence, this paper hopes to achieve near-replicas detection in high noise environment, avoiding extracting the content of web page. The algorithm is proven efficient in our experiment testing: the recall rate of web pages reaches 96.7%, and the precision rate reaches 97.8%.	algorithm;content-control software;edit distance;experience;redundancy (engineering);sensitivity and specificity;web content;web page;web search engine	Junxiu An;Pengsen Cheng	2013	JSW		edit distance;page view;computer science;data mining;printer-friendly;database;hits algorithm;world wide web;information retrieval;statistics	Web+IR	-30.70273475822036	-54.7713286105575	138450
0ebf7a35a5bca320ba72bcb87292359d6e7bb0ef	major topic detection and its application to opinion summarization	topic detection;sentence retrieval;opinion summarization			Lun-Wei Ku;Li-Ying Lee;Tung-Ho Wu;Hsin-Hsi Chen	2005		10.1145/1076034.1076161	multi-document summarization;automatic summarization;pattern recognition;data mining;information retrieval	NLP	-24.974188330553524	-61.355901185463075	138462
5b53eb5ae19fec06268eab55f2eac2924a206236	are clickthroughs useful for image labelling?	electronic mail;query reformulation;information science;search engines;filters;image classification;labeling intelligent agent search engines filters electronic mail image classification conferences information science australia humans;image search;intelligent agent;web search;humans;query logs;clickthrough data;labeling;conferences;australia	In this paper we look at how images can be labelled as a result of click throughs from searches. One approach acts as a filter on image searches specifically, while the other approach propagates labels to images from their containing pages, where those pages were labelled themselves using clickthrough as a filter on text search. Then the paper reports on an experiment where users ranked for relevance six methods for labelling images, comparing the two clickthrough-based methods with flickr's amateur explicit labelling, Getty's professional explicit labelling, Google's standard image search, and the new Google Image Labeller. As well as comparing the accuracy of the proposed image labelling methods and discovering that automatic methods outperform explicit human labelling methods, the experiment suggests clickthrough data is reliable with very few clicks for image classification purposes.	computer vision;flickr;getty thesaurus of geographic names;image retrieval;relevance	Helen Ashman;Michael Antunovic;Christoph Donner;Rebecca Frith;Eric Rebelos;Jan-Felix Schmakeit;Gavin Smith;Mark Truran	2009	2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology	10.1109/WI-IAT.2009.35	labeling theory;contextual image classification;information science;computer science;artificial intelligence;data mining;world wide web;intelligent agent;information retrieval;search engine	Vision	-28.88393492729072	-55.14458885269918	138521
24abf1f292dd8355b26d448113f007e78ccf760f	web document clustering: a feasibility demonstration	document clustering;web documents;search engine;satisfiability;web search engine;clustering method;linear time;suffix tree clustering	Users of Web search engines are often forced to sift through the long ordered list of document “snippets” returned by the engines. The IR community has explored document clustering as an alternative method of organizing retrieval results, but clustering has yet to be deployed on the major search engines. The paper articulates the unique requirements of Web document clustering and reports on the first evaluation of clustering methods in this domain. A key requirement is that the methods create their clusters based on the short snippets returned by Web search engines. Surprisingly, we find that clusters based on snippets are almost as good as clusters created using the full text of Web documents. To satisfy the stringent requirements of the Web domain, we introduce an incremental, linear time (in the document collection size) algorithm called Suffix Tree Clustering (STC), which creates clusters based on phrases shared between documents. We show that STC is faster than standard clustering methods in this domain, and argue that Web document clustering via STC is both feasible and potentially beneficial.	algorithm;archive;cluster analysis;computer cluster;organizing (structure);requirement;suffix tree clustering;time complexity;web page;web search engine;world wide web	Oren Zamir;Oren Etzioni	1998		10.1145/290941.290956	time complexity;data stream clustering;document clustering;web search engine;fuzzy clustering;computer science;data mining;cluster analysis;brown clustering;world wide web;information retrieval;search engine;affinity propagation;satisfiability;clustering high-dimensional data;conceptual clustering	Web+IR	-29.25383513391858	-57.14342511690629	138542
ca6c3676e5cad81aab7b36a58bbcdd43e8c854bb	short text opinion detection using ensemble of classifiers and semantic indexing	semantic indexing;classification;machine learning;text normalization;sentiment analysis	The popularity of social networks has attracted attention of companies. The growing amount of connected users and messages posted per day make these environments fruitful to detect needs, tendencies, opinions, and other interesting information that can feed marketing and sales departments. However, the most social networks impose size limit to messages, which lead users to compact them by using abbreviations, slangs, and symbols. As a consequence, these problems impact the sample representation and degrade the classification performance. In this way, we have proposed an ensemble system to find the best way to combine the state-of-the-art text processing approaches, as text normalization and semantic indexing techniques, with traditional classification methods to automatically detect opinion in short text messages. Our experiments were diligently designed to ensure statisti∗Corresponding author Email addresses: jlochter@acm.org (Johannes V. Lochter), ferrazzanett@wisc.edu (Rafael F. Zanetti), dreller@acm.org (Dominik Reller), talmeida@ufscar.br (Tiago A. Almeida) Preprint submitted to Expert Systems with Applications June 15, 2016	algorithm;blog;co-training;database normalization;dictionary;email;experiment;expert system;learning classifier system;lexicography;model selection;online and offline;sensor;social network;text normalization;web application	Johannes V. Lochter;Rafael F. Zanetti;Dominik Reller;Tiago A. Almeida	2016	Expert Syst. Appl.	10.1016/j.eswa.2016.06.025	biological classification;computer science;machine learning;pattern recognition;data mining;information retrieval;sentiment analysis	Web+IR	-21.954807700614644	-61.070736973555725	138569
8f88f028c6e236811ac014fdd0e722cb00bbc7c0	cuni at the clef 2015 ehealth lab task 2		We present our participation as the team of the Charles University in Prague at the CLEF eHealth 2015 Task 2. We investigate performance of different retrieval models, linear interpolation of multiple models, and our own implementation of blind relevance feedback for query expansion. We employ MetaMap as an external resource for annotating the collection and the queries, then conduct retrieval at concept level rather than word level. We use MetaMap for query expansion. We also participate in the multilingual task where queries were given in several languages. We use Khresmoi medical translation system to translate the queries from Czech, French, and German into English. For the other languages we use translation by Google Translate and Bing Translator.	bing translator;google translate;linear interpolation;machine translation;query expansion;relevance feedback	Shadi Saleh;Feraena Bibyna;Pavel Pecina	2015			world wide web;ehealth;internet privacy;clef;engineering	NLP	-32.29082747269418	-64.00458867084618	138597
ac5d98eea570fc57d086858eda7b7a0069ab84ed	exploiting entities in social media	named entity extraction;social search;twitter	Over the past couple of years micro blogging platforms, such as Twitter, have become extremely popular for information generation and dissemination. Each day hundreds of millions of tweets are being published, containing fresh and trending information that is highly valuable for online users. However, discovering relevant information from such sources is becoming harder due to their rapid growth and the fact that social fragments are often short and noisy. Aggregation techniques such as clustering are often used for extracting this relevant information, since interesting signals begin to emerge when these fragments are grouped together. Clustering large amount of short tweets with limited features is however a challenging task in itself. In this paper, we propose to aggregate tweets by pivoting on entities and mapping them to topics that are already defined in websites such as Wikipedia and Freebase. This allows us to aggregate tweets in a more reliable and feasible way while providing interesting aggregated information about entities present in these fragments. Our analysis using large amounts of tweets shows that such an approach indeed works well. We present encouraging results and various interesting applications centered on entities.	aggregate data;blog;cluster analysis;entity;experience;freebase;front and back ends;information overload;scalability;social media;wikipedia	Omar Alonso;Qifa Ke;Kartikay Khandelwal;Srinivas Vadrevu	2013		10.1145/2513204.2513210	engineering;data mining;internet privacy;world wide web	ML	-23.94491615980514	-53.31034339161792	138677
353d9217ae20b0c5b3a45b6139d2bcf9cee8e941	a semantic classification approach for online product reviews	electronic commerce;information sources;e commerce;bayes methods;classification;naive bayes classifier;internet;decision making electronic commerce internet pattern classification classification feature extraction bayes methods;feature extraction;pattern classification;feature selection;sentence classification semantic classification online product review e commerce world wide web information source customer decision making feature selection sentence tagging naive bayes classifier heuristic rule;information gain;semi supervised document clustering;learning systems decision making tagging fuzzy logic fuzzy sets chaos information technology australia portals niobium;em;conference proceeding	With the fast growth of e-commerce, product reviews on the Web have become an important information source for customersý decision making when they plan to buy products online. As the reviews are often too many for customers to go through, how to automatically classify them into different semantic orientations (i.e. recommend/not recommend) has become a research problem. Different from traditional approaches that treat a review as a whole, our approach performs semantic classifications at the sentence level by realizing reviews often contain mixed feelings or opinions. In this approach, a typical feature selection method based on sentence tagging is employed and a naïve bayes classifier is used to create a base classification model, which is then combined with certain heuristic rules for review sentence classification. Experiments show that this approach achieves better results than using general naïve bayes classifiers.	e-commerce;feature selection;heuristic;information source;naive bayes classifier;naivety;orientation (graph theory);world wide web	Chao Wang;Jie Lu;Guangquan Zhang	2005	The 2005 IEEE/WIC/ACM International Conference on Web Intelligence (WI'05)	10.1109/WI.2005.12	e-commerce;the internet;naive bayes classifier;feature extraction;biological classification;computer science;machine learning;pattern recognition;data mining;em;kullback–leibler divergence;world wide web	AI	-23.31388717381	-57.96935804694659	138784
04f61779ddff161c850421806be18156e304f003	text stream processing	text mining;text processing;data stream;entity recognition;text stream;word sense disambiguation;topic detection;sentiment analysis;semantic web;web mining;social network analysis;stream processing;natural language processing;semantic analysis	The aim of this tutorial is to present an overview of text stream processing starting with a description and properties of text streams, and continuing with a series of text processing techniques and their applicability to text streams. Among the text processing techniques we are going to describe entity extraction and resolution, event and fact extraction, word sense disambiguation, sentiment analysis, summarization, social network analysis, all in the context of text streams.  The goal is to present the list of problems and challenges arising when processing text streams and to show how they can be approached using text mining, natural language processing and semantic analysis techniques and tools. The tutorial will describe available approaches and show some demos on text data streams, using publicly available tools.	named-entity recognition;natural language processing;semantic analysis (compilers);sentiment analysis;social network analysis;stream processing;text corpus;text mining;word sense;word-sense disambiguation	Dunja Mladenic;Marko Grobelnik;Blaz Fortuna;Delia Rusu	2012		10.1145/2254129.2254138	natural language processing;text graph;text simplification;text mining;computer science;noisy text analytics;data mining;information retrieval;co-occurrence networks;sentiment analysis	NLP	-24.781430955299555	-55.660800794287354	138848
02a8ad3525658a64a8a5857da462eb792e67c86e	asymmetric and context-dependent semantic similarity among ontology instances	semantic similarity;user needs;context dependent;similarity measure	In this paper we propose an asymmetric semantic similarity among instances within an ontology. We aim to define a measurement of semantic similarity that exploit as much as possible the knowledge stored in the ontology taking into account different hints hidden in the ontology definition. The proposed similarity measurement considers different existing similarities, which we have combined and extended. Moreover, the similarity assessment is explicitly parameterised according to the criteria induced by the context. The parameterisation aims to assist the user in the decision making pertaining to similarity evaluation, as the criteria can be refined according to user needs. Experiments and an evaluation of the similarity assessment are presented showing the efficiency of the method.	context (computing);experiment;iterative method;metadata repository;semantic similarity;similarity learning;web ontology language	Riccardo Albertoni;Monica De Martino	2008	J. Data Semantics	10.1007/978-3-540-77688-8_1	semantic similarity;computer science;context-dependent memory;data mining;database;linguistics;information retrieval;similarity heuristic;dishin	Web+IR	-29.446231121070017	-59.30986576753832	138850
bf3cdd9dbeafa73b01e062547b7ecb37df692eef	building a microblog corpus for search result diversification		Queries that users pose to search engines are often ambiguous either because different users express different query intents with the same query terms or because the query is underspecified and it is unclear which aspect of a particular query the user is interested in. In the Web search setting, search result diversification, whose goal is the creation of a search result ranking covering a range of query intents or aspects of a single topic respectively, has been shown in recent years to be an effective strategy to satisfy search engine users. We hypothesize that such a strategy will also be beneficial for search on microblogging platforms. Currently, progress in this direction is limited due to the lack of a microblog-based diversification corpus. In this paper we address this shortcoming and present our work on creating such a corpus. We are able to show that this corpus fulfils a number of diversification criteria as described in the literature. Initial search and retrieval experiments evaluating the benefits of de-duplication in the diversification setting are also reported.	dbpedia;data deduplication;diversification (finance);experiment;relevance;text corpus;web search engine;world wide web	Ke Tao;Claudia Hauff;Geert-Jan Houben	2013		10.1007/978-3-642-45068-6_22	query expansion;web query classification;ranking;data mining;web search query;world wide web;information retrieval;search engine	Web+IR	-32.794386387586286	-55.07279838384633	139005
dde71bdd2742825b2d5285c22dbb242658f9a597	information retrieval challenges in computational advertising	sponsored search;computational advertising;display adver tising	"""Computational advertising is an emerging scientific sub-discipline, at the intersection of large scale search and text analysis, information retrieval, statistical modeling, machine learning, classification, optimization, and microeconomics. The central challenge of computational advertising is to find the """"best match"""" between a given user in a given context and a suitable advertisement. The aim of this tutorial is to present the state of the art in Computational Advertising, in particular in its IR-related aspects, and to expose the participants to the current research challenges in this field. The tutorial does not assume any prior knowledge of Web advertising, and will begin with a comprehensive background survey. Going deeper, our focus will be on using a textual representation of the user context to retrieve relevant ads. At first approximation, this process can be reduced to a conventional setup by constructing a query that describes the user context and executing the query against a large inverted index of ads. We show how to augment this approach using query expansion and text classification techniques tuned for the ad-retrieval problem. In particular, we show how to use the Web as a repository of query-specific knowledge and use the Web search results retrieved by the query as a form of a relevance feedback and query expansion. We also present solutions that go beyond the conventional bag of words indexing by constructing additional features using a large external taxonomy and a lexicon of named entities obtained by analyzing the entire Web as a corpus. The last part of the tutorial will be devoted to a potpourri of recent research results and open problems inspired by Computational Advertising challenges in text summarization, natural language generation, named entity recognition, computer-human interaction, and other SIGIR-relevant areas."""	automatic summarization;bag-of-words model;computation;document classification;human–computer interaction;information retrieval;inverted index;lexicon;machine learning;mathematical optimization;named entity;natural language generation;online advertising;order of approximation;query expansion;relevance feedback;statistical classification;statistical model;taxonomy (general);web search engine;world wide web	Andrei Z. Broder;Evgeniy Gabrilovich;Vanja Josifovski	2010		10.1145/2063576.2064037	computer science;multimedia;search advertising;information retrieval	Web+IR	-31.292325781034084	-54.975710638870794	139320
3e33fc58f86b36c81ea16cc23f79f17eb487b531	ecological evaluation of persuasive messages using google adwords	scientific research;experimental research;affective text variation;fast experiment building;central idea;google adwords;ecological evaluation;nlp task;message impact;fast methodology;persuasive message;existing commercial tool	In recent years there has been a growing interest in crowdsourcing methodologies to be used in experimental research for NLP tasks. In particular, evaluation of systems and theories about persuasion is difficult to accommodate within existing frameworks. In this paper we present a new cheap and fast methodology that allows fast experiment building and evaluation with fully-automated analysis at a low cost. The central idea is exploiting existing commercial tools for advertising on the web, such as Google AdWords, to measure message impact in an ecological setting. The paper includes a description of the approach, tips for how to use AdWords for scientific research, and results of pilot experiments on the impact of affective text variations which confirm the effectiveness of the approach. To appear at ACL 2012	cartesian perceptual compression;crowdsourcing;design of experiments;experiment;google adwords;levinson recursion;natural language processing;theory	Marco Guerini;Carlo Strapparava;Oliviero Stock	2012			simulation;computer science;artificial intelligence;multimedia	NLP	-24.82840310068037	-55.81866524841556	139369
0f7592ef5ad8cbb1c43a15f3ca1c05ca0ca2069c	computational community interest for ranking	user evaluation;web pages;information retrieval;blog;lda;ranking;community interest;user;document retrieval;information need;topic	Ranking documents with respect to users' information needs is a challenging task, due, in part, to the dynamic nature of users' interest with respect to a query, which can change over time. In this paper, we propose an innovative method for characterizing the interests of a community of users at a specific point in time and for using this characterization to alter the ranking of documents retrieved for a query. By generating a community interest vector (CIV) for a given query, we measure the community interest by computing a score in a specific document or web page retrieved by the query. This score is based on a continuously updated set of recent (daily or past few hours) user-oriented text data. When applying our method in ranking Yahoo! Buzz results, the CIV score improves relevant results by 16% as determined by real-world user evaluation.	civ (rail travel);computation;information needs;information retrieval;text corpus;web page	Xiaozhong Liu;Vadim von Brzeski	2009		10.1145/1645953.1645987	document retrieval;information needs;user;query expansion;web query classification;ranking;ranking;computer science;web page;data mining;database;world wide web;information retrieval	Web+IR	-28.07555487188582	-54.82042006541914	139469
27bcf3416f693c04de2819b857d8211777b9cf44	cumulative citation recommendation: classification vs. ranking	knowledge base acceleration;information filtering;cumulative citation recommendation	Cumulative citation recommendation refers to the task of filtering a time-ordered corpus for documents that are highly relevant to a predefined set of entities. This task has been introduced at the TREC Knowledge Base Acceleration track in 2012, where two main families of approaches emerged: classification and ranking. In this paper we perform an experimental comparison of these two strategies using supervised learning with a rich feature set. Our main finding is that ranking outperforms classification on all evaluation settings and metrics. Our analysis also reveals that a ranking-based approach has more potential for future improvements.	entity;knowledge base;supervised learning	Krisztian Balog;Heri Ramampiaro	2013		10.1145/2484028.2484151	computer science;pattern recognition;data mining;ranking svm;information retrieval	Web+IR	-26.90514135658304	-61.354729543078754	139750
004f7e667f5bb3b0d996d6a14d67a123cb61f7b2	squall2sparql: a translator from controlled english to full sparql 1.1		This paper reports on the participation of the system squall2sparql in the QALD-3 question answering challenge for DBpedia. squall2sparql is a translator from SQUALL, a controlled natural language for English, to SPARQL 1.1, a standard expressive query and update language for linked open data. It covers nearly all features of SPARQL 1.1, and is directly applicable to any SPARQL endpoint.	communication endpoint;controlled natural language;dbpedia;linked data;question answering;sparql	Sébastien Ferré	2013			linked data;question answering;information retrieval;database;sparql;computer science;controlled natural language	NLP	-33.17346264611967	-65.22330723201283	139823
9d2eebf04292bf7ed00c6dc8780e4ba84ffa80ca	combining link and content information for scientific topics discovery	citation analysis;information science;information retrieval;bibliographic context scientific topics discovery citation analysis text analysis;text analysis;citation;joints;bibliographic context;content information;machine learning;citation scientific topics discovery content information link information;clustering algorithms;scientific information systems citation analysis information retrieval;probabilistic logic;link information;scientific topics discovery;information resources information analysis artificial intelligence bibliometrics citation analysis information science information retrieval content based retrieval data mining web pages;algorithm design and analysis;scientific information systems	The analysis of current approaches combining links and contents for scientific topics discovery reveals that the two sources of information (i.e. links and contents) are considered to be heterogeneous. Therefore, in this paper, we propose to integrate link and content information by exploiting the links semantics to enrich the textual content of documents. This idea is then implemented and evaluated. Experiments carried out on two real-world datasets show the good performances of our approach over state of the art techniques that combine citation and content information for scientific topics discovery.	experiment;performance	Nacim Fateh Chikhi;Bernard Rothenburger;Nathalie Aussenac-Gilles	2008	2008 20th IEEE International Conference on Tools with Artificial Intelligence	10.1109/ICTAI.2008.136	algorithm design;information science;computer science;data science;machine learning;data mining;probabilistic logic;cluster analysis;citation analysis;information retrieval	Robotics	-26.99573694813914	-57.860741238980744	140085
8a29c086ad6699ce4b63e3a740c42c1a24960f28	building active internet portals for document sharing using association rules	internet portal;association rules;data mining;association rule;push technology;document sharing	An internet portal provides a simple way of putting popular web functions together. It is common for an individual to group his/her preferred websites into a personal portal. However, a user normally relies on search engines to look for information so a significant amount of time is often spent on searching the internet. This paper describes a form of push technology that automatically delivers documents to the personal internet portal. The association rules of data mining techniques are used to discover the relationships between documents so that the right document can be delivered to the right person.	association rule learning;portals	Jacqueline Wong;Timon Du;Honglei Li	2004	IJEB	10.1504/IJEB.2004.004559	the internet;association rule learning;internet draft;computer science;data mining;world wide web;information retrieval	DB	-30.147680969166924	-52.96618807799119	140178
9b462f21d3bd186c5b72a9a82bc4a29d6cfb07e2	topical tags vs non-topical tags: towards a bipartite classification?	topicality and non topicality of tags;folksonomy;user testing session;delicious;umbrella tags;latent semantic analysis	In this paper we investigate whether it is possible to create a computational approach that allows us to distinguish topical tags (i.e. talking about the topic of a resource) and non-topical tags (i.e. describing aspects of a resource that are not related to its topic) in folksonomies, in a way that correlates with humans. Towards this goal, we collected 21 million tags (1.2 million unique terms) from Delicious and developed an unsupervised statistical algorithm that classifies such tags by applying a word space model adapted to the folksonomy space. Our algorithm analyses the co-occurrence network of tags to a target tag and exploits graph-based metrics for their classification. We validated its outcomes against a reference classification made by humans on a limited number of terms in three separate tests. The analysis of the outcomes of our algorithm shows, in some cases, a consistent disagreement among humans and between humans and our algorithm about what constitutes a topical tag, and suggests the rise of a new category of overly generic tags (i.e. umbrella tags).	adobe creative suite;algorithm;discrepancy function;emergence;experiment;flickr;folksonomy;information science;inter-rater reliability;left 4 dead 2;mendeley;naval tactical data system;obsession (video game);social network;tag (game);tag (metadata);test set;verification and validation;web page	Valerio Basile;Silvio Peroni;Fabio Tamburini;Fabio Vitali	2015	J. Information Science	10.1177/0165551515585283	latent semantic analysis;computer science;data mining;world wide web;information retrieval	ML	-21.980468538117933	-60.15694404241632	140206
96823677b723472e8e99384b313e64271459e418	recommending scientific articles using bi-relational graph-based iterative rwr	iterative rwr;old article recommendation;recommending scientific article;bi relational graph;new article recommendation	The overabundance of scientific article information has created much inconvenience to researchers seeking interesting articles online. In this paper, we provide a Bi-Relational graph to represent the heterogenous information of scientific article recommendation system, which includes three parts: the article content similarity, researcher interest correlation, and researcher-article readership. Meanwhile, an iterative random walk with restarts learning method is proposed on the Bi-Relational graph to recommend a researcher rating for each article by making use of the known information. The proposed method has ability to perform both old and new article recommendation. A series of experiments on CiteULike dataset have shown that our method is more effective than other testing methods in the paper.	experiment;iterative method;recommender system;running with rifles;scientific literature	Geng Tian;Liping Jing	2013		10.1145/2507157.2507212	computer science;data mining;world wide web;information retrieval	AI	-26.608835823330107	-57.10242301551247	140220
41d2f866d8d04843f21055d7cd696053f7aac6ec	user-based document clustering by redescribing subject descriptions with a genetic algorithm	relevance information retrieval;documentation;simulation;document clustering;information retrieval;algorithms;genetic algorithm	Information retrieval systems have used clustering of documents and queries to improve both retrieval efficiency and retrieval effectiveness. Normally, clustering involves grouping together static descriptions of documents by their similarity to each other, though user-based clustering suggests that usage patterns concerning co-relevance can form a basis for clustering. This article reports that clusters of co-relevant documents obtain increasingly similar descriptions when a genetic algorithm is used to adapt subject descriptions so that documents become more effective in matching relevant queries and failing to match nonrelevant queries. As a result of the increased similarity, clustering algorithms can more accurately group documents into useful clusters. The findings of this work were reached through simulation experiments. © 1991 John Wiley u0026 Sons, Inc.	cluster analysis;genetic algorithm	Michael D. Gordon	1991	JASIS	10.1002/(SICI)1097-4571(199106)42:5%3C311::AID-ASI1%3E3.0.CO;2-J	documentation;clustering high-dimensional data;information retrieval;relevance (information retrieval);computer science;data mining;document retrieval;fuzzy clustering;document clustering;cluster analysis;brown clustering	NLP	-27.52630801284374	-59.23231780265024	140238
6aaa1f8421b3ea0d47aed6f6aa9bd41a9316f316	sentence level fact based search engine: news fact finder	heuristic optimization and search;substring matching;search engine;search engines internet query processing relevance feedback;sentence level fact based search engine;suffix arrays;query processing;substring matching sentence level fact based search engine news fact finder internet suffix array;search engines;natural language processing heuristic optimization and search sentence level search engine suffix arrays;sentence level search engine;search engines arrays indexes internet computer science information services web sites;information services;suffix array;arrays;indexes;internet;heuristic optimization;web sites;success rate;computer science;news fact finder;relevance feedback;natural language processing	Users searching the Internet for news are not able to find relevant fact-based results for certain queries using the major search engines. Queries that require exact substring matching in order to obtain very relevant results are not currently possible. Furthermore, search engines do not discriminate in returning results that are opinions and not quantifiable facts. Our sentence level search engine, News Fact Finder, is designed using suffix arrays, filters out opinions, and produces very relevant results that are attractive to users. The News Fact Finder produces a 73% success rate of providing relevant fact based results.	internet;substring;suffix array;web search engine	Cristina Ribeiro;Ricardo Salmon;Swathi Amarala;Christina A. Hamada	2010	2010 IEEE International Conference on Information Reuse & Integration	10.1109/IRI.2010.5558906	computer science;data mining;database;world wide web;information retrieval;search engine	DB	-30.90860459527874	-56.01777080916315	140263
a0cba46e5976ac91111320b65877deaddf58bd79	big data technology and ethics considerations in customer behavior and customer feedback mining	big data;ethics considerations;customer feedback;user activity;changing emotion	The rapidly increasing attention to customer behavior and satisfaction by department stores and commercial companies and development in social media as well as online systems has promoted production and research in user engagement pattern recognition, user network analysis, topic detection from customer feedback, text-based sentiment analysis, etc. With the development of Internet of Things and social media network, ethics consideration has also playing an important role in application of big data, especially customer behavior and feedback mining. Our proposed novel system, which extracts the important topics or issues from Skype customer feedback sources and measures the emotion associated with those topics using Vibe metric, can be a good example in this area. Unlike other previous research, which has focused on extracting the user sentiments either globally or in separate topics, our work focuses on tracking the correlated emotional trajectories across all the important issues from Skype customer feedback over time. Moreover, it also provides a platform for both studying customer emotions and tracking how the emotions regarding different important topics correlatively change over time by leveraging unstructured textual customer feedback data and structured user activity telemetry data.	big data;customer relationship management;internet of things;pattern recognition;sentiment analysis;social media;text-based (computing);vibe	Xin Deng	2017	2017 IEEE International Conference on Big Data (Big Data)	10.1109/BigData.2017.8258399	sentiment analysis;data mining;market research;consumer behaviour;computer science;big data;computational linguistics;social media;internet of things	AI	-22.046932976443138	-52.31082834562297	140302
98e305290f977e064fce7c7e2a59f5750b5dab6d	homogeneous segmentation and classifier ensemble for audio tag annotation and retrieval	audio segmentation;roc curve homogeneous segmentation classifier ensemble audio tag annotation audio tag retrieval music database audio novelty curve audio feature extraction frame based feature vector sequence format ensemble classifier svm classifier adaboost classifier calibrated probability scores probability ensemble ranking ensemble mirex 2009 audio tag classification task f measure;probability ensemble;audio signal processing;probability;classifier ensemble;measurement;ensemble method;support vector machines;audio retrieval;ranking ensemble;information retrieval;standard deviation;audio novelty curve;training;feature extraction training measurement classification algorithms accuracy support vector machine classification;audio tag retrieval;mirex 2009 audio tag classification task;homogeneous segmentation;feature vector;accuracy;frame based feature vector sequence format;feature extraction;automatic annotation;svm classifier;signal classification;classification algorithms;roc curve;support vector machine classification;f measure;ensemble classifier;audio feature extraction;support vector machines audio signal processing feature extraction information retrieval probability signal classification;music database;adaboost classifier;ensemble method audio segmentation audio tag annotation audio tag retrieval;calibrated probability scores;audio tag annotation	Audio tags describe different types of musical information such as genre, mood, and instrument. This paper aims to automatically annotate audio clips with tags and retrieve relevant clips from a music database by tags. Given an audio clip, we divide it into several homogeneous segments by using an audio novelty curve, and then extract audio features from each segment with respect to various musical information, such as dynamics, rhythm, timbre, pitch, and tonality. The features in frame-based feature vector sequence format are further represented by their mean and standard deviation such that they can be combined with other segment-based features to form a fixed-dimensional feature vector for a segment. We train an ensemble classifier, which consists of SVM and AdaBoost classifiers, for each tag. For the audio annotation task, the individual classifier outputs are transformed into calibrated probability scores such that probability ensemble can be employed. For the audio retrieval task, we propose using ranking ensemble. We participated in the MIREX 2009 audio tag classification task and our system was ranked first in terms of F-measure and the area under the ROC curve given a tag.	adaboost;audio signal processing;distribution ensemble;ensemble learning;feature vector;html5 audio;list of online music databases;pitch (music);receiver operating characteristic;support vector machine	Hung-Yi Lo;Ju-Chiang Wang;Hsin-Min Wang	2010	2010 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2010.5583009	statistical classification;support vector machine;audio mining;speech recognition;feature vector;audio signal processing;feature extraction;computer science;machine learning;pattern recognition;probability;accuracy and precision;standard deviation;f1 score;receiver operating characteristic;measurement;statistics	ML	-24.412667975946427	-62.686945551775274	140375
494fbfcb9eb0cf0aaea52634824d91aa119a171a	a random graph walk based approach to computing semantic relatedness using knowledge from wikipedia	random graph;empirical study;model based approach;semantic relatedness;random walk;feature extraction;institutional repository research archive oaister;computational semantics;natural language processing;named entity	Determining semantic relatedness between words or concepts is a fundamental process to many Natural Language Processing applications. Approaches for this task typically make use of knowledge resources such as WordNet and Wikipedia. However, these approaches only make use of limited number of features extracted from these resources, without investigating the usefulness of combining various different features and their importance in the task of semantic relatedness. In this paper, we propose a random walk model based approach to measuring semantic relatedness between words or concepts, which seamlessly integrates various features extracted from Wikipedia to compute semantic relatedness. We empirically study the usefulness of these features in the task, and prove that by combining multiple features that are weighed according to their importance, our system obtains competitive results, and outperforms other systems on some datasets.	natural language processing;random graph;semantic network;semantic similarity;wikipedia;wordnet	Ziqi Zhang;Anna Lisa Gentile;Lei Xia;José Iria;Sam Chapman	2010			natural language processing;random graph;semantic similarity;semantic computing;feature extraction;computer science;pattern recognition;data mining;empirical research;random walk;computational semantics	AI	-27.05556239943493	-64.45277335563715	140719
da47b32b165c8855e0eb8590096d1d7d3b1e8c34	an improved random walk algorithm based on correlation coefficient to find scientific communities	databases;similarity metric;citation graph;pattern clustering citation analysis;citation analysis;pattern clustering;support vector machines;online scientific literature clustering;random walk graph clustering algorithm;search engines;graph clustering;clustering algorithms iterative algorithms databases web sites computer science software engineering software algorithms publishing clustering methods heart;scientific communities;random walk;random walk graph clustering algorithm scientific communities world wide web online scientific literature clustering citation patterns citation graph;clustering algorithms;world wide web;scientific communication;correlation coefficent;random walk algorithm;correlation;communities;correlation coefficient;citation patterns;algorithm design and analysis;citation graph random walk algorithm correlation coefficent scientific community	With the growth of publishing scientific literature on the World Wide Web, there is a great demand for clustering online scientific literature by using the citation patterns. A scientific community in the citation graph represents related papers on a single topic. In this paper we improve the random walk graph clustering algorithm to find scientific communities by using correlation coefficient in the citation graph as the similarity metric. Our experiment results show that the approach performs better than the original random walk graph clustering method.	algorithm;citation graph;cluster analysis;coefficient;scientific literature;world wide web	Hong Yu;Xiaoliang Yang;Xianchao Zhang;Yuansheng Yang	2008	2008 International Conference on Computer Science and Software Engineering	10.1109/CSSE.2008.1226	random graph;correlation clustering;computer science;data science;theoretical computer science;data mining;clustering coefficient;citation analysis;random walk	DB	-26.58560027995628	-56.46593503890437	141012
6bf9e5031f1c3fed8d43417b5d7e318513089e8c	the geography of twitter topics in london	geotemporal;latent dirichlet allocation;twitter;topic modelling;social media	Article history: Received 19 October 2015 Received in revised form 18 March 2016 Accepted 4 April 2016 Available online xxxx Social media data are increasingly perceived as alternative sources to public attitude surveys because of the volume of available data that are time-stamped and (sometimes) precisely located. Such data can be mined to provide planners, marketers and researchers with useful information about activities and opinions across time and space. However, in their raw form, textual data are still difficult to analyse coherently and Twitter streams pose particular interpretive challenges because they are restricted to just 140 characters. This paper explores the use of an unsupervised learning algorithm to classify geo-tagged Tweets from Inner London recorded during typical weekdays throughout 2013 into a small number of groups, following extensive text cleaning techniques. Our classification identifies 20 distinctive and interpretive topic groupings, which represent key types of Tweets, fromdescribing activities or informal conversations betweenusers, to the use of check-in applets. Ourmotivation is to use the classification to demonstrate how the nature of the content posted on Twitter varies according to the characteristics of places and users. Topics and attitudes expressed through Tweets are found to vary substantially across Inner London, and by time of day. Some observed variations in behaviour on Twitter can be attributed to the inferred demographic and socio-economic characteristics of users, but place and local activities can also exert a considerable influence. Overall, the classification was found to provide a valuable framework for investigating the content and coverage of Twitter usage across Inner London. © 2016 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).	algorithm;applet;linear discriminant analysis;lookup table;mined;plasma cleaning;social media;text corpus;tweeter;unsupervised learning;while	Guy Lansley;Paul A. Longley	2016	Computers, Environment and Urban Systems	10.1016/j.compenvurbsys.2016.04.002	latent dirichlet allocation;social media;geography;computer science;artificial intelligence;machine learning;data mining;database;multimedia;topic model;world wide web;cartography	Web+IR	-24.881292849319433	-52.670608052972014	141362
b70ff97a3550bca43e8b06b1b04af14b979fbca5	sanaphor: ontology-based coreference resolution		We tackle the problem of resolving coreferences in textual content by leveraging Semantic Web techniques. Specifically, we focus on noun phrases that coreference identifiable entities that appear in the text; the challenge in this context is to improve the coreference resolution by leveraging potential semantic annotations that can be added to the identified mentions. Our system, SANAPHOR, first applies state-of-the-art techniques to extract entities, noun phrases, and candidate coreferences. Then, we propose an approach to type noun phrases using an inverted index built on top of a Knowledge Graph (e.g., DBpedia). Finally, we use the semantic relatedness of the introduced types to improve the stateof-the-art techniques by splitting and merging coreference clusters. We evaluate SANAPHOR on CoNLL datasets, and show how our techniques consistently improve the state of the art in coreference resolution.	cluster analysis;dbpedia;entity;inverted index;knowledge graph;knowledge base;open-source software;semantic web;semantic similarity;video post-processing	Roman Prokofyev;Alberto Tonon;Michael Luggen;Loic Vouilloz;Djellel Eddine Difallah;Philippe Cudré-Mauroux	2015		10.1007/978-3-319-25007-6_27	natural language processing;computer science;data mining;information retrieval	NLP	-27.465445618316792	-65.15440043208322	141364
43292e41f11ee251ca1fe564555dd6cc5fbe6a0c	feature selection with linked data in social media		Feature selection is widely used in preparing highdimensional data for effective data mining. Increasingly popular social media data presents new challenges to feature selection. Social media data consists of (1) traditional high-dimensional, attribute-value data such as posts, tweets, comments, and images, and (2) linked data that describes the relationships between social media users as well as who post the posts, etc. The nature of social media also determines that its data is massive, noisy, and incomplete, which exacerbates the already challenging problem of feature selection. In this paper, we illustrate the differences between attributevalue data and social media data, investigate if linked data can be exploited in a new feature selection framework by taking advantage of social science theories, extensively evaluate the effects of user-user and user-post relationships manifested in linked data on feature selection, and discuss some research issues for future work.	data mining;experiment;feature selection;linked data;open-source software;social media;theory	Jiliang Tang;Huan Liu	2012		10.1137/1.9781611972825.11	pattern recognition	AI	-22.546166364310096	-57.98958713347988	141497
154ec572e53485a22226d242f7b3fb402ea2eb28	predicting health inspection results from online restaurant reviews.		Informatics around public health are increasingly shifting from the professional to the public spheres. In this work, we apply linguistic analytics to restaurant reviews, from Yelp, in order to automatically predict official health inspection reports. We consider two types of feature sets, i.e., keyword detection and topic model features, and use these in several classification methods. Our empirical analysis shows that these extracted features can predict public health inspection reports with over 90% accuracy using simple support vector machines.	informatics;support vector machine;topic model	Samantha Wong;Hamid R. Chinaei;Frank Rudzicz	2016	CoRR		machine learning;computer science;topic model;artificial intelligence;data science;support vector machine;public health;data mining;informatics;analytics	NLP	-21.834668362912762	-56.93110371584853	141650
433543abc2c1fea0dbeb80629e660edba9a6e749	towards person google: multimodal person search and retrieval	multimedia retrieval;multimodal fusion;perforation;video analysis;indexing and retrieval;personalized search	With the increasing amount of available multimedia data, efficient systems for searching and retrieving relevant AV documents are needed. Since keyword based indexing is very time consuming and inefficient due to linguistic and semantic ambiguities, content based multimedia retrieval systems have been proposed, that search and retrieve AV documents based on audio and visual features. While content based image retrieval has been a very active research field only some work has been done in the field of person specific search and retrieval, where the goal is to find a AV document with a specific person present within the audio and/or the visual stream. This article describes an original system for multimodal person search (Person Google) within AV documents and provides some initial performance results.	content-based image retrieval;multimodal interaction	Lutz Goldmann;Amjad Samour;Thomas Sikora	2007		10.1007/978-3-540-77051-0_37	visual word;computer science;multimedia;world wide web;information retrieval;human–computer information retrieval	Web+IR	-31.047702799933067	-57.45216325459465	141675
541ee34de84ed0331eb32211f8f4028e341f6465	an analysis of the joint venture japanese text prototype and its effect on system performance	tipster contractor;textract system;japanese-language counterpart;english microelectronics;fifth message understanding conference;japanese-language application;system performance;muc-5 evaluation;english joint venture;tipster data extraction;japanese text prototype;jme application	"""The TIPSTER Data Extraction and Fifth Message Understanding Conference (MUC-5) tasks focused on the process of dataextraction. This is a procedure in which prespecified types of information are identified within free text, extracted, and inserted automatically within a template. Three TIPSTER contractors -BBN, GE/CMU, NMSU/Brandeis -participated in the August '93 MUC-5 evaluation for both the English joint venture (EJV) and English microelectronics (EME) domains and their Japanese-language counterparts, the ]3V and 3ME applications. Two other contractors -SRI and SRA -participated in the EJV and 33V domains alone. CMU's Textract system took part in the 3apanese-language domains only. Of the five systems that tested in both English and Japanese, a l l but one scored higher in the Japaneselanguage applications according to both the summary error-based scores and reca11/precision-based metrics. This overall result has lead some participants and observers to suggest that Japanese is an """"easier"""" language than English."""	encrypted media extensions;message understanding conference;prototype;sequence read archive	Steve Moiorano	1993			speech recognition;engineering;operations management;engineering drawing	NLP	-31.542033605415014	-65.0308736017672	141774
f01eb0f886f6e54dfc58779aa8840fa40dd14deb	a language model approach to capture commercial intent and information relevance for sponsored search	information model;score function;sponsored search;satisfiability;expectation maximization;online advertising;web search;relevance;parameter estimation;em algorithm;empirical evaluation;language model;commercial intent	A fundamental task of sponsored search is how to find the best match between web search queries and textual advertisements. To address this problem, we explicitly characterize the criteria for an advertisement to be a 'good match' to a query from two aspects (it should be relevant with the query from information perspective, and it should be able to capture and satisfy the commercial intent in the query). Correspondingly, we introduce in this paper a mixture language model of two parts: a commercial model which characterizes language bias of commercial intent leveraging on users' clicks on advertisements, and an informational model which is a traditional language model with consideration of the entropy of each word to capture informational relevance. We then introduce a regularized expectation-maximization (EM) algorithm model for parameters estimation, and integrate query commercial intent into the scoring function to boost overall click efficiency. Empirical evaluation shows that our model achieves better performance as compared to a well tuned classical language model and deliberated TFIDF-pLSI model (6% and 5% precision improvement at our operating point in production environment of 30% recall, and 5.3% and 6.3% AUC improvement), and performs superior to the KL Divergence language model for tail queries (0.5% nDCG improvement). Live traffic test shows over 2% CTR lift and 2.5% RPS lift as well.	deployment environment;estimation theory;expectation–maximization algorithm;kullback–leibler divergence;language model;operating point;personalization;relevance;scoring functions for docking;search engine marketing;tf–idf;web search engine;web search query;web server	Lei Wang;Mingjiang Ye;Yu Zou	2011		10.1145/2063576.2063665	natural language processing;online advertising;expectation–maximization algorithm;computer science;artificial intelligence;data science;machine learning;data mining;database;world wide web;information retrieval;statistics;language model	ML	-27.150757376264014	-53.28725340058101	141922
fd20e8ea38f51173d8f8aefe29f15f79485b965c	gold standard creation for microblog retrieval: challenges of completeness in irmidis 2017		Microblogging sites like Twitter, Facebook, etc., are important sources of first-hand accounts during disaster situations, and have the potential to significantly aid disaster relief efforts. The IRMiDis track at FIRE 2017 focused on developing and comparing IR approaches to automatically identify and match tweets that indicate the need or availability of a resource, leading to the creation of a benchmark dataset for future improvements in this task. However, based on our experiments, we argue that the gold standard data obtained in the track is substantially incomplete. We also discuss some reasons why it may have been so, and provide some suggestions for making more robust ground truth data in such tasks.	benchmark (computing);experiment;ground truth	Ribhav Soni;Sukomal Pal	2018		10.1145/3184558.3191622	completeness (statistics);information retrieval;query expansion;word2vec;language model;relevance feedback;social media;microblogging;wordnet;computer science	ML	-28.663847375073356	-64.11680960508156	141929
9303255f863b1adb45fd35f6c99c7db2ad8b93eb	bursty event detection from microblog: a distributed and incremental approach	topic drifting;event detection;social network;期刊论文;temporal topic model	As a new form of social media, microblogs (e.g., Twitter and Weibo) are playing an important role in people’s daily life. With the rise in popularity and size of microblogs, there is a need for distributed approaches that can detect bursty event with low latency from the short-text data stream. In this paper, we propose a distributed and incremental temporal topic model for microblogs called Bursty Event dEtection (BEE+). BEE+ is able to detect bursty events from short-text dataset and model the temporal information. And BEE+ processes the post-stream incrementally to track the topic drifting of events over time. Therefore, the latent semantic indices are preserved from one time period to the next. In order to achieve real-time processing, we design a distributed execution framework based on Spark engine. To verify its ability to detect bursty event, we conduct experiments on a Weibo dataset of 6,360,125 posts. The results show that BEE+ can outperform the baselines for detecting the meaningful bursty events and track the topic drifting. Copyright © 2015 John Wiley & Sons, Ltd.	baseline (configuration management);burst transmission;experiment;john d. wiley;real-time clock;semantic web;sensor;social media;text corpus;topic model	Jianxin Li;Jianfeng Wen;Zhenying Tai;Richong Zhang;Weiren Yu	2016	Concurrency and Computation: Practice and Experience	10.1002/cpe.3657	data science;pattern recognition;data mining;social network	NLP	-23.72962662517875	-52.29365408955841	142399
2b0d2b5a536e2eaa6a1cdab6f1e8d11f602abdab	annotating images with suggestions - user study of a tagging system	human assisted learning;user interface;image classification;restricted boltzmann machine;image tagging;crowdsourcing	This paper explores the concept of image-wise tagging. It introduces a web-based user interface for image annotation, and a novel method for modeling dependencies of tags using Restricted Boltzmann Machines which is able to suggest probable tags for an image based on previously assigned tags. According to our user study, our tag suggestion methods improve both user experience and annotation speed. Our results demonstrate that large datasets with semantic labels (such as in TRECVID Semantic Indexing) can be annotated much more efficiently with the proposed approach than with current class-domain-wise methods, and produce higher quality data.	automatic image annotation;computer vision;database;experiment;feature (computer vision);feature extraction;mpeg media transport;marginal model;restricted boltzmann machine;usability testing;user experience;user interface;web application	Michal Hradis;Ales Láník;Jirí Král;Pavel Zemcík;Pavel Smrz	2012		10.1007/978-3-642-33140-4_14	contextual image classification;computer science;machine learning;data mining;restricted boltzmann machine;user interface;world wide web;crowdsourcing;information retrieval	AI	-27.77461796858183	-52.10856597676824	142848
8815ca2f4d48a7dfd553dcad51314604c05f246b	an enhanced feature extraction technique for improving sentiment analysis in arabic language	social network services;support vector machines;feature extraction;classification algorithms;sentiment analysis;merging;book reviews	Sentiment analysis (SA) is a modern text mining disciplinary that gained notable position due its various application in social networks (SN) and many internet domains. Since, it is used for discovering audience directions, and impressions about products or any subjects discussed in the internet via social media. Personal opinions availability in SN gave SA a significant attention to discussions makers on modern corporation. In this paper, we uses machine learning techniques via many features and same corpus states in Arabic language, comparing their results, and illustrating the significance of terms merging and pruning in various figures, which helps in the field of SA performance increasing purposes.	feature extraction;internet;machine learning;sentiment analysis;social media;social network;text mining	Fahd Alqasemi;Amira Abdelwahab;Hatem M. Abdelkader	2016	2016 4th IEEE International Colloquium on Information Science and Technology (CiSt)	10.1109/CIST.2016.7805075	natural language processing;speech recognition;computer science;data mining;sentiment analysis	DB	-22.11896084102597	-57.6357831966359	142860
4a18b7c3c285d9e7fc120256b7d31b23ff29e66e	event recognition on news stories and semi-automatic population of an ontology	event recognition;topology;cybernetics;ontologies data mining machine learning topology robustness artificial intelligence cybernetics electronic equipment testing life testing system testing;information extraction;simulation;electronic equipment testing;data mining;greedydual;learning technology;algorithm;machine learning;life testing;cache replacement;system testing;artificial intelligence;robustness;ontologies;web caching	This paper describes a system which recognizes events on news stories. Our system classifies stories and populates a hand-crafted ontology with new instances of classes defined in it. Currently, our system recognizes events which can be classified as belonging to a single category and it also recognizes overlapping events within one article (more than one event is recognized). In each case, the system provides a confidence value associated to the suggested classification. Our system uses Information Extraction and Machine Learning technologies. The system was tested using a corpus of 200 news articles from an archive of electronic news stories describing the academic life of the Knowledge Media (KMi). In particular, these news stories describe events such as a project award, publications, visits, etc.)	archive;information extraction;machine learning;population;semiconductor industry;text corpus	Maria Vargas-Vera;David Celjuska	2004	IEEE/WIC/ACM International Conference on Web Intelligence (WI'04)	10.1109/WI.2004.65	cybernetics;computer science;ontology;artificial intelligence;data science;machine learning;data mining;system testing;world wide web;information extraction;robustness	Web+IR	-28.488726170775777	-65.48583552262807	143028
3e455e84368643f00cadc94c109afe95b2cbbd45	an explorative association-based search for the semantic web	keyword based searching method;information resource;web pages;spreading activation paradigm explorative association based search semantic web web information keyword based searching method search approach ontologies information retrieval information resource semantic association search system web pages exploration mechanism;semantic web information retrieval ontologies artificial intelligence;information retrieval;search approach;semantics;frequency measurement;spreading activation paradigm;ontologies artificial intelligence;semantic information retrieval;web information;spreading activation semantic association based search system semantic web ontology semantic information retrieval;semantic association based search system;semantics semantic web ontologies frequency measurement web pages joining processes;exploration mechanism;joining processes;spreading activation;semantic web;ontologies;semantic association search system;ontology;explorative association based search	Due to the explosive growth of the amount of Web information, the effectiveness of keyword-based searching methods appears to reach a limit. One major reason is that the mixture of content and presentation information hinders machines in understanding the context of Web information and as a result, the performance of the existing search approaches degenerates. To address this challenge, Tim Berners-Lee of W3C envisioned the Semantic Web. In the Semantic Web, the meaning (semantics) of each term of Web information is defined based on ontologies, thus machines are able to retrieve information that is semantically associated with resources containing input keywords. In this paper, we propose the semantic association search system (SASS), which takes into account the associations between resources (web pages) as well as keywords. To achieve this goal, we first created metrics to evaluate the relative importance of each association between resources, and then developed an exploration mechanism based on the spreading activation paradigm to follow the relevant paths of such associations. Demonstrative cases were tested to validate our approach, and the results showed the effectiveness and great potential of our approach.	display resolution;information retrieval;ontology (information science);programming paradigm;scalability;self-information;semantic web;sensitivity and specificity;spreading activation;web page;world wide web	Myungjin Lee;Wooju Kim;Taehyung George Wang	2010	2010 IEEE Fourth International Conference on Semantic Computing	10.1109/ICSC.2010.17	web modeling;data web;semantic search;web standards;computer science;ontology;artificial intelligence;semantic web;ontology;social semantic web;web page;data mining;semantic web stack;database;semantics;linguistics;spreading activation;web intelligence;world wide web;information retrieval;semantic analytics	DB	-29.69756755684327	-57.231931817260616	143319
66d3ae0ca99af10279ad93cc875297ddc93058c8	logmap family participation in the oaei 2016		We present the participation of LogMap and its variants in the OAEI 2018 campaign. The LogMap project started in January 2011 with the objective of developing a scalable and logic-based ontology matching system. This is our eight participation in the OAEI and the experience has so far been very positive. LogMap is one of the few systems that participates in (almost) all OAEI tracks. 1 Presentation of the system LogMap [11, 13] is a highly scalable ontology matching system that implements the consistency and locality principles [12]. LogMap also supports (real-time) user interaction during the matching process, which is essential for use cases requiring very accurate mappings. LogMap is one of the few ontology matching system that (i) can efficiently match semantically rich ontologies containing tens (and even hundreds) of thousands of classes, (ii) incorporates sophisticated reasoning and repair techniques to minimise the number of logical inconsistencies, and (iii) provides support for user intervention during the matching process. LogMap relies on the following elements, which are keys to its favourable scalability behaviour (see [11, 13] for details). Lexical indexation. An inverted index is used to store the lexical information contained in the input ontologies. This index is the key to efficiently computing an initial set of mappings of manageable size. Similar indexes have been successfully used in information retrieval and search engine technologies [2]. Logic-based module extraction. The practical feasibility of unsatisfiability detection and repair critically depends on the size of the input ontologies. To reduce the size of the problem, we exploit ontology modularisation techniques. Ontology modules with well-understood semantic properties can be efficiently computed and are typically much smaller than the input ontology (e.g. [5]). Propositional Horn reasoning. The relevant modules in the input ontologies together with (a subset of) the candidate mappings are encoded in LogMap using a Horn propositional representation. Furthermore, LogMap implements the classic Dowling-Gallier algorithm for propositional Horn satisfiability [6]. Such encoding, although incomplete, allows LogMap to detect unsatisfiable classes soundly and efficiently. Axiom tracking. LogMap extends Dowling-Gallier’s algorithm to track all mappings that may be involved in the unsatisfiability of a class. This extension is key to implementing a highly scalable repair algorithm. Local repair. LogMap performs a greedy local repair; that is, it repairs unsatisfiabilities on-the-fly and only looks for the first available repair plan. Semantic indexation. The Horn propositional representation of the ontology modules and the mappings is efficiently indexed using an interval labelling schema [1] — an optimised data structure for storing directed acyclic graphs (DAGs) that significantly reduces the cost of answering taxonomic queries [4, 19]. In particular, this semantic index allows us to answer many entailment queries as an index lookup operation over the input ontologies and the mappings computed thus far, and hence without the need for reasoning. The semantic index complements the use of the propositional encoding to detect and repair unsatisfiable classes. 1.1 LogMap variants in the 2018 campaign As in previous campaigns, in the OAEI 2018 we have participated with two additional variants: LogMapLt is a “lightweight” variant of LogMap, which essentially only applies (efficient) string matching techniques. LogMapBio includes an extension to use BioPortal [8, 9] as a (dynamic) provider of mediating ontologies instead of relying on a few preselected ontologies [3]. In previous years we also participated with LogMapC5. 1.2 Adaptations made for the 2018 evaluation LogMap’s algorithm described in [11, 13, 16, 15, 14] has been adapted with the following new functionalities: i HOBBIT adaptation. We have implemented the required interface classes to run LogMap under the HOBBIT platform.6 LogMap can currently be evaluated in five different tracks available in the HOBBIT platform.7 ii Ontology division module. This module extends LogMap’s ontology overlapping estimation module to compute a number of divisions of the input ontologies and to create a set of smaller matching subtasks [10]. iii Obsolete classes. We have extended the lexical and structural indexation modules to ignore classes in the ontology annotated as obsolete. 1.3 Link to the system and parameters file LogMap is open-source and released under GNU Lesser General Public License 3.0.8 LogMap components and source code are available from the LogMap’s GitHub page: https://github.com/ernestojimenezruiz/logmap-matcher/. 5 LogMapC is a variant of LogMap which, in addition to the consistency and locality principles, also implements the conservativity principle (see details in [20–22, 18]). 6 https://gitlab.com/ernesto.jimenez.ruiz/logmap-hobbit 7 https://git.project-hobbit.eu/ernestoj/logmapsystem 8 http://www.gnu.org/licenses/ LogMap distributions can be easily customized through a configuration file containing the matching parameters. LogMap, including support for interactive ontology matching, can also be used directly through an AJAX-based Web interface: http://krrwebtools.cs.ox. ac.uk/. This interface has been very well received by the community since it was deployed in 2012. More than 3,000 requests coming from a broad range of users have been processed so far. 1.4 LogMap as a mapping repair system Only a very few systems participating in the OAEI competition implement repair techniques. As a result, existing matching systems (even those that typically achieve very high precision scores) compute mappings that lead in many cases to a large number of unsatisfiable classes. We believe that these systems could significantly improve their output if they were to implement repair techniques similar to those available in LogMap. Therefore, with the goal of providing a useful service to the community, we have made LogMap’s ontology repair module (LogMap-Repair) available as a self-contained software component that can be seamlessly integrated in most existing ontology matching systems [17, 7]. 1.5 LogMap as a matching task division system LogMap also includes a novel module to divide the ontology alignment task into (independent) manageable subtasks [10]. This component relies on LogMap’s lexical index, a neural embedding model [23] and locality-based modules [5]. This module can be integrated in existing ontology alignment systems as a external module. The preliminaty results in [10] are encouraging as the division enabled systems to complete some large-scale matching tasks. 2 General comments and conclusions Please refer to http://oaei.ontologymatching.org/2018/results/ for the results of the LogMap family in the OAEI 2018 campaign. 2.1 Comments on the results As in previous campaigns, LogMap has been one of the top systems and one of the few systems that participates in (almost) all tracks. Furthermore, it has also been one of the few systems implementing repair techniques and providing (almost) coherent mappings in all tracks. LogMap’s main weakness is that the computation of candidate mappings is based on the similarities between the vocabularies of the input ontologies; hence, in the cases where the ontologies are lexically disparate or do not provide enough lexical information LogMap is at a disadvantage.	ajax (programming);coherence (physics);component-based software engineering;computation;data structure;directed acyclic graph;gnu;greedy algorithm;horn-satisfiability;information retrieval;inverted index;locality of reference;lookup table;mike lesser;ontology (information science);ontology alignment;open-source software;real-time clock;scalability;string searching algorithm;vocabulary;web search engine	Ernesto Jiménez-Ruiz;Bernardo Cuenca Grau;Valerie V. Cross	2016			computer science	Web+IR	-32.534302742078474	-65.92383208521282	143320
d7a63e565058a187a534e2b445d0716989df543b	a new method for generating the chinese news summary based on fuzzy reasoning and domain ontology	fuzzy reasoning;fuzzy sets;chinese news summary;domain ontology	This paper presents a new method for automatically generating the Chinese weather news summary based on fuzzy reasoning and the domain ontology, where the weather ontology, the time ontology and the geography ontology are predefined by domain experts. The summary is composed of candidate sentences which have higher scores, where the experimental data are adopted from the Chinese weather news website of Taiwan. The experimental results show that the proposed method outperforms the methods presented in [9] and [10] for automatically generating the Chinese news summary.	ontology (information science);web ontology language	Shyi-Ming Chen;Ming-Hung Huang	2013		10.1007/978-3-642-36546-1_8	upper ontology;computer science;ontology;artificial intelligence;data mining;database;fuzzy set;ontology-based data integration;information retrieval;suggested upper merged ontology	AI	-28.903192200853507	-65.76812653010262	143470
c87f613c3a373087405bcd4902f946049fdad0b5	a self-organising map approach for clustering of xml documents	unsupervised learning;xml document handling self organising feature maps;document handling;cluster structured data self organising map approach xml documents clustering internet unsupervised learning approach;self organising feature maps;tree structure;xml;xml document;xml machine learning internet unsupervised learning html search engines neural networks data mining humans feeds;bag of words;structural properties;self organising map;structured data	"""The number of XML documents produced and available on the Internet is steadily increasing. It is thus important to devise automatic procedures to extract useful information from them with little or no intervention by a human operator. In this paper, we investigate the efficacy of an unsupervised learning approach, namely self-organising maps (SOMs), for the automatic clustering of XML documents. Specifically, we consider a relatively large corpus of XML formatted data from the INEX initiative and evaluate it using two different self-organising map models. The first model is the classical SOM model, and it requires the XML documents to be represented by real-valued vectors, obtained using a """"bag of words"""" (or better a """"bag of tags"""") approach. The other model is the SOM for structured data (SOM-SD) approach which is able to cluster structured data, and it is possible to feed the model with tree structured representations of the XML documents, thus explicitly preserving the structural information in the documents. The experimental results show that the SOM model exhibits quite a poor performance on this problem domain which requires the ability to encode structural properties of the data. The SOM-SD model, on the other hand, is able to produce a good clustering and generalization performance."""	artificial neural network;bag-of-words model;cluster analysis;encode;formal proof;graph (abstract data type);internet;problem domain;self-organization;self-organizing map;test set;time complexity;unsupervised learning;vii;xml	Francesca Trentini;Markus Hagenbuchner;Alessandro Sperduti;Franco Scarselli	2006	The 2006 IEEE International Joint Conference on Neural Network Proceedings	10.1109/IJCNN.2006.246898	well-formed document;unsupervised learning;xml validation;binary xml;simple api for xml;xml;xml schema;computer science;document structure description;machine learning;xml framework;data mining;xml database;xml schema;database;information retrieval;efficient xml interchange	DB	-25.15739539243826	-64.97739809686384	143786
40e9c00ff495478fe59a460c7cd3cc4c2f8af62b	beyond independent relevance: methods and evaluation metrics for subtopic retrieval	evaluation metrics;baseline relevance ranking;query likelihood relevance ranking;independent relevance;traditional retrieval method;maximal marginal relevance;ranking strategy;subtopic retrieval problem;non-traditional retrieval problem;subtopic retrieval;intrinsic topic difficulty;language models;mixture model;language model	We present a non-traditional retrieval problem we call subtopic retrieval. The subtopic retrieval problem is concerned with finding documents that cover many different subtopics of a query topic. In such a problem, the utility of a document in a ranking is dependent on other documents in the ranking, violating the assumption of independent relevance which is assumed in most traditional retrieval methods. Subtopic retrieval poses challenges for evaluating performance, as well as for developing effective algorithms. We propose a framework for evaluating subtopic retrieval which generalizes the traditional precision and recall metrics by accounting for intrinsic topic difficulty as well as redundancy in documents. We propose and systematically evaluate several methods for performing subtopic retrieval using statistical language models and a maximal marginal relevance (MMR) ranking strategy. A mixture model combined with query likelihood relevance ranking is shown to modestly outperform a baseline relevance ranking on a data set used in the TREC interactive track.	algorithm;baseline (configuration management);information retrieval;language model;marginal model;maximal set;meet-me room;mixture model;precision and recall;relevance;text retrieval conference;utility	ChengXiang Zhai;William W. Cohen;John D. Lafferty	2003		10.1145/860435.860440	ranking;relevance;computer science;pattern recognition;mixture model;data mining;information retrieval;language model	Web+IR	-27.602876147513193	-62.67342199032549	143791
22f4436b0e748b297ffb420d51a45a40b37e3602	chinese web comments clustering analysis with a two-phase method	connected graph model chinese web comments clustering analysis blog comment web comment two phase clustering algorithm synonymous comments combination;graph theory;cluster algorithm;pattern clustering;web pages;connected graph model;synonymous comments combination;information services;data mining;chinese web comments clustering analysis;dairy products;web comment;cluster analysis;internet;clustering algorithms clustering methods information services web sites internet web pages fuzzy systems machine intelligence web mining yarn;clustering method;merging;clustering algorithms;book reviews;graph model;pattern clustering graph theory information analysis internet;information analysis;blog comment;two phase clustering algorithm	Usually a meaningful web topic has tens of thousands of comments, especially the hot topics. It is valuable if we congregate the comments into clusters and find out the mainstreams. However, such analysis has two difficulties. First, there is no explicit link relationship between web comments just like those among web pages or Blog comments. The other problem is, most of the comments are very short, even one or two words. Therefore the traditional clustering algorithms such as CURE and DBSCAN cannot work if applied to these comments directly. In this paper we propose a two-phase algorithm, which will first combine the highly synonymous comments into a longer one based on a connected graph model, and then apply the improved clustering methods to the new collections. Experimental results on two real data sets show that our algorithm performs better than traditional algorithms such as CURE.	algorithm;blog;cluster analysis;comment (computer programming);connectivity (graph theory);dbscan;two-phase commit protocol;two-phase locking;web page	Yexin Wang;Lu Zhao;Yan Zhang	2009	2009 Sixth International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2009.560	computer science;graph theory;data science;machine learning;data mining;cluster analysis;world wide web	DB	-26.574930906827838	-56.38844426525482	143802
24c777268dcf274d9fdcc59716c1dd5a5587e19b	detecting task-based query sessions using collaborative knowledge	silicon;cluster algorithm;distance function;pattern clustering;task based query session detection;wikipedia;centroid based clustering algorithm;collaborative knowledge;query log session breaking;search engines;semantics silicon internet encyclopedias electronic publishing clustering algorithms;query formulation;semantics;query inter arrival times;density based clustering algorithm;web sites internet pattern clustering query formulation search engines;web search engine;query lexical content;internet;wikipedia task based query session detection collaborative knowledge web search engine centroid based clustering algorithm density based clustering algorithm query inter arrival times distance function query lexical content wiktionary;web sites;clustering algorithms;web data mining;query clustering web data mining query log session breaking task based session;electronic publishing;encyclopedias;query clustering;query logs;wiktionary;task based session	Our research challenge is to provide a mechanism for splitting into user task-based sessions a long-term log of queries submitted to a Web Search Engine (WSE). The hypothesis is that some query sessions entail the concept of user task. We present an approach that relies on a centroid-based and a density-based clustering algorithm, which consider queries inter-arrival times and use a novel distance function that takes care of query lexical content and exploits the collaborative knowledge collected by Wiktionary and Wikipedia.	algorithm;care-of address;cluster analysis;performance;preprocessor;sensor;web services enhancements;web search engine;wikipedia	Claudio Lucchese;Salvatore Orlando;Raffaele Perego;Fabrizio Silvestri;Gabriele Tolomei	2010	2010 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology	10.1109/WI-IAT.2010.281	web query classification;the internet;web search engine;metric;computer science;data mining;brand;database;semantics;cluster analysis;electronic publishing;silicon;web search query;world wide web;information retrieval;encyclopedia	Robotics	-26.864907979577808	-64.95157121122334	143964
5d19b5f90a3449e675a8bb3986dff1cac183aa77	plagiarism and authorship analysis: introduction to the special issue	special issue;authorship analysis	The Internet has facilitated both the dissemination of anonymous texts as well as easy ‘‘borrowing’’ of ideas and words of others. This has raised a number of important questions regarding authorship. Can we identify the anonymous author of a text by comparing the text with the writings of known authors? Can we determine if a text, or parts of it, has been plagiarized? Such questions are clearly of both academic and commercial importance. The task of determining or verifying the authorship of an anonymous text based solely on internal evidence is a very old one, dating back at least to the medieval scholastics, for whom the reliable attribution of a given text to a known ancient authority was essential to determining the text’s veracity. More recently, the problem of authorship attribution has gained greater prominence due to new applications in forensic analysis, humanities scholarship, and electronic commerce, and the development of computational methods for addressing the problem. Over the last century and more, a great variety of methods have been applied to authorship attribution problems of various sorts. One can roughly trace the evolution of methods through three main stages. In the earliest stage researchers sought a single numeric function of a text to discriminate between authors. In a later stage, statistical multivariate discriminant analysis was applied to word frequencies and related numerical features. Most recently, machine learning methods and highdimensional textual features have been applied to sets of training documents to	computation;e-commerce;internet;linear discriminant analysis;machine learning;numerical analysis;statistical model;stylometry;text-based (computing);veracity;verification and validation;word lists by frequency	Efstathios Stamatatos;Moshe Koppel	2011	Language Resources and Evaluation	10.1007/s10579-011-9136-1	scholarship;artificial intelligence;natural language processing;the internet;attribution;data mining;computer science;plagiarism detection;word lists by frequency	Web+IR	-19.939071313107718	-57.26471594495985	144147
6bf0a4421f6b82cd92be0c796d279e9b1544b7f3	mixing crowdsourcing and graph propagation to build a sentiment lexicon: feelings are contagious		This paper describes a method for building a sentiment lexicon. Its originality is to combine crowdsourcing via a Game With A Purpose (GWAP) with automated propagation of sentiments through a spreading algorithm, both using the lexical JeuxDeMots network as data source and substratum. We present the game designed to collect sentiment data, and the principles and assumptions underlying the action of the algorithm that propagates them within the network. Finally, we give a qualitative evaluation of the data obtained for both the game and the spreading done by the algorithm.	crowdsourcing;lexicon;software propagation	Mathieu Lafourcade;Nathalie Le Brun;Alain Joubert	2016		10.1007/978-3-319-41754-7_23	natural language processing;computer science;data mining	Web+IR	-22.45600538026455	-54.356755033111206	144421
660e01a53591e994f6249df612768472373edb75	rule based temporal inference		Time-wise knowledge is relevant in knowledge graphs as the majority facts are true in some time period, for instance, (Barack Obama, president of, USA, 2009, 2017). Consequently, temporal information extraction and temporal scoping of facts in knowledge graphs have been a focus of recent research. Due to this, a number of temporal knowledge graphs have become available such as YAGO and Wikidata. In addition, since the temporal facts are obtained from open text, they can be weighted, i.e., the extraction tools assign each fact with a confidence score indicating how likely that fact is to be true. Temporal facts coupled with confidence scores result in a probabilistic temporal knowledge graph. In such a graph, probabilistic query evaluation (marginal inference) and computing most probable explanations (MPE inference) are fundamental problems. In addition, in these problems temporal coalescing, an important research in temporal databases, is very challenging. In this work, we study these problems by using probabilistic programming. We report experimental results comparing the efficiency of several state-of-the-art systems. 1998 ACM Subject Classification D.1.6 Logic Programming	inference engine;information extraction;knowledge graph;logic programming;marginal model;scope (computer science);temporal database;wikidata;yago	Melisachew Wudage Chekol;Heiner Stuckenschmidt	2017		10.4230/OASIcs.ICLP.2017.4	information extraction;open text;computer science;theoretical computer science;machine learning;rule-based system;temporal database;probabilistic logic;inference;artificial intelligence;graph	AI	-28.23134486352486	-65.24919125198109	144622
47949f24756646e124090acd7d7dc2500e0a599e	document ranking with citation information and oversampling sentence classification in the luima framework			oversampling;ranking (information retrieval)	Apoorva Bansal;Zheyuan Bu;Biswajeet Mishra;Silun Wang;Kevin D. Ashley;Matthias Grabmair	2016		10.3233/978-1-61499-726-9-33	data mining;oversampling;information retrieval;citation;computer science;ranking;sentence	NLP	-25.027130835964094	-61.97024690694548	144784
266bfeac8b8e00f063473ecfee120c8e81e45d39	a framework for identifying textual redundancy	approximate algorithm;set cover	The task of identifying redundant information in documents that are generated from multiple sources provides a significant challenge for summarization and QA systems. Traditional clustering techniques detect redundancy at the sentential level and do not guarantee the preservation of all information within the document. We discuss an algorithm that generates a novel graph-based representation for a document and then utilizes a set cover approximation algorithm to remove redundant text from it. Our experiments show that this approach offers a significant performance advantage over clustering when evaluated over an annotated dataset.	approximation algorithm;automatic summarization;cluster analysis;experiment;set cover problem;triple modular redundancy	Kapil Thadani;Kathleen McKeown	2008		10.3115/1599081.1599191	computer science;machine learning;pattern recognition;data mining;set cover problem	NLP	-26.482689661020235	-63.520467856198486	144954
462d821d550a0c8bc8773515f206663d2f33c9a9	a comparative study on two large-scale hierarchical text classification tasks' solutions	nearest neighbor searches;hierarchical svm;learning algorithms;patent classification;ranking approach large scale text classification hierarchical text classification text classification comparative study k nn hierarchical svm similarity measure;learning algorithm;hierarchical text classification;support vector machines;training;bm25;text analysis;feature selection strategies;text classification;large scale;listweak;listweak large scale hierarchical text classification tasks patent classification learning algorithms feature selection strategies text categorization hierarchical support vector machine k nearest neighbor bm25;patents;k nn;large scale hierarchical text classification tasks;large scale text classification;classification algorithms;taxonomy;comparative study;pattern classification;text analysis learning artificial intelligence patents pattern classification support vector machines;k nearest neighbor;feature selection;ranking approach;patents support vector machines text categorization nearest neighbor searches training classification algorithms taxonomy;support vector machine;learning artificial intelligence;similarity measure;hierarchical support vector machine;text categorization	Patent classification is a large scale hierarchical text classification (LSHTC) task. Though comprehensive comparisons, either learning algorithms or feature selection strategies, have been fully made in the text categorization field, few work was done for a LSHTC task due to high computational cost and complicated structural label characteristics. For the first time, this paper compares two popular learning frameworks, namely hierarchical support vector machine (SVM) and k nearest neighbor (k-NN) that are applied to a LSHTC task. Experiment results show that the latter outperforms the former in this LSHTC task, which is quite different from the usual results for normal text categorization tasks. Then this paper does a comparative study on different similarity measures and ranking approaches in k-NN framework for LSHTC task. Conclusions can be drawn that k-NN is more appropriate for the LSHTC task than hierarchical SVM and for a specific LSHTC task. BM25 outperforms other similarity measures and List Weak gains a better performance than other ranking approaches. We also find an interesting phenomenon that using all the labels of the retrieved neighbors can remarkably improve classification performance over only using the first label of the retrieved neighbors.	categorization;computational complexity theory;document classification;feature selection;k-nearest neighbors algorithm;machine learning;statistical classification;support vector machine	Jian Zhang;Hai Zhao;Bao-Liang Lu	2010	2010 International Conference on Machine Learning and Cybernetics	10.1109/ICMLC.2010.5580696	support vector machine;computer science;machine learning;pattern recognition;data mining;feature selection;taxonomy	ML	-20.825296163815842	-63.673355966440866	145045
bd0cd1505b68b827022fbdfae8e3a5d836566b48	improving biomedical information retrieval by linear combinations of different query expansion techniques	computational biology bioinformatics;algorithms;computer appl in life sciences;microarrays;bioinformatics	Biomedical literature retrieval is becoming increasingly complex, and there is a fundamental need for advanced information retrieval systems. Information Retrieval (IR) programs scour unstructured materials such as text documents in large reserves of data that are usually stored on computers. IR is related to the representation, storage, and organization of information items, as well as to access. In IR one of the main problems is to determine which documents are relevant and which are not to the user’s needs. Under the current regime, users cannot precisely construct queries in an accurate way to retrieve particular pieces of data from large reserves of data. Basic information retrieval systems are producing low-quality search results. In our proposed system for this paper we present a new technique to refine Information Retrieval searches to better represent the user’s information need in order to enhance the performance of information retrieval by using different query expansion techniques and apply a linear combinations between them, where the combinations was linearly between two expansion results at one time. Query expansions expand the search query, for example, by finding synonyms and reweighting original terms. They provide significantly more focused, particularized search results than do basic search queries. The retrieval performance is measured by some variants of MAP (Mean Average Precision) and according to our experimental results, the combination of best results of query expansion is enhanced the retrieved documents and outperforms our baseline by 21.06 %, even it outperforms a previous study by 7.12 %. We propose several query expansion techniques and their combinations (linearly) to make user queries more cognizable to search engines and to produce higher-quality search results.	baseline (configuration management);computer;computers;content-based image retrieval;hl7publishingsubsection <query>;information needs;information retrieval;knowledge organization;query expansion;question (inquiry);scour inc.;web search engine;web search query	Ahmed AbdoAziz Ahmed Abdulla;Hongfei Lin;Bo Xu;Santosh Kumar Banbhrani	2016		10.1186/s12859-016-1092-8	document retrieval;biology;query expansion;ranking;dna microarray;relevance;cognitive models of information retrieval;computer science;bioinformatics;theoretical computer science;concept search;data mining;adversarial information retrieval;term discrimination;data retrieval;information retrieval;query language;search engine;human–computer information retrieval	Web+IR	-31.299904107068716	-59.0485963111864	145078
85480c71a07d0fec237451411ef9124c5a2d6630	mapping knowledge domains of chinese digital library research output, 1994–2010	91c15;digital libraries library research;social network analysis;co word analysis;mapping knowledge domains	The aim of this paper is to identify the research paradigms on digital libraries in China while compared with that of international digital libraries research via scientometric analysis. Co-word network constructed by keywords in documents and their co-occurrence relationships is a kind of mapping knowledge domains, which represents the cognitive and intellectual structure of science. A total of 6068 and 1250 papers published between 1994 and 2010 were, respectively retrieved from the China National Knowledge Infrastructure (CNKI) and ScienceDirect databases with a topic search of digital libraries or digital library in abstracts of papers. This paper uses methods of co-word analysis, social network analysis and mapping knowledge domains as theory basis, with assistance of softwares of UCINET and Netdraw, to construct the co-word network of digital libraries/library research in China, present the study status quo and evolution on digital libraries/library in China and analyze the research paradigm structure of digital libraries/library in China.	database;digital library;library (computing);programming paradigm;scientometrics;social network analysis	Limei Zhao;Qingpu Zhang	2011	Scientometrics	10.1007/s11192-011-0428-4	social network analysis;digital library;social science;computer science;data science;world wide web	AI	-26.200625362535863	-57.75569722233426	145177
28882c8584dd6dc04f1f86b2b7d395314e3ff838	tweetspector: entity-based retrieval of tweets	keywords;real time;profiles;user feedback;profile;disambiguation;entity;twitter;demo;company	TweetSpector is a tool for demonstrating entity-based of retrieval of tweets. The various features of this tool include: entity profile creation, real-time tweet classification, active improvement of the created profiles through user feedback, and the dashboard displaying different metrics.	real-time clock	Surender Reddy Yerva;Zoltán Miklós;Flavia Grosan;Alexandru Tandrau;Karl Aberer	2012		10.1145/2348283.2348441	computer science;data mining;entity;world wide web;information retrieval	ML	-25.881459430798017	-52.874130119416556	145305
a0e5979e85cb8a2ee5038ada418d5f21550d0147	on using the normalized compression distance to cluster web search results	web search	Current Web search engines return long lists of ranked documents that users are forced to sift through to find relevant documents. This paper introduces a new approach for clustering Web search results, based on the notion of clustering by compression. Compression algorithms allow defining a similarity measure based on the degree of common information. Classification methods allow clustering similar data without any previous knowledge. The clustering by compression procedure is based on a parameter-free, universal, similarity distance, the normalized compression distance or NCD, computed from the lengths of compressed data files. Our goal is to apply the clustering by compression algorithm in order to cluster the documents returned by a Web search engine in response to a user query.	algorithm;cluster analysis;data compression;network computing devices;preprocessor;relevance;semantic similarity;similarity measure;upgma;web page;web search engine;world wide web;bzip2	Alexandra Cernian;Liliana Dobrica;Dorin Carstoiu;Valentin Sgarciu	2010			computer science	ML	-29.125774637042273	-57.9009151312918	145471
8c8602b22e965cd6ce027568fce02f530401e7ae	emotion artificial intelligence derived from ensemble learning		We present in this work a predictive analytics framework that can computationally identify and categorize opinions expressed in text to discover and analyze attitudes towards a particular topic or product. We provide a new approach based on an ensemble model of three widely used sentiment analysis algorithms: TextBlob, OpinionFinder and Stanford NLP. In this work we investigated the performance of these latter algorithms on large, real datasets. Then, we designed two ensembles (1) one based on multivariate regression that computes a final prediction from three classification algorithms and (2) an ensemble that is based on majority rule. We computed the accuracy of the ensemble framework on labeled real datasets used in the literature that include tweets, as well as Amazon, Yelp and IMDb movie reviews. Our experiments indicated that the ensemble algorithms outperformed all three sentiment algorithms. The ensemble learning algorithm draws from the strengths of the individual sentiment algorithms, avoiding the need to select just one algorithm, creating a stronger tool for harnessing Emotion AI. This approach creates promises beyond the tweets and reviews analyzed here and can potentially be applied to marketing, finance, politics, and beyond.	algorithm;artificial intelligence;bio-inspired computing;categorization;emoji;ensemble learning;experiment;feature engineering;general linear model;laptop;natural language processing;predictive modelling;sentiment analysis	Anasse Bari;Goktug Saatcioglu	2018	2018 17th IEEE International Conference On Trust, Security And Privacy In Computing And Communications/ 12th IEEE International Conference On Big Data Science And Engineering (TrustCom/BigDataSE)	10.1109/TrustCom/BigDataSE.2018.00266	predictive analytics;computer network;sentiment analysis;categorization;multivariate statistics;computer science;ensemble learning;statistical classification;machine learning;text mining;ensemble forecasting;artificial intelligence	Robotics	-19.87966644318875	-53.280305921644135	145623
3d454d230f04e396d8d5379a2621689793157cb7	kb video retrieval at trecvid 2010		This paper describes KB Video Retrieval's participation in the TREC Video Retrieval Evaluation for 2010. This year we submitted results for the Semantic Indexing, Known-item Search, Instance Search, and Event Detection in Internet Multimedia tasks. Our goal this year was to evaluate ranking strategies and expand our knowledge based approach to a variety of data sets and tasks.		David Etter	2010			computer science;multimedia;world wide web;information retrieval	Web+IR	-31.44410110892265	-62.6515012269849	145905
82beb5a6bf49701506d6028ee73afc3fa4368aaf	overlap between google and bing web search results!: twitter to the rescue?	google;search engine;search result comparison;social media search;web search;twitter;bing	Access to diverse perspectives nurtures an informed citizenry. Google and Bing have emerged as the duopoly that largely arbitrates which English language documents are seen by web searchers. We present our empirical study over the search results produced by Google and Bing that shows a large overlap. Thus, citizens may not gain different perspectives by simultaneously probing them for the same query. Fortunately, our study also shows that by mining Twitter data one can obtain search results that are quite distinct from those produced by Google and Bing. Additionally, the users found those results to be quite informative.	information;web search engine	Rakesh Agrawal;Behzad Golshan;Evangelos E. Papalexakis	2015		10.1145/2817946.2820604	google panda;google hacking;google penalty;google hummingbird;site map;organic search;web search engine;geography;search engine optimization;qwant;google penguin;meta element;advertising;internet privacy;world wide web;scraper site	ML	-32.875360812905214	-53.65678187272091	146261
28ac6725a89a0b9104c57be7506617fbb85678c0	i want what i need!: analyzing subjectivity of online forum threads	dialogue act;online forums;binary classification;subjectivity	Online forums have become a popular source of information due to the unique nature of information they contain. Internet users use these forums to get opinions of other people on issues and to find factual answers to specific questions. Topics discussed in online forum threads can be subjective seeking personal opinions or non-subjective seeking factual information. Hence, knowing subjectivity orientation of threads would help forum search engines to satisfy user's information needs more effectively by matching the subjectivities of user's query and topics discussed in the threads in addition to lexical match between the two. We study methods to analyze the subjectivity of online forum threads. Experimental results on a popular online forum demonstrate the effectiveness of our methods.	information needs;information source;internet;web search engine	Prakhar Biyani;Cornelia Caragea;Amit Singh;Prasenjit Mitra	2012		10.1145/2396761.2398675	binary classification;computer science;machine learning;data mining;subjectivity;multimedia;world wide web	Web+IR	-27.790045320411988	-53.21277214414086	146441
e6a92bf8ed82455b536bffacc7ce552136abdb21	information retrieval framework for technology survey in biomedical and chemistry literature		The Technology survey task deals with the retrieval of information that can best answer a scientific question. This task is more challenging in biomedical and chemistry domains due to diverse conventions applied for naming the entities. In order to address this challenge, the work reported here presents an ad-hoc retrieval task that has been evaluated during the TRECCHEM-2011 for its ability to support retrieval from the biomedical and chemistry literature. The core of the framework contains nearly 1.3 million patents and full-text articles that were indexed with pre-selected biomedical concepts. Altogether, four runs were submitted based on different query formulation strategies and they exhibited competitive results.	entity;hoc (programming language);information retrieval;yahoo! answers	Harsha Gurulingappa;Bernd Müller;Martin Hofmann-Apitius;Juliane Fluck	2011			information retrieval;data mining;human–computer information retrieval;computer science;data science	Web+IR	-32.017280268521205	-62.80615080508203	146532
89d0576315f7e8d859793b0ae0b9d1ab21602b84	an anew based fuzzy sentiment analysis model		Within the framework of the Intelligent Data Suite (IDS) that is being developed by the company Prometeus Global Solutions, there is a Sentiment Analysis and Opinion Mining module focused on detecting ‘dangerous’ (to the tool user company) messages on Social Media. This can be useful for sending ‘early warnings’ to alert tool user company analysts to take preventive measures against potentially harmful messages. In this paper, a brief description of IDS features, regarding tweets filtering and classification is firstly presented. Affective Norms for English Words (ANEW) provides a set of normative emotional ratings for a large number of words in English and three emotions (valence, arousal and dominance) measures for each term. It is used as a basis for describing a fuzzy model containing five categories for representing the opinion of a microblogging text (very negative, negative, neutral, positive and very positive). The proposal is implemented and tested on the IDS framework.	cluster analysis;genetic algorithm;prometheus;protologism;sensor;sentiment analysis;social media	Georges F. Félix;José Angel Olivas;Arturo Peralta;Francisco P. Romero;Jesús Serrano-Guerrero	2018	2018 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)	10.1109/FUZZ-IEEE.2018.8491492	sentiment analysis;fuzzy logic;machine learning;artificial intelligence;microblogging;normative;computer science;affect (psychology);social media	SE	-21.102244702290424	-56.98148188667948	146779
a27502e7d8ede81452e9d35c791a8bd6da1c93d8	tweeting disaster: hashtag constructions and collisions	information architecture;data stream;social web;word order;natural disaster;information design;communication design;twitter;hashtags;social media;disasters	In this paper, we describe the issues surrounding the use of various hashtags by Twitter users who are attempting to exchange information about recent natural disasters. During these disasters, hashtag usage was somewhat mired by inconsistent formats, spellings, and word ordering. This paper argues for systems that can help bridge this issue by creating participant-centered data streams that can collect and re-route these conversations.	hashtag	Liza Potts;Joyce Seitzinger;Dave Jones;Angela Harrison	2011		10.1145/2038476.2038522	word order;social web;disaster;social media;natural disaster;computer science;communication design;information design;internet privacy;world wide web;computer security;information architecture	AI	-22.36367435330512	-54.709664441304774	146903
42d6547636b3fd15201ac7e531791afe3f612764	(1+1)-evolutionary gradient strategy to evolve global term weights in information retrieval		In many contexts of Information Retrieval (IR), term weights play an important role in retrieving the relevant documents responding to users’ queries. The term weight measures the importance or the information content of a keyword existing in the documents in the IR system. The term weight can be divided into two parts, the Global Term Weight (GTW) and the Local Term Weight (LTW). The GTW is a value assigned to each index term to indicate the topic of the documents. It has the discrimination value of the term to discriminate between documents in the same collection. The LTW is a value that measures the contribution of the index term in the document. This paper proposes an approach, based on an evolutionary gradient strategy, for evolving the Global Term Weights (GTWs) of the collection and using Term Frequency-Average Term Occurrence (TF-ATO) as the Local Term Weights (LTWs). This approach reduces the problem size for the term weights evolution which reduces the computational time helping to achieve an improved IR effectiveness compared to other Evolutionary Computation (EC) approaches in the literature. The paper also investigates the limitation that the relevance judgment can have in this approach by conducting two sets of experiments, for partially and fully evolved GTWs. The proposed approach outperformed the Okapi BM25 and TF-ATO with DA weighting schemes methods in terms of Mean Average Precision (MAP), Average Precision (AP) and Normalized Discounted Cumulative Gain (NDCG). Osman Ali Sadek Ibrahim ASAP Research Group, School of Computer Science The University of Nottingham. CS Dept., Minia University, Al-Minya, Egypt. e-mail: psxoi@nottingham.ac.uk Dario Landa-Silva ASAP Research Group, School of Computer Science The University of Nottingham e-mail: dario.landasilva@nottingham.ac.uk	analysis of algorithms;archive;cs games;computer memory;computer science;email;evolutionary computation;experiment;gradient;information retrieval;machine learning;okapi bm25;osman yaşar;relevance;self-information;tf–idf;time complexity	Osman Ali Sadek Ibrahim;Dario Landa Silva	2016		10.1007/978-3-319-46562-3_25	machine learning;data mining;mathematics;information retrieval	Web+IR	-28.07859935775343	-60.370256721628344	146944
766093cbbd95127767c4a92ceeedb8bd901b21a2	search engine click spam detection based on bipartite graph propagation	label propagation;frequent sequential patterns;user session model;click spam	"""Using search engines to retrieve information has become an important part of people's daily lives. For most search engines, click information is an important factor in document ranking. As a result, some websites cheat to obtain a higher rank by fraudulently increasing clicks to their pages, which is referred to as """"Click Spam"""". Based on an analysis of the features of fraudulent clicks, a novel automatic click spam detection approach is proposed in this paper, which consists of 1. modeling user sessions with a triple sequence, which, to the best of our knowledge, takes into account not only the user action but also the action objective and the time interval between actions for the first time; 2. using the user-session bipartite graph propagation algorithm to take advantage of cheating users to find more cheating sessions; and 3. using the pattern-session bipartite graph propagation algorithm to obtain cheating session patterns to achieve higher precision and recall of click spam detection. Experimental results based on a Chinese commercial search engine using real-world log data containing approximately 80 million user clicks per day show that 2.6% of all clicks were detected as spam with a precision of up to 97%."""	algorithm;anti-spam techniques;click fraud;precision and recall;ranking (information retrieval);software propagation;spamming;web search engine	Xin Li;Min Zhang;Yiqun Liu;Shaoping Ma;Yijiang Jin;Liyun Ru	2014		10.1145/2556195.2556214	computer science;multimedia;internet privacy;world wide web	Web+IR	-24.925232846869164	-53.932476180082055	146999
28a9d9566b73740ab3f7c4a5edd4cbc2072e3b11	hierarchical incident ticket classification with minimal supervision	manuals;real world datasets hierarchical incident ticket text classification minimal supervision manual labelling good quality predictions two stage technique hierarchical clustering graph clustering community finding topic modelling active learning approach performance evaluation prediction quality prediction efficiency;text mining;manuals labeling clustering algorithms servers communities monitoring feature extraction;text mining multi class classification;servers;monitoring;multi class classification;feature extraction;clustering algorithms;communities;text analysis pattern classification pattern clustering;labeling	In this paper, we introduce a novel approach for incident ticket classification that aims at minimizing the manual labelling effort while achieving good-quality predictions. To accomplish this, we devise a two-stage technique that employs hierarchical clustering using a combination of graph clustering (community finding) and topic modelling as first stage, followed by either another round of hierarchical clustering or an active learning approach as second stage. We evaluate the performance of our method in terms of manual labelling effort, prediction quality and efficiency on three real-world datasets and demonstrate that classical approaches to text classification are not well suited for incident ticket texts.	cluster analysis;document classification;hierarchical clustering;topic model	Andrii Maksai;Jasmina Bogojeska;Dorothea Wiesmann	2014	2014 IEEE International Conference on Data Mining	10.1109/ICDM.2014.81	correlation clustering;labeling theory;text mining;fuzzy clustering;feature extraction;computer science;canopy clustering algorithm;machine learning;multiclass classification;consensus clustering;pattern recognition;data mining;cluster analysis;brown clustering;server;conceptual clustering	ML	-21.27089254805026	-61.48033878505779	147210
22f1b050cfe0b9a0bd1acbb0e8c02f845ec4b8dc	co-occurrence analysis focused on blogger communities	web pages;web mining web pages subspace blogger based cooccurrence analysis context sensitive problem;web pages principal component analysis performance analysis intelligent agent blogs web mining social network services intelligent networks communities technological innovation;data mining;blogger communities;indexes;internet;internet data mining;principal component analysis;contextually consistent web pages blogger based co occurrence analysis blogger communities;writing;production;communities;contextually consistent web pages;blogs;blogger based co occurrence analysis	We studied the problem of finding a subspace of Web pages that is contextually consistent for co-occurrence analysis. We looked at blogs and proposed blogger-based co-occurrence analysis, which assumes that two items are relevant to each other if they appear in any of the blog entries posted by the same blogger. We show that (1) blogger-based analysis outperforms conventional page-based analysis in solving context-sensitive problems and that (2) analysis focused on bloggers forming a community yields better performance compared with that focused on isolated bloggers.	blog;blogger;centrality;context-sensitive grammar;experiment	Shin-ya Sato;Kensuke Fukuda;Toshio Hirotsu;Satoshi Kurihara;Toshiharu Sugawara	2008	2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology	10.1109/WIIAT.2008.28	database index;the internet;computer science;web page;data mining;writing;world wide web;information retrieval;principal component analysis	Robotics	-26.134404132837165	-55.85316687181687	147223
2b81946ed04cd22f7f40099fa782e183c2ee6382	customer gender prediction based on e-commerce data	support vector machines;training;data mining;vegetation;internet;feature extraction;blogs	Demographic attributes of customers such as gender, age, etc. provide important information for e-commerce service providers in marketing and personalization of web applications. However, online customers often do not provide this kind of information due to privacy issues and other security-related reasons. In this paper, we proposed a method for predicting the gender of customers based on their catalog viewing data on e-commerce systems, such as the date and time of access, list of categories and products viewed, etc. We employ a machine learning approach and investigate a number of features derived from catalog viewing information to predict the gender of viewers. Experiments were conducted on datasets provided by the PAKDD'15 Data Mining Competition and achieved the good result. The results 81.2% on balanced accuracy and 81.4% on macro F1 score showed that basic features such as viewing time, products/categories features used together with more advanced features such as products/categories sequence and transfer features effectively facilitate gender prediction of customers.	data mining;e-commerce payment system;f1 score;machine learning;personalization;privacy;web application	Duc Duong;Hanh Tan;Son Pham	2016	2016 Eighth International Conference on Knowledge and Systems Engineering (KSE)	10.1109/KSE.2016.7758035	support vector machine;the internet;feature extraction;computer science;data science;machine learning;data mining;vegetation	AI	-19.80742867665198	-52.31012354601391	147281
318f21272796e82088e4f1a01583e205ef9c493d	deep web information retrieval process: a technical survey	information retrieval;sru;crawler;z39 50;schema mapping;oai pmh;deep web;ontology	"""Web crawlers specialize in downloading web content and analyzing and indexing from surface web, consisting of interlinked HTML pages. Web crawlers have limitations if the data is behind the query interface. Response depends on the querying party’s context in order to engage in dialogue and negotiate for the information. In this article, the authors discuss deep web searching techniques. A survey of technical literature on deep web searching contributes to the development of a general framework. Existing frameworks and mechanisms of present web crawlers are taxonomically classified into four steps and analyzed to find limitations in searching the deep web. DOI: 10.4018/jitwe.2010010101 2 International Journal of Information Technology and Web Engineering, 5(1), 1-22, January-March 2010 Copyright © 2010, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited. traction first uses the two regularities of the domain knowledge and interface similarity to assign the tasks that are proposed from users and chooses the most effective set of sites to visit by ontology inspection. The conventional search engine has limitations in indexing the deep web pages so there is a requirement of an efficient algorithm to search and index the deep web pages (Akilandeswari & Gopalan, 2008). Figure 1 shows the barrier in information extraction in the form of search form or login form. Contributions. This article attempts to find the limitations of the current web crawlers in searching the deep web contents. For this purpose a general framework for searching the deep web contents is developed as per existing web crawling techniques. In particular, it concentrates on survey of techniques extracting contents from the portion of the web that is hidden behind search interface in large searchable databases with the following points. After profound analysis of entire work• ing of deep web crawling process, we extracted qualified steps and developed a framework of deep web searching Taxonomic classification of different • mechanisms of the deep web extraction as per synchronism with developed framework Comparison of different algorithms web • searching with their advantages and limitations Discuss the limitations of existing web • searching mechanisms in large scale crawling of deep web CUrrENT DEEP WEB INforMaTIoN rETrIEVal fraMEWorK After exhaustive analysis of existing deep web information retrieval processes, a deep web information retrieval framework is developed, in which different tasks in deep web crawling are idetified, arranged and aggregated in sequential manner . This framework is useful for understanding entire working of deep web Figure 1. Query or credentials required for contents extraction 20 more pages are available in the full version of this document, which may be purchased using the """"Purchase"""" button on the product's webpage: www.igi-global.com/article/deep-web-information-retrievalprocess/41725"""	algorithm;credential;database;deep web;download;html;information extraction;information retrieval;international journal of information technology;login;surface web;traction teampage;web content;web crawler;web engineering;web page;web search engine;world wide web	Dilip Kumar Sharma;A. K. Sharma	2010	IJITWE	10.4018/jitwe.2010010101	web service;web application security;web mining;web development;web modeling;site map;data web;web mapping;web design;web standards;computer science;web crawler;web navigation;ontology;social semantic web;web page;data mining;semantic web stack;database;web intelligence;world wide web;website parse template;information retrieval	Web+IR	-31.935164973277992	-53.14780161936897	147288
aa5ed26be05dbf86f8608e7b9281c3efd5826d66	search engine query clustering using top-k search results	similarity metric;clustering validation;clustering validation search egine query clustering top k search results;cluster algorithm;pattern clustering;search engine;query processing;search engines;search engines pattern clustering query processing recommender systems;k means;upper bound;clustering method;user requirements;query similarity identification search engine query clustering top k search results query recommendation query logs user requirements agglomerative clustering;top k search results;cluster validity;computational efficiency;query logs;search egine query clustering;recommender systems;measurement clustering algorithms search engines google scalability computational efficiency upper bound	Clustering of search engine queries has attracted significant attention in recent years. Many search engine applications such as query recommendation require query clustering as a pre-requisite to function properly. Indeed, clustering is necessary to unlock the true value of query logs. However, clustering search queries effectively is quite challenging, due to the high diversity and arbitrary input by users. Search queries are usually short and ambiguous in terms of user requirements. Many different queries may refer to a single concept, while a single query may cover many concepts. Existing prevalent clustering methods, such as K-Means or DBSCAN cannot assure good results in such a diverse environment. Agglomerative clustering gives good results but is computationally quite expensive. This paper presents a novel clustering approach based on a key insight--search engine results might themselves be used to identify query similarity. We propose a novel similarity metric for diverse queries based on the ranked URL results returned by a search engine for queries. This is used to develop a very efficient and accurate algorithm for clustering queries. Our experimental results demonstrate more accurate clustering performance, better scalability and robustness of our approach against known baselines.	algorithm;baseline (configuration management);cluster analysis;dbscan;k-means clustering;requirement;sim lock;scalability;user requirements document;web search engine;web search query	Yuan Hong;Jaideep Vaidya;Haibing Lu	2011	2011 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology	10.1109/WI-IAT.2011.224	search-oriented architecture;constrained clustering;data stream clustering;query expansion;web query classification;fuzzy clustering;computer science;canopy clustering algorithm;cure data clustering algorithm;data mining;database;cluster analysis;brown clustering;web search query;world wide web;dbscan;information retrieval;search engine;recommender system;spatial query;clustering high-dimensional data	DB	-28.89557859294411	-56.86193086256001	147495
4746fd3a5f07428d0c090def09e70afeb90c32f6	can text summaries help predict ratings? a case study of movie reviews	rating inference;text summarization;opinion classification;content representation and processing;natural language processing	This paper presents an analysis of the rating inference task --- the task of correctly predicting the rating associated to a review, in the context of movie reviews. For achieving this objective, we study the use of automatic text summaries instead of the full reviews. An extrinsic evaluation framework is proposed, where full reviews and different types of summaries (positional, generic and sentiment-based) of several compression rates (from 10% to 50%) are evaluated. We are facing a difficult task; however, the results obtained are very promising and demonstrate that summaries are appropriate for the rating inference problem, performing at least equally to the full reviews when summaries are at least 30% compression rate. Moreover, we also find out that the way the review is organised, as well as the style of writing, strongly determines the performance of the different types of summaries.	automatic summarization	Horacio Saggion;Elena Lloret;Manuel Palomar	2012		10.1007/978-3-642-31178-9_33	natural language processing;computer science;automatic summarization;data mining;information retrieval	NLP	-25.806430194480452	-64.83105662016567	147505
4f06e46dbb2520e5f1762c87c1b67c4284380eaa	author name disambiguation using vector space model and hybrid similarity measures	libraries;hierarchical clustering;electronic mail;hybrid similarity;measurement;name disambiguation;vector space model;f1 score author name disambiguation hybrid similarity measures people differentiation people grouping name ambiguity problem digital citation e mail id coauthor affiliation information retrieval field enhanced vector space model publication mixed citation problem split citations problem evaluation metrics;vectors;atomic layer deposition clustering algorithms educational institutions vectors electronic mail libraries measurement;clustering algorithms;hybrid similarity name disambiguation vector space model hierarchical clustering;atomic layer deposition;vectors citation analysis text analysis	Differentiating people on the basis of their names has always been a complex issue and our desire for grouping people, in a particular domain, based on their attributes is growing day by day. Despite years of research and a bunch of proposed techniques, the name ambiguity problem remains largely unsolved and the so far proposed techniques have faced one problem or the other. In case of author name disambiguation in digital citations, additional attributes like e-mail ID and affiliation of author and co-authors, which are normally available in publications, can help a lot in disambiguation process. Vector space model has traditionally been used in information retrieval field with great degree of success and we explore its use in case of author name disambiguation here. In this paper we propose an enhanced vector space model for disambiguating authors and their publications. Experimental results show that additional attributes present in publications can help a lot in disambiguation and solve the name ambiguity problem with a great degree of confidence. From the study we conducted and the experimental results obtained we conclude that both mixed citation and split citations problem can be handled very efficiently. We obtained a great deal of improvement in evaluation metrics obtaining F1 score of 0.97.	cluster analysis;digital library;email;emoticon;f1 score;heuristic (computer science);information retrieval;internationalized domain name;lazy evaluation;library (computing);word-sense disambiguation	Tasleem Arif;Rashid Ali;M. Asger	2014	2014 Seventh International Conference on Contemporary Computing (IC3)	10.1109/IC3.2014.6897162	computer science;artificial intelligence;data science;machine learning;data mining;hierarchical clustering;atomic layer deposition;cluster analysis;vector space model;information retrieval;measurement	SE	-26.803779675336642	-58.72425127892207	147588
364e6d440ae0b80c6288936806dc9841d0544df0	toward a semantic granularity model for domain-specific information retrieval	granular computing;search engine;three dimensions;information retrieval;computer model;model performance;document ranking;ranking function;domain specific search;theory;algorithms;experimentation;domain ontology;domain specificity	Both similarity-based and popularity-based document ranking functions have been successfully applied to information retrieval (IR) in general. However, the dimension of semantic granularity also should be considered for effective retrieval. In this article, we propose a semantic granularity-based IR model that takes into account the three dimensions, namely similarity, popularity, and semantic granularity, to improve domain-specific search. In particular, a concept-based computational model is developed to estimate the semantic granularity of documents with reference to a domain ontology. Semantic granularity refers to the levels of semantic detail carried by an information item. The results of our benchmark experiments confirm that the proposed semantic granularity based IR model performs significantly better than the similarity-based baseline in both a bio-medical and an agricultural domain. In addition, a series of user-oriented studies reveal that the proposed document ranking functions resemble the implicit ranking functions exercised by humans. The perceived relevance of the documents delivered by the granularity-based IR system is significantly higher than that produced by a popular search engine for a number of domain-specific search tasks. To the best of our knowledge, this is the first study regarding the application of semantic granularity to enhance domain-specific IR.	baseline (configuration management);benchmark (computing);british informatics olympiad;computational model;domain-specific language;experiment;information retrieval;ontology (information science);ranking (information retrieval);relevance;web search engine	Xin Yan;Raymond Y. K. Lau;Dawei Song;Xue Li;Jian Ma	2011	ACM Trans. Inf. Syst.	10.1145/1993036.1993039	three-dimensional space;semantic similarity;semantic computing;granular computing;semantic search;computer science;machine learning;data mining;database;world wide web;information retrieval;theory;search engine	Web+IR	-32.43696098281486	-61.15840483940199	147788
1476d40e82ed7b8fc75ad702ba368e734af5ac7f	scalable clustering of news search results	search engine;clustering;cluster system;realtime clustering;news search;query based clustering;news clustering	In this paper, we present a system for clustering the search results of a news search engine. The news search interface includes the relevant news articles to a given query organized in terms of related news stories. Here each cluster corresponds to a news story and the news articles are clustered into stories. We present a system that clusters the search results of a news search system in a fast and scalable manner. The clustering system is organized into three components including offline clustering, incremental clustering and realtime clustering. We propose novel techniques for clustering the search results in realtime. The experimental results with large collections of news documents reveal that our system is both scalable and also achieves good accuracy in clustering the news search results.	algorithm;cluster analysis;online and offline;real-time computing;scalability;streaming media;web search engine	Srinivas Vadrevu;Choon Hui Teo;Suju Rajan;Kunal Punera;Byron Dom;Alexander J. Smola;Yi Chang;Zhaohui Zheng	2011		10.1145/1935826.1935918	correlation clustering;data stream clustering;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;cure data clustering algorithm;data mining;cluster analysis;brown clustering;world wide web;dbscan;information retrieval;search engine;clustering high-dimensional data;conceptual clustering	Web+IR	-26.448850479146614	-54.740454926155294	147790
404f0cf549fc7bfec7d839b86cafab03ba1f2587	evolving support vector machines using whale optimization algorithm for spam profiles detection on online social networks in different lingual contexts		Abstract Detecting spam profiles is considered as one of the most challenging issues in online social networks. The reason is that these profiles are not just a source for unwanted or bad advertisements, but could be a serious threat; as they could initiate malicious activities against other users. Realizing this threat, there is an incremental need for accurate and efficient spam detection models for online social networks. In this paper, a hybrid machine learning model based on Support Vector Machines and one of the recent metaheuristic algorithms called Whale Optimization Algorithm is proposed for the task of identifying spammers in online social networks. The proposed model performs automatic detection of spammers and gives an insight on the most influencing features during the detection process. Moreover, the model is applied and tested on different lingual datasets, where four datasets are collected from Twitter in four languages: Arabic, English, Spanish, and Korean. The experiments and results show that the proposed model outperforms many other algorithms in terms of accuracy, and provides very challenging results in terms of precision, recall, f-measure and AUC. While it also helps in identifying the most influencing features in the detection process.	algorithm;social network;spamming;support vector machine	Ala' M. Al-Zoubi;Hossam Faris;Ja'far Alqatawna;Mohammad A. Hassonah	2018	Knowl.-Based Syst.	10.1016/j.knosys.2018.04.025	arabic;support vector machine;data mining;machine learning;computer science;metaheuristic;artificial intelligence;algorithm;social network	DB	-20.364308304506633	-56.200219243567446	147987
5fc3e0c74d872800c09f370ea8b3ab06070988bd	placing user-generated content on the map with confidence	geographic information retrieval;geolocation	We describe a method that predicts the location of user-generated content using textual features alone. Unlike previous methods for geotagging text documents, our proposed method is not sensitive to how we discretize space. We also discover that spatial resolution has an impact on the prediction accuracy, which allows us to trade-off the spatial resolution of the predicted location against our confidence about its accuracy. Our method can be used to estimate the error in document's predicted location, enabling us to filter out poor quality predictions. We evaluate the proposed method extensively on user-generated content collected from two different social media sites, Flickr and Twitter. Our evaluation examines its performance on the geotagging task and with respect to different parameters. We achieve state-of-the-art results for all three tasks: location prediction, error estimation and result ranking and also provide a theoretical explanation of the effect of spatial resolution factor on geotagging accuracy. Our findings provide valuable insights into the design of geotagging systems and their quality control.	discretization;flickr;geotagging;social media;user-generated content	Suradej Intagorn;Kristina Lerman	2014		10.1145/2666310.2666433	computer science;data mining;geolocation;world wide web;information retrieval	HCI	-25.572916484349083	-52.594564440981905	148038
f9681ba23f0166bdd090260cb79b84ab79b3dbd1	improving information retrieval precision by finding related queries with similar information need using information scent	search engine information scent information retrieval clustering;search engine;history;motion pictures;query processing;search engines;information retrieval;information needs;information foraging;query session mining;data mining;internet;engines;information retrieval precision;engines games history clustering algorithms information retrieval motion pictures data mining;clustering;games;information scent;clustering algorithms;world wide web;document retrieval;user behavior;information need;search engines information needs internet query processing;information foraging theory;document retrieval information retrieval precision information need information scent world wide web search engine query session mining information foraging theory	"""Web is a valuable source of information and it is expanding at an enormous speed. Search engines provide the interface to access to this vast pool of information. Users express their information need through the input query to retrieve the relevant information which does not prove to be effective as input query entered by the user is too short to get the information need of the user. To retrieve the information according to a particular information need from a big pool of information available on the web is a big challenge. This paper proposes a method to find the related queries which approximate the information need of the input query issued to the search engine. This is accomplished using Information scent and content of clicked pages in query sessions mining. Information scent is derived from the information foraging theory in which user behavior in the information environment is guided by information scent. Information need of the query sessions is modeled using information scent and content of clicked URLs and query sessions with similar information need are clustered. The clusters which closely approximate the information need of input query are used to suggest queries with similar information need for a given input query. The suggested queries are ranked in order of their degree of relevance with respect to information need of the input query. Retrieval precision of search engine is improved as suggested queries help to retrieve document relevant to the need of user efficiently and quickly. Experimental study has been conducted on the dataset collected from """"Google"""" search engine Web history to confirm the improvement of precision of information retrieval."""	approximation algorithm;history of the world wide web;information foraging;information needs;information retrieval;information source;relevance;web search engine	Suruchi Chawla;Punam Bedi	2008	2008 First International Conference on Emerging Trends in Engineering and Technology	10.1109/ICETET.2008.23	query expansion;web query classification;ranking;relevance;cognitive models of information retrieval;computer science;information integration;information filtering system;concept search;data mining;information quality;web search query;world wide web;information retrieval;query language;search engine;human–computer information retrieval	DB	-30.40099257028572	-53.01103357807475	148221
426f61323a8b07c8159f4be93e7f87c9f5f16649	classification-based resource selection	resource selection;distributed information retrieval;query classification;machine learning;federated search;web search	In some retrieval situations, a system must search across multiple collections. This task, referred to as federated search, occurs for example when searching a distributed index or aggregating content for web search. Resource selection refers to the subtask of deciding, given a query, which collections to search. Most existing resource selection methods rely on evidence found in collection content. We present an approach to resource selection that combines multiple sources of evidence to inform the selection decision. We derive evidence from three different sources: collection documents, the topic of the query, and query click-through data. We combine this evidence by treating resource selection as a multiclass machine learning problem. Although machine learned approaches often require large amounts of manually generated training data, we present a method for using automatically generated training data. We make use of and compare against prior resource selection work and evaluate across three experimental testbeds.	federated search;information retrieval;machine learning;test set;testbed;web search engine	Jaime Arguello;James P. Callan;Fernando Diaz	2009		10.1145/1645953.1646115	query expansion;web query classification;computer science;machine learning;data mining;database;web search query;world wide web;information retrieval	NLP	-31.16444309401331	-60.978164970013985	148389
0ae6050ad5251e28b6b8bc1bf338a1db148ceef8	automatization of scientific articles classification according to universal decimal classifier		This research examines the problems of automatic scientific articles classification according to Universal Decimal Classifier. To reveal the structure of the train data its visualization was obtained using the recursive feature elimination algorithm. Further; the study provides a comparison of TF-IDF and Weirdness – two statistic-based metrics of keyword significance. The most efficient classification methods are explained: cosine similarity method, naïve Bayesian classifier and artificial neural network. This research explores the most effective for text categorization structure of the multi-layer perceptron and derives appropriate conclusions.	algorithm;artificial neural network;bayesian network;boosting (machine learning);categorization;cosine similarity;document classification;latent semantic analysis;multilayer perceptron;naive bayes classifier;recursion;scientific literature;tf–idf;universal decimal classification	Aleksandr Romanov;Konstantin Lomotin;Ekaterina Kozlova	2017				Web+IR	-20.935788913387317	-65.55547492601653	148718
