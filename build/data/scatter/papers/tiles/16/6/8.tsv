id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
076b219ce5504ae44995ceefbdd9f1539e10080b	querying cardinal directions between complex objects in data warehouses	data warehouse design;spatio temporal olap;cardinal directions	Data warehouses help to store and analyze large multidimens onal datasets and provide enterprise decision support. With an increased availabili ty of spatial data in recent years, several new strategies have been proposed to enable their integrati on in o data warehouses and and perform complex OLAP analysis. Cardinal directions have turned out t be very important qualitative spatial relations due to their numerous applications in spatial way finding, GIS, qualitative spatial reasoning and in domains such as cognitive sciences, AI and robotics. T hey are frequently used as selection and restriction criteria in spatial queries. In data wareho uses, cardinal directions can be used to perform spatial OLAP and feature navigation operations. In this article, we introduce and develop the Objects Interaction Graticule(OIG) approach to query the cardinal direction relations among spatio-temporal objects in data warehouses. First, we appl y tiling strategy that determines the zones belonging to the nine cardinal directions of each spatial ob ject at a particular time and intersects them. This leads to a collection of gridsover time called the Objects Interaction Graticule (OIG). For each grid cell, the information about the spatial object s that intersect it is stored in a Objects Interaction Matrix. In the second phase, an interpretation method is applied to these matrices to determine the cardinal direction between the moving object s. The results obtained for each valid instantover the objects’ lifetime describe the variation in the obj ects movement over time. Thisis integrated as a spatio-temporal OLAP operation in a novel moving objects data warehouse (MODW) that provides an extensible framework for supporting compl ex structured objects. Finally, we define new directional predicates that extend MDX querying and lev erage OLAP between moving objects.	.mdx;artificial intelligence;cognitive science;decision support system;geographic information system;graphical user interface;interpretation (logic);multitier architecture;online analytical processing;physical design (electronics);query language;robotics;spatial–temporal reasoning;syntactic predicate;tiling window manager	Ganesh Viswanathan;Markus Schneider	2014	Fundam. Inform.	10.3233/FI-2014-1040	object-based spatial database;cardinal direction;computer science;data science;data mining;database	DB	-29.6385171521714	8.019509141579045	40828
702044a28787d70df71facdc8347286fd4baa58d	database query using menus and natural language fragments			natural language	R. N. Cuff	1982				ML	-32.07816041222122	8.555009710770838	41115
42c7cfcc40e2ac0b61d01eda757be5a4551c3318	efficient query processing for streamed xml fragments	busqueda informacion;modelizacion;base donnee;streaming;tranchage;execution time;query processing;redundancia;information retrieval;information transmission;lenguaje xpath;structure arborescente;transmision continua;traitement requete;xml language;real time;interrogation base donnee;database;xpath language;interrogacion base datos;base dato;swinburne;real time processing;metric;processing time;modelisation;slicing;transmission en continu;redundancy;estructura arborescente;recherche information;langage expression chemin;query evaluation;temps reel;tree structure;chapeado;tiempo real;temps traitement;temps execution;metrico;tratamiento pregunta;tiempo ejecucion;efficient query processing;modeling;tiempo proceso;database query;langage xml;lenguaje xml;metrique;redondance	Unlike in traditional databases, queries on XML streams are bounded not only by memory but also by real time processing. Recently proposed Hole-Filler model is promising for information transmission and publication, by slicing XML data into low consuming, easy synchronized fragments. However, XPath queries evaluate the elements in streamed XML data, not the XML fragments, and operation dependence caused by fragments decelerates processing efficiency. By taking advantage of schema information for XML, this paper proposes a model of tid tree to optimize queries over XML fragments by removing “redundant” operations. It then proposes XFPro for processing XPath queries on XML fragments to achieve processing and memory efficiency. Our performance study shows that XFPro performs well both on execution time and memory metrics.	streaming media;xml	Huan Huo;Guoren Wang;Xiaoyun Hui;Rui Zhou;Bo Ning;Chuan Xiao	2006		10.1007/11733836_33	xml validation;xml encryption;simple api for xml;xml;systems modeling;processing instruction;metric;xml schema;streaming xml;computer science;xml framework;xml database;xml schema;database;tree structure;xml signature;redundancy;programming language;world wide web;efficient xml interchange	DB	-30.730260079599393	5.640700032118479	41281
b6a51df38bc2236037bfd3673815865631edb4e7	the chase procedure and its applications in data exchange	004;chase chase termination data exchange incomplete information	The initial and basic role of the chase procedure was to test logical implication between sets of dependencies in order to determine equivalence of database instances known to satisfy a given set of dependencies and to determine query equivalence under database constrains. Recently the chase procedure has experienced a revival due to its application in data exchange. In this chapter we review the chase algorithm and its properties as well as its application in data exchange. 1998 ACM Subject Classification H.2.5 [Heterogeneous Databases]: Data translation	algorithm;cad data exchange;chase (algorithm);database;turing completeness	Adrian Onet	2013		10.4230/DFU.Vol5.10452.1	computer science;theoretical computer science;chase;algorithm	DB	-24.382514280115593	10.082409963539119	41631
2b07ba46676fd0ee1cae141284c61687484e89ef	conceptual integration of multiple partial geometric models	base donnee;modele geometrique;modelo 3 dimensiones;integration information;modele 3 dimensions;cuestion respuesta;database;base dato;three dimensional model;pregunta documental;scientific database;question documentaire;information integration;integracion informacion;query;cerebral cortex;question reponse;modele donnee;geometric model;question answering;geometrical model;data models;modele conceptuel;modelo geometrico	Many scientific databases need to manage complex 3D geometric data such as models of the cerebral cortex. Often the complexity of the data forces users to construct muliple, simpler representations, which cover the real, complete model only partially and approximately. In order to recover the original information one needs to integrate these partial and incomplete models. In this paper, we first develop a conceptual model for objects and relationships in 3D geometric data, as well as their partial and approximate representations. We then establish mapping relationships between different approximations of the same data. Finally, we present a geometric information integration technique that will perform the integration where possible, and determine, for some cases, when the integration cannot be performed.	approximation algorithm;database	Simone Santini;Amarnath Gupta	2002		10.1007/3-540-45816-6_35	data modeling;question answering;computer science;artificial intelligence;information integration;geometric modeling;data mining;database;algorithm	DB	-32.0469628105797	9.654276439357092	41672
09ed70942c506d27fd86ba3cf1029e69270e261a	qed: a novel quaternary encoding to completely avoid re-labeling in xml updates	labeling scheme;quaternary;dynamic xml;update	The method of assigning labels to the nodes of the XML tree is called a labeling scheme. Based on the labels only, both ordered and un-ordered queries can be processed without accessing the original XML file. One more important point for the labeling scheme is the label update cost in inserting or deleting a node into or from the XML tree. All the current labeling schemes have high update cost, therefore in this paper we propose a novel quaternary encoding approach for the labeling schemes. Based on this encoding approach, we need not re-label any existing nodes when the update is performed. Extensive experimental results on the XML datasets illustrate that our QED works much better than the existing labeling schemes on the label updates when considering either the number of nodes or the time for re-labeling.	xml tree	Changqing Li;Tok Wang Ling	2005		10.1145/1099554.1099692	computer science;data mining;database;world wide web;quaternary	DB	-30.75306314960947	4.768024768125653	41759
d431a970fc25912703ec122b373eca192ebb1ade	fusing automatically extracted annotations for the semantic web		This research focuses on the problem of semantic data fusion. Although various solutions have been developed in in the research communities focusing on databases and formal logic, the choice of an appropriate algorithm is non-trivial because the performance of each algorithm and its optimal configuration parameters depend on the type of data, to which the algorithm is applied. In order to be reusable, the fusion system must be able to select appropriate techniques and use them in combination. Moreover, because of the varying reliability of data sources and algorithms performing fusion subtasks, uncertainty is an inherent feature of semantically annotated data and has to be taken into account by the fusion system. Finally, the issue of schema heterogeneity can have a negative impact on the fusion performance. To address these issues, we propose KnoFuss: an architecture for Semantic Web data integration based on the principles of problem-solving methods. Algorithms dealing with different fusion subtasks are represented as components of a modular architecture, and their capabilities are described formally. This allows the architecture to select appropriate methods and configure them depending on the processed data. In order to handle uncertainty, we propose a novel algorithm based on the DempsterShafer belief propagation. KnoFuss employs this algorithm to reason about uncertain data and method results in order to refine the fused knowledge base. Tests show that	algorithm;belief propagation;database;knowledge base;modular programming;problem solving;schema evolution;semantic web;software propagation;uncertain data	Andriy Nikolov	2010			computer science;data mining;database;information retrieval	DB	-22.336628873112073	6.789952575178273	41804
88326a95f3bc59570b32dd5ea7431c8d9da0b2b7	similarity-based relaxed instance queries	instance queries;description logics;concept similarity measures	In Description Logics (DL) knowledge bases (KBs), information is typically captured by clear-cut concepts. For many practical applications querying the KB by crisp concepts is too restrictive; a user might be willing to lose some precision in the query, in exchange of a larger selection of answers. Similarity measures can offer a controlled way of gradually relaxing a query concept within a user-specified limit. In this paper we formalize the task of instance query answering for DL KBs using concepts relaxed by concept similarity measures (CSMs). We investigate computation algorithms for this task in the DL EL, their complexity and properties for the CSMs employed regarding whether unfoldable or general TBoxes are used. For the case of general TBoxes we define a family of CSMs that take the full TBox information into account, when assessing the similarity of concepts.	algorithm;computation;description logic;knowledge base;tbox	Andreas Ecke;Rafael Peñaloza;Anni-Yasmin Turhan	2015	J. Applied Logic	10.1016/j.jal.2015.01.002	description logic;computer science;artificial intelligence;theoretical computer science;data mining;database;mathematics	AI	-23.92802068999814	7.795168071876122	41879
155e4fcb824185ef387b76a2fcd756811a6bfa41	a reinforcement learning solution for allocating replicated fragments in a distributed database	ubicacion;replication;reinforcement learning;q learning;allocation;replicacion;journal article;distributed database design;apren;articulo cientifico;computacion;q learning diseno de bases de datos distribuidas;ap	Due to the complexity of the data distribution problem in Distributed Database Systems, most of the proposed solutions divide the design process into two parts: the fragmentation and the allocation of fragments to the locations in the network. Here we consider the allocation problem with the possibility to replicate fragments, minimizing the total cost, which is in general NP-complete, and propose a method based on Q-learning to solve the allocation of fragments in the design of a distributed database. As a result we obtain for several cases, logical allocation of fragments in a practical time.	algorithmic learning theory;approximation;artificial neural network;bellman equation;concurrency (computer science);concurrency control;distributed database;flocking (behavior);fragmentation (computing);genetic algorithm;international standard serial number;interpolation;iteration;lookup table;markov decision process;np-completeness;ptc integrity;q-learning;read-only memory;recommender system;reinforcement learning;self-replicating machine	Abel Rodríguez;Darien Rosa Paz;Marisela Mainegra Hing;Luisa Manuela González González	2007	Computación y Sistemas	10.13053/cys-11-2-1165	computer science;artificial intelligence;machine learning;database	DB	-25.46211716665371	14.135194449482372	41973
d847a04f784860d133a6928339767d2f3c265884	a generalized model for data: a nested relational approach				Serafim Dahl;Kjell Lindqvist	1996			machine learning;nested set model;semi-structured model;logical data model;artificial intelligence;mathematics	DB	-30.83226465757107	8.973869984399713	42011
51871a81ef1e0995183a21e772019a27c1d4ff24	complementary relations and their concept lattices in relational databases	normalized scales;frequency modulation;information systems;data query systems concept lattices relational databases data storage information systems data query data mining relational database model data storage technique formal concept analysis;relational databases data mining;lattices;concept lattices;relational database;data mining;data storage technique;relational database model;data storage;data query;concept lattice;complementary relations;binary relation;normalized scales relational databases complementary relations concept lattices;relational databases;data query systems;information system;point of view;integrated circuits;context;formal concept analysis;lattices relational databases data mining context aware services data analysis laboratories deductive databases information processing computers information systems	The storage of data is a key issue of information systems, which is an important foundation for data query and data mining. Relational database model has been proven to be a very useful data-storage technique. As information is stored as data in relational databases, the induction of concepts from data is a pivotal topic in the data mining field. Formal Concept Analysis (FCA) turns out to be a perfect instrument for a meaningful and conceptual exploration of the stored data. In FCA, conceptual scaling provides a complete framework for transforming any many-valued context (i.e., relation/table) into a context (called a derived context), in which each many valued attribute is given a scale. The attributes in a scale basically describe meaningful features of the values of the initial attribute. From the logical point of view, complement operation plays a very important role in relational databases and data query systems. In this paper, we provide the connections between the concepts of binary relations and those of complementary binary relations, and propose an approach toward normalizing (complementary) scales, i.e., each (complementary)scale can be represented by a set of statements. One advantage of normalizing scales is to avoid generating huge derived relations, and hence this approach reduces storage cost. By the normalization, the concept lattice of the complement of a derived relation is reduced to a combination of the concept lattice of the derived relation and a set of statements.	data mining;database model;formal concept analysis;image scaling;information system;relational database;relational model;statement (computer science)	Yuxia Lei;Yuefei Sui;Cungen Cao	2009	2009 Fifth International Conference on Semantics, Knowledge and Grid	10.1109/SKG.2009.36	relational model;relational algebra;relational database;computer science;data mining;database;information retrieval;information system	DB	-24.050564503058375	5.775382070055495	42046
ff657142e53dbdcdd6f0885256d05f8e12c96295	browsing and querying in object-oriented database	object oriented data model;object oriented;system design;object oriented database	We present a new interface for Object-Oriented Database Management Systems (OODBMSS). The GOODIES1 system combines and expands the functions of many existing interface systems, introducing some new concepts for improved browsing in an 00 DBMS. The implementation of GOODIES proposes a new approach to database interfaces development: instead of being strongly dependent of the underlying DBMS, GOODIES is based on the main features of the object-oriented data model. The system design is based on an internal model and on an external model. The internal model defines the relationships that bind the interface to the DBMS. The external model determines the possible interaction between the user and the interface system. This paper describes the concepts of the design of GOODIES.	browsing;data model;management system;systems design	Juliano Lopes de Oliveira;Ricardo de Oliveira Anido	1993		10.1145/170088.170169	computer science;database;programming language;object-oriented programming;database design;systems design	DB	-32.48092043660431	11.619159572454965	42065
f82034c634df6ae3b281cc53796d4acca8a253f0	logtalk processing of step part 21 files	tratamiento datos;modelizacion;utilisation information;intercambio informacion;uso informacion;text;information use;prolog;tipo dato;data processing;traitement donnee;texte;logical programming;data type;data model;modelisation;programmation logique;object oriented;echange information;information exchange;object oriented approach;oriente objet;logic programs;type donnee;texto;programacion logica;modeling;orientado objeto;internal standard	STEP is an international standard for modeling information used in manufacturing activities; Part 21 is a STEP component that standardizes the exchange of this information through text files. We are working on applying logic programming techniques to processing STEP data models. The STEP standard specifies the entities, attributes, consistency rules, and functions used for describing and validating manufacturing information. Most STEP entities and data types are organized into hierarchies, making an object-oriented approach the most straight-forward implementation solution. Our work uses Logtalk, an object oriented extension to Prolog, as the primary implementation tool.	logtalk	Paulo Moura;Vincent Marchetti	2006		10.1007/11799573_45	systems modeling;information exchange;data processing;data type;data model;computer science;artificial intelligence;internal standard;database;programming language;object-oriented programming;prolog;algorithm	DB	-30.681087765539758	13.816852799632928	42073
a88203b55b56377d9664747b8c9f7ada03cd01c8	systematic predicate invention in inductive logic programming	learning algorithm;algorithm complexity;complejidad algoritmo;coaccion;contrainte;inductive logic programming;algorithme apprentissage;logical programming;constraint;complexite algorithme;programmation logique;algoritmo aprendizaje;programacion logica	Abs t r ac t . We propose in this paper a new approach for learning predicate definitions from examples and from an initial theory. The particularity of this approach consists in inventing both a new predicate symbol and a specification for this predicate at most steps of learning. The specifications that are built axe incomplete and imprecise, what is modelized by introducing the notion of a-interpretation. At the end of the learning task, some invented predicates are removed by unfolding techniques. The remaining predicates either enable to simplify the program, or axe defined by recursive programs. In the second case, the program could not have been learned without inventing these predicates. The method has been implemented in a system, called SPILP, which has been successfully tested for inventing predicates which simplify the learned programs as well as for inventing recursively defined predicates. Let us point out that the introduction of a-interpretations gives us a general framework for dealing with imprecise specifications and that SPILP can work, even when the target concepts are also incompletely defined by a-interpretations.	inductive logic programming;predicate (mathematical logic);predicate variable;recursion;recursive definition;unfolding (dsp implementation)	Lionel Martin;Christel Vrain	1997		10.1007/3540635149_48	functional predicate;computer science;artificial intelligence;theoretical computer science;machine learning;database;mathematics;constraint;programming language;algorithm	PL	-19.832924240641933	11.832494892153269	42101
3aa8d2a7493fe56a317f7ac2f2a9108e71a633e0	a probabilistic-logical framework for ontology matching	markov logic;004 informatik;statistical relational learning;probabilistic logic;ontology matching	Ontology matching is the problem of determining correspondences between concepts, properties, and individuals of different heterogeneous ontologies. With this paper we present a novel probabilistic-logical framework for ontology matching based on Markov logic. We define the syntax and semantics and provide a formalization of the ontology matching problem within the framework. The approach has several advantages over existing methods such as ease of experimentation, incoherence mitigation during the alignment process, and the incorporation of a-priori confidence values. We show empirically that the approach is efficient and more accurate than existing matchers on an established ontology alignment benchmark dataset.	benchmark (computing);logical framework;markov chain;markov logic network;ontology (information science);ontology alignment	Mathias Niepert;Christian Meilicke;Heiner Stuckenschmidt	2010			upper ontology;ontology alignment;description logic;statistical relational learning;ontology inference layer;computer science;ontology;machine learning;pattern recognition;data mining;ontology language;probabilistic logic;ontology-based data integration;process ontology;suggested upper merged ontology	AI	-22.183289320267452	6.956198598287217	42305
975bfb9894c86c6c75d63df3782c335539a70572	parts, compositions and decompositions of functions in engineering ontologies	part whole relation;functional composition;functional decomposition;mereology;technical function;engineering ontology	In this paper I explore the possibility of introducing in engineering ontologies a generic part-whole relation for functions of technical artefacts by functional composition or functional decomposition. I show by means of the postulates of mereology that general functional compositions and decompositions cannot define such a relation. Yet, one can argue that functional decompositions that are acceptable in engineering may define a proper generic part-whole relation for functions. This possibility requires that (i) the part-whole relation is relative to the specific organisation of functions in decompositions, (ii) there is no strict symmetry between functional composition and decomposition, and (iii) functional decomposition is not transitive.	ontology (information science)	Pieter E. Vermaas	2009		10.3233/978-1-60750-047-6-34	combinatorics;discrete mathematics;pure mathematics;mathematics	DB	-19.629886585398634	5.668803930058381	42310
48f67514ebe9ee651981da7ef88bd8d1b64e3824	open answer set programming for the semantic web	description logics;answer set programming;open answer set programming;rule based;open domain reasoning;nonmonotonic reasoning;semantic web;description logic;logic programs;knowledge representation	We extend answer set programming (ASP) with, possibly infinite, open domains. Since this leads to undecidable reasoning, we restrict the syntax of programs, while carefully guarding knowledge representation mechanisms such as negation as failure and inequalities. Reasoning with the resulting extended forest logic programs (EFoLPs) can be reduced to finite answer set programming, for which reasoners are available. We argue that extended forest logic programming is a useful tool for uniformly representing and reasoning with both ontological and rule-based knowledge, as they can capture a large fragment of the OWL DL ontology language equipped with DL-safe rules. Furthermore, EFoLPs enable nonmonotonic reasoning, a desirable feature in locally closed subareas of the Semantic Web.	answer set programming;finite model property;fixed point (mathematics);knowledge representation and reasoning;least fixed point;logic programming;national fund for scientific research;negation as failure;non-monotonic logic;ontology (information science);programming paradigm;semantic web;simulation;stable model semantics;the forest;undecidable problem	Stijn Heymans;Davy Van Nieuwenborgh;Dirk Vermeir	2007	J. Applied Logic	10.1016/j.jal.2006.02.001	rule-based system;knowledge representation and reasoning;f-logic;description logic;stable model semantics;computer science;negation as failure;artificial intelligence;theoretical computer science;non-monotonic logic;answer set programming;functional logic programming;mathematics;reasoning system;inductive programming;programming language;deductive reasoning;logic programming;algorithm	AI	-20.43744006455457	9.827354147053939	42400
ffef3eb900e164259267c551a9b60a5d8e02e505	brul: a putback-based bidirectional transformation library for updatable views		In work on relational databases, the view-update problem is about how to translate update operations on the view table to corresponding update operations on the source table properly. It is a problem that the translation policies are not unique in many situations. Relational lenses try to solve this problem by providing a list of combinators that let the user write get functions (queries) with specified updated policies for put functions (updates); however this can only provide limited control of update policies which still may not satisfy the user’s real needs. In this paper, we implement a library Brul that provides putback-based basic combinators for the user to write the put function with flexible update policies easily; from the put function, a unique get function can be derived automatically. Brul is implemented in terms of BiGUL, a core bidirectional programming language which has been formalized in Agda and implemented as a Haskell library.	agda;aggregate data;bidirectional transformation;combinatory logic;haskell;programmer;programming language;relational database	Tao Zan;Li Liu;Hsiang-Shang Ko;Zhenjiang Hu	2016			world wide web;internet privacy;computer science	PL	-28.501666499192616	11.24647281995768	42658
5fcdb7deb9ec31163740da2d5188bee0824b832b	regression databases: probabilistic querying using sparse learning sets	sparse learning set;query language;conceptual evaluation algorithm regression database data model sparse learning set probabilistic query probability distributions query language relational algebra;relation algebra;probability;query processing;conceptual evaluation algorithm;probability distributions;set theory data models probability query languages query processing regression analysis relational algebra relational databases;regression model;set theory;data model;query languages;probabilistic query;random variable;regression analysis;relational databases;regression database data model;relational algebra;first order logic;project selection;random variables database languages algebra decision making relational databases data models arithmetic uncertainty probability distribution sensor phenomena and characterization;data models	We introduce regression databases (REDB) to formalize and automate probabilistic querying using sparse learning sets. The REDB data model involves observation data, learning set data, views definitions, and a regression model instance. The observation data is a collection of relational tuples over a set of attributes; the learning data set involves a subset of observation tuples, augmented with learned attributes, which are modeled as random variables; the views are expressed as linear combinations of observation and learned attributes; and the regression model involves functions that map observation tuples to probability distributions of the random variables, which are learned dynamically from the learning data set. The REDB query language extends relational algebra project-select queries with conditions on probabilities of first-order logical expressions, which in turn involve linear combinations of learned attributes and views, and arithmetic comparison operators. Such capability relies on the underlying regression model for the learned attributes. We show that REDB queries are computable by developing conceptual evaluation algorithms and by proving their correctness and termination	algorithm;computable function;computation;correctness (computer science);data model;database;first-order predicate;mathematical optimization;query language;relational algebra;relational database management system;relational operator;sparse matrix	Alexander Brodsky;Carlotta Domeniconi;David Etter	2006	2006 5th International Conference on Machine Learning and Applications (ICMLA'06)	10.1109/ICMLA.2006.44	computer science;machine learning;data mining;database;query language;regression analysis;statistics	DB	-26.988269430227618	8.500723151196194	42691
5858c1c87907d6f2dcbe43e0e832a8329d251285	transformation of dynamic integrity constraints into transaction specifications	integrity constraints	Abstract   Dynamic database behaviour can be specified by dynamic integrity constraints, which determine  admissible  sequences of database states, and by transaction specifications, which induce  executable  sequences. Constraints are expressed by formulae of temporal logic, whereas transactions are defined by pre⧸postconditions in predicate logic. This article presents concepts and rules for transforming dynamic constraints into transaction specifications in order to prepare integrity monitoring by transactions. At first, such transition graphs must be constructed from temporal formulae that have paths corresponding to admissible sequences. Then these graphs are utilized to refine and to simplify pre⧸postconditions systematically so that every executable state sequence will become admissible, too.		Udo W. Lipeck	1990	Theor. Comput. Sci.	10.1016/0304-3975(90)90014-9	real-time computing;computer science;theoretical computer science;data integrity;database;mathematics	DB	-23.460989648768418	15.899149773415244	43028
2f2449885e95eb921228c5282f67c13445ee6c23	xevolve: an xml schema evolution framework	xml schema;schema;unified model;polynomial time;xml;xml document;visibly pushdown automata	This paper presents XEvolve, a framework that unifies streaming validation of XML documents, and efficient testing of equivalence and inclusion of specifications for various XML schema languages. For these purposes, this framework relies on Visibly Pushdown Automata (VPA) as a unifying model for the various schema languages. Schemas are first translated into VPA; standard algorithms for VPA can be then used to validate documents as well as to test equivalence or inclusion of schemas. In general, inclusion and equivalence are tested in exponential-time. However, when the given specifications are provided as DTD or XSD, these tests have a polynomial-time complexity with respect to the automaton size. Moreover, in this case the memory foot-print of the validation does not depend on the size of the input document but only on its depth.	algorithm;automata theory;pushdown automaton;schema evolution;stack (abstract data type);time complexity;turing completeness;xml schema	François Picalausa;Frédéric Servais;Esteban Zimányi	2011		10.1145/1982185.1982530	well-formed document;xml validation;xml encryption;simple api for xml;xml;relax ng;xml schema;streaming xml;computer science;document type definition;xs3p;document definition markup language;document structure description;xml framework;xml schema;database;document schema definition languages;schematron;xml signature;programming language;xml schema editor;algorithm;efficient xml interchange	DB	-25.449850127335917	12.056884930795048	43187
29ec8690ce9ddc883b7b6ac74733385936ba9ce1	three easy pieces on schema mappings for tree-structured data		Schema mappings specify how data organized under a source schema should be reorganized under a target schema. For tree-structured data, the interplay between these specifications and the complex structural conditions imposed by the schemas, makes schema mappings a very rich formalism. Classical data management tasks, well-understood in the relational model, once again become challenging theoretical problems. Recent results on non-mixing integrity constraints help to push the decidability frontier further for three such problems: consistency of mappings, membership in the composition of mappings, and query answering.	data integrity;data model;relational model;schema evolution;semantics (computer science);xml schema	Claire David;Filip Murlak	2017			data mining;database;schema (psychology);data model;computer science	DB	-29.16336783357882	10.510143771952688	43231
d46e8da69e633cd1c41c28c70103a4e4821bdf76	ontological aspects in the formalisation of the framenet inheritance relationship	frames;natural language;formalisation;ontology;formal language;inheritance relationship;framenet;frame semantics	FrameNet is a lexical semantic resource consisting of a set of frames related by semantic links. It is a rich network of concepts and relationships. However, from the point of view of some researches, it has some limitations such as the lack of formalisation. In this paper, we propose to formalise the elements of FrameNet and the inheritance relationship, based on ontological principles in order to facilitate its use in inferences. Furthermore, it is expected that the formalisation of the FrameNet elements solves problems related to ambiguities. In this case, rules were formulated in formal language and were applied to natural language sentences in order to assess their degree of membership to a particular scene. The results showed that this approach can help produce better defined frames.	framenet	Alexandra Moreira;Alcione de Paiva Oliveira;Maria Margarida Salomão;Fabio Ribeiro Cerqueira	2015	IJMSO	10.1504/IJMSO.2015.070824	natural language processing;formal language;framenet;computer science;ontology;linguistics;natural language;algorithm;frame semantics	NLP	-20.827948165148502	5.108266812816585	43251
8d665a9b3f26baf5eea3867de2685ec6bb4ca3a4	brokering infrastructure for minimum cost data procurement based on quality-quantity models	quality cost optimization;consumidor;modelizacion;quality assurance;informacion economica;integer linear programming;optimisation;system structure;recherche multibase;distributed database;programacion entera;organisation entreprise;optimizacion;query processing;compra;economic information;consommateur;bundling information goods;procurement;marche information;comercializacion;interrogation base donnee;integer linear;venta conjunta;tipo dato;base repartida dato;vente groupee ou liee;interrogacion base datos;economic model;bundling;data type;contrato;programmation en nombres entiers;modelo economico;enterprise organization;commercialisation;modelisation;aseguracion calidad;organizacion empresa;modele economique;quality requirement;programacion lineal;mercado informacion;brokering service;base de donnees repartie;integer programming;structure systeme;cost optimization;settore ing inf 05 sistemi di elaborazione delle informazioni;marche contrat;marketing;consumer;information economics;information market;estructura datos;traitement de la requete;linear programming;programmation lineaire;quality cost;achat;information economique;market information;optimization;structure donnee;tratamiento pregunta;data quality;information system;bundle of data;type donnee;information marche;modeling;assurance qualite;programming;informacion mercado;data structure;database query;systeme information;integer linear program;cost model;business process;purchases;estructura sistema;busqueda multibase;structured data;sistema informacion;multidatabase retrieval	Inter-organization business processes involve the exchange of structured data across information systems. We assume that data are exchanged under given condition of quality (offered or required) and prices. Data offer may include bundling schemes, whereby different types of data are offered together with a single associated price and quality. We describe a brokering algorithm for obtaining data from peers, by minimizing the overall cost under quality ∗The work presented in this paper has been partially supported by the eG4M MIUR FIRB project on e-Government in Mediterranean Countries and the MIUR FIRB MAIS project Multi-channel Adaptive Information Systems: models, methodology, qualifying object-oriented platform and architectures for the flexible on-line information systems	algorithm;business process;database;information system;integer programming;linear programming;np-completeness;optimal matching;polynomial;procurement;product bundling;purchasing;requirement;schema (genetic algorithms);subsumption architecture	Alessandro Avenali;Carlo Batini;Paola Bertolazzi;Paolo Missier	2008	Decision Support Systems	10.1016/j.dss.2007.10.012	quality costs;programming;simulation;systems modeling;integer programming;data quality;consumer;procurement;data type;data model;computer science;economic model;operations management;business process;operations research;information system	DB	-28.06039827322357	12.659386705566975	43438
e065fe94abee02323050dc283e9ff4c3007e0dd6	unique state and automatical action abstracting based on logical mdps with negation	modelizacion;base relacional dato;proceso markov;reinforcement learning;relational reinforcement learning;decision markov;interpretacion abstracta;relational database;calcul analogique;modelisation;apprentissage renforce;state space method;methode espace etat;processus markov;state space;markov process;base de donnees relationnelle;markov decision;markov decision process;interpretation abstraite;abstract interpretation;aprendizaje reforzado;modeling;metodo espacio estado;analog calculus;calculo analogico	In this paper we introduce negation into Logical Markov Decision Processes, which is a model of Relational Reinforcement Learning. In the new model nLMDP the abstract state space can be constructed in a simple way, so that a good property of complementarity holds. Prototype action is also introduced into the model. A distinct feature of the model is that applicable abstract actions can be obtained automatically with valid substitutions. Given a complementary abstract state space and a set of prototype actions, a model-free Θ-learing method is implemented for evaluating the state-action-substitution value funcion.		Song Zhiwei;Chen Xiaoping	2006		10.1007/11881223_118	markov decision process;systems modeling;relational database;computer science;state space;artificial intelligence;machine learning;mathematics;markov process;reinforcement learning;algorithm	AI	-20.75852392922653	11.687296932837986	43690
1bb3e05fa9ec27bcde46c5e639305881844507d4	integrating a part relationship into an open oodb system using metaclasses	modeling language;document processing	The part-whole semantic relationship (the part relationship, for short) is an important modeling primitive in many advanced application domains such as manufacturing, design, and document processing. In this paper, we examine the problem of integrating such a construct into an OODB system. Specifically, two questions are addressed in this regard. This first is: Can a part relationship be made an intrinsic construct of an existing OODB system without having to rewrite a substantial portion of the system? The second: Can an “open” OODB system which claims to support such an integration really do so, and, more specifically, can the integration be done using a metaclass mechanism which purports to bring extensibility to the VODAK Model Language (VML)? To demonstrate that both questions can be answered “yes,” we introduce and discuss the details of a custom VML metaclass—the “HolonymicMeronymic” metaclass—which we have built. This metaclass comprises two items, an “instance” type and an “instance-instance” type. Together, the two endow the classes of a part hierarchy and their instances with structure and behavior consistent with our comprehensive part relationship model and the notions of “part” and “whole.” Complete descriptions of each of these two aspects of the metaclass are presented and their effect on schema construction and database usage is discussed.	database schema;document processing;extensibility;metaclass;rewrite (programming)	Michael Halper;James Geller;Yehoshua Perl;Wolfgang Klas	1994		10.1145/191246.191252	natural language processing;document processing;computer science;artificial intelligence;data mining;database;modeling language;world wide web;information retrieval;algorithm	DB	-30.541312007147184	11.772277802876738	43771
b05809cc43f6ef2d7826b06e1af3fabe4a0400aa	computational investigation of a model linking algorithm for the energy-economy interaction model	interaction model	Abstract   A model linking algorithm which is based on the algorithmic concept employed in the Project Independence Evaluation System (PIES) is developed for a dynamic energy/economic interaction model. The convergence of the algorithm is tested empirically and the results show that the algorithm converges to the desired accuracy. Potential application to generalized networks is discussed.  Results of this work confirms the possibility of utilizing the PIES algorithm concept as a model integration scheme in general network situations.	algorithm;computation	Byong-Hun Ahn;Baek-seo Seong	1983	Computers & OR	10.1016/0305-0548(83)90022-9	mathematical optimization;simulation;computer science;artificial intelligence;mathematics	NLP	-19.757806844439347	4.393768362610989	43897
6d2b0e8846b59a103e398947975224d8d8aefa63	closed sets of boolean terms in relational databases	relational database		boolean algebra;relational database	Andrzej Jankowski;Zbigniew Michalewicz	1991	Fundam. Inform.		domain relational calculus;database theory;sql;relational model/tasmania;nested set model;relational model;codd's theorem;relational calculus;entity–relationship model;relational database;computer science;database normalization;database model;database;conjunctive query;candidate key;object-relational impedance mismatch;database design;codd's 12 rules	DB	-30.31405526182677	9.1570879918981	44094
57a9c2f839409e017ca656d37a8717a4049507f2	view consistency in software development	developpement logiciel;frase;semantics;semantica;semantique;methode algebrique;sentence;object oriented;desarrollo logicial;algebraic method;software development;oriente objet;phrase;metodo algebraico;orientado objeto	An algebraic approach to the view consistency problem in software development is provided. A view is formalised as a sentence of a viewpoint language; a viewpoint is given by a language and its semantics. Views in possibly different viewpoints are compared over a common view for consistency by a heterogenous pull-back construction. This general notion of view consistency is illustrated by several examples from viewpoints used in object-oriented software development.	software development	Martin Wirsing;Alexander Knapp	2002		10.1007/978-3-540-24626-8_24	computer science;software development;database;semantics;programming language;object-oriented programming;algorithm	SE	-29.899568087268534	14.324672315695242	44158
10ab4ee308ac3470724d293a022216b31a5aa877	probabilistic dl reasoning with pinpointing formulas: a prolog-based approach		When modeling real world domains we have to deal with information that is incomplete or that comes from sources with different trust levels. This motivates the need for managing uncertainty in the Semantic Web. To this purpose, we introduced a probabilistic semantics, named DISPONTE, in order to combine description logics with probability theory. The probability of a query can be then computed from the set of its explanations by building a Binary Decision Diagram (BDD). The set of explanations can be found using the tableau algorithm, which has to handle non-determinism. Prolog, with its efficient handling of non-determinism, is suitable for implementing the tableau algorithm. TRILL and TRILL are systems offering a Prolog implementation of the tableau algorithm. TRILL builds a pinpointing formula, that compactly represents the set of explanations and can be directly translated into a BDD. Both reasoners were shown to outperform state-of-the-art DL reasoners. In this paper, we present an improvement of TRILL , named TORNADO, in which the BDD is directly built during the construction of the tableau, further speeding up the overall inference process. An experimental comparison shows the effectiveness of TORNADO. All systems can be tried online in the TRILL on SWISH web application at http://trill.ml.unife.it/. Under consideration in Theory and Practice of Logic Programming (TPLP).	association for logic programming;binary decision diagram;description logic;experiment;long division;method of analytic tableaux;nondeterministic algorithm;probabilistic semantics;prolog;semantic web;semantic reasoner;speedup;tornado;web application	Riccardo Zese;Elena Bellodi;Giuseppe Cota;Evelina Lamma;Fabrizio Riguzzi	2018	CoRR		theoretical computer science;computer science;web application;semantic web;probabilistic logic;semantics;inference;description logic;binary decision diagram;prolog	AI	-23.33958275176965	7.965298867711832	44188
07f2e999867a86e1300bed5f5c5e523bf03cf709	computing similarity dependencies with pattern structures	attribute implications;data dependencies;pattern structures;association rules and data dependencies;formal concept analysis	Functional dependencies provide valuable knowledge on the relations between the attributes of a data table. To extend their use, generalizations have been proposed, among which purity and approximate dependencies. After discussing those generalizations, we provide an alternative definition, the similarity dependencies, to handle a similarity relation between data-values, hence un-crisping the basic definition of functional dependencies. This work is rooted in formal concept analysis, and we show that similarity dependencies can be easily characterized and computed with pattern structures.	approximation algorithm;computation;experiment;formal concept analysis;functional dependency;in-memory database;pure function;scalability;semantic similarity;similarity measure;table (information)	Jaume Baixeries;Mehdi Kaytoue-Uberall;Amedeo Napoli	2013			armstrong's axioms;dependency theory;computer science;formal concept analysis;theoretical computer science;machine learning;data mining;database;mathematics;functional dependency	DB	-24.22575717332636	6.151604534024737	44294
9dc7ce1f7b04b485193e830ed123e825495b1580	meeran eer model enhanced with structure methods	structure methods;computacion informatica;integrity enforcement;cardinality consistency;data model;ciencias basicas y experimentales;cardinality constraints;grupo a;entity relationship data model;entity relationship	Entity relationship (ER) schemas include cardinality constraints, that restrict the dependencies among entities within a relationship type. The cardinality constraints have direct impact on the application maintenance, since insertions or deletions of entities or relationships might affect related entities. Indeed, maintenance of a system or of a database can be strengthened to enforce consistency with respect to the cardinality constraints in a schema. Yet, once an ER schema is translated into a logical database schema, or translated within a system, the direct correlation between the cardinality constraints and maintenance transactions is lost, since the components of the ER schema might be decomposed among those of the logical database schema or the target system. In this paper, a full solution to the enforcement of cardinality constraints in EER schemas is given. We extend the enhanced ER (EER) data model with structure-based update methods that are fully defined by the cardinality constraints. The structure methods are provably terminating and cardinality faithful, i.e., they do not insert new inconsistencies and can only decrease existing ones. A refined approach towards measuring the cardinality consistency of a database is introduced. The contribution of this paper is in the automatic creation of update methods, and in building the formal basis for proving their correctness. r 2002 Elsevier Science Ltd. All rights reserved.	cardinality (data modeling);constraint (mathematics);correctness (computer science);data model;database schema;enhanced entity–relationship model;entity;erdős–rényi model;machine that always halts;rewriting	Mira Balaban;Peretz Shoval	2002	Inf. Syst.	10.1016/S0306-4379(01)00050-3	cardinality;entity–relationship model;data model;computer science;data mining;database;database schema;algorithm	DB	-26.68779198836135	13.096502482705626	44388
15b23f4339e65c1e385c381086e47167cc14cf27	a meta-translation system for object-oriented to relational schema translations	object oriented			A. Ramfos;N. J. Fiddian;W. Alex Gray	1991			database;natural language processing;computer science;object-oriented programming;conceptual schema;schema (psychology);relational calculus;artificial intelligence	DB	-31.074643053164102	10.237640168145449	44635
6f8983e29352df8517346f5248843a8bd3207479	modeling biological networks by action languages via answer set programming	optimisation sous contrainte;programmation booleenne;boolean programming;modelizacion;constrained optimization;action language;jeune alimentaire prolonge;programacion conjunto respuesta;biological system;representacion conocimientos;syntax;systeme intelligent;biological model;answer set programming;sistema inteligente;circonscription;biologia molecular;semantics;bioinformatique;intelligence artificielle;modelo biologico;logical programming;programacion booleana;syntaxe;constraint satisfaction;semantica;semantique;circumscription;arabidopsis thaliana;starvation;optimizacion con restriccion;modelisation;satisfaction contrainte;systeme biologique;biological network model action language answer set programming logic programs arabidopsis thaliana sulfur systems knowledge inference semantics pathways;modele biologique;programmation logique;programmation par ensemble reponse;molecular biology;action language answer set programming;intelligent system;constraint programming;representation connaissance;artificial intelligence;inteligencia artificial;satisfaccion restriccion;bioinformatica;knowledge representation;sintaxis;biological network model;programacion logica;modeling;sistema biologico;ayuno alimenticio prolongado;biological network;circonscripcion;bioinformatics;biologie moleculaire	We describe an approach to modeling biological networks by action languages via answer set programming. To this end, we propose an action language for modeling biological networks, building on previous work by Baral et al. We introduce its syntax and semantics along with a translation into answer set programming, an efficient Boolean Constraint Programming Paradigm. Finally, we describe one of its applications, namely, the sulfur starvation response-pathway of the model plant Arabidopsis thaliana and sketch the functionality of our system and its usage.	action language;answer set programming;biological network;constraint programming;gene regulatory network;programming paradigm;stable model semantics	Steve Dworschak;Susanne Grell;Victoria J. Nikiforova;Torsten Schaub;Joachim Selbig	2007	Constraints	10.1007/s10601-007-9031-y	constraint programming;constrained optimization;biological network;systems modeling;syntax;constraint satisfaction;programming domain;reactive programming;action language;computer science;artificial intelligence;answer set programming;functional logic programming;models of abnormality;semantics;programming paradigm;inductive programming;fifth-generation programming language;programming language theory;circumscription;algorithm	AI	-20.07827328961787	12.327198897935084	44701
093578dc4794d9547f927a4eea3a8fc3400f2890	stable model semantics for tuple-generating dependencies revisited	tuple generating dependencies;default negation;expressive power;computational complexity;stable model semantics;query answering	Normal tuple-generating dependencies (NTGDs) are TGDs enriched with default negation, a.k.a. negation as failure. Query answering under NTGDs, where negation is interpreted according to the stable model semantics, is an intriguing new problem that gave rise to flourishing research activity in the database and KR communities. So far, all the existing works that investigate this problem, except for one recent paper that adopts an operational semantics based on the chase, follow the so-called logic programming (LP) approach. According to the LP approach, the existentially quantified variables are first eliminated via Skolemization, which leads to a normal logic program, and then the standard stable model semantics for normal logic programs is applied. However, as we discuss in the paper, Skolemization is not appropriate in the presence of default negation since it fails to capture the intended meaning of NTGDs, while the operational semantics mentioned above fails to overcome the limitations of the LP approach. This reveals the need to adopt an alternative approach to stable model semantics that is directly applicable to NTGDs with existentially quantified variables. We propose such an approach based on a recent characterization of stable models in terms of second-order logic, which indeed overcomes the limitations of the LP approach. We then perform an in-depth complexity analysis of query answering under prominent classes of NTGDs based on the main decidability paradigms for TGDs, namely weak-acyclicity, guardedness and stickiness. Interestingly, weakly-acyclic NTGDs give rise to robust and highly expressive query languages that allow us to solve in a declarative way problems in the second level of the polynomial hierarchy.		Mario Alviano;Michael Morak;Andreas Pieris	2017		10.1145/3034786.3034794	natural language processing;stable model semantics;dependency theory;computer science;programming language;computational complexity theory;expressive power;algorithm	DB	-21.950092281428653	12.906654687404675	45137
26a3f10d28ebfb2a26b7fb7275f4d7d98ce60497	detecting inconsistency and incompleteness in access control policies		It is a key issue for detecting inconsistency and incompleteness in the management of access control policies. Traditionally the separate management of subjects and objects lead to the problem that the inconsistency and incompleteness detection is too complicated. In this paper, we use of partial order relationship between subjects and objects to constitute a directed acyclic graph (DAG) model. During the construction of the model, inconsistent and incomplete policies are detected. Finally, the experimental results verify the correctness and effectiveness of the method.		Hongbin Zhang;Pengcheng Ma;Meihua Wang	2018		10.1007/978-3-030-00009-7_65	computer science;directed acyclic graph;access control;correctness;distributed computing	DB	-26.350426195052382	13.18902612621088	45289
a27df50223067770f81ff284e7d617ccbe53c4a2	description logics over lattices with multi-valued ontologies	application domain;multi-valued semantics;description logic alc;multi-valued ontology;clear-cut manner;general concept inclusion;logic w;new semantics;reasoning method	Uncertainty is unavoidable when modeling most application domains. In medicine, for example, symptoms (such as pain, dizziness, or nausea) are always subjective, and hence imprecise and incomparable. Additionally, concepts and their relationships may be inexpressible in a crisp, clear-cut manner. We extend the description logic ALC with multi-valued semantics based on lattices that can handle uncertainty on concepts as well as on the axioms of the ontology. We introduce reasoning methods for this logic w.r.t. general concept inclusions and show that the complexity of reasoning is not increased by this new semantics.	algorithm;application domain;automata theory;automated reasoning;automaton;complexity class;dls format;de morgan's laws;description logic;directed acyclic graph;first-order logic;formal system;morgan;ontology (information science);pspace-complete;preprocessor;semantic reasoner;subsumption architecture	Stefan Borgwardt;Rafael Peñaloza	2011		10.5591/978-1-57735-516-8/IJCAI11-135	discrete mathematics;theoretical computer science;mathematics;algorithm	AI	-20.628279603244355	8.394026590992594	45315
09ee2b488300fed056ee2cb8b7e03302ba688def	condensed representation of database repairs for consistent query answering	conjunctive queries;base donnee;maintenance;interrogation base donnee;database;interrogacion base datos;base dato;integrite;integridad;integrity;integrity constraints;mantenimiento;reparation;query answering;reparacion;database query;repair	Repairing a database means bringing the database in accordance with a given set of integrity constraints by applying modifications that are as small as possible. In the seminal work of Arenas et al. on query answering in the presence of inconsistency, the possible modifications considered are deletions and insertions of tuples. Unlike earlier work, we also allow tuple updates as a repair primitive. Update-based repairing is advantageous, because it allows rectifying an error within a tuple without deleting the tuple, thereby preserving other consistent values in the tuple. At the center of the paper is the problem of query answering in the presence of inconsistency relative to this refined repair notion. Given a query, a trustable answer is obtained by intersecting the query answers on all repaired versions of the database. The problem arising is that, in general, a database can be repaired in infinitely many ways. A positive result is that for conjunctive queries and full dependencies, there exists a condensed representation of all repairs that permits computing trustable query answers. 1 General Problem Description In most database textbooks, databases that violate integrity constraints are considered as “illegal” and hence disregarded. Nevertheless, databases containing inconsistencies are common in real life, and it is important to understand how to treat this phenomenon. At the time inconsistencies are detected, “cleaning” the database is often problematic, as there is generally not one single deterministic way of rectifying inconsistencies. The least we can do is prohibit inconsistencies from being propagated into query answers visible by end-users. For example, to avoid another dioxin-in-food crisis, the government is continuously watching dioxin levels in food samples. The first tuple of the dioxin database in Fig. 1 says that the sample 110 was taken on 17 Jan 2002 and analyzed the day after by the ICI lab, which found a normal dioxin level. The obvious integrity constraint “The sample date must be prior to the analysis date” is violated by sample 220. This inconsistency can be “cleaned” in several ways, for example, by antedating the sample date or by postdating the analysis date of sample 220.	conjunctive query;data integrity;database;ici (programming language);plasma cleaning;real life;rectifier	Jef Wijsen	2003		10.1007/3-540-36285-1_25	online aggregation;query optimization;boolean conjunctive query;computer science;data integrity;data mining;database;conjunctive query;view;range query;information retrieval	DB	-25.558360741211157	11.025216438433095	45419
16d98f725dcd9a20ddb9a69bc05ccb9b62d20e3f	a unifying semantics for active databases using non-markovian theories of actions	base relacional dato;tratamiento transaccion;control theory;sistema experto;proceso markov;active database;accounting;semantics;base connaissance;discrete time;theorie commande;relational database;comptabilite;semantica;semantique;theorem proving;demonstration theoreme;processus markov;base donnee active;markov process;rule based programming;base donnee relationnelle;base conocimiento;active databases;contabilidad;systeme gestion base donnee;systeme expert;demostracion teorema;tiempo discreto;transaction processing;temps discret;sistema gestion base datos;database management system;situation calculus;traitement transaction;knowledge base;expert system	Over the last fifteen years, database management systems (DBMSs) have been enhanced by the addition of rule-based programming to obtain active DBMSs. One of the greatest challenges in this area is to formally account for all the aspects of active behavior using a uniform formalism. In this paper, we formalize active relational databases within the framework of the situation calculus by uniformly accounting for them using theories embodying non-Markovian control in the situation calculus. We call these theories active relational theories and use them to capture the dynamics of active databases. Transaction processing and rule execution is modelled as a theorem proving task using active relational theories as background axioms. We show that major components of an ADBMS may be given a clear semantics using active relational theories.	active database;automated theorem proving;logic programming;relational database;relational theory;semantics (computer science);situation calculus;transaction processing	Iluju Kiringa;Raymond Reiter	2003		10.1007/978-3-540-24607-7_8	knowledge base;discrete time and continuous time;codd's theorem;relational calculus;transaction processing;relational database;computer science;artificial intelligence;database;semantics;automated theorem proving;markov process;situation calculus;expert system;algorithm	AI	-21.34162478334351	11.449607076197687	45581
8752adda33df445a11579c418c5da701606c499f	a multi-strategy approach for detecting and correcting conservativity principle violations in ontology alignments		In order to enable interoperability between ontology-based systems, ontology matching techniques have been proposed. However, when the generated mappings suffer from logical flaws, their usefulness may be diminished. In this paper we present a multi-strategy approach to detect and correct violations of the so-called conservativity principle where novel subsumption entailments between named concepts in one of the input ontologies are considered as unwanted. The practical applicability of the proposed approach is experimentally demonstrated on the datasets from the Ontology Alignment Evaluation Initiative (OAEI).	algorithm;experiment;interoperability;logic programming;ontology (information science);ontology alignment;scalability;subsumption architecture;turing completeness	Alessandro Solimando;Ernesto Jiménez-Ruiz;Giovanna Guerrini	2014			computer science;data mining;ontology alignment;ontology;interoperability;ontology (information science)	AI	-22.80486730701724	7.86268360737042	45583
53fcdf6fe588981e50a110d07a8d246fd611e8e7	on parsing strategies and closure	performance problem;human performance;parsing complexity;parsing strategy;natural language syntax;natural language;performance model;computational complexity;competence model;tm complexity;performance limitation;augmented transition network;context free grammar;turing machine	This paper proposes a welcome hypothesis: a computationally simple device z is sufficient for processing natural language. Traditionally it has been argued that processing natural language syntax requires very powerful machinery. Many engineers have come to this rather grim conclusion; almost all working parers are actually Turing Machines (TM), For example, Woods believed that a parser should have TM complexity and specifically designed his Augmented Transition Networks (ATNs) to be Turing Equivalent.	augmented transition network;natural language;parsing;turing completeness;turing machine	Kenneth Ward Church	1980			natural language processing;human performance technology;augmented transition network;computer science;turing machine;theoretical computer science;machine learning;linguistics;natural language;context-free grammar;computational complexity theory;algorithm;dtime	NLP	-20.206896667796013	13.82039072951486	45788
35866744f332542d7e31df1e2fd34baea9249610	safe datalog queries with linear constraints	base relacional dato;query language;programmation logique avec contrainte;base donnee temporelle;programacion logica con restriccion;relational database;linear constraint;lenguaje interrogacion;upper bound;base donnee relationnelle;temporal databases;langage interrogation;upper and lower bounds;constraint logic programming;information system;analisis semantico;analyse semantique;systeme information;lower bound;semantic analysis;sistema informacion	In this paper we consider Datalog queries with linear constraints. We identify several syntactical subcases of Datalog queries with linear constraints, called safe queries, and show that the least model of safe Datalog queries with linear constraints can be evaluated bottom-up in closed-form. These subcases include Datalog with only positive and upper-bound or only negative and lower bound constraints or only half-addition, upper and lower bound constraints. We also study other subcases where the recognition problem is decidable.	bottom-up parsing;datalog	Peter Z. Revesz	1998		10.1007/3-540-49481-2_26	discrete mathematics;computer science;database;mathematics;upper and lower bounds;algorithm	DB	-25.368636865420797	11.478079817893548	45955
eb067e50cc90e2c6797dcd47f5172638f2053026	the functional structure of os/360: part iii data management	data management	Concepts underlying the data-management capabilities of OS/360 are introduced; distinctive features of the access methods, catalog, and relevant system macroinstructions are discussed.#R##N##R##N#To illustrate the way in which the control program adapts to actual input/output requirements, a read operation is examined in considerable detail.		William A. Clark	1966	IBM Systems Journal	10.1147/sj.51.0030	real-time computing;computer science;data mining;database	DB	-32.24978456719633	11.279370991270378	45966
24e5a066e1b7e4d511ab049fcedd9b1fa3feb024	automated reasoning support for first-order ontologies	modelizacion;representacion conocimientos;ontologie;book chapter;web semantique;service web;logica descripcion;intelligence artificielle;web service;automated reasoning;modelisation;raisonnement automatique;first order;lenguaje descripcion;web semantica;logique ordre 1;representation connaissance;semantic web;artificial intelligence;ontologia;inteligencia artificial;description logic;knowledge representation;modeling;ontology;formal ontology;first order logic;langage description;servicio web;razonamiento automatico;logique description;logica orden 1;description language	Formal ontologies play an increasingly important role in demanding knowledge representation applications like the Semantic Web. Regarding automated reasoning support, the mainstream of research focusses on ontology languages that are also Description Logics, such as OWL-DL. However, many existing ontologies go beyond Description Logics and use full first-order logic. We propose a novel transformation technique that allows to apply existing model computation systems in such situations. We describe the transformation and some variants, its properties and intended applications to ontological reasoning.	automated reasoning;automated theorem proving;computation;description logic;disjunctive normal form;existential quantification;experiment;first-order logic;first-order predicate;knowledge representation and reasoning;logic programming;ontology (information science);programming model;semantic web;soundness (interactive proof);suggested upper merged ontology;undecidable problem;unique name assumption	Peter Baumgartner;Fabian M. Suchanek	2006		10.1007/11853107_2	natural language processing;knowledge representation and reasoning;computer science;artificial intelligence;ontology;first-order logic;ontology language;reasoning system;automated reasoning;web ontology language;algorithm	AI	-20.68527221142375	9.555923419277004	45997
7e47d608a0ba0e75949f6a6143a5325415a02566	identifying robust plans through plan diagram reduction	query processing;supercomputer education research centre;computer science automation formerly school of automation;parameter space;robust performance;database query	Estimates of predicate selectivities by database query optimizers often differ significantly from those actually encountered during query execution, leading to poor plan choices and inflated response times. In this paper, we investigate mitigating this problem by replacing selectivity error-sensitive plan choices with alternative plans that provide robust performance. Our approach is based on the recent observation that even the complex and dense “plan diagrams” associated with industrial-strength optimizers can be efficiently reduced to “anorexic” equivalents featuring only a few plans, without materially impacting query processing quality. Extensive experimentation with a rich set of TPC-H and TPCDS-based query templates in a variety of database environments indicate that plan diagram reduction typically retains plans that are substantially resistant to selectivity errors on the base relations. However, it can sometimes also be severely counter-productive, with the replacements performing much worse. We address this problem through a generalized mathematical characterization of plan cost behavior over the parameter space, which lends itself to efficient criteria of when it is safe to reduce. Our strategies are fully non-invasive and have been implemented in the Picasso optimizer visualization tool.	database;diagram;ibm tivoli storage productivity center;mathematical optimization;optimizing compiler;selectivity (electronic)	Harish Doraiswamy;Pooja N. Darera;Jayant R. Haritsa	2008	PVLDB	10.14778/1453856.1453976	query optimization;simulation;computer science;data mining;database;parameter space;programming language;statistics	DB	-25.31892963868616	5.66256811954328	46087
a2cefb469e21ad590f97c39099fa2335bb520406	category-theoretic co-products, schema discrepancies and role abstractions in information systems	information system		information system	Robert M. Colomb	1996			computer science;database;data mining;information system;information schema;three schema approach;abstraction;schema (psychology)	DB	-31.72453616376294	10.434491679874263	46104
d68c71ecc56032043d1720b7ca8eff8d7fb97618	the hybrid technique for reference materialization in object query processing	query language;pointer based techniques;oql;query processing;single valued attributes;rule based;performance;object query evaluation;software performance evaluation;value based techniques;query optimization;reference material;query optimization reference materialization query processing object reference resolution object query evaluation pointer based techniques value based techniques hybrid technique performance single valued attributes collection valued attributes algebraic transformations rule based query optimizer object oriented database speedup object oriented query languages oql object relational databases sql 1999;object oriented query languages;sql 1999;query languages;hybrid approach;object reference resolution;rule based query optimizer;object relational databases;object oriented;query evaluation;collection valued attributes;reference materialization;object oriented databases;object oriented database;speedup;object oriented languages;object relational;software performance evaluation object oriented databases query processing query languages object oriented languages;hybrid technique;query processing partitioning algorithms materials science and technology object oriented databases spatial databases database languages proposals object oriented modeling algebra assembly;algebraic transformations	Resolving object references, or reference materialization, is a fundamental operation in object query evaluation. Existing reference materialization techniques fall into two categories: pointer-based and value-based. We identify several drawbacks of existing techniques, and propose a hybrid technique that combines the advantages of each category. This technique relaxes the limitations of valuebased techniques, while preserving much of their performance advantage over pointer-based techniques; it performs well in the cases when no existing algorithm is applicable or efficient. The hybrid technique shows even stronger performance advantages when moving from single-valued to collection-valued attributes. We present algebraic transformations to enable the hybrid technique in the rule-based query optimizer. Initial experimental results using a commercial object-oriented database show that the hybrid approach achieves significant speedup over current algorithms in many cases. The initial motivation for our work was optimization and evaluation of object-oriented query languages, particularly OQL [ODMG]. However, the key features we have concentrated on, references and collectionvalued attributes, are present in object-relational products and the SQL:1999 proposal [EM99].	algorithm;linear algebra;logic programming;mathematical optimization;object data management group;object query language;object-relational database;pointer (computer programming);query optimization;sql:1999;speedup	Quan Wang;David Maier;Leonard D. Shapiro	2000		10.1109/IDEAS.2000.880560	rule-based system;computer science;theoretical computer science;database;programming language;object-oriented programming;query language;object query language	DB	-28.664745880922816	7.639078160924455	46182
40709c0e27d0fe6f9265b6c29847e63392f4001d	multiple aggregations over data streams	information systems;metadata;information retrieval;data stream;greedy heuristic;optimization problem;semistructured data;data analysis;navigation;searching browsing;cost optimization;synthetic data;high speed;data stream management system;conference proceeding	Monitoring aggregates on IP traffic data streams is a compelling application for data stream management systems. The need for exploratory IP traffic data analysis naturally leads to posing related aggregation queries on data streams, that differ only in the choice of grouping attributes. In this paper, we address this problem of efficiently computing multiple aggregations over high speed data streams, based on a two-level LFTA/HFTA DSMS architecture, inspired by Gigascope.Our first contribution is the insight that in such a scenario, additionally computing and maintaining fine-granularity aggregation queries (phantoms) at the LFTA has the benefit of supporting shared computation. Our second contribution is an investigation into the problem of identifying beneficial LFTA configurations of phantoms and user-queries. We formulate this problem as a cost optimization problem, which consists of two sub-optimization problems: how to choose phantoms and how to allocate space for them in the LFTA. We formally show the hardness of determining the optimal configuration, and propose cost greedy heuristics for these independent sub-problems based on detailed analyses. Our final contribution is a thorough experimental study, based on real IP traffic data, as well as synthetic data, to demonstrate the effectiveness of our techniques for identifying beneficial configurations.	computation;experiment;greedy algorithm;heuristic (computer science);mathematical optimization;optimization problem;referring expression generation;synthetic data	Rui Zhang;Nick Koudas;Beng Chin Ooi;Divesh Srivastava	2005		10.1145/1066157.1066192	optimization problem;navigation;greedy algorithm;computer science;theoretical computer science;data mining;database;data stream mining;data analysis;metadata;information system;synthetic data	DB	-25.228780885120987	4.389777173838461	46253
e1e4e7861d1da372e543ff7d4243afe4a30c9663	confronting database complexities	model design;user needs;history;database designers;database users;database management systems;information retrieval;distributed heterogeneous databases;database tools database complexities database technology hierarchical database model relational database model object oriented databases distributed heterogeneous databases specialized database models database designers database programmers database users;database management systems data structures;hierarchical database model;relational database model;object oriented;data structures;database systems;relational model;distributed databases;database tools;relational databases;centralized control;object oriented databases;relational databases object oriented databases data models object oriented modeling file systems centralized control database systems information retrieval distributed databases history;database technology;specialized database models;database complexities;object oriented modeling;file systems;data models;database programmers	Database technology is exploding, as the hierarchical and relational models give way to object-oriented, distributed heterogeneous, and other kinds of specialized models. Designers, programmers, and users need new tools.<<ETX>>	database;programmer	Clement T. Yu;Weiyi Meng	1994	IEEE Software	10.1109/52.281712	relational model;data structure;computer science;data mining;database;world wide web;distributed database	DB	-32.56987722247619	12.106995677545545	46414
e3b1b4ffa56313562c66387491e27b60b4722295	a family of incomplete relational database models	relational database	In this paper. we utilize intervals for unknown values in incomplete relational databases. We use tables to represent unknown relations. First, we define three partial tuple types in a table to specify incompleteness relationships among tuples of the same table. For tuples of different tables, we distinguish between the cases where incompleteness are introduced at the relation level, tuple level or attribute-value level. And, based on these relationships among tuples in different tables, we present a family of incomplete relational database models. tuples in the set of cubes (also called d-rectangles) are called candidate tuples for the tmknown tuple t, and exactly one of the points is the tmknown tuple. This approach allows database operations to be. transformed into operations in Computational Geometry, leading to efficient operator evaluations [OlaO 88a]. Following [ImiL 841 we use the terminology that a table in the incomplete database enviromnent represents a relation some tuples of which are unknown. Tables contain partial tupks (i.e., tuples with incomplete components) as well as total tuples (i.e., tuples whose components are all known). For each of the models, the query evaluation is sound (i.e., no incorrect results are derivable). None of the models is complete (i.e., all valid conclusions are derivable). We briefly compare two of the models in the family with other approaches. Considering each table tuple as a set of d-dimensional cubes, each model in the family of models presented in this paper can be considered as a geometric database model. We are presently implementing a version of one of the models. We briefly summarize the geometric operations and the primitive update semantics being utilii in the implementation. Figure 1. Geometric View of Partial Tuples	computation;computational geometry;database model;olap cube;relational database;table (database)	Adegbemiga Ola;Gultekin Özsoyoglu	1989			database theory;nested set model;relational model;relational calculus;entity–relationship model;relational database;computer science;database normalization;database model;database;database schema;object-relational impedance mismatch;database design	DB	-25.93915449343724	10.625841290025246	46421
9933b7cee82553067cc822f21a9666ed10999e88	join index hierarchies for supporting efficient navigations in object-oriented databases	indexation;object oriented database	A join index hierarchy method is proposed to handle the “goto’s on disk” problem in objectoriented query processing. The method constructs a hierarchy of join indices and transforms a sequence of pointer chasing operations into a simple search in an appropriate join index file, and thus accelerates navigation in object-oriented databases. The method extends the join index structure studied in relational and spatial databases, supports both forward and backward navigations among objects and classes, and localizes update propagations in the hierarchy. Our performance study shows that partial join index hierarchy outperforms several other indexing mechanisms in object-oriented query processing.	chomsky hierarchy;database index;disk buffer;experiment;goto;mathematical optimization;pointer (computer programming);spatial database	Zhaohui Xie;Jiawei Han	1994			computer science;data mining;database;information retrieval	DB	-28.968677957063043	5.219932003167267	46422
567a540d7402658451dec6b07efec6a31898f466	completeness management for rdf data sources		The Semantic Web is commonly interpreted under the open-world assumption, meaning that information available (e.g., in a data source) captures only a subset of the reality. Therefore, there is no certainty about whether the available information provides a complete representation of the reality. The broad aim of this article is to contribute a formal study of how to describe the completeness of parts of the Semantic Web stored in RDF data sources. We introduce a theoretical framework allowing augmentation of RDF data sources with statements, also expressed in RDF, about their completeness. One immediate benefit of this framework is that now query answers can be complemented with information about their completeness. We study the impact of completeness statements on the complexity of query answering by considering different fragments of the SPARQL language, including the RDFS entailment regime, and the federated scenario. We implement an efficient method for reasoning about query completeness and provide an experimental evaluation in the presence of large sets of completeness statements.	analysis of algorithms;application domain;best practice;best, worst and average case;communication endpoint;conjunctive query;crowdsourcing;dbpedia;data integrity;data quality;description logic;digital back-propagation;donald becker;dublin core;enumerated type;experiment;identifier;jack lutz;linked data;missing data;np-completeness;open-world assumption;overhead (computing);rdf schema;relation (database);relational database;relevance;resource description framework;sparql;semantic web;sieve (mail filtering language);statement (computer science);three-valued logic;time complexity;timestamp-based concurrency control;void;vocabulary;web ontology language;world wide web;xml	Fariz Darari;Werner Nutt;Giuseppe Pirrò;Simon Razniewski	2018	TWEB	10.1145/3196248	information retrieval;completeness (statistics);computer science;rdf;sparql;metadata	Web+IR	-25.291661549573988	8.17947630316929	46428
f82d89bb67d27ebeb59031b9d48626d420e678e6	enhanced er to relational mapping and interrelational normalization	institutional repositories;computacion informatica;fedora;grupo de excelencia;relational database;vital;computer aided software engineering;ciencias basicas y experimentales;relational model;normal form;vtls;ils;entity relationship	This paper develops a method that maps an enhanced Entity-Relationship (ER1) schema into a relational schema and normalizes the latter into an inclusion normal form (IN-NF). Unlike classical normalization that concerns individual relations only, IN-NF takes interrelational redundancies into account and characterizes a relational database schema as a whole. The paper formalizes the sources of such interrelational redundancies in ER1 schemas and speci®es the method to detect them. Also, we describe brie ̄y a Prolog implementation of the method, developed in the context of a Computed-Aided Software Engineering shell and present a case study. q 2000 Elsevier Science B.V. All rights reserved.	algorithm;boyce–codd normal form;case preservation;data dependency;database design;database normalization;database schema;digraphs and trigraphs;enhanced entity–relationship model;entity;erdős–rényi model;fo (complexity);formal system;global variable;join dependency;map;new foundations;program optimization;prolog;recursion;relational database;software engineering;third normal form;vertex-transitive graph	Manuel Kolp;Esteban Zimányi	2000	Information & Software Technology	10.1016/S0950-5849(00)00132-4	schema migration;information schema;relational model;relational calculus;semi-structured model;entity–relationship model;relational database;computer science;artificial intelligence;database normalization;data mining;database;computer-aided software engineering;superkey	DB	-27.987408042096398	11.656503782346997	46448
ec5eaee15952bbffe95829fe97b77f3a598489d7	incremental maintenance of dynamic datalog programs.		This paper is an extended abstract of [5], which discusses the incremental maintenance of materialized ontologies in a rule-enabled Semantic Web. The reader may wonder how this relates to Datalog, a language that has been proposed in the deductive database context. However, the semantics underlying Web ontology languages can often be realized by translating the ontology into appropriate Datalog programs. Therefore, our solution for incrementally maintaining materialized ontologies actually builds on the incremental maintenance of dynamic Datalog programs. Materialization generally allows to speed up query processing and inferencing by explicating the implicit entailments which are sanctioned by the rules of the program. The complexity of reasoning with Datalog is thereby shifted from query time to update time. We assume that materialization techniques will frequently be important for the Semantic Web to achieve a scalable solutions, since read access to is predominant in a Web setting. Central to materialization are maintenance techniques that allow to incrementally update a materialization when changes occur. We present a novel solution that allows to cope with changes in rules and facts for Datalog programs. To achieve this we extend a known approach for the incremental maintenance of views (intentional predicates) in deductive databases. We show how our technique can be employed for a broad range of existing Web ontology languages, such as RDF/S and subsets of OWL. Our technique can be applied to a wide range of ontology languages, namely those that can be axiomatized by a set of rules in Datalog with stratified negation. The challenge that has not been tackled before is dealing with updates and new definitions of rules. However, our solution extends a declarative algorithm for the incremental maintenance of views [4] that was developed in the deductive database context.	algorithm;datalog;deductive database;negation as failure;ontology (information science);resource description framework;scalability;semantic web	Raphael Volz;Steffen Staab;Boris Motik	2003			programming language;datalog;computer science	DB	-24.558960935040545	8.749872582328436	46740
21c48485250f671c88d97521e40a8891ec1d961f	an entity relationship programming language	lenguaje programacion;entity relationship model;query language;computer languages;base donnee;syntax;high level languages;finance;programming language;binary relationships entity relationship programming language syntax integrated e r programming language query language general purpose programming language entity sets mutually disjoint entity type unique multiattribute key;unique;langage manipulation donnee;interrogation base donnee;lenguaje manipulacion dato;runtime environment;database;interrogacion base datos;base dato;modelo entidad relacion;modele entite relation;indexing terms;syntaxe;multiattribute;key;binary relationships;entity sets;mutually disjoint;computer languages database languages data models relational databases runtime environment environmental economics finance;environmental economics;entity relationship programming language;entity type;langage programmation;relational databases;sintaxis;database languages;database query;integrated e r programming language;data manipulation language;entity relationship;data models;general purpose programming language	The syntax for an integrated E-R programming language is presented. The problems that arise when a query language is embedded in a general-purpose programming language are discussed. Other E-R languages are also discussed. The requirements for the language and a syntax for an E-R model in which entity sets are mutually disjoint and each entity type has a unique, perhaps multiattribute, key are presented. The syntax for a more limited model restricted to binary relationships between entity types and without attributes is presented. Some implementation considerations are discussed. >		Ashok Malhotra;Harry M. Markowitz;Yakov Tsalalikhin;Donald P. Pazel;Luanne M. Burns	1989	IEEE Trans. Software Eng.	10.1109/32.31369	natural language processing;fourth-generation programming language;abstract syntax;very high-level programming language;language primitive;data manipulation language;specification language;programming domain;entity–relationship model;data control language;computer science;third-generation programming language;syntax;database;programming paradigm;low-level programming language;fifth-generation programming language;visual programming language;programming language;homoiconicity;programming language specification;high-level programming language;query language;abstract syntax tree;syntax error	SE	-29.644151824845274	12.717997381215234	46998
137c68cc75603af36a04db2c3b3936a9df0f4c89	dyst: dynamic and scalable temporal text indexing	temporal document;query processing;dynamic and scalable temporal text indexing;text analysis;temporal text containment queries;indexing costs neodymium prototypes transaction databases information science html xml search engines keyword search;very large database;document database;dyst;indexing;temporal document dyst dynamic and scalable temporal text indexing temporal text containment queries document database very large databases;search cost;very large databases indexing query processing temporal databases text analysis;temporal databases;very large databases;text indexing	An increasing number of documents in companies and other organizations are now only available electronically, and exist in several versions updated at different times. In order to provide efficient support for temporal text-containment queries (query for all versions of documents that contained one or more particular words at a particular time) temporal text-indexes are needed. In this paper we present DyST, a dynamic and scalable temporal text index. The goal of DyST is to provide the same efficiency in terms of search cost and space usage as the previous approaches developed for small and medium document databases, while at the same time providing only logarithmically increasing search cost for very large databases. We present the architecture of DyST and describe how inserts and searches are performed. Based on a prototype we also present an evaluation of performance based on real-life temporal documents	database;document;prototype;real life;scalability;space–time tradeoff	Kjetil Nørvåg;Albert Overskeid Nybø	2006	Thirteenth International Symposium on Temporal Representation and Reasoning (TIME'06)	10.1109/TIME.2006.12	computer science;data mining;database;information retrieval	DB	-31.575901802165706	4.401125081000275	47523
5e611e967c1d273069488a20f7695603c7bd4320	on interpretations of relational languages and solutions to the implied constraint problem	program conversion;relational database;automatic programming;first order;constraint programming;database design;constraints	The interconnection between conceptual and external levels of a relational database is made precise in terms of the notion of “interpretation” between first-order languages. This is then used to obtain a methodology for discovering constraints at the external level that are “implied” by constraints at the conceptual level and by conceptual-to-external mappings. It is also seen that these concepts are important in other database issues, namely, automatic program conversion, database design, and compile-time error checking of embedded database languages. Although this the deals exclusively with the relational approach, it also discusses how these ideas can be extended to hierarchical and network databases.	compile time;compiler;database design;embedded database;embedded system;first-order predicate;interconnection;interpretation (logic);relational database	Barry E. Jacobs;Alan R. Aronson;Anthony C. Klug	1982	ACM Trans. Database Syst.	10.1145/319702.319730	constraint programming;database theory;relational model;codd's theorem;relational calculus;entity–relationship model;relational database;computer science;theoretical computer science;database model;first-order logic;data mining;database;conjunctive query;programming language;view;database schema;consistency;alias;object-relational impedance mismatch;database design	DB	-28.722319924239788	11.176433752168148	47533
756ad828fd07ab0e3e042ed0246f737ddaab2759	using active objects for query processing		Most object-oriented databases can only be accessed by a programming language rather than a query language, although an ad hoc query facility is a desirable feature of such systems. In this paper, we present a way to process queries to extract data from a collection of interacting objects. We adopt the object-oriented paradigm to describe the operational semantics of queries given as expressions of a query algebra. An expression can be transformed into a collection of active objects performing the operations. We show that we are able to implement query processing using the existing framework for collections of objects, which is based on objects modeled as processes. Our approach makes parallel query processing possible.	apl;database;hoc (programming language);interaction;operational semantics;programming language;programming paradigm;query language	Ralf Jungclaus;Gunter Saake	1990			data mining;query expansion;web search query;computer science;information retrieval;query language;sargable;query optimization	DB	-30.528166010271136	9.389973605339685	47605
38f596253457910ff89dcf6285d59d21e29f0464	automatic verification of transactions on an object-oriented database	automatic verification;lenguaje programacion;relational data model;tratamiento transaccion;object oriented data model;programming language;functional programming;theorem prover;semantic mapping;integrity constraints;base donnee orientee objet;langage programmation;programmation fonctionnelle;object oriented databases;object oriented database;information system;analisis semantico;transaction processing;analyse semantique;programacion funcional;higher order logic;systeme information;traitement transaction;semantic analysis;sistema informacion	1 I n t r o d u c t i o n Static integrity constraints are essential in mission-critical application domains, where one wants to offer integrity preserving update operations to clients. One way to enforce database integrity is by testing at run-time those constraints that are possibly violated by a transaction before allowing the transaction to commit. Various techniques, surveyed in [1], have been proposed to optimize such a test for a limited class of simple constraints, such as key and referential integrity. But commiting complex transactions on large amounts of data becomes increasingly difficult if the constraint language is extended to include full first-order logic formula with bounded quantifications over arbi trary collection types. A second approach towards integrity maintenance aims at a compile-time reduction of the amount of run-time transaction overhead due to integrity constraint checking. This approach was first introduced by Sheard & Stemple ([2]) for the relational data model. It uses a theorem prover to verify that a transaction will never raise an integrity conflict, provided that the database was in a consistent state before the transaction was executed. Transactions are complex updates involving multiple relations, whereas the constraint language includes quantifications and aggregate constructs. These are related to expressions in higher-order logic for automatic proof assistance. Another related compile-time approach is proposed in [3,4]. It exploits several techniques of abstract interpretation for the task of compile-time transaction verification in an 0 2 database system extended with a notion of declarative integrity Our e-mail addresses are: [spel t ,ba ls ters ] @cs.utwente. nl	abstract interpretation;aggregate data;automated theorem proving;compile time;compiler;data integrity;data model;database;email;first-order logic;first-order predicate;mission critical;nl (complexity);overhead (computing);referential integrity;relational model;transaction verification	David Spelt;Herman Balsters	1997		10.1007/3-540-64823-2_22	relational model;higher-order logic;transaction processing;computer science;theoretical computer science;data integrity;database;automated theorem proving;programming language;functional programming;information system;algorithm	DB	-26.77743027850964	12.317915846285832	47647
03629e41f60c60747477ed3cb1238b5dad5e69e3	dynamic query optimization under access limitations and dependencies	inf;computer science and information systems;query optimization;functional dependency;necessary and sufficient condition;dynamic query;polynomial time;query answering;data retrieval	Unlike relational tables in a database, data sources on the Web typically can only be accessed in limited ways. In particular, some of the source fields may be required as input and thus need to be mandatorily filled in order to access the source. Answering queries over sources with access limitations is a complex task that requires a possibly recursive evaluation even when the query is non-recursive. After reviewing the main techniques for query answering in this context, in this article we consider the impact of functional and inclusion dependencies on dynamic query optimization under access limitations. In particular, we address the implication problem for functional dependencies and simple full-width inclusion dependencies, and prove that it can be decided in polynomial time. Then we provide necessary and sufficient conditions, based on the dependencies together with the data retrieved at a certain step of the query answering process, that allow avoiding unnecessary accesses to the sources.	algorithm;chase (algorithm);conjunctive query;dynamic programming;form (html);functional dependency;mathematical optimization;np-completeness;np-hardness;query optimization;query plan;recursion;referential integrity;relational database;relevance;time complexity;world wide web	Andrea Calì;Diego Calvanese;Davide Martinenghi	2009	J. UCS	10.3217/jucs-015-01-0033	time complexity;sargable;query optimization;query expansion;web query classification;boolean conjunctive query;dependency theory;computer science;query by example;data mining;database;rdf query language;functional dependency;view;data retrieval;information retrieval;query language	DB	-26.62498325957003	7.992196655524743	47864
0b12c24d4c722156aeacbcde2532eac8f7059b74	implementing memoization in a streaming xquery processor	workload;bea;base donnee;streaming;query processing;behavioral analysis;transmision continua;xml language;interrogation base donnee;database;interrogacion base datos;base dato;cache memory;antememoria;antememoire;transmission en continu;traitement question;analyse comportementale;memoisation;charge travail;analisis conductual;xml query processing;funcion memo;information system;carga trabajo;memoization;use case;database query;systeme information;langage xml;lenguaje xml;sistema informacion	In this paper, we describe an approach to boosting the performance of an XQuery engine by identifying and exploiting opportunities to share processing both within and across XML queries. We first explain where sharing opportunities arise in the world of XML query processing. We then describe an approach to shared XQuery processing based on memoization, providing details of an implementation that we built by extending the streaming XQuery processor that BEA Systems incorporates as part of their BEA WebLogic Integration 8.1 product. To explore the potential performance gains offered by our approach, we present results from an experimental study of its performance over a collection of use-case-inspired synthetic query workloads. The performance results show that significant overall gains are indeed available.	cache (computing);compile time;compiler;database normalization;experiment;heuristic (computer science);lazy evaluation;lookup table;memoization;oracle weblogic server;rewrite (programming);rewriting;runtime system;software bug;synthetic intelligence;turing completeness;xml;xquery	Yanlei Diao;Daniela Florescu;Donald Kossmann;Michael J. Carey;Michael J. Franklin	2004		10.1007/978-3-540-30081-6_4	use case;xml;memoization;cpu cache;computer science;operating system;data mining;database;programming language;world wide web;information system	DB	-27.857688037339926	6.206751450278793	47975
ee02bff3020f8376a05fd5aa28d06ea183d19324	selectively storing xml data in relations	base relacional dato;base donnee;sistema experto;mise a jour;human computer interaction;computer communication networks;definicion tipo documento;user interface;sql;xml language;definition type document;interrogation base donnee;database;interrogacion base datos;base dato;semantics;intelligence artificielle;relational database;data mining;semantica;semantique;database management;artificial intelligent;actualizacion;analyse syntaxique;analisis sintaxico;syntactic analysis;base donnee relationnelle;xml document;artificial intelligence;inteligencia artificial;information system;systeme expert;information storage and retrieval;database query;langage xml;lenguaje xml;updating;document type definition;communication service;knowledge discovery;expert system	This paper presents a new framework for users to select relevant data from an XML document and store it in an existing relational database, as opposed to previous approaches that shred the entire XML document into a newly created database of a newly designed schema. The framework is based on a notion of XML2DB mappings. An XML2DB mapping extends a (possibly recursive) DTD by associating element types with semantic attributes and rules. It extracts either part or all of the data from an XML document, and generates SQL updates to increment an existing database using the XML data. We also provide an efficient technique to evaluate XML2DB mappings in parallel with SAX parsing. These yield a systematic method to store XML data selectively in an existing database.	algorithm;data integrity;emoticon;mathematical optimization;parsing;recursion;relational database;sql;shredding (disassembling genomic data);xml	Wenfei Fan;Lisha Ma	2006		10.1007/11827405_3	well-formed document;xml catalog;xml validation;xml encryption;xml namespace;simple api for xml;xml;relax ng;xml schema;streaming xml;computer science;document type definition;document structure description;database model;xml framework;data mining;xml database;xml schema;database;semantics;xml signature;world wide web;xml schema editor;expert system;efficient xml interchange	DB	-32.36221665346766	9.802588734204576	47993
74d2058a2528e8fa2a935398c3138052b9a371a8	the possibility problem for probabilistic xml (extended version)			xml	Antoine Amarilli	2014	CoRR		artificial intelligence;theoretical computer science;data mining;algorithm	Theory	-31.590043620619177	6.680748599399412	48047
f792390ca6a74588fde0f339fc5fec9ba3edfd73	incorporating ontology-based semantics into conceptual modelling	class hierarchy;uml;semantics;orm;er;conceptual modelling;ontouml;eer;ontoorm;ontology;ontoer	With the increasing complexity of applications and user needs, recent research has shifted from a data-information level to a human semantic level interaction. Research has begun to address the increasing use and development of ontologies in various applications, strongly motivated by the semantic web initiative. However, existing conceptual models are not rich enough to incorporate ontologies in one single conceptual schema. To improve this situation, it is necessary to refine modelling formalisms and make them more expressive while ensuring they remain semantically sound. We argue that conceptual modelling methodologies would be semantically richer if they were able to express the semantics of a domain that arises in concrete application scenarios. This paper investigates the incorporation of ontologies into three popular conceptual modelling methodologies, presenting the Ontological Entity-Relationship ( OntoER ) model, Ontological Object Role Modelling ( OntoORM ) and the Ontological Unified Modelling Language ( OntoUML ) class diagram. An extended conceptual framework for modelling ontologies and a transformation algorithm for mapping ontological constructs to relational schemata are provided so that querying the database through the conceptualisation of the database can be managed.	conceptual schema	Somluck La-Ongsri;John F. Roddick	2015	Inf. Syst.	10.1016/j.is.2015.02.003	natural language processing;unified modeling language;conceptual model;computer science;knowledge management;object-relational mapping;ontology;data mining;database;semantics	DB	-32.411916383534795	13.582677879935385	48105
a4dd8076f3bdb5e38feb7f23f19c61c019440995	an extended er algebra to support semantically richer queries in erdbms		In this paper we present the foundations for a semantically rich main-memory DBMS based on the entity-relationship data model. The DBMS is fully operational and performs all queries that are illustrated in the paper. So far, the ER model is mainly used as a conceptual model and mapped into the relational model. Semantics like the relationships among entities or the cardinality ratio constraints are not explicit in the relational model. This paper treats the ER model as a logical model for the user and we use the relational as the physical model in our ER model based DBMS - ERDBMS. We use CISC (complex instruction set computing) operators but implement them efficiently in main-memory data storage. This paper concentrates on the extended ER algebra. Our high-level query language ERSQL and the main memory implementation are elaborated in [14].		Moritz Wilfer;Shamkant B. Navathe	2015		10.1007/978-3-319-25264-3_46	theoretical computer science;data mining;information retrieval	DB	-30.305082822724362	11.013036777470331	48146
7e4982a3b469fc81ee2227babc8220d32c4af515	client-server architecture for pre and post-processing of real problems involving two-dimensional generalized coordinates	web databases;web architecture;mobile computing for the internet;web data integration;advanced web applications	Purpose – The aim of this paper is to propose a Web environment for pre-processing and post-processing for 2D problems in generalized coordinate systems. Design/methodology/approach – The system consists of a Web service for client-server communication, a database for user information, simulation requests and results storage, a module of (for) calculation processing (front-end) and a graphical interface for visualization of discretized mesh (back-end). Findings – The Web system was able to model real problems and situations, where the user can describe the problem or upload a geometry file descriptor, generated from computer graphics software. The Web system, programmed for finite difference solutions, was able to generate a mesh from other complex methods, such as finite elements method, adapting it to the proposed Web system, respecting the finite difference mesh structure. Research limitations/implications – The proposed Web system is limited to solve partial differential equations by finite difference...	server (computing);video post-processing	José Luiz Vilas Boas;Fábio T. Matsunaga;Neyva Maria Lopes Romeiro;Jacques Duilio Brancher	2015	IJWIS	10.1108/IJWIS-12-2014-0044	web service;website architecture;web modeling;data web;web mapping;web-based simulation;web design;computer science;theoretical computer science;operating system;web navigation;data mining;database;distributed computing;world wide web;web server;mashup	Theory	-32.3933372920717	17.019666001301594	48150
a723d446c886ebec80ca090dd632399041805080	from semanticobjects to semantic software engineering	software engineering;object relational database	One of the most important pieces of a database management system is its query language and the underlying algebra. This paper describes the query language of SemanticObjects, a development framework for object relational applications, and its associated object relational algebra. The paper also describes a semantic software engineering methodology that streamlines the development of object relational applications based on SemanticObjects.		Phillip C.-Y. Sheu;Atsushi Kitazawa	2007	Int. J. Semantic Computing	10.1142/S1793351X07000020	semantic data model;domain relational calculus;query optimization;sql;relational model/tasmania;relational model;relational calculus;entity–relationship model;relational database;computer science;query by example;object;database model;data mining;database;programming language;database design	SE	-31.090414723225077	10.760713235403035	48465
d7a128d6c0444c5e53327ca564485c008f3862ff	temporal database architecture enhancements		Database systems used in the past were characterized by storing only current valid states., which is not., however., optimal for intelligent systems and applications. Temporal paradigm allows us to delimit the object state with the time validity regarding the modeled granularity. Temporal systems are based on the extension of conventional approaches., which do not provide powerful solutions. Database server architecture is described with emphasis on the optimization options to improve and shorten the data retrieval process. In this paper., we attach significance to the migrated row., which forces the system to load multiple data blocks from the database into the memory. The solution., based on the evolution steps., is implemented using the mapping module inside the memory of the database instance.	data retrieval;database server;mathematical optimization;programming paradigm;server (computing);temporal database	Michal Kvet;Emil Krsák;Karol Matiasko	2018	2018 22nd Conference of Open Innovations Association (FRUCT)	10.23919/FRUCT.2018.8468305	database;granularity;architecture;intelligent decision support system;database server;temporal database;data retrieval;computer science	DB	-29.962213665089998	6.742575019809463	48533
f39014826ba3d21cf40185ce87b5a5cfa7cba1bd	introducing abduction into (extensional) inductive logic programming systems	abduction;learning;circonscription;inductive logic programming;logical programming;circumscription;aprendizaje;incomplete data;apprentissage;abduccion;programmation logique;background knowledge;logic programs;programacion logica;abductive logic programming;circonscripcion;knowledge base	We propose an approach for the integration of abduction and induction in Logic Programming. In particular, we show how it is possible to learn an abductive logic program starting from an abductive background knowledge and a set of examples. By integrating Inductive Logic Programming with Abductive Logic Programming we can learn in presence of incomplete knowlegde. Incomplete knowledge is handled by designating some pieces of information as abducibles, that is, possible hypotheses which can be assumed, provided that they are consistent with the current knowledge base. We then specialize the frameworks for FOIL, an ILP system adopting extensional coverage. In particular, we propose an extension of the FOIL algorithm that is able to learn from incomplete data.	abductive reasoning;inductive logic programming	Evelina Lamma;Paola Mello;Michela Milano;Fabrizio Riguzzi	1997		10.1007/3-540-63576-9_107	knowledge base;computer science;artificial intelligence;machine learning;inductive programming;logic programming;circumscription;algorithm;abductive logic programming	AI	-20.08576174257432	10.949416622027703	48544
c862496e0298d1c9c0fa7d2f39f51d1f5059f17f	sroiq syntax approximation by using nominal schemas		Nominal schemas is a recently introduced extension of description logics which makes it possible to express rules which generalize DL-safe ones. A tractable description logic, ELROVn, has been identified. This leads us to the question: can we improve approximate reasoning results by employing nominal schemas? In this paper, we investigate how to approximately cast SROIQ into ELROVn. Using a datalog-based tractable algorithm, a preliminary evaluation shows that our approach can indeed do approximate SROIQ-reasoning with a high recall.	approximation algorithm;case-based reasoning;cobham's thesis;datalog;description logic;nominal type system;ontology (information science);tron	Cong Wang;David Carral;Pascal Hitzler	2013			syntax;mathematics;description logic;datalog;algorithm;schema (psychology)	AI	-20.0360832701089	9.248719066943258	48631
081c366bdb4af9af80406f1f52f13cce372e7f1d	database query processing using finite cursor machines	model specification;relation algebra;query processing;abstract state machine;data stream;database query processing;database;semijoin;upper and lower bounds;database query;relational algebra	We introduce a new abstract model of database query processing, finite cursor machines, that incorporates certain data streaming aspects. The model describes quite faithfully what happens in so-called “one-pass” and “two-pass query processing”. Technically, the model is described in the framework of abstract state machines. Our main results are upper and lower bounds for processing relational algebra queries in this model, specifically, queries of the semijoin fragment of the relational algebra.	abstract state machines;cursor (databases);database;relational algebra	Martin Grohe;Yuri Gurevich;Dirk Leinders;Nicole Schweikardt;Jerzy Tyszkiewicz;Jan Van den Bussche	2007	Theory of Computing Systems	10.1007/s00224-008-9137-7	online aggregation;sargable;query optimization;query expansion;relational model;codd's theorem;boolean conjunctive query;entity–relationship model;relational algebra;relational database;computer science;query by example;theoretical computer science;database model;data mining;database;mathematics;conjunctive query;web search query;view;alias;database design;query language;spatial query	DB	-29.44910288389657	8.693963532171503	48686
43aa9d7cf6733f435b8761ab0c11b47d89ba3ec1	graank: exploiting rank correlations for extracting gradual itemsets	data mining;gradual dependencies;gradual itemsets;ranking comparison;gradual rules;rank correlation	Gradual dependencies of the form the more A, the more B offer valuable information that linguistically express relationships between variations of the attributes. Several formalisations and automatic extraction algorithms have been proposed recently. In this paper, we first present an overview of these methods. We then propose an algorithm that combines the principles of several existing approaches and benefits from efficient computational properties to extract frequent gradual itemsets.	algorithm;bitmap;causality;computation;database;gradual typing;relevance;sensor	Anne Laurent;Marie-Jeanne Lesot;Maria Rifqi	2009		10.1007/978-3-642-04957-6_33	computer science;machine learning;pattern recognition;data mining;rank correlation	ML	-24.149525737875038	6.054824815397966	48716
7f5b82731061fe24baaa6c414a79894ca15660a3	spatial data base queries: relational algebra versus computational geometry	relation algebra;urban planning;spatial data;computational geometry;relational database;geometric modelling	Conventional queries against relational databases can be expressed in relational algebra. But, when dealing with geometric and spatial queries, one also needs to use computational geometry algorithms.	computation;computational geometry;database;relational algebra	Robert Laurini;Françoise Milleret-Raffort	1988		10.1007/BFb0027520	relational model;relational calculus;computational geometry;relational algebra;relational database;computer science;theoretical computer science;relation algebra;urban planning;database;spatial analysis;conjunctive query;spatial database	DB	-29.656799609694392	8.275076594170706	48724
2b8c08fab02d6ac70cf56d88d08058b66eb70860	database views using data abstraction	database views;data abstraction		abstraction (software engineering);view (sql)	Burt M. Leavenworth	1981			database;data modeling;view;database design;abstraction;computer science	NLP	-31.109914330569666	10.623795071179151	48806
5a5912e4b3e7f382babc155009047192287436ab	query aspects approach to web search			web search engine	Daniel Crabtree;Xiaoying Gao;Peter Andreae	2016	Web Intelligence	10.3233/WEB-160338	sargable;query optimization;query expansion;web modeling;web query classification;ranking;semantic search;web crawler;concept search;search engine	Web+IR	-33.49357889813768	6.197610579993383	48895
da8764532fe2a9aec919db5474e512c92f672756	query optimization for wireless sensor network databases in the madwise system	query optimization;wireless sensor network	We propose a comprehensive approach to distributed query processing in wireless sensor networks. In our proposal we reinterpret the classical approach to database system design according to the wireless sensor networks context, and we redefine the aspects related to the definition of a query language, data model, query algebra, and query optimization strategies. We show that our approach enables optimizations of the query plan which may reduce the costs, in terms of consumed energy, of orders of magnitude.	data (computing);data acquisition;data model;data modeling;database;mathematical optimization;network topology;program optimization;query language;query optimization;query plan;systems design;transducer	Giuseppe Amato;Paolo Baronti;Stefano Chessa	2007			wireless sensor network;wireless network;query language;database;wi-fi array;mobile wireless sensor network;query optimization;query plan;key distribution in wireless sensor networks;computer science	DB	-29.839780513141196	6.937406135866185	48990
10c8a335cb85baab465e1cf92b71584e1912a08d	inductive logic programming: derivations, successes and shortcomings	formal specification;pac learning;inductive logic programming;design rules;protein structure;finite element mesh;first order;drug design;explanation based learning;background knowledge;fault diagnosis	Inductive Logic Programming (ILP) is a research area which investigates the construction of first-order definite clause theories from examples and background knowledge. ILP systems have been applied successfully in a number of real-world domains. These include the learning of structure-activity rules for drug design, finite-element mesh design rules, rules for primary-secondary prediction of protein structure and fault diagnosis rules for satellites. There is a well established tradition of learning-in-the-limit results in ILP. Recently some results within Valiant's PAC-learning framework have also been demonstrated for ILP systems. In this paper it is argued that algorithms can be directly derived from the formal specifications of ILP. This provides a common basis for Inverse Resolution, Explanation-Based Learning, Abduction and Relative Least General Generalisation. A new general-purpose, efficient approach to predicate invention is demonstrated. ILP is underconstrained by its logical specification. Therefore a brief overview of extra-logical constraints used in ILP systems is given. Some present limitations and research directions for the field are identified.	algorithm;explanation-based learning;first-order predicate;general-purpose markup language;inductive logic programming;inductive reasoning;language identification in the limit;probably approximately correct learning;theory	Stephen Muggleton	1993		10.1145/181668.181671	protein structure;computer science;artificial intelligence;theoretical computer science;machine learning;first-order logic;formal specification;programming language;probably approximately correct learning;algorithm;drug design	ML	-20.44600301094912	13.012234394108741	49039
98a7ed07005a4fd63c77ecb94d1a0df00ee14f81	application of modbus protocol based on μc /tcpip in water saving irrigation in facility agricultural		As long as the deepen application of the Intelligent equipment technology in the Water Saving Irrigation, a hot topics in research is coming out, that is how to increase the proportion of the efficiency and output for the high tech facility in manufacture. The Modbus protocol based on u C/TCPIP will play an important role in the Water Saving Irrigation application system, comparing to the others, it has such advantages of the Reliability of communication technology and real-time performance. After analyzing components of Modbus/TCP protocol, we applied it on real-time u C/OS-II kernel and ARM7 software and hardware and hardware environment with Modbus frame embedded in TCP frame. Test results show that Modbus/TCP protocol can achieve industrial standards. The protocol realizes the interconnection of control network and information network and has a broad application prospect in the agricultural water saving irrigation.	internet protocol suite;modbus	Jin-lei Li;Wen-gang Zheng;Chang-jun Shen;Ke-wu Wang	2013		10.1007/978-3-642-54344-9_34	environmental planning;environmental engineering;water resource management	Crypto	-29.433767693671133	17.64078091530834	49056
ace8479c7edb5dd518d393a32aa28f956461c82f	architecture of deductive database systems	application development;logic programming database management systems knowledge based systems;database management systems;deductive databases relational databases database languages navigation buildings solids prototypes robustness concrete logic programming;logic programming;database queries deductive database systems queries reasoning application development declarative logic based languages expert systems knowledge based applications;database query;knowledge based systems;global analysis;deductive databases;knowledge base;expert system	Deductive databases are coming of age with the emergence of efficient and easy-to-use systems that support queries, reasoning, and application development on databases through declarative logic-based languages. The progress is demonstrated by the completion of prototype systems offering such levels of generality, performance, and robustness that they support well complex application development. An overview of the architectures and techniques of these systems is provided. The main motivations for development of the deductive database systems (DDSs) are: (1) to provide support for advanced database applications, with a focus on expert systems and knowledge-based applications; and (2) to provide better support for traditional database applications by integrating the application development and database queries into one language, thus solving the impedance mismatch problems. The key implementation problems for DDSs pertain to finding efficient executions for the given set of rules and queries. For this purpose, the DDS performs a global analysis of rules. The cornerstone of this analysis is the notion of bound arguments and free arguments of predicates.<<ETX>>	characteristic impedance;deductive database;emergence;expert system;prototype;robustness (computer science)	Carlo Zaniolo	1990	Digest of Papers Compcon Spring '90. Thirty-Fifth IEEE Computer Society International Conference on Intellectual Leverage	10.1109/CMPCON.1990.63731	database theory;computer science;theoretical computer science;database model;data mining;database;datalog;view;database schema;graph database;alias;database testing;database design	DB	-28.92595599592137	11.079069051873699	49242
a573888e1e8c187c312eea8815adf68d410db0ea	extending oodb with behavioral temporal management capability	gemstone object oriented database behavioral temporal management object history management temporal database data change database application object behavioral history data management data model query language storage system prototype system;query language;storage system;query processing;history database systems object oriented databases information retrieval data models database languages computer science prototypes;data model;query languages;temporal database;query processing temporal databases object oriented databases data structures query languages;object oriented database systems;data structures;temporal databases;object oriented databases;object oriented database	In order to manage the history of objects in a database, some researchers have proposed temporal database systems. However, data change is always caused by the behavior of objects. A database application may not only need the data history of objects but also the behavioral history of objects. Current temporal databases only consider the data history of objects but do not take the behavioral history of objects into account. The purpose of this paper is to extend the capability of current object-oriented databases so that they can manage the data history and behavioral history of objects. The paper discusses a data model, query language and a storage system. A prototype system is developed on the Gemstone object oriented database system.		Gwo-Dong Chen;Yeong-Hsen Lee;Chen-Chung Liu	1996		10.1109/CMPSAC.1996.544592	data modeling;data definition language;query optimization;database theory;object-based spatial database;intelligent database;data structure;database tuning;computer science;query by example;object-relational mapping;data administration;database model;data mining;database;temporal database;view;database schema;information retrieval;database testing;database design;query language;object query language;spatiotemporal database	Robotics	-29.709322770306173	8.785580358535583	49305
70a3727f7191d7f69901d33f2309993d9292aa0e	protection in data type abstractions using constraints on data values	data type			K. W. Short	1981	Comput. J.	10.1093/comjnl/24.2.118	data type;computer science;data mining;database;programming language	DB	-30.951965546044594	10.756047328796537	49393
aab4abc5657c2489c85b0052163b13fa8c2a21e7	behavioral petri nets: a model for diagnostic knowledge representation and reasoning	linear algebra;model based diagnosis;indexing terms;knowledge representation and reasoning;fault diagnosis knowledge representation petri nets;p invariant computation diagnostic knowledge representation reasoning model based diagnosis behavioral petri net reachability reachability graph analysis;petri nets artificial intelligence parallel processing reachability analysis fuzzy logic knowledge representation linear algebra production fuzzy reasoning formal specifications;petri nets;knowledge representation;petri net;reachability analysis;parallel processing;fault diagnosis	Some of the most popular approaches to model-based diagnosis consist of reasoning about a model of the behaviour of the system to be diagnosed by considering a set of observations about such a system and by explaining it in terms of a set of initial causes. This process has been widely modeled via logical formalisms essentially taking into account declarative aspects. In this paper, a new approach is proposed, where the diagnostic process is captured within a framework based on the formalism of Petri nets. We introduce a particular net model, called Behavioral Petri Net (BPN), We show how the formalization of the diagnostic process can be obtained in terms of reachability in a BPN and can be implemented by exploiting classical analysis techniques of Petri nets like reachability graph analysis and P-invariant computation. Advantages of the proposed methods, like suitability to parallel processing and exploitation of linear algebra techniques, are then pointed out.		Luigi Portinale	1997	IEEE transactions on systems, man, and cybernetics. Part B, Cybernetics : a publication of the IEEE Systems, Man, and Cybernetics Society	10.1109/3477.558794	knowledge representation and reasoning;discrete mathematics;stochastic petri net;computer science;artificial intelligence;theoretical computer science;process architecture;petri net;algorithm	SE	-22.707325495589192	15.73542237280054	49583
3c5f8fe970b2bf9440eb137240ce82bda8e5adff	did i damage my ontology? a case for conservative extensions in description logic		In computer science, ontologies are dynamic entities: to adapt them to new and evolving applications, it is necessary to frequently perform modifications such as the extension with new axioms and merging with other ontologies. We argue that, after performing such modifications, it is important to know whether the resulting ontology is a conservative extension of the original one. If this is not the case, then there may be unexpected consequences when using the modified ontology in place of the original one in applications. In this paper, we propose and investigate new reasoning problems based on the notion of conservative extension, assuming that ontologies are formulated as TBoxes in the description logic ALC. We show that the fundamental such reasoning problems are decidable and 2EXPTIME-complete. Additionally, we perform a finer-grained analysis that distinguishes between the size of the original ontology and the size of the additional axioms. In particular, we show that there are algorithms whose runtime is ‘only’ exponential in the size of the original ontology, but double exponential in the size of the added axioms. If the size of the new axioms is small compared to the size of the ontology, these algorithms are thus not significantly more complex than the standard reasoning services implemented in modern description logic reasoners. If the extension of an ontology is not conservative, our algorithm is capable of computing a concept that witnesses non-conservativeness. We show that the computed concepts are of (worst-case) minimal size.	2-exptime;algorithm;best, worst and average case;computer science;description logic;entity;ontology (information science);time complexity;unintended consequences;worst-case complexity	Silvio Ghilardi;Carsten Lutz;Frank Wolter	2006			algorithm;ontology-based data integration;theoretical computer science;process ontology;description logic;ontology (information science);ontology inference layer;upper ontology;suggested upper merged ontology;computer science;ontology language	AI	-23.496533957650325	8.48707147166102	49634
2ea56a4f2574068068e3bfd5275de57662381062	preserving multi-view consistency in diagrammatic knowledge representation	multi view modeling;diagrammatic knowledge representation;metamodeling	"""Multi-view conceptual modeling provides means for representing, with diagrammatic means, the knowledge describing a """"system under study"""" whose complexity cannot be captured in a single comprehensible representation. Typical examples are available in the field of enterprise modeling, where models are inherently layered or partitioned, a feature that must be enabled at meta-modeling level by means of abstraction and decomposition. Multi-view modeling must provide means for coping with the complexity of enterprise knowledge representations through consistency preservation techniques across multiple, interrelated views. The paper at hand formulates the conceptual functions fulfilled by multi-view modeling and provides a demonstrative implementation in the context of the Semantic Object Model enterprise modeling method."""	diagram;enterprise modelling;knowledge representation and reasoning;metamodeling	Dominik Bork;Robert Andrei Buchmann;Dimitris Karagiannis	2015		10.1007/978-3-319-25159-2_16	metamodeling;computer science;theoretical computer science;machine learning;data mining	AI	-31.332915431008093	13.439354058917683	49642
b612c9c871c92500e1a624cbd6d2c6ac75bb8afc	"""resolving the """"weak status"""" of weak entity types in entity-relationships schemas"""	entity relationship model;modelo entidad relacion;dependence;dependance;modele entite relation;algorithme;algorithm;object oriented;oriente objet;systeme gestion base donnee;schema base donnee logique;sistema gestion base datos;orientado objeto;database management system;entity relationship;dependencia;algoritmo	Entity Relationship schemas include weak entity types, whose entities are identified by their inter-relationships to other entities. During the translation of the EER schema into a logical database schema, the weak entity types are either translated into logical units of the database (a relation or a class), or are embedded as attributes of other logical units. Formal studies of EER schemas either ignore the presence of weak entity types, or simplify their dependency structure. The presence of weak entity types in an EER schema may be problematic: A weak entity may not be easy to identify because it may be related to other weak entities in various ways, thus causing problems in schema comprehension, as well as in mapping it to a logical database schema. We claim that the presence of weak entity types in an EER schema belongs to an intermediate design stage, but the final EER schema must not include such entity types. We introduce an algorithm for resolving the status of weak entity types, following user directions. If the directions are legal, the algorithm yields an EER schema without weak entity types. The advantage of our approach is twofold: First, the translation of an EER schema into a logical database schema can be fully automated. This is essential for upgrading the EER model to support full database management. Second, it enables a fully formal study of the EER model.	algorithm;cluster analysis;data model;database model;database schema;diagram;embedded system;enhanced entity–relationship model;weak entity	Mira Balaban;Peretz Shoval	1999		10.1007/3-540-47866-3_25	entity–relationship model;computer science;data mining;database;database schema;weak entity;algorithm	DB	-30.31229052242792	13.437131920952007	49766
4127d580b1b2cae9af4b5df3dcbac70562ad9dab	keyword query expansion paradigm based on recommendation and interpretation in relational databases		Due to the ambiguity and impreciseness of keyword query in relational databases, the research on keyword query expansion has attracted wide attention. Existing query expansion methods expose users’ query intention to a certain extent, but most of them cannot balance the precision and recall. To address this problem, a novel two-step query expansion approach is proposed based on query recommendation and query interpretation. First, a probabilistic recommendation algorithm is put forward by constructing a term similarity matrix and Viterbi model. Second, by using the translation algorithm of triples and construction algorithm of query subgraphs, query keywords are translated to query subgraphswith structural and semantic information. Finally, experimental results on a real-world dataset demonstrate the effectiveness and rationality of the proposed method.		Yingqi Wang;Nianbin Wang;Lianke Zhou	2017	Scientific Programming	10.1155/2017/7613026	online aggregation;sargable;query optimization;query expansion;web query classification;ranking;boolean conjunctive query;computer science;query by example;data mining;database;rdf query language;web search query;view;information retrieval;query language;object query language;spatial query	DB	-32.070648588556345	4.481899698576938	49914
b562ed66c4326edaaa7f6fd1b0c043ddc1b10a8d	transforming xml to rdf(s) with temporal information		The Resource Description Framework (RDF) is a model for representing resources on the Web. With the widespread acceptance of RDF in various applications (e.g., knowledge graph), a huge amount of RDF data is being proliferated. Therefore, transforming legacy data resources into RDF data is of increasing importance. In addition, time information widely exists in various real-world applications and temporal Web data has been represented and managed in the context of temporal XML. In this paper, we concentrate on transformation of temporal XML (eXtensible Markup Language) to temporal RDF data. We propose the mapping rules and mapping algorithms which can transform the temporal XML Schema and document into temporal RDF Schema and temporal RDF triples, respectively. We illustrate our mapping approach with an example and implement a prototype system. It is demonstrated that our mapping approach is valid.	algorithm;cardinality (data modeling);international standard book number;interval arithmetic;markup language;microarchitecture;prototype;rdf schema;resource description framework;snapshot (computer storage);transistor;uc browser;xml schema	Dan Yang;Li Yan	2018	CIT		rdf;distributed computing;xml;database;computer science;rdf schema;xml schema;graph	Web+IR	-33.31998604798959	5.6668059983790755	50060
619dd36bbc11c8c8533f786aaa86391c78271819	the relational production language: a production language for relational databases	relational database		relational database management system	Lois M. L. Delcambre;James N. Etheredge	1988			domain relational calculus;sql;relational model;database;relational database;data definition language;database design;alpha (programming language);relational calculus;computer science	NLP	-31.03900706682173	9.538133069085658	50066
7046cd97431db0f7c2261509b04fd3c55b0d37a5	a syntactical approach to the database construction method from document images		This paper presents a database construCtion method fromJOurnalcontentimagesfocuslngOntheloglCalstruCtureconstruction.Inthispaper,WeprOpOSeaVariant OftheregularexpressiontorepresentalogicalstruCt11re andlayoutofthedocument.Thisexpressionisextended inordertoanalyzetokenslocatedinaplane.Wealso discussthesystematicdefinitionoftheextendedreg111arexpressionfromtheschemaofnestedrelationthat StOreSthecomtentsofjollrna15. 1INTRODtJCTION Recentlymanystudieshavebeendoneontheinformationextractionfrom documents.Asaresult,many documentprocesslngmethodsareproposedandapplied to various kinds ofdocumentssuch as o爪cedocuments tl】,neWSpaperS【2】,businesscardst3]andsoon・There SeemStObetwoapproachesforthedocumentprocess1ng.Oneis basedon the numerical-Valuedfeaturesof the documentimages.In this approach,thelogical StruCtureandlayoutofdocumentsareimplicitlydefined inthealgorithms【4,5】・Theotheristheknowledgedirected approachin which severalkinds ofr111es are usedwithnumerical-Valuebasedalgorithms[6,7l・We discuss a database construction method from journal COntentimages focuslng On thelogicalstructure conStruCtion.KnowledgeisindispensabletoanalyzeJOurnalcontentsbecausethelayoutofthecontentsaremuch differentdependingonthejournals・Therefore,Weadopt theknowledge-directedapproach. AdatabaseschemarepresentsaloglCalstructureof data.General1y,databaLSemOdelshavesu爪cientpower to de航nethe structureofvariouskindsofdata.Weuse databaseschemaa5theloglCalstruCt11reOftheobjective documents.Inourapproach,knowledgeisrepresented inakindofregularexpressiontllatisconstructedfrom thedatabaseschemabyaddingthelayoutinformation. ThisapproachhaBthefo1lowlngadvantages・ ●ThestruCturede£nitionabilityoftheexpressionis expectedtobeaspowerfu1astheadopteddatabase model.Therefore,the expression can beapplied to various kindsofdocuments. ●Sincethelogicalstructure hasalready been de一 重nedasthedataba5eSChema,theusershaveonly toaddthelayoutinbrmationtotheschema・This reducesthetlSer’sworkonknowledgede丘nition・ ●ThegeneratedstruCtureisdirectlymappedtothe database data. Asforthejournalcontent,WeuSethenestedrelational databasemodel【8】・Inthispaper,Wefirstoverviews ournllmerical-Valuebasedprocesses,thenshowstheextendedregularexpressionandparserforit・ 2 PRECEDENT PROCESSES Mainrolesofdoc11mentprOCeSSlngSyStemSaretO l・eXtraCtSegmentSfromdocumentimagesthatcorrespondtologicalunitsofthedocuments, 2.claBSifythesegmentsintoclassesand 3.construCttheloglCalstructureofthedocuments・ Fromthedatabaseconstructionpointofview,thesegment,theclassandthelogicalstructurecorrespondto attributevalue,attributeandschema,reSpeCtively. Ordinalrelation of the relationalmodelis viewed asasimpletableinwhichtheattrib11teValuemustbe atomic.The schema ofrelationsis defined as aset of attribute names.The nested relationaldatabase has recursive schema in which the attribute value can be anyofatomicvalue,tupleandrelation・Therecursive schemaofthenestedrelationisrepresentedwithatree struCturelikefig.1thatrepresentsaloglCalstruCtureOf contentsofajournal.Acontentcorrespondstoatuple ofthenested relation. For the purpose ofthe database construCtion,We needtodecomposecontentimagesinthelevelofatomic attrib11teVal11eSandclassifythemintothecla5SOfthe attrib11teS.Thissectionoverviewsourapproachtothe segmentationandtheclassi6cationofjournalcontents・ SegmentationInthisprocess,reCtangularsegments shownin允g.2areextracted・Thenthesesegmentsare orderedasm11ChaLSpOSSible.Intheordinaldocuments, loglCalunitsofthedocumentarenaturallyorderedin theleft-uppertOright-bottomdirection・However,SOme JOurnalshavecomplicatedlayoutsanditisdi爪cultto orderthesegmentsunlquely.Werepresentadocument ThisresearchwaBSuppOrtedinpartbyaScienti鮎Resea・rCh Gr&nt-in-AidfromtheMinistryofEducation,ScienceandCulture oり8panunderGra.ntNo.04229225.	numerical analysis;recursion	Atsuhiro Takasu;Shin'ichi Satoh;Eishi Katsura	1992			mathematics;artificial intelligence;computer vision;database;planar	DB	-27.034892741249422	14.393827903582366	50128
919b4477ddabf86d6d4c4d9188a081e9f3519e92	structured modeling language for automated modeling in causal networks	structural model;relational database;causal models	The paper presents a structured modeling lan­ guage (SML) and a relational database framework for specification and automated genera­ tion of causal models. The framework describes a relational database scheme for encoding a li­ brary of causal network templates modeling the basic components in a modeling domain. SML provides a formal language for specifying mod­ els as structured components that can be com­ posed from the basic components. The lan­ guage enables specification of models as param­ eterized relational queries that can be instan­ tiated for specific model instances. The pa­ per describes an algorithm that, given a library and a specification, computes a causal model in time and space linear in the number of ba­ sic components. The algorithm enables model reuse by combining model fragments from the template library to compose new models. The present automated modeling approach has been implemented using the structured query lan­ guage (SQL) and a relational database envi­ ronment. The approach has been successfully used for modeling an automated work-cell in a real-life digital manufacturing application.	algorithm;bayesian network;causal filter;causal model;formal language;modeling language;real life;relational database;sql	Yousri El Fattah	1999			natural language processing;data definition language;sql;relational model;relational calculus;entity–relationship model;relational database;computer science;database model;machine learning;data mining;database;modeling language;programming language;database design;causal model	SE	-31.30938371058542	10.000739151096521	50146
08de71c1963f7e5ed68f809b48e1a54e4e35ed31	regular path queries on graphs with data	query language;graph databases;register automata;data model;social network;regular expressions;query evaluation;data values;semantic web;regular path queries;biological database;regular expression	Graph data models received much attention lately due to applications in social networks, semantic web, biological databases and other areas. Typical query languages for graph databases retrieve their topology, while actual data stored in them is usually queried using standard relational mechanisms.  Our goal is to develop techniques that combine these two modes of querying, and give us query languages that can ask questions about both data and topology. As the basic querying mechanism we consider regular path queries, with the key difference that conditions on paths between nodes now talk not only about labels but also specify how data changes along the path. Paths that combine edge labels with data values are closely related to data words, so for stating conditions in queries, we look at several data-word formalisms developed recently. We show that many of them immediately lead to intractable data complexity for graph queries, with the notable exception of register automata, which can specify many properties of interest, and have NLogspace data and Pspace combined complexity. As register automata themselves are not easy to use in querying, we define two types of extensions of regular expressions that are more user-friendly, and develop query evaluation techniques for them. For one class, regular expressions with memory, we achieve the same bounds as for automata, and for the other class, regular expressions with equality, we also obtain tractable combined complexity of query evaluation. In addition, we show that results extends to analogs of conjunctive regular path queries.	automata theory;automaton;biological database;cobham's thesis;data model;graph database;pspace;query language;regular expression;semantic web;social network;usability	Leonid Libkin;Domagoj Vrgoc	2012		10.1145/2274576.2274585	discrete mathematics;computer science;theoretical computer science;path expression;database;programming language;regular expression;query language;spatial query	DB	-24.349846588530056	10.139040177430799	50168
62bfa6c910bee927016ba081a7e09d5e4a4424aa	translating wfs query to sql/xml query	relational database;geospatial data	The purpose of the WFS specification, proposed by the the OpenGIS Consortium (OGC), is to describe the manipulation operations over geospatial data using GML. Web servers providing WFS service are called WFS Servers. WFS servers publish GML views of geographic features stored in data sources such that the user can query and update data sources through a user defined feature type schema. In this work, we study the problem of answering WFS queries through a feature type schema, when the data is stored in a relational database. A feature type is specified by the feature type schema and a set of correspondence assertions. The feature type's correspondence assertions formally specify relationships between the feature type schema and the relational database schema. We define the semantic for WFS query answering and present an algorithm that translate a WFS query defined over a feature type schema into a SQL/XML query defined over the relational database schema.	algorithm;consortium;database schema;geography markup language;path expression;relational database;sql;sql/xml;xml schema	Vânia Maria Ponte Vidal;Fernando Cordeiro Lemos;Fábio Feitosa	2005			computer science;sql/xml;database;information retrieval;query language;rdf query language;view;sargable;query optimization;query by example;spatial query	DB	-32.6446284828609	8.090915588316273	50378
3881ae141619799764fff92e035ef4823258aa51	constructing topos from rdf data	mathematics;resource description framework;data structures category theory data analysis;data mining;topos rdf triples rdf data representation categorical semantics abstract description category theory formal methodology resource description framework data analysis;yttrium;cities and towns;resource description framework yttrium cities and towns context buildings mathematics data mining;context;buildings	A significant component of data analysis is related to study relations between pieces of data -- something that is the very fabric of Resource Description Framework (RDF). In such a context, there is a need for a formal methodology of building structures reflecting logical aspects of RDF data, and reasoning about them. Category theory, a branch of modern mathematics, has been seem by many as an abstract description of relations between entities. Multiple constructs and tools provided by category theory allow for modelling and analyzing any phenomena. A topos is a category with an extra structure representing elements of logic. This structure allows for applying categorical semantics to study logic encoded by topos. The paper proposes a methodology for representing RDF data as a topos. It presents required mechanisms of category theory and shows the process of creating components of topos based on RDF triples. The elements of the logic of a topos in the context of RDF are presented and discussed.	categorical logic;category theory;correctness (computer science);embedded system;entity;knowledge-based systems;linear algebra;resource description framework	Marek Reformat;Tim Put	2015	2015 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)	10.1109/WI-IAT.2015.73	computer science;yttrium;rdf;data mining;algorithm;rdf schema	AI	-21.694903578726525	5.2754413401101745	50782
216194fd3962261d75a93a17376adb6ca3e8845b	a normal form for xml documents	xml data;functional dependencies;computacion informatica;relational database;functional dependency;ciencias basicas y experimentales;normal form;xml document;dtds;design;grupo a	This article takes a first step towards the design and normalization theory for XML documents. We show that, like relational databases, XML documents may contain redundant information, and may be prone to update anomalies. Furthermore, such problems are caused by certain functional dependencies among paths in the document. Our goal is to find a way of converting an arbitrary DTD into a well-designed one, that avoids these problems. We first introduce the concept of a functional dependency for XML, and define its semantics via a relational representation of XML. We then define an XML normal form, XNF, that avoids update anomalies and redundancies. We study its properties, and show that XNF generalizes BCNF; we also discuss the relationship between XNF and normal forms for nested relations. Finally, we present a lossless algorithm for converting any DTD into one in XNF.	algorithm;boyce–codd normal form;database normalization;functional dependency;lossless compression;relational database;xml	Marcelo Arenas;Leonid Libkin	2002		10.1145/974750.974757	well-formed document;xml catalog;xml validation;xml encryption;xml namespace;simple api for xml;xml;relax ng;xml schema;computer science;document type definition;document structure description;xml framework;data mining;xml database;xml schema;database;functional dependency;xml signature;xml schema editor;information retrieval;efficient xml interchange	DB	-28.632738251491173	9.01455497064389	50853
215b37288ab46cbea97db01ca89a3c19e33fb99e	n-gram inverted index structures on music data for theme mining and content-based information retrieval	inverted index;information retrieval;index structure;content based music information retrieval;suffix tree;digital music;theoretical analysis;music information retrieval;n gram inverted index;information system;data structure;music theme mining	Content-based music information retrieval and theme mining are two key problems in digital music information systems, where ''themes'' mean the longest-repeating patterns in a piece of music. However, most data structures constructed for retrieving music data cannot be efficiently used to mine the themes of music pieces, and vice versa. The suffix tree structure can be used for both functions, nevertheless its size is too large and its maintenance is somewhat difficult. In this paper, a kind of index structure is introduced, which adopts the idea of inverted files and that of n-gram. It can be used to retrieve music data as well as to mine music themes. Based on the index and several useful concepts, a theme mining algorithm is proposed, and the theoretical analysis is also given. In addition, two implementations of a content-based music information retrieval algorithm are presented. Experiments show the correctness and efficiency of the proposed index and algorithms.		Changping Wang;Jianzhong Li;Shengfei Shi	2006	Pattern Recognition Letters	10.1016/j.patrec.2005.09.012	speech recognition;inverted index;digital audio;data structure;computer science;data mining;information retrieval;information system	Vision	-31.413377823236033	4.301158540821031	50861
a22d0465cf8243050f15a37b4f7878d72471e60d	model-theoretic inseparability and modularity of description logic ontologies	computational complexity;modularisation;description logic;reasoning	The aim of this paper is to introduce and study model-theoretic notions of modularity in description logic and related reasoning problems. Our approach is based on a generalisation of logical equivalence that is called modeltheoretic inseparability. Two TBoxes are inseparable w.r.t. a vocabulary Σ if they cannot be distinguished by the Σ-reducts of their models and thus can equivalently be replaced by one another in any application where only vocabulary items from Σ are relevant. We study in-depth the complexity of deciding inseparability for the description logics EL and ALC and their extensions with inverse roles. We then discuss notions of modules of a TBox based on model-theoretic inseparability and develop algorithms for extracting minimal modules from acyclic TBoxes. Finally, we provide an experimental evaluation of our module extraction algorithm based on the large-scale medical TBox Snomed ct.	algorithm;description logic;directed acyclic graph;end-user computing;ontology (information science);quantum entanglement;separable polynomial;systematized nomenclature of medicine;tbox;theory;turing completeness;vocabulary	Boris Konev;Carsten Lutz;Dirk Walther;Frank Wolter	2013	Artif. Intell.	10.1016/j.artint.2013.07.004	discrete mathematics;description logic;computer science;artificial intelligence;theoretical computer science;mathematics;computational complexity theory;reason;algorithm	AI	-20.705080421135786	8.435502497817131	51098
0d07b93cb2ee53cdfa27b116102f38c8e8b921f7	tractable reasoning and efficient query answering in description logics: the dl-lite family	ontology languages;conjunctive queries;description logics;query processing;dl lite;low complexity;query optimization;satisfiability;data base management system;query evaluation;polynomial time;description logic;query answering;knowledge base	We propose a new family of description logics (DLs), called DL-Lite, specifically tailored to capture basic ontology languages, while keeping low complexity of reasoning. Reasoning here means not only computing subsumption between concepts and checking satisfiability of the whole knowledge base, but also answering complex queries (in particular, unions of conjunctive queries) over the instance level (ABox) of the DL knowledge base. We show that, for the DLs of the DL-Lite family, the usual DL reasoning tasks are polynomial in the size of the TBox, and query answering is LogSpace in the size of the ABox (i.e., in data complexity). To the best of our knowledge, this is the first result of polynomial-time data complexity for query answering over DL knowledge bases. Notably our logics allow for a separation between TBox and ABox reasoning during query evaluation: the part of the process requiring TBox reasoning is independent of the ABox, and the part of the process requiring access to the ABox can be carried out by an SQL engine, thus taking advantage of the query optimization strategies provided by current database management systems. Since even slight extensions to the logics of the DL-Lite family make query answering at least NLogSpace in data complexity, thus ruling out the possibility of using on-the-shelf relational technology for query processing, we can conclude that the logics of the DL-Lite family are the maximal DLs supporting efficient query answering over large amounts of instances.	abox;adobe flash lite;algorithm;best, worst and average case;box counting;cobham's thesis;conceptual schema;conjunctive query;data model;description logic;dynamic language runtime;experiment;first-order logic;knowledge base;l (complexity);mathematical optimization;maximal set;polynomial;query optimization;reasoning system;relation (database);relational database;requirement;sql;select (sql);subsumption architecture;tbox;time complexity;unary operation;whole earth 'lectronic link;worst-case complexity	Diego Calvanese;Giuseppe De Giacomo;Domenico Lembo;Maurizio Lenzerini;Riccardo Rosati	2007	Journal of Automated Reasoning	10.1007/s10817-007-9078-x	sargable;knowledge base;query optimization;query expansion;web query classification;description logic;boolean conjunctive query;computer science;artificial intelligence;theoretical computer science;data mining;database;query language	AI	-24.30954596279786	8.650626505048363	51226
9152b8aaf1b8aa30e77d533b91471b3c2e0b0f79	activity networks with delays an application to toxicity analysis		ANDy, Activity Networks with Delays, is a discrete time framework aimed at the qualitative modelling of time-dependent activities. The modular and concise syntax makes ANDy suitable for an easy and natural modelling of time-dependent biological systems (i.e., regulatory pathways). Activities involve entities playing the role of activators, inhibitors or products of biochemical network operation. Activities may have given duration, i.e., the time required to obtain results. An entity may represent an object (e.g., an agent, a biochemical species or a family of thereof) with a local attribute, a state denoting its level (e.g., concentration, strength). Entities levels may change as a result of an activity or may decay gradually as time passes by. The semantics of ANDy is formally given via high-level Petri nets ensuring this way some modularity. As main results we show that ANDy systems have finite state representations even for potentially infinite processes and it well adapts to the modelling of toxic behaviours. As an illustration, we present a classification of toxicity properties and give some hints on how they can be verified with existing tools on ANDy systems. A small case study on blood glucose regulation is provided to exemplify the ANDy framework and the toxicity properties.	biological system;entity;exemplification;finite-state machine;high- and low-level;petri net	Franck Delaplace;Cinzia Di Giusto;Jean-Louis Giavitto;Hanna Klaudel	2018	Fundam. Inform.	10.3233/FI-2018-1677	discrete mathematics;toxicity;syntax;model checking;machine learning;discrete time and continuous time;modular design;petri net;modularity;artificial intelligence;semantics;computer science	AI	-23.974316735519633	16.519011645156013	51288
8d9fbdb19d53695acfeb1b094fac4fb29596783c	rasp: a resource allocator for software projects	resource allocation;project manager;software management;artificial intelligent;logic programs;expert system	Rasp is a resource allocator that tries to apply artificial intelligence techniques in the project management field. Rasp expresses and manipulates all aspects concerning resource allocation exploiting a combined approach which integrates the logic programming paradigm with spreadsheet technology.	artificial intelligence;logic programming;programming paradigm;random-access stored-program machine;spreadsheet	C. Bertazzoni;Fosca Giannotti	1990		10.1145/98894.98907	real-time computing;resource allocation;computer science;knowledge management;artificial intelligence;expert system	AI	-27.842870364587203	17.760493084265192	51518
40bfd0ca71aecea7648d6eaa5c5c013468897173	analysis and validation of information access through mono, multidimensional and dynamic taxonomies	busqueda informacion;estensibilidad;modelizacion;commerce electronique;base dato multidimensional;navegacion informacion;comercio electronico;faceted classification;information retrieval;base donnee tres grande;navigation information;branching;interrogation base donnee;information browsing;interrogacion base datos;systematique;information access;multidimensional database;classification;modelisation;very large database;sistematica;validation information;recherche information;ramificacion;information validation;taxonomy;acces information;ramification;analyse information;acceso informacion;base donnee multidimensionnelle;extensibilite;scalability;systeme gestion base donnee;very large databases;modeling;sistema gestion base datos;database management system;information analysis;database query;clasificacion;electronic trade	Access to complex information bases through multidimensional, dynamic taxonomies (also improperly known as faceted classifications) is becoming a hot concept both in research and in industry. In this paper, the major shortcomings of conventional, monodimensional taxonomic approaches, such as the independence of different branches of the taxonomy and insufficient scalability, are discussed. The dynamic taxonomy approach, the first and most complete model for multidimensional taxonomic access to date, is reviewed and compared to conventional taxonomies. We analyze the reducing power of dynamic taxonomies and conventional taxonomies and report experimental results on real data, which confirm that monodimensional taxonomies are not useful for browsing/retrieval on large databases, whereas dynamic taxonomies can effectively manage very large databases and exhibit a very fast convergence. ACM Classification	acm computing classification system;conceptual schema;database;document retrieval;faceted classification;information access;information retrieval;naivety;partial template specialization;scalability;taxonomy (general)	Giovanni Maria Sacco	2006		10.1007/11766254_56	scalability;systems modeling;branching;biological classification;computer science;data mining;database;ramification;data analysis;world wide web;taxonomy;very large database	DB	-27.373545026421006	5.703233248837112	51869
393e3e3162d0bf33d40b98451ef17e9cc241683a	the viewpoint mechanism for object-oriented databases modelling, distribution and evolution		Over the past years, most of the research dealing with the object multiple representation and evolution has proposed to enrich the monolithic vision of the classical object approach in which an object belongs to one hierarchy class. In databases, much work has been done towards extending models with advanced tools such as view technology, schema evolution support, multiple classification, role modelling and viewpoints. In particular, the integration of the viewpoint mechanism to the conventional object-oriented data model gives it flexibility and allows one to improve the modelling power of objects. The viewpoint paradigm refers to the multiple description, the distribution, and the evolution of object. Also, it can be an undeniable contribution for a distributed design of complex databases. The motivation of this paper is to define an object data model integrating viewpoints in databases and to present a federated database architecture integrating multiple viewpoint sources following a local-as-extended-view data integration approach.	data model;federated database system;programming paradigm;schema evolution;viewpoint	Fouzia Benchikha;Mahmoud Boufaïda	2007	CIT		data modeling;method;systems modeling;biological classification;computer science;information integration;theoretical computer science;data mining;database;multiplicity;object-oriented programming;software maintenance;distributed database	DB	-32.35825133843617	12.823720026079389	51989
0cb059b970410fb8dc9633f7692436b6d2146cf1	a data structure and query algorithm for a database of areal entities			algorithm;data structure;entity	D. J. Abel;J. L. Smith	1984	Australian Computer Journal		data mining;computer science;database;information retrieval;view;data structure;database design;query optimization	DB	-31.049563410533814	8.000987294710763	52013
00eed467f328077471ac757b1be7818821a6d3a2	object projection views in the dynamic relational model	relational database;satisfiability;functional dependency;relational model	"""User views in a relational database obtained through a single projection (""""projection views"""") are considered in a new framework. Specifically, such views where each tuple in the view represents an object (""""object projection views"""") are studied using the dynamic relational model of [V1,V2], which captures the evolution of the database through consecutive updates. Attribute sets which yield object projection views are characterized using the static and dynamic functional dependencies satisfied by the database. Object projection views are then described using the static and dynamic fd's """"inherited"""" from the original database, and the notion of age-closure [V1]. Finally, the impact of dynamic constraints on the view update problem is studied in a limited context. The paper demonstrates that new, useful information about views can be obtained by looking at the evolution of the database as captured by the dynamic relational model."""	functional dependency;relational database;relational model	Victor Vianu	1984		10.1145/588011.588042	relational model/tasmania;information schema;relational model;relational calculus;semi-structured model;entity–relationship model;relational database;computer science;theoretical computer science;database model;data mining;database;projection;functional dependency;view;database schema;database design;satisfiability	DB	-29.1364797179581	9.846202778474739	52116
b6aedce91cd23c4e909e9310962123cbdfe7d963	the complexity and approximation of fixing numerical attributes in databases under integrity constraints	conjunctive queries;approximate algorithm;satisfiability;inconsistent databases;optimization problem;database repairs;integrity constraints;query answering;consistent query answering	"""Consistent query answering is the problem of characterizing and computing the semantically correct answers to queries from a database that may not satisfy certain integrity constraints. Consistent answers are characterized as those answers that are invariant under all minimally repaired versions of the original database. We study the problem of repairing databases with respect to denial constraints by fixing integer numerical values taken by attributes. We introduce a quantitative definition of database repair, and investigate the complexity of several decision and optimization problems. Among them, Database Repair Problem (DRP): deciding the existence of repairs within a given distance to the original instance, and CQA: deciding consistency of answers to simple and aggregate conjunctive queries under different semantics. We provide sharp complexity bounds, identifying relevant tractable and intractable cases. We also develop approximation algorithms for the latter. Among other results, we establish: (a) The @D""""2^P-hardness of CQA. (b) That DRP is MAXSNP-hard, but has a good approximation. (c) The intractability of CQA for aggregate queries for one database atom denials (plus built-ins), and also that it has a good approximation."""	approximation;data integrity;database;numerical analysis	Leopoldo E. Bertossi;Loreto Bravo;Enrico Franconi;Andrei Lopatenko	2008	Inf. Syst.	10.1016/j.is.2008.01.005	optimization problem;database theory;computer science;theoretical computer science;data integrity;data mining;database;conjunctive query;satisfiability	DB	-24.462136289078217	11.003982951490888	52304
51fdd62f5e0dc0c298c47dffa8980cf9ee66ccc4	mapping from a conceptual schema to a target internal schema	relational database;conceptual schema;base donnee relationnelle		conceptual schema	Donal J. Flynn;Alberto H. F. Laender	1985	Comput. J.	10.1093/comjnl/28.5.508	information schema;semi-structured model;logical schema;relational database;computer science;conceptual schema;document structure description;database;database schema	DB	-31.673889248522467	9.432673848725477	52396
2099a64a1dc872712f910f4f0cf3cef1fe546b30	description of structure and behavior of systems with concurrency- a rule-based approach	rule based			Ljubomir Stoitschev;Anatoliy Antonov	1986			rule-based system;natural language processing;computer science;artificial intelligence;concurrency;theoretical computer science	Logic	-22.49261652376264	17.123369031752084	52470
b38ca88aa98d0b68c4e72984f3de4e4d8720b5bc	xpath query evaluation based on the stack encoding	time complexity;xml database;xml pattern matching;trees;query evaluation;pattern matching;twig joins;xml query processing;paths;linear space;xml databases	The twig join, which is used to find all occurrences of a twig pattern in an XML database, is a core operation for XML query processing. A great many strategies for handling this problem have been proposed and can be roughly classified into two groups. The first group decomposes a twig pattern (a small tree) into a set of binary relationships between pairs of nodes, such as parent-child and ancestor-descendant relations; and transforms a tree matching problem into a series of simple relation look-ups. The second group decomposes a twig pattern into a set of paths. Among all this kind of methods, the approach based on the so-called stack encoding by Bruno et. al. [2] is very interesting, which can represent in linear space a potentially exponential (in the number of query nodes) number of matching paths. However, the available processes for generating such compressed paths suffer some redundancy and can be significantly improved. In this paper, we analyze this method and show that the time complexities of path generation in its two main procedures: PathStack and TwigStack can be reduced from O(m2 ·n) to O(m ·n), where m and n are the sizes of the query tree and document tree, respectively. Experiments have been done to compare ours and some existing startegies, which shows that using our method much less time is needed to generate matching paths.	pc bruno;redundancy (engineering);time complexity;twig;xml database;xpath	Yangjun Chen;Donovan Cooke	2009		10.1145/1557626.1557634	computer science;theoretical computer science;data mining;xml database;database	DB	-30.101481253269345	4.496115214167221	52495
fff9043554f0aad1b411c5b9c991fd6edd708569	natural language querying over databases using cascaded crfs	relational database;structured query language;machine learning;natural language;conditional random field;graph model;text indexing;formal language	Retrieving information from relational databases using a natural language query is a challenging task. Usually, the natural language query is transformed into its approximate SQL or formal languages. However, this requires knowledge about database structures, semantic relationships, natural language constructs and also handling ambiguities due to overlapping column names and column values. We present a machine learning based natural language search system to query databases without any knowledge of Structure Query Language (SQL) for underlying database. The proposed system - Cascaded Conditional Random Field is an extension to Conditional Random Fields, an undirected graph model. Unlike traditional Conditional Random Field models, we offer efficient labelling schemes to realize enhanced quality of search results. The system uses text indexing techniques as well as database constraint relationships to identify hidden semantic relationships present in the data. The presented system is implemented and evaluated on two real-life datasets.	cascaded integrator–comb filter;natural language	Kishore Varma Indukuri;Srikumar Krishnamoorthy;P. Radha Krishna	2010		10.1007/978-3-642-15576-5_47	search-oriented architecture;natural language processing;language identification;sargable;data definition language;query optimization;natural language programming;sql;.ql;formal language;universal networking language;data manipulation language;object language;data control language;relational database;computer science;query by example;data mining;database;rdf query language;structured prediction;natural language;view;conditional random field;null;query language;object query language	DB	-32.668230420153925	5.815324466663867	52524
8a06438ecf9e013223c817a9f64e60a18d768588	acquisition and validation of complex object database schemata supporting multiple inheritance	complex objects;theoretical framework;artificial intelligent;object oriented;multiple inheritance;point of view;knowledge representation	We present an intelligent tool for the acquisition of object-oriented schemata supporting multiple inheritance, which preserves taxonomy coherence and performs taxonomic inferences. Its theoretical framework is based onterminological logics, which have been developed in the area of artificial intelligence. The framework includes a rigorous formalization of complex objects, which is able to express cyclic references on the schema and instance level; asubsumption algorithm, which computes all impliedspecialization relationships between types; and an algorithm to detectincoherent types, i.e., necessarily empty types. Using results from formal analyses of knowledge representation languages, we show that subsumption and incoherence detection are computationally intractable from a theoretical point of view. However, the problems appear to be feasible in almost all practical cases.	artificial intelligence;computational complexity theory;genetic algorithm;knowledge representation and reasoning;multiple inheritance;subsumption architecture;t-norm fuzzy logics	Sonia Bergamaschi;Bernhard Nebel	1994	Applied Intelligence	10.1007/BF00872108	knowledge representation and reasoning;multiple inheritance;computer science;artificial intelligence;theoretical computer science;machine learning;object-oriented programming;algorithm	AI	-19.701036507331608	9.46781641373532	52589
26fc76d47fa5d27f8dc8005b3f422cc3b5a3ff0a	implementing the division operation on a database containing uncertain data	graph matching;probabilistic partial values;uncertain data;partial values	Uncertain data in databases were originally denoted as null values which were later generalized to partial values Based on the concept of partial values we have further generalized the notion to probabilistic partial values In this paper an important operation division is fully studied to handle partial values and probabilistic partial values Due to the uncertainty of partial values and probabilistic partial values the corresponding extended division may produce maybe tuples and maybe tuples with degrees of uncertainty respectively To process this extended division we decompose a relation consisting of partial val ues or probabilistic partial values into a set of relations containing only de nite values Bipartite graph matching techniques are then applied to develop e cient algorithms for the extended division that handles partial values The re nement on the maybe result is also discussed Finally we study the extended division that handles probabilistic partial values keywords partial values probabilistic partial values graph matching uncertain data To whom all correspondence should be sent	algorithm;database;matching (graph theory);ues (cipher);uncertain data	Frank Shou-Cheng Tseng;Arbee L. P. Chen;Wei-Pang Yang	1996	J. Inf. Sci. Eng.		mathematical optimization;discrete mathematics;probabilistic database;data mining;mathematics;matching	DB	-25.714811647833564	10.751571264622129	52653
c470e73ae74e8868da71a887d9145d2b74428a24	extending aggregation constructs in uml	developpement logiciel;agregacion;graphical language;conceptual model;object oriented programming;software engineering;aggregation;specification language;desarrollo logicial;software development;agregation;lenguaje especificacion;information system;knowledge representation;programmation orientee objet;langage specification;systeme information;lenguaje grafico;langage graphique;sistema informacion	In this paper we provide a characterization of aggregation as used in static conceptual modeling of applications. Based on this characterization we suggest changes to UML notation that allow for more precise characterization of aggregate structures.	aggregate data;aggregate function;unified modeling language	Minika Saksena;María M. Larrondo-Petrie;Robert B. France;Matthew P. Evett	1998		10.1007/978-3-540-48480-6_33	natural language processing;knowledge representation and reasoning;specification language;uml tool;computer science;conceptual model;software development;applications of uml;programming language;object-oriented programming;information system;algorithm	DB	-30.17958414087212	13.979493820043523	52767
16d9a11645ff6fcc172cf245e746dfedf130404a	horizontal query optimization on ordered semistructured data	regular expression;query optimization;query language;semi structured data	The exchange and storage of XML data is becoming increasingly important. In contrast to conventional semistructured data 4, 1], the labels in a document-oriented representation such as XML are ordered. Furthermore, regular expressions (DTDs) describe the horizontal (and vertical) structure of the data. Traditional query languages for semi-structured data ignore the horizontal order and are therefore limited in their expressiveness and optimizability. We describe a query language for querying ordered semistruc-tured data. This query language provides primitives for specifying more powerful queries on ordered semistructured data. Furthermore, we describe how horizontal type information in DTDs is used to optimize queries based on nite automata.	automata theory;automaton;cpu cache;cache (computing);data structure;experiment;mathematical optimization;program optimization;quantum finite automata;query language;query optimization;regular expression;semi-structured data;semiconductor industry;storage model;xml	Hartmut Liefke	1999			database;semi-structured data;data mining;query language;computer science;regular expression;query optimization	DB	-29.74454601652459	5.8471356616605386	52870
97053a33028fcfa1794e3867651069098c688b30	formal definition of mappings in a data base	non procedural language;data type;data base;relational model;language extension;relational calculus;high level language	"""The purpose of this paper is to analyze the problem of mapping between different levels of a data base.A data Base is viewed as an Abstract Object upon which it is possible to operate with a given set of operations, the result of which depends both on the """"type"""" of the object and on its state.Following this approach and utilizing some algebraic formalism we analyze the mappings between external views of a data base and the conceptual view of the data base itself. The main result is a requirement on the consistency between the process of deriving an external view from the conceptual view and the translation of external (user) operations into conceptual operations.It is also shown that this requirement is not sufficient to characterize the """"interference"""" arising from a shared use of a Data Base.Informal and formal examples illustrating these concepts are given."""	database;interference (communication);linear algebra;semantics (computer science)	Paolo Paolini;Giuseppe Pelagatti	1977		10.1145/509404.509413	relational model;relational calculus;data type;data model;computer science;theoretical computer science;database;programming language;high-level programming language;algorithm	DB	-29.3418001271261	11.26630235262425	53036
a01ec611e7ba66789043bf62ffa81f899fe5b3cb	temporal coupling verification in time series databases	temporal coupling verification;time series patterns;time series;meta aggregation;time series databases;causality	Time series are often generated by continuous sampling or measurement of natural or social phenomena. In many cases, events cannot be represented by individual records, but instead must be represented by time series segments (temporal intervals). A consequence of this segment-based approach is that the analysis of events is reduced to analysis of occurrences of time series patterns that match segments representing the events. A major obstacle on the path toward event analysis is the lack of query languages for expressing interesting time series patterns. We have introduced SQL/LPP (Perng and Parker, 1999). Which provides fairly strong expressive power for time series pattern queries, and are now able to attack the problem of specifying queries that analyze temporal coupling, i.e., temporal relationships obeyed by occurrences of two or more patterns. In this paper, we propose SQL/LPP+, a temporal coupling verification language for time series databases. Based on the pattern definition language of SQL/LPP (Perng and Parker, 1999), SQL/LPP+ enables users to specify a query that looks for occurrences of a cascade of multiple patterns using one or more of Allen's temporal relationships (Allen, 1983) and obtain desired aggregates or meta-aggregates of the composition. Issues of pattern composition control are also discussed.	aggregate data;data mining;database;query language;sql server compact;sampling (signal processing);time series	Chang-Shing Perng;Douglas Stott Parker	2000	Journal of Intelligent Information Systems	10.1023/A:1008777711333	causality;computer science;theoretical computer science;time series;data mining;database;statistics	DB	-25.82960194990343	13.996902797564411	53176
60e1309c4cc78f3c175605c2cb61675f03c093c7	test routines based on symbolic logical statements	symbolic logical statements	symbolic logical statements		Richard D. Eldred	1959	J. ACM	10.1145/320954.320957	computer science;artificial intelligence;symbolic data analysis	PL	-20.694693915441142	17.69637415342598	53546
2a00e867097b6bd3822c44284d00c1d72fd03441	a reasoner for generalized bayesian dl-programs	logic programs	In this paper, we describe an ongoing reasoner implementation for reasoning with generalized Bayesian dl-programs and thus for dealing with deterministic ontologies and logic programs and probabilistic (mapping) rules in an integrated framework.	ontology (information science);semantic reasoner	Livia Predoiu	2008			semantic reasoner;machine learning;data mining;mathematics;algorithm	AI	-21.18174796125441	7.364921623027783	53567
966ae0cc179d1740f87bd42e07848596e2e20f60	semantics of network data manipulation languages: an object-oriented approach	integrated management;abstract data type;data type;satisfiability;data model;object oriented approach;network structure;language design;data manipulation language	and correctness of dml programs can be proved. An axiomatic basis for defining the semantics of navigational data manipulation languages is presented. This basis consists of an abstraction of the network data model achieved by three abstract data types, an assertion language to express Properties of database states, and a DML to Program the transactions. The proof rules of the DML constructs and the axioms defined on the data types can be used to establish the correctness of transactions. Potential applications of the proposed formalism in language design, semantic definition of existing languages, and integrity management, are outlined via examples.	abstract data type;assertion (software development);correctness (computer science);data manipulation language;data model;network model;semantics (computer science)	Dipayan Gangopadhyay;Umeshwar Dayal;James C. Browne	1982			natural language processing;data modeling;data manipulation language;data type;type safety;data model;computer science;database;programming language;abstract data type;logical data model;data mapping;satisfiability;complex data type	PL	-30.562464705851596	11.801645712731473	53728
8fc66ce8a34884da5a832298aa9570825f8ded1a	type annotation for adaptive systems		Typing systems define the admissible structures and behaviours of models formed by their instances, possibly defining relations among types in terms of inheritance and composition. Such a general notion is at the basis both of object-oriented analysis and design, and of metamodeling techniques, for both of which the relations with the field of graph transformations have been explored [10, 9]. When using an approach based on graph transformation theory, admissible structures are defined by graph constraints, either explicitly presented as such or included in the graph type, while behaviours are defined by collections of rules, possibly regulated by some control mechanism. This supports a view of an application domain as defined by a type graph and a policy for model generation (i.e. a collection of constraints) and evolution (i.e. a collection of rules). However, the development of concrete applications often requires bringing together concerns that are expressed with respect to different domains, or are cross-domains. In such cases, two possibilities are typically offered: either to merge different domains in a comprehensive one, which might become inflexible in view of possible evolutions of the application, or to maintain the domains separated, but to build explicit connections among them through specific constructs. In particular, triple graphs are a way to construct explicit relations between types in two different domains, allowing the definition of traces of model transformations [15]. In a series of recent papers we have proposed annotations to add context-specific information in a flexible way to elements of a graph, while preserving the separation of the original domains [4, 3]. In this paper, we bring the notion of annotation to bear on the context of model adaptation, by proposing to replace the notion of typing as traditionally expressed via a typing morphism from an instance graph to a type graph with the notion of annotation of model elements with type information, thus embedding the type information in the model itself. This achieves three important objectives:	application domain;graph rewriting;metamodeling;tracing (software);transformation theory;type signature;typing	Paolo Bottoni;Andrew Fish;Francesco Parisi-Presicce	2016		10.4204/EPTCS.231.1	bioinformatics;theoretical computer science;mathematics;algorithm	PL	-31.25173149988882	14.723392517798176	53841
405bcffd60f0cf957b52457bce07a072817626dc	a cost-based optimizer for sparql queries		The cost of answering a query against an ontology is affected by at least three elements: the size of the ontology, the strategy followed to combine the data, and the order or plan in which data is processed. In the context of the Semantic Web, very large ontologies have been defined; therefore, techniques to identify efficient evaluation strategies are needed. We propose cost-based optimization techniques for SPARQL queries. In our approach, ontologies are modeled as a deductive database. The extensional database is comprised of meta-level predicates that represent the information explicitly modeled by the ontology; for each RDFS built-in vocabulary term, we define a meta-level predicate (e.g., subClassOf). The intensional database corresponds to the deductive rules that implement the semantics of the vocabulary terms (e.g., the transitive properties of the subClassOf term). Currently, we have developed the following techniques:	canonical account;deductive database;intensional logic;mathematical optimization;ontology (information science);rdf schema;sparql;semantic web;vocabulary	Edna Ruckhaus;Maria-Esther Vidal;Eduardo Ruiz;Javier Sierra	2008			sparql;database;computer science	DB	-24.770050822151347	8.59861522598394	53920
f1f7717e051f50afd70370b238bee507f84e08f2	when rule engine meets big data: design and implementation of a distributed rule engine using spark		Rule Engines have been widely used both in industry and academia since they can separate rule knowledge from implementation logic conveniently and flexibly. However, traditional rule engine systems can not deal with big data, because of the limitations of memory and computing capacity of one single computer. Consequently, some researchers have proposed distributed rule engine to meet this challenge. But these solutions still do not work well when enormous amounts of facts are involved, for the reasons such as high cost of moving data, data imbalance and so on. We present the SparkRE system, a distributed rule engine based on Spark, to support big data reasoning. In particular, SparkRE uses DataFrame representing distributed working memory which holds enormous amounts of facts, and implements the rule condition-testing mechanism by the query execution engine of Spark SQL. In addition, we design a rule language tailored for SparkRE and an efficient inference engine. An experimental evaluation shows SparkRE's capability of matching 26 million facts, and reveals its scalability and performance.	algorithmic efficiency;big data;business rules engine;fault tolerance;frame (networking);image scaling;immutable object;inference engine;spark;sql;scalability	Jindou Zhang;Jinxing Yang;Jing Li	2017	2017 IEEE Third International Conference on Big Data Computing Service and Applications (BigDataService)	10.1109/BigDataService.2017.17	rule-based system;computer science;theoretical computer science;data mining;sql;memory management;scalability;spark (mathematics);big data;inference engine;cognition	DB	-24.96180161869672	7.015420918246447	54220
f13cad94064e78cc14667630fbaad18456a0e949	an implementation of relational interface to an information retrieval system	information retrieval system	In the paper a specialized relational interface to information retrieval systems is presented. Its aim is to enable the user to obtain aggregated data (e.8. total information about objects described by the database) in a simple and concise way. An additional relational database is produced by extraction of data from the main database. Data to be extracted are defined by the user in the terms of the DDL. The relational database is processed by mean$ of the DML based on the relational algbra. Additional instructions of the DML enable the user to output relations in a tabular form and to maintain relations between subsequent executions of the manipulation program. I. GENEZIS OF THE PROJECT Data stored in files of most of present-day information retrieval systems (IRS) are usually used in two ways: (1) They are content identifiers in comparisons of analysed information requests with stored records. (This group of data consists of key-words extracted from titles, abstracts or special descriptor fields, classification scheme symbols, and others used as auxiliary selection criteria.) (2) They are means of presenting the selected information to the user. (In reference-providing systems this group of data consists of bibliographic document descriptions and abstracts. In data-providing systems any data stored in the database may serve for these purposes.) Therefore, the substantial part of typical information storage and retrieval system activities concerns: (I) Retrieval of document or fact descriptions in response to the submitted queries. (2) Editing of system catalogues, indexes etc., (3) Database maintenance (data entry, verification and actualization). In some systems, however, particular data may serve as a potential source for automatic generation of aggregated data and statistics describing the overall structure and content of objects described by a database. To use the data for those purposes the following facilities may be needed: -selecting and counting of records, which meet given criteria. -computation of totals, averages and other userdefined functions, -interlacing of retrieval and computations, -restructuring the data structures being processed, -auxiliary input for user data (e.g. input of userdefined classification schemes), -tabular output. *Present address: lnstytut INTE. Al. NiepodlegJoSci 188. 00-950 Warszawa. Poland. These facilities, together with information retrieval and database maintenance routines are.usually provided by Database Management Systems (DBMS). The costefficiency considerations may not justify. however. the use of complex and rather expensive DBMS’s for the purpose outlined earlier. The expected frequency of use of the mentioned additional facilities compared to frequency of use of standard data entry and retrieval procedures may be low. A convenient non-procedural query language, instead of data manipulation language may be preferred by most of system users. There are, also. information storage and retrieval systems supported by conventional retrieval-oriented software: which do not enable the more sophisticated data processing. The above considerations have led to the formulation of a problem of expanding the facilities of standard information retrieval software towards the more advanced data processing. As a solution of this problem an implementation of a specialized interface to an information storage and retrieval system is proposed. The relational mode1 of data[l] with a data manipulation language based on relational algebra has been recognized as the most suitable for this purpose. Relational databases are conceptually simple. so that the operations, which may be carried out on them are transparent. Non-programmer users may interpret relations as two-dimensional tables. Transformations carried out on relations by means of relational algebra operators may be interpreted in terms of operations acting on rows and columns of tables. Further. as it is presented below, a generalized method for converting relations into presentable tables of output may be used. Thus the use of relational model should enable non-programmer users to formulate a wide range of their application problems in an easy and straightforward way. It has been assumed that the proposed system should closely cooperate with IRS to complement its facilities. rather than to replace them. IRS procedures are used for	column (database);comparison and contrast of classification schemes in linguistics and metadata;computation;computer data storage;data definition language;data manipulation language;data structure;identifier;information retrieval;programmer;query language;relational algebra;relational database management system;relational model;table (database);table (information)	Jaroslaw Dobosz;Boleslaw K. Szymanski	1981	Inf. Syst.	10.1016/0306-4379(81)90024-7	sql;relational model;entity–relationship model;data model;relational database;computer science;database normalization;database model;data mining;database;data retrieval;information retrieval;object-relational impedance mismatch;database design	DB	-29.62177334920083	6.507929782675905	54241
5f13ec39b782fe2733a68763c8e53337609d763f	a metric for evaluating the usability of file systems	user interfaces file organisation ontologies artificial intelligence query processing software metrics software performance evaluation;software metrics;navigation ontologies measurement semantics books usability file systems;query processing;software performance evaluation;ontologies artificial intelligence;property value pairs evaluation metric file system usability personal computer file system paradigms relational file system rfs ontologies file organization;user interfaces;file organisation	Since the amount of files stored in a personal computer is continuously growing, there is a need to efficiently organize them. This paper proposes a metric that evaluates the ability of a file system to assist the user in organizing his files. This metric does not apply to specific file systems, but rather to file system paradigms. A relational file system (RFS) is also proposed, as an alternative to the classical, hierarchical one (HFS). By allowing the user to associate property-value pairs with the files, and to classify according to those pairs, RFS brings more flexibility and efficiency in the organization. RFS also supports advanced queries for grouping the files, and can be semantically enhanced, with ontologies. This relational file system proved to be more efficient for organizing files, according to the proposed metric, and in some real-life simulations.	experiment;intelligent agent;kernel (operating system);metric;ontology (information science);operating system;organizing (structure);overhead (computing);personal computer;real life;remote file sharing;simulation;usability;xslt/muenchian grouping	Ciprian Oprisa;Adrian Colesa;Iosif Ignat	2012	2012 14th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing	10.1109/SYNASC.2012.45	self-certifying file system;torrent file;device file;computer file;zap file;computer science;class implementation file;operating system;data mining;database;open;data file;user interface;world wide web;design rule for camera file system;software metric	OS	-33.60502738120701	4.438792335246462	54429
4af927dcdee194a2a29e9427c9c2f066c101490a	xjoin index: indexing xml data for efficient handling of branching path expressions	database indexing;relational databases database indexing xml tree data structures query processing;query processing;attribute selection;indexing xml tree graphs books data structures query processing performance evaluation database systems data engineering;parent child relationship;tree data structures;indexation;xml;relational databases;data structure;path expressions;b sup trees xjoin index xml data indexing branching path expressions structural join algorithm attribute selections parent child relationships data structures	We consider the problem of indexing XML data for solving branching path expressions with the aim of reducing the number of joins to be executed and we propose a simple yet efficient join indexing approach to shrink the twig before applying any structural join algorithm. The indexing technique we propose, that we call XJoin Index, precomputes some structural (semi-)join results thus reducing the number of joins to be computed. Precomputed (semi-)joins support the following operations: (i) attribute selections, possibly involving several attributes; (ii) detection of parent-child relationships; (ii) counting selections, like Find all books with at least 3 authors. Unlike other approaches, based on specialized data structures XJoin Index is entirely based on B/sup +/-trees and can be coupled with any structural join algorithm proposed so far.	algorithm;book;data structure;join (sql);path expression;precomputation;twig;xml	Elisa Bertino;Barbara Catania;Wen Qiang Wang	2004	Proceedings. 20th International Conference on Data Engineering	10.1109/ICDE.2004.1320059	database index;xml;data structure;relational database;computer science;data mining;xml database;database;tree;programming language;sort-merge join;information retrieval	DB	-29.1383985607247	5.25704293282803	54691
20718ca2f1aa887539894eda2f9b9be901cec168	simplifying xml schema: single-type approximations of regular tree languages	xml schema;complexity;approximation;xml	XML Schema Definitions (XSDs) can be adequately abstracted by the single-type regular tree languages. It is well known that these form a strict subclass of the robust class of regular unranked tree languages. Sadly, in this respect, XSDs are not closed under the basic operations of union and set difference, complicating important tasks in schema integration and evolution. The purpose of this paper is to investigate how the union and difference of two XSDs can be approximated within the framework of single-type regular tree languages. We consider both optimal lower and upper approximations. We also address the more general question of how to approximate an arbitrary regular tree language by an XSD and consider the complexity of associated decision problems.	approximation;regular tree grammar;tree automaton;xml schema	Wouter Gelade;Tomasz Idziaszek;Wim Martens;Frank Neven;Jan Paredaens	2013	J. Comput. Syst. Sci.	10.1016/j.jcss.2013.01.009	xml validation;discrete mathematics;complexity;xml;relax ng;xml schema;computer science;theoretical computer science;document structure description;approximation;xml schema;database;mathematics;document schema definition languages;programming language;xml schema editor;algorithm	DB	-25.33313329900803	11.240880242975647	55092
89d74827832f939a0da36a565643d6b206204a65	interpreting relational databases in the rdf domain	relational data;sql;relational database;data model;denotational semantic;denotational semantics;semantic web;rdf	"""The W3C's """"Direct Mapping of Relational Data to RDF"""" defines a simple, practical and intuitive interpretation of SQL database tables as RDF graphs. This document specifies the formal data models for RDB (Relational DataBase) and RDF and defines a denotational semantics of RDB in the RDF domain. We show how this mapping treats all of the important features of SQL tables, like cardinality and NULLs, and yields an RDF graph which preserves the relational information."""	amiga rigid disk block;data model;denotational semantics;relational database;resource description framework;sql;table (database)	Alexandre Bertails;Eric Prud'hommeaux	2011		10.1145/1999676.1999699	rdf/xml;cwm;data definition language;sql;nested set model;codd's theorem;relational calculus;relational database;computer science;sparql;database model;data mining;database;conjunctive query;information retrieval;null;rdf schema	DB	-31.493940288507098	8.255663150086612	55112
3ac7cc8fb180fa100e7266471832910efe2cd436	review - query optimization for xml.	query optimization			Yuqing Wu	2000	ACM SIGMOD Digital Review		sargable;query optimization;query expansion;streaming xml;computer science;query by example;database;rdf query language;web search query;information retrieval;query language;object query language	DB	-32.461696551279374	7.406369782638028	55146
0f1a1804a08719f53e23075cb9bc39701b876709	querying knowledge graphs by example entity tuples	computers;lattices;lattices query processing correlation usability business;user query intent knowledge graph query database knowledge nonprofessional users gqbe graph query by example weighted hidden maximum query graph input query tuples;awards activities;writing;query processing graph theory graphical user interfaces;relational databases;system architecture gqbe knowledge graph querying example entity tuples real world knowledge graphs complex structured graph queries query system usability user friendly gui;educational institutions awards activities lattices writing computers relational databases;query processing graph theory	We witness an unprecedented proliferation of knowledge graphs that record millions of entities and their relationships. While knowledge graphs are structure-flexible and content-rich, they are difficult to use. The challenge lies in the gap between their overwhelming complexity and the limited database knowledge of non-professional users. If writing structured queries over “simple” tables is difficult, complex graphs are only harder to query. As an initial step toward improving the usability of knowledge graphs, we propose to query such data by example entity tuples, without requiring users to form complex graph queries. Our system, Graph Query By Example (GQBE), automatically discovers a weighted hidden maximum query graph based on input query tuples, to capture a user's query intent. It then efficiently finds and ranks the top approximate matching answer graphs and answer tuples. We conducted experiments and user studies on the large Freebase and DBpedia datasets and observed appealing accuracy and efficiency. Our system provides a complementary approach to the existing keyword-based methods, facilitating user-friendly graph querying. To the best of our knowledge, there was no such proposal in the past in the context of graphs.	answer set programming;approximation algorithm;complexity;dbpedia;entity;experiment;freebase;graph (discrete mathematics);knowledge graph;query by example;regular expression;usability	Nandish Jayaram;Arijit Khan;Chengkai Li;Xifeng Yan;Ramez Elmasri	2015	2016 IEEE 32nd International Conference on Data Engineering (ICDE)	10.1109/ICDE.2016.7498391	online aggregation;sargable;query optimization;query expansion;web query classification;boolean conjunctive query;relational database;computer science;query by example;theoretical computer science;lattice;data mining;database;graph;rdf query language;programming language;web search query;writing;graph database;query language;spatial query	DB	-33.52217278244852	4.600688882606293	55207
bc9563250e76c9d63d56a6a63256057f8cba55f7	meta-modeling extension of horn-sroiq and query answering		We investigate reasoning and query answering in expressive domain knowledge bases (KBs) that are in OWL 2 Full and contain large individual data sets. For this, we introduce Hi(Horn-SROIQ) and meta-queries based on HiLog semantics. Hi(Horn-SROIQ) is extended from Horn-SROIQ, the horn fragment of the most expressive description logic (DL) for OWL 2 with well known low data complexity, by allowing classes and roles to be used as individuals. Accordingly, meta-queries are obtained from conjunctive queries by allowing variables to appear in the class and role positions. For reasoning, we first provide a method of reducing satisfiability checking and conjunctive query answering in Hi(Horn-SROIQ) to the corresponding reasoning tasks in Horn-SROIQ soundly and completely, then we show that meta-query answering in Hi(Horn-SROIQ) can be captured by conjunctive query answering. Based on this, we obtain that adding meta-modeling capability to Hi(HornSROIQ) has no impact on the complexity of the considered reasoning tasks. These results make Hi(Horn-SROIQ) appealing for practical usage.	conjunctive query;description logic;hilog;knowledge base;metamodeling;web ontology language	Zhenzhen Gu	2016			web query classification;query expansion;information retrieval;web search query;sargable;query language;rdf query language;mathematics;query optimization	AI	-24.095223851092335	8.479791156625929	55307
3433d785ac9a6f1ced1706dc4393f8b7dd20809c	query containment for conjunctive queries with regular expressions	query language;conjunctive queries;semistructured data;path expressions;regular expression	The management of semistructured data has recently rccoivcd significant attention because of the need of several applications to model and query large volumes of irregular data. This paper considers the problem of query containment for a query language over semistructured data, STRUQLO, that contains the essential feature common to all such languages, namely the ability to specify regular path expressions over the data. We show hcrc that containment of STRUQLO queries is decidable. First, we give a semantic criterion for STRUQLO query containment: WC show that it suffices to check containment on only finitely many canonical databases. Second, we give a syntactic criteria for query containment, based on a notion of query mappings, which extends containment mappings for conjunctive queries. Third, wc consider a certain fragment of STRUQLO, obtained by imposing restrictions on the regular path expressions, and show that query containment for this fragment of STRUQLO is NP complete.	conjunctive query;database;karp's 21 np-complete problems;path expression;query language;regular expression;writing commons	Daniela Florescu;Alon Y. Halevy;Dan Suciu	1998		10.1145/275487.275503	boolean conjunctive query;computer science;path expression;database;conjunctive query;programming language;information retrieval;regular expression;query language;spatial query	DB	-24.909629713058386	10.896853599870482	55314
b454877f0af7010074982bb1fcc1a08a385d8377	the array database that is not a database: file based array query answering in rasdaman	array query answering;petabyte archives;in-situ feature;array dbmss;in-situ processing;array databases;preexisting archives;database query;core data structure;query processing;data structure	Array DBMSs extend the set of supported data structures in databases with (potentially large) multi-dimensional arrays. This information category actually comprises a core data structure in many scientific applications.rnrnWhen it comes to Petabyte archives, storage costs prohibit importing (i.e., copying) such data into a database. Therefore, in-situ processing of database queries is required, that is: evaluating queries on the original files, without previous insertion into the database. We have implemented such an in-situ feature for the rasdaman Array DBMS. In this demonstration, we show with rasdaman how query processing in array databases can simultaneously rely on arrays stored in the database -- as usual -- and in operating system files, like preexisting archives.	array dbms;rasdaman	Peter Baumann;Alex Mircea Dumitru;Vlad Merticariu	2013		10.1007/978-3-642-40235-7_32	database theory;array dbms;database tuning;computer science;data mining;database;view;database schema;information retrieval;database testing;database design	DB	-29.128466288561977	4.86772073479847	55315
3d97a22ff44c8c4b250f34897d44b9632548a898	a new data organizing algorithm for parallel searching	distributed data;extraction information;distributed system;algoritmo paralelo;base donnee repartie;systeme reparti;distributed database;parallel algorithm;algoritmo busqueda;information extraction;algorithme recherche;search algorithm;distributed computing;base repartida dato;algorithme parallele;computer network;allocative efficiency;sistema repartido;factor analysis;data allocation;simulation study;networked systems;data retrieval;heuristic algorithm;partial match query;multinode;extraction informacion	Abstract   This article focuses on the problem of distributing a data base (i.e., a set of records) in a practical distributed computer network system to facilitate parallel searching. In this distributed data base computer network system, we assume that all records are stored in nodes. Whenever a query occurs, all nodes are searched concurrently. In this article, we introduce a concept widely used by statisticians — the factor analysis technique. We show that the factor analysis technique can be used to propose a new allocation method with which multiattribute data records can be allocated onto several nodes such that the maximum node accessing concurrency can be achieved when responding to partial match queries. A mathematical verification and some experimental results show that our method can indeed be used to improve the multinode data allocation efficiency for concurrent accessing. In addition, the simulation studies comparing the proposed method with Chang's heuristic algorithm in terms of the average response time, in response to all possible partial match queries, show the former to be more effective.	algorithm;organizing (structure)	T. Shung-Ming	1996	Journal of Systems and Software	10.1016/0164-1212(94)00064-6	allocative efficiency;heuristic;computer science;theoretical computer science;operating system;database;distributed computing;parallel algorithm;factor analysis;distributed database;data retrieval;information extraction;search algorithm	SE	-27.253719893099042	4.216727445768047	55379
4a637b2f9f86cc280888455ef24bd4453a799eea	extending dlr with labelled tuples, projections, functional dependencies and objectification		We introduce an extension of the n-ary description logic DLR to deal with attribute-labelled tuples (generalising the positio nal notation), with arbitrary projections of relations (inclusion dependencies), gener ic functional dependencies and with global and local objectification (reifying rel ations or their projections). We show how a simple syntactic condition on the appea rance of projections and functional dependencies in a knowledge base makes the language decidable without increasing the computational complexity o f the basicDLR language.	computational complexity theory;description logic;dynamic language runtime;functional dependency;knowledge base;referential integrity	Alessandro Artale;Enrico Franconi	2016	CoRR		tuple;discrete mathematics;description logic;syntax;computational complexity theory;objectification;decidability;positional notation;functional dependency;mathematics	DB	-22.536611448023937	10.168925925679526	55544
d69b4eaacc68d5d691b3b41805a81f58471548c1	specification of dynamics for knowledge-based systems	machine abstraite;inference motor;representacion conocimientos;sistema experto;syntax;knowledge based system;formal specification;abstract state machine;language theory;maquina abstracta;semantics;base connaissance;intelligence artificielle;teoria lenguaje;syntaxe;semantica;semantique;universiteitsbibliotheek;specification language;abstract machine;specification formelle;especificacion formal;motor inferencia;conceptual modelling;artificial intelligence;base conocimiento;formal specication;lenguaje especificacion;inteligencia artificial;systeme expert;logic programs;knowledge representation;sintaxis;representation connaissances;langage specification;article in monograph or in proceedings;moteur inference;theorie langage;knowledge base;expert system	During the last years, a number of formal specification languages for knowledge-based systems have been developed. Characteristic for knowledge-based systems are a complex knowledge base and an inference engine which uses this knowledge to solve a given problem. Specification languages for knowledge-based systems have to cover both aspects: they have to provide means to specify a complex and large amount of knowledge and they have to provide means to specify the dynamic reasoning behaviour of a knowledge-based system. This paper will focus on the second aspect, which is an issue considered to be unsolved. For this purpose, we have surveyed existing approaches in related areas of research. We have taken approaches for the specification of information systems (i.e., Language for Conceptual Modelling and Troll), approaches for the specification of database updates and the dynamics of logic programs (Transaction Logic and Dynamic Database Logic), and the approach of Abstract State Machines.	abstract state machines;finite-state machine;formal specification;inference engine;information system;knowledge base;knowledge-based systems;specification language;transaction logic	Pascal van Eck;Joeri Engelfriet;Dieter Fensel;Frank van Harmelen;Yde Venema;Mark Willems	1998		10.1007/BFb0055495	natural language processing;knowledge representation and reasoning;knowledge base;syntax;specification language;computer science;artificial intelligence;philosophy of language;knowledge-based systems;machine learning;formal specification;database;semantics;abstract machine;expert system;algorithm;abstract state machines	AI	-21.33173606353164	11.116263766761248	55674
be1260fc05412fcda6093f17851ea094d2dbc84c	a methodology for conceptual design of office data bases	base donnee;metodologia;database;methodologie;conceptual design;methodology	Paper forms are a widely used mean to collect and communicate data in the office environment. As a consequence, they are a very natural type of user requirements and an effective starting point in data base design of office applications. In the paper a methodology for conceptual design of office data bases is described. It assumes forms as input documents to express requirements and produces as the result of the design process an Entity Relationship conceptual schema. Strategies are proposed to extract from the initial requirements conceptual structures and integrate them into a global description.	conceptual schema;data integrity;database;entity–relationship model;requirement;source-to-source compiler;user requirements document	Carlo Batini;G. Barbara Demo;Antonio Di Leva	1984	Inf. Syst.	10.1016/0306-4379(84)90008-5	conceptual model;computer science;conceptual schema;methodology;data mining;conceptual design;database	DB	-31.479250496210202	13.682499215764604	55736
6bd9a6b69440563239857e194628bd91405b82c7	data generalization on object - relational data model of video annotation database	data model	In the treatment of water by the use of a cationic and/or anionic exchange resin, catalyzed hydrazine compound is added to the feed water to alleviate the iron fouling of the ion exchange resin.	data model;relational model	Uma P. Maheswari;M. RajaRam	2007			relational model;computer science;data mining;semi-structured model;relational database;database;data model;data modeling;database design;database model;change data capture	DB	-32.52645526118642	9.155787439758742	55778
05e8630fe20d0b66d02d204aa2299a0b083a14d1	sequenced spatio-temporal aggregation in road networks	road network;book chapter;spatio temporal databases;discrete time;space time;etl;temporal aggregation;data warehousing;business intelligence;data structure;data integration	Many applications of spatio-temporal databases require support for sequenced spatio-temporal (SST) aggregation, e. g., when analyzing traffic density in a city. Conceptually, an SST aggregation produces one aggregate value for each point in time and space.  This paper is the first to propose a method to efficiently evaluate SST aggregation queries for the COUNT, SUM, and AVG aggregation functions. Based on a discrete time model and a discrete, 1.5 dimensional space model that represents a road network, we generalize the concept of (temporal) constant intervals towards constant rectangles that represent maximal rectangles in the space-time domain over which the aggregation result is constant. We propose a new data structure, termed SST-tree, which extends the Balanced Tree for one-dimensional temporal aggregation towards the support for two-dimensional, spatio-temporal aggregation. The main feature of the Balanced Tree to store constant intervals in a compact way by using two counters is extended towards a compact representation of constant rectangles in the space-time domain. We propose and evaluate two variants of the SST-tree. The SSTT-tree and SSTH-tree use trees and hashmaps to manage spacestamps, respectively. Our experiments show that both solutions outperform a brute force approach in terms of memory and time. The SSTH-tree is more efficient in terms of memory, whereas the SSTT-tree is more efficient in terms of time.	avg;aggregate data;aggregate function;analysis of algorithms;approximation algorithm;brute-force search;data structure;experiment;global positioning system;granule (oracle dbms);max;maximal set;relational database management system;self-balancing binary search tree;social network aggregation;spatiotemporal database;steiner tree problem;temporal database;tree traversal	Igor Timko;Michael H. Böhlen;Johann Gamper	2009		10.1145/1516360.1516368	discrete time and continuous time;data structure;computer science;theoretical computer science;data integration;data warehouse;space time;data mining;database;business intelligence;algorithm	DB	-29.265351022142774	7.533065546935642	55832
a509b32c818b19005bdfbff54c1624adbaddce05	an xml-based methodology for parametric temporal database model implementation	database system;parametric temporal database;temporal data models;canstorex;data model;temporal database;functional model;xml storage;database implementation	Parametric data model is one of dimensional data models. It defines attributes as functions, modeling a real world object into a single tuple in a database. Such one-to-one correspondence between an object in the real world and a tuple provides various advantages in modeling dimensional data, avoiding self-joins which frequently appear in temporal data models which fragment an object into multiple tuples. Despite its modeling advantages, it is impractical to implement the parametric data model on top of conventional database systems because of the data model's variable attribute sizes. However, such implementation challenge can be resolved by XML because XML is flexible for data boundaries. In this paper, we present an XML-based implementation methodology for the parametric temporal data model. In our implementation, we develop our own XML storage called CanStoreX (Canonical Storage for XML) and build the temporal database system on top of the CanStoreX.		Seo-Young Noh;Shashi K. Gadia;Shihe Ma	2008	Journal of Systems and Software	10.1016/j.jss.2007.08.018	xml validation;data modeling;database theory;semi-structured model;data model;computer science;function model;database model;xml framework;data mining;xml database;xml schema;database;temporal database;database schema;physical data model;information retrieval;database design;hierarchical database model	DB	-30.30537224898749	9.793219037341148	55870
2eb5d38a3b4b99e16455eb7cf2ddf6536214cb0a	on handling exceptions	exception handling;verifying exception handling rule;tool usable;information system;current literature;finite state machines;unique input	The current literature of information systems has dealt extensively with all kinds of exceptions. There are several studies defining the concept of exception and even providing classifications. However, no studies provide a method for verifying the rules in order to handle exceptions and to achieve the goals set by an organization's rules. In this paper, a model employing a set of unique input/output (UIO) sequences is presented for verifying such rules. The model originally presented for Finite State Machines (FSM) has been modified to include concepts of exception handling and will be used to form a tool usable for verifying exception handling rules in OISs.	exception handling;finite-state machine;information system;input/output;verification and validation	Heikki Saastamoinen;George M. White	1995		10.1145/224019.224051	computer science;data mining;database;algorithm	SE	-32.22114256648728	14.057853244486434	56048
d09801944393a35541c99a25f8a12a5c2b3254be	3d meta model generation with application in 3d object retrieval		In the application of 3D object retrieval we search for 3D objects similar to a given query object. When a user searches for a certain class of objects like 'planes' the results can be unsatisfying: Many object variations are possible for a single class and not all of them are covered with one or a few example objects. We propose a meta model representation which corresponds to a procedural model with meta-parameters. Changing the meta-parameters leads to different variations of a 3D object. For the meta model generation a single object is constructed with a modeling tool. We automatically extract a procedural representation of the object. By inserting meta-parameters we generate our meta model. The meta model defines a whole object class. The user can choose a meta model and search for all objects similar to any instance of the meta model to retrieve all objects of a certain class from a 3D object database. We show that the retrieval precision is significantly improved using the meta model as retrieval query.	metamodeling	Roman Getto;Johannes Merz;Arjan Kuijper;Dieter W. Fellner	2017		10.1145/3095140.3095146	object-oriented design;object class;computer vision;artificial intelligence;parametric model;method;metamodeling;computer science;object model;data mining;generative model	Vision	-31.027649798986065	14.913428390421025	56573
277e8369204985a6f781fa3fb962e1e6904088ae	a note on two simple transformations for improving the efficiency of an ilp system	software tool;systeme apprentissage;knowledge discovery from databases;engenharia do conhecimento engenharia electrotecnica electronica e informatica;program transformation;inductive logic programming;complejidad programa;logical programming;specification programme;transformation programme;artigo em livro de atas de conferencia internacional;learning systems;transformacion programa;inductive reasoning;programmation logique;program complexity;program specification;programacion logica;especificacion programa;complexite programme;raisonnement inductif	Inductive Logic Programming (ILP) systems have had noteworthy successes in extracting comprehensible and accurate models for data drawn from a number of scientific and engineering domains. These results suggest that ILP methods could enhance the model-construction capabilities of software tools being developed for the emerging discipline of “knowledge discovery from databases.” One significant concern in the use of ILP for this purpose is that of efficiency. The performance of modern ILP systems is principally affected by two issues: (1) they often have to search through very large numbers of possible rules (usually in the form of definite clauses); (2) they have to score each rule on the data (usually in the form of ground facts) to estimate “goodness”. Stochastic and greedy approaches have been proposed to alleviate the complexity arising from each of these issues. While these techniques can result in order-of-magnitude improvements in the worstcase search complexity of an ILP system, they do so at the expense of exactness. As this may be unacceptable in some situations, we examine two methods that result in admissible transformations of clauses examined in a search. While the methods do not alter the size of the search space (that is, the number of clauses examined), they can alleviate the theorem-proving effort required to estimate goodness. The first transformation simply involves eliminating literals using a weak test for redundancy. The second involves partitioning the set of literals within a clause into groups that can be executed independently of each other. The efficacy of these transformations are evaluated empirically on a number of well-known ILP datasets. The results suggest that the transformations can provide, under some circumstances, significant gains as the complexity of clauses sought increases. †COPPE/Sistemas, UFRJ, Brazil and LIACC, Universidade do Porto, Portugal. ‡Oxford University Computing Laboratory, Wolfson Building, Parks Road, Oxford UK. §LIACC and FEUP, Universidade do Porto, Portugal.	database;greedy algorithm;inductive logic programming;inductive reasoning;literal (mathematical logic)	Vítor Santos Costa;Ashwin Srinivasan;Rui Camacho	2000		10.1007/3-540-44960-4_14	epistemology;computer science;artificial intelligence;theoretical computer science;inductive reasoning;machine learning;database;mathematics;programming language;algorithm	DB	-19.599216124750047	14.652075335427538	56829
17b79e8b33d62eebdf63a9a23199be3c575ee46f	transforming queries from a relational schema to an object schema: a prototype based on f-logic	relational schema;transforming query;equivalent object schema;transforming queries	This paper describes a technique to support interoperable query processing when multiple heterogeneous knowledge servers are accessed. The problem is to support query transformation transparently, so a user can pose queries locally, without any need of global knowledge about diierent data models and schema. In a companion paper, an architecture for supporting interoperability was described and we provided some details of transforming queries from an object schema to an equivalent relational schema. In this paper, we focus on transforming SQL source queries, posed against a rela-tional schema, to XSQL queries to be evaluated against equivalent object schema. We describe some functional modules in detail, namely an extractor module (EM) which extracts semantics from a source query, and a heterogeneous mapping module (HTM) which maps among entities in diierent schema, based on some mapping rules which reeect global knowledge of the models, schema and query languages. The local and global dictionary knowledge is represented in a canonical form, using a high-level logic representation, namely F-logic.	acm computing surveys;data model;data structure;database schema;declarative programming;dictionary;entity;f-logic;gri;html;high- and low-level;information systems;information system;interoperability;interoperation;knowledge management;lexicon;logic programming;machine translation;map;naruto shippuden: clash of ninja revolution 3;pegasus;programming language;prolog;prototype;qualitative comparative analysis;query language;randomness extractor;sql;schematic;search engine optimization;select (sql);source-to-source compiler	Yahui Chang;Louiqa Raschid	1994		10.1007/3-540-58495-1_16	schema migration;information schema;semi-structured model;logical schema;conceptual schema;document structure description;star schema;data mining;database;document schema definition languages;conjunctive query;database schema;xml schema editor;information retrieval	DB	-32.953109731551066	8.497257641510581	57068
a6fc9459da364e20b1e1407670cf082f2a9de341	extensible data modeling for statistical databases	data model	A conceptual data model is a critical key factor to achieve the goal of a database being designed. The requirements of today’s statistical database applications call for powerful modeling techniques. There are a number of data modeling problems that cannot be solved by using existing statistical data models. This paper proposes conceptual data modeling techniques to meet the requirements of a large shared statistical database application. An object-oriented statistical data model (OOSDM) is constructed using object-oriented concepts. The OOSDM represents statistical data in different levels of abstraction which enhances the semantic richness and theion which enhances the semantic richness and the extensibility of the conceptual model. The extensibility is achieved through an inheritance mechanism. ‘Ihe semantic richness shows that the model is extensible in a manageable way without damaging the semantics of the model.	conceptual schema;data model;data modeling;extensibility;principle of abstraction;requirement;statistical database	Malee Wongsaroje	1995			idef1x;data modeling;data model;computer science;database	DB	-32.007757712683734	12.258644423828686	57106
574fcb55d0fdbe3731d29eef021c9be4f0a21da7	distributed top-n query processing with possibly uncooperative local systems	different technique;top-n query;suitable tuples;local databases;relational databases;top-n query processing;uncooperative local database system;different method;uncooperative local system;user query;appropriate site;local system;distributed databases	We consider the problem of processing top-N queries in a distributed environment with possibly uncooperative local database systems. For a given top-N query, the problem is to nd the N tuples that satisfy the query the best but not necessarily completely in an eÆcient manner. Top-N queries are gaining popularity in relational databases and are expected to be very useful for e-commerce applications. Many companies provide the same type of goods and services to the public on the Web, and relational databases may be employed to manage the data. It is not feasible for a user to query a large number of databases. It is therefore desirable to provide a facility where a user query is accepted at some site, suitable tuples from appropriate sites are retrieved and the results are merged and then presented to the user. In this paper, we present a method for constructing the desired facility. Our method consists of two steps. The rst step determines which databases are likely to contain the desired tuples for a given query so that the databases can be ranked based on their desirability with respect to the query. Four di erent techniques are introduced for this step with one requiring no cooperation from local systems. The second step determines how the ranked databases should be searched Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large Data Base Endowment. To copy otherwise, or to republish, requires a fee and/or special permission from the Endowment. Proceedings of the 29th VLDB Conference, Berlin, Germany, 2003 and what tuples from the searched databases should be returned. A new algorithm is proposed for this purpose. Experimental results are presented to compare di erent methods and very promising results are obtained using the method that requires no cooperation from local databases. keywords: Top-N queries, distributed databases, query processing.	algorithm;distributed database;e-commerce;relational database;vldb;world wide web	Clement T. Yu;George Philip;Weiyi Meng	2003			online aggregation;sargable;query optimization;query expansion;web query classification;boolean conjunctive query;computer science;query by example;data mining;database;rdf query language;web search query;range query;distributed database;information retrieval;local system;query language;spatial query	DB	-26.74753851596238	6.736396696342849	57131
b837a3239fe7a1f0328e6756ee31a2f036660936	integrating approximate summarization with provenance capture		How to use provenance to explain why a query returns a result or why a result is missing has been studied extensively. Recently, we have demonstrated how to uniformly answer these types of provenance questions for first-order queries with negation and have presented an implementation of this approach in our PUG (Provenance Unification through Graphs) system. However, for realisticallysized databases, the provenance of answers and missing answers can be very large, overwhelming the user with too much information and wasting computational resources. In this paper, we introduce an (approximate) summarization technique that generates compact representations of why and why-not provenance. Our technique uses patterns as a summarized representation of sets of elements from the provenance, i.e., successful or failed derivations. We rank these patterns based on their descriptiveness (we use precision and recall as quality measures for patterns) and return only the top-k summaries. We demonstrate how this summarization technique can be integrated with provenance capture to compute summaries on demand and how sampling techniques can be employed to speed up both the summarization and capture steps. Our preliminary experiments demonstrate that this summarization technique scales to large instances of a real-world dataset.	approximation algorithm;automatic summarization;best, worst and average case;computation;computational resource;data integrity;database;datalog;experiment;first-order predicate;grammar-based code;han unification;precision and recall;sampling (signal processing)	Seokki Lee;Xing Niu;Bertram Ludäscher;Boris Glavic	2017			automatic summarization;information retrieval;provenance;computer science	DB	-25.15509630641263	7.27636928750754	57176
c9f62dda1060c6d9ecc424d7bed3954f8fce2dde	query rewriting for swift (first) answers	information resources;empirical study;query processing;query processing query rewriting database query tuples query evaluation decision makers information overload micropayment mechanism database query optimization system resources query response time user waiting time heuristic decomposition oracle server;internet query processing information resources;query optimization;information overload;information search;decision maker;internet;query evaluation;waiting time;query rewrite;query processing computer society databases delay costs information retrieval prototypes web server internet investments;query rewriting;database query;progressive query evaluation;www	ÐTraditionally, the answer to a database query is construed as the set of all tuples that meet the criteria stated. Strict adherence to this notion in query evaluation is, however, increasingly unsatisfactory because decision makers are more prone to adopting an exploratory strategy for information search which we call agetting some answers quickly, and perhaps more later.o From a decision-maker's perspective, such a strategy is optimal for coping with information overload and makes economic sense (when used in conjunction with a micropayment mechanism). These new requirements present new opportunities for database query optimization. In this paper, we propose a progressive query processing strategy that exploits this behavior to conserve system resources and to minimize query response time and user waiting time. This is accomplished by the heuristic decomposition of user queries into subqueries that can be evaluated on demand. To illustrate the practicality of the proposed methods, we describe the architecture of a prototype system that provides a nonintrusive implementation of our approach. Finally, we present experimental results obtained from an empirical study conducted using an Oracle Server that demonstrate the benefits of the progressive query processing strategy. Index TermsÐWWW, Internet, progressive query evaluation, query optimization, query rewrite.	algorithm;autonomous robot;experiment;heuristic (computer science);ibm informix;information overload;mathematical optimization;micropayment;middleware;online aggregation;oracle database;prototype;query optimization;requirement;response time (technology);rewrite (programming);rewriting;sql;user profile;www	Kian-Lee Tan;Cheng Hian Goh;Beng Chin Ooi	2000	IEEE Trans. Knowl. Data Eng.	10.1109/69.877503	online aggregation;sargable;decision-making;query optimization;query expansion;web query classification;the internet;ranking;computer science;query by example;information overload;data mining;database;rdf query language;web search query;empirical research;view;world wide web;information retrieval;query language	DB	-25.808690282258375	4.610679233696564	57279
262ee4431dbdd52504410f56a3fcbee974f05823	obtaining more answers from information integration systems	incomplete information;information integration	The current generation of rewriting-algorithms in source-centric (local-as-view) information integration systems all produce a reformulated query that retrieves what has been thought of as ”the best obtainable” answer, given the circumstances that the source-centric approach introduces incomplete information into the virtual global relations. This ”best obtainable” answer does not however allow partial information. We define the semantics of partial facts, and provide two methods for computing partial answers. The first method is tableau-based and is a generalization of the ”inverse-rules” approach. The second method is a generalization of the rewriting approach, and is based on partial containment mappings introduced in the paper.	algorithm;approximation;computer science;document object model;email;emoticon;long division;method of analytic tableaux;rewriting	Gösta Grahne;Victoria Kiricenko	2002			complete information;data mining;database;information integration;information quality;computer science;information retrieval;semantics;rewriting	DB	-25.92283341652638	8.537496635945272	57349
b61179976fa435cd320a44b8e721b1dcd2d1d616	an object-oriented data model for multiple representation of object semantics	multiple representation;modelizacion;base donnee;object oriented model;object oriented data model;multimedia;database;base dato;semantics;semantica;semantique;consistencia;data model;modelisation;object oriented;consistance;oriente objet;object oriented database;modeling;orientado objeto;consistency	Abstract#R##N##R##N#This paper proposes the “poly-aspect data model,” which is an extension of the object-oriented data model. The purpose of the poly-aspect data model is to represent in a natural way the multimedia data in the database. One of the features of the multimedia data is the diversified representation of the entities of the real world. In the poly-aspect data model, the entity is interpreted as an object, and a model is provided so that multiple complete representations are given to the object. Such an object is called the poly-aspect object. In the poly-aspect data model, an object may have more than one instance as its representation. The class defines the format of the instance. The multiple instance of the same object are distinguished by the class names. When the attributes of multiple instances are defined by the same superclass, those attributes are interpreted to have the same semantics. The same value is shared among the instances, and the consistency is retained.#R##N##R##N##R##N##R##N#As a realization example of the poly-aspect object, the architecture of the “Sarah” database management system, which is currently being constructed experimentally is described.	data model	Tomoyuki Ishimaru;Shunsuki Uemura	1996	Systems and Computers in Japan	10.1002/scj.4690270903	method;systems modeling;object model;semi-structured model;data model;computer science;object;theoretical computer science;data mining;database;semantics;data transfer object;consistency;programming language;object-oriented programming;logical data model;god object;database design;object definition language	Robotics	-30.10735833488824	12.565533033650988	57353
ce7912a778283bd70933b59a72427dfc403035b0	the formal representation of semantic on stratum attribute data oriented 3d geo-modeling	rocks geophysical techniques geophysics computing knowledge representation;data integrity;three dimensional;inference rule;geophysics computing;stratum ontology semantics stratum attribute data 3d geomodeling rock attribute boreholes geoontology;normal form;rocks;knowledge representation;geology buried object detection global positioning system geographic information systems ontologies minerals channel hot electron injection computer aided instruction data engineering geoscience;geophysical techniques	In geology field, the rock attribute is the reference to judge the strata relationship between boreholes. It is the key issues to transfer the qualitative attribute to quantitative data by using of geo-ontology. Being a case of geo-ontology the Stratum-ontology is a formalized representation and an instance to meet the quantitative need. According to the domination features of carbonate rock, the semantic contents of stratum-ontology are defined. Based on the construction of Stratum-ontology, the formal representation of semantic with BNF normal form is developed, which is the criterion for the judgment of stratum relation. According to the judgment and stratum inference rule, a digital three-dimensional geological model of a research district is constructed as a case.	3d modeling;akaike information criterion;beta normal form;dominating set;knowledge representation and reasoning;pet rock;web ontology language	Lei Xu;Lixin Wu;Defu Che;Zhenfeng Cai	2006	2006 IEEE International Symposium on Geoscience and Remote Sensing	10.1109/IGARSS.2006.231	knowledge representation and reasoning;three-dimensional space;computer science;artificial intelligence;data science;data integrity;data mining;rule of inference	Embedded	-22.392620434890848	4.856244419919452	57533
d1e7b15f66ec099ce289c5a777d6038e48ec0fe9	ensuring multilevel database security using fuzzy logic	fuzzy logic		database security;fuzzy logic	Hakan Koksal;Adnan Yazici	1998			fuzzy set operations;fuzzy electronics;fuzzy logic;database;data mining;database theory;computer science;database security	Security	-30.257012822090367	8.844020190564185	57744
2672d7feea062b38374bf17ef7743015fa558701	weaker forms of monotonicity for declarative networking: a more fine-grained answer to the calm-conjecture	cloud programming;distributed database;journal contribution;relational transducer;expressive power;consistency;coordination	The CALM-conjecture, first stated by Hellerstein [23] and proved in its revised form by Ameloot et al. [13] within the framework of relational transducer networks, asserts that a query has a coordination-free execution strategy if and only if the query is monotone. Zinn et al. [32] extended the framework of relational transducer networks to allow for specific data distribution strategies and showed that the nonmonotone win-move query is coordination-free for domain-guided data distributions. In this paper, we complete the story by equating increasingly larger classes of coordination-free computations with increasingly weaker forms of monotonicity and make Datalog variants explicit that capture each of these classes. One such fragment is based on stratified Datalog where rules are required to be connected with the exception of the last stratum. In addition, we characterize coordination-freeness as those computations that do not require knowledge about all other nodes in the network, and therefore, can not globally coordinate. The results in this paper can be interpreted as a more fine-grained answer to the CALM-conjecture.	declarative programming	Tom J. Ameloot;Bas Ketsman;Frank Neven;Daniel Zinn	2014		10.1145/2594538.2594541	computer science;theoretical computer science;data mining;database;mathematics;consistency;programming language;distributed database;expressive power;algorithm	HCI	-20.676760789028503	10.477492911109811	57865
0d691663e272c9591a85e0c19782f5e99ed8cb94	generating deductive database explanations	levels of abstraction;deductive databases	Existing explanation systems for deductive databases show forests of proof trees. Although proof trees are often useful, they are only one possible interesting representation. We argue that an explanation system for deductive databases must be able to generate explanations at several levels of abstraction. One possible and well known technique to achieve this exibility is to instrument meta-interpreters. It is, however, not often used because of its ineeciency. On the other hand, deductive databases often generate intermediate information stored in the physical database. This information can be considered as a low-level trace giving a faithful picture of what has happened at the relational level. The deductive reasoning is lost but can be very easily recovered by a meta-interpreter. In this article we describe a technique to generate explanations by integrating a relational trace and an instru-mented meta-interpreter. The expensive aspects of meta-interpretation are reduced by the use of the trace which avoids many costly calculations. The exibility of meta-interpretation is preserved, as illustrated by the generation of three diierent kinds of explanations: a box-oriented trace, a multi-SLD-AL tree and abstract AND trees. This technique enables powerful explanation systems to be implemented with very few modiications of the deductive database mechanism itself.	deductive database;high- and low-level;principle of abstraction;steiner tree problem	Sarah Mallet;Mireille Ducassé	1999			natural language processing;computer science;database;programming language	DB	-20.111462209530703	16.456316492204895	58086
b1af192c88778b59e0c40e6c14131ed920e0817c	prefetching ldd: a benefit-oriented approach	location service;prefetches;wireless networks;prefetch;location dependent data ldd;indexing of information;location;pervasive services;wireless telecommunication systems;information management;cost effectiveness;ubiquitous computing;dependent data;computer science;data handling;quality of service;pervasive service;mobile computing;benefit analysis;mobile data management;data acquisition;location dependent data;problem solving	Location dependent data (LDD) are those closedly related to specific locations. Services involving LDD are becoming more and more popular in recent years. To provide a high quality of service for these applications, prefetching LDD before a mobile client reaches the LDD site is an important factor to consider. In this paper, we analyze this problem and propose a benefit-oriented method for cost-effective prefetch of LDD. Our performance study shows that the method is superior to other traditional methods.	cpu cache;display resolution;lego digital designer;link prefetching;quality of service	Chao-Chun Chen;Chiang Lee;Chun-Chiang Wang;Yu-Chi Chung	2006		10.1145/1143549.1143770	instruction prefetch;embedded system;cost-effectiveness analysis;quality of service;computer science;cost–benefit analysis;operating system;wireless network;group method of data handling;database;information management;data acquisition;location;mobile computing;world wide web;ubiquitous computing;computer network	DB	-31.172388293529398	17.177670912778495	58337
e69c033c83fbb8c260d4fc0e0c5863d7d5e0c08a	implementação em java de um algoritmo de árvore de decisão acoplado a um sgbd relacional		Nowadays one of the most researched topics in data mining is its integration with database. The reflexes of these efforts already appear in the major commercial databases, which are supplying integrated solutions of data mining. This work evaluates the coupling of an unusual decision tree algorithm to a relational database management system, and analyzes the performance of the implementation in Java.	algorithm;data mining;decision tree;java;list of algorithms;relational database management system;unified model	Mauricio Onoda;Nelson F. F. Ebecken	2001			database;relational database management system;data mining;decision tree learning;decision tree;minimum description length;computer science;data warehouse;java	DB	-31.459325241434428	8.238526545862884	58516
5e4865e9f3ff3057bbb44257218334ada155c92d	creation of a java-based interactive modeling environment with tape library model example	system recovering speed;storage management digital simulation information retrieval interactive systems software libraries;tape storage;storage disk;memory management;software libraries;information retrieval;storage management;java based tape library simulator;java based interactive modeling environment;tape library model;backup environment;java software libraries robots information retrieval scheduling algorithm delay switches throughput analytical models graphical user interfaces;software simulation model;interaction model;software simulation model java based interactive modeling environment tape library model backup environment restore environment data retrieval tape storage storage disk system recovering speed tape drives simtape java based tape library simulator allocation schemes;restore environment;allocation schemes;tape drives;high performance;interactive systems;simulation model;data retrieval;simtape;digital simulation	Within high performance backup/restore environment there are cases that require the retrieval of massive data with huge size from tape storage to disks. The data retrieval time affects the system utilization or the system recovering speed. Some schemes were/will be proposed to place data across tape drives for minimized data retrieval latency. SimTape, a Java-based tape library simulator, is built to compare above mentioned allocation schemes which minimize data seek time and tape switch time respectively. This paper describes the development and application of SimTape's software simulation model, also, for the design and optimization of the record, readout and data recovery processes common to all existing tape library processes. The simulation model provides an accurate statistical report from which each scenario can be evaluated	aggregate data;algorithm;backup;bottleneck (engineering);computer data storage;data recovery;data retrieval;disk storage;hard disk drive performance characteristics;internet;java;library (computing);magnetic tape data storage;mathematical optimization;response time (technology);simulation;streaming media;tape drive;tape library	Lingfang Zeng;Dan Feng;Fang Wang;Zhuo Cheng;Qiang Zou	2006	2006 International Workshop on Networking, Architecture, and Storages (IWNAS'06)	10.1109/IWNAS.2006.23	embedded system;real-time computing;magnetic tape data storage;computer science;operating system;simulation modeling;database;world wide web;data retrieval;memory management	DB	-33.489626640087025	17.24118078480341	58518
f7bbe560fae12e9d76b182d37b6d5943b7d22b00	object-oriented conceptual graphs	object oriented model;object oriented;conceptual graph;program specification	In this paper a state based view of conceptual graphs borrowed from the Object-Z program speciication language is introduced. This new view is contrasted with the object-oriented model developed by Sowa 5]. The new model is demonstrated by reducing Sowa's example proof from 18 steps to 4 steps. The new model uses relations connecting object pre-states and post-states to represent object methods, rather than using messages as concepts as in Sowa's version. We argue that the new model is clearer because it is based on a simple state transition and that this leads to more eecient theorem proving and programming.	automated theorem proving;conceptual graph;john f. sowa;object-z;state transition table	Gerard Ellis	1995		10.1007/3-540-60161-9_35	conceptual graph;object model;computer science;conceptual schema;database;programming language;object-oriented programming	Logic	-26.52647194058453	18.06655018119091	59095
366c9ff16e9db4631217907148c82a18f07cc5df	case-based planning	case based planning	We briefly examine case-based planning starting with the seminal work of Hammond. Derivational analogy represents an important shift of technical emphasis that helped mature the techniques. The choice of abstraction level is equally important. We conclude by discussing theoretical underpinnings and by providing some pointers to current directions.	abstraction layer;automated planning and scheduling;pointer (computer programming)	Michael T. Cox;Hector Muñoz-Avila;Ralph Bergmann	2005	Knowledge Eng. Review	10.1017/S0269888906000592	computer science;artificial intelligence;management science	HCI	-22.284667300877974	14.636804919190599	59297
9a873ec4115577595ee71790c293f479cea06caf	an oo database migrates to the web	client server systems internet relational databases object oriented methods object oriented languages;object oriented modeling prototypes data models relational databases satellites java standards development paper products geodesy performance analysis;object oriented methods;client server systems;relational database;internet;smalltalk;relational databases;object oriented languages;remote fetching oo database migration web relational database oo paradigm project goal java based mapping client query capabilities geographic objects smalltalk mapping prototype corba specification	The authors faced the dual challenge of first converting a relational database to the OO paradigm, then migrating it to the Web. They describe the procedures and technologies they used. The basic architecture of the resulting system is presented. The project goal was to create a Java based mapping client that would provide display and query capabilities for a set of geographic objects (features) that would be retrieved from the Smalltalk mapping prototype acting as a server. We planned to base communication on the Corba specification. We also planned to keep the remote fetching of objects completely transparent to the end user, who would be able to manipulate the features as if they were local.	world wide web	Maria Cobb;Harold Foley;Ruth Wilson;Miyi Chung;Kevin Shaw	1998	IEEE Software	10.1109/52.676716	relational database;computer science;database;programming language;world wide web;database design	Vision	-33.36960406721979	12.245319094577559	59355
46c1fa3a24c05af85d7cb8d78352952f0901a1b9	the metaclinic database system: a metadata approach to building research database systems	database system		relational database management system	Thomas Caldwell	2004			database tuning;database schema;metadata repository;metadata;database;database catalog;database testing;database design;database theory;computer science	DB	-31.575994767843138	9.153534362757405	59585
4594ce6ee2c4dda3359bb514a5975e2897fb6b1b	specifying data bases management systems by using rm-odp engineering language	reference model;complex network;schemas;rm odp;object model;database management system;distributed system	Distributed systems can be very large and complex. The various considerations that influence their design can result in a substantial specification, which requires a structured framework that has to be managed successfully. The purpose of the RMODP is to define such a framework. The Reference Model for Open Distributed Processing (RM-ODP) provides a framework within which support of distribution, inter-working and portability can be integrated. It defines: an object model, architectural concepts and architecture for the development of ODP systems in terms of five viewpoints. Which include an information viewpoint. Since the usage of Data bases management systems (DBMS) in complex networks is increasing considerably, we are interested, in our work, in giving DBMS specifications through the use of the three schemas (static, dynamic, invariant). The present paper is organized as follows. After a literature review, we will describe then the subset of concepts considered in this work named the database management system (DBMS) object model. In the third section, we will be interested in the engineering language and DMBS structure by describing essentially DBMS objects. Finally, we will present DBMS engineering specifications and makes the connection between models and their instances. This introduces the basic form of the semantic approach we have described here.	abstract syntax;complex network;database;distributed computing;management system;metamodeling;parallel computing;rm-odp;reference model;semantics (computer science);software framework;software portability;unified modeling language	Jalal Laassiri;Said El Hajji;Mohamed Bouhdadi;Ghizlane Orhanou;Youssef Balouki	2010	CoRR		reference model;object model;computer science;data mining;database;programming language;complex network	DB	-33.03118941920313	13.014970646274628	59637
9e8d782a43c6616c87929f410194983a9bdaf45c	fast maintenance of semantic integrity assertions using redundant aggregate data	enforcement method;large class;program logic;certain set;redundant data;correctness proof;consistent database state;redundant aggregate data;semantic integrity assertion;fast maintenance;database system;consistent state;relational calculus assertion;semantic integration	Semantic integrity assertions are predicates that define consistent database states. To enforce such assertions, a database system must prevent any update from mapping a consistent state to an inconsistent one. In this paper, we describe an enforcement method that is efficient for a large class of relational calculus assertions. The method automatically selects minima and maxima of certain sets to maintain as redundant data in the database. This redundant data is sufficient for enforcing all of the assertions in the class, yet it can be easily maintained. Correctness proofs are expressed in Hoare's program logic.	aggregate data;semantic web	Philip A. Bernstein;Barbara T. Blaustein;Edmund M. Clarke	1980			semantic integration;computer science;data mining;database;programming language	DB	-26.471504057992572	12.79456264242106	59711
019d225400f5d52b73ec4e1b6f5c27201bd5aa66	modeling fiscal data with the data cube vocabulary		We present a fiscal data model based on the Data Cube Vocabulary, which we developed for the OpenBudgets.eu project. The model defines component properties out of which data structure definitions for concrete datasets can be composed. Based on initial usage experiments, simple validation constraints have been formulated.	data cube;data model;data structure;experiment;vocabulary	Jindrich Mynarz;Vojtech Svátek;Sotirios Karampatakis;Jakub Klímek;Charalampos Bratsas	2016			data cube;natural language processing;speech recognition;vocabulary;computer science;artificial intelligence	DB	-32.874649763262475	9.717009314664592	60013
170ab369f7b0ba39bd707e7cdc4e8201cfb8f09c	a computerised reservation system using a relational database augmented by constraint based techniques	relational database		relational database	Steven A. Battle;Richard McClatchey	1995			data mining;database schema;database;relational model;component-oriented database;computer science;object-relational impedance mismatch;relational database;database design;database model;spatiotemporal database	DB	-31.39559025815082	9.186367271057717	60036
102d1e4d0e5cd1af8416eba8df28bc2ee4e2efce	on the estimation of query execution time in object-oriented databases at the early design stages	mathematical model;distribution function;database management;estimating equation;generating function;information system;laplace stieltjes transform;system design	Due to the complexity of modern object-oriented database management systems’ (OODBMS) query execution processes it is rather hard for a system designer to predict performance characteristics of an information system under development at the early design stages. This paper proposes novel mathematical model and methods for evaluation of query execution time for OODBMS. These methods provide estimation equations for two basic nary algorithms employed in OODBMS: Forward Join and Reverse Join. The proposed methods are based on Generating Functions and Laplace-Stieltjes Transformation apparatus, and allow to use arbitrary distribution functions for the definition of query execution algorithm’s and database objects’ parameters (number of objects, predicate selectivity, index scan time, etc.). For some degenerate cases corresponding corollaries are obtained with simplified equations. Database page organization is addressed by equations which extend Yao’s formula for arbitrary distributions.	algorithm;database;information system;mathematical model;run time (program lifecycle phase);selectivity (electronic);systems design;yao graph	Aleksey V. Burdakov;Yuri A. Grigorev;Andrey D. Ploutenko	2002			generating function;data mining;estimating equations;laplace–stieltjes transform;database;theoretical computer science;information system;computer science;systems design;database design;object-oriented programming	DB	-29.69203408492553	9.638435453219905	60156
3efbe7b948099cdc54b9f4263549531bb95f3396	a comparative study of sql based approaches for mining association rules over relational dbms			object-relational database;relational database management system;sql	Mohamed A. Shouman;Mohamed Aziz;Abdel Hadi N. Ahmed;Nora Zaki Abdel Fatah	2010	Egyptian Computer Science Journal		relational database management system;nested set model;sql;engineering;relational database;database;association rule learning;quel query languages;data mining;query by example;null (sql)	DB	-31.204328720698786	8.812543242382224	60654
54c774db9739287c551222986357783698ded0e4	comparison of scoring and order approach in description logic el(d)	user preferences;description logic	In this paper we study scoring and order approach to concept interpretation in description logics. Only concepts are scored/ordered, roles remain crisp. The concepts in scoring description logic are fuzzified, while the concepts in order description logic are interpreted as preorders on the domain. These description logics are used for preferential user-dependent search of the best instances. In addition to the standard constructors we add top-k retrieval and aggregation of user preferences. We analyze the relationship between scoring and order concepts and we introduce a notion of order-preserving concept constructors.	description logic	Veronika Vaneková;Peter Vojtás	2010		10.1007/978-3-642-11266-9_59	discrete mathematics;description logic;computer science;mathematics;algorithm	AI	-23.228035080213612	6.94743646427993	60670
85d542ce705eb3fcb923ca93d781d93c4a6ee061	subsky: efficient computation of skylines in subspaces	degradation;information retrieval;relational database;drives;multi dimensional;indexes;cities and towns;information retrieval computer science relational databases drives scalability indexes costs cities and towns data security degradation;relational databases;scalability;computer science;data security	Given a set of multi-dimensional points, the skyline contains the best points according to any preference function that is monotone on all axes. In practice, applications that require skyline analysis usually provide numerous candidate attributes, and various users depending on their interests may issue queries regarding different (small) subsets of the dimensions. Formally, given a relation with a large number (e.g.,ge 10) of attributes, a query aims at finding the skyline in an arbitrary subspace with a low dimensionality (e.g., 2). The existing algorithms do not support subspace skyline retrieval efficiently because they (i) require scanning the entire database at least once, or (ii) are optimized for one particular subspace but incur significant overhead for other subspaces. In this paper, we propose a technique SUBSKY which settles the problem using a single B-tree, and can be implemented in any relational database. The core of SUBSKY is a transformation that converts multi-dimensional data to 1D values, and enables several effective pruning heuristics. Extensive experiments with real data confirm that SUBSKY outperforms alternative approaches significantly in both efficiency and scalability.	algorithm;b-tree;computation;data structure;experiment;html element;heuristic (computer science);ibm notes;overhead (computing);pei-yuan wei;relational database;scalability;monotone	Yufei Tao;Xiaokui Xiao;Jian Pei	2006	22nd International Conference on Data Engineering (ICDE'06)	10.1109/ICDE.2006.149	relational database;computer science;theoretical computer science;data mining;database	DB	-29.38016802513517	4.719359054517791	60749
c2323a98888362511ef892881f927ad468190e05	a generalized mapping language for network data structures	data structure	This paper addresses itself to the problem of mapping network data structures into network data structures. There are many reasons for studying mapping functions for data bases; some of these are cataloged in 111. The reason for studying network to network maps is simply that complex data structures can naturally be represented by networks. Previous studies, such as [l], have restricted themselves to hierarchical data structures in order to avoid the ambiguity of relationships that arise when dealing with network data structures. When performing network data base retrieval[2], two elements must be specified in order to carry out the retrieval: the required data item names must be given, plus the relationships among the data items. The mapping problem differs from retrieval in that data items and relationships must be determined for both the source and target data bases. Thus in performing a mapping operation, a data base search must be carried out in both the source and target data bases for the appropriate data items. In this paper, a generalized mapping language will be defined. The language will have a linguistic formulation, in order to structurally analyze and easily implement the language constructs. The mapping language will be a high level non procedural language, so that a non-technical user of the data base system would be able to perform complex mapping operations. As a subset of the language, full retrieval capabilities for network data bases are implemented. Finally, the language is compared to other recent research on mapping.	data item;data structure;hierarchical database model;high-level programming language;map;procedural programming	Robert H. Bonczek;Andrew B. Whinston	1977	Inf. Syst.	10.1016/0306-4379(77)90006-0	natural language processing;data structure;computer science;theoretical computer science;machine learning;data mapping	DB	-28.433552370008996	10.27852743199305	60823
8d4476ab9840b92f7b4e8b6444dc82bb0aed3215	a fast and compact data structure of storing multi-attribute relations among words	time complexity;natural languages;tree data structures;data structures couplings natural languages natural language processing dictionaries speech recognition information science intelligent systems knowledge representation speech analysis;computational complexity natural languages tree data structures;computational complexity;natural language;multi attribute relation retrieval fast compact data structure multi attribute relation storage word relation natural language processing systems efficient data structure trie leaf linkage worst case time complexity;data structure;natural language processing	Word relation is primitive knowledge and it is very useful for natural language processing systems. In the traditional systems, although each knowledge dictionary is constructed separately, recent natural language applications become more complex by integrating the above multi-attribute relationships. This paper presents an efficient data structure by introducing a trie that can define the linkage among leaves. The linkage enables us to share the basic words required for multi-attribute relations. Theoretical observations show that the worst-case time complexity of retrieving multi-attribute relations is a constant. From the simulation results, it is shown that the presented method is about 1/3 smaller than that of the competitive methods.	data structure	Kazuhiro Morita;Toru Sumitomo;Masao Fuketa;Jun-ichi Aoe	1998		10.1109/ICSMC.1998.725084	natural language processing;language identification;cache language model;question answering;data structure;computer science;artificial intelligence;theoretical computer science;machine learning;sparse language;natural language;algorithm	Theory	-26.60634259748377	7.604818134340213	60851
6c514cf72a3142e8ddfac9c566f908e4f9a5c9a9	some approaches for processing sqlf nested queries		"""This article deals with query processing techniques for the SQLf language which is an extended version of SQL supporting imprecise queries interpreted in the framework of fuzzy sets. SQLf, as well as SQL, allows for the use of nested queries, in which a (fuzzy) condition involved in a select block, calls on another select block (the nested one). Two types of processing strategies for nested queries are discussed. The first one tends to take advantage of existing database management systems (DBMS) to process fuzzy queries thanks to an additional layer which is in charge of translating the initial query into a Boolean one. In this perspective, the performances obtained depend strongly on the efficiency of the underlying DBMS. The other strategy is slightly different and it is situated in the context of the design of systems involving specific algorithms for processing fuzzy queries. In this article, the focus is put on algorithms related to the generic nesting construct """"exists."""" © 1996 John Wiley & Sons, Inc."""	algorithm;database;fuzzy set;john d. wiley;nested transaction;performance;sql;sqlf;situated	Patrick Bosc	1996	Int. J. Intell. Syst.	10.1002/(SICI)1098-111X(199609)11:9%3C613::AID-INT2%3E3.0.CO;2-N	strategy;computer science;artificial intelligence;data mining;database;fuzzy set;algorithm;fuzzy control system;systems design	DB	-28.608657150205367	9.244317411711652	60884
d36c47cbbd75b67fdaedf8a7efd980487eaad928	extending relational database functionality with data inconsistency resolution support	machine learning;relational database;data model	Resolving inconsistent data is a problem of critical practical importance. Inconsistent data arises whenever an attribute takes on multiple, inconsistent, values. This may occur when a particular entity is stored multiple times in one database, or in multiple databases that are combined. We’ve developed an architecture and methodology that extends relational databases with support for systematic data inconsistency resolution. We plan to employ a probabilistic data model and a machine learning approach.	data model;machine learning;relational database	Ilya Pevzner	2003			relational model;data mining;database;relational database;semi-structured model;data model;data modeling;database design;change data capture;database model;computer science	DB	-29.724051599572867	9.673797036957847	60890
973afb58b8fefb10f434f24c508ee57aca3442d2	visualizing regions with a new split-screen view for the online tool travis		The online tool travis can synthesize a k-bounded Petri net model from a reachability graph and unfold a k-bounded Petri net to its reachability graph. Synthesis is built on the theory of regions, but where travis and other existing tools only show the synthesized model, we want to present the synthesized model as well as the calculated regions after a user has performed the synthesis procedure. In this paper, we present a new split-screen module of travis which is able to visualize regions, calculated during the synthesis procedure, as well as markings of places related to single states of the reachability graph, calculated during the unfolding procedure. Both sets, i.e. regions and markings, help to really understand the relations between the behavioral and the synthesized models at hand, as well as the underlying fundamental concepts of state-based synthesis and reachability analysis. With these new features, travis is tailored to be used in a teaching environment to help students understand the concepts and notions of state-based synthesis.	petri net;reachability;transition system;travis ci;unfolding (dsp implementation)	Benjamin Meis;Robin Bergenthum	2018			computer graphics (images);split screen;computer science	Logic	-23.419005663781952	17.23838747923637	60926
f23e6e955ae1e5ccc899a2360f1e27c706ec1b2b	the logic of adaptive behavior : knowledge representation and algorithms for the markov decision process framework in first-order domains	sequential decision making;reinforcement learning;dynamic program;adaptive behavior;general intelligence;artificial intelligent;first order;markov decision process	Learning and reasoning in large, structured, probabilistic worlds is at the heart of artificial intelligence. Markov decision processes have become the de facto standard in modeling and solving sequential decision making problems under uncertainty. Many efficient reinforcement learning and dynamic programming techniques exist that can solve such problems.#R##N#Until recently, the representational state-of-the-art in this field was based on propositional representations.#R##N##R##N#However, it is hard to imagine a truly general, intelligent system that does not conceive of the world in terms of objects and their properties and relations to other objects. To this end, this book studies lifting Markov decision processes, reinforcement learning and dynamic programming to the first-order (or, relational) setting. Based on an extensive analysis of propositional representations and techniques, a methodological translation is constructed from the propositional to the relational setting. Furthermore, this book provides a thorough and complete description of the state-of-the-art, it surveys vital, related historical developments and it contains extensive descriptions of several new model-free and model-based solution techniques.	adaptive behavior;algorithm;first-order predicate;knowledge representation and reasoning;markov chain;markov decision process	Martijn van Otterlo	2008			markov decision process;partially observable markov decision process;decision engineering;adaptive behavior;first-order logic;g factor;mathematics;reinforcement learning;algorithm	ML	-20.689143515764748	7.893665174687203	60950
f88d93f594cc40f1bb51f87a111cc5cceb19afb9	exploiting schema and documentation for summarizing relational databases		Schema summarization approaches are used for carrying out schema matching and developing user interfaces. Generating schema summary for any given database is a challenge which involves identifying semantically correlated elements in a database schema. Research efforts are being made to propose schema summarization approaches by exploiting database schema and data stored in the database. In this paper, we have made an effort to propose an efficient schema summarization approach by exploiting database schema and the database documentation. We propose a notion of table similarity by exploiting referential relationship between tables and the similarity of passages describing the corresponding tables in the database documentation. Using the notion of table similarity, we propose a clustering based approach for schema summary generation. Experimental results on a benchmark database show the effectiveness of the proposed approach.	documentation;relational database management system	Ammar Yasir;Mittapally Kumara Swamy;P. Krishna Reddy	2012		10.1007/978-3-642-35542-4_7	information schema;data mining;database;information retrieval	DB	-33.637996681327486	4.977182820652728	60973
13b5d728a09d545e74a634a811c64225147d05f2	discrete system dynamic simulation - interactive language sds	dynamic simulation;discrete system	SDS (Simulation of Dynamics of Systems) is an interactive simulation language intended for the Discrete Simulation of models that can be shown by systems of algebra formulas. These models are apstract, dynamic with a discrete modification of state and with uniformed-discrete change of time, with deterministic and stohastic connections that can be linear or nonlinear. The simulation language SDS is especially suitable for the simulation of dynamics of systems that can be described by elements of state, elements of modification of state, and flows of matter, enery and informations. The main characteristics of the language are: it is easily learned and simple for use, it has a high degree of cimilarity between the program and the simulational model, the programming is simple and it is not conditioned on the knowledge of programming technics, it assures scemantic and synthetic control, already in tite procedure of creating the program, it automatically defines the position of instruction in cue of execution, it assures, by use of external data set, the independence of program from data, it makes interactive work of more users at onle possible, it is of modular structure, so that the use of external program packages, there is a possibility of widening on behalf of the user, it demands modest computer resources, it is independent of the operating system of computers, it can be installed on most of the existing computers.	discrete system;simulation	Slobodan Andric	1983		10.1007/978-3-642-69295-6_28	dynamic simulation;simulation;theoretical computer science;distributed computing	Robotics	-24.720386748632325	17.424862644237255	61152
f3157bec22661539500d14e15e57c5f1e105ba51	construction of deterministic transition graphs from dynamic integrity constraints	database system;bottom up;temporal logic;dynamic behaviour;integrity constraints	Here systems, in particular database systems, are considered whose dynamic behaviour is characterized by state sequences that evolve stepwise, and whose integrity constraints are specified by means of temporal logic. Monitoring temporal formulae in state sequences can be reduced to following paths in transition graphs by only checking nontemporal edge labels in each state. This paper presents an algorithm how to construct deterministic transition graphs from temporal formulae in a bottom-up way corresponding to the formula structure. These graphs ensure at least provisional admissibility of system behaviour up to a present state and at most potential admissibility of future behaviour. Moreover, deterministic graphs have considerable advantages over general transition graphs.	data integrity;ptc integrity	Udo W. Lipeck;Dasu Feng	1988		10.1007/3-540-50728-0_41	real-time computing;temporal logic;computer science;top-down and bottom-up design;data integrity;data mining;database	DB	-23.486742895056043	15.849070485582693	61158
7bf913fe98e993efd0d107b9a02f45b8fdcbdf9c	a verification framework for agent programming with declarative goals	i 2 4;declarative goals;f 3 2;beliefs;i 2 5;agents;f 3 1;programming logic;agent programming;agent programming language	A long and lasting problem in agent research has been to close the gap between agent logics and agent programming fr The main reason for this problem of establishing a link between agent logics and agent programming frameworks is iden explained by the fact that agent programming frameworks have hardly incorporated the concept of a declarative goal. Instead, such frameworks have focused mainly on plans or g als-to-do instead of the end goals to be realised which are also called goals-to-be. In this paper, the programming language GOAL is introduced which incorporates such declarative goals. The notion of a commitment strategy—one of the main theoretical insights due to agent logics, which explains the relation between beliefs and goals— construct a computational semantics for GOAL. Finally, a proof theory for proving properties of GOAL agents is introduce the main contribution of this paper, rather than the language GOAL itself, is that we offer a complete theory of agent prog in the sense that our theory provides both for a programming framework and a programming logic for such agents. An program is proven correct by using this programming logic.  2005 Elsevier B.V. All rights reserved. MSC: F.3.1; F.3.2; I.2.5; I.2.4	3apl;agent-oriented programming;computational semantics;computer programming;concurrent computing;correctness (computer science);formal specification;intelligent agent;jal (compiler);java;naruto shippuden: clash of ninja revolution 3;natural mapping (interface design);operational semantics;programming language;programming paradigm;semantics (computer science);temporal logic;test plan;theory;viz: the computer game;way to go	Frank S. de Boer;Koen V. Hindriks;Wiebe van der Hoek;John-Jules Ch. Meyer	2007	J. Applied Logic	10.1016/j.jal.2005.12.014	constraint programming;declarative programming;programming domain;epistemology;reactive programming;computer science;knowledge management;artificial intelligence;extensible programming;software agent;belief;functional logic programming;programming paradigm;procedural programming;symbolic programming;inductive programming;fifth-generation programming language;programming language theory;programming language;logic programming;algorithm;concurrent object-oriented programming	AI	-23.508644047298635	17.887096240623233	61165
71f9d0a975c393f0f409b4399af6217198aaed14	exploiting lineage for confidence computation in uncertain and probabilistic databases	databases;trio prototype database system;batch algorithms;database system;probability;query processing;uncertainty;prototypes;uncertain probabilistic databases;confidence computation algorithms;space exploration;relational database;data mining;probabilistic database;large synthetic dataset confidence computation algorithms uncertain probabilistic databases relational databases query processing batch algorithms trio prototype database system;computational modeling;large synthetic dataset;database systems;relational databases probability query processing;inference algorithms;relational databases;scalability;computational efficiency;relational databases query processing database systems space exploration uncertainty prototypes scalability cleaning data mining;cleaning;data models	We study the problem of computing query results with confidence values in ULDBs: relational databases with uncertainty and lineage. ULDBs, which subsume probabilistic databases, offer an alternative decoupled method of computing confidence values: Instead of computing confidences during query processing, compute them afterwards based on lineage. This approach enables a wider space of query plans, and it permits selective computations when not all confidence values are needed. This paper develops a suite of algorithms and optimizations for a broad class of relational queries on ULDBs. We provide confidence computation algorithms for single data items, as well as efficient batch algorithms to compute confidences for an entire relation or database. All algorithms incorporate memoization to avoid redundant computations, and they have been implemented in the Trio prototype ULDB database system. Performance characteristics and scalability of the algorithms are demonstrated through experimental results over a large synthetic dataset.	algorithm;computation;lineage (evolution);memoization;probabilistic database;prototype;query plan;relational database;scalability;synthetic intelligence	Anish Das Sarma;Martin Theobald;Jennifer Widom	2008	2008 IEEE 24th International Conference on Data Engineering	10.1109/ICDE.2008.4497511	relational database;computer science;theoretical computer science;data mining;database	DB	-26.015812409143287	5.1458951562274065	61172
7ea5deb508ab72e994011c69c07c36af038fe138	fuzzy orderings in flexible query answering systems	fuzzy orderings;vague query system;flexible query answering systems;relational databases;query answering	This paper describes the benefits of using fuzzy orderings in flexible query answering systems. We provide a brief overview of those results from the theory of fuzzy orderings that are necessary to couple fuzzy orderings with flexible querying in a meaningful synergistic way. As one case study, we discuss a simple and pragmatic variant of a flexible query answering system – the so-called Vague query system (VQS). The integration of fuzzy orderings into that system is provided in full detail along with examples.		Ulrich Bodenhofer;Josef Küng	2004	Soft Comput.	10.1007/s00500-003-0308-9	sargable;query optimization;relational database;computer science;web search query;query language	Robotics	-28.34180874488223	9.48039234250556	61323
a64e2ea8c51ecae6fa8ba66dbb19565171ecdad0	pbitree coding and efficient processing of containment joins	xml documents;xml tree data structures query processing;query processing framework;query processing;pbitree coding;data engineering;tree data structures;ancestor descendant relationship;indexation;tree structure;xml;xml document;query processing framework pbitree coding containment join processing algorithm tree structured data xml documents xml node elements ancestor descendant relationship;containment join processing algorithm;tree structured data;xml node elements	This paper addresses issues related to containment join processing in tree-structured data such as XML documents. A containment join takes two sets of XML node elements as input and returns pairs of elements such that the containment relationship holds between them. While there are previous algorithms for processing containment joins, they require both element sets either sorted or indexed. This paper proposes a novel and complete containment query processing framework based on a new coding scheme, PBiTree code. The PBiTree code allows us to determine the ancestor-descendant relationship between two elements from their PBiTree-based codes efficiently. We present algorithms in the framework that are optimized for various combinations of settings. In particular, the newly proposed partitioning based algorithms can process containment joins efficiently without sorting or indexes. Experimental results indicate that the containment join processing algorithms based on the proposed coding scheme outperform existing algorithms significantly.	algorithm;binary space partitioning;code;database;experiment;join (sql);mathematical optimization;query optimization;regional lockout;sorting;xml	Wei Wang;Haifeng Jiang;Hongjun Lu;Jeffrey Xu Yu	2003		10.1109/ICDE.2003.1260808	xml;information engineering;computer science;theoretical computer science;data mining;database;programming language	DB	-29.557884598616734	4.847043372623464	61553
29bc92af997760952c13e69bf411763782b2793c	inference-proof view update transactions with forwarded refreshments	inference proof;confidentiality policy;controlled query evaluation;transaction;a priori knowledge;inference control;reversibility;view update;propositional logic;lying approach;integrity constraint;view refreshment;information system;security;controlled execution	More specifically, within the framework of the lying approach to CQE, we study how the server should translate a view update request issued by a client into a new database state in an inference-proof way. In order to avoid dangerous inferences, some such updates have to be denied even though the new database instance would be compatible with the set of integrity constraints declared in the schema and supposed to be known to the client. In contrast, seen from the client's point of view some other updates leading to an incompatible instance should not be denied.	data integrity;database;server (computing)	Joachim Biskup;Christian Gogolin;Jens Seiler;Torben Weibert	2011	Journal of Computer Security	10.3233/JCS-2011-0420	a priori and a posteriori;computer science;information security;data integrity;data mining;database;propositional calculus;computer security;information system	DB	-26.777055552918565	12.75981559762679	61581
35bfe09959319a80a4f244ae44da3685b87eb6e7	observations on the odmg-93 proposal	odmg-93 proposal;object-oriented database language	Despite its hasty claim, it is NOT a “standard”. Rather, it is a work-in-progress proposal for an object-oriented database language and language bindings to it for C++ and Smalltalk. ODMG (Object-Oriented Database Management Group) is not a formal “standards” body. It is a committee formed by five vendors of first-generation object-oriented database systems (OODB). (For several years, some of these vendors have offered products that are not much more than persistent storage managers for object-oriented programming languages, but the misleading label “Object-Oriented Database System” has been stuck on such products in the market.) And now the misleading label “standard” seems to be attached to the ODMG-93 work-in-progress proposals.	c++;database;object data management group;persistence (computer science);programming language;query language;smalltalk	Won Young Kim	1994	SIGMOD Record	10.1145/181550.181552	data definition language;data mining;database;database schema;information retrieval;database design	DB	-30.715186695385437	11.389738030910408	61827
c7214268f5eb3c1626b313e0c5bfc16d6aeea34a	engineering aggregation operators for relational in-memory database systems			in-memory database;relational database management system	Ingo Müller	2016			database theory;relational model;entity–relationship model;relational database;knowledge management;database model;data mining;database;object-relational impedance mismatch;database design	DB	-31.150135316737828	9.22754779347534	61832
2cb453f1c9c5d5ace01c32c10d3a7a951153de42	properties of weak conditional independence	bayes estimation;base relacional dato;conditional independence;bayesian network;probabilidad condicional;probabilite conditionnelle;dependance multivaluee;complete axiomatization;intelligence artificielle;relational database;probabilistic approach;dependencia multivaluada;reseau bayes;estimacion bayes;multivalued dependency;red bayes;object oriented;enfoque probabilista;approche probabiliste;necessary and sufficient condition;bayes network;base donnee relationnelle;oriente objet;artificial intelligence;inteligencia artificial;conditional probability;orientado objeto;estimation bayes	Object-oriented Bayesian networks (OOBNs) facilitate the design of large Bayesian networks by allowing Bayesian networks to be nested inside of one another. Weak conditional independence has been shown to be a necessary and sufficient condition for ensuring consistency in OOBNs. Since weak conditional independence plays such an important role in OOBNs, in this paper we establish two useful results relating weak conditional independence with weak multivalued dependency in relational databases. The first result strengthens a previous result relating conditional independence and multivalued dependency. The second result takes a step towards showing that the complete axiomatization for weak multivalued dependency is also complete for full weak conditional independence.	axiomatic system;bayesian network;multivalued dependency;relational database	Cory J. Butz;Manon J. Sanscartier	2002		10.1007/3-540-45813-1_46	econometrics;computer science;machine learning;pattern recognition;bayesian network;mathematics;statistics	AI	-22.003029206072696	11.816778066998099	61888
5d46a746bb89f34a71da235a408a12784a4b8544	query containment for highly expressive datalog fragments		The containment problem of Datalog queries is well known to be undecidable. There are, however, several Datalog fragments for which containment is known to be decidable, most notably monadic Datalog and several “regular” query languages on graphs. Monadically Defined Queries (MQs) have been introduced recently as a joint generalization of these query languages. In this paper, we study a wide range of Datalog fragments with decidable query containment and determine exact complexity results for this problem. We generalize MQs to (Frontier-)Guarded Queries (GQs), and show that the containment problem is 3ExpTime-complete in either case, even if we allow arbitrary Datalog in the sub-query. If we focus on graph query languages, i.e., fragments of linear Datalog, then this complexity is reduced to 2ExpSpace. We also consider nested queries, which gain further expressivity by using predicates that are defined by inner queries. We show that nesting leads to an exponentially increasing hierarchy for the complexity of query containment, both in the linear and in the general case. Our results settle open problems for (nested) MQs, and they paint a comprehensive picture of the state of the art in Datalog query containment.	datalog;monadic predicate calculus;query language;undecidable problem	Pierre Bourhis;Markus Krötzsch;Sebastian Rudolph	2014	CoRR		database;datalog;programming language;algorithm	DB	-24.16189752331724	10.781753657497847	62099
80f253025f837d279f52340f08cdf8e17c877ef8	generalizing possibility-distribution-fuzzy-relational-models	databases;semantic ambiguity;imprecise attribute value;membership attribute values;relation algebra;database theory fuzzy set theory relational algebra possibility theory generalisation artificial intelligence uncertainty handling;query processing;information science;fuzzy sets fuzzy set theory cities and towns information science educational institutions algebra marine vehicles query processing databases intelligent systems;fuzzy relation;uncertainty handling;fuzzy relational model;imprecise attribute value model generalization possibility distribution fuzzy relational model semantic ambiguity membership attribute values extended relational algebra attribute properties;fuzzy set theory;fuzzy sets;extended relational algebra;attribute properties;marine vehicles;algebra;relational model;intelligent systems;cities and towns;possibility theory;generalisation artificial intelligence;model generalization;database theory;relational algebra;possibility distribution	A genemlized possibility-distribution-fuzzy-relationalmodel is proposed considering semantic ambiguity for membership attribute values and ambiguity contained in them. And then the extended relational algebra is shown. In order to eliminate the semantic ambiguity, a membership attribute is attached to each attribute. This clarifies the origin of membership attribute values. What a membership attribute value means depends on the property of the attribute. In order to eliminate ambiguity contained an membership attribute values, those values are expressed b y possibility distributions. This clarifies what effects an imprecise attribute value has on its membership attribute value. Therefore, there is no semantic ambiguity for the membership attribute values and no ambiguity in them in the extended relational model.	relational algebra;relational model	Mitsuru Nakata	1999		10.1109/KES.1999.820238	discrete mathematics;variable and attribute;attribute domain;relation;machine learning;data mining;mathematics	DB	-27.389685265685106	8.658993644598795	62187
242e110f1b022e9bdc132a5d7a706b11c3c70998	types and classes of machine learning and data mining	data mining;functional programming;machine learning;classes;inductive inference;data types	The notion of a statistical model, as inferred and used in statistics, machine learning and data mining, is examined from a semantic point of view. Data types and type-classes for models are developed that allow models to be manipulated in a type-safe yet flexible way. The programming language Haskell-98, with its system of polymorphic types and type-classes, is used as the meta-language for this exercise so one of the by-products is a running program.	data mining;machine learning	Lloyd Allison	2003			algorithmic learning theory;computer science;online machine learning;machine learning;pattern recognition;data mining;data pre-processing;data stream mining;inductive programming;abstract data type;active learning	ML	-19.505854144807973	11.752863602824773	62385
89de5253547da844d2470b13b912c3fe7daeda8a	incremental maintenance of materialized xml views	incremental maintenance;view function;xquery view function;new view value;entire database state;old view;new view;entire view;xml view;underlying database;view schema	We investigate the problem of incremental maintenance of ma terialized XML views. We are considering the case where the under lying database is a relational database and the view exposed to querying is a materialized XML view. Then, updates to the underlyi ng database should be reflected to the stored XML view, to keep it consistent with the source data, without recreating the entire view from the database after each source update. Unlike related work t hat uses algebraic methods, we use source-to-source compositional transformations, guided by the database and view schemata. We firs t translate SQL updates to pure (update-free) XQuery express ions that reconstruct the entire database state, reflecting the u pdated values in the new state. Then, we synthesize the right-inverse o f the XQuery view function, guided by the view schema. This invers e function is applied to the old view to derive the old database state, which in turn is mapped to the new database state through the u pdate function, and then is mapped to the new view through the v iew function. The resulting view-to-view function is normaliz ed and translated to XQuery updates that destructively modify the materialized view efficiently to reflect the new view values.	linear algebra;materialized view;relational database;sql;source data;xml;xquery	Leonidas Fegaras	2011		10.1007/978-3-642-23091-2_2	materialized view;computer science;data mining;xml database;database;view;database schema;information retrieval	DB	-29.499188188867397	10.441601958582199	62418
00f5fe51228b08f84c9f66222fff6974e11c908d	a note on the translation of conceptual data models into description logics: disjointness and covering assumptions	covering;disjointness;conceptual modeling;description logics;uml;dls;presentation;orm;conceptual data models;er;unified modeling language;non monotonic reasoning	Conceptual modeling is nowadays mostly done using languages such as Entity-Relationship (ER) Models, Unified Modeling Language (UML), and Object-Role Modeling (ORM). These models are used to depict the ontological organization of relevant concepts or entities. Such formalisms share a common modeling approach, based on the notions of class or entity and the relations or associations between classes or entities. Recent developments in knowledge representation using logic-based ontologies have created new possibilities for conceptual data modeling. It also raises the question of how existing conceptual models using ER, UML or ORM could be translated into Description Logics (DLs), a family of logics that have proved to be particularly appropriate for formalizing ontologies and reasoning about them. Given a conceptual data model, two assumptions are usually made that are not explicitly stated but need to be clarified for its DL translation: (1) disjointness assumption: all the classes are to be assumed pairwise disjoint if not specified otherwise; and (2) covering assumption: the content of every class must correspond to the union of its immediate subclasses (this includes the assumption that we do not consider anything apart from what is expressed in the model). In this paper we propose two simple procedures to assist modelers with integrating these assumptions into their models, thereby allowing for a more complete translation into DLs.	class diagram;conceptual schema;data model;data modeling;description logic;entity;entity–relationship model;knowledge representation and reasoning;object-role modeling;ontology (information science);unified modeling language	Giovanni Casini;Aurona Gerber;Thomas Andreas Meyer	2012		10.1145/2389836.2389839	conceptual model;theoretical computer science;data mining;mathematics;algorithm	AI	-23.003819490659566	9.46696470995046	62492
32027ff31bb6ad0a7fd14c5907138479f0a4a00d	view selection under multiple resource constraints in a distributed context		The use of materialized views in commercial database systems and data warehousing systems is a common technique to improve the query performance. In past research, the view selection issue has essentially been investigated in the centralized context. In this paper, we address the view selection problem in a distributed scenario. We rst extend the AND-OR view graph to capture the distributed features. Then, we propose a solution using constraint programming for modeling and solving the view selection problem under multiple resource constraints in a distributed context. Finally, we experimentally show that our approach provides better performance resulting from evaluating the quality of the solutions in terms of cost saving.	centralized computing;constraint programming;database;experiment;genetic algorithm;heuristic (computer science);ibm tivoli storage productivity center;materialized view;run time (program lifecycle phase);selection algorithm;telecommunications network	Imene Mami;Zohra Bellahsene;Remi Coletta	2012		10.1007/978-3-642-32597-7_25	computer science;theoretical computer science;data mining;database	DB	-26.45860749511038	4.644338987151122	62512
f0a1e8ace19ad662bc764654468383cc64e924d5	updating legacy databases through wrappers: data consistency management	web based applications;data integrity;operational case tool legacy databases data consistency management wrapping databases web based applications federated systems data wrappers updating wrappers legacy data access neutral interface relational interface object oriented interface xml interface interface referential integrity control wrapper architecture transformational paradigm wrappers code;cobol;software maintenance;database management systems;xml database management systems data integrity software maintenance software architecture computer aided software engineering;reverse engineering wrapping relational databases data systems data models object oriented databases xml computer aided software engineering database languages scattering;software architecture;computer aided software engineering;object oriented;integrated control;case tool;xml;schema mapping;data consistency	Wrapping databases allows them to be reused in earlier unforeseen contexts, such as Web-based applications or federated systems. Data wrappers, and more specifically updating wrappers (that not only access legacy data but also update them), can provide external clients of an existing (legacy) database with a neutral interface and augmented capabilities. For instance, a collection of COBOL files can be wrapped in order to allow external application programs to access them through a relational, object-oriented or XML interface, while providing referential integrity control. We explore the principles of a wrapper architecture that addresses the problems of legacy data consistency management. The transformational paradigm is used as a rigorous formalism to define schema mappings as well as to generate as much as possible of the code of the wrappers. The generation is supported by an operational CASE tool.	cobol;code generation (compiler);computer-aided software engineering;data model;database;emulator;federation (information technology);programming paradigm;referential integrity;semantics (computer science);system administrator;web application;world wide web;wrapping (graphics);xml	Philippe Thiran;Geert-Jan Houben;Jean-Luc Hainaut;Djamal Benslimane	2004	11th Working Conference on Reverse Engineering	10.1109/WCRE.2004.41	software architecture;xml;computer science;operating system;software engineering;data integrity;data mining;database;cobol;programming language;data consistency;software maintenance;computer-aided software engineering	DB	-32.982083358473595	12.248863886572641	62803
50bda7947278c5e8b326241dfe4d16faa1823f85	conceptual models for spatio-temporal applications	conceptual model	Improved support for modeling information systems involving time-varying, georeferenced information, termed spatio-temporal information, has been a longterm user requirement in a variety of areas, such as cadastral systems that capture the histories of landparcels, routing systems computing possible routes of vehicles, and weather forecasting systems. This chapter concerns the conceptual database design phase for such spatio-temporal information systems and presents two models, namely the spatio-temporal Entity Relationship (ER) Model and the Extended spatio-temporal Unified Modeling Language (UML) as proposed in [33,34] and [26], respectively. The conceptual design phase focuses on expressing application requirements without the use of computer metaphors. The design should be understandable to the user and complete, so that it can be translated into the logical phase that follows without any further user input. Popular conceptual models include the ER model [6], IFO [2], OMT [30], and UML [17]. For conventional administrative systems, exemplified by the “supplier-supplies-parts” paradigm, the available modeling notations and techniques that support the conceptual and logical modeling phases are mature and adequate. However, this is not the case in non-standard systems managing spatio-temporal, multimedia, VLSI, image, and voice data. Rather, these lead to new and unmet requirements for modeling techniques. The basic rationale behind the work presented here is to introduce new modeling techniques, based on minimal extensions of existing models, developed to accommodate the peculiarities of the combined spatial and temporal information. The ER Model and the UML have been extended as prototypical examples for this purpose. First, we present the fundamental aspects of the spatio-temporal domain, covering concepts such as objects, properties, and relationships. Based on these, in the ER approach, we present a small set of constructs aimed at improving the ability to conveniently model spatio-temporal information at the conceptual level. These constructs may be included in a wide range of existing conceptual data models, improving their modeling capabilities without fundamentally changing the models. We incorporate the proposed modeling constructs into the ER model ([5]), resulting in the semantically richer Spatio-Temporal ER (STER) model [33].	conceptual schema;data model;database design;design rationale;entity–relationship model;erdős–rényi model;information system;programming paradigm;requirement;routing;spatiotemporal database;unified modeling language;user requirements document;very-large-scale integration	Nectaria Tryfona;Rosanne Price;Christian S. Jensen	2003		10.1007/978-3-540-45081-8_3	conceptual model;conceptual model;conceptual system;domain model	DB	-32.02062336924034	13.442378654318485	63442
0fc705d6d67cd69e6d7ef17d05f636b0c4bb3aeb	an abductive framework for information exchange in multi-agent systems	abduction;intercambio informacion;multiagent system;multi agent system;dining philosopher;simultaneidad informatica;intelligence artificielle;logical programming;local knowledge;concurrency;abduccion;programmation logique;echange information;information exchange;artificial intelligence;inteligencia artificial;diner philosophe;sistema multiagente;programacion logica;simultaneite informatique;cena filosofica;systeme multiagent	In this paper, we propose a framework for information exchange among abductive agents whose local knowledge bases are enlarged with a set of abduced hypotheses. We integrate the aspects of information exchange and abductive reasoning, and show theoretically the information inferred by the single abductive agent as a product of joint reasoning activity. We show examples, like dining philosophers, resource exchange and speculative computation, and give an implementation of the space of interactions based on CLP (SET ).	abductive reasoning;computation;dining philosophers problem;information exchange;interaction;speculative execution	Marco Gavanelli;Evelina Lamma;Paola Mello;Paolo Torroni	2004		10.1007/978-3-540-30200-1_3	information exchange;concurrency;computer science;artificial intelligence;machine learning;multi-agent system;database;operations research;algorithm;abductive logic programming	AI	-20.98282514399621	11.2743278997582	63585
7b2f824dec66f4857ccda8adaef29985dfc8116a	entity-relationship and object-oriented model automatic clustering	base relacional dato;agregacion;entity relationship model;cluster algorithm;design tool;object oriented model;concepcion sistema;conceptual database design;semantics;modelo entidad relacion;conceptual clustering;relational database;modele entite relation;semantica;semantique;aggregation;semantic model;conceptual schema;semantic distance;object oriented;clustering;system design;base donnee relationnelle;agregation;oriente objet;semantic distances;database design;orientado objeto;conception systeme;entity relationship;object model	Automatic clustering of semantic models allows a multilevel abstraction of the same reality and reduces design complexity. This paper is concerned with the development and application of automatic clustering of Entity-Relationship (E-R) diagrams and object models. This paper does not consider physical clustering which aims at improving performance of databases but deals with conceptual clustering whose objective is to facilitate the understanding of the database schema. We provide an automatization of conceptual schema clustering leading to a unification of past approaches. Automatization is achieved through the definition of semantic distances between concepts and the use of a clustering algorithm. For entity-relationship clustering, we define three different distances (visual, hierarchical and cohesive) depending on the semantic richness. Object model clustering is based on structural, semantic and communication characteristics of objects. Our approach has been implemented and applied to a great number of examples, leading to interesting results. Applications of automatic clustering in several areas and their potential as a communication, documentation and design tools for E-R diagrams and object oriented models are described and discussed.	entity–relationship model	Jacky Akoka;Isabelle Comyn-Wattiau	1996	Data Knowl. Eng.	10.1016/S0169-023X(96)00007-9	correlation clustering;constrained clustering;document clustering;fuzzy clustering;entity–relationship model;flame clustering;computer science;artificial intelligence;consensus clustering;data mining;database;semantics;cluster analysis;brown clustering;conceptual clustering	PL	-30.32595713086239	13.275529675605233	63809
bce5d10e0b5aeea38f6ea5ae42538fa33beef623	adding integrity constraints to owl.	relational database;satisfiability;integrity constraints	Schema statements in OWL are interpreted quite differently from analogous statements in relational databases. If these statements are meant to be interpreted as integrity constraints (ICs), OWL’s interpretation may seem confusing and/or inappropriate. Therefore, we propose an extension of OWL with ICs that captures the intuition behind ICs in relational databases. We show that, if the constraints are satisfied, we can disregard them while answering a broad range of positive queries.	abox;constraint satisfaction;data integrity;ptc integrity;peano axioms;relational database;semantic reasoner;tbox;web ontology language	Boris Motik;Ian Horrocks;Ulrike Sattler	2007			relational database;computer science;data integrity;data mining;database;programming language;satisfiability	Web+IR	-25.230215285858264	9.822398947579899	63841
79b707caf49afdef9face729e15d89855e57c6a8	on the semantics of linking and importing in modular ontologies	theorie locale;representacion conocimientos;ontologie;local theory;correspondance ontologie;ontology mapping;web semantique;logica descripcion;semantics;logical programming;semantica;semantique;analyse syntaxique;lenguaje descripcion;programmation logique;analisis sintaxico;web semantica;syntactic analysis;logique ordre 1;representation connaissance;semantic web;ontologia;teoria local;description logic;knowledge representation;programacion logica;ontology;correspondencia ontologia;first order logic;langage description;logique description;logica orden 1;description language	"""Modular ontology languages, such as Distributed Description Logics (DDL), E-connections and Packagebased Description Logics (P-DL) offer two broad classes of approaches to connect multiple ontology modules: the use of mappings or linkings between ontology modules e.g., DDL and E-connections; and the use of importing e.g., P-DL. The major difference between the two approaches is on the usage of """"foreign terms"""" at the syntactic level, and on the local model disjointness at the semantic level. We compare the semantics of linking in DDL and E-connections, and importing in P-DL within the Distributed First Order Logics (DFOL) framework. Our investigation shows that the domain disjointness assumption adopted by the linking approach leads to several semantic di�culties. We explore the possibility of avoiding some of these difficulties using the importing approach to linking ontology modules. Disciplines Artificial Intelligence and Robotics This article is available at Iowa State University Digital Repository: http://lib.dr.iastate.edu/cs_techreports/246 On the Semantics of Linking and Importing in Modular Ontologies ? Jie Bao, Doina Caragea, Vasant G Honavar Artificial Intelligence Research Laboratory, Department of Computer Science, Iowa State University, Ames, IA 50011-1040, USA Department of Computing and Information Sciences Kansas State University, Manhattan, KS 66506, USA 1{baojie, honavar}@cs.iastate.edu, dcaragea@ksu.edu Abstract. Modular ontology languages, such as Distributed Description Logics (DDL), E-connections and Package-based Description Logics (P-DL) offer two broad classes of approaches to connect multiple ontology modules: the use of mappings or linkings between ontology modules e.g., DDL and E-connections; and the use of importing e.g., P-DL. The major difference between the two approaches is on the usage of “foreign terms” at the syntactic level, and on the local model disjointness at the semantic level. We compare the semantics of linking in DDL and E-connections, and importing in P-DL within the Distributed First Order Logics (DFOL) framework. Our investigation shows that the domain disjointness assumption adopted by the linking approach leads to several semantic difficulties. We explore the possibility of avoiding some of these difficulties using the importing approach to linking ontology modules. Modular ontology languages, such as Distributed Description Logics (DDL), E-connections and Package-based Description Logics (P-DL) offer two broad classes of approaches to connect multiple ontology modules: the use of mappings or linkings between ontology modules e.g., DDL and E-connections; and the use of importing e.g., P-DL. The major difference between the two approaches is on the usage of “foreign terms” at the syntactic level, and on the local model disjointness at the semantic level. We compare the semantics of linking in DDL and E-connections, and importing in P-DL within the Distributed First Order Logics (DFOL) framework. Our investigation shows that the domain disjointness assumption adopted by the linking approach leads to several semantic difficulties. We explore the possibility of avoiding some of these difficulties using the importing approach to linking ontology modules."""	artificial intelligence;computer science;description logic;liang-jie zhang;ontology (information science);robotics	Jie Bao;Doina Caragea;Vasant Honavar	2006		10.1007/11926078_6	natural language processing;description logic;semantic integration;computer science;artificial intelligence;parsing;semantic web;ontology;first-order logic;mathematics;semantics;algorithm	AI	-21.67865140352424	10.152792502413568	63892
5f060ace3945580614a463e05a049b1c737dfc4c	modelling vague content and structure querying in xml retrieval with a probabilistic object-relational framework	busqueda informacion;document structure;base donnee;estructura documental;information retrieval;structure document;xml language;interrogation base donnee;database;interrogacion base datos;base dato;probabilistic approach;xml retrieval;hierarchical classification;recherche information;enfoque probabilista;approche probabiliste;classification hierarchique;clasificacion jerarquizada;database query;object relational;langage xml;lenguaje xml	Many XML retrieval applications require relevance-oriented ranking of retrieved elements in order to capture the vagueness inherent to the information retrieval process. This relevance-oriented ranking should not only support vagueness at the content level, but also at the structural level. In this paper, we use a probabilistic object-relational framework to model representation and retrieval strategies that take into account vagueness at both content and structure level. Our approach makes use of established database technology combined with sound probability theory, thus allowing for fast and flexible prototyping of various representation and retrieval strategies.	information retrieval;microsoft outlook for mac;object-relational database;query language;relevance;requirement;stemming;user (computing);vagueness;xml retrieval	Mounia Lalmas;Thomas Roelleke	2004		10.1007/978-3-540-25957-2_34	xml;computer science;document structure description;data mining;database;information retrieval;divergence-from-randomness model	Web+IR	-32.953254730497484	7.146853588467794	64015
f8972125b6293c21ed8c313ad52490fb241ae508	bips: a layered predicative query language for a dbtg database system	query language;database system		data base task group;query language	Amílcar Sernadas;Graça Gaspar;José Granado	1982			web search query;data control language;query language;data definition language;rdf query language;database;query by example;query optimization;view;computer science	DB	-31.234919322397833	9.01766609201878	64147
146cd3b756493d53bde34e6d8c83c9df7b4354cb	spatial databases - accomplishments and research needs	database indexing;databases;developpement logiciel;distributed system;query language;architecture systeme;systeme reparti;research needs;systeme information geographique;geographic information system;spatial data;query processing;multidimensional;reseau ordinateur;interrogation base donnee;data management;multimedia information systems spatial databases data management data analysis geographic information systems space spatial data types operators spatial query languages spatial query processing spatial indexes clustering techniques network data field data spatial data management data warehouses;interrogacion base datos;spatial index;ingenieria logiciel;spatial data type;multimedia information system;software engineering;lenguaje interrogacion;spatial database;computer network;query languages;tipificacion;sistema repartido;typing;object oriented;geographic information systems;desarrollo logicial;spatial databases;software development;multimedia databases;typage;red ordenador;genie logiciel;oriente objet;field data;arquitectura sistema;spatial data structures;langage interrogation;systeme gestion base donnee;information system;data warehouses;data warehouse;system architecture;spatial databases data analysis information analysis geographic information systems taxonomy database languages spatial indexes query processing costs load modeling;sistema gestion base datos;orientado objeto;database management system;spatial data structures visual databases geographic information systems query languages query processing data warehouses multimedia databases database indexing;database query;object relational;systeme information;sistema informacion geografica;cost model;sistema informacion;visual databases	Spatial databases, addressing the growing data management and analysis needs of spatial applications such as Geographic Information Systems, have been an active area of research for more than two decades. This research has produced a taxonomy of models for space, spatial data types and operators, spatial query languages and processing strategies, as well as spatial indexes and clustering techniques. However, more research is needed to improve support for network and field data, as well as query processing (e.g., cost models, bulk load). Another important need is to apply spatial data management accomplishments to newer applications, such as data warehouses and multimedia information systems. The objective of this paper is to identify recent accomplishments and associated research needs of the near term.	cluster analysis;geographic information system;query language;spatial analysis;spatial database;spatial query;taxonomy (general)	Shashi Shekhar;Sanjay Chawla;Sivakumar Ravada;Andrew Fetterer;Xuan Liu;Chang-Tien Lu	1999	IEEE Trans. Knowl. Data Eng.	10.1109/69.755614	spatial data infrastructure;computer science;data warehouse;spatial network analysis software;data mining;database;spatial database;information retrieval;query language	DB	-29.384503867829853	8.476189602828311	64150
c32921c53f7b084789f724a6860d6005df9a3602	a decidable very expressive n-ary description logic for database applications (extended abstract)		We introduce DLR ,̀ an extension of the n-ary propositionally closed description logic DLR to deal with attribute-labelled tuples (generalising the positional notation), projections of relations, and global and local objectification of relations, able to express inclusion, functional, key, and external uniqueness dependencies. The logic is equipped with both TBox and ABox axioms forming a DLR` knowledge base (KB). We show how a simple syntactic restriction on the appearance of projections sharing common attributes in the KB makes reasoning in the language decidable with the same computational complexity as DLR. The obtained DLR ̆ n-ary description logic is able to encode more thoroughly conceptual data models such as EER, UML, and ORM.	abox;computational complexity theory;conceptual schema;data model;description logic;dynamic language runtime;encode;enhanced entity–relationship model;knowledge base;object-relational mapping;propositional calculus;tbox;unified modeling language	Alessandro Artale;Enrico Franconi;Rafael Peñaloza;Francesco Sportelli	2017			discrete mathematics;description logic;decidability;mathematics	AI	-22.833909664947782	9.987008283018067	64306
cc3721bc043b7b4bd0a1627e0741eec19e13ee85	a review of approaches for representing rcc8 in owl	rcc8;owl;expressive power;boolean operation;semantic web;knowledge representation;spatial knowledge representation	This paper investigates several approaches for qualitative spatial knowledge representation on the Semantic Web, by using RCC8 relations. We discuss several issues arising when representing RCC8 in OWL DL, e.g., the lack of required features like role reflexivity, role Boolean operators, and role inclusion axioms. We conclude that, although some of these features are to be included in the new version of the OWL standard, OWL 2, this language still lacks the expressive power to support role negations, conjunctions, and disjunctions, and complex role inclusion axioms.	knowledge representation and reasoning;logical connective;region connection calculus;semantic web;web ontology language	Frederik Hogenboom;Flavius Frasincar;Uzay Kaymak	2010		10.1145/1774088.1774396	natural language processing;knowledge representation and reasoning;semantic web rule language;computer science;artificial intelligence;semantic web;expressive power;algorithm	AI	-21.168291401025954	8.81123539818768	64423
d33feb0daa252ad279dc74449860d14f17cbe61a	the backchase revisited	conjunctive queries;chase;core;semantic query optimization;constraints;backchase	Semantic query optimization is the process of finding equivalent rewritings of an input query given constraints that hold in a database instance. In this paper, we report about a Chase & Backchase (C&B) algorithm strategy that generalizes and improves on well-known methods in the field. The implementation of our approach, the Pegasussystem, outperforms existing C&B systems an average by two orders of magnitude. This gain in performance is due to a combination of novel methods that lower the complexity in practical situations significantly.	algorithm;chase (algorithm);complexity;database;mathematical optimization;query optimization;semantic query	Michael Meier	2013	The VLDB Journal	10.1007/s00778-013-0333-y	core;query optimization;boolean conjunctive query;computer science;theoretical computer science;chase;data mining;database;conjunctive query	DB	-24.701599576998337	8.062436024840684	64455
3a195aba87ffe6f1e76c2bc10568734ce9142d09	database slicing on relational databases			relational database management system	Dávid Tengeri;Ferenc Havasi	2014	Acta Cybern.	10.14232/actacyb.21.4.2014.6	relational database management system;relational model;mathematics;database schema;database;relational database;database design;database model;database theory;spatiotemporal database	DB	-31.100444894690273	9.046929514905043	64509
9f1023349166c606fa0f976cc0ac8c37410a0a76	a summary of user experience with the sql data sublanguage	user experience		sql;sublanguage;user experience	Donald D. Chamberlin	1980			sql;data mining;sublanguage;database;user experience design;computer science	OS	-32.53514883198707	7.017991257781083	64732
a332a895eea7db6e3146e8ac139800a9e9a50fb6	modal logics for ai planning	automated planning;logic artificial intelligence;dynamic logic;modal logic;frame problem;modal logics ai planning dynamic logic automated plan formation;ai planning;planning artificial intelligence formal logic	"""This paper presents a survey on current research in modal and dynamic logic in the theory of AI planning techniques together with a rst assessment of the role these concepts and techniques may play in the overall process of automated plan formation. First, the use of modal logics for modelling planning problems is motivated. After showing the computational problems {besides the well known frame problem{ of the situation calculus by an example, it is argued that a modal framework for modelling planning problems ooers several advantages compared with classical planning approaches. Then, the basic ideas of modal logics are presented, and two existing approaches to planning using a modal framework are reviewed. At the end, an outlook towards eecient implementations of modal logics is given. 1 The Very Idea The rst attempts to use formal logic for automated plan generation, like Green's situation calculus , or Kowalski's extensions Kow79] were completely based on rst order predicate calculus and plans were extracted directly from automated deductions. The major advantage of these approaches is the use of a uniform framework with clear semantics. However, straightforward implementations of this approach did not achieve the eeciency to be useful for practical applications. An applicable implementation of a logic approach, as in STRIPS, had to employ more procedurally oriented inference techniques making semantic considerations much more diicult Lif87]. This situation has not changed signiicantly up to now. Although research in deduction systems has led to considerable advances, the problem to implement a deduction system for rst-order predicate calculus, that can generally solve \real-world"""" problems has still to be regarded as unsolved. However, classical rst-order predicate calculus is just one of many logics that are known and have been investigated for some decades. It must be regarded as a general-purpose tool that is more or less well suited to diierent applications. Some other candidates in formal logic seem to ooer more advantages while still preserving semantic clarity. The basic motivation to use modal logic for planning tasks comes from the observation that the so-called Kripke structures (see HC68] for details), which constitute the model theoretic semantics for modal logics exactly parallels the setup in planning tasks: Kripke structures A slightly shorter version of this paper appeared in Proc."""	automated planning and scheduling;computation;computational problem;first-order logic;formal system;frame problem;general-purpose modeling;kripke semantics;kripke structure (model checking);microsoft outlook for mac;modal logic;natural deduction;parallels desktop for mac;procedural programming;strips;situation calculus;theory;whole earth 'lectronic link	Jürgen Dix;Joachim Posegga;Peter H. Schmitt	1990			dynamic logic;description logic;computer science;artificial intelligence;machine learning;automated reasoning;multimodal logic;algorithm	AI	-19.247226416182944	13.226254760882924	64876
b0cfeca9986202e5da77cc291206eeef71ea59c1	determining topological relationship of fuzzy spatiotemporal data integrated with xml twig pattern	fuzzy spatiotemporal data;topological relationship;xml twig pattern;transformation;region encoding scheme	How to determine topological relationships is one of the most important operations on fuzzy spatiotemporal data. The proposed strategies impose strict restrictions on structure and data types of fuzzy spatiotemporal data, and fall short in their abilities to handle fuzzy attributes extension and fuzzy time extension. To overcome these limitations, in this paper, we first establish a fuzzy spatiotemporal data model based on XML. Then, we propose strategies of transforming two general fuzzy spatiotemporal data trees into one binary fuzzy spatiotemporal data tree. In succession, an effective algorithm to match the desired twigs is proposed after extending the region coding scheme to compatible with fuzzy spatiotemporal data. Our approach adopts XML twig pattern technique to determine topological relationship continuously so that it can reduce unnecessary execution time of querying the desired nodes. More importantly, we use a pointer array to eliminate unnecessary execution time of twig matching. Finally, the experimental results demonstrate the performance advantages of our approach.	algorithm;binary tree;data model;experiment;pointer (computer programming);regional lockout;run time (program lifecycle phase);spatiotemporal database;succession;tree traversal;twig;xml namespace	Luyi Bai;Li Yan;Zongmin Ma	2012	Applied Intelligence	10.1007/s10489-012-0395-3	transformation;theoretical computer science;data mining;database	DB	-29.71069734280998	4.789199159347735	64898
0d20ac27cc1663e902ac3525a84ab8ec02137e43	semantic vs. structural resemblance of classes	semantic similarity;fuzzy set;fuzzy set theory	We present an approach to determine the similarity of classes which utilized <italic>fuzzy</italic> and <italic>incomplete</italic> terminological knowledge together with schema knowledge.  We clearly distinguish between <italic>semantic</italic> similarity determining the degree of resemblance according to real world semantics, and <italic>structural correspondence</italic> explaining <italic>how</italic> classes can actually be interrelated.  To compute the <italic>semantic similarity</italic> we introduce the notion of <italic>semantic relevance</italic> and apply fuzzy set theory to reason about both terminological knowledge and schema knowledge.	database schema;fuzzy set;semantic similarity;set theory	Peter Fankhauser;Martin Kracker;Erich J. Neuhold	1991	SIGMOD Record	10.1145/141356.141383	natural language processing;semantic similarity;semantic computing;computer science;data mining;fuzzy set	DB	-21.03384182636221	5.348397095313932	65031
70a9177b0ac784464745399885aa8da97105e3dc	a method for implementing a probabilistic model as a relational database	relational data model;constraint propagation;dynamic pro gramming;relational database;probabilistic inference;probabilistic model;linear equations;probability model;database management system	This paper discusses a method for im­ plementing a probabilistic inference system based on an extended relational data model. This model provides a unified approach for a variety of applications such as dynamic pro­ gramming, solving sparse linear equations, and constraint propagation. In this frame­ work, the probability model is represented as a generalized relational database. Subse­ quent probabilistic requests can be processed as standard relational queries. Conventional database management systems can be easily adopted for implementing such an approxi­ mate reasoning system.	data model;data structure;dynamic programming;inference engine;linear equation;local consistency;mathematical optimization;reasoning system;relational database management system;relational model;software propagation;sparse matrix;statistical model	S. K. Michael Wong;Cory J. Butz;Yang Xiang	1995			statistical model;database theory;nested set model;relational model;statistical relational learning;relational calculus;semi-structured model;entity–relationship model;data model;probabilistic relevance model;relational database;computer science;probabilistic database;database model;machine learning;data mining;database;mathematics;linear equation;conjunctive query;object-relational impedance mismatch;database design;local consistency;divergence-from-randomness model	DB	-30.620217670935936	9.242299970031734	65105
9bfacf22a25e7d8235e790f9493aa3a4120060b3	combining nonmonotonic reasoning and belief revision: a practical approach	representacion conocimientos;systeme intelligent;algoritmo busqueda;algorithme recherche;sistema inteligente;search algorithm;circonscription;satisfiabilite;systeme base connaissances;satisfiability;circumscription;nonmonotonic reasoning;belief revision;intelligent system;knowledge representation;representation connaissances;local search;knowledge based systems;circonscripcion;knowledge base;knowledge engineering	In this paper, a new syntax-based approach to belief revision is presented. It is developed within a nonmonotonic framework that allows a twosteps handling of inconsistency to be adopted. First, a disciplined use of nonmonotonic ingredients is made available to the knowledge engineer to prevent many inconsistencies that would occur if a standard logical interpretation and representation of beliefs were conducted. Remaining inconsistencies are considered unexpected and revised by weakening the formulas occurring in any minimally inconsistent subbase, as if they were representing exceptional cases that do not actually occur. While the computation of revised knowledge bases remains intractable in the worst case, our approach benefits from an efficient local search-based heuristic technique that empirically proves often viable, even in the context of very large propositional applications.	belief revision;best, worst and average case;computation;heuristic;interpretation (logic);knowledge base;knowledge engineer;local search (optimization);non-monotonic logic	Brigitte Bessant;Éric Grégoire;Pierre Marquis;Lakhdar Sais	1998		10.1007/BFb0057439	knowledge representation and reasoning;knowledge base;computer science;artificial intelligence;local search;non-monotonic logic;knowledge engineering;mathematics;belief revision;circumscription;algorithm;satisfiability;search algorithm	AI	-19.61313187982608	9.959122252862116	65109
13a65c01c58d1fd5a105321f9cf43bd9edd6701f	object-based storage model for object-oriented database	object oriented data model;storage management;object oriented database;data layout	The current storage models for Object-Oriented DataBase (OODB), which organize data as objects according to the Object-Oriented Data Model (OODM), are mainly established on the block storage devices. In this way, the storage manager does not have detailed knowledge of the characteristics of the underlying storage devices, and the storage subsystem does not have the semantic knowledge of the data stored in the block storage devices, so it is very difficult to implement some workload-dependent tasks such as data layout and caching. Furthermore, the storage subsystem of OODB has to organize the objects in the pages, which is not well-suited to the objects storage. In this paper, we present an Object-Based Storage Model (OBSM) for OODB by using the recently-standardized Object-based Storage Device (OSD) interface. OSD offloads the storage management into the storage device itself and provides an object interface to data, which brings more intelligence into the storage subsystem. In the first glance at using OBSD in OODB, we explore the methods to map OODM to OBSM including direct mapping, mapping clustering to collection and declustering a large object to a series of sub-objects, and analyze the benefits to the storage subsystem of OODB by using OBSM such as providing storage functionalities offloading, providing objects sharing pool, providing integrative object persistence.	block (data storage);cache (computing);cluster analysis;computer data storage;data model;database;database storage structures;object storage;object-based language;openbsd;persistence (computer science);requirement;storage model	Zhongmin Li;Zhanwu Yu	2007		10.1007/978-3-540-74784-0_36	parallel computing;converged storage;object storage;computer science;operating system;data mining;database;information repository;world wide web	DB	-32.7300244496594	12.180567305559778	65134
7ec3de9aeb436f3230f4c6bae92832958ffadb28	querying graph databases	graph databases;expressiveness;conjunctive regular path queries;containment;query evaluation	Graph databases have gained renewed interest in the last years, due to its applications in areas such as the Semantic Web and Social Networks Analysis. We study the problem of querying graph databases, and, in particular, the expressiveness and complexity of evaluation for several general-purpose query languages, such as the regular path queries and its extensions with conjunctions and inverses. We distinguish between two semantics for these languages. The first one, based on simple paths, easily leads to intractability, while the second one, based on arbitrary paths, allows tractable evaluation for an expressive family of languages.  We also study two recent extensions of these languages that have been motivated by modern applications of graph databases. The first one allows to treat paths as first-class citizens, while the second one permits to express queries that combine the topology of the graph with its underlying data.	cobham's thesis;general-purpose markup language;graph database;query language;semantic web	Pablo Barceló	2013		10.1145/2463664.2465216	computer science;clique-width;theoretical computer science;database;containment;expressivity;graph;graph database;intersection graph;query language;graph rewriting	DB	-23.484429780944396	9.296028146898692	65288
b1ad73042382ee45714103ba3722191512cf908b	guideline for the design of medley: a distributed multiple database system	database system			Lois M. L. Delcambre;Elizabeth Troutman Adams;A. Birjandi;M. Farooq;A. Hayatgheyb;C. Lillie;Elaine Lisboa;M. K. Sadagopan;Don Vines	1980			database;guideline;computer science	DB	-32.35734942944292	9.777513946437677	65360
58188574e3d5c0cd9dcca73b00064e006763d644	data modeling of knowledge rules: an oracle prototype	relational model;data modeling;entity relationship diagram;expert systems;knowledge base	Knowledge rules are outlined declaratively in a knowledge base repository. Each rule is created independently for storage in the repository. This paper provides an approach to apply the techniques of traditional entity-relationship data modeling to structure the knowledge rules for storage as a database schema in a relational database management system. Utilization of entity relationship model and relational database for modeling knowledge rules provides for a more standardized mechanism for structuring knowledge rules. Storage of knowledge rules in a relational database shall also bring about improved integration with business applications, besides having the availability of services provided for transactional database applications. The paper utilizes the Oracle database for illustrating the application of the concepts through a sample set of knowledge rules.  The approach is explained through a prototype in Oracleu0027s PL/SQL Server Pages.	data modeling;prototype	Rajeev Kaula	2012	JSW		data modeling;knowledge base;relational model;entity–relationship model;relational database;computer science;knowledge management;database model;knowledge-based systems;data mining;database;knowledge extraction;view;database schema;database design	DB	-33.12859940492312	10.817513287263793	65673
53edf8302b72dc2bdbc878c07f623586853e67d0	query processing techniques for arrays	query language;database system;medical imagery;image numerique;base donnee;multiple support;array query optimization;execution time;query processing;image databank;optimization technique;appui multiple;database;base dato;procesador panel;array processor;lenguaje interrogacion;processeur tableau;user defined functions;medical image;traitement question;banco imagen;memory usage optimization;banque image;imagen numerica;declarative query language;imagineria medica;imagerie medicale;pipelined evaluation;temps execution;procesador oleoducto;langage interrogation;digital image;functional requirement;apoyo multiple;array manipulation language;processeur pipeline;tiempo ejecucion;pipeline processor	Arrays are an appropriate data model for images, gridded output from computational models, and other types of data. This paper describes an approach to array query processing. Queries are expressed in AML, a logical algebra that is easily extended with user-defined functions to support a wide variety of array operations. For example, compression, filtering, and algebraic operations on images can be described. We show how AML expressions involving such operations can be treated declaratively and subjected to useful rewrite optimizations. We also describe a plan generator that produces efficient iterator-based plans from rewritten AML expressions.	computation;computational model;data compression;data model;database;filter (signal processing);image processing;interpreter (computing);iterator;linear algebra;mathematical optimization;query language;requirement;rewrite (programming);user-defined function	Arunprasad P. Marathe;Kenneth Salem	1999	The VLDB Journal	10.1007/s007780200062	computer science;theoretical computer science;user-defined function;database;programming language;functional requirement;digital image;algorithm;query language	DB	-29.46607357026788	8.47521001055797	65812
05a7f97443cb559b2e9a642a12dd094e138799c7	teleo-reactive abductive logic programs	production systems;teleo reactive programs;abductive logic programming;lps	Teleo-reactive (TR) programs are a variety of production systems with a destructively updated database that represents the current state of the environment. They combine proactive behaviour, which is goal-oriented, with reactive behaviour, which is sensitive to the changing environment. They can take advantage of situations in which the environment opportunistically solves the system’s goals, recover gracefully when the environment destroys solutions of its goals, and abort durative actions when higher priority goals need more urgent attention. In this paper, we present an abductive logic programming (ALP) representation of TR programs, following the example of our ALP representation of the logic-based production system language LPS. The operational semantics of the representation employs a destructively updated database, which represents the current state of the environment, and avoids the frame problem of explicitly reasoning about the persistence of facts that are not affected by the updates. The model-theoretic semantics of the representation is defined by associating a logic program with the TR program, the sequence of observations and actions, and the succession of database states. In the semantics, the task is to generate actions so that all of the program’s goals are true in a minimal model of this associated logic program.	abductive logic programming;abductive reasoning;complex event processing;data integrity;database;formal system;frame problem;functional reactive programming;graceful exit;interference (communication);knowledge representation and reasoning;lightweight portable security;operational semantics;persistence (computer science);production system (computer science);programming paradigm;prototype;scalability;succession;system programming language;the times;theory	Robert A. Kowalski;Fariba Sadri	2012		10.1007/978-3-642-29414-3_3	computer science;artificial intelligence;theoretical computer science;algorithm;abductive logic programming	AI	-19.576974599730264	8.827814163664325	66286
8d5660f3ca8757e0a04096be93f40b8e47e9a6e0	processing read-only queries over views with generalization	processing read-only queries	ABS’I IiACT __ ___...l The traditional Query Modification appi oa(. h to query processing is inappropriate for views involving generalization. We use a combination of modification and materialization for queries over such isiews. Furthermore, by choosing modification or materialization ae part of global optimization, we permit more optimization than would be provided by a purely modifying approach.	database;global optimization;mathematical optimization	David Goldhirsch;Laura Yedwab	1984			theoretical computer science;database;information retrieval	DB	-28.909455462790426	5.836411182480885	66407
8206e122b43c1464bc4fdc7b10749c130e0ac4bd	attribute inheritance implemented on top of a relational database system	database system;data integrity;lattices;query processing;relational databases database systems lattices transaction databases object oriented databases knowledge based systems query processing joining processes satellite broadcasting;synchronisation data integrity relational databases;relational database;multiple attribute inheritance relational database system integrity checking query evaluation transaction synchronization tasks;satellite broadcasting;synchronisation;transaction databases;query evaluation;database systems;integrity constraints;joining processes;integrity checking;relational database system;relational databases;object oriented databases;multiple attribute inheritance;knowledge based systems;transaction synchronization tasks	An implementation technique which solves integrity checking, query evaluation, and transaction synchronization tasks in database systems with multiple attribute inheritance and uses the support of a relational database system is presented. It is shown how to implement an integrity checker and a transaction synchronization component of a database system with attribute inheritance. The basic idea is to map arbitrary integrity constraints, queries, and locks associated with classes of an inheritance lattice onto integrity constraints, queries, and locks associated with relations of an underlying relational database. >	relational database management system	Stefan Böttcher	1989		10.1109/ICDE.1990.113504	referential integrity;relational model;database transaction;rollback;relational database;computer science;database model;knowledge-based systems;data integrity;data mining;database;distributed computing;conjunctive query;single table inheritance;database schema;alias;database design	DB	-29.046895456680698	9.336754171209432	66492
99653bdcea7a3168338649f4b08f467c1b10032b	text file inversion: an evaluation		This paper compares inversion of text files with inversion of more structured records. The unique characteristics of textual data which restrict the utility of inversion are identified and discussed. Inversion is shown to be useful only for small, static data bases, and when full text search is not required.	database;sap netweaver business warehouse;string searching algorithm;text corpus	R. M. Bird;J. B. Newsbaum;J. L. Trefftzs	1978		10.1145/800128.804166	computer science;data mining;database;information retrieval	DB	-32.084861908937086	4.568697590424546	66579
254b23c428a7c45b0a8065428bfa31b5f706b76d	modeling vs encoding for the semantic web	expressiveness;ontology modeling and encoding;algebra;functional languages;semantic web;haskell;semantic engineering	The Semantic Web emphasizes encoding over modeling. It is built on the premise that ontology engineers can say something useful about the semantics of vocabularies by expressing themselves in an encoding language for automated reasoning. This assumption has never been systematically tested and the shortage of documented successful applications of Semantic Web ontologies suggests it is wrong. Rather than blaming OWL and its expressiveness (in whatever flavor) for this state of affairs, we should improve the modeling techniques with which OWL code is produced. I propose, therefore, to separate the concern of modeling from that of encoding, as it is customary for database or user interface design. Modeling semantics is a design task, encoding it is an implementation. Ontology research, for applications in the Semantic Web or elsewhere, should produce languages for both. Ontology modeling languages primarily support ontological distinctions and secondarily (where possible and necessary) translation to encoding languages.	artificial intelligence;automated reasoning;database;linked data;modeling language;ontology (information science);ontology engineering;run time (program lifecycle phase);semantic web;user interface design;vocabulary;web ontology language;yet another	Werner Kuhn	2010	Semantic Web	10.3233/SW-2010-0012	natural language processing;web modeling;semantic web rule language;ontology inference layer;computer science;ontology;semantic web;social semantic web;semantic web stack;database;programming language;owl-s	Web+IR	-32.20964113591167	14.476438971909849	66845
396a9145bef0b61889fb2a674c3c1c9aa5a5722d	rdf metadata for xml access control	xml access control;temporal data;flexible security granularity;rdf metadata;tree extension;rxacl;xml security;xml document;access control;association objects;security policy	In this paper we present an access control framework that provides flexible security granularity for XML documents. RDF statements are used to represent security objects and to express security policy. The concepts of simple security object and association security object are defined. Our model allows to express and enforce access control on XML trees and their associations. Access control rules, corresponding to (s, o, ±a) triples, are represented as RDF statements with properties access type, user, and object. A history file is maintained for each user that allows decision-making using temporal data.	access control;ada;resource description framework;xml	Vaibhav Gowadia;Csilla Farkas	2003		10.1145/968559.968567	plain old xml;xml catalog;xml validation;rdf/xml;xml encryption;cwm;xml base;simple api for xml;xml;streaming xml;computer science;document structure description;xml framework;xml database;xml schema;database;xml signature;world wide web;xml schema editor;information retrieval;efficient xml interchange	DB	-32.00201877528015	10.81422100681589	66862
bb65e7eca1e3d16c475be565e860eabb789d99a6				algorithm;angularjs;direct-broadcast satellite;experiment;frequency response;futures studies;point of interest;relational database management system;spatial query	Xi Guo;Yoshiharu Ishikawa;Yonghong Xie;Aziguli Wulamu	2016	World Wide Web	10.1007/s11280-016-0422-0	database;internet privacy;world wide web	DB	-32.32255329457557	7.482743690057743	67120
944f8b2792c71df52be2cd5756bfad914889ba36	rdf updates with constraints		This paper deals with the problem of updating an RDF database, expected to satisfy user-defined constraints as well as RDF intrinsic semantic constraints. As updates may violate these constraints, side-effects are generated in order to preserve consistency. We investigate the use of nulls (blank nodes) as placeholders for unknown required data as a technique to provide this consistency and to reduce the number of side-effects. Experimental results validate our goals.	deterministic algorithm;experiment;side effect (computer science);triplestore	Mirian Halfeld Ferrari Alves;Carmem S. Hara;Flavio R. Uber	2017		10.1007/978-3-319-69548-8_16	rdf;blank;rdf schema;database;computer science	AI	-26.278348794094175	12.764339010006507	67179
0166124aad7e08d6ca524b9a4d369e01e6912409	shreddr: pipelined paper digitization for low-resource organizations	cell phone;database system;data collection;measurement system;decision maker;kictd;computer vision;large scale;machine learning;design and implementation;machine readable forms;batch process;developing regions;computing for development;document processing;data quality;paper forms;smartphone	"""For low-resource organizations working in developing regions, infrastructure and capacity for data collection have not kept pace with the increasing demand for accurate and timely data. Despite continued emphasis and investment, many data collection efforts still suffer from delays, inefficiency and difficulties maintaining quality. Data is often still """"stuck"""" on paper forms, making it unavailable for decision-makers and operational staff. We apply techniques from computer vision, database systems and machine learning, and leverage new infrastructure -- online workers and mobile connectivity -- to redesign data entry with high data quality. Shreddr delivers self-serve, low-cost and on-demand data entry service allowing low-resource organizations to quickly transform stacks of paper into structured electronic records through a novel combination of optimizations: batch processing and compression techniques from database systems, automatic document processing using computer vision, and value verification through crowd-sourcing. In this paper, we describe Shreddr's design and implementation, and measure system performance with a large-scale evaluation in Mali, where Shreddr was used to enter over a million values from 36,819 pages. Within this case study, we found that Shreddr can significantly decrease the effort and cost of data entry, while maintaining a high level of quality."""	batch processing;computer vision;crowdsourcing;data quality;database;document processing;high-level programming language;machine learning;mali (gpu)	Kuang Chen;Akshay Kannan;Yoriyasu Yano;Joseph M. Hellerstein;Tapan S. Parikh	2012		10.1145/2160601.2160605	simulation;data quality;computer science;data mining;database	Web+IR	-33.3963139805768	17.78039104981749	67234
b010bf4c31cc4a31769a9a9dc825cc8fcb0fa8f0	achieving zero information-loss in a classical database environment	database system;information loss;temporal database;relational model;secure system	The research in temporal databases has, so far, concentrated on the history of an object as it exists in the real world. Instead, in this paper we view the history of an object as it is recorded in a database. Such a history is obtained by extr (insert, modify, and Yr lating the outcome of the u$ttes elete) made to the object at drscrete instants. Our model su ports two kinds of query-users: the s stem-user, and the coszical-user. For the classical-user, E P t e interface to the database is identical to the usual interface in classical snapshot databases. We extend the classical relational model so that a transaction, i.e., an update or a (retrieval) query is recorded in such a way that its effect can be determine d at any time in the future; thus, our model is a zero information-loss model logical structure im P I Theorem 1). The oeed upon the mode allows us to give a powerful algebra or the system-user to query the circumstantial information surrounding updates and queries. In addition, a single execution of a query can be identified with the relation it retrieves; thus, a user can query queries, query queries on queries, ad infinitum. The model represents an application of temporal databases to mainstream databases. It can be used in auditing, and as a foundation for building secure systems.	command & conquer:yuri's revenge;database transaction;knowledge-based systems;relational model;snapshot (computer storage);temporal database;transaction log;workbench	Gautam Bhargava;Shashi K. Gadia	1989			database theory;relational model;intelligent database;semi-structured model;database transaction;surrogate key;database tuning;relational database;computer science;database model;data mining;database;temporal database;view;database schema;physical data model;information retrieval;alias;object-relational impedance mismatch;database testing;database design;spatiotemporal database	DB	-30.021731267044352	9.018426018167135	67275
a18b0e8a47d1bc07e09d120eb1eb30a7fcce064c	searching the deep web using proactive phrase queries	natural language queries;proactive search engine;deep web	This paper proposes ipq, a novel search engine that proactively transforms query forms of Deep Web sources into phrase queries, constructs query evaluation plans, and caches results for popular queries offline. Then at query time, keyword queries are simply matched with phrase queries to retrieve results. ipq embodies a novel dual-ranking framework for query answering and novel solutions for discovering frequent attributes and queries. Preliminary experiments show the great potentials of ipq.	cpu cache;deep web;experiment;online and offline;proactive parallel suite;web search engine	Wensheng Wu;Tingting Zhong	2013		10.1145/2487788.2487854	web query classification;computer science;database;web search query;world wide web;information retrieval;deep web;query language;spatial query	DB	-32.56377714202581	4.6215102982671254	67393
d21a627823e9ce98da3741098318fa5840ee0093	modeling and diagnosis characterization using timed observation theory		In this paper we propose using Timed Observation Theory as a powerful framework for model-based diagnosis. It provides a global formalism for modeling a dynamic tool (TOM4D) designed to characterize and compute diagnoses of a structure under investigation		I. Fakhfakh;Marc Le Goc;Luis Torres;Corinne Curt	2012			computer science;theoretical computer science;data mining;formalism (philosophy)	Theory	-22.637386502373392	15.580487305553214	67444
98239c473549645c3801c0fff114af3673827cc0	a towards an extended relational algebra for software architecture	relation algebra;software maintenance;algorithm;software architecture;parameterized matching;multiple patterns;finite automata;system development;prev encoding	Software architecture is often structured as box-and-arrow graphs and has important implications for system development and maintenance. We propose an extended relational algebra to support presentation and manipulation of both architectural structures and implications. The core structure of this algebra is the extended architectural relation (EAR). An EAR is a mapping from an architectural relation (AR) to a multi-set of attributes (M), where the AR is an ordinary relation representing an architectural structure, and the M represents a multi-set representing a type of architectural implication. A set of EAR operations is then defined to support EAR manipulations. The main advantage of this extended algebra over ordinary relational algebras is that the architectural implications (the M part) are presented and manipulated together with the architectural structures (the AR part). This paper first discusses why we propose the algebra, then briefly introduces what the algebra is, and finally describes how to use the algebra in a real scenario.	ear tag;relational algebra;software architecture;visual programming language	Zude Li;Mechelle Gittens;Syed Shariyar Murtaza;Nazim H. Madhavji	2010	ACM SIGSOFT Software Engineering Notes	10.1145/1764810.1764823	software architecture;computer science;engineering;theoretical computer science;software engineering;relation algebra;finite-state machine;programming language;software maintenance;algorithm	SE	-29.573478561644404	13.572497334905693	67974
d983e79d6b6bd283953cb709209f6327b25b088f	the canny skipper - a puzzle for demonstrating data structures and recursion	josephus puzzle;canny skipper;data structures;recursion	Recursion is an interesting concept in problem solving. One interesting algorithm which has been used to illustrate various aspects of recursion is the Josephus Puzzle. This paper describes a similar problem, that of the Canny Skipper, and shows how it can be used, both in the form of problem solving via tactile play, and deriving an algorithm using data structures and recursion.	algorithm;canny edge detector;data structure;problem solving;recursion;skipper (former orm designer)	Michael Wirth	2014		10.1145/2597959.2597966	calculus;mathematics;algorithm	AI	-23.024845555675647	14.878681996109684	68199
8341bdad773391fa3a6626310d37174ae4c646b2	roadeye: a system for personalized retrieval of dynamic road conditions	roads spatial databases indexing mobile communication visualization sensors;r tree;personalized r tree indexing road conditions trip planning;indexing;road conditions;personalized;traffic information systems input output programs query processing;trip planning;disk io roadeye personalized dynamic road conditions retrieval dynamically changing road conditions quality driving experience safe driving experience trip planning augmentation ψr tree r tree based index linked lists user queried road conditions query response times	Awareness of dynamically changing road conditions is crucial for a safe and quality driving experience, as well as, in augmenting trip planning. This work addresses the problem of keeping users informed in a timely and personalized manner about road conditions arising from both scheduled and ad hoc events. We propose Road Eye, a system for personalized retrieval of dynamic road conditions. The key contribution of Road Eye is the psi R-tree, which is a novel R-tree-based index augmented with linked lists for facilitating quick and personalized retrieval of user-queried road conditions. Our performance study indicates that the psi R-tree is indeed effective in retrieving dynamic road conditions with reduced query response times and disk I/Os.	awareness;data structure;hash table;hoc (programming language);linked list;personalization;r-tree;smart city	Anirban Mondal;Avinash Sharma;Kuldeep Yadav;Abhishek Tripathi;Atul Singh;Nischal M. Piratla	2014	2014 IEEE 15th International Conference on Mobile Data Management	10.1109/MDM.2014.42	r-tree;search engine indexing;simulation;computer science;database;world wide web	DB	-31.025822846783782	16.967385867956263	68285
9caadcf3f846d43dd9da472153b5db74442b31d6	ksrquerying: xml keyword with recursive querying	loosely structured search;search engine;keyword search;recursive querying;xml;experimental evaluation	We propose an XML search engine called KSRQuerying. The search engine employs recursive querying techniques, which allows a query to query the results of a previous application of itself or of another query. It answers recursive queries, keyword-based queries, and loosely structured queries. KSRQuerying uses a sort-merge algorithm, which selects subsets from the set of nodes containing keywords, where each subset contains the  smallest  number of nodes that: (1) are  closely   related to each other, and (2) contain at least one occurrence of each keyword. We experimentally evaluated the quality and efficiency of KSRQuerying and compared it with 3 systems: XSeek, Schema-Free XQuery, and XKSearch.	recursion;xml	Kamal Taha;Ramez Elmasri	2009		10.1007/978-3-642-03555-5_4	search-oriented architecture;xml;computer science;data mining;database;keyword density;information retrieval;search engine	DB	-31.88066062565929	4.8144414309565855	68383
b9c84462772343297ecb17b41b73fc99fccd9008	application of case-based reasoning for traction substation design	database indexing;level 2;railway engineering;case similarity calculation model;current;case base reasoning;switching circuits;case similarity calculation model traction substation case based reasoning case base searching indexing;level 2 indexing;substation automation;data type;object oriented programming;architecture presentation;substations switches circuit breakers product design switching circuits indexing current object oriented modeling voltage application software;product main model;software architecture;traction substation design;traction substation;railway;nearest algorithm;traction case based reasoning database indexing object oriented programming product design railway engineering software architecture substation automation;indexing;object oriented;traction;indexation;cognition;it adoption;substations;railway case based reasoning traction substation design product main model architecture presentation object oriented presentation product structure level 2 indexing data type case similarity calculation model database indexing nearest algorithm;object oriented presentation;circuit breakers;product design;case based reasoning;spare parts;algorithm design and analysis;searching indexing;product structure;case base	The paper analyzes traction substationpsilas structure and spare parts, product main model. It mainly studies the application of case-based reasoning (CBR) in the design of traction substation. The application adopts architecture and object-oriented presentation to represent productpsilas structure and case base. It adopts level-2 indexing to determine searching indexing and standardizes original data to make it suitable for systempsilas similarity calculation. It chooses corresponding similarity model calculation based on data type. It adopts case similarity calculation model with partial and whole similarity integrated and level-2 indexing based on database indexing and nearest algorithm.	case-based reasoning;traction teampage;traction substation	Shuang Zhang;Zhiliang Zhu;Huaiyu Xu;Xingwei Wang	2008		10.1109/CSSE.2008.526	simulation;computer science;theoretical computer science;railway engineering;database;product design;programming language;object-oriented programming	AI	-33.57445613081661	16.83922477081954	68420
1340ec840a2dd0ec24bfefe280c47644da3ecefb	narrative planning: compilations to classical planning	journal article;artificial intelligence;keywords classical planning	A model of story generation recently proposed by Riedl and Yo ung casts it as planning, with the additional condition that story characters behave inte ntionally. This means that characters have perceivable motivation for the actions they take. I show tha t t is condition can be compiled away (in more ways than one) to produce a classical planning problem t hat can be solved by an off-the-shelf classical planner, more efficiently than by Riedl and Young’ s specialised planner.	automated planning and scheduling;compiler;planner;tag (game)	Patrik Haslum	2012	J. Artif. Intell. Res.	10.1613/jair.3602	computer science;artificial intelligence;mathematics	ML	-19.19006048328188	4.387676368299281	68593
1ec276109c74f74d35c342eaa4942cf25cd57939	lars: a logic-based framework for analytic reasoning over streams		The recent rise of smart applications has drawn interest to logical reasoning over data streams. Different query languages and stream processing/reasoning engines were proposed. However, due to a lack of theoretical foundations, the expressivity and semantics of these diverse approaches were only informally discussed. Towards clear specifications and means for analytic study, a formal framework is needed to characterize their semantics in precise terms. We present LARS, a Logic-based framework for Analyzing Reasoning over Streams, i.e., a rule-based formalism with a novel window operator providing a flexible mechanism to represent views on streaming data. We establish complexity results for central reasoning tasks and show how the prominent Continuous Query Language (CQL) can be captured. Moreover, the relation between LARS and ETALIS, a system for complex event processing is discussed. We thus demonstrate the capability of LARS to serve as the desired formal foundation for expressing and analyzing different semantic approaches to stream processing/reasoning and engines.	cobham's thesis;complex event processing;computational complexity theory;contextual query language;encode;expressive power (computer science);inference engine;logic programming;modeling language;operational semantics;sparql;semantics (computer science);stream (computing);stream processing;window operator	Harald Beck;Minh Dao-Tran;Thomas Eiter	2015	Artif. Intell.	10.1016/j.artint.2018.04.003	operator (computer programming);machine learning;streams;analytic reasoning;artificial intelligence;query language;linear temporal logic;sparql;answer set programming;mathematics;complex event processing	AI	-19.91971093141939	8.815984537420336	68621
da329bd694418e8b5f6b6244ab02d81bc47bfc06	learning relations and logic programs	logic programs	Inductive Logic Programming (ILP) is an emerging research area at the intersection of machine learning, logic programming and software engineering. The first workshop on this topic was held in 1991 in Portugal (Muggleton, 1991). Subsequently, there was a workshop tied to the Future Generation Computer System Conference in Japan in 1992, and a third one in Bled, Slovenia, in April 1993 (Muggleton, 1993). Ideas related to ILP are also appearing in major AI and machine learning conferences and journals. Although European-based and mainly sponsored by ESPRIT, ILP aims at becoming equally represented elsewhere; for example, among researchers in America who are investigating relational learning and first order theory revision (see, for example, the papers in Birnbaum and Collins, 1991) and within the computational learning theory community. This year's IJCAI workshop on ILP is a first step in this direction, and includes recent work with a broader range of perspectives and techniques. Many different problem settings have been proposed, but it is still possible to abstract similarities and identify one simplified ILP problem:	computational learning theory;executable;inductive logic programming;inductive reasoning;international joint conference on artificial intelligence;linc;machine learning;program transformation;programmer;software engineering	Francesco Bergadano;Daniele Gunetti	1994	Knowledge Eng. Review	10.1017/S0269888900006615	dynamic logic;computer science;computational logic;axiomatic semantics;philosophy of logic	AI	-22.31846973136738	14.617005997082735	68915
d0a5446b878cbfaa8f655f995ee2c65480a0dc72	a layered rule-based architecture for approximate knowledge fusion?	multi agent system;rule based system;rule based;heterogeneous agents;dynamic logic;engineering and technology;teknik och teknologier;real world application;propositional logic;semantic web;fusion rule;description logic;first order logic;deductive databases	In this paper we present a framework for fusing approximate knowledge obtained from various distributed, heterogenous knowledge sources. This issue is substantial in modeling multi-agent systems, where a group of loosely coupled heterogeneous agents cooperate in achieving a common goal. In paper [5] we have focused on defining general mechanism for knowledge fusion. Next, the techniques ensuring tractability of fusing knowledge expressed as a Horn subset of propositional dynamic logic were developed in [13,16]. Propositional logics may seem too weak to be useful in real-world applications. On the other hand, propositional languages may be viewed as sublanguages of first-order logics which serve as a natural tool to define concepts in the spirit of description logics [2]. These notions may be further used to define various ontologies, like e.g. those applicable in the Semantic Web. Taking this step, we propose a framework, in which our Horn subset of dynamic logic is combined with deductive database technology. This synthesis is formally implemented in the framework of HSPDL architecture. The resulting knowledge fusion rules are naturally applicable to real-world data.	approximation algorithm;deductive database;description logic;dynamic logic (modal logic);first-order logic;first-order predicate;formal language;logic programming;loose coupling;multi-agent system;ontology (information science);robotics;semantic web	Barbara Dunin-Keplicz;Linh Anh Nguyen;Andrzej Szalas	2010	Comput. Sci. Inf. Syst.	10.2298/CSIS100209015D	dynamic logic;rule-based system;description logic;computer science;artificial intelligence;theoretical computer science;semantic web;first-order logic;data mining;propositional calculus;multimodal logic;autoepistemic logic	AI	-20.788127868039247	9.100971512671602	69005
1d388ef8a2fb3c336a63b4ded9141f84e93de3b5	mining the meaningful term conjunctions from materialised faceted taxonomies: algorithms and complexity	estensibilidad;information sources;algorithm complexity;algorithm analysis;complexite calcul;information source;source information;reutilizacion;complejidad algoritmo;knowledge extraction;systematique;complexity;data mining;reuse;vista materializada;materialized view;domain knowledge;complejidad computacion;complexite algorithme;sistematica;fouille donnee;computational complexity;indexation;decouverte connaissance;taxonomy;extraction connaissances;extraccion conocimiento;descubrimiento conocimiento;algorithms;analyse algorithme;extensibilite;scalability;knowledge extraction and reuse;materialised faceted taxonomies;busca dato;analisis algoritmo;fuente informacion;reutilisation;knowledge discovery;vue materialisee	A materialised faceted taxonomy is an information source where the objects of interest are indexed according to a faceted taxonomy. This paper shows how from a materialised faceted taxonomy, we can mine an expression of the Compound Term Composition Algebra that specifies exactly those compound terms (conjunctions of terms) that have non-empty interpretation. The mined expressions can be used for encoding in a very compact form (and subsequently reusing), the domain knowledge that is stored in existing materialised faceted taxonomies. A distinctive characteristic of this mining task is that the focus is given on minimising the storage space requirements of the mined set of compound terms. This paper formulates the problem of expression mining, gives several algorithms for expression mining, analyses their computational complexity, provides techniques for optimisation, and discusses several novel applications that now become possible.	algorithm;computational complexity theory;faceted classification;information source;mathematical optimization;mined;regular expression;requirement;taxonomy (general)	Yannis Tzitzikas;Anastasia Analyti	2005	Knowledge and Information Systems	10.1007/s10115-005-0205-x	materialized view;complexity;scalability;computer science;artificial intelligence;data mining;reuse;knowledge extraction;computational complexity theory;domain knowledge;algorithm	ML	-31.99568796103671	6.797056939898509	69308
b76344b018e54ea50b173ad71ca1967583929dfc	medical fuzzy control systems with fuzzy arden syntax		Arden Syntax is a formal language for representing and processing medical knowledge that is employed by knowledge-based medical systems. In HL7 International’s Arden Syntax version 2.9 (Fuzzy Arden Syntax), the syntax was extended by formal constructs based on fuzzy set theory and fuzzy logic, including fuzzy control. These concepts are used to model linguistic and propositional uncertainty – which is inherent to medical knowledge – in a variety of clinical situations. Using these fuzzy methods, we can create medical fuzzy control systems (MFCSs), in which linguistic control rules are used and evaluated in parallel. Their results are aggregated so that gradual transitions between otherwise discrete control states are enabled. In this paper, we discuss the implementation of MFCSs in Fuzzy Arden Syntax. Through code examples from FuzzyArdenKBWean, an MFCS for weaning support in mechanically ventilated patients after cardiac surgery, we illustrate the implementation of fuzzy control.	formal language;fuzzy control system;fuzzy logic;fuzzy set;health level 7;set theory	Jeroen S. de Bruin;Christian J. Schuh;Andrea Rappelsberger;Klaus-Peter Adlassnig	2017		10.1007/978-3-319-66830-7_51	arden syntax;syntax;fuzzy logic;clinical decision support system;natural language processing;fuzzy set;formal language;fuzzy control system;computer science;artificial intelligence	AI	-19.833352100256747	7.194967077376611	69536
924b990cb1bd6309f28a69480bfcae52ff44d288	using the bottom clause and mode declarations on fol theory revision from examples	search space;top down;prediction accuracy;knowledge base	Theory revision systems are designed to improve the accuracy of an initial theory, producing more accurate and comprehensible theories than purely inductive methods. Such systems search for points where examples are misclassified and modify them using revision operators. This includes trying to add antecedents in clauses usually generated in a top-down approach, considering all the literals of the knowledge base. This leads to a huge search space which dominates the cost of the revision process. ILP Mode Directed Inverse Entailment systems restrict the search for antecedents to the literals of the bottom clause. In this work the bottom clause and modes declarations are introduced to improve the efficiency of theory revision antecedent addition. Experimental results compared to FORTE revision system show that the runtime of the revision process is on average three orders of magnitude faster, and generate more comprehensible theories without decreasing the accuracy. Moreover, the proposed theory revision approach significantly improves predictive accuracy over theories generated by Aleph system.	belief revision;first-order logic	Ana Luísa Duboc;Aline Paes;Gerson Zaverucha	2008		10.1007/978-3-540-85928-4_11	knowledge base;discrete mathematics;computer science;artificial intelligence;machine learning;top-down and bottom-up design;database;mathematics;algorithm	AI	-19.683069545209282	14.649276152155299	69537
5d4c1836594143d8807d06e13e18683133103edd	model transformations? transformation models!	developpement logiciel;modelizacion;class diagram;base relacional dato;entity relationship model;transformation model;lenguaje uml;semantics;model transformation;langage modelisation unifie;modelo entidad relacion;langage ocl;interpretacion abstracta;relational database;modele entite relation;semantica;semantique;modelisation;object oriented;desarrollo logicial;unified modelling language;software development;relational model;base donnee relationnelle;property a;architecture basee modele;oriente objet;interpretation abstraite;point of view;abstract interpretation;modeling;orientado objeto;transformation rule derivation;model driven architecture;object constraint language;lenguaje forzado objeto;arquitectura basada modelo	Much of the current work on model transformations seems essentially operational and executable in nature. Executable descriptions are necessary from the point of view of implementation. But from a conceptual point of view, transformations can also be viewed as descriptive models by stating only the properties a transformation has to fulfill and by omitting execution details. This contribution discusses the view that model transformations can be abstracted as being transformation models. As a simple example for a transformation model, the well-known transformation from the Entity-Relationship model to the Relational model is shown. A transformation model in this contribution is nothing more than an ordinary, simple model, i.e., a UML/MOF class diagram together with OCL constraints. A transformation model may transport syntax and semantics of the described domain. The contribution thus covers two views on transformations: An operational model transformation view and a descriptive transformation model view.	circuit complexity;class diagram;domain-specific language;entity–relationship model;executable;meta-object facility;model transformation;object constraint language;point of view (computer hardware company);qvt;relational model;unified modeling language;verification and validation;whole earth 'lectronic link	Jean Bézivin;Fabian Büttner;Martin Gogolla;Frédéric Jouault;Ivan Kurtev;Arne Lindow	2006		10.1007/11880240_31	unified modeling language;relational model;systems modeling;entity–relationship model;relational database;computer science;artificial intelligence;software development;class diagram;database;mathematics;semantics;programming language;object-oriented programming;algorithm;object constraint language	DB	-30.059326719732407	14.409775963524652	69589
c40cde0300b74f9408c900a9234cd9ec63e53b18	a review and design of framework for storing and querying rdf data using nosql database		This paper reviews existing systems and describes a design of RDF database system that uses NoSQL database to store the data which aims to enhance performance of the Semantic Web applications. RDF data is a standard of data in the form of Subject-Predicate-Object called Triples and stored in database called Triple Store. Typically RDF database system uses SPARQL query language to query the RDF data from Triple Store database, e.g. Jena TDB. Our design of RDF database system uses NoSQL database, i.e.,MongoDB, to store the data in JSON-LD format and query by using query API of NoSQL database. We will use the Berlin SPARQL Benchmark to compare the performance of Triple Store and NoSQL systems.	apache jena semantic web framework;application programming interface;benchmark (computing);database;json;json-ld;mongodb;nosql;query language;resource description framework;sparql;triplestore	Chanuwas Aswamenakul;Marut Buranarach;Kanda Runapongsa Saikaew	2014			nosql;semantic web;rdf;information retrieval;database;rdf schema;sparql;xml database;computer science	DB	-33.175424242302334	5.5333508703056085	69764
983167a174a4ce5b5fbfe1a83a876cea579505e7	key-based problem decomposition for relational constraint satisfaction problems	databases;database system;relational databases constraint satisfaction problems;decomposition;constraint processing;measurement;job shop scheduling;decomposition constraint processing databases;relational database;constraint satisfaction problems;schedules relational databases encoding database systems measurement job shop scheduling;data storage;database systems;input csp key based problem decomposition relational constraint satisfaction problem data residing relational database system passive data storage backend automated decomposition;schedules;relational database system;constraint satisfaction problem;relational databases;experimental evaluation;encoding	Constraint satisfaction problems (CSP) are often posed over data residing in relational database systems, which serve as passive data-storage back ends. Several studies have demonstrated a number of important advantages to having database systems capable of natively modelling and solving CSPs. This paper studies the automated decomposition of the input CSP, borrowing another distinctive idea of relational databases, normalization, to improve its solution time. Experimental evaluations for two case studies show the potential benefit of the approach.	algorithm;constraint satisfaction problem;correctness (computer science);cryptographic service provider;data integrity;database normalization;embedded system;pl/sql;problem solving;programming paradigm;relational database management system	James J. Lu;Sebastien Siva	2011	2011 IEEE 23rd International Conference on Tools with Artificial Intelligence	10.1109/ICTAI.2011.22	job shop scheduling;relational model;relational database;computer science;theoretical computer science;data mining;database;constraint satisfaction problem	DB	-27.305304266159165	6.633994056253255	69774
363b257d276947d2f29b2494e26fa8693dec9c64	an algebraic framework for temporal attribute characteristics	application development;query language;application management;temporal data;temporal data models;data management;68p15;temporal database;temporal databases;temporal data semantics;attribute characteristics;malleable attributes;temporal algebra;database management system	Most real-world database applications manage temporal data, i.e., data with associated time references that capture a temporal aspect of the data, typically either when the data is valid or when the data is known. Such applications abound in, e.g., the financial, medical, and scientific domains. In contrast to this, current database management systems offer preciously little built-in query language support for temporal data management. This situation persists although an active temporal database research community has demonstrated that application development can be simplified substantially by built-in temporal support. This paper's contribution is motivated by the observation that existing temporal data models and query languages generally make the same rigid assumption about the semantics of the association of data and time, namely that if a subset of the time domain is associated with some data then this implies the association of any further subset with the data. This paper offers a comprehensive, general framework where alternative semantics may co-exist. It supports so-called malleable and atomic temporal associations, in addition to the conventional ones mentioned above, which are termed constant. To demonstrate the utility of the framework, the paper defines a characteristics-enabled temporal algebra, termed CETA, which defines the traditional relational operators in the new framework. This contribution demonstrates that it is possible to provide built-in temporal support while making less rigid assumptions about the data and without jeopardizing the degree of the support. This moves temporal support closer to practical applications.	aggregate data;algorithm;built-in self-test;canonical account;data model;general-purpose modeling;lineage (evolution);linear algebra;query language;relational operator;sql;temporal database	Michael H. Böhlen;Johann Gamper;Christian S. Jensen	2006	Annals of Mathematics and Artificial Intelligence	10.1007/s10472-006-9022-5	data management;computer science;data mining;database;temporal database;information retrieval	DB	-28.19351029449624	9.1272438260215	69791
67c743e1656be9061323afdd0a349f92495b9e5c	semantic-rich user-defined relationship as a main constructor in object oriented database				Oscar Díaz;Peter M. D. Gray	1990			method;data mining;database schema;computer science;object-oriented programming;database;database design	DB	-30.9493710566	10.369395435499278	70014
6e936b5cc5f2b554dfab910491541f26ac23827e	synops - generation of partial languages and synthesis of petri nets		We present the command line tool SYNOPS. It allows the term-based construction of partial languages consisting of different kinds of causal structures representing runs of a concurrent system: labeled directed acyclic graphs (LDAGs), labeled partial orders (LPOs), labeled stratified directed acyclic graphs (LSDAGs) and labeled stratified order structures (LSOs). It implements region based algorithms for the synthesis of place/transition nets and general inhibitor nets from behavioural specifications given by such partial languages.	algorithm;causal filter;command-line interface;concurrency (computer science);directed acyclic graph;petri net	Robert Lorenz;Markus Huber;Christoph Etzel;Dan Zecha	2012				Logic	-22.939356697137796	17.161498406035648	70171
c172ba28b60cbe4ddbcd8ba1f663981d38f695e5	an implementation platform for query-answering in default logics: theoretical underpinnings	systeme intelligent;sistema inteligente;circonscription;logical programming;circumscription;logica defecto;programmation logique;consistency checking;intelligent system;logique defaut;information system;query answering;default logic;programacion logica;systeme information;circonscripcion;sistema informacion	We present a uniform framework for implementing the variety of consistency checks needed for an implementation platform for query-answering in default logics. Our approach is centered around the concept of local proof procedures that allow for validating each inference step when it is performed. The resulting system is unique in offering simultaneously the expressiveness of multiple default logics.		Torsten Schaub;Pascal Nicolas	1997		10.1007/3-540-63614-5_19	computer science;artificial intelligence;data mining;default logic;circumscription;information system;algorithm	Logic	-21.2125874244708	10.9159336491848	70442
a65f849ff50ef6a7ecfda21f73d92dc605cbf243	answering why-not questions on metric probabilistic range queries	probabilistic logic extraterrestrial measurements uncertainty computers databases multimedia communication	Metric probabilistic range queries (MPRQ) have received substantial attention due to their utility in multimedia and text retrieval, decision making, etc. Existing MPRQ studies generally aim to improve query efficiency and resource usage. In contrast, we define and offer solutions to why-not questions on MPRQ. Given an original metric probabilistic range query and a why-not set W of uncertain objects that are absent from the query result, a why-not question on MPRQ explains why the uncertain objects in W do not appear in the query result, and provides refinements of the original query and/or W with the minimal penalty, so that the uncertain objects in W appear in the result of the refined query. Specifically, we propose a framework that consists of three efficient solutions, one that modifies the original query, one that modifies the why-not set, and one that modifies both the original query and the why-not set. Extensive experiments using both real and synthetic data sets offer insights into the properties of the proposed algorithms, and show that they are effective and efficient.	document retrieval;experiment;greedy algorithm;penalty method;probabilistic turing machine;range query (data structures);range query (database);result set;sampling (signal processing);synthetic data	Lu Chen;Yunjun Gao;Kai Wang;Christian S. Jensen;Gang Chen	2016	2016 IEEE 32nd International Conference on Data Engineering (ICDE)	10.1109/ICDE.2016.7498288	range query;sargable;query optimization;query expansion;web query classification;ranking;boolean conjunctive query;computer science;theoretical computer science;data mining;database;rdf query language;range query;information retrieval;query language;spatial query	DB	-25.009856522953985	4.5068234897893475	70719
db4aa42f3a9e657bc7089528d0d54152a998d376	a global constraint for mining sequential patterns with gap constraint		Sequential pattern mining (SPM) under gap constraint is a challenging task. Many efficient specialized methods have been developed but they are all suffering from a lack of genericity. The Constraint Programming (CP) approaches are not so effective because of the size of their encodings. In [7], we have proposed the global constraint PREFIX-PROJECTION for SPM which remedies to this drawback. However, this global constraint cannot be directly extended to support gap constraint. In this paper, we propose the global constraint GAPSEQ enabling to handle SPM with or without gap constraint. GAP-SEQ relies on the principle of right pattern extensions. Experiments show that our approach clearly outperforms both CP approaches and the state-of-the-art cSpade method on large datasets.	algorithm;constraint logic programming;constraint programming;data mining;encode;experiment;generic programming;real life;sequential pattern mining;super paper mario	Amina Kemmar;Samir Loudni;Yahia Lebbah;Patrice Boizumault;Thierry Charnois	2016		10.1007/978-3-319-33954-2_15	constraint logic programming;mathematical optimization;constraint programming;binary constraint;constraint satisfaction;constraint learning;machine learning;mathematics;complexity of constraint satisfaction;algorithm;hybrid algorithm;local consistency	AI	-23.200241004053943	12.40672004203312	70811
5d51ff0f3c2e6c9ccf5d34fd279167663788bdce	constraints and triggers: a method for ensuring data quality in an object-oriented geo database kernel	gis;object oriented;triggers;integrity constraints;object oriented database;data quality;constraints	We present a concept to integrate constraints and triggers into an object-oriented geo database kernel and propose an extension of the underlying object definition language to specify constraints and triggers. To reduce the number of constraints, we provide a mechanism to determine all objects which are subject to constraints and triggers.	constraint (mathematics);data quality;database trigger;kernel (operating system)	Ludger Becker;Hendrik Ditt;Klaus H. Hinrichs;Andreas Voigtmann	1999		10.1145/320134.326039	real-time computing;geomatics;data quality;computer science;data integrity;data mining;database;object-oriented programming;database schema;database design	DB	-30.92388984290613	10.552065515222534	70839
2c5f71aacf3d55efcaefc87bba6f592457ab5558	an analytical framework for particle and volume data of large-scale combustion simulations	scalability issues;feature extraction and tracking;data transformation and representation	This paper presents a framework to enable parallel data analyses and visualizations that combine both Lagrangian particle data and Eulerian field data of large-scale combustion simulations. Our framework is characterized by a new range query based design that facilitates mutual queries between particles and volumetric segments. Scientists can extract complex features, such as vortical structures based on vector field classifications, and obtain detailed statistical information from the corresponding particle data. This framework also works in reverse as it can extract vector field information based on particle range queries. The effectiveness of our approach has been demonstrated by an experimental study on vector field data and particle data from a large-scale direct numerical simulation of a turbulent lifted ethylene jet flame. Our approach provides a foundation for scalable heterogeneous data analytics of large scientific applications.	computer simulation;direct numerical simulation;experiment;numerical analysis;range query (data structures);range query (database);scalability;turbulence;vortex	Franz Sauer;Hongfeng Yu;Kwan-Liu Ma	2013		10.1145/2535571.2535590	computer science;data science;theoretical computer science;data mining	HPC	-32.061636300021924	16.427001122610708	71079
406b4e37ec5988be52b6b8777b9a69f1dc70ce0e	rewriting xql queries on xml repositories	base relacional dato;query language;conjunctive queries;procesamiento informacion;red www;web databases;interrogation base donnee;interrogacion base datos;query optimization;relational database;lenguaje interrogacion;query evaluation;information processing;base donnee relationnelle;xml document;world wide web;langage interrogation;reseau www;information system;traitement information;database query;systeme information;langage xml;sistema informacion	XQL is one of the query languages proposed for querying XML documents on the world wide web. In this paper, we consider the logical rewriting of XQL query expressions in order to improve the efficiency of query evaluation when XML document type definitions (DTDs) are present. We first define three classes of constraint which can be derived from a given DTD. With a suitable representation of an XML repository R as a relational database D, it turns out that these DTD constraints correspond to tuple- and equality-generating dependencies which must hold on D.#R##N##R##N#Next, we identify a subset of XQL queries on R which is equivalent to a class of conjunctive queries on D. Given a conjunctive query C equivalent to an XQL query Q, we then apply techniques from relational dependency theory to reduce the number of conjuncts in C, yielding query C′. Conjunctive query C′ can then be mapped back to an XQL query Q′ in which redundant filter subexpressions and unnecessary selections have been removed.#R##N##R##N#Whether Q′ can be evaluated more efficiently than Q depends on whether or not appropriate indices exist in R. However, the techniques presented in this paper can provide a query optimizer with a set of equivalent XQL expressions from which to select the best. The representation of queries in relational form and of constraints as dependencies should also permit the application of these results to other XML query languages.	rewriting;xml;xquery	Peter T. Wood	2000		10.1007/3-540-45033-5_15	sargable;query optimization;query expansion;web query classification;xml;boolean conjunctive query;information processing;relational database;computer science;data mining;database;conjunctive query;web search query;information retrieval;information system;query language	DB	-28.393169763045627	8.969665251816565	71122
05dfa0e98697bff531e0ac7b5f633559620c3cde	fuzzy quantified structural queries to fuzzy graph databases		This paper deals with fuzzy quantified queries in a graph database context. We study a particular type of structural quantified query and show how it can be expressed in the language FUDGE that we previously proposed. A processing strategy based on a compilation mechanism that derives regular (nonfuzzy) queries for accessing the relevant data is also described.	fuzzy logic;graph database	Olivier Pivert;Olfa Slama;Virginie Thion	2016		10.1007/978-3-319-45856-4_18	fuzzy classification;fuzzy set operations;graph database	DB	-24.536276246022748	6.31235896305798	71212
48d60c7e9c8b1eaf97484c7ee771bd53807516c4	type based querying of xml data streams			xml	George Richard Russell	2006				DB	-33.375832854628236	6.798653041037144	71251
0420abce5921d7e80838ab746da96c1b4bc71474	holistic twig joins on indexed xml documents	optimal solution;indexation;xml document	Finding all the occurrences of a twig pattern specified by a selection predicate on multiple elements in an XML document is a core operation for efficient evaluation of XML queries. Holistic twig join algorithms were proposed recently as an optimal solution when the twig pattern only involves ancestordescendant relationships. In this paper, we address the problem of efficient processing of holistic twig joins on all/partly indexed XML documents. In particular, we propose an algorithm that utilizes available indices on element sets. While it can be shown analytically that the proposed algorithm is as efficient as the existing state-of-the-art algorithms in terms of worst case I/O and CPU cost, experimental results on various datasets indicate that the proposed index-based algorithm performs significantly better than the existing ones, especially when binary structural joins in the twig pattern have varying join selectivities.	algorithm;best, worst and average case;bottom-up proteomics;central processing unit;heuristic (computer science);holism;input/output;join (sql);selectivity (electronic);top-down and bottom-up design;twig;xml	Haifeng Jiang;Wei Wang;Hongjun Lu;Jeffrey Xu Yu	2003		10.1016/B978-012722442-8/50032-X	xml validation;xml;computer science;theoretical computer science;data mining;database	DB	-29.815705202083436	4.544717075254398	71402
23f4eef0fbfcca072e6f257f9a7e0828ec915036	a report generator for database and web applications	visual programming	Report generation is one of the most important tasks for database and e-commerce applications. Current report tools typically provide a set of predefined components that are used to specify report layout and format. However, available layout options are limited, and WYSIWYG formatting is not allowed. This paper proposes a four-phase report generation process to overcome these problems. The first phase retrieves source tables from the database. The second phase reorganizes the layout of the source tables by transferring the source tables into a set of new flat tables (in the first normal form). The third phase restructures the flat tables into a nested table (report) by specifying the report structure. The last phase formats the report with a WYSIWYG format editor supporting a number of formatting rules designed specifically for nested reports. Each phase of the proposed process supports visual programming, giving an easy-to-use user interface and allowing very flexible report layouts and formats. A visual end-user-programming tool, called TPS, is developed to demonstrate the proposed process and show that reports with sophisticated layouts can be created without writing low-level report generation programs. key words: report generation, visual programming, layout transformation, report structure, and report formatting	e-commerce;first normal form;high- and low-level;nested sql;programming tool;report generator;user interface;visual programming language;wysiwyg	Woei-Kae Chen;Pin-Ying Tu	2012	IEICE Transactions		computer science;database;visual programming language;programming language;information retrieval	HCI	-30.311041498668594	16.22389885009405	71513
1e4245852383a6a6ffaa035316b8a3504e6556e0	tgv: a tree graph view for modeling untyped xquery	xquery evaluation;heterogeneous environment;tgv;cost model;extensible optimization	"""XML [7] has become a de facto standard to exchange and represent any kind of data in various contexts. XML data can be manipulated using the XQuery [31] language, which can express though a compact and comprehensive way any queries and transformations. An XQuery expression is evaluated as follows. (1) the expression is rewritten into a """"canonical XQuery"""", then (2) it is modeled in an internal representation and nally (3) it is optimized and executed. In [9], Chen et al. proposed to rewrite XQuery expressions in """"canonical XQuery"""". However, their solution is restricted to a rather limited subset of XQuery and does not support complex XQuery expressions. [4] introduced the TPQ model to represent a single FWR statements as a Tree Pattern and a formula. Then [9] proposed the GTP model which generalize their approach to support several FWR. However, their model cannot capture well all the expressiveness of XQuery expressions, cannot handle mediation problems and do not support extensible optimizations. In our paper, we made three contributions. First, we extend the rules developed by [9] to rewrite any XQuery expression into a """"canonical XQuery"""". Second, we design a new model called TGV which supports all the functionnalities of XQuery, uses an intuitive representation compliant with mediation issues, and provides a support for optimization and cost information. Finally, we provide a generic cost model coupled with the TGV."""	analysis of algorithms;distributed computing;entity–relationship model;mathematical optimization;rectifier;rewrite (programming);standard generalized markup language;turing completeness;xml;xquery	Nicolas Travers;Tuyet-Tram Dang-Ngoc;Tianxiao Liu	2007		10.1007/978-3-540-71703-4_92	computer science;data mining;database;information retrieval	DB	-29.640912214904258	10.796232687827516	71670
b9a7fa1b9bfa0b8fa34919b432fead079db14c63	managing open systems now that the glass-house has gone	distributed database;distributed system management;critical system;open system;system management	Knowledge reuse through networks of large KBS p. 13 Expressing temporal behaviour with extended ECA rules p. 23 Temporal databases: an event-oriented approach p. 38 Object comprehensions: a query notation for object-oriented databases p. 55 Expressivity of typed logic paradigms for object-oriented databases p. 73 Algebraic computation of the weak well-founded model for general deductive databases p. 90 Benchmarking parallel SQL database machines p. 105 Branching transactions: a transaction model for parallel databases p. 121 A strategy for semantic integrity enforcement in a parallel database machine p. 137 On interface objects in object-oriented databases p. 153 Efficient access to FDM objects stored in a relational database p. 170 A conceptual language for querying object-oriented data p. 187 The Jupiter system: a prototype for multi-database inter-operability p. 205 A model for heterogeneous distributed database systems p. 221	computation;database machine;deductive database;distributed database;distributed transaction;event condition action;finite difference method;interoperability;knowledge-based systems;operability;parallel database;prototype;relational database;sql;semantic web;type theory	Richard Barker	1994		10.1007/3-540-58235-5_31	systems management;computer science;database;open system;distributed database	DB	-30.64506117125906	10.615738994536756	71724
d7685de8777846be34dfec9295e3b92ff72fb07f	using active data in a uims	dynamic change;user interface management system;data model;active objects	An active data model is one that not only stores data, but also acts upon changes to the data in ways that reflect application domain semantics. This paper introduces an active object-oriented model based on incremental attribute evaluation (one-way constraints) and discusses how it can be used to support a number of tasks in a User Interface Management System. It is shown how this model can provide unified support for lexical, syntactic, and semantic feedback, how the model supports the specification and implementation of dynamically changing graphical layouts, and how the model can be used as the basis for support of user extensible interfaces.	active object;application domain;data model;graphical user interface;lexicon;management system;one-way function;user interface management systems	Tyson R. Henry;Scott E. Hudson	1988		10.1145/62402.62429	data model;computer science;data mining;database;world wide web;logical data model	DB	-31.0710951592073	12.55189541283191	71957
5c1caa9f22fc9385c071d5ef54c050ee3d57241b	an alternating well-founded semantics for query answering in disjunctive databases	systeme intelligent;sistema inteligente;circonscription;program transformation;logical programming;circumscription;deductive database;well founded semantics;programmation logique;base dato deductiva;intelligent system;non monotonic reasoning;base donnee deductive;information system;query answering;analisis semantico;analyse semantique;programacion logica;systeme information;disjunctive logic programming;semantic analysis;circonscripcion;sistema informacion	The well{founded semantics has been introduced for normal databases (i.e. databases that may have default negation in their rule bodies, but do not have disjunctions). In this paper we propose an extension of the well{founded semantics to the disjunctive case. For this purpose we investigate the alternating xpoint approach of Van Gelder, Ross and Schlipf 16], and develop a suitable generalization to the case of disjunctive rule heads. Given a disjunctive database P , the new alternating well{founded semantics derives a set ADWFSP of partial Herbrand interpretations of P. This set coincides with the set of minimal models if there are no default negations in the database. For general disjunctive databases it is always not empty (if all rule heads are non{empty), i.e. ADWFSP is consistent. The alternating well{founded semantics is very useful for query answering in disjunctive databases. During a xpoint computation the nal set ADWFSP is approximated by a sequence (In)n2I N 0 of sets In of partial Herbrand interpretations. At any step of the xpoint computation it holds: If the query already holds in In, then the query will also hold in ADWFSP, and the computation can be terminated. For other semantics like the semantics of stable and partial stable models, so far no computations are known that have this property.	3d xpoint;approximation algorithm;computation;database;disjunctive normal form;well-founded semantics	Dietmar Seipel	1998		10.1007/BFb0056015	stable model semantics;computer science;artificial intelligence;theoretical computer science;database;mathematics;programming language;circumscription;information system;algorithm	DB	-21.201492946090795	11.648592561167288	72049
3168b86ff8d06c0a4538a00c76345875035096dd	model based deduction for database schema reasoning	busqueda informacion;automated deduction;model based reasoning;raisonnement base sur modele;base donnee;automatic proving;information retrieval;deduction basee modele;computer model;xml language;logique intermediaire;interrogation base donnee;database;logica descripcion;interrogacion base datos;base dato;demostracion automatica;satisfiability;logica defecto;demonstration automatique;intermediate language;recherche information;intermediate logic;xml;logique defaut;description logic;default logic;logica intermediaria;database query;langage xml;lenguaje xml;disjunctive logic programming;krhyper;logique description	We aim to demonstrate that automated deduction techniques, in particular those following the model computation paradigm, are very well suited for database schema/query reasoning. Specifically, we present an approach to compute completed paths for database or XPath queries. The database schema and a query are transformed to disjunctive logic programs with default negation, using a description logic as an intermediate language. Our underlying deduction system, KRHyper, then detects if a query is satisfiable or not. In case of a satisfiable query, all completed paths – those that fulfill all given constraints – are returned as part of the computed models. The purpose of our approach is to dramatically reduce the workload on the query processor. Without the path completion, a usual XML query processor would search the database for solutions to the query. In the paper we describe the transformation in detail and explain how to extract the solution to the original task from the computed models. We understand this paper as a first step, that covers a basic schema/query reasoning task by model-based deduction. Due to the underlying expressive logic formalism we expect our approach to easily adapt to more sophisticated problem settings, like type hierarchies as they evolve within the XML world.	abox;algorithm;automated theorem proving;blocking (computing);computation;correctness (computer science);database schema;description logic;disjunctive normal form;formal system;knowledge representation and reasoning;modal logic;natural deduction;programming paradigm;query language;semantics (computer science);xml schema;xpath	Peter Baumgartner;Ulrich Furbach;Margret Groß-Hardt;Thomas Kleemann	2004		10.1007/978-3-540-30221-6_14	computer simulation;online aggregation;sargable;query optimization;query expansion;web query classification;xml;boolean conjunctive query;computer science;artificial intelligence;query by example;theoretical computer science;database;rdf query language;programming language;view;database schema;algorithm;query language	DB	-21.21176442038722	11.194771810459791	72077
54d002e4346dc88e56bf651a4583edcd65391955	access control for data integration in presence of data dependencies		Defining access control policies in a data integration scenario is a challenging task. In such a scenario typically each source specifies its local access control policy and cannot anticipate data inferences that can arise when data is integrated at the mediator level. Inferences, e.g., using functional dependencies, can allow malicious users to obtain, at the mediator level, prohibited information by linking multiple queries and thus violating the local policies. In this paper, we propose a framework, i.e., a methodology and a set of algorithms, to prevent such violations. First, we use a graph-based approach to identify sets of queries, called violating transactions, and then we propose an approach to forbid the execution of those transactions by identifying additional access control rules that should be added to the mediator. We also state the complexity of the algorithms and discuss a set of experiments we conducted by using both real and synthetic datasets. Tests also confirm the complexity and upper bounds in worst-case scenarios of the proposed algorithms.	access control;algorithm;best, worst and average case;consistency model;data dependency;experiment;functional dependency;graph (discrete mathematics);mediator pattern;referential integrity;synthetic data;synthetic intelligence;system administrator	Mehdi Haddad;Jovan Stevovic;Annamaria Chiasera;Yannis Velegrakis;Mohand-Said Hacid	2014		10.1007/978-3-319-05813-9_14	computer science;theoretical computer science;data mining;database	DB	-26.270845327517232	12.968915812068076	72109
2958fc1582da65935b9302192ec5af03d0d83320	automated deduction techniques for the management of personalized documents	automated deduction;information technology;theorem prover;real world application;first order	This work is about a “real-world” application of automated deduction. The application is the management of documents (such as mathematical textbooks) as they occur in a readily available tool. In this “Slicing Information Technology tool”, documents are decomposed (“sliced”) into small units. A particular application task is to assemble a new document from such units in a selective way, based on the user's current interest and knowledge. It is argued that this task can be naturally expressed through logic, and that automated deduction technology can be exploited for solving it. More precisely, we rely on first-order clausal logic with some default negation principle, and we propose a model computation theorem prover as a suitable deduction mechanism. Beyond solving the task at hand as such, with this work we contribute to the quest for arguments in favor of automated deduction techniques in the “real world”. Also, we argue why we think that automated deduction techniques are the best choice here.	automated theorem proving;computation;first-order predicate;natural deduction	Peter Baumgartner;Ulrich Furbach	2003	Annals of Mathematics and Artificial Intelligence	10.1023/A:1022976016809	computer science;artificial intelligence;theoretical computer science;machine learning;mathematics;automated theorem proving;programming language;information technology;algorithm	SE	-19.584321613622578	13.70972176645028	72571
4ff9830099ceee0afaccd9badb84e4e000ceb877	data-intensive xquery debugging with instant replay	forward backward;relation algebra;relational database;design and implementation;functional language	We explore the design and implementation of Rover, a postmortem debugger for XQuery. Rather than being based on the traditional breakpoint model, Rover acknowledges XQuery's nature as a functional language: the debugger follows a declarative debugging paradigm in which a user is enabled to observe the values of selected XQuery subexpressions. Rover has been designed to hook into Pathfinder, an XQuery compiler that emits relational algebra plans for evaluation on commodity relational database back-ends. The debugger instruments the subject query with fn:trace() calls which, at query runtime, populate database tables with relational representations of XQuery item sequences. Thanks to Pathfinder's loop-lifting compilation strategy, a Rover trace (1) may span multiple XQuery for iteration scopes and (2) allows for interactive debugging sessions that can arbitrarily replay iterations in a unique forward/backward fashion. Since the query runtime as well as the debugger are database-supported, Rover is scalable and supports the observation of very data-intensive XQuery expressions.	algorithmic program debugging;breakpoint;compiler;data-intensive computing;debugger;functional programming;iteration;lambda lifting;population;programming paradigm;relational algebra;relational database;rover (the prisoner);scalability;table (database);xquery	Torsten Grust;Jan Rittinger;Jens Teubner	2007		10.1145/1328158.1328162	relational database;computer science;theoretical computer science;relation algebra;database;programming language;functional programming	DB	-28.69771043150264	16.17154880466011	72848
4651ee5c01992b7264e488139c982e98101efc48	querying with intrinsic preferences	extraction information;base relacional dato;query language;base donnee;logical framework;information extraction;systeme aide decision;database;base dato;automatisation;pregunta documental;user preferences;sistema ayuda decision;prise decision;relational database;automatizacion;lenguaje interrogacion;question documentaire;decision support system;present day;base donnee relationnelle;query;langage interrogation;information system;toma decision;extraccion informacion;systeme information;sistema informacion;automation	The handling of user preferences is becoming an increasingly important issue in present-day information systems. Among others, preferences are used for information filtering and extraction to reduce the volume of data presented to the user. They are also used to keep track of user profiles and formulate policies to improve and automate decision making. We propose a logical framework for formulating preferences and its embedding into relational query languages. The framework is simple, and entirely neutral with respect to the properties of preferences. It makes it possible to formulate different kinds of preferences and to use preferences in querying databases. We demonstrate the usefulness of the framework through numerous examples.	decision problem;defeasible reasoning;information filtering system;information system;logical framework;mathematical optimization;preference elicitation;programming paradigm;query language;relational algebra;relational database;relational model;software propagation;user (computing);user profile	Jan Chomicki	2002		10.1007/3-540-45876-X_5	logical framework;decision support system;relational database;computer science;artificial intelligence;automation;data mining;database;information extraction;information system;query language	AI	-33.48060934329072	9.366171145892359	72947
6213e54ceea91529976dc2b66f901a69de9fd531	run-time adaptivity for search computing	incollection;exact results;query optimization;web service;query evaluation;adaptive query processing	In Search Computing, queries act over internet resources, and combine access to standard web services with exact results and to ranked search services. Such resources often provide limited statistical information that can be used to inform static query optimization, and correlations between the values and ranks associated with different resources may only become clear at query runtime. As a result, search computing seems likely to benefit from adaptive query processing, where information obtained during query evaluation is used to change the way in which a query is executing. This chapter provides a perspective on how run-time adaptivity can be achieved in the context of Search Computing.	computation;interaction;lecture notes in computer science;mathematical optimization;p (complexity);panta rhei;query optimization;relational database management system;springer (tank);vldb;web service;world wide web	Daniele Braga;Michael Grossniklaus;Norman W. Paton	2010		10.1007/978-3-642-19668-3_15	online aggregation;sargable;query optimization;query expansion;web query classification;ranking;computer science;query by example;concept search;data mining;database;rdf query language;web search query;view;information retrieval;query language;search engine;object query language;spatial query	DB	-30.45019824706559	5.76198230991253	73056
8510c0181ffc520fd986b802b9f60db7fe8e2db6	answering queries using views over description logics knowledge bases	query optimization;information integration;de scription logic;data warehousing;difference set;description logic;knowledge base	Answering queries using views amounts to computing the answer to a query having information only on the extension of a set of precomputed queries (views). This problem is relevant in several fields, such as information integration, query optimization, and data warehousing, and has been studied recently in different settings. In this paper we address answering queries using views in a setting where intensional knowledge about the domain is represented using a very expressive Description Logic equipped with n-ary relations, and queries are nonrecursive datalog queries whose predicates are the concepts and relations that appear in the Description Logic knowledge base. We study the problem under different assumptions, namely, closed and open domain, and sound, complete, and exact information on view extensions. We show that under the closed domain assumption, in which the set of all objects in the knowledge base coincides with the set of objects stored in the views, answering queries using views is already intractable. We show also that under the open domain assumption the problem is decidable in double exponential time.	datalog;description logic;intensional logic;knowledge base;mathematical optimization;precomputation;query optimization;question answering;time complexity	Diego Calvanese;Giuseppe De Giacomo;Maurizio Lenzerini	2000			knowledge base;query optimization;description logic;computer science;artificial intelligence;information integration;theoretical computer science;data mining;database;difference set	DB	-25.793485286111117	8.536619441521744	73126
f2a379002af41f6d1a5200f411909719e96a3e49	explaining entailments and patching modelling flaws		Ontology authoring is a sophisticated task and requires domain as well as some amount of background knowledge in formal logic. In fact, it is not only novice users that are commonly faced with comprehension problems with respect to the influence of complex or nested ontological axioms on reasoning. We provide practical insights into the development of tableau-based methods for explaining the key inference services, namely unsatisfiability, subsumption, and non-subsumption as well as techniques for patching ontologies in order to establish a user desired entailment.	method of analytic tableaux;ontology (information science);subsumption architecture	Thorsten Liebig;Stephan Scheele	2008	KI		mathematical analysis;kernel (image processing);polygon;mathematics;convolution;basis function;finite set	AI	-19.74550614279829	6.091593254529271	73189
a59e27e4b6bd50c8dcd865d77bfaf2ca4d460d33	schema-aware labelling of xml documents for efficient query and update processing in semcrypt	xml document		xml	Katharina Grün;Michael Karlinger;Michael Schrefl	2006	Comput. Syst. Sci. Eng.		database;xml schema (w3c);streaming xml;information retrieval;document structure description;relax ng;computer science;xml schema;xml schema editor;xml validation;efficient xml interchange	DB	-33.28987713000542	7.547908804169728	73326
60e8531ca7f6815d89e6b4a2e9f62543bde93a24	on matrix representations of participation constraints	matrix representation;functional dependency;conference item;conceptual modelling;database design	We discuss the existence of matrix representations for generalised and minimum participation constraints which are frequently used in database design and conceptual modelling. Matrix representations, also known as Armstrong relations, have been studied in literature e.g. for functional dependencies and play an important role in example-based design and for the implication problem of database constraints. The major tool to achieve the results in this paper is a theorem of Hajnal and Szemerédi on the occurrence of clique graphs in a given graph.	andrás hajnal;armstrong's axioms;clique problem;conceptual schema;database design;functional dependency;relational database	Sven Hartmann;Uwe Leck;Sebastian Link	2009		10.1007/978-3-642-04947-7_10	matrix representation;computer science;theoretical computer science;data mining;database;functional dependency;database design	AI	-25.636935960448554	10.823488424885216	73520
759471b25150c78c61478efd6325e03b54864fad	inside relational databases with examples in access (2. ed.)	relational database		relational database	Mark Whitehorn;Bill Marklyn	2007		10.1007/978-1-84628-687-2	database theory;sql;relational database management system;nested set model;relational model;relational database;computer science;database model;data mining;database;information retrieval;object-relational impedance mismatch;database design	DB	-31.18356190061723	9.03455721413677	73563
5604052b5873b7e67f8d3f5140887e0f8a23e1dc	top-down systems design	derived relation;defined relation;view;relational database;explicit updates;update methods;snapshot;dynamic derived relation;implicit updates;copy;base relations;static derived relation	Effective systems design is premised upon a thorough knowledge of the whole system. Such knowledge can best be acquired by gaining insight via an hierarchical approach. Logically, this should begin at the highest level and proceed downward because this is how a system is conceived. This viewpoint will be reinforced by a more detailed consideration of the TOP-DOWN approach highlighting the advantages and reconciling the disadvantages as perceived by this author.		Ray D. Wheeler	1979		10.1145/800177.810040	computer science;theoretical computer science;data mining;database	EDA	-28.031357341679666	10.036438806648206	73634
6cbca2f7369003f75e3dbbe4778e3e9482dc1f10	mapeamento de definições xml schema para sql: 1999	xml schema;content analysis	This paper presents an algorithm to map W3C XML Schema definitions into SQL:1999 database schemas. The choice of SQL:1999 was due to its facilities, especially the ability to create abstract types. The approach is founded on a content analysis carried out on a collection of 199 XSDs (XML Schema Definitions) obtained from the Web in order to identify which constructs are effectively used in practice and to determine proper mapping strategies. The algorithm handles key aspects such as order preservation and the cardinality of the elements, and uses the inlining technique to reduce data fragmentation in the created tables.	abstract type;algorithm;data definition language;fragmentation (computing);inline expansion;sql;sql:1999;world wide web;xml schema	Patrícia Martins;Alberto H. F. Laender	2005			data mining;database;streaming xml;xml schema (w3c);document structure description;relax ng;xml schema;xml validation;computer science;xml schema editor;sql:1999	Web+IR	-33.40090989036568	8.010429771052786	73732
c95f166c9434b2cbbc9a94a86fd85a1296ad290b	performance of a composite attribute and join index	base relacional dato;join index;relational databases data structures;structural integrity constraints;etude experimentale;accesibilidad;simulation;interrogation base donnee;database;interrogacion base datos;attribute index;performance of systems;simulacion;relational database;indexing terms;structural integrity constraints attribute index composite index b sub c tree b sup tree join index tuples database common data values;accessibility;tuples;data structures;indexation;analyse performance;performance analysis;base donnee relationnelle;composite index;relational databases;systeme gestion base donnee;common data values;structural integrity;access method;sistema gestion base datos;database management system;b sub c tree;data structure;estudio experimental;database query;relational databases data structures application software database languages data models dispersion artificial intelligence computer aided manufacturing cadcam;accessibilite;b sup tree;analisis eficacia	The use of a composite index known as the B/sub c/-tree is presented; it is based on the concept of the B/sup +/-tree and serves the dual purpose of an attribute and join index and indirectly implements the link sets. The leaf node of the B/sub c/-tree incorporates in each leaf node a reference to all tuples in the database that share common data values of a shared domain. In addition to improving the performance of the join and selection operations, the composite index facilitate the enforcement of structural integrity constraints. The author also presents the results of simulations that compare the performance of this approach with the simple join technique. The proposed approach, in the case of the simulated database, is seen to provide better performance for an average domain value size of greater than between 2 and 4 bytes. >		Bipin C. Desai	1989	IEEE Trans. Software Eng.	10.1109/32.21741	data structure;relational database;computer science;data mining;database;programming language;engineering drawing	DB	-28.333751705201774	4.884369428406907	73803
196d549b3ba9da61db452a323995e15f88f79c22	a basis for the constructive approach to programming		Although programming is a difficult and creative activity, useful strategies and heuristics exist for solving programming problems. We analyse some of the most fundamental and productive among them; their knowledge and conscious application should help the programmers in constructing programs, both by stimulating their thinking and by helping them to recognise classical situations. The precise framework for the. analysis is provided by the specification language Z. For editorial reasons the description in some sections of this paper has had to be curtailed.	heuristic (computer science);programmer;specification language;z notation	Bertrand Meyer	1980			constructive;natural language processing;inductive programming;specification language;procedural programming;programming paradigm;heuristics;symbolic programming;artificial intelligence;programming domain;computer science	AI	-21.934828093389896	14.770456857961864	73929
c4b2cc3d44883fc2a415f07629f57d9e3c6f8cb4	xml metadata generation for vector images			vector graphics;xml	Byungwoo Kim;Jong P. Yoon	2003			streaming xml;xml framework;xml catalog;information retrieval;metadata;database;xml encryption;metadata repository;xml schema editor;computer science;efficient xml interchange	HPC	-33.61063030033538	7.392828647275431	73961
4c37cb9d156e6197d66a531cdea54b2568b86caf	incorporating data types in an extensible database architecture	data type	The limited set of data types and the difficulty of incorporating new ones in current DBMSs prevent many applications from efficiently using a DBMS to store data. We discuss the merits of introducing externally defined types (EDTs) to bridge the gap between the data structures of the application programming language and the data model of the database. We show that the DBMS needs to know the set of functions to be called on an EDT to permit easy use of EDTs both in programming languages and by the DBMS, and we propose ways to make the definition and implementation of new types in the DBMS easier on both the DBMS and the type implementor. Functions are central to our approach, as they may be invoked from within internal DBMS components by suitable extensions. We examine how to create access paths to support externally denned predicate functions by materialization or physical clustering, and how query transformations may help reduce the number of different access methods and storage structures. By treating EDTs as abstractions, the amount of mechanism which must be added to the DBMS can be kept small.		Paul F. Wilms;Peter M. Schwarz;Hans-Jörg Schek;Laura M. Haas	1988			data independence;sargable;computer science;data mining;database;programming language;component-oriented database	DB	-30.11814041021343	10.504252618905669	74332
221b5a73bf479e83d6b8b7fa2dd8cd41c16f580b	an efficient implementation of a flexible xpath extension	query language;settore inf 01 informatica;querying xml documents;xml documents compression;xml documents representation;efficient implementation;indexation;xml document;xml documents indexing	In this paper we present an efficient implementation of different flexible queries (that constitute an extension of the XPath query language) to be executed on XML documents represented by using a recent structure called XML Wavelet Tree (XWT) [3]. A XWT represents the XML document compressed by using only about 35% of its original size, but it also provides some implicit self-indexing features that help to obtain not only efficient implementations of standard XPath queries, but also of extended ones. This is shown based on the implementation of the flexible structure based constraints, below and near [7].	experiment;query language;query plan;wavelet tree;xml;xpath	Nieves R. Brisaboa;Ana Cerdeira-Pena;Gonzalo Navarro;Gabriella Pasi	2010			well-formed document;xml catalog;xml validation;xml encryption;simple api for xml;xml;xml schema;streaming xml;computer science;xpath 2.0;document type definition;document structure description;xml framework;data mining;xml database;xml schema;database;xml signature;xml schema editor;cxml;information retrieval;query language;efficient xml interchange	DB	-31.72703763036078	5.765382810122882	74600
e32db7b652058008e73f2c6cc46081a635873a98	grouping queries with sv-semantics in preference sql		Preference database queries become more and more important in Data Warehousing or Decision Support Systems. In these environments the standard SQL GROUP BY operation with aggregate functions is extensively used in formulating queries. In this paper, we focus on the novel GROUPING functionality of Preference SQL which substantially extends the common aggregation features of SQL GROUP BY. It fully supports the Substitutable Values semantics for preference queries, enriched by comfortable aliasing mechanisms. This yields to an increased intuitive readability of complex queries, which in turn can reduce clearly the cognitive load for OLAP programmers. In addition we show how a correlated subquery in SQL can be written simpler and evaluated faster using the GROUPING clause.	aggregate data;aggregate function;aliasing;correlated subquery;database;decision support system;online analytical processing;programmer;sql;systemverilog;xslt/muenchian grouping	Markus Endres;Patrick Roocks;Manuel Huber;Werner Kießling	2014			sql;online analytical processing;correlated subquery;computer science;decision support system;database;null (sql);data warehouse;query by example;spatial query	DB	-28.83661885679126	6.534125195519802	74624
a5723384fa5b4913c104dfb625c734269c3e17cb	computing repairs of inconsistent dl-programs over el ontologies		Description Logic (DL) ontologies and non-monotonic rules are two prominent Knowledge Representation (KR) formalisms with complementary features that are essential for various applications. Nonmonotonic Description Logic (DL) programs combine these formalisms thus providing support for rule-based reasoning on top of DL ontologies using a well-defined query interface represented by so-called DL-atoms. Unfortunately, interaction of the rules and the ontology may incur inconsistencies such that a DL-program lacks answer sets (i.e., models), and thus yields no information. This issue is addressed by recently defined repair answer sets, for computing which an effective practical algorithm was proposed for DL-LiteA ontologies that reduces a repair computation to constraint matching based on so-called support sets. However, the algorithm exploits particular features of DL-LiteA and can not be readily applied to repairing DL-programs over other prominent DLs like EL. Compared to DL-LiteA, in EL support sets may neither be small nor only few support sets might exist, and completeness of the algorithm may need to be given up when the support information is bounded. We thus provide an approach for computing repairs for DL-programs over EL ontologies based on partial (incomplete) support families. The latter are constructed using datalog query rewriting techniques as well as ontology approximation based on logical difference between EL-terminologies. We show how the maximal size and number of support sets for a given DL-atom can be estimated by analyzing the properties of a support hypergraph, which characterizes a relevant set of TBox axioms needed for query derivation. We present a declarative implementation of the repair approach and experimentally evaluate it on a set of benchmark problems; the promising results witness practical feasibility of our repair approach.		Thomas Eiter;Michael Fink;Daria Stepanova	2016	J. Artif. Intell. Res.	10.1613/jair.5047	theoretical computer science;data mining;mathematics;algorithm	AI	-23.831891105242796	8.336939451032048	74741
1bfe8fc0448175da3a00ddbccdea243253927620	xml matchers: approaches and challenges	xml schema;xsd;dtd;xml source clustering;schema matching;uncertainty management in xml matchers	Schema Matching, i.e. the process of discovering semantic correspondences between concepts adopted in different data source schemas, has been a key topic in Database and Artificial Intelligence research areas for many years. In the past, it was largely investigated especially for classical database models (e.g., E/R schemas, relational databases, etc.). However, in the latest years, the widespread adoption of XML in the most disparate application fields pushed a growing number of researchers to design XML-specific Schema Matching approaches, called XML Matchers, aiming at finding semantic matchings between concepts defined in DTDs and XSDs. XML Matchers do not just take wellknown techniques originally designed for other data models and apply them on DTDs/XSDs, but they exploit specific XML features (e.g., the hierarchical structure of a DTD/XSD) to improve the performance of the Schema Matching process. The design of XML Matchers is currently a well-established research area. The main goal of this paper is to provide a detailed description and classification of XML Matchers. We first describe to what extent the specificities of DTDs/XSDs impact on the Schema Matching task. Then we introduce a template, called XML Matcher Template, that describes the main components of an XML Matcher, their role and behavior. We illustrate how each of these components has been implemented in some popular XML Matchers. We consider our XML Matcher Template as the baseline for objectively comparing approaches that, at first glance, might appear as unrelated. The introduction of this template can be useful in the design of future XML Matchers. Finally, we analyze commercial tools implementing XML Matchers and introduce two challenging issues strictly related to this topic, namely XML source clustering and uncertainty management in XML Matchers.	academy;algorithm;artificial intelligence;baseline (configuration management);cluster analysis;correctness (computer science);data model;data modeling;data visualization;database model;database schema;diagram;dictionary;domain-specific language;graphical user interface;interaction;interoperability;modeling language;real life;relational database;thesaurus;xml namespace;xml schema	Santa Agreste;Pasquale De Meo;Emilio Ferrara;Domenico Ursino	2014	Knowl.-Based Syst.	10.1016/j.knosys.2014.04.044	xml catalog;xml validation;simple api for xml;xml;relax ng;xml schema;streaming xml;computer science;document type definition;document structure description;xml framework;data mining;xml database;xml schema;database;xml signature;xml schema editor;information retrieval;efficient xml interchange	DB	-33.049596657010945	5.901994177307339	75120
bfab29620df7264ed93497acff987896bcccd3a5	word segmentation based on database semantics in nchiql	conceptual model;satisfiability;domain knowledge;word segmentation;natural language;disambiguation;database query;natural language processing	In this paper a novel word-segmentation algorithm is presented to delimit words in Chinese natural language queries in NChiql system, a Chinese natural language query interface to databases. Although there are sizable literatures on Chinese segmentation, they cannot satisfy particular requirements in this system. The novel word-segmentation algorithm is based on the database semantics, namely Semantic Conceptual Model (SCM) for specific domain knowledge. Based on SCM, the segmenter labels the database semantics to words directly, which eases the disambiguation and translation (from natural language to database query) in NChiql.	algorithm;database;dictionary;natural language user interface;requirement;text segmentation;word-sense disambiguation	Xiaofeng Meng;Shuang Liu;Shan Wang	2000	Journal of Computer Science and Technology	10.1007/BF02948870	natural language processing;language identification;text segmentation;data control language;computer science;conceptual model;database;rdf query language;natural language;programming language;view;database schema;domain knowledge;database design;computational semantics;satisfiability	DB	-32.901273727655	8.70998903569845	75533
656d45937c3ddfbf9d345cc985dd11154a897f0f	mapping considerations in the design of schemas for the relational model	functional dependencies;design process;relational databases interference personnel design methodology process design software engineering;interferences;external schema;independent components;logical database design;mapping functions;relational database;functional dependency;conceptual schema;design method;relational model;model development;database design;relational database mode;relational database mode conceptual schema external schema functional dependencies independent components interferences logical database design mapping functions;independent component;design methodology	The typical design process for the relational database model develops the conceptual schema and each of the external schemas separately and independently from each other. This paper proposes a new design methodology that constructs the conceptual schema in such a way that overlappings among external schemas are reflected. If the overlappings of external schemas do not produce transitivity at the conceptual level, then with our design method, the relations in the external schemas can be realized as a join over independent components. Thus, a one-to-one function can be defined for the mapping between tuples in the external schemas to tuples in the conceptual schema. If transitivity is produced, then we show that no such function is possible and a new technique is introduced to handle this special case.	conceptual schema;database model;one-to-one (data model);relational database;relational model;vertex-transitive graph	Sabah S. Al-Fedaghi;Peter Scheuermann	1981	IEEE Transactions on Software Engineering	10.1109/TSE.1981.234512	design methods;computer science;conceptual schema;theoretical computer science;data mining;database;functional dependency	DB	-31.168848256765227	11.894548390538397	75537
c25907101bb5476557f32554ff4217d0f33b369a	types of tasks in database schema modelling and their support by a knowledge-based tool.	knowledge base		database schema;knowledge-based systems	Zh. Angelov;D. Obretenov	1990			schema migration;semi-structured model;computer science;knowledge management;conceptual schema;data mining;database;database schema	DB	-32.054248037770094	10.220662264418978	75583
6a8c35930900297898b18ee6709be5a4d1463e31	answer set programming ? a domain in need of explanation: a position paper	declarative programming;answer set programming	This paper describes the problems with debugging tools for answer set programming, a declarative programming paradigm. Current approaches are difficult to use on most applications due to the considerable bottlenecks in communicating the reasons to the user. In this paper we examine the reasons for this and suggest some possible future directions.	answer set programming;debugging;declarative programming;np (complexity);programming paradigm;stable model semantics	Martin Brain;Marina De Vos	2008				HCI	-21.627866279266925	14.492609144309784	75709
099c7a0ec8900658838e7721bbf2ae289d205d62	"""review of """"problem-solving methods in artificial intelligence by nils j. nilsson"""", mcgraw-hill pub"""	artificial intelligent;problem solving method	This book is not a survey on theorem proving programs, but the description of a program developed from 1960 to 1965. In the first part there are some generalities on artificial intelligence, and in the second part, some logical explanations, necessary for the comprehension of the program. The third part describes the program. The program has three important features:-it is general. It can study more than one formal system. It receives as data the inference rules and the axioms of the formal system which it must study.-it is capable of invention. It can work without knowing the theorem to be proven. It tries to find interesting theorems; it has only the definition of the interest of a theorem.-it proves theorems, but also metatheorems which are new productions and meta-metatheorems which are new means to get productions. This feature has a great heuristic value. This textbook explains the theoretical ideas underlying problem-solving by heuristically guided, trial-and-error search processes. These search methods are explained by the use of a uniform vocabulary, and several theoretical results about the properties of heuristic search are presented. Several simple example problems, puzzles, and games are used to illustrate the techniques. The author refers to instances in which these same techniques have been successfully applied to problems much more complex than the example problems in his book. The book also includes three chapters that deal with resolution-based theorem-proving in the predicate calculus and its applications to problem solving. Each chapter contains exercises and a section on bibliographical and historical remarks that list some of the more important references related to the subject of the chapter.	artificial intelligence;automated theorem proving;first-order logic;formal system;heuristic;list comprehension;problem solving;tree-meta;vocabulary	W. W. Bledsoe	1971	SIGART Newsletter	10.1145/1056578.1056583	computer science;artificial intelligence;operations research;cognitive science	AI	-21.58356507159065	14.481223556669404	75756
e2a54c51c6b0872a05aaf7e6784d6297465f2a5c	intelligent safety verification for pipeline based on evalpsn	defeasible deontic reasoning;safety verification;evalpsn;pipeline valve control	We have developed an annotated logic program called an Extended Vector Annotated Logic Program with Strong Negation(abbr. EVALPSN), which can deal with defeasible deontic reasoning and contradiction. We have already applied EVALPSN to safety verification and control such as railway interlocking safety verification. In this paper, we show pipeline valve safety verification to avoid liquid mixture accidents with a simple example for brewery pipeline control.		Kazumi Nakamatsu;Kenji Kawasumi;Atsuyuki Suzuki	2005			engineering;artificial intelligence;computer security;algorithm	EDA	-22.248439309493627	16.522389772516085	75843
c1b0175924b20ea362d925fe485b9e9025f52727	posse: a framework for optimizing incremental view maintenance at data warehouse	computer maintenance;gestion informacion;base donnee repartie;distributed database;almacenamiento informacion;base repartida dato;spectrum;information organization;organizacion informacion;information storage;maintenance informatique;incremental view maintenance;information management;organisation information;stockage information;information system;gestion information;data warehouse;systeme information;sistema informacion	We propose the Posse 1 framework for optimizing incremen-tal view maintenance at data warehouses. To this end, we show how for a particular method of consistent probing it is possible to have the power of SQL view queries with multiset semantics, and at the same time have available a spectrum of concurrency from none at all as in previously proposed solutions to the maximum concurrency obtained by issuing all probes in parallel. We then show how optimization of the probing process can be used to select various degrees of concurrency for the desired tradeoos of concurrency against processing cost and message size.	concurrency (computer science);concurrent algorithm;correctness (computer science);materialized view;mathematical optimization;optimizing compiler;query plan;sql	Kevin O'Gorman;Divyakant Agrawal;Amr El Abbadi	1999		10.1007/3-540-48298-9_11	spectrum;timestamp-based concurrency control;optimistic concurrency control;isolation;computer science;data warehouse;data mining;database;distributed computing;information management;distributed database;information system;distributed concurrency control	DB	-26.85940753543208	4.971633751537621	76006
0e6933bf8f66a6c3759562e521ca5d766d74c034	storing metagraph model in relational, document-oriented, and graph databases				Valeriy M. Chernenkiy;Yuriy E. Gapanyuk;Yuriy Kaganov;Ivan Dunin;Maxim Lyaskovsky;Vadim Larionov	2018				DB	-30.934148286984165	7.819466333846711	76047
8fc608157954c9e8f272d1f31cf4c300bb63f023	the use of alternative data models in data warehousing environments	thesis	........................................................................................................................................ ii Acknowledgements...................................................................................................................... iv Table of	approximate computing;approximation algorithm;care-of address;data integrity;data model;disk space;materialized view;ptc integrity;principle of good enough;programmer;query optimization;relational database management system;relational model	Victor Gonzalez Castro	2009			data science;data mining;database	DB	-31.06993964927484	8.439965079704539	76211
d1df747b592c5cf6382da448657c2d5901bf3147	object oriented design and maintainable code: a relational perspective				Christina Davies;Alan Curtis;Jill Doake;Brian Lazell	1996			object-oriented design;knowledge management;database;computer science	HCI	-31.409646057630958	10.85542432170637	76336
882f11eff76e53e6b7bdb7d7c20bf75066cf7a3a	the generation of continuous semantic constraints from semantic propositions	semantic proposition;continuous semantic constraint	"""Language comprehension is an exceedingly complex process which requires the extensive use of many di f ferent kinds of information in order to be successfully accomplished. One potent ial ly very important type of information which has to date been largely ignored is the degree to which possible interpretations are sensible. While the sensibleness of candidate interpretations has long been recognized to be important, sensibleness has usually been treated as if it were an all-or-none property. However, it is clear that many things are more-or-less sensible and, therefore, the relat ive sensibleness of alternative interpretations may well be extremely useful information. For example, Oden (1977) has argued that degree of sensibleness information is required in order to disambiguate sentences to obtain the meaning that people normally do and has proposed language processing mechanisms which would use this information. The degree of sensibleness of an interpretat ion depends on the degree to which the semantic constraints of that interpretat ion are sat is f ied. Therefore, to account for the continuous nature of sensibleness, semantic constraints must be fuzzy restr ict ions (Zadeh, 1975). A semantic constraint w i l l be defined to be a function associated with a part icular semantic relat ion which specif ies, for every combination of semantic elements which may enter into that re la t ion, the degree of sensibleness of the resultant semantic structure. The present paper outlines how such semantic constraints may be generated from the kinds of knowledge already represented in current semantic memory models (e .g . , Norman & Rumelhart, 1975), plus the fuzzy predicates and operations which w i l l be necessary in order to handle other problems l ike the continuousness of subjective class membership. Defining semantic constraints to be functions makes it natural to think of complex semant i c constraints as being compositions of simpler constraints. Furthermore, since semantic constraints are considered to be bound to part icular semantic relat ions, the decomposition of a constra int may be expected to paral lel the decompos i t ion of i t s associated semantic re lat ion. This appears to be what happens in most cases, but there are certain """"configural constraints"""" which do not seem to be derivable from component constraints corresponding to pr imit ive semantic relations (see Oden, 1977, for deta i ls ) . However, such configural constraints seem to be re lat ive ly exceptional and, consequently, semantic constraints w i l l s t i l l be """"cognit ively economical."""" More importantly, it w i l l be argued below that elementary (non-composed) semantic constraints, whether d i rect ly associated with pr imit ive semant i c relations or conf igural, are based upon specific semantic propositions which would be in semantic memory anyway. In a fundamental sense, a l l knowledge is constraining. For example, knowing that it is -15° today affects the sensibleness of the statement """"Maxine went swimming in Lake Mendota th is morning."""" More generally useful knowledge specif ies information about the normal and/or necessary properties of things which may enter into pa r t i cular case relations with part icular verbs. The most elementary knowledge of th is sort (select ional restr ic t ions) is often considered to be part of the basic meaning of the verb. However, the more interesting semantic constraints are those which are based on much less elementary knowledge, such as that only people normally drive trucks, which we might represent as:"""	database engine tuning advisor;elementary;fuzzy logic;internet key exchange;linear algebra;maxine;resultant;sorting algorithm	Gregg C. Oden	1977			semantic interoperability;semantic similarity;semantic computing;semantic integration;semantic search;semantic grid;knowledge management;social semantic web;semantic compression;semantic equivalence	AI	-19.531021456419673	6.639993384912708	76339
08917c140e9ae60dedef918d4640c1d8b801023b	approximate joins for data-centric xml	databases;xml data;data integrity;approximation algorithms;sorting;tree edit distance;tree data structures;runtime;windowed pq grams;data sources;transforms;xml;ordered tree matching;approximation methods;xml tree data structures;join matches elements;data integration applications;xml sorting computer science application software data analysis internet polynomials shape partitioning algorithms;approximate join strategies;windowed pq grams data centric xml data integration applications join matches elements data sources xml data approximate join strategies ordered tree matching;data centric xml	In data integration applications, a join matches elements that are common to two data sources. Often, however, elements are represented slightly different in each source, so an approximate join must be used. For XML data, most approximate join strategies are based on some ordered tree matching technique. But in data-centric XML the order is irrelevant: two elements should match even if their subelement order varies. In this paper we give a solution for the approximate join of unordered trees. Our solution is based on windowed pq-grams. We develop an efficient technique to systematically generate windowed pq-grams in a three-step process: sorting the unordered tree, extending the sorted tree with dummy nodes, and computing the windowed pq-grams on the extended tree. The windowed pq-gram distance between two sorted trees approximates the tree edit distance between the respective unordered trees. The approximate join algorithm based on windowed pq-grams is implemented as an equality join on strings which avoids the costly computation of the distance between every pair of input trees. Our experiments with synthetic and real world data confirm the analytic results and suggest that our technique is both useful and scalable.	approximation algorithm;computation;dummy variable (statistics);experiment;grams;graph edit distance;join (sql);n-gram;relevance;scalability;sorting;synthetic intelligence;tree (data structure);window function;xml	Nikolaus Augsten;Michael H. Böhlen;Curtis E. Dyreson;Johann Gamper	2008	2008 IEEE 24th International Conference on Data Engineering	10.1109/ICDE.2008.4497490	hash join;recursive join;xml;computer science;sorting;theoretical computer science;data integrity;data mining;database;tree;approximation algorithm	DB	-29.55818044930975	4.996164860758313	76518
d7d2596bb93e2e3e479361980b7b0abef6d3d14f	an entity - relationship framework for information resource management	information resource management;entity relationship	We outline a framework for information resource management based on the relational and entity—relationship models of data base management. They are enlarged to encompass the variety of information resources now found in many organizations — including data files, data analysis procedures, text files, decision models, expert systems and knowledge bases, and human information resources (both individuals and groups).	entity–relationship model	Robert W. Blanning	1988	Information & Management	10.1016/0378-7206(88)90042-0	information technology management;entity–relationship model;data management;computer science;knowledge management;personal information management;management information systems;data mining;database;risk management information systems;structure of management information;information management;human resource management system;information system	HPC	-32.841666775172676	10.262461908485003	76532
6ac8842674da35ad218489839a363febf834f98c	using views in a multilevel secure database management system	remuneration;abstracts data models access control database systems remuneration;abstracts;database systems;access control;database management system;multilevel security;data models	The use of database views in database management systems that enforce user level discretionary and nondiscretionary access control policies is discussed. This discussion involves several issues such as how should views be classified?, what types of mechanisms should be used to define views?, etc. Mapping between views, view updating, and aggregation and inference problems are also discussed.	access control;database;management system;multilevel security;view (sql)	Billy G. Claybrook	1983	1983 IEEE Symposium on Security and Privacy	10.1109/SP.1983.10009	data modeling;database theory;computer science;access control;data mining;database;world wide web;database schema;database testing;database design	Security	-31.629889220293734	10.80364644744374	76558
e1e44546a62d520b1dd61094a38f94128331068d	model-based diagnosis meets error diagnosis in logic programs		A lot of attention has been paid in the last years in the logic programming community to the design of automatic declarative error diagnosers (after Shapiro's seminal work [10]). Such diagnosers are declarative in the sense that they do not need any understanding of the computational behavior of the program but they need to know only the intended interpretation of the program (the set of answers that the program should compute). Information on such an intended behavior is obtained querying an oracle, usually corresponding to the	computation;declarative programming;interpretation (logic);logic programming;need to know;oracle database;user error	Luca Console;Gerhard Friedrich;Daniele Theseider Dupré	1993		10.1007/BFb0019402	computer science;theoretical computer science;algorithm	AI	-20.66813256463127	14.56165011148419	76665
7fd2f04a48a9f45c1530cdd5f20efed215415a79	probabilistic inductive logic programming	inductive logic programming;artificial intelligent;first order;machine learning;statistical relational learning;probabilistic reasoning	Probabilistic inductive logic programming aka. statistical relational learning addresses one of the central questions of artificial intelligence: the integration of probabilistic reasoning with machine learning and first order and relational logic representations. A rich variety of different formalisms and learning techniques have been developed. A unifying characterization of the underlying learning settings, however, is missing so far. In this chapter, we start from inductive logic programming and sketch how the inductive logic programming formalisms, settings and techniques can be extended to the statistical case. More precisely, we outline three classical settings for inductive logic programming, namely learning from entailment, learning from interpretations, and learning from proofs or traces, and show how they can be adapted to cover state-of-the-art statistical relational learning approaches.	artificial intelligence;inductive logic programming;inductive reasoning;machine learning;relational algebra;statistical relational learning;tracing (software)	Luc De Raedt;Kristian Kersting	2008		10.1007/978-3-540-78652-8_1	natural language processing;multi-task learning;algorithmic learning theory;inductive bias;statistical relational learning;sequence learning;horn clause;computer science;artificial intelligence;machine learning;functional logic programming;first-order logic;computational logic;inductive transfer;probabilistic logic;inductive programming;deductive reasoning;prolog;logic programming;probabilistic logic network;algorithm	AI	-19.360260247457987	11.597855284920524	76749
200b21e9dd28122d02eea4ef9e27bca62cd40837	external schemas of entity-relationship based data base management systems	data base management system;entity relationship		entity–relationship model;management system	Tok Wang Ling	1988			knowledge management;data mining;database;structure of management information	DB	-32.43324817794702	10.089361990887653	77132
75874869b31648af28ba0e0474213f04a2b4bf78	on the utility of cfdi		We consider the description logic CFDI∀− nc , a feature-based dialect that allows capturing value restrictions, a variety of identification constraints, and unqualified feature inverses. We introduce PTIME algorithms for various reasoning tasks in this logic, such as knowledge base consistency and logical implication and discuss the necessity of restrictions over CFDInc to maintain tractability. We then show how CFDI∀− nc ’s modeling capabilities make it suitable for capturing relational and object-relational data sources (including of n-ary relations) in a natural way. In addition, we show that CFDI∀− nc can simulate reasoning in DL-Litecore. We also discuss an approach to capturing a limited variant of role hierarchies within CFDI∀− nc .	algorithm;description logic;knowledge base;object-relational database;p (complexity);relational model;simulation;value restriction	David Toman;Grant E. Weddell	2015				AI	-27.69340888523343	9.772320847741476	77214
e82b101fd632a84d128d4ce68d7eda4359a45f30	logical foundation for object-oriented xml database	object oriented;xml database	XML正在成为Internet上数据描述和交换的标准．面向对象的方法具有很强的建模能力，例如继承、非单调继承、多态性、复杂数据结构等．将面向对象的特征引入到XML中，可以提高XML语言的建模能力．用继承扩展XML的模式语言DTD支持元素继承、非单调多重继承、重载、阻断、多态和冲突处理机制．XML，RL是基于高级数据模型的以规则为基础的XML查询语言，用面向对象的特征扩展XML-RL支持多态元素、包含元素．系统地给出面向对象的XML数据库的基于逻辑的语法和语义．	xml database	Xiaolin Zhang;Guoren Wang;Huilin Liu	2006	Journal of Computer Research and Development		plain old xml;xml catalog;xml validation;xml encryption;xml base;simple api for xml;xml;streaming xml;computer science;document type definition;document structure description;database model;xml framework;data mining;xml database;xml schema;database;xml signature;programming language;object-oriented programming;database schema;xml schema editor;information retrieval;database design;efficient xml interchange	DB	-31.553954408093897	9.298769454605491	77518
995c89c53a26d1857eed3cb4c9a0d7e84d2c6109	the attribute hypergraph model - toward a unified view of software objects.				Ying Yang;Frances Hunt	1990			attribute domain;computer science;theoretical computer science;data mining;database	DB	-30.371376118411085	11.79167663048267	77531
bd191970e0d8581002347afb18179d87ca4e02d0	optimizing schema-last tuple-store queries in graphd	tuple store;graph store;database;query optimization;relational database;schema last;object oriented;automatic indexing;triple store	Current relational databases require that a database schema exist prior to data entry and require manual optimization for best performance. We describe the query optimization techniques used by graphd, the schema-last, automatically indexed tuple-store which supports freebase.com, a large world-writable database. Graphd is a log-structured store with a query optimizer based on a functional operator tree over the domain of sorted integer sets which accumulate naturally as tuples are appended to the store. We demonstrate that a set-based optimizer can deliver performance that is roughly comparable to traditional RDBMS query optimization techniques applied to a fixed schema.	database schema;freebase;mathematical optimization;optimizing compiler;query optimization;relational database management system	Scott M. Meyer;Jutta Degener;John Giannandrea;Barak Michener	2010		10.1145/1807167.1807283	sargable;query optimization;boolean conjunctive query;semi-structured model;relational database;computer science;data mining;database;object-oriented programming;view;database schema;graph database;information retrieval	DB	-29.507124660693773	6.0980583119507905	77690
b681460b476dc0ba6d200aaf8b9c10a1ddc7abb0	the hcc-tree: an efficient index structure for object oriented databases	index structure;object oriented database	Object oriented database systems, in contrast to traditional relational database systems, allow the scope of a query against a class to be either the class itself or all classes in the class hierarchy rooted at the class. If object oriented databases have to achieve acceptable performance levels against such queries, we need indexes that support efficient retrieval of instances from a single class as well as from all classes in a class hierarchy. In this pa per, we propose a new index structure called h&-tree (hierarchy class Chain tree) that sup ports both kinds of retrieval &ciently. Moreover, the update cost of the index structure is bounded by the h$ght of the h&-tree. We have implemented hcC-trees along with Htrees and CH-trees (two other index structures that have been proposed in the literature) and report a detailed performance analysis of the three structures. The performance study reveals that h&-trees perform much better than the other two structures under most circumstances. The balanced behaviour of hcC-tree under all kinds of queries and in the presence of updates shows that it is a promising index structure for the future. Pcmbiarion to copy without fee all or pori of thb material ti gnanted provided that the copier are not made or di&ibuted jor direct commercial adrantapc, the VLDB copMht notice and the title of the publication and itr date appear, and notice ia piwen that cop&l in bp permiwion of the V&p La-c Data Bare Bndowment. To copy othcrwiw, or to -b&h, rcqminr a jee and/or special permkkn jwm the Endowment. Proceed&p of the 20th VLDB Conference Santiago, Chile, 1994 S . Seshadri Computer Science Dept. IIT, Bombay 400 076 India seshadriOcse.iitb.ernet .in	b+ tree;class hierarchy;cluster analysis;computer science;constraint handling rules;database index;dex;encrypted key exchange;experiment;integrated information theory;oracle internet directory;relational database;relevance;requirement;silicon controlled rectifier;vldb;writing commons	B. Sreenath;S. Seshadri	1994			computer science;database	DB	-29.464389227331424	5.22714751300622	77853
d934419c4101ef9ecc650171fd7b0c8d76d87ff1	when owl met dl-lite	description logic;query language	Recent research in the area of ontology representation for the Semantic Web has led to several different language proposals. Among the others, it led to the standardization of OWL, on one hand, and to the emergence of the DL-Lite family of Description Logics, on the other hand. These two outcomes mainly differ on their objective: while OWL aims to be a standard, and as such, it is tailored towards expressivity, the main goal of the languages in the DL-Lite family is to allow accessing huge amount of data, by maintaining tractability, and delegating query processing to a DBMS. In this paper we show how DL-Lite can meet OWL. Specifically, we focus on DL-LiteA, the language in the DL-Lite family that is closest to OWL, and introduce the SparSQL language, a concrete epistemic query language, inspired by both SQL and SPARQL, that allows posing expressive tractable queries over DL-LiteA ontologies. Finally, we introduce the main novel DL-LiteA features beyond OWL. The capability of handling such features, together with the SparSQL queries, are some of the new functionalities recently implemented in the MASTRO system.	adobe flash lite;cobham's thesis;database;description logic;emergence;experiment;ontology (information science);query language;rdf schema;sparql;sql;semantic web;usability;web ontology language	Claudio Corona;Emma Di Pasquale;Antonella Poggi;Marco Ruzzi;Domenico Fabio Savo	2008				Web+IR	-24.11980750650041	8.798493798345614	77871
ef528952d9cc9ba30d5a39380b1de8775e10ee28	rule-based mechanism for constraint checking in logic programs	rule based		logic programming	Elmar Eder;Yan Liu	1990				AI	-22.152302558672574	16.955787068160014	77963
244825f42d549d87f7284cbcc4b8579d19c872e7	descriptive granularity	set theory;data mining;formal logic	We present here a formal syntax and semantics for a notion of a descriptive granularity. We do so in terms of three abstract models: Descriptive, Semantic, and Granular. The descriptive model formalizes the syntactical concepts and properties of the data mining, or learning process. Semantic model formalizes its semantical properties. The Granular model establishes a relationship between the Descriptive and Semantic models in terms of a formal satisfaction relation.	data mining;formal grammar	Anita Wasilewska	2010	2010 IEEE International Conference on Granular Computing	10.1109/GrC.2010.117	natural language processing;effective descriptive set theory;computer science;theoretical computer science;data mining;mathematics;logic;set theory	SE	-20.576135831387404	6.2062759277243025	78204
25bcb1b97d8f858430026eb99204f380d96fa51f	universal query language for unified state model	databases;electronic data processing;query languages;mathematical models;operator theory;programming languages	Unified State Model (USM) is a single data model that allows conveying objects of major programming languages and databases. USM exploits and emphasizes common properties of their data models. USM is equipped with mappings from these data models onto it. With USM at hand, we have faced the next natural research question whether numerous query languages for the data subsumed by USM can be clearly mapped onto a common language. We have designed and proposed such a language called the Unified Query Language (UQL). UQL is intended to be a minimalistic and elegant query language that allows expressing queries of languages of data models covered by USM. In this paper we define UQL and its concise set of operators. Next we conduct a mild introduction into UQL features by showing examples of SQL and ODMG OQL queries and their mapping onto UQL. We conclude by presenting the mapping of the theoretical foundations of these two major query languages onto UQL. They are the multiset relational algebra and the object query algebra. This is an important step towards the establishment of a fully-fledged common query language for USM and its subsumed data models.	contextual query language;data model;database;object data management group;object query language;programming language;relational algebra;sql	Piotr Wisniewski;Krzysztof Stencel	2014	Fundam. Inform.	10.3233/FI-2014-968	natural language processing;electronic data processing;operator theory;computer science;artificial intelligence;mathematical model;database;rdf query language;programming language;algorithm;query language;object query language	DB	-29.61230377266418	10.772473513052507	78225
054fbd09db5c393bafea8c209e22a9ecf2c7eabd	visualizing concepts with euler diagrams	g440 human computer interaction	An ontology comprises a set of statements (called axioms) that capture properties of individuals, concepts and roles. Individuals represent particular elements of the modelled domain, with concepts and roles corresponding to classes and binary relations, respectively. The primary (formal) notations for ontology modelling are symbolic, such as description logics (DLs) or OWL. To aid with accessibility and understandability, standard ontology editors often provide visualisation support. For example, Protégé includes a plugin visualisation package, OWLViz, that shows derived hierarchical relationships but does not show complete information about the known relationships between the concepts. This paper presents a new ontology visualization tool, ConceptViz, that uses Euler diagrams to represent concepts. These diagrams have the advantage that their topological properties reflect the semantic information that they convey; the structural correspondence between topology and semantics called iconicity by Peirce [3]. For instance, the containment of one curve by another reflects a subsumption relationships (i.e. set inclusion) between concepts. The asymmetric, transitive nature of curve containment reflects the asymmetric, transitive nature of (proper) subsumption. These properties motivate the choice of Euler diagrams as an effective medium for ontology visualisation. Furthermore, we combine asserted and inferred ontology information to directly visualise facts that users would otherwise need to consult several different sources to ascertain.	accessibility;description logic;diagram;euler;euler–lagrange equation;ontology (information science);protégé;subsumption architecture;upper ontology;web ontology language	Jim Burton;Gem Stapleton;John Howse;Peter Chapman	2014		10.1007/978-3-662-44043-8_9	upper ontology;computer science;ontology;theoretical computer science;data mining;algorithm	AI	-20.155454203397767	5.673926406440748	78408
8bd1488a6d768f12da3123bfc5c433e3d67370c1	plogs: materializing datalog programs with mapreduce for scalable reasoning	owl;semantics knowledge based systems cognition owl optimization resource description framework scalability;parallel processing datalog inference mechanisms;semantics;resource description framework;parallel inference semantic web datalog mapreduce;knowledge bases plogs datalog programs scalable reasoning semantic data owl 2 rl semantics swrl rules datalog language dependency aware approach parallel materialization datalog rule execution mapreduce jobs optimizations rule evaluation process rule execution order sampling based method rule dependency semantic rule sets;cognition;optimization;scalability;knowledge based systems	With the rapid growth of semantic data, scalable reasoning has attracted more, more attention. However, most existing works about scalable reasoning focus only on RDFS/OWL ter Horst semantics, which are small fragments of OWL 2 RL,, have limitation in expressivity. As OWL 2 RL semantics extended with SWRL rules can be expressed by datalog language, materialization of datalog programs is widely adopted in traditional reasoners. In this paper, we propose a dependency-aware approach on parallel materialization of datalog programs for scalable reasoning. We first present an algorithm to automate the translation from a Datalog rule execution into MapReduce jobs,, make several optimizations for the algorithm to speed up the rule evaluation process. Since the rule execution order has significant impact on reasoning performance due to the dependencies among rules. We then propose a sampling-based method to capture rule dependency,, design a dependency-aware strategy to schedule rule evaluation. Finally, we establish a system to evaluate the proposed approach with a series of semantic rule sets on large synthetic, real knowledge bases. The experimental results show that the proposed optimizations have significant effectiveness, our system achieves approximately linear scalability.	academy;algorithm;datalog;distributed memory;mapreduce;rdf schema;sampling (signal processing);scalability;semantic web rule language;shattered world;synthetic intelligence;web ontology language	Haijiang Wu;Jie Liu;Tao Wang;Dan Ye;Jun Wei;Hua Zhong	2016	2016 Intl IEEE Conferences on Ubiquitous Intelligence & Computing, Advanced and Trusted Computing, Scalable Computing and Communications, Cloud and Big Data Computing, Internet of People, and Smart World Congress (UIC/ATC/ScalCom/CBDCom/IoP/SmartWorld)	10.1109/UIC-ATC-ScalCom-CBDCom-IoP-SmartWorld.2016.0025	scalability;semantic web rule language;cognition;computer science;artificial intelligence;theoretical computer science;rdf;database;semantics;datalog;programming language	AI	-24.358232196748514	7.159857064863447	78553
e1d034706c5de9ddde8d2e3a13bac4a5a5fcf86c	ontology patterns and beyond - towards a universal pattern language		In this paper we argue for a broader view of ontology patterns and therefore present different use-cases where drawbacks of the current declarative pattern languages can be seen. We also discuss usecases where a declarative pattern approach can replace procedural-coded ontology patterns. With previous work on an ontology pattern language in mind we argue for a general pattern language.	declarative programming;encode;interoperability;ontology (information science);pattern language	Olaf Noppens;Thorsten Liebig	2009				DB	-22.671739406438352	9.291748686946248	78612
0a08c52a593a22ae0bd0ba21e6a463f73fd6d3c3	towards data warehouse design	conceptual data warehouse model;temporal data;object modelling;conceptual model;data warehouse design;data warehouse	This paper focuses on data warehouse modelling. The conceptual model we defined, is based on object concepts extended with specific concepts like generic classes, temporal classes and archive classes. The temporal classes are used to store the detailed evolutions and the archive classes store the summarised data evolutions. We also provide a flexible concept allowing the administrator to define historised parts and non-historised parts into the warehouse schema. Moreover, we introduce constraints which configure the data warehouse behaviour and these various parts. To validate our propositions, we describe a prototype dedicated to the data warehouse design.	archive;constraint (mathematics);prototype;system administrator;type class	Franck Ravat;Olivier Teste;Gilles Zurfluh	1999		10.1145/319950.320028	object model;data transformation;dimensional modeling;computer science;conceptual model;data science;data warehouse;data mining;database;temporal database	DB	-33.58275052101059	10.755372446878177	78782
1a18b51a518caacb6e505cc8ed3964f615fed5d4	specification-based approach for denotational semantic of orthogonal object/relational dbms	verification;formal specification	Abstract   The issue of the article is at the crossroads of databases modeling, software engineering and databases verification using formal methods. Development of databases software would be provided with a high-level specification suitable for formal reasoning about correctness properties. Formal specification techniques help discover problems in system requirements, inconsistencies and incompleteness can be resolved. Thus, we see a specified object/relational database management system (ORDBMS) as a compelling challenge. Toward this goal, we propose a formal specification–based approach to describe a denotational semantic of an orthogonal object/relational model, a compiler for  SQL3  queries language and an implementation of execution engine of queries over imperative generic finite maps interface. This approach is of functional style based on inductive definitions and a high-order type theory realized within  Coq  proof assistant .  Our work is a preamble step toward a verified ORDBMS.	object-relational database;relational database management system	Amel Benabbou;Safia Nait Bahloul	2014		10.1016/j.procs.2014.05.280	formal methods;formal verification;computer science;theoretical computer science;data mining;formal specification;database;programming language;algorithm	DB	-29.995877198135315	12.133257907248169	78858
0f4fb059bd41a8d16f8a818b904d02ffdbf23da1	from oo through deduction to active databases - rock, roll & rap	lenguaje programacion;query language;database system;sistema activo;programming language;deductive object oriented database;active database;rule based;lenguaje interrogacion;systeme actif;active system;deductive database;base dato deductiva;object oriented;langage programmation;relational database system;oriente objet;langage interrogation;base donnee deductive;information system;orientado objeto;data structure;systeme information;sistema informacion;deductive databases	One important thread within advanced database systems research is the notion of rule-based database systems. The power of definite rules coupled with relational technology has led to the emergence of deductive databases. However, while this type of database system provides more advanced functionality, it suffers from other limitations of relational database systems such as the lack of data structures. The realisation that the object-oriented approach is complementary to the deductive one and that the two can be combined to produce deductive object-oriented databases with all the benefits of both represents an important breakthrough for rule-based database systems. An alternative to the deductive rule approach is the active rule approach. Active rules are more powerful than deductive rules but lack thc advantages of the sound theoretical foundation of the latter. The two ideas can be combined to produce an active DOOD provided that the integration is treated with care.		M. Howard Williams;Norman W. Paton	1997		10.1007/3-540-63774-5_113	database theory;relational database management system;data structure;computer science;artificial intelligence;data mining;database;programming language;object-oriented programming;information system;algorithm;query language	DB	-28.863085922707413	11.397330117208636	78883
355720860a2ce886ee5ea1e5749aea4eb1674d8d	warp:warp around data placement technique for serpentine tapes	magnetic heads;application software;information retrieval;bridges;maintenance engineering;data engineering;maintenance engineering data engineering explosions information retrieval switches magnetic heads educational institutions petroleum application software bridges;petroleum;cost effectiveness;explosions;switches;data placement	Due to the information explosion we are witnessing, a growing number of applications store, maintain, and retrieve large volumes of data, where the data is required to be available online or near-online [10, 3, 8]. These data repositories are implemented using hierarchical storage structures (HSS). One of the components of HSS is tertiary storage, which provides a cost-effective storage for the vast amount of data manipulated by these applications. To bridge the access-gap between the tertiary storage and the secondary storage, we propose a data placement technique, namely WARP: Wrap ARound data Placement. WARP may reduce the access time by 1 order of magnitude, depending on the tape device specifications and object sizes. An important feature of WARP is that it optimizes access-time independently of the retrieval order. We have implemented this technique on an IBM 3590 tape drive and have observed up to 5 times improvement in access-time as compared to other data placement techniques. Moreover, we report on the use of WARP with multiple load/unload position tapes (of the future).	access time;auxiliary memory;computer data storage;hierarchical storage management;high-speed serial interface;ibm 3590;information explosion;nearline storage;tape drive	Ali E. Dashti	2001	2001 Eighteenth IEEE Symposium on Mass Storage Systems and Technologies	10.1109/MSS.2001.10028	embedded system;real-time computing;computer hardware;engineering	Embedded	-33.553286558390226	17.095159999966974	78909
693331315d9b8cefcd4d86d7182e8341f4901250	relevant logic programming	expressive power;logic programs	In this paper we present a fragment of (positive) relevant logic which can be computed by a straightforward extension to SLD resolution while allowing full nesting of implications. These two requirements lead quite naturally to a fragment in which the major feature is an ambiguous ‘user-level’ conjunction which is interpreted intensionally in ‘query’ positions and extensionally in ‘assertion’ positions. These restrictions allow a simple and efficient extension to SLD resolution (and more particularly, the PROLOG evaluation scheme) with quite minor loss in expressive power.	logic programming;prolog;requirement;sld resolution	A. W. Bollen	1991	Journal of Automated Reasoning	10.1007/BF01880329	computer science;sld resolution;theoretical computer science;programming language;expressive power;algorithm	AI	-19.89695191720679	15.11508236657848	78940
d0c8ac13c06fe10f4d4a3aefa3826dc1229e3963	how to blast your database - a study of stored procedures for blast searches	estensibilidad;base relacional dato;alignement;base donnee;nucleotides;interrogation base donnee;serveur informatique;database;interrogacion base datos;base dato;relational database;busca local;logiciel libre;alineamiento;base donnee relationnelle;servidor informatico;software libre;extensibilite;scalability;experimental evaluation;local search;basic local alignment search tool;database query;alignment;recherche locale;open source software;open source;computer server	Stored procedures are an important feature of all major database systems that allows to execute application logic within database servers. This paper reports on experiences to implement a popular scientific algorithm, the Basic Local Alignment Search Tool (BLAST), as stored procedures within a relational database. We implemented the un-gapped, nucleotide version of the BLAST algorithm with four different relational database engines, both commercial and open source. In an experimental evaluation, we compared our dbBLASTimplementations with a standard file-based BLAST implementation from NCBI with regard to the implementation effort, runtime performance, and scalability. It shows that although our dbBLAST runs faster than the file-based BLAST program for short query sequences, all implementations lack scalability. However, the results also indicate that stored procedures require significant less development effort—both in time and space—than traditional programming approaches.	algorithm;analysis of algorithms;blast;business logic;computation;database server;database-centric architecture;inverted index;open-source software;pervasive psql;physical design (electronics);prototype;relational database;run time (program lifecycle phase);scalability;sequence alignment;sequence database;stored procedure;string (computer science)	Uwe Röhm;Thanh-Mai Diep	2006		10.1007/11733836_58	nucleotide;scalability;relational database;computer science;local search;data mining;database;world wide web;server	DB	-27.830306476516515	6.041560691719951	78977
3edffae164efa26b96c2d8ca6df108801b64d398	communicating control knowledge to a deductive database system	tuple space;transputer;speculative processing;parallel lisp;deductive databases	In relational or deductive database systems the user can hardly (or only in a rather implicit way) change the deduction process used for answering a query. For example, the user cannot specify that–for a given query–some rules of a deductive database are irrelevant for computing the answer of that query and hence should be disregarded, or that–while computing the answer of a query–some tuples should be preferred over other tuples. In [Sch91] we have introduced a deductive database system which offers the user a framework for specifying such control knowledge. Thereby the user can adapt the deduction process of our deductive database system to the application at hand. In this paper we will briefly recapture the architecture of this system. Then we will present two examples, and we will show how these two examples can be solved more efficiently with our system.	deductive database;natural deduction;relational database management system;relational model;relevance	Helmut Schmidt	1992		10.1145/131214.131269	computer science;tuple space;theoretical computer science;database;programming language	DB	-27.712213392487556	9.874690421699048	79101
10f676d450c583979301ffd9cc2a7f326501e2ec	fusion queries over internet databases	optimisation;base donnee repartie;distributed database;red www;optimizacion;interrogation base donnee;base repartida dato;interrogacion base datos;data fusion;information integration;fusion donnee;databases and the web;world wide web;optimization;reseau www;information system;fusion datos;database query;large classes;systeme information;sistema informacion	Fusion queries search for information integrated from distributed, autonomous sources over the Internet. We investigate techniques for e cient processing of fusion queries. First, we focus on a very wide class of query plans that capture the spirit of many techniques usually considered in existing systems. We show how to e ciently nd good query plans within this large class. We provide additional heuristics that, by considering plans outside our target class of plans, yield further performance improvements.	algorithm;autonomous robot;database;heuristic (computer science);internet;mathematical optimization;oracle fusion middleware;response time (technology)	Ramana Yerneni;Yannis Papakonstantinou;Serge Abiteboul;Hector Garcia-Molina	1998		10.1007/BFb0100977	computer science;information integration;data mining;database;sensor fusion;world wide web;distributed database;information system	DB	-25.856584614660083	4.606792126016922	79264
4a6d7a0c931d1bbefa2046886262f4819aeabeb9	extensible objects for database evolution: language features and implementation issues	language features;extensible objects;implementation issues;database evolution;object oriented	One of the limitations of commercially available object-oriented DBMSs is their inability to deal with objects that may change their type during their life and which exhibit a plurality of behaviors. Several proposals have been made to overcome this limitation. An analysis of these proposals is made to show the impact of more general modeling functionalities on the object implementation technique. 5th International Workshop on Database Programming Languages, Gubbio, Italy, 1995 1 Extensible Objects for Database Evolution: Language Features and Implementation Issues	galileo	Antonio Albano;Milena Diotallevi;Giorgio Ghelli	1995			simulation;computer science;data mining;database;programming language;object-oriented programming	DB	-30.733661120374748	11.2468322737143	79344
66155572e37799488f0a6d46068e9e7c6f1872c4	a model of user-oriented reduct construction based on minimal set cover		An implicit assumption of many learning algorithm is that all attributes in the attribute set are of the same importance. However, this assumption is unreasonable or practical. If attributes in the attribute set are considered of non-equal importance with respect to their own situation, then the model obtained from those attributes would be more realistic. This paper designed a user oriented reduction model based on the minimal set cover by seamlessly combining an order of attributes that describes user preferences. The accessibility and efficiency of this algorithm is shown by an example.		Suqing Han;Guimei Yin	2012		10.1007/978-3-642-34679-8_10	data mining;reduct;computer science;set cover problem	Logic	-21.21953040516387	5.681952765697898	79555
3f0112bbb16b9bd3fda877302b5526888d24cc11	lazy clause generation reengineered	conference paper;hybrid approach;finite domain;computer software;sat solver;combinatorial optimization	Lazy clause generation is a powerful hybrid approach to combinatorial optimization that combines features from SAT solving and finite domain (FD) propagation. In lazy clause generation finite domain propagators are considered as clause generators that create a SAT description of their behaviour for a SAT solver. The ability of the SAT solver to explain and record failure and perform conflict directed backjumping are then applicable to FD problems. The original implementation of lazy clause generation was constructed as a cut down finite domain propagation engine inside a SAT solver. In this paper we show how to engineer a lazy clause generation solver by embedding a SAT solver inside an FD solver. The resulting solver is flexible, efficient and easy to use. We give experiments illustrating the effect of different design choices in engineering the solver.	backjumping;boolean satisfiability problem;combinatorial optimization;constraint programming;experiment;lazy evaluation;mathematical optimization;software propagation;solver	Thibaut Feydy;Peter J. Stuckey	2009		10.1007/978-3-642-04244-7_29	mathematical optimization;combinatorial optimization;computer science;theoretical computer science;mathematics;boolean satisfiability problem;programming language;algorithm	AI	-19.517345688000074	15.967058530409581	79654
09a87a2c73d850d222ee25cc44dee56df8e54304	representing and reasoning about open-textured predicates	open-textured predicate	In this paper, I will describe a method for representing and reasoning about open-textured predicates. This method is being implemented in CHIRON, a system I am developing in the domain of United States personal income tax planning.	ibm system i	Kathryn E. Sanders	1991		10.1145/112646.112663	data mining;predicate (grammar);computer science	AI	-25.305062407160033	18.06664365726581	79782
61769fe034218d62faf5d3e0a91dafb065d56ffa	recent advances in integrating owl and rules (technical communication)	rule-based approach;rule interchange format rif;recent advance;central issue;semantic web technology stack;entry point;rif core;naive union;naive approach;technical communication;owl-centric perspective;rule-based formalisms	As part of the quest for a unifying logic for the Semantic Web Technology Stack, a central issue is finding suitable ways of integrating description logics based on the Web Ontology Language (OWL) with rule-based approaches based on logic programming. Such integration is difficult since naive approaches typically result in the violation of one or more desirable design principles. For example, while both OWL 2 DL and RIF Core (a dialect of the Rule Interchange Format RIF) are decidable, their naive union is not, unless carefully chosen syntactic restrictions are applied. We report on recent advances and ongoing work by the authors in integrating OWL and rules. We take an OWL-centric perspective, which means that we take OWL 2 DL as a starting point and pursue the question of how features of rulebased formalisms can be added without jeopardizing decidability. We also report on incorporating the closed world assumption and on reasoning algorithms. This paper essentially serves as an entry point to the original papers, to which we will refer throughout, where detailed expositions of the results can be found.	algorithm;closed-world assumption;description logic;entry point;logic programming;ontology (information science);rule interchange format;semantic web;web ontology language	Matthias Knorr;David Carral;Pascal Hitzler;Adila Krisnadhi;Frederick Maier;Cong Wang	2012		10.1007/978-3-642-33203-6_20	semantic web rule language;computer science;artificial intelligence;data mining;programming language;rule interchange format;algorithm	AI	-21.182561192895985	8.896356049230821	79802
c8594815cd5f8716924089234555a58b9f5a4ad6	guest editorial for acm tecs special issue on effective divide-and-conquer, incremental, or distributed mechanisms of embedded designs for extremely big data in large-scale devices	pollutant dynamics;air pollution source;cyber physical system;multi quadcopter;control laws	As the size of data grows at a rapid speed, the demand for big data analysis has significantly increased in recent years, ranging from decentralized data centers, cloud servers, and sensor networks to the Internet of Things (IoT). Big data usually come with two types. One has an excessively huge volume of samples, and the other exhibits extremely large dimensions. Both of them jeopardize the performance of computing systems. Data analysts usually have to deal with limited storage, processing speed, communication bandwidth, and power consumption, especially when resources are constrained. What is worse, in the era of big data, the scale that engineers are handling is beyond billions. Moreover, when plenty of devices interact with each other (e.g., mobile devices, sensors, and the IoT), they form a large-scale network. This has relatively deepened the difficulty of data analysis. In view of this problem, many famous and off-the-shelf tools, such as Apache Hadoop and Apache Spark, are developed to handle large-scale analyses by using distributed processing. These tools usually employ divide-and-conquer architectures in their implementations. With such architectures, when a large-scale dataset is segmented into subsets, the original problem can accordingly be divided into subproblems, separately processed in each machine. Despite the convenient framework, there is still no effective solution to embedded devices. Besides, not every algorithm can be converted into a divide-and-conquer version and gives optimal solutions. In contrast to divide-and-conquer mechanisms, incremental analysis is another approach for dealing with big data because it does not rely on distributed architectures. Incremental analysis allows the system to add new samples without reprocessing whole samples because earlier calculation results can be reserved for updating the system in the future. Furthermore, if the data size is far beyond the capability of one machine, the entire dataset is divided into several batches, subsequently processed by a single machine. The combination of incremental and divide-and-conquer mechanisms yields more flexibility than an individual one. Therefore, how to take advantage of such a combination to resolve big data problems is the prior concern. The articles selected from submission presented a recent update that covers front-end data aggregation and back-end data postprocessing. The work “Distributed Multirepresentative Refusion Approach for Heterogeneous Sensing Data Collection” by A. Liu et al. examined data collection for sensor networks and the IoT. They developed an effective refusion method for handling data pooling in sink nodes. Regarding back-end techniques, the studies by X. Chen et al., B. Liu et al., and S.-Y. Kung et al. respectively presented new solutions to supervised analysis, unsupervised techniques, and dimensional reduction. These approaches are widely used in machine learning. The work “A Load-Balancing Divide-and-Conquer SVM Solver” by X. Chen et al. advanced the idea of divide-and-conquer Support Vector Machines (SVMs) by devising a loadbalancing mechanism for distributed and embedded systems. B. Liu et al. presented “Decentralized Sparse Subspace Clustering” that partitioned a large data matrix into	algorithm;apache hadoop;apache spark;big data;clustering high-dimensional data;computer cluster;data aggregation;data center;distributed computing;embedded system;entity–relationship model;internet of things;load balancing (computing);machine learning;mobile device;sensor;solver;sparse;support vector machine	Bo-Wei Chen;Wen Ji;Zhu Li	2017	ACM Trans. Embedded Comput. Syst.	10.1145/3068457	embedded system;real-time computing;simulation;computer science;cyber-physical system;computer security	AI	-27.070919311610858	16.147644368476488	79918
bbfc38e6fc49eb4bfbae31b631d1c265185b5ced	from semanticobjects to structured natural language	natural language;natural language processing	We are especially interested in retrieving information from a database via natural language like queries that can be understood by the computer, and recognized that a natural language interface to databases can quickly become complicated because of the difficulties of translating natural language queries into a formal query language. Domain-dependent knowledge was required most of the time, and this usually has to be customized for every system domain. This has been a major barrier for implementing a successful natural language interface in the past. In the SemanticObjects project [10] we have been taking a different approach to this problem. First was to extend the traditional relational algebra into a more powerful object relational algebra (ORA) [10]. Second, as reported in this paper we devise a structured natural language (SNL) that can be mapped directly into an expression in ORA. Third, we propose an interactive procedure that allows users to compose SNL queries based on the recursive nature of ORA. This would confine the possible queries that may be posed by the user. Finally, we describe an approach to translating queries expressed in natural language into SNL queries, and its associated problems. This paper is organized as follows. In Sec. 2, we summarize the existing approaches to NL interface for databases (NLIDB). Section 3 summarizes the object relational algebra. Section 4 describes the structured natural language SNL and the mapping between ORA and SNL. Section 5 describes an interactive procedure that	application domain;brute-force search;database;in the beginning... was the command line;nl (complexity);nl-complete;natural language user interface;nl (format);numerical aperture;openraster;query language;recursion;relational algebra;sql;search algorithm;semantic analysis (compilers);software portability;triangulated irregular network	Phillip C.-Y. Sheu;Atsushi Kitazawa;Chihiro Ishii;Kenichi Kaneko;Fei Xie	2007	Int. J. Semantic Computing	10.1142/S1793351X07000160	natural language processing;language identification;structured english;data definition language;natural language programming;sql;universal networking language;codd's theorem;question answering;relational calculus;data manipulation language;object language;natural language user interface;data control language;computer science;database;linguistics;conjunctive query;natural language;programming language;temporal annotation;information extraction	DB	-27.24825271759915	7.650580128964969	80306
93ebcca3d10faff04c25a6fd384299dca6735632	de l'intégration de données à la composition de services web. (from data integration to web services composition)		Tool integration in software development environments is a major problem, on one hand, for users of these environments, and on the other hand, for tool builders and suppliers. In this paper, we focus on persistent data integration, which is one of the two main points of tool integration. The purpose of this work is to provide a formal data model that includes most of the semantics of the data manipulated to answer problems raised by persistent data integration. To achieve this goal, we introduce a classical data model, and provide a set of operators which are given for the object transformation. We also provide an example describing the transformation process. Towards a Model for Persistent Data Integration Olivier Perrin and Nacer Boudjlida Centre de Recherche en Informatique de Nancy (CRIN-CNRS) University of Nancy I Campus Scientifique — B.P. 239 54506 Vandoeuvre-lès-Nancy — FRANCE email: operrin@loria.fr, nacer@loria.fr Abstract. Tool integration in software development environments is a major problem, on one hand, for users of these environments, and on the other hand, for tool builders and suppliers. In this paper, we focus on persistent data integration, which is one of the two main points of tool integration. The purpose of this work is to provide a formal data model that includes most of the semantics of the data manipulated to answer problems raised by persistent data integration. To achieve this goal, we introduce a classical data model, and provide a set of operators which are given for the object transformation. We also provide an example describing the transformation process. Tool integration in software development environments is a major problem, on one hand, for users of these environments, and on the other hand, for tool builders and suppliers. In this paper, we focus on persistent data integration, which is one of the two main points of tool integration. The purpose of this work is to provide a formal data model that includes most of the semantics of the data manipulated to answer problems raised by persistent data integration. To achieve this goal, we introduce a classical data model, and provide a set of operators which are given for the object transformation. We also provide an example describing the transformation process.	campus party;data model;email;integrated development environment;linear algebra;perrin number;software development;web service;world wide web	Olivier Perrin	2010				DB	-31.33479842508397	11.649993587044573	80354
477fd34fa668eb0a1ffa8581d89e8ffc89bc6cac	advanced cardinality estimation in the xml query graph model.	graph model	Reliable cardinality estimation is one of the key prerequisite for effective cost-based query optimization in database systems. The XML Query Graph Model (XQGM) is a tuple-based XQuery algebra that can be used to represent XQuery expressions in native XML database management systems. This paper enhances previous works on reliable cardinality estimation for XQuery and introduces several inference rules that deal with the unique features of XQGM, such as native support for Structural Joins, nesting, and multi-way merging. These rules allow to estimate the cardinalities of XQGM operators taken at runtime. Using this approach, we can support classical join reordering with appropriate statistical information, perform cost-based query unnesting, and help to find the best evaluation strategy for value-based joins. The effectiveness of our approach for query optimization is evaluated using the cost-based query optimizer of XTC, which is our prototype of a native XML database management system.	algorithm;experiment;heuristic (computer science);join (sql);mathematical optimization;prototype;query optimization;rewrite (programming);rewriting;run time (program lifecycle phase);scalability;xml database;xquery	Andreas M. Weiner	2011			graph power;edge-transitive graph;factor-critical graph;discrete mathematics;graph bandwidth;null graph;graph property;clique-width;simplex graph;data mining;database;voltage graph;distance-hereditary graph;graph;windmill graph;butterfly graph;quartic graph;complement graph;line graph;strength of a graph	DB	-29.05210042775679	6.69048010644687	80633
daa6d2e02609d9c788e5820bbdb7a5ff21deee0a	computed answer from uncertain knowledge: a model for handling uncertain information	fuzzy sets;knowledge based systems;uncertainty modelling	In this work we present a model for handling uncertain information. The concept of fuzzy knowledge-base is defined as a quadruple of background knowledge. Specifically, the latter is defined by the proximity of predicates and terms; a deduction mechanism: a fuzzy Datalog program; a connecting algorithm, which connects the background knowledge with the program, and a decoding set of the program which helps us determine the uncertainty level of the results. We also suggest a possible evaluation strategy.	algorithm;datalog;information;knowledge base;knowledge management;natural deduction;quadruple-precision floating-point format	Ágnes Achs	2007	Computers and Artificial Intelligence		type-2 fuzzy sets and systems;computer science;knowledge management;artificial intelligence;knowledge-based systems;data mining;mathematics;fuzzy set;algorithm	AI	-22.31021018026794	6.563720891552636	80667
b455dc98531030e2bef7dac8c46793f9617624ab	data driven approximation with bounded resources		This paper proposes BEAS, a resource-bounded scheme for querying relations. It is parameterized with a resource ratio α ∈ (0, 1], indicating that given a big dataset D, we can only afford to access an α-fraction of D with limited resources. For a query Q posed on D, BEAS computes exact answers Q(D) if doable and otherwise approximate answers, by accessing at most α|D| amount of data in the entire process. Underlying BEAS are (1) an access schema, which helps us identify and fetch the part of data needed to answer Q, (2) an accuracy measure to assess approximate answers in terms of their relevance and coverage w.r.t. exact answers, (3) an Approximability Theorem for the feasibility of resource-bounded approximation, and (4) algorithms for query evaluation with bounded resources. A unique feature of BEAS is its ability to answer unpredictable queries, aggregate or not, using bounded resources and assuring a deterministic accuracy lower bound. Using real-life and synthetic data, we empirically verify the effectiveness and efficiency of BEAS.	aggregate data;approximation algorithm;experiment;real life;relevance;synthetic data	Yang Cao;Wenfei Fan	2017	PVLDB	10.14778/3099622.3099628	theoretical computer science;data mining;database;mathematics;algorithm	DB	-24.32869682457104	5.0638985848231926	80751
12e0bf4985ae721c7e7418c7deedf7e989de1f6e	extending a dbms with spatial operations	spatial data;query processing;relational database;equal opportunities;database design;database management system	A central problem in modern database design is how to resolve spatial operations with normal database operations in an extended relational database environment. A data architecture that matches the requirements for efficient processing of spatial queries in the extended database environment is proposed. It provides an equal opportunity for both the spatial components and the non-spatial components of the data to participate in query processing and optimization. The notion of extended operators to integrate homogeneously both spatial and non-spatial operations is introduced. Although intended primarily for spatial data, extended operators also provide a proper interface for integrating multi-media data into a database environment. The implications of this data architecture are presented. They include their effects on standard database operations, how spatial operations are integrated into the database management system (DBMS) for efficient processing, and how query processing and optimization are performed in this architecture. The operations of insertion and deletion, relational-based selection and join, and spatial-based selection and join are redefined in terms of extended operators. Spatial query processing is also described using extended operators. This data architecture can be built on top of an extensible database management system. Since it is dedicated towards efficient spatial query processing, this architecture can be used for testing and validating the extensibility of such systems and their effectiveness for supporting spatial data.	analysis of algorithms;bi-directional text;data architect;data architecture;data integrity;data structure;database design;definition;extensibility;mathematical optimization;online and offline;overhead (computing);relational database;requirement;spatial database;spatial query	Walid G. Aref;Hanan Samet	1991		10.1007/3-540-54414-3_44	sargable;query optimization;database theory;relational database management system;relational model;database transaction;database tuning;relational database;computer science;database normalization;database model;data mining;database;view;database schema;spatial database;information retrieval;alias;object-relational impedance mismatch;database design;spatiotemporal database;component-oriented database	DB	-29.768257515749593	7.784577585227164	80772
42b22efe7e05f4a1fb742a40423e98d20a42670b	a powerful and efficient structural pattern recognition system	pattern recognition	This paper describes a general structural pattern recognition system. It introduces code—a description language for patterns (also called concepts) based on boolean logic and set theory. Internal machine representations for concepts and objects are given which directly influence the efficiency of the system. In particular, a data structure, called a “graft”, is described. A method of storing and accessing concept descriptions is also given. This feature of the system is partly an attempt to restrict the recognition process to those concepts that might be relevant to an object. Finally, the recognition process is described and the reasons for its power and efficiency are discussed.	structural pattern;syntactic pattern recognition	Brian L. Cohen	1977	Artif. Intell.	10.1016/0004-3702(77)90023-6	feature;computer science;artificial intelligence;data mining	Vision	-20.74723110285996	5.584617664243682	80796
01626e29691ba6fa02ed1eb1c1b1f403d1463277	specifying access control policies for xml documents with xpath	formal specification;xml access control;access control policy;file system;xpath;natural language;xml;xml document;access control	Access control for XML documents is a non-trivial topic, as can be witnessed from the number of approaches presented in the literature. Trying to compare these, we discovered the need for a simple, clearand unambiguous language to state the declarative semantics of an access control policy. All current approaches state the semantics in natural language, which has none of the above properties. This makes it hard to assess whether the proposed algorithms are correct (i.e., really implement the described semantics). It is also hard to assess the proposed policy on its merits, and to compare it to others (for file systems for instance). This paper shows how XPath can be used to specify the semantics of an access control policy for XML documents. Using XPath has great advantages: it is standard technology, widely used and it has clear and easy syntax and semantics. We use the developed framework to give a formal specification of the five most prominent approaches of access controlfor XML documents from the literature.	access control;algorithm;formal specification;natural language;xml;xpath	Irini Fundulaki;Maarten Marx	2004		10.1145/990036.990046	well-formed document;xml catalog;xml validation;xml encryption;simple api for xml;xml;xslt;xml schema;streaming xml;computer science;xpath 2.0;document structure description;xml framework;data mining;xml database;xml schema;database;schematron;xml signature;programming language;xml schema editor;computer security;cxml;efficient xml interchange	DB	-24.17211212245257	13.299622436352102	81057
1ecb4b23cc70cb0164594c5f67a833170d376e90	inconsistency tolerance in owl 2 ql knowledge and action bases		A Business Process (BP) is constituted by (i) data that describes the state of affairs and (ii) a set of activities (to be performed) over this data. The activities, when combined in what is usually referred to as an execution flow, achieve some business goal. When analyzing BPs, one is usually interested in querying possible execution flows to extract useful information and verifying dynamic properties over them.1 To this effect, BPs are typically modeled via high-level specifications of activities [17], which are later compiled into an executable code. Since the logics of business processes is captured by these specifications, tools for querying and analyzing possible execution flows are extremely valuable for companies [11,12,16]. Fully taking into account the presence of data significantly complicates the analysis of execution flows since it makes the system to be an infinite state one in general. On the other hand, it is often assumed that the data is simple enough and does not significantly impact on the analysis of possible execution flows. For these reasons, in the classical modeling paradigm of BPs, the data part is typically abstracted away, and the analysis is carried out under this simplification. In knowledge-intensive applications, however, where a crucial aspect of BPs is to properly represent the allowed evolutions of the data component, the classical modeling paradigm is not appropriate and one has to take fully into account data in the specification of BPs [14,13,20,1]. For example, in healthcare systems, an electronic record of a patient is the “data” to be manipulated by a process, and “activities” determine how to modify the patient’s information (e.g., the registration of patient examination results). In this application scenario it is inappropriate to abstract away from patient records in process specifications. Thus, there is a need for developing and studying formalisms in which both the data component and the process component are first-class citizens. A number of recent proposals follow this approach [9,3,2], which is commonly referred to as Data-Centric Business Processes (DCBPs). The dynamic properties one is interested to verify are typically expressed in some variant of temporal logic, such as LTL, CTL, or the (first-order) μ-calculus [18,19], a very expressive temporal logic subsuming most of the other temporal formalisms. In the traditional BP setting, the verification of temporal properties is based on finite-state model checking [8]. However, in DCBPs, the presence of data makes the number of different states of the system potentially infinite. Hence, the verification of dynamic prop-	business process;compiler;executable;first-class citizen;first-order predicate;high- and low-level;linear temporal logic;model checking;programming paradigm;text simplification;verification and validation;web ontology language	Diego Calvanese;Evgeny Kharlamov;Marco Montali;Dmitriy Zheleznyakov	2012			computer science;web ontology language;data mining	SE	-27.08615967777111	15.042447426775546	81085
618d93b92ce9efa81fb7a3402f87a40ba0cec7a6	logic-based approach to semantic query optimization	base relacional dato;query language;optimisation;coaccion integridad;optimizacion;contrainte integrite;query formulation;interrogation base donnee;interrogacion base datos;formulacion pregunta;query optimization;relational database;formulation question;lenguaje interrogacion;deductive database;basedato deductiva;logique ordre 1;integrity constraint;integrity constraints;base donnee relationnelle;optimization;langage interrogation;base donnee deductive;systeme gestion base donnee;semantic query optimization;sistema gestion base datos;database management system;database query;one order logic;first order logic;logica orden 1;deductive databases	The purpose of semantic query optimization is to use semantic knowledge (e.g., integrity constraints) for transforming a query into a form that may be answered more efficiently than the original version. In several previous papers we described and proved the correctness of a method for semantic query optimization in deductive databases couched in first-order logic. This paper consolidates the major results of these papers emphasizing the techniques and their applicability for optimizing relational queries. Additionally, we show how this method subsumes and generalizes earlier work on semantic query optimization. We also indicate how semantic query optimization techniques can be extended to databases that support recursion and integrity constraints that contain disjunction, negation, and recursion.	acm transactions on database systems;algorithm;automated theorem proving;bus (computing);cisco ios;compiler;const (computer programming);correctness (computer science);data integrity;deductive database;feedback;first-order logic;first-order predicate;first-order reduction;heuristic (computer science);integrated circuit;intensional logic;interpreter (computing);mathematical optimization;modifier key;program optimization;prolog;query optimization;recursion;relational database management system;sql;semantic query;subsumption architecture	Upen S. Chakravarthy;John Grant;Jack Minker	1990	ACM Trans. Database Syst.	10.1145/78922.78924	sargable;query optimization;semantic computing;query expansion;web query classification;boolean conjunctive query;computer science;query by example;data integrity;data mining;database;web search query;algorithm;query language	DB	-27.156885671124744	11.181998840590486	81413
60f58fd166a2e8db0b6fcfcc5de025a3712f703b	design non-recursive and redundant-free xml conceptual schema with hypergraph (extended abstract)	xml schema;xml database;conceptual model;data type;relational database;functional dependency;data model;conceptual schema	  Data Type Definition(DTD) and XML Schema Definition(XSD) are the logical schema of an XML model, but there is no standard  format for the conceptual schema of an XML model. Conceptual modeling is a very important first step for constructing a database  application. A conceptual model describes a system that is being built. Abstract ideas are made concrete as the ideas are  represented in a formal notation. A formal conceptual model has a number of advantages. First, it helps designers understand  and document the application under construction. Second, it facilitates development of algorithms that derive the underlying  database schemas. In this paper, a real world of interest is described in a conceptual-model Hypergraph, which is a generic  conceptual model. It is a Hypergraph because its hyperedges, or simply edges, are not necessarily binary. Its vertices represent  sets of objects and its edges represent relationships among the vertices. Edges in a Hypergraph can be directed or undirected,  depending on whether the underlying relationships are functional or non-functional. As opposed to relational databases, in  this paper we are interested in constructing XML database applications with “good” properties. Two properties are particularly  outstanding. First, the database should not have redundant data because redundant data lead to multiple-update problem once  a single copy is modified. Second, since joins are expensive, the number of generated scheme trees, which are a generic hierarchical  storage structure, should be as few as possible in order to reduce the number of joins required to answer a query. Users can  draw a Hypergraph as XML conceptual schema with data relationships among elements as a result of specified functional dependency  and multivalued dependency.    	conceptual schema;recursion;xml	Joseph Fong;Wai Yin Mok;Haizhou Li	2011		10.1007/978-3-642-20244-5_5	xml validation;semi-structured model;relax ng;logical schema;xml schema;data type;data model;streaming xml;relational database;computer science;three schema approach;conceptual schema;conceptual model;xs3p;document definition markup language;document structure description;database model;star schema;data mining;xml database;xml schema;database;document schema definition languages;functional dependency;database schema;xml schema editor;information retrieval;efficient xml interchange	Theory	-30.18675694398029	11.701408352584583	81986
5ee35281c2c5345e13890b7dcef3d17ee0506023	glog: a high level graph analysis system using mapreduce	high level dataflow system glog high level graph analysis system mapreduce distributed data processing complex graph analysis high level query language relational graph data model datalog pig;computer languages;indexes;engines;algebra;merging;optimization;indexes optimization engines algebra educational institutions computer languages merging;relational databases data flow computing datalog graph theory	With the rapid growth of graphs in different applications, it is inevitable to leverage existing distributed data processing frameworks in managing large graphs. Although these frameworks ease the developing cost, it is still cumbersome and error-prone for developers to implement complex graph analysis tasks in distributed environments. Additionally, developers have to learn the details of these frameworks quite well, which is a key to improve the performance of distributed jobs. This paper introduces a high level query language called GLog and proposes its evaluation method to overcome these limitations. Specifically, we first design a RG (Relational-Graph) data model to mix relational data and graph data, and extend Datalog to GLog on RG tables to support various graph analysis tasks. Second, we define operations on RG tables, and show translation templates to convert a GLog query into a sequence of MapReduce jobs. Third, we propose two strategies, namely rule merging and iteration rewriting, to optimize the translated jobs. The final experiments show that GLog can not only express various graph analysis tasks in a more succinct way, but also achieve a better performance for most of the graph analysis tasks than Pig, another high level dataflow system.	cognitive dimensions of notations;control flow;data model;dataflow;datalog;distributed computing;experiment;high-level programming language;iteration;mapreduce;mathematical optimization;query language;requirement;residential gateway;rewriting	Jun Gao;Jiashuai Zhou;Chang Zhou;Jeffrey Xu Yu	2014	2014 IEEE 30th International Conference on Data Engineering	10.1109/ICDE.2014.6816680	database index;wait-for graph;computer science;theoretical computer science;data mining;database;graph;programming language;graph database;graph rewriting	DB	-28.80119265236183	6.748578820851699	82045
026ee44893eb9e77e5f652c83c280e7bd815117b	semantic caching via query matching for web sources	reliability;page generation;tourism information system;polynomial time algorithm;optimization;query translation;www	A semantic caching scheme suitable for wrappers wrapping web sources is presented. Since the web sources have typically weaker querying capabilities than conventional databases, existing semantic caching schemes cannot be applied directly. A seamlessly integrated query translation and capability mapping between the wrappers and web sources in semantic caching is described. In addition, an analysis on the match types between the user's input query and cached queries is presented. Semantic knowledge acquired from the data can be used to avoid unnecessary access to the web sources by transforming the cache miss to the cache hit. A polynomial time algorithm based on the proposed query matching technique is presented to find the best matched query in the cache. Experimental results reveal the effectiveness of the proposed semantic caching scheme.	autonomous robot;cpu cache;cache (computing);database;experiment;linear algebra;locality of reference;naruto shippuden: clash of ninja revolution 3;p (complexity);page replacement algorithm;schematic;wrapping (graphics)	Dongwon Lee;Wesley W. Chu	1999		10.1145/319950.319960	query optimization;query expansion;web query classification;computer science;data mining;reliability;database;web search query;world wide web;information retrieval;statistics	DB	-32.07380459554481	7.135080456793471	82054
e592bfd5d32cb1890c2918f67646bfe0f60a6ecc	an algebraic query model for effective and efficient retrieval of xml fragments	query processing;keyword search;xml document	Finding a suitable fragment of interest in a nonschematic XML document with a simple keyword search is a complex task. To deal with this problem, this paper proposes a theoretical framework with a focus on an algebraic query model having a novel query semantics. Based on this semantics, XML fragments that look meaningful to a keyword-based query are effectively retrieved by the operations defined in the model. In contrast to earlier work, our model supports filters for restricting the size of a query result, which otherwise may contain a large number of potentially irrelevant fragments. We introduce a class of filters having a special property that enables significant reduction in query processing cost. Many practically useful filters fall in this class and hence, the proposed model can be efficiently applied to real-world XML documents. Several other issues regarding algebraic manipulation of the operations defined in our query model are also formally discussed.	database;linear algebra;relevance;search algorithm;xml	Sujeet Pradhan	2006			xml validation;xml encryption;sargable;query optimization;query expansion;web query classification;xml;ranking;boolean conjunctive query;computer science;query by example;document structure description;xml framework;data mining;database;rdf query language;web search query;xml schema editor;information retrieval;query language;efficient xml interchange	DB	-26.70087525471553	7.693667079820559	82184
36029b85c8b9ea6200078ecec206431cae300867	keeping secrets in incomplete databases	information systems;controlled query evaluation;incomplete databases;inference control;query evaluation;information system	Controlled query evaluation (CQE) preserves confidentiality in information systems at runtime. A confidentiality policy specifies the information a certain user is not allowed to know. At each query, a censor checks whether the answer would enable the user to learn any classified information. In that case, the answer is distorted, either by lying or by refusal. We introduce a framework in which CQE can be analyzed wrt. possibly incomplete logic databases. For each distortion method, lying and refusal, a class of confidentiality-preserving mechanisms is presented. Furthermore, we specify a third approach that combines lying and refusal and compensates the disadvantages of the respective uniform methods. The enforcement methods are compared to the existing methods for complete databases.	censoring (statistics);confidentiality;database;distortion;first-order logic;information system;modal logic;prototype;run time (program lifecycle phase)	Joachim Biskup;Torben Weibert	2007	International Journal of Information Security	10.1007/s10207-007-0037-7	computer science;data mining;database;computer security;information system;query language	DB	-26.803786486731386	12.630344151472315	82240
63611eaa55d28ca4ecf84ec432ae154e05074861	workshop on learning with logics and logics for learning (llll)	bayesian network;pac learning;intelligence artificielle;artificial intelligent;machine learning;computational logic;identification in the limit;artificial intelligence;inteligencia artificial;information system;description logic;logic programs;higher order logic;systeme information;first order logic;on line learning;sistema informacion	Logic is a fundamental and useful representation for knowledge in Artificial Intelligence. In the area of Machine Learning, various types of computational logic, such as logic programming, first-order logic, description logic and higher-order logic, have developed by incorporating with various types of learning methodologies including identification in the limit, PAC learning, on-line learning, query learning, machine discovery and learning based on Bayesian networks. Recently, on the other hand, machine learning procedures have begun to provide semantics to logic and foundations of some procedures in mathematics.		Akihiro Yamamoto;Kouichi Hirata	2005		10.1007/11780496_19	dynamic logic;instance-based learning;algorithmic learning theory;description logic;higher-order logic;statistical relational learning;computer science;artificial intelligence;machine learning;bayesian network;first-order logic;computational logic;inductive transfer;computational learning theory;active learning;multimodal logic;probably approximately correct learning;information system;algorithm;autoepistemic logic	Theory	-19.496302644902617	11.088463625243767	82374
bcc310c5a6361af313e0f6779abaed15314430d3	decomposition of a relation scheme into boyce-codd normal form	relational data;dependency preserving;decomposition;efficient algorithm;relational database;boyce codd normal form;data dependence;relational data base;lossless join	Decomposition into Boyce-Codd Normal Form (BCNF) with a lossless join and preservation of dependencies is desired in the design of a relational database scheme. However, there may be no decomposition of a relation scheme into BCNF that is dependency preserving, and the known algorithms for lossless join decomposition into BCNF require exponential time and space. In this paper we give an efficient algorithm for lossless join decomposition and show that the problem of deciding whether a relation scheme has a dependency-preserving decomposition into BCNF is NP-hard. The algorithm and the proof assume that all data dependencies are functional. We then discuss the extension of our techniques to the case where data dependencies are multivalued.	algorithm;boyce–codd normal form;data dependency;lossless compression;np-hardness;relation (database);relational database;time complexity	Don-Min Tsou;Patrick C. Fischer	1980		10.1145/800176.809996	discrete mathematics;lossless-join decomposition;theoretical computer science;join dependency;database;mathematics;functional dependency	DB	-27.091732000923265	9.625690901372636	82642
1930c36bfdb66758c186cc38931bc5629a1ce0e5	context reasoning through a multiple logic framework	rt mlr;rule based approach;object oriented methods;military applications;radar tracking;temporal logic;military application real time multi logic reasoner context aware system rt mlr rule based method object oriented architecture inference engine first order logic fuzzy logic temporal logic event based approach data fusion approach;rule based method;context radar tracking cognition correlation java fuzzy logic;inference mechanisms;context aware system;data fusion;ubiquitous computing fuzzy logic inference mechanisms knowledge based systems military computing object oriented methods sensor fusion software architecture;real time multi logic reasoner;fuzzy logic;software architecture;object oriented architecture;cognition;military application;ubiquitous computing;data fusion approach;inference engine;correlation;sensor fusion;data fusion context aware systems rule based approach military applications;context aware systems;event based approach;context;first order logic;knowledge based systems;military computing;java	This work presents an implemented framework called Real Time Multi Logic Reasoner (RT-MLR) that aims to help a development of context-aware systems. RT-MLR has been designed combining the rule based method with the regular object-oriented architecture. RT-MLR has an inference engine that allows to express and merge knowledge through rules in three logic types: First-Order Logic, Fuzzy Logic and Temporal Logic. This engine works based on an event based approach, which supports a continuous monitoring of system domain. RTMLR has been exemplified base on its capability to perform context reasoning in a military application according to data fusion approach.	drools;first-order logic;first-order predicate;fuzzy logic;inference engine;learning to rank;logic programming;ontology (information science);real-time clock;semantic reasoner;temporal logic;unified modeling language;windows rt	Pablo Rangel;José G. Carvalho Junior;Milton Ramos Ramirez;Jano Moreira de Souza	2010	2010 Sixth International Conference on Intelligent Environments	10.1109/IE.2010.29	dynamic logic;description logic;interval temporal logic;computer science;semantic reasoner;theoretical computer science;bunched logic;non-monotonic logic;machine learning;data mining;automated reasoning;probabilistic logic network;multimodal logic;temporal logic of actions;autoepistemic logic	Robotics	-23.092747643298466	4.575322820300749	82655
c06de1b1170d21a36a744f8da834cac6c10bcf50	inductive representations of rdf graphs	inductivegraphs;inductive graphs;functional programming;graph;functionalprogramming;rdf;haskell	RDF forms the basis of the semantic web technology stack. It is based on a directed graph model where nodes and edges are identified by URIs. Occasionally, such graphs contain literals or blank nodes. The existential nature of blank nodes complicates the graph representation. In this paper we propose a purely functional representation of RDF graphs using a special form of inductive graphs called inductive triple graphs. We employ logical variables to represent blank nodes. This approach can be implemented in any functional programming language such as Haskell and Scala.	blank node;directed graph;function representation;functional programming;graph (abstract data type);haskell;inductive reasoning;literal (mathematical logic);programming language;resource description framework;scala;semantic web;solution stack	José Emilio Labra Gayo;Johan Jeuring;Jose María Álvarez Rodríguez	2014	Sci. Comput. Program.	10.1016/j.scico.2013.12.011	rdf/xml;cwm;computer science;theoretical computer science;rdf;graph;programming language;functional programming;algorithm;rdf schema	PL	-22.653723188094443	13.403931930930558	82705
a1cd6c850940ab0f60d2dc93968fa0cf271e3e6b	semantic web architecture: stack or two towers?	modelizacion;ontologie;web semantique;exact solution;service web;intelligence artificielle;compatibilidad;solucion exacta;web service;modelisation;software architecture;web semantica;compatibility;semantic web;artificial intelligence;ontologia;compatibilite;inteligencia artificial;solution exacte;modeling;closed world assumption;ontology;architecture logiciel;servicio web	We discuss language architecture for the Semantic Web, and in particular different proposals for extending this architecture with a rules component. We argue that an architecture that maximises compatibility with existing languages, in particular RDF and OWL, will benefit the development of the Semantic Web, and still allow for forms of closed world assumption and negation as failure. Up until recent times it has been widely accepted that the architecture the Semantic Web will be based on a hierarchy of languages, each language both exploiting the features and extending the capabilities of the layers below. This has been famously illustrated in Tim Berners-Lee’s “Semantic Web Stack” diagram [3] (see Figure 1). As a result of the work of the W3C Web Ontology Working Group, the “Ontology” layer has now been instantiated with the Web Ontology Language OWL [2]. Since then, attention has turned to the rules layer, and much effort has been devoted to the design of suitable rules languages. Perhaps influenced by some of this work, recently seen versions of the Semantic Web Stack diagram have illustrated a weakened version of the layering idea, with rules and ontologies (OWL) sitting side by side on top of a layer labelled as the “DLP bit of OWL/Rules” [4] (see Figure 2). Unfortunately, this modified stack is based on some fundamental misconceptions about the semantic relationships between the various languages. In particular, the modified stack suggests that DLP [7] can be layered on top of RDFS and form a common basis for parallel Rules (presumably intended as Datalog/Logic Programming style rules) and OWL layers. This suggestion is, however, based on incorrect assumptions about the semantics of DLP. In particular, if we want Datalog style closed world semantics for Rules (in order to support Negation as Failure), as is argued by some proponents, then the resulting rules language is only a syntactic extension of DLP, and is not semantically compatible with DLP—in fact DLP is a subset of Horn rules and has standard First Order semantics. Fig. 1. Semantic Web Stack Fig. 2. Latest version of the Semantic Web Stack Of course it is possible to treat DLP rules as having Datalog semantics (i.e., semantics based on a closed world assumption and Herbrand models [6]). In this case, however, DLP is no longer semantically compatible with OWL and so cannot be situated below OWL in the stack. In fact, when given such a semantics, DLP (and rules languages that extend DLP) are not even semantically compatible with RDF [9]. This is easy to see if we imagine querying an RDF ontology with a more expressive query language, for example one that includes counting or negation (as, for example, SQL). Given an ontology containing only a single RDF triple: 〈#pat〉〈#knows〉〈#jo〉. the answer to a query asking if pat knows exactly one person would be “no” under RDF’s open world semantics, but “yes” under the closed world semantics of Datalog. It is thus more appropriate to view DLP with Datalog semantics as being layered directly on top of the XML layer. Datalog rules, and various extensions such as negation as failure (NAF) would then naturally layer on top of (this version of) DLP. Similarly, OWL and other First Order extensions (such as FOL or SCL [10]) would naturally layer on top of RDFS.4 It has been suggested that the two different semantics (Datalog and First Order) could be unified in some overarching “Logic Framework”, although it is an open research problem as to how this could be done. Fig. 3. Semantic Web Stack with Datalog Rules This more precise analysis of the semantic relationships between the various languages demonstrates that the Datalog view of DLP and of rules actually leads to a stack like the one illustrated in Figure 3, where the Datalog languages and First Order languages are in two separate towers. The Proof and Trust layers have been omitted, as these are currently rather speculative, as has the overarching “Logic Framework”, given that, as mentioned above, there is currently no suggestion as to what kind of logic might instantiate this layer. An alternative view of DLP is as a subset of First Order Horn clauses (as proposed in [7]). In this case DLP can be seen simply as a subset of OWL (although more useful 4 There is an issue with the meta-level features of RDFS, which has been resolved in OWL by having one language “species” that layers on top of the First Order subset of RDFS (i.e., OWL DL) and another language species that layers on top of the whole of RDFS (i.e., OWL Full). lightweight OWL subsets could be imagined, e.g., based on the EL description logic, which covers many important use cases, and for which all key inference problems can be solved in polynomial time [1]). A First Order rules language such as SWRL can then be layered on top of OWL. More expressive languages such as full First Order Logic (First Order Predicate Calculus) would layer naturally on top of SWRL [11]. Fig. 4. Semantic Web Stack with First Order Rules The resulting stack is illustrated in Figure 4 (the DLP/lightweight OWL subset layer has been omitted, but could be inserted between RDFS and OWL). This language architecture has many attractive features when compared to the one illustrated in Figure 3. On the one hand, rules in this framework extend existing work on both RDFS and OWL, as well as providing a foundation for further extensions within a coherent semantic framework. Features such as closed world assumption and negation as failure (NAF) can be supported by powerful query languages—queries already have a closed world flavour (because distinguished variables can only bind to named individuals), and it is natural to extend this with NAF by way of query subtraction (e.g., the answer to the query “faculty(?x) and NAF professor(?x)” can be computed by subtracting the answer to the query “professor(?x)” from the answer to the query “faculty(?x)”). These features are already supported in query languages such as SPARQL [14] and nRQL [8] (the query language implemented in the Racer system). Moreover, recent work on integrating rules with OWL suggests that future versions of this framework could include, e.g., a decidable subset of SWRL, and a principled integration of OWL and Answer Set Programming [5, 12, 13]. On the other hand, adopting Datalog rules (and DLP with Datalog semantics) would effectively establish two Semantic Webs, with little or no semantic interoperability between the rules based Semantic Web and the ontology based Semantic Web, even at the RDF level. These two versions of the Semantic Web would inevitably be in competition with each other, and this would make the Semantic Web much less appealing: new users would be presented with a difficult choice as to which part to choose, and in choosing would sacrifice semantic interoperability with the other part.	answer set programming;box counting;closed-world assumption;coherence (physics);datalog;description logic;diagram;digital light processing;first-order logic;horn clause;inferring horizontal gene transfer;logic programming;nato architecture framework;negation as failure;ontology (information science);open research;open world;p (complexity);polynomial;programming style;query language;rdf schema;resource description framework;sparql;sql;semantic web rule language;semantic web stack;semantic interoperability;situated;solution stack;speculative execution;stable model semantics;structured text;web ontology language;xml	Ian Horrocks;Bijan Parsia;Peter F. Patel-Schneider;James A. Hendler	2005		10.1007/11552222_4	enterprise architecture framework;web service;reference architecture;software architecture;space-based architecture;website architecture;semantic computing;closed-world assumption;semantic web rule language;systems modeling;web standards;computer science;artificial intelligence;semantic web;ontology;social semantic web;data mining;semantic web stack;database;solution architecture;compatibility;world wide web;owl-s	Web+IR	-21.32738620091532	9.350957212262545	82722
a0cbd25b7fd162b12d67cd68bf32d5e8daaf1597	reasoning about relational granulation in modal logics	multimodal logic;mathematics;history;neck;information science;closed world environment;relational granulation reasoning;rough set theory;equivalence class;logic;inference mechanisms formal logic rough set theory mathematical operators;inference mechanisms;set theory;mathematical operators;kripke model;data analysis;modal logic;equivalence relation;modal operator;formal logic;modal operator kripke model modal logic system s5 rough set theory multimodal logic relational granulation reasoning open world environment closed world environment equivalence class equivalence relation;open world environment;humans;computer science;modal logic system s5;logic set theory problem solving humans computer science information science neck history mathematics data analysis;problem solving	It is well known that the Kripke model for the modal logic system S5 can be interpreted as an approximation space in rough set theory. In this paper, we generalize the interpretation to relational granulation. We consider two multimodal logics for reasoning about relational granulation in open world and closed world environments respectively. In an open world environment, two objects are granulated into the same equivalence class only if they have the same relationship with other objects, while in a closed world environment; two objects are granulated into the same equivalence class if and only if they have the same relationship with other objects. Such equivalence relations are represented by derived modalities from modal operators representing the relationships between objects.	approximation;kripke semantics;modal logic;multimodal interaction;open world;relational database;rough set;set theory;turing completeness	Churn-Jung Liau;T. Y. Lin	2005	2005 IEEE International Conference on Granular Computing	10.1109/GRC.2005.1547348	discrete mathematics;information science;computer science;artificial intelligence;mathematics;logic;algorithm	Robotics	-26.99089294866532	8.579376067542439	82787
6e26db66ef36e2ee23d51134db5b8effdc25e2bf	incremental and sql-based data grid mining algorithm for mobility prediction of mobile users	globus toolkit;open source globus toolkit;location tracking distributed mining mobility rules knowledge grid grid computing;location tracking;location based service;mobile web environment;sql;workstation clusters data mining grid computing internet message passing mobile computing open systems parallel algorithms sql;mobility rules;cluster of workstations;incremental mining;data mining;mobility prediction;distributed mining;knowledge grid;sql based data grid mining;data mining prediction algorithms clustering algorithms algorithm design and analysis distributed algorithms mesh generation databases workstations message passing pattern analysis;mobile web;internet;message passing interface;incremental distributed algorithm;geographically distributed data grid;message passing interface extended with grid services sql based data grid mining mobility prediction incremental distributed algorithm mobile user mobile web environment parallel data mining algorithm distributed data mining algorithm geographically distributed data grid mobility pattern location based service workstation cluster open source globus toolkit;grid service;message passing;mobility pattern;distributed data mining algorithm;incremental algorithm;distributed data mining;workstation clusters;mobile computing;open systems;message passing interface extended with grid services;distributed algorithm;grid computing;parallel data mining algorithm;geographic distribution;data grid;open source;mobile user;workstation cluster;parallel algorithms	In this paper, we propose a new SQL based incremental distributed algorithm for predicting the next location of a mobile user in a mobile web environments. Parallel and Distributed data mining algorithm is applied on moving logs stored in geographically distributed data grid to generate the mobility pattern, which provides various location based services to the mobile users. One of the existing works for deriving mobility pattern is re-executing the algorithm from scratch results in excessive computation. In our work, we have designed new incremental algorithm by maintaining infrequent mobility patterns, which avoids unnecessary scan of full database. We built data grid system on a cluster of workstation using open source Globus Toolkit (GT) and Message Passing Interface extended with Grid Services (MPICH-G2). The experiments were conducted on original data sets with incremental addition of data and the computation time was recorded for each data sets. We analyzed our results with various sizes of data sets and it shows the time taken to generate mobility pattern by incremental mining algorithm is less than re-computing approach.	computation;data mining;distributed algorithm;experiment;location-based service;mpich;message passing interface;open-source software;sql;time complexity;workstation	U. Sakthi;R. S. Bhuvaneswaran	2009	2009 International Conference on Computational Science and Its Applications	10.1109/ICCSA.2009.6	distributed algorithm;sql;message passing;the internet;mobile web;computer science;message passing interface;operating system;location-based service;data grid;database;distributed computing;parallel algorithm;open system;mobile computing;world wide web;grid computing	Robotics	-30.6127603025177	17.05702248058886	82806
dd94b4e4f03924e7518887573137396538a2cf3f	two-faced data		Intent One data fragment has several alternative structural representations tailored toward specic data manipulation approaches.		Vadim Zaytsev	2015			data mining;data profiling;data set;computer science	DB	-31.1449349460966	7.085353680963959	82900
314ca851e275f33f2badc607791b12f9b007749b	an inductive logic programming framework to learn a concept from ambiguous examples	learning algorithm;analisis estructural;apprentissage inductif;biologia molecular;inductive logic programming;intelligence artificielle;algorithme apprentissage;logical programming;programmation logique;molecular biology;inductive learning;learning problems;artificial intelligence;inteligencia artificial;acido nucleico;analyse structurale;acide nucleique;structural analysis;algoritmo aprendizaje;programacion logica;nucleic acid;object relational;biologie moleculaire	"""We address a learning problem with the following peculiarity : we search for characteristic features common to a learning set of objects related to a target concept. In particular we approach the cases where descriptions of objects are ambiguous : they represent several incompatible realities. Ambiguity arises because each description only contains indirect information from which assumptions can be derived about the object. We suppose here that a set of constraints allows the identification of """"coherent"""" sub-descriptions inside each object. We formally study this problem, using an Inductive Logic Programming framework close to characteristic induction from interpretations. In particular, we exhibit conditions which allow a pruned search of the space of concepts. Additionally we propose a method in which a set of hypothetical examples is explicitly calculated for each object prior to learning. The method is used with promising results to search for secondary substructures common to a set of RNA sequences."""	ambiguous grammar;coherence (physics);inductive logic programming	Dominique Bouthinon;Henry Soldano	1998		10.1007/BFb0026694	nucleic acid;artificial intelligence;machine learning;mathematics;structural analysis;algorithm	ML	-20.072790484806205	10.725689356062894	83054
0cd33790e9edfeea0d4d839f800326cf45ca0c84	identifying pre-conditions with the z/eves theorem prover	skeleton electrical capacitance tomography unified modeling language natural languages read only memory automation testing formal specifications encapsulation;encapsulation;logical implication;preconditions identification;electrical capacitance tomography;object oriented methods;formal specification;data description;formal specifications;formal specification skeleton;natural languages;testing;logical implication preconditions identification z eves theorem prover graphical data model omt object model formal specification skeleton constraints formal data description data model instance modification operations;skeleton;data model;theorem proving;theorem prover;computer aided software engineering;data model instance modification operations;unified modeling language;z eves theorem prover;graphical data model;formal data description;read only memory;data description formal specification theorem proving computer aided software engineering data models object oriented methods;constraints;data models;object model;omt object model;automation	Starting from a graphical data model (a subset of the OMT object model), a skeleton of formal specification can be generated and completed to express several constraints and provide a precise formal data description. Then standard operations to modify instances of this data model can be systematically specified. Since these operations may invalidate the constraints, it is interesting to identify their preconditions. In this paper, the Z-EVES theorem prover is used to calculate and try to simplify the preconditions of these operations. Then, the developer may identify a set of conditions and use the prover to verify that they logically imply the pre-condition. Y. Ledru. Identifying pre-conditions with the Z/EVES theorem prover. In Proceedings of the 13th International Conference on Automated Software Engineering. pp. 32-41, IEEE Computer Society Press, Honolulu, 1998. This material is presented to ensure timely dissemi nation of scholarly and technical work. Copyright and all rights therein ar e retained by authors or by other copyright holders. All persons copying thi s information are expected to adhere to the terms and constraints inv oked by each author's copyright. In most cases, these works may not be re posted without the explicit permission of the copyright holder. ©1998 IEEE. Personal use of this material is permit ted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted compo nent of this work in other works must be obtained from the IEEE. Identifying pre-conditions with the Z/EVES theorem prover Yves Ledru Laboratoire Logiciels Systèmes Réseaux IMAG B.P. 72, F-38402 St-Martin-d’Hères cedex, FRANCE Yves.Ledru@imag.fr	automated theorem proving;data model;demoscene compo;formal specification;graphical user interface;precondition;software engineering	Yves Ledru	1998		10.1109/ASE.1998.732566	computer science;theoretical computer science;formal specification;automated theorem proving;programming language;algorithm	SE	-26.43936794159923	18.00136013427203	83121
8953870f24d1cdc0ce6ccad24ebae4d4bea7e1eb	applying dac principles to the rdf graph data model	conference paper	In this paper we examine how Discretionary Access Control principles, that have been successfully applied to relational and XML data, can be applied to the Resource Description Framework (RDF) graph data model. The objective being to provide a baseline for the specification of a general authorisation framework for the RDF data model. Towards this end we provide a summary of access control requirements for graph data structures, based on the different characteristics of graph models compared to relational and tree data models. We subsequently focus on the RDF data model and identify a list of access rights based on SPARQL query operations; propose a layered approach to authorisation derivation based on the graph structure and RDFSchema; and demonstrate how SQL GRANT and REVOKE commands can be adapted to cater for delegation of privileges in SPARQL.	authorization;baseline (configuration management);data model;data structure;defense in depth (computing);discretionary access control;graph (abstract data type);requirement;resource description framework;sparql;sql;xml	Sabrina Kirrane;Alessandra Mileo;Stefan Decker	2013		10.1007/978-3-642-39218-4_6	turtle;computer science;sparql;data mining;database;rdf query language;information retrieval;rdf schema	Web+IR	-32.873693549658185	6.289336748169511	83173
a9164b82f35e054b71ae94bacac1bab02ac260fb	web query optimizer	electrical capacitance tomography;information resources;technological innovation;web query optimizer;cost function;query processing;wrappers;information retrieval;prototypes;metrics;query optimization;testing;web environment;randomized relational optimizer;cbr tool;feedback;rewriting systems;wqo;database systems;statistics;websources;wide area networks information resources query processing rewriting systems relational databases;relational databases;wide area environment;webwrapper cost model;mediators;metrics web query optimizer mediators wqo wrappers websources wide area environment cbr tool capability based rewriting tool randomized relational optimizer web environment webwrapper cost model;cost model;wide area networks;technological innovation feedback educational institutions cost function prototypes testing database systems information retrieval electrical capacitance tomography statistics;capability based rewriting tool	We demonstrate a Web Query Optimizer (WQO) within an architecture of mediators and wrappers, for WebSources of limited capability in a wide area environment. The WQO has several innovative features including a CBR (capability based rewriting) Tool, an enhanced randomized relational optimizer extended to a Web environment, and a WebWrapper cost model that can provide relevant metrics for accessing WebSources. The prototype has been tested against a number of WebSources.	analysis of algorithms;case-based reasoning;mathematical optimization;prototype;query optimization;randomized algorithm;rewriting	Vladimir Zadorozhny;Laura Bright;Louiqa Raschid;Tolga Urhan;Maria-Esther Vidal	2000		10.1109/ICDE.2000.839484	query optimization;relational database;computer science;data mining;feedback;database;prototype;software testing;world wide web;metrics	DB	-32.618934756347095	6.522530532686032	83175
5f14616f8f0b8a6de0d1c2213be6d100ca358da5	a new uml-compatible object relationship notation (orn)	data mart;object relationship notation;information science curriculum;decision support system;unified modeling language;data warehouse;oracle database;database management system	Object Relationship Notation (ORN) is a declarative scheme that permits the semantics of a variety of relationships to be conveniently modeled and then defined to a Database Management System (DBMS), which then allows the DBMS to automatically enforce these semantics. The Unified Modeling Language (UML), which has gained much popularity since its beginning in 1994, provides another notation for describing the semantics of relationships. We have revised ORN to be compatible with this notation. This paper presents the new UML-compatible ORN.	database;declarative programming;management system;unified modeling language	Erika T. Neal	2000		10.1145/1127716.1127756	computer science;data mining;database;programming language	DB	-32.814587311264766	11.116761645150618	83411
a862dd111261991013a4c01776aff40b20b9a0e7	efficient method for processing consecutive events over streams with a negation link	query processing;educational institutions radiofrequency identification electronic mail pattern matching conferences data engineering indexes;negation link event streams consecutive event processing in negations pattern matching complex events filter data streams query processors	Recently developed query processors for complex events filter data streams to match specific patterns and transform streaming data into meaningful information. To specify the patterns to be matched, complex event processing languages adopt a negation to present an event that should not appear at a specific position. A negation engenders more expressive query forms; however, the indexes must be looked up for each result sequence. In this paper, we review an efficient processing technique for a negation and propose the consecutive event processing in negations (CEPIN) for processing consecutive events in a query with negations. The key component of this proposal is the investigation as to whether the present event type is identical to a previous event type. If it is identical, a calculation to determine the resulting sequence range is not necessary; moreover, the time required for the processing of a negation and the reverse order of a link can be saved. Our experiment shows a reduction in the number of processes and calculations required per event.	central processing unit;complex event processing;negation as failure;stream (computing)	Young Cheul Kim;Jongik Kim;Hyung-Ju Cho;Tae-Sun Chung	2014	2014 International Conference on IT Convergence and Security (ICITCS)	10.1109/ICITCS.2014.7021762	computer science;theoretical computer science;data mining;database	DB	-29.598959478476182	4.43469199622	83756
05d21a0984a92310131917ed22c255ff29a93b6c	sciql, a query language for science applications	scientific application;query language;query processing;scientific data;universiteitsbibliotheek;relational database system	Scientific applications are still poorly served by contemporary relational database systems. At best, the system provides a bridge towards an external library using user-defined functions, explicit import/export facilities or linked-in Java/C# interpreters. Time has come to rectify this with SciQL1, a SQL query language for scientific applications with arrays as first class citizens. It provides a seamless symbiosis of array-, set-, and sequence- interpretation using a clear separation of the mathematical object from its underlying implementation. A key innovation is to extend value-based grouping in SQL:2003 with structural grouping, i.e., fixed-sized and unbounded groups based on explicit relationships between their dimension attributes. It leads to a generalization of window-based query processing with wide applicability in science domains. This paper is focused on the language features, extensively illustrated with examples of its intended use.	first-class function;java;monetdb;query language;relational database;sql;sql:2003;seamless3d;select (sql);xslt/muenchian grouping	Martin L. Kersten;Milena Ivanova;Niels Nes	2010		10.1145/1966895.1966896	sargable;data definition language;query optimization;.ql;relational database management system;query by example;rdf query language;view;query language;data;object query language	PL	-29.45955275954815	10.20950504398683	83809
35fb7fc68635fca07696b3bbbba3f11375e15831	synchronized regular expressions	search algorithm;pattern matching;string matching;regular expression;formal language	Abstract   Text manipulation is one of the most common tasks for everyone using a computer. The increasing number of textual information in electronic format that every computer user collects everyday stresses the need of more powerful tools to interact with texts. Indeed, much work has been done to provide nonprogramming tools that can be useful for the most common text manipulation issues. Regular Expressions (RE), introduced by Kleene, are well–known in the formal language theory. RE received several extensions, depending on the application of interest. In almost all the implementations of RE search algorithms (e.g. the egrep [A] UNIX command, or the Perl [17] language pattern matching constructs) we find  backreferences  (as defind in [1]), i.e. expressions that make reference to the string matched by a previous subexpression. Generally speaking, it seems that all the kinds of synchronizations between subexpressions in a RE can be very useful when interacting with texts. Therefore, we introduce the Synchronized Regular Expressions (SRE) as a derivation of the Regular Expressions. We use SRE to present a formal study of the already known backreferences extension, and of a new extension proposed by us, which we call the  synchronized exponents . Moreover, since we are talking about formalisms that should have a practical utility and can be used in the real world, we have the problem of how to present SRE to the  final users . Therefore, in this paper we also propose a user–friendly syntax for SRE to be used in implementations of SRE–powered search algorithms.	regular expression	Giuseppe Della Penna;Benedetto Intrigila;Enrico Tronci;Marisa Venturini Zilli	2001	Electr. Notes Theor. Comput. Sci.	10.1016/S1571-0661(04)00327-5	formal language;computer science;theoretical computer science;pattern matching;mathematics;programming language;regular expression;algorithm;string searching algorithm;search algorithm	Logic	-25.826136763576695	17.790662811013824	83977
8bc8e57486c90c77cbfcf3b371b71bec51a7af01	handling incomplete and complete tables in tabled logic programs	search space;inductive logic programming;logical programming;programmation logique;logic programs;programacion logica	Most of the recent proposals in tabling technology were designed as a means to improve the performance of particular applications in key aspects of tabled evaluation like re-computation and scheduling. The discussion we address in this work was also motivated by our recent attempt [1] of applying tabling to Inductive Logic Programming (ILP) [2]. ILP applications are very interesting for tabling because they have huge search spaces and do a lot of re-computation. Moreover, we found that they are an excellent case study to improve some practical limitations of current tabling execution models. In particular, we next focus on the table space and how to efficiently handle incomplete and complete tables. Tabling is about storing answers for subgoals so that they can be reused when a repeated call appears. On the other hand, most ILP algorithms are interested in example satisfiability, not in the answers: query evaluation stops as soon as an answer is found. This is usually implemented by pruning at the Prolog level. Unfortunately, pruning over tabled computations results in incomplete tables : we may have found several answers but not the complete set. Thus, usually, when a repeated call appears we cannot simply trust the answers from an incomplete table because we may loose part of the computation. The simplest approach, and the one that has been implemented in most tabling systems, is to throw away incomplete tables, and restart the evaluation from scratch. In this work, we propose a more aggressive approach where, by default, we keep incomplete tables around. Whenever a call for an incomplete table appears, we first consume the answers from the table. If the table is exhausted, then we will restart the evaluation from the beginning. Later, if the subgoal is pruned again, then the same process is repeated until eventually the subgoal is completely evaluated. The main goal of this proposal is to avoid re-computation when the already stored answers are enough to evaluate a repeated call. This idea is closer to the spirit of the just enough tabling (JET) proposal of Sagonas and Stuckey [3]. Our approach works well in the ILP setting, where queries are often very similar, and thus already stored answers are enough to evaluate a		Ricardo Rocha	2006		10.1007/11799573_34	searching the conformational space for docking;artificial intelligence;theoretical computer science;algorithm	OS	-20.346470811086675	14.668070754244273	84316
af996521b6e9124d2b4ec7e2ebf85ce9ca9e4ac5	testing natural language grammars		Testing grammars has one big difference from testing software: natural language has no formal specification, so ultimately we must involve a human oracle. However, we can automate many useful subtasks: detect ambiguous constructions and contradictory grammar rules, as well as generate minimal and representative set of examples that cover all the constructions. Think of the whole grammar as a haystack, and we suspect there are a few needles–we cannot promise automatic needle-removal, but instead we help the human oracle to narrow down the search.	formal specification;natural language	Inari Listenmaa	2018	2018 IEEE 11th International Conference on Software Testing, Verification and Validation (ICST)	10.1109/ICST.2018.00054	computer science;real-time computing;formal specification;haystack;natural language;software;suspect;natural language processing;oracle;artificial intelligence;rule-based machine translation;grammar	SE	-25.64692066356988	17.354537462612694	84533
084a03155e877d25dfd78e4cbbbe0b996f7aa3db	dynamic derived relations within the raquel ii dbms	derived relation;defined relation;view;relational database;explicit updates;update methods;snapshot;dynamic derived relation;implicit updates;copy;base relations;static derived relation	An implementation technique for supporting dynamic derived relations within a particular relational DBMS (the RAQUEL II DBMS) is presented. Included is a discussion of the RAQUEL II concepts of permanent and temporary relations, base and derived relations, and explicit and implicit updates. The methods presented here for support of updates in regard to dynamic derived relations take the form of immediate updates. Proofs are presented to show the validity of the update methods.	object-relational database;relational database management system	Kathryn C. Kinsley;James R. Driscoll	1979		10.1145/800177.810034	computer science;theoretical computer science;data mining;database	DB	-28.513026298833655	9.827233962779962	84699
0e6b7e09572c2ef77599787a5943c972a46c97ca	querying virtual hierarchies using virtual prefix-based numbers	prefix based numbering;xml;virtual hierarchy;view query	Prefix-based numbering is a popular method for numbering nodes in a hierarchy. But prefix-based numbering breaks down when a node's location within a hierarchy changes, such as when XML data is queried after being transformed by an XSLT program or when data is reformatted in the return clause of an inner FLWR expression in a nested XQuery program. A query on transformed data cannot be evaluated as efficiently since the extant prefix-based node numbers cannot be used (unless the data is materialized and then renumbered, which can be expensive). In this paper we present a novel strategy to virtually transform the data without instantiating and renumbering. Our method, which we call virtual prefix-based numbering, couples each prefix-based node number with a level array that locates the node in the numbering space of the virtual hierarchy. The virtual numbering space preserves the property that location-based relationships between nodes can be determined by comparing (virtual) numbers.	flwor;instance (computer science);location-based service;xml;xquery;xslt	Curtis E. Dyreson;Sourav S. Bhowmick;Ryan Grapp	2014		10.1145/2588555.2610506	xml;computer science;global value numbering;theoretical computer science;database;programming language;prefix delegation;algorithm	DB	-29.57140841192677	5.6677879838831435	84817
449e3aea4c3c0e41b2efa73159d9eebc2ba85d90	cooperative information systems	databases;computer architecture;electronic commerce;manufacturing;management information systems;information retrieval;information systems;application software	This paper summarizes the model management approach to generic schema management. The goal is to raise the level of abstraction of data manipulation for programmers of design tools and other model-driven applications. We explain the main concepts of model management, report on some recent progress on model matching algorithms, and sketch a category-theoretic approach to a formal semantics for model management.	algorithm;category theory;enhanced entity–relationship model;expect;information system;model-driven architecture;phil bernstein;programmer;semantics (computer science);standard data model	Enrico Blanzieri;Paolo Giorgini;Paolo Massa;Sabrina Recla	2001		10.1007/3-540-44751-2	e-commerce;information technology architecture;computing;application software;information technology management;information engineering;computer science;knowledge management;three schema approach;personal information management;management information systems;database;systems development life cycle;manufacturing;automated information system;information architecture;information system;software system;human–computer information retrieval	DB	-31.68476003189946	10.788215160484768	84936
7d869c1ebd75a28ea42d550def07d36e38393f74	fine grained information integration with description logics.		We outline an approach to query optimization in which a description logic (DL) reasoner serves a crucial strategic role, and present an example application of our approach in fine grained information integration. In particular, the application demonstrates how the internal structure of an unfolded Btree can be captured as a terminology, and how an access plan that navigates this internal structure can be found with the aid of a DL reasoner.	b-tree;description logic;mathematical optimization;query optimization;query plan;semantic reasoner	Huizhu Liu;David Toman;Grant E. Weddell	2002			description logic;information integration;theoretical computer science;computer science	AI	-29.05862408902144	10.105035547961103	85219
bf9ece3e5a88e648fa79463afed44471805c7fdd	from spreadsheets to relational databases and back	type safe data migration;spreadsheets;data migration;relational database;functional programming;data model;bi directional transformations	This paper presents techniques and tools to transform spreadsheets into relational databases and back. A set of data refinement rules is introduced to map a tabular datatype into a relational database schema. Having expressed the transformation of the two data models as data refinements, we obtain for free the functions that migrate the data. We use well-known relational database techniques to optimize and query the data. Because data refinements define bi-directional transformations we can map such database back to an optimized spreadsheet. We have implemented the data refinement rules and we constructed Haskell-based tools to manipulate, optimize and refactor Excel-like spreadsheets.	code refactoring;data model;database schema;haskell;refinement (computing);relational database;spreadsheet;table (information)	Jácome Cunha;João Saraiva;Joost Visser	2009		10.1145/1480945.1480972	data modeling;sql;data migration;relational model;entity–relationship model;data model;relational database;computer science;database model;data mining;database;change data capture;conjunctive query;programming language;functional programming;view;database design	DB	-29.470273242213796	11.021420957062865	85444
a7710115185112b8f7130ee109cded2d5bfb2571	hypothesis support for information integration in four-valued logics	integration information;logical programming;systeme base connaissances;deductive database;information integration;programmation logique;base dato deductiva;informatique theorique;integracion informacion;base donnee deductive;logic programs;analisis semantico;analyse semantique;programacion logica;knowledge based systems;semantic analysis;computer theory;deductive databases;knowledge base;informatica teorica	We address the problem of integrating information coming from different sources. The information consists of facts that a central server collects and tries to combine using (a) a set of logical rules, i.e. a logic program, and (b) a hypothesis representing the server's own estimates. In such a setting incomplete information from a source or contradictory information from different sources necessitate the use of many-valued logics in which programs can be evaluated and hypotheses can be tested. To carry out such activities we propose a formal framework based on Belnap's four-valued logic. In this framework we work with the class of programs defined by Fitting and we develop a theory for information integration. We also establish an intuitively appealing connection between our hypothesis testing mechanism on the one hand, and the well-founded semantics and Kripke-Kleene semantics of Datalog programs with negation, on the other hand.		Yann Loyer;Nicolas Spyratos;Daniel Stamate	2000		10.1007/3-540-44929-9_37	knowledge base;computer science;artificial intelligence;information integration;data mining;database;algorithm	Theory	-21.170756554582702	11.05716076794254	85500
67ae36ce6cb79cf7facec64f750ff7bf4f6edefb	an architecture for evolutionary database system design	database system;information science;application software;software maintenance;software systems;computer architecture;database systems application software costs computer architecture information science software maintenance software systems;database systems;database design;data management system	We outline herein an approach to evolutionary database systems design and evolutionary database design which is intended to lower the total costs of maintaining, developing and executing application programs against a database. The premise is that it should be possible to have an evolutionary database system which executes efficiently while providing explicit support for changes in application usage. The key concepts are: hierarchical layering of a database specification, inclusion of static/dynamic characteristics in specification of the database, and a hybrid compile/interpret system for the execution phase of the data management system itself.	database;systems design	Tosiyasu L. Kunii;James C. Browne;Hideko S. Kunii	1978		10.1109/CMPSAC.1978.810418	cost database;data modeling;database theory;application software;database server;intelligent database;database transaction;database tuning;information science;computer science;systems engineering;data administration;software engineering;database model;data mining;database;software maintenance;view;database schema;distributed database;physical data model;database testing;database design;software system;component-oriented database	DB	-32.85472021680478	12.272248957422349	85659
2dd02ecf440e3e16e250a29f4c28e0a5cf3fc7b4	an introduction to aditi deductive database system	deductive databases		deductive database	Jayen Vaghani;David B. Kemp;Peter J. Stuckey	1991	Australian Computer Journal		computer science	DB	-30.52526001581617	9.670230584865568	85668
1084d14de2d78758f4e219fcb49480563f89906c	an auditing protocol for spreadsheet models	modelizacion;interfase usuario;spreadsheet;end user computing;auditing;user interface;spreadsheets;protocol design;useful information;informacion util;modelisation;auditing software;spreadsheet errors;interface utilisateur;audicion;tableur;modeling;audit;information utile	Errors are prevalent in spreadsheets and can be extremely difficult to find. A number of audits of existing spreadsheets have been reported, but few details have been given about how the audits were performed. We developed and tested a new spreadsheet auditing protocol designed to find errors in operational spreadsheets. Our work showed which auditing procedures, used in what sequence and combination, were most effective across a wide range of spreadsheets. It also provided useful information on the size and complexity of operational spreadsheets, as well as the frequency with which certain types of errors occur. 2008 Elsevier B.V. All rights reserved.	canonical account;general-purpose markup language;operational semantics;problem domain;spreadsheet	Stephen G. Powell;Kenneth R. Baker;Barry Lawson	2008	Information & Management	10.1016/j.im.2008.03.004	economics;computer science;data mining;database;management;audit;world wide web	SE	-33.10545756099216	15.990075359223617	85855
da7f5c305b90e6bc34bad4f7201584ce3e84ad5a	experiments in reactive constraint logic programming	constraint logic programs;dynamic environment	In this paper we study a reactive extension of constraint logic programming. Our primary concerns are search problems in a dynamic environment , where interactions with the user (e.g. in interactive multi-criteria optimization problems) or interactions with the physical world (e.g. in time evolving problems) can be modeled and solved eeciently. Our approach is based on a complete set of query manipulation commands for both the addition and the deletion of constraints and atoms in the query. We deene a fully incremental model of execution which, contrary to other proposals, retains as much information as possible from the last derivation preceding a query manipulation command. The completeness of the execution model is proved in a simple framework of transformations for CSLD derivations, and of constraint propagation seen as chaotic iteration of closure operators. A prototype implementation of this execution model is described and evaluated on two applications.	constraint logic programming;interaction;iteration;local consistency;mathematical optimization;prototype;search problem;software propagation	François Fages;Julian Fowler;Thierry Sola	1998	J. Log. Program.	10.1016/S0743-1066(98)10008-0	constraint logic programming;concurrent constraint logic programming;constraint programming;discrete mathematics;constraint satisfaction;computer science;artificial intelligence;theoretical computer science;mathematics;programming language;algorithm	PL	-19.829220221988106	15.875442616822523	85908
62485ef93def4343fd3813131a25dee655a75135	a generic set theory-based pattern matching approach for the analysis of conceptual models	structural model;conceptual modeling;business process improvement;conceptual model;set theory;modeling language;model comparison;pattern matching;modeling tool	Recognizing patterns in conceptual models is useful for a number of purposes, like revealing syntactical errors, model comparison, and identification of business process improvement potentials. In this contribution, we introduce an approach for the specification and matching of structural patterns in conceptual models. Unlike existing approaches, we do not focus on a certain application problem or a specific modeling language. Instead, our approach is generic making it applicable for any pattern matching purpose and any conceptual modeling language. In order to build sets representing structural model patterns, we define operations based on set theory, which can be applied to arbitrary sets of model elements and relationships. Besides a conceptual specification of our approach, we present a prototypical modeling tool that shows its applicability.	pattern matching;set theory	Jörg Becker;Patrick Delfmann;Sebastian Herwig;Lukasz Lis	2009		10.1007/978-3-642-04840-1_6	natural language processing;data modeling;conceptual model;computer science;conceptual schema;conceptual model;machine learning;domain model;data mining;modeling language;programming language	SE	-31.608228954824504	14.001658803435852	86007
a24ed818c54a1a4195cf1308df5816a6234b1e84	a hardware pattern matching algorithm on a dataflow	adaptacion;traitement texte;base donnee;information retrieval;flot donnee;recuperacion de informacion;database;algorithme;algorithm;algorritmo;recherche information;pattern matching;adaptation;base datos;data flow;word processing		algorithm;dataflow;pattern matching	Sakti Pramanik;Chung-Ta King	1985	Comput. J.	10.1093/comjnl/28.3.264	dataflow architecture;data flow diagram;speech recognition;computer science;pattern matching;signal programming;database;programming language;algorithm;adaptation	EDA	-27.292321768322985	6.037804836136783	86318
d012f3321835f58a20f764d1d7bfdb124e3ea32b	basic algorithms and tools for rough non-deterministic information analysis	dependencia dato;no determinismo;data dependency;relation equivalence;analisis datos;rough set theory;probabilistic approach;systeme non deterministe;systeme deterministe;reduction donnee;analisis programa;non deterministic system;data analysis;equivalence relation;non determinism;couverture;sistema determinista;non determinisme;theorie ensemble approximatif;data dependence;enfoque probabilista;approche probabiliste;dependance donnee;reduccion datos;analyse donnee;analyse information;coverage;data reduction;program analysis;sistema no determinista;information system;analyse programme;rough set;relacion equivalencia;information analysis;systeme information;deterministic system;sistema informacion;cobertura	Rough non-deterministic information analysis is a framework for handling the rough sets based concepts, which are defined in not only DISs (Deterministic Information Systems) but also NISs (Non-deterministic Information Systems), on computers. NISs were proposed for dealing with information incompleteness in DISs. In this paper, two modalities, i.e., the certainty and the possibility, are defined for each concept like the definability of a set, the consistency of an object, data dependency, rule generation, reduction of attributes, criterion of rules support, accuracy and coverage. Then, each algorithm for computing two modalities is investigated. An important problem is how to compute two modalities depending upon all derived DISs. A simple method, such that two modalities are sequentially computed in all derived DISs, is not suitable. Because the number of all derived DISs increases in exponential order. This problem is uniformly solved by means of applying either in f and sup information or possible equivalence relations. An information analysis tool for NISs is also presented.	algorithm	Hiroshi Sakai;Akimichi Okuma	2004		10.1007/978-3-540-27794-1_10	rough set;computer science;artificial intelligence;machine learning;mathematics;data analysis;algorithm;statistics	EDA	-21.667114275914	11.196429680407755	86348
45041676530d804f36da6d4e684e14f63d47d9a7	semantic indexing based on description logics	indexation;description logic	A method for constructing and maintaining a `semantic index' using a system based on description logics is described. A persistent index into a large number of objects is built by classifying the objects with respect to a set of indexing concepts and storing the resulting relation between object-ids and most speciic indexing concepts on a le. These les can be incre-mentally updated. The index can be used for eeciently accessing the set of objects matching a query concept. The query is classiied, and, based on subsumption and disjointness reasoning with respect to indexing concepts, instances are immediately categorized as hits, misses or candidates with respect to the query. Based on the index only, delayless feedback concerning the cardinality of the query (upper and lower bounds) can be provided during query editing.	categorization;description logic;subsumption architecture	Albrecht Schmiedel	1994			latent semantic indexing;semantic computing;semantic similarity;information retrieval;search engine indexing;description logic;natural language processing;indexation;artificial intelligence;computer science	DB	-26.576840660114094	7.739070881135521	86377
b7c982d7344ffbb52f0c9781af381f034f84b2bd	object-preserving class transformations		Object-Preserving Class Transformations Paul L. Bergstein Northeastern University, College of Computer Science Cullinane Hall, 360 Huntington Ave., Boston MA 02115 pberg@corwin.CCS.northeastern.EDU Abstract Reorganization of classes for object-oriented programming and object-oriented database design has recently received considerable attention in the literature. In this paper a small set of primitive transformations is presented which forms an orthogonal basis for object-preserving class reorganizations. This set is proven to be correct, complete, and minimal. The primitive transformations help form a theoretical basis for class organization and are a powerful tool for reasoning about particular organizations.	computer science;database design	Paul L. Bergstein	1991		10.1145/117954.117977	method;deep-sky object;object model;computer science;object;programming language;object-oriented programming;object definition language	PL	-27.866103175528725	14.653278488601043	86733
6e21628966ccae7c7c5a8c07054d3ed538913e2a	verification and validation of uml conceptual schemas with ocl constraints	conceptual modeling;conceptual model;satisfiability;conceptual schema;ocl;information system;point of view;verification and validation;article;constraints	To ensure the quality of an information system, it is essential that the conceptual schema that represents the knowledge about its domain is semantically correct. The semantic correctness of a conceptual schema can be seen from two different perspectives. On the one hand, from the point of view of its definition, a conceptual schema must be right. This is ensured by means of verification techniques that check whether the schema satisfies several correctness properties. On the other hand, from the point of view of the requirements that the information system should satisfy, a schema must also be the right one. This is ensured by means of validation techniques, which help the designer understand the exact meaning of a schema and to see whether it corresponds to the requirements. In this article we propose an approach to verify and validate UML conceptual schemas, with arbitrary constraints formalized in OCL. We have also implemented our approach to show its feasibility.	addendum;conceptual schema;correctness (computer science);database schema;formal verification;information system;maximal set;object constraint language;point of view (computer hardware company);prototype;requirement;terminate (software);unified modeling language;verification and validation	Anna Queralt;Ernest Teniente	2012	ACM Trans. Softw. Eng. Methodol.	10.1145/2089116.2089123	schema migration;conceptual model;logical schema;computer science;three schema approach;conceptual schema;conceptual model;data mining;database;programming language;database schema	DB	-32.539480090813235	14.285291992222627	87256
ce49faa86f4ae8574a73451afa115721d76d5459	timed automata model to improve the classification of a sequence of images	general methods;timed automata	The aim of this paper is to propose the use of a dynamic plot model to improve landcover classification on a sequence of images. This new approach consists in representing the plot as a dynamic system and in modeling its evolution (knowledge about crop cycles) using the timed automata formalism. In order to refine results obtained by a traditional classifier, observations given by a preliminary classification of images are matched with expected states provided by an automaton simulation. The paper presents the modeling captured by the timed automata formalism and the general method, which is based on prediction and postdiction mechanisms, that have been adopted to improve the classification of a sequence of images. Finally, the interest of the method is demonstrated through experimental results.	automata theory;dynamical system;experiment;high- and low-level;refinement (computing);semantics (computer science);sensor;simulation;statistical classification;timed automaton	Christine Largouët;Marie-Odile Cordier	2000			simulation;computer science;artificial intelligence;algorithm	AI	-24.103102152835948	16.319349001918173	87459
488f349215874f6b4d2efa97ee1b5da9b90d6a33	expressive power of an algebra for data mining	additional key words and phrases: algebra;expressive power;data mining	The relational data model has simple and clear foundations on which significant theoretical and systems research has flourished. By contrast, most research on data mining has focused on algorithmic issues. A major open question is: what's an appropriate foundation for data mining, which can accommodate disparate mining tasks&quest; We address this problem by presenting a database model and an algebra for data mining. The database model is based on the 3W-model introduced by Johnson et al. [2000]. This model relied on black box mining operators. A main contribution of this article is to open up these black boxes, by using generic operators in a data mining algebra. Two key operators in this algebra are regionize, which creates regions (or models) from data tuples, and a restricted form of looping called mining loop. Then the resulting data mining algebra MA is studied and properties concerning expressive power and complexity are established. We present results in three directions: (1) expressiveness of the mining algebra; (2) relations with alternative frameworks, and (3) interactions between regionize and mining loop.	data mining	Toon Calders;Laks V. S. Lakshmanan;Raymond T. Ng;Jan Paredaens	2006	ACM Trans. Database Syst.	10.1145/1189770	concept mining;functional analysis;text mining;data processing;computer science;data mining;database;language;algorithmics;information extraction;algorithm	DB	-24.868328754143008	10.601114590715303	87627
d253f2c234304b9e10ff8d8269bd819e053db98c	ct-olap: temporal multidimensional data model and algebra for moving objects	moving object;sequenced queries;multidimensional data;discrete time;olap;traffic management;multidimensional database;temporal database;associative algebra;data warehousing;traffic analysis;moving objects;temporal databases;multidimensional databases;aggregation queries	Sequenced queries are queries that are conceptually evaluated at each time instant. Therefore, this kind of queries return functions of time. Some applications (e.g., traffic analysis) need support for sequenced queries on data about continuous changes. Current temporal OLAP technology does not support this type of queries. Existing TOLAP models and algebras are based on discrete time. Hence, they can not represent data and query about continuous changes (i.e., continuous functions of time). In this paper, we propose a conceptual multidimensional model, CT-OLAP, that captures continuous functions of time. Its associated algebra supports sequenced analytical queries. CT-OLAP extends an existing powerful multidimensional model and algebra. CT-OLAP can be implemented in the Secondo DBMS. The motivation for this work is based on the requirements to a tool for traffic jam analysis formulated by the Municipality of Bozen-Bolzano, Italy.	ct scan;data model;jam;online analytical processing;requirement;traffic analysis	Eilverijus Kondratas;Igor Timko	2007		10.1145/1317331.1317346	computer science;data science;data warehouse;data mining;database;temporal database	DB	-29.330426308504723	7.778220095096199	87790
54264df10a6cd2f8af58f02fe0f31a6d1cbbfd47	models for studying concurrency control performance: alternatives and implications	design automation;knowledge base design;semantic modelling;terminological knowledge acquisition;conceptual modelling;concurrency control;knowledge integration;database design;knowledge based systems	A number of recent studies hale exammed the performance of concurrent\ control algorithms for database management s\stems The results reported to date rather than bemg defimove ha\e tended to be contradIctor> In 011s paper rather than presenting “yet another algorithm performance stud)” we crlbcally mvesngate the assumwons made m the models used m past studies and their Imphcatlons We employ a “complete” model of a database emlronment to study the relatn~e performance of three dlfferent approaches to the concurrency control problem under a \arleb of modehng assumptions We shon how differences m the underlymg assumpnons explam the seemmgl\ contradlctorq performance results We also examine how reahsnc the various assumptions would be for “real” database sqqtems	algorithm;concurrency control;database;yet another	Rakesh Agrawal;Michael J. Carey;Miron Livny	1985		10.1145/318898.318909	knowledge base;knowledge integration;computer science;knowledge management;knowledge-based systems;concurrency control;knowledge engineering;open knowledge base connectivity;data mining;database;domain knowledge;database design	DB	-31.35556229211034	12.934584954678057	87795
46f8533ebb6f79b107b1e388790d54c31922eec0	sparql-dl implementation experience		Recently, SPARQL-DL was introduced in [7] as a rich query language for OWL-DL ontologies. It provides an OWL-DL-like semantics for SPARQL basic graph patterns which involves as special cases both conjunctive ABox queries and mixed TBox/RBox/ABox queries over Description Logic (DL) ontologies. This paper describes the implementation of a SPARQL-DL query engine and discusses several optimizations. We investigate the new challenges brought by the additional expressivity of SPARQL-DL by extending a well-known benchmark.	abox;benchmark (computing);description logic;ontology (information science);query language;sparql;tbox	Petr Kremen;Evren Sirin	2008			abox;database;description logic;sparql;ontology (information science);expressivity;query language;semantics;computer science;graph	Web+IR	-24.234424740072694	8.813032517792559	87939
9a92b675217424c95f4daee26326f32c9ddf9946	dynamic object roles - adjusting the notion for flexible modeling	object oriented languages inheritance modelling;impedance;computer languages;conceptual modeling;design engineering;maintenance;multiple inheritance conceptual modeling dynamic object roles modeling language flexible modeling;information technology;conceptual model;data engineering;modeling language;development tool;multiple inheritance;computer science information technology relational databases computer languages impedance usability documentation maintenance data engineering design engineering;relational databases;computer science;inheritance;usability;object oriented languages;flexible modeling;documentation;dynamic object roles	The conceptual modeling of real-life situations often requires expressing some kind of multiple, multiaspect or dynamic specialization. On the other hand, multiple or dynamic inheritance impose troublesome anomalies and thus are sporadically and not fully supported by development tools. In This work we suggest a new approach to the concept of dynamic object roles, capable of expressing all the mentioned special kinds of specialization while avoiding the anomalies of multiple inheritance. The base mechanism of the notion is described and the possible design decisions concerning its realization in a modeling language and implementation, having impact on its overall usability, are discussed.	modeling language;multiple inheritance;partial template specialization;programming tool;real life;usability	Andrzej Jodlowski;Piotr Habela;Jacek Plodzien;Kazimierz Subieta	2004	Proceedings. International Database Engineering and Applications Symposium, 2004. IDEAS '04.	10.1109/IDEAS.2004.22	computer science;conceptual model;theoretical computer science;data mining;database;programming language;information technology	SE	-32.19225543224667	12.26174964748563	87951
acedd586b38df0f95ed26cc9382bff2fac93dde1	user defined aggregates in object-relational systems	graph search;online aggregation;aggregates data mining rain bayesian methods proposals database systems humidity chromium;data mining functions;query processing;sql3;bayesian methods;data mining;ease of use;expressive power;sadl;query processing data mining relational databases object oriented databases;object relational databases;aggregates;chromium;database systems;humidity;object relational database;rain;sadl user defined aggregates object relational database data mining functions partial results recursive queries sql3 expressive power online aggregation monotonic aggregation high level aggregate definition language;user defined aggregates;relational databases;object oriented databases;high level aggregate definition language;recursive queries;proposals;object relational;monotonic aggregation;partial results	User-defined aggregates are essential in many advanced database applications, particularly in expressing data mi ning functions, but they find little support in current system s including Object-Relational databases. Three serious lim itations of current systems are (i) the inability of introdu cing new aggregates (e.g., by coding them in procedural language as originally proposed in SQL3), (ii) the inability of returning partial results during the computation (e.g., to support online aggregation), and (iii) the inability of using aggregates in recursive queries (e.g., to express Bill o f Materials and optimized graph searches). In this paper, we presents a unified solution to these problems which realizes SQL3 original proposal for user-defined aggregates (UDAs), and adds significant improvements in terms of expressive power and ease of use: in fact our SQL-AG system also supports online aggregation, monotonic aggregation, and a high-level aggregate definition language named SADL. We focus on applications of UDAs and SADL.	aggregate data;computation;data mining;decision support system;expanded memory;hierarchical and recursive queries in sql;high- and low-level;high-level programming language;non-monotonic logic;object-relational database;online aggregation;procedural programming;recursion;sql server compact;sql:1999;semantic application design language;temporal database;usability	Haixun Wang;Carlo Zaniolo	2000		10.1109/ICDE.2000.839400	online aggregation;chromium;computer science;theoretical computer science;humidity;data mining;database;expressive power	DB	-29.80488626094373	10.409272561800233	87979
db033eff7678aedd03c9e2aa009c6c516fb18cea	goal-oriented query rewriting for owl 2 ql		We present an optimized query rewriting algorithm for OWL 2 QL that computes the rewriting set of a user query by avoiding unnecessary inferences and extended clause subsumption checks. The evaluation shows a significant performance improvement in comparison to other similar approaches. Alternatively, instead of a rewriting set, the algorithm can produce an equivalent non-recursive datalog program.	adobe flash lite;algorithm;artificial intelligence;automated reasoning;computation;datalog;description logic;iswc;lecture notes in computer science;ontology (information science);presto;recursion;resolution (logic);rewriting;scalability;semantic reasoner;subsumption architecture;time complexity;unfolding (dsp implementation);web ontology language	Alexandros Chortaras;Despoina Trivela;Giorgos B. Stamou	2011			query rewriting;programming language;web ontology language;datalog;goal orientation;computer science;rewriting	AI	-22.711307387091185	11.077746021813956	88047
8ea06ffc3a04e42d746409a32e84a94a829f1bfa	optimizing distributed spatial joins using r-trees	distributed database;spatial data;spatial join;uml;spatial database;xml;query;schema diagram;global schema;cost model	One of the basic problems in distributed databases is how to efficiently perform distributed joins. Spatial databases are particularly appropriate for distribution, but we need special techniques to deal with spatial data efficiently.In this work we study the distributed spatial join problem and how to perform this operation efficiently. We develop cost models for estimating the cost of this operation. We study the issues involved in optimizing it and develop specific techniques using R-Trees. Our techniques outperform other widely-used approaches for this operation.	algorithm;approximation;distributed database;join (sql);optimizing compiler;r-tree;semiconductor industry;spatial database	Orlando Karam;Frederick E. Petry	2005		10.1145/1167350.1167417	computer science;theoretical computer science;data mining;database;spatial database;spatial query	DB	-29.931455173920188	7.113482766637213	88192
905d2d0d549d02082326795ad4b8b325cc2ee659	specification of the database behavior through the active object paradigm	formal specification;information systems;database management systems;database behavior specification;active database;real world object classification;information system dynamics;automaton;event detection;active database design;process design;pressing;relational implementation;transaction databases;persistent active object;monitoring;database systems;active objects;event condition action paradigm;event condition action;production systems;active databases;active object paradigm;relational databases;object oriented databases;interaction model;information system;object oriented modeling;relational implementation active database design active object paradigm database behavior specification information system dynamics real world object classification persistent active object automaton event condition action paradigm;object oriented modeling relational databases transaction databases database systems production systems process design information systems monitoring event detection pressing	The purpose ofthis paper concerns the use oj’the active object paradigm to design active dntahases. The Active object is an interesting means to best represent the dynamic of an infiwmation syxtem. We propose a cla,wi$cation of real world objects fidlowing two categories: i) the persistent active object that describes an Object oflering sewices when reqiiested and reacts to events produced in its environment and ii) the activity that represents a functionality (or a process) ($ the information system. The main idea of this paper ,focuses on the .spec$cation of the behavior .f an active object through an ‘automaton ’ in agreement with the Event(bndition-Action paradigm. Interactions between objects are modeled by an interaction nzodel in,yired hy the OhiiT approach. To illustrate our model, we show how to map the specijication ( f t h e two active object categories towards a relational implementation.	active object;automaton;information system;interaction;programming paradigm	Madjid Meziane;Youssef Amghar	1996		10.1109/DEXA.1996.558287	method;object model;computer science;artificial intelligence;object-oriented design;data mining;database;distributed computing;data transfer object;information system;object definition language	DB	-29.230988202879608	13.88619273350901	88197
1523f657d6d6183152bae5b79cdd802c0a9ffe87	querying log data with metric temporal logic		We propose a novel framework for ontology-based access to temporal log data using a datalog extension datalogMTL of the Horn fragment of the metric temporal logic MTL. We show that datalogMTL is EXPSPACE-complete even with punctual intervals, in which case full MTL is known to be undecidable. We also prove that nonrecursive datalogMTL is PSPACE-complete for combined complexity and in AC for data complexity. We demonstrate by two real-world use cases that nonrecursive datalogMTL programs can express complex temporal concepts from typical user queries and thereby facilitate access to temporal log data. Our experiments with Siemens turbine data and MesoWest weather data show that datalogMTL ontology-mediated queries are efficient and scale on large datasets.	ac0;algorithm;apache parquet;apache spark;datalog;expspace;emoticon;expectation propagation;experiment;fink;floor and ceiling functions;ontology (information science);ordered pair;pspace-complete;postgresql;recursive definition;rewriting;rule-based system;sql;scalability;stream (computing);streaming media;temporal logic;time of arrival;undecidable problem;usability;web search engine	Sebastian Brandt;Elem Güzel Kalayci;Vladislav Ryzhikov;Guohui Xiao;Michael Zakharyaschev	2018	J. Artif. Intell. Res.	10.1613/jair.1.11229	undecidable problem;artificial intelligence;machine learning;mathematics;temporal logic;theoretical computer science;use case;datalog	DB	-32.633905384379986	5.993419916144845	88215
b2bcf25f249f45281eff86622c12e4a143734fd3	the generalized pre-grouping transformation: aggregate-query optimization in the presence of dependencies	surrogate-join transformation;optimization technique;aggregate-query optimization;generalized pre-grouping transformation;hierarchical pre-grouping;olap aggregate query;formal definition;similar transformation;pre-grouping transformation;access methods new processing;general algebraic definition;hierarchical surrogate key;query optimization;access method;database management;similarity transformation;data warehouse;information systems	One of the recently proposed techniques for the efficient evaluation of OLAP aggregate queries is the usage of clustering access methods. These methods store the fact table of a data warehouse clustered according to the dimension hierarchies using special attributes called hierarchical surrogate keys. In the presence of these access methods new processing and optimization techniques have been recently proposed. One important such optimization technique, called Hierarchical Pre-Grouping, uses the hierarchical surrogate keys in order to aggregate the fact table tuples as early as possible and to avoid redundant joins. In this paper, we study the Pre-Grouping transformation, attempting to generalize its applicability and identify its relationship to other similar transformations. Our results include a general algebraic definition of the PreGrouping transformation along with the formal definition of sufficient conditions for applying the transformation. Using a provided theorem we show that Pre-Grouping can be applied in the presence of functional and inclusion dependencies without the explicit usage of hierarchical surrogate keys. An additional result of our study is the definition of the Surrogate-Join transformation that can modify a join condition using a number of dependencies. To our knowledge, Surrogate-Join does not belong to any of the Semantic Query Transformation types discussed in the past.	aggregate data;aggregate function;cluster analysis;data integrity;join (sql);linear algebra;mathematical optimization;online analytical processing;query optimization;referential integrity;sql;semantic query;semiconductor industry;surrogate key;xslt/muenchian grouping	Aris Tsois;Timos K. Sellis	2003			matrix similarity;query optimization;computer science;data warehouse;data mining;database;access method;information retrieval;information system	DB	-28.105362989809258	5.078739913973942	88232
2876cce41b52d3bee58ea45136ac61cd57fbe159	qbd * : a graphical query language with recursion	databases;entity relationship model;query language;interfase usuario;conceptual data model;base donnee;graphical query language;query by diagram;user interface;information retrieval;relacion hombre maquina;visual interaction;query formulation;interrogation base donnee;database;interrogacion base datos;base dato;query languages graphical user interfaces information retrieval;conceptual model;man machine relation;pregunta documental;recursividad;indexing terms;lenguaje interrogacion;ease of use;question documentaire;query languages;expressive power;recursivite;qbd;graphical user interfaces;conceptual data model qbd graphical query language recursion databases standard user interface query by diagram;langage visuel;recursion;visual language;query;graphic user interface;design for quality database languages power system modeling visual databases data mining user interfaces data models graphical user interfaces spatial databases human computer interaction;interface utilisateur;langage interrogation;relation homme machine;lenguage visual;recursivity;standard user interface;database query	A system to query databases using diagrams as a standard user interface is proposed. The system, called Query by Diagram* (QBD*), makes use of a conceptual data model, a query language on this model, and a graphical user interface. The conceptual model is the entity-relationship model. The query language, whose expressive power allows recursive queries, supports visual interaction. The main characteristics of the interface are ease of use and the availability of a rich set of primitives for schema selection and query formulation. The expressive power of QBD* and G/sup +/, which are the only languages allowing recursive queries to be expressed graphically are compared. >		Michele Angelaccio;Tiziana Catarci;Giuseppe Santucci	1989	IEEE Trans. Software Eng.	10.1109/32.60295	online aggregation;sargable;query optimization;recursion;query expansion;web query classification;boolean conjunctive query;computer science;query by example;theoretical computer science;graphical user interface;database;rdf query language;programming language;query language;object query language;spatial query	SE	-32.026001148577286	8.748547536647024	88463
b1c2e74f5f70b2169066929bd80e86296700edf9	on integrating event definition and event detection	representacion conocimientos;logica temporal;event definition;nonmonotonic logic;active database;temporal logic;circonscription;event detection;event monitoring;circumscription;first order;monitoring;base donnee active;logique ordre 1;reasoning about time;representation connaissance;active databases;monitorage;knowledge representation;monitoreo;logique temporelle;first order logic;circonscripcion;logica orden 1	We develop, in this paper, a representation of time and events that supports a range of reasoning tasks such as monitoring and detection of event patterns which may facilitate the explanation of root cause(s) of faults. We shall compare two approaches to event definition: the active database approach in which events are defined in terms of the conditions for their detection at an instant, and the knowledge representation approach in which events are defined in terms of the conditions for their occurrence over an interval. We shall show the shortcomings of the former definition and employ a three-valued temporal first order nonmonotonic logic, extended with events, in order to integrate both definitions.	active database;knowledge representation and reasoning;non-monotonic logic	Nadim Obeid;Raj B. K. N. Rao	2009	Knowledge and Information Systems	10.1007/s10115-009-0193-3	knowledge representation and reasoning;computer science;artificial intelligence;first-order logic;data mining;mathematics;algorithm	AI	-25.020663006189842	13.887613839989134	88633
c9572192b40a679384e67291146dd21c10cd035d	integrating subtyping, matching and type quantification: a practical perspective	programming language;model complexity	We report on our experience gained in designing, implementing and using a strongly-typed persistent programming language (TooL)  which integrates object types, subtyping, type matching, and type quantification. Our work complements recent type-theoretical  studies of subtyping and type matching by focusing on the issue of how to integrate both concepts into a practical, orthogonal  programming language. We also shed some light on the subtle typing issues which we encountered during the construction of  a substantial bulk data library where it was necessary to trade-off subtyping against type matching. Our practical experience  suggests that the benefits of an integration of subtyping and type matching are achieved at the expense of a significant increase  in modeling complexity.  		Andreas Gawecki;Florian Matthes	1996		10.1007/BFb0053055	computer science;theoretical computer science;programming language;algorithm	PL	-21.84410633308301	13.11389126710885	89008
30611ab27d96028717f75e7e007346e22868f7fb	lispo2: a persistent object-oriented lisp	complex objects;object oriented data model;development environment;object oriented;office automation;knowledge base	"""Large and complex design applications such as CASE, office automation and knowledge based applications require sophisticated development environments supporting modeling, persistence and evoIution of complex objects. This paper presents a brief overview of a persistent object-oriented LISP named LISPOe combining both language and database facilities. LISPO2 """"s a Lisp-based environment providing the 02 object-oriented data model extended with the notions of constructor and exception. Persistence is achieved through the use of a persistent name space. These features centered around an interpreter facilitate quick prototyping of ~pplicatioas. 1 I n t r o d u c t i o n In order to support new application domains such as CASE, once automation or knowledge bases, a development environment has to provide both modeling power and database facilities. The former requires the representation and manipulation of complex objects (programs, documents, rules...), while the latter requires sharing and persistence between program invocations. Databases and programming languages have tried separately to cope with these types of applications. Traditional database systems support persistence, but fait to model objects either in their structural or behaviorM complexity. For instance, first normal form relational systems do not deM with structural complexity. Moreover, to express complex behavior, application programmers have to use a query language embedded in a programming language. They are thus faced with the well known """"impedance mismatch"""" problem [Cope84]: they have to learn two different languages and map continuously their different models. In contrast, programming languages, in particular object-oriented languages [Gold83] [Stro86] [Meye88], offer powerful features such as encapsulation, inheritance and exception handling that ease the design, implementation and evolution of modular and robust applications. However they support only a limited form of persistence through the use of files. Therefore the programmer has to flatten the highly interconnected in-core data structure onto the linear format of files. These error-prone conversions interfere with the application logic, decreasing programmer productivity. Recently, efforts to integrate database and programming languages have come either from database people producing object-oriented databases [Bane88], [Bane87], [Andr87], and [Maie86] or from programming language people producing persistent languages [Schm77], [Atki81], and [Alba851. These efforts focus on eliminating the major bottleneck to programmer productivity in such systems: the impedance mismatch. LISPO2 belongs in this trend. It is a Lisp-based lmlguage devoted to supporting incremental and"""	apl;business logic;characteristic impedance;cognitive dimensions of notations;computer-aided software engineering;constructor (object-oriented programming);core data;data model;data structure;data transfer object;database normalization;document;embedded system;encapsulation (networking);exception handling;first normal form;impedance matching;interpreter (computing);knowledge base;lisp;persistence (computer science);programmer;programming language;programming productivity;query language;structural complexity (applied mathematics);whole earth 'lectronic link	Gilles Barbedette	1990		10.1007/BFb0022181	knowledge base;method;object model;computer science;artificial intelligence;database;development environment;programming language;object-oriented programming	DB	-30.72659031929487	11.108904333792145	89058
91a5f71f91dce47054a67a5ab465b3f0d03df313	a framework for modular erdf ontologies	scoped negation as failure;local semantics;local closed world and open world assumptions;modular erdf ontologies;68t30;68t27	The success of the Semantic Web is impossible without any form of modularity, encapsulation, and access control. In an earlier paper, we extended RDF graphs with weak and strong negation, as well as derivation rules. The ERDF #n-stable model semantics of the extended RDF framework (ERDF) is defined, extending RDF(S) semantics. In this paper, we propose a framework for modular ERDF ontologies, called modular ERDF framework, which enables collaborative reasoning over a set of ERDF ontologies, while support for hidden knowledge is also provided. In particular, the modular ERDF stable model semantics of modular ERDF ontologies is defined, extending the ERDF #n-stable model semantics. Our proposed framework supports local semantics and different points of view, local closed-world and open-world assumptions, and scoped negation-as-failure. Several complexity results are provided.	access control;embedded rdf;encapsulation (networking);negation as failure;ontology (information science);open world;semantic web;stable model semantics	Anastasia Analyti;Grigoris Antoniou;Carlos Viegas Damásio;Ioannis Pachoulakis	2013	Annals of Mathematics and Artificial Intelligence	10.1007/s10472-013-9350-1	computer science;theoretical computer science;data mining;database	AI	-22.525073964451103	9.017069835611474	89460
1867b9064721a32efce257eeab843abc112c42c1	the ramification problem in temporal databases: concurrent execution	temporal database	In this paper, we study the ramification problem in the setting of temporal databases. Standard solutions from the literature on reasoning about action are inadequate because they rely on the assumption that fluents persist, and because actions have effects on the next situation only. In this paper, we provide a solution to the ramification problem based on an extension of the situation calculus and the work of McCain and Turner. More specifically, we study the case in which two or more actions execute concurrently, a particularly complex problem. © 2010 Wiley Periodicals, Inc.	ramification problem;temporal database	Nikos Papadakis;Dimitris Plexousakis;Grigoris Antoniou	2010	Int. J. Intell. Syst.	10.1002/int.20408	computer science;artificial intelligence;mathematics;temporal database;algorithm	DB	-19.461236345448196	8.112518204301088	89505
b54d2556702f3176451cc797213d5b35c7817456	ckr: live demo: using contexts and exceptions for representing evolving knowledge states		Context representation of (description logic based) knowledge has recently gained interest in the Semantic Web community and a number of logic based solutions have been proposed. In this regard, in our previous works we have introduced the DL based Contextualized Knowledge Repository (CKR) framework: we recently proposed an extension of CKR with the possibility to represent and reason with defeasible axioms and exceptions in context dependent axioms. The goal of this demo is to demonstrate the use of contexts and exceptions in representing evolving situations: the demo visualizes the evolution of a soccer match by showing the (possibly non-monotonic) changes in the knowledge across different events composing the match.	defeasible reasoning;description logic;exception handling;peano axioms;semantic web	Loris Bozzato;Luciano Serafini;Gaetano Calabrese	2017			data mining;computer science	AI	-19.905243716045973	8.537715959654335	89545
36ea7a57e55fe04cb12d1b199c219508dab3e4cd	a computational paradigm that integrates rule-based and model-based reasoning in expert systems	modelizacion;model based reasoning;representacion conocimientos;sistema experto;prolog;regle production;rule based;estrategia;intelligence artificielle;raisonnement;algorithme;strategy;resolucion problema;modelisation;algorithm;unification;razonamiento;artificial intelligence;inteligencia artificial;systeme expert;reasoning;knowledge representation;representation connaissances;modeling;strategie;unificacion;problem solving;resolution probleme;production rule;regla produccion;algoritmo;expert system	Abstract#R##N##R##N#This article presents a new computational paradigm that integrates rule-based and model-based reasoning in expert systems. Our experience in expert systems research and development indicates that the rule-based technique is simple, elegant, and efficient; whereas the model-based approach is complex but powerful, CPU-consuming but robust. Combining both the rule-based and the model-based methods into one paradigm means having the best of both worlds. to achieve this goal, we have extended the Prolog unification algorithm to accommodate semantic unification. the resulting computational procedure is named R.M. This new inference procedure uses rule-based reasoning by default, and it automatically invokes model-based reasoning when all the rules become inapplicable, but it returns to rule-based reasoning whenever the rules become usable again. the idea behind this problem-solving strategy is to achieve maximum efficiency as well as robustness in expert systems. Examples are used throughout the article to illustrate our notions. the article also sketches an application in the domain of telecommunication networks maintenance and describes our experimental results.	computation;expert system;logic programming;model-based reasoning;programming paradigm	Newton S. Lee	1990	Int. J. Intell. Syst.	10.1002/int.4550050202	legal expert system;conflict resolution strategy;systems modeling;strategy;computer science;artificial intelligence;unification;model-based reasoning;machine learning;reasoning system;prolog;expert system;reason;algorithm	AI	-20.52568625291642	11.180179121071898	89600
ead0bb9c2dc9eea39cb4be4ae9da9b726b7ded0a	a tutorial on query answering and reasoning over probabilistic knowledge bases		Large-scale probabilistic knowledge bases are becoming increasingly important in academia and industry alike. They are constantly extended with new data, powered by modern information extraction tools that associate probabilities with knowledge base facts. This tutorial is dedicated to give an understanding of various query answering and reasoning tasks that can be used to exploit the full potential of probabilistic knowledge bases. In the first part of the tutorial, we focus on (tuple-independent) probabilistic databases as the simplest probabilistic data model. In the second part of the tutorial, we move on to richer representations where the probabilistic database is extended with ontological knowledge. For each part, we review some known data complexity results as well as discuss some recent results.		Ismail Ilkan Ceylan;Thomas Lukasiewicz	2018		10.1007/978-3-030-00338-8_3	information extraction;data mining;ontology;knowledge base;data model;probabilistic logic;probabilistic database;exploit;computer science	AI	-21.664968033500003	7.5557946534494915	89631
ec1cac62c1771967ff85bd6c1b1a5c48b45e7448	fine-grained relevance feedback for xml retrieval	dempster shafer theory of evidence;xml retrieval;xml ir;dempster shafer theory;relevance feedback	This demonstration presents an XML IR system that allows users to give feedback of different granularities and types, using Dempster-Shafer theory of evidence to compute expanded and reweighted queries.	relevance feedback;xml retrieval	Hanglin Pan;Ralf Schenkel;Gerhard Weikum	2008		10.1145/1390334.1390561	dempster–shafer theory;computer science;data mining;database;information retrieval;statistics	Web+IR	-32.64474579068069	7.006125221962722	89922
8d5fdd4cfdb8a11bd7889af8f03f067f86b793f6	on the expressive power of xquery-based update languages	query language;base donnee;mise a jour;formal specification;xml language;interrogation base donnee;database;interrogacion base datos;base dato;recommandation;lenguaje interrogacion;data model;specification formelle;actualizacion;expressive power;especificacion formal;recomendacion;recommendation;modele donnee;langage interrogation;database query;langage xml;lenguaje xml;updating;data models	XQuery 1.0, the XML query language which is about to become a W3C Recommendation, lacks the ability to make persistent changes to instances of its data model. A number of proposals to extend XQuery with update facilities have been made lately, including a W3C Working Draft. In order to investigate some of the different constructs that are introduced in these proposals, we define an XQuerybased update language that combines them. By doing so, we show that it is possible to give a concise, complete and formal definition of such a language. We define subsets of this language to examine the relative expressive power of the different constructs, and we establish the relationships between these subsets in terms of queries and updates that can be expressed. Finally, we discuss the relationships between these subsets and existing XQuery-based update languages.	data model;expressive power (computer science);primitive recursive function;query language;recursion (computer science);row hammer;simulation;xml;xquery	Jan Hidders;Jan Paredaens;Roel Vercammen	2006		10.1007/11841920_7	data modeling;xml;data model;computer science;data mining;formal specification;database;programming language;expressive power;query language	DB	-29.96598184746086	10.414450882686001	89935
1ea471965c62072fdd52f1c4f3cc3df89ca6ca51	parameterized complexity for the database theorist	parameterized complexity;graph data model;complex relationship discover;semantic associations;semantic relationships;semantic web;semantic querying;rdf	Parameterized complexity theory provides a framework for a fine-grain complexity analysis of algorithmic problems that are intractable in general. In recent years, ideas from parameterized complexity theory have found their way into various areas of computer science, such as artificial intelligence [15], computational biology [1, 21], and, last but not least, database theory [16, 19]. This short paper is a gentle introduction to the theory, focusing on the results most relevant for database theory. Interested readers are referred to Downey and Fellow’s monograph [6] to learn more about parameterized complexity theory. The paper is organised as follows: In Section 2 we describe two simple fixed-parameter tractable algorithms in an informal way. Section 3 presents the formal framework of parameterized complexity theory. Section 4 is a brief survey of the parameterized complexity of database query evaluation.	algorithm;analysis of algorithms;artificial intelligence;cobham's thesis;computation;computational biology;computational complexity theory;computer science;database theory;parameterized complexity	Martin Grohe	2002	SIGMOD Record	10.1145/637411.637428	semantic data model;semantic interoperability;cwm;parameterized complexity;semantic similarity;semantic computing;semantic web rule language;semantic search;semantic grid;computer science;semantic web;rdf;social semantic web;linked data;data mining;semantic web stack;semantic compression;database;semantic equivalence;semantic technology;probabilistic latent semantic analysis;graph database;information retrieval;semantic analytics	Theory	-21.89195263841363	8.254995592554856	90107
325d4e851c039dcd70c8c8b482bef5c753abebf0	on separation between interface, implementation, and representation in object dbmss	application development;database systems application software object oriented databases programming profession laboratories computer interfaces computer languages design automation computer aided manufacturing cadcam;design principle;programming language;design and development;object oriented design;object oriented software;data format;code reuse;object oriented database management system;object oriented databases;object oriented database;object oriented technology;representation object dbmss interface implementation;database management system	In this paper we present a model that supports a clean separat ion between the concepts of interface, implementation, and representation. We present s everal problems that are difficult to solve in the absence of such separation and describe how the propos ed model can be used to provide a solution. We also describe the principles that can be used to implement the proposed model in an existing object-oriented database management system.	database	Yuri Leontiev;M. Tamer Özsu;Duane Szafron	1998		10.1109/TOOLS.1998.711010	method;computer science;data access object;object;theoretical computer science;object-relational mapping;object-oriented design;common object request broker architecture;database;programming language;portable object;database design;object definition language	DB	-31.983915576447767	12.036530618497451	90141
09caeb247f12a6f80a2aa1b070a6cd96ddbcc0f3	schema extraction for semi-structured data.	semistructured data;semi structured data;knowledge base	The emerging eld of semistructured data leads to new ways of rep resenting data as schemaless or self describing However in many applications data has often some regularity and ignoring the possibly partial structure hinders the abilities to interpret the data and to access them e ciently In this paper we investigate a knowledge based approach for discovering partial implicit structures from semistructured data We show that semistructured data represented in the form of labeled directed graphs can be typed using description logics	description logic;directed graph;semi-structured data;semiconductor industry	Mohand-Said Hacid;Lina Fatima Soualmia;Farouk Toumani	2000			data mining;database;information retrieval	DB	-27.87218737008816	9.477123298470323	90182
58957d1ed2555404d03f5249a1788c0830ea423a	bi-level clustering in telecommunication fraud		In this paper we describe a fraud detection clustering algorithm applied to the telecom industry. This is an ongoing work that is being developed in collaboration with a leading telecom operator. The choice of clustering algorithms is justified by the need of identifying clients’ abnormal behaviors through the analysis of huge amounts of data. We propose a novel bi-level clustering methodology, where the first level is concerned with the clustering of transactional data and the second level gathers data from the first phase, along with other information, to build high-level clusters.		Luis Pedro Mendes;Joana Dias;Pedro Godinho	2012			operator (computer programming);computer science;telecommunications;cluster analysis;transaction data	ML	-28.92585110842119	17.382839249279538	90216
1d38eedbcb7a8ad4d4326d24ed9e95390f59f46d	tuplerank and implicit relationship discovery in relational databases	workload;base relacional dato;search engine;buscador;web pages;red www;reseau web;referentiel;integrite;referencial;integridad;information access;relational database;hierarchical classification;integrity;internet;integrity constraints;charge travail;base donnee relationnelle;classification hierarchique;acces information;referential;world wide web;acceso informacion;moteur recherche;carga trabajo;clasificacion jerarquizada	Google's successful PageRank brings to the Web an order that well re ects the relative importance of Web pages. Inspired by PageRank, we propose a similar scheme called TupleRank for ranking tuples in a relational database. Database tuples naturally relate to each other through referential integrity constraints declared in the schema. However, such constraints cannot capture more general relationships such as similarity. Furthermore, relationships determined statically from the database schema do not re ect actual query patterns that arise at runtime. To address these de ciencies of static TupleRank, we introduce the notion of query-driven TupleRank. We develop techniques to compute query-driven TupleRank accurately and eÆciently with low space requirement. We further augment query-driven TupleRank so that it can better utilize the access frequency information collected from the workload. Preliminary experiment results demonstrate that TupleRank is both informative andintuitive, and they con rm the advantages of query-driven TupleRank over static TupleRank.	data integrity;database schema;information;naruto shippuden: clash of ninja revolution 3;pagerank;referential integrity;relational database;run time (program lifecycle phase);world wide web	Xiao Huang;Qiang Xue;Jun Yang	2003		10.1007/978-3-540-45160-0_44	the internet;relational database;computer science;artificial intelligence;operating system;machine learning;web page;data integrity;data mining;database;distributed computing;superkey;world wide web;database schema;computer security;algorithm;search engine	DB	-25.796186823746275	4.708995476714576	90265
4d68bd54551537be3ff4fa98e4d22ea4ad8d8309	chasefun: a data exchange engine for functional dependencies at scale		Despite their wide use and importance, target functional dependencies (fds) are still a bottleneck for the state-of-the-art Data Exchange (DE) engines. The consequences range from incomplete support to support at the expense of an important overhead in performance. We demonstrate here ChaseFUN, a DE engine that succeeds in effectively mitigating and taming this overhead, thus making target fds affordable even for very large-sized, complex scenarios. ChaseFUN is a custom chase-based system that essentially relies on exploiting chase step ordering and constraint interaction, so as to piecemeal process, parallelize and dramatically speed-up the chase. Interestingly, the structures and concepts at the core of our system moreover allow it to seamlessly uncover a range of usually opaque details of the chase. As a result, ChaseFUN’s two main strengths are: (i) its significant scalability and performance and (ii) its ability to provide detailed, granular insight on the DE process. Across our demonstration scenarios, we will emphasize our system’s practical performance and ability to scale to very large source instances and sets of constraints. Furthermore, we will aim at providing the user with a novel, behind-the-scenes view on the internals of the ongoing chase process, as well as on the intrinsic structure of a DE scenario. CCS Concepts •Information systems Ñ Data exchange;	chase (algorithm);functional dependency;overhead (computing);scalability	Angela Bonifati;Ioana Ileana;Michele Linardi	2017		10.5441/002/edbt.2017.63	database;data mining;computer science;data exchange;functional dependency	DB	-24.766386274589085	5.791282672557713	90340
b8047697bc0c5539dfc94d04594837c8b19083c5	w3c xml query language			query language;xml;xquery		2009		10.1007/978-0-387-39940-9_3983	xml validation;xml encryption;xml base;query optimization;query expansion;xml;data control language;streaming xml;document structure description;xml framework;xml database;xml schema;rdf query language;xml signature;web search query;xml schema editor;query language;efficient xml interchange	DB	-33.04774582473276	7.578563458360948	90354
c8569cb2d31381e53a33512a0bd185bd8c70ac49	semantics for the jason variant of agentspeak (plan failure and some internal actions)	practical programming;agent-based software development;bdi-based programming;internal action;jason variant;predefined internal action;operational semantics;current goal;plan failure;mental state;programming language;formal semantics;internal actions	Jason is a platform for agent-based software development that is characterised both by being based on a programming language with formal semantics as well as having many language and platforms features that are very useful for practical programming, but not fully formalised. In this paper, we make significant progress in the direction of formalising the aspects of the variant of AgentSpeak that is interpreted by Jason that were not included in previous work on giving formal semantics to AgentSpeak. In particular, we give semantics to the plan failure handling mechanism which is unique to Jason, and also for some of the predefined internal actions that can alter an agent's mental state. Such internal actions are essential for some aspects of BDI-based programming, such as checking or dropping current goals or intentions, and therefore need to be formally defined within the operational semantics of the language.	agentspeak;jason	Rafael H. Bordini;Jomi Fred Hübner	2010		10.3233/978-1-60750-606-5-635	computer science;artificial intelligence;operational semantics;algorithm	Theory	-24.139120803989844	17.16127624312044	90576
c0dfbd9459d8befd0274adf203eb478b4d6a153e	querying relational concept lattices	relational concept analysis;relational queries;formal concept analysis	Relational Concept Analysis (RCA) constructs conceptual abstractions from objects described by both own properties and interobject links, while dealing with several sorts of objects. RCA produces lattices for each category of objects and those lattices are connected via relational attributes that are abstractions of the initial links. Navigating such interrelated lattice family in order to find concepts of interest is not a trivial task due to the potentially large size of the lattices and the need to move the expert’s focus from one lattice to another. In this paper, we investigate the navigation of a concept lattice family based on a query expressed by an expert. The query is defined in the terms of RCA. Thus it is either included in the contexts (modifying the lattices when feasible), or directly classified in the concept lattices. Then a navigation schema can be followed to discover solutions. Different navigation possibilities are discussed.	aggregate data;bioinformatics;conceptual graph;cyclic redundancy check;daily active users;formal concept analysis;hall effect;image scaling;international conference on web services;john f. sowa;lattice graph;lecture notes in computer science;p (complexity);quantifier (logic);query language;relational database;requirement;sparql;service composability principle;springer (tank);user requirements document;web service;z notation	Zeina Azmeh;Marianne Huchard;Amedeo Napoli;Mohamed Rouane Hacene;Petko Valtchev	2011			relational calculus;computer science;formal concept analysis;artificial intelligence;machine learning;data mining;database;mathematics;lattice miner	DB	-23.93607481157254	5.711691352630701	91142
a0e7953b41e7e3ec5882f3739ec17528bc7dd85f	the virtues of locking by symbolic names	tratamiento datos;lock hardware;fijacion;base donnee repartie;verrouillage;distributed database;locking;base repartida dato;data processing;traitement donnee;algorithme;algorithm;algorritmo;data base management system;concurrency control;controle concurrence;cerradura;control concurrencia;systeme gestion base donnee;information system;sistema gestion base datos;systeme information;sistema informacion;serrure	L'algorithme propose determine, par une pre-analyse, quelles entites peuvent etre debloquees. Les transactions ne sont pas consideres seulement pour les blocages exclusifs et l'utilisation de blocages partages est admise. Une methode est proposee pour prevenir le probleme des redemarrages en cascade		Ouri Wolfson	1987	J. Algorithms	10.1016/0196-6774(87)90049-6	data processing;computer science;concurrency control;distributed database;information system;algorithm	PL	-27.247534692775723	6.0123959806209	91254
ce0cfd84c8af5616d233cbde6ec7c0fa57fa7077	extraction of object-oriented schemas from existing relational databases: a form-driven approach	object-oriented databases;reverse engineering;integration;relational databases;validation.;forms;information extraction;relational database;object oriented	In this paper, we present our Form-driven approach for reverse engineering of relationa databases. This methodology uses the information extracted from both form structure and instances as a database reverse engineering input using an interaction with a user. Through a combination of forms structures and data instances analysis, forms relational sub-schemas and their constraints are derived. These relational sub-schemas are mapped to object sub-schemas, which will be merging into global object-oriented schema that presents the whole underlying databases. The resulting global object-oriented schema must be validated as a rich and correct representation of the application domain.	application domain;data dependency;data mining;database schema;relational database;reverse engineering;verification and validation	Mimoun Malki;André Flory;Mustapha Kamal Rahmouni	2002	Informatica, Lith. Acad. Sci.		data definition language;database theory;relational model;entity–relationship model;relational database;computer science;database model;data mining;database;programming language;object-oriented programming;database schema;information extraction;information retrieval;object-relational impedance mismatch;database design;reverse engineering;spatiotemporal database	DB	-31.462710517098238	10.515069932074145	91295
6b30506d0ef8fa2d2e224c7942fded5737b4c3eb	high performance distributed parallel query processing			database	Yi Jiang;David Taniar;Clement H. C. Leung	2001	Comput. Syst. Sci. Eng.		query expansion;database;query optimization;computer science;sargable	DB	-31.469972020425224	7.323365623433166	91716
b8f4b9dc8e52fc95afd04e85146738734d06104d	a taxonomy of types of granularity	databases;grain size;mathematics;taxonomy switches humans laboratories grain size artificial intelligence machine learning databases data mining mathematics;conceptual model;indexing terms;data mining;machine learning;software development;taxonomy;artificial intelligence;humans;switches	Multiple different understandings and uses exist of what granularity is and how to implement it, where the former influences success of the latter with regards to storing granular data and using granularity for reasoning over the data or information. We propose a taxonomy of types of granularity and discuss for each leaf type how the entities or instances relate within its granular level. Such unambiguous distinctions can guide a conceptual modeler to better distinguish between the types of granularity and the software developer to improve on implementations of granularity.	centrality;decision tree;entity;fuzzy sets and systems;fuzzy logic;granular computing;john f. sowa;lecture notes in computer science;rough set;software developer;taxonomy (general);theory;yao graph	C. Maria Keet	2006	2006 IEEE International Conference on Granular Computing	10.1109/GRC.2006.1635767	index term;network switch;computer science;conceptual model;theoretical computer science;software development;machine learning;data mining;database;taxonomy;grain size;corporate taxonomy	SE	-29.55090569756632	12.919992409179777	91767
d9d28b2ffcc6f10d20eabbf070a7837800328ee8	efficient processing of xml twig queries with all predicates	databases;pediatrics;mptwig;query processing;information science;database management systems;logic predicates;logic;predicates xml twig query encoding scheme;xml database;tree pattern;trees mathematics;data mining;xml twig queries;artificial neural networks;twig pattern;pattern matching;xml database management systems formal logic pattern recognition query processing trees mathematics;xml encoding databases logic algorithm design and analysis information science pattern matching costs query processing;xml;formal logic;pattern recognition;twig query;mptwig xml twig queries twig pattern xml database xpattern tree pattern logic predicates;anodes;encoding;encoding scheme;algorithm design and analysis;xpattern;predicates	Finding all the occurrences of a twig pattern in an XML database is a core operation for efficient evaluation of XML queries. Some researches have proposed part solutions to process XML twig queries with AND,OR, or NOT predicates. However, very Little work has handled a twig query with three predicates comprehensively. In this paper, we propose a novel path-partitioned encoding scheme, and present a powerful XPattern extended from tree pattern with logic predicates. We also develop a holistic twig join algorithm, called MPTwig, which is designed for efficient matching an XML twig pattern with compound and nested predicates. We show that MPTwig based on path-partitioned encoding scheme guarantee the I/O and CPU optimality. Finally, experimental results on a representative data set indicate that the proposed algorithm performs significantly.	twig;xml	Xiaoshuang Xu;Yucai Feng;Feng Wang	2009		10.1109/ICIS.2009.74	xml validation;computer science;theoretical computer science;data mining;xml schema;database	DB	-29.780450973274437	4.6530377142180965	91889
6bc49a28cd2e209e9568d21e8ae8a6e695629eb1	a discussion of a report by ehud shapiro	algorithm analysis;learning;aprendizaje;apprentissage;analyse algorithme;analisis algoritmo	The paper is an annotated summary of Ehud Shapiro’s report, “The Induction of Theories from Facts.” In the view of this author, Shapiro’s report forms a very good foundation for work in the field of learning. It gives a clear definition of the term “learning” in a way which both is intuitively acceptable and renders learning algorithms amenable to precise analysis. It also establishes a paradigm for learning algorithms which is precise enough that it can serve as a benchmark for future development as well as for the analysis of presently available algorithms.	algorithm;benchmark (computing);machine learning;programming paradigm;rendering (computer graphics)	Ranan B. Banerji	1987	Computational Intelligence	10.1111/j.1467-8640.1987.tb00216.x	artificial intelligence	ML	-19.843095159218233	12.00220726390391	92042
10e443e3624933faad7c2c32ee45a4de4a8217e4	web service composition as planning, revisited: in between background theories and initial state uncertainty	web service composition;ai planning	Thanks to recent advances, AI Planning has become the underlying technique for several applications. Amongst these, a prominent one is automated Web Service Composition (WSC). One important issue in this context has been hardly addressed so far: WSC requires dealing with background ontologies. The support for those is severely limited in current planning tools. We introduce a planning formalism that faithfully represents WSC. We show that, unsurprisingly, planning in such a formalism is very hard. We then identify an interesting special case that covers many relevant WSC scenarios, and where the semantics are simpler and easier to deal with. This opens the way to the development of effective support tools for WSC. Furthermore, we show that if one additionally limits the amount and form of outputs that can be generated, then the set of possible states becomes static, and can be modelled in terms of a standard notion of initial state uncertainty. For this, effective tools exist; these can realize scalable WSC with powerful background ontologies. In an initial experiment, we show how scaling WSC instances are comfortably solved by a tool incorporating modern planning heuristics.	automated planning and scheduling;heuristic (computer science);image scaling;ontology (information science);scalability;semantics (computer science);web service;world sudoku championship	Jörg Hoffmann;Piergiorgio Bertoli;Marco Pistore	2007			simulation;computer science;artificial intelligence;machine learning;data mining	AI	-22.231459626659273	8.194886331521515	92149
9a7922921c40daca552c38e9ab7c324fb35a2f3b	stmodelviz: a 3d spatiotemporal gis using a constraint-based approach	data integrity;query;3d spatiotemporal system;article;constraints;object constraint language	Advances in data acquisition techniques and model simulations produce increasing volume of 3D spatiotemporal data. However, existing systems provide limited capabilities to manage such data. This paper reports an effort to design and implement a prototype system for 3D spatiotemporal data. Due to the complexity of such data, the ability to verify their integrity is central to the data management system. We adopted a constraint-based approach which addresses data integrity explicitly. In the article, we define constraint conditions, formulate constraints using a formal language we have extended and evaluate constraints using enhanced computational algorithms. We focus on a set of relational integrity constraints pertaining to the spatial, temporal and spatiotemporal properties of 3D spatiotemporal data. We extended the Object Constraint Language (OCL) to handle spatiotemporal (ST) objects. ST–OCL is used to describe and record constraints. Constraints expressed by ST–OCL statements are evaluated by the enhanced algorithms to identify different topological relations between 3D spatiotemporal objects. The prototype system demonstrates how a constraint-based approach can be used to develop DBMS capabilities in managing 3D spatiotemporal objects. Using the dynamic repartitioning of airspace sectors as an application example, we show that the capabilities of the prototype to manage 3D spatiotemporal objects can be customized for specific domain applications. 2014 Elsevier Ltd. All rights reserved.	algorithm;computational geometry;data acquisition;data integrity;formal language;geographic information system;interactivity;mason;object constraint language;prototype;simulation;spatiotemporal database;spatiotemporal pattern	Jing Li;David W. S. Wong	2014	Computers, Environment and Urban Systems	10.1016/j.compenvurbsys.2014.02.002	computer science;theoretical computer science;data integrity;data mining;database;object constraint language;spatiotemporal database	DB	-30.63981701877067	11.921392334986763	92236
bb89b6f26d4fc8bd03bfee368a682e2a3a5c9648	stratified constructive disjunction and negation in constraint programming		Constraint Programming (CP) is a powerful declarative programming paradigm combining inference and search in order to find solutions to various type of constraint systems. Dealing with highly disjunctive constraint systems is notoriously difficult in CP. Apart from trying to solve each disjunct independently from each other, there is little hope and effort to succeed in constructing intermediate results combining the knowledge originating from several disjuncts. In this paper, we propose If-Then-Else (ITE), a lightweight approach for implementing stratified constructive disjunction and negation on top of an existing CP solver, namely SICStus Prolog clpfd. Although constructive disjunction is known for more than three decades, it does not have straightforward implementations in most CP solvers. ITE is a freely available library proposing stratified and constructive reasoning for various operators, including disjunction and negation, implication and conditional. Our preliminary experimental results show that ITE is competitive with existing approaches that handle disjunctive constraint systems.		Arnaud Gotlieb;Dusica Marijan;Helge Spieker	2018	2018 IEEE 30th International Conference on Tools with Artificial Intelligence (ICTAI)	10.1109/ICTAI.2018.00026	machine learning;constructive;operator (computer programming);artificial intelligence;constraint programming;computer science;declarative programming;negation;inference;solver;prolog	DB	-20.323787093415977	14.537022546662289	92381
4d33a0335ce2c76952cc4ecd186e9402f1fc7335	integrating object and relational technologies	encapsulation;object oriented modeling context modeling data structures space technology paper technology database systems computer languages encapsulation terminology;computer languages;static typechecking;paper technology;abstract data types;relational databases abstract data types object oriented databases;class extents;object oriented relational database integration;abstract data type;data type;data encapsulation;indexing;object oriented;data structures;indexation;database systems;common abstract interfaces;indexing object oriented relational database integration data encapsulation precondition abstract data types inheritance ordering common abstract interfaces static typechecking atomic data data structures class extents;terminology;inheritance ordering;relational databases;space technology;object oriented databases;object oriented database;atomic data;context modeling;data structure;object oriented modeling;precondition	A precondition for integrating object-oriented and relational technologies is an integration of their respective models. Once this is achieved, One naturally asks whether technology developed within acter strings), and data structures (such as tuples and sets). Instances of primitive data types are always values. An ADT is implemented using a Primitive data type to store its state (e.g., a tuple is often appropriate this PuPose). the special contexts of the original models can be used in the context of the unijied model. Due to space restrictions, this position paper outlines a general approach to unifying the models, and leaves for the panel discussion a consideration of problems seen for integrating technologies. Unifying 00 and Relational Models Danforth and others have formalized a model that unifies 00 and relational approaches [Dan92]. The model was developed to support static typechecking for the rule-based RDL/l database system [Kier90]. In the context of this discussion, the essential features of the resulting model are: ADTs (abstract data types) provide the sole mechanism for encapsulating data and operations. An inheritance ordering on ADTs reflects common abstract interfaces (not necessarily common implementations) and supports static typechecking. There is a distinction between “value” and “object” ADT instances. A value is always owned by a single containing data structure, whereas objects have lifetimes and identities independent of any containing structure, and can be shared by multiple structures. Primitive data types provided by the model include atomic data (such as integers and charRelations in the Model A relation (as in the traditional relational model) is modeled as an instance of a “relation ADT,” whose state includes a set of tuples and other data (likely including ADT instances) used to implement consistency constraints, locking information, indexes, cursors, etc. The operations provided by relation ADTs support adding and deleting tuples, opening cursors, retrieving tuples, etc. Operations that create new relations (e.g., the traditional select, join, etc.) are provided as primitives by programming languages based on the model. For example, an Object Oriented SQL might be provided (also, see [Dan921 for an example based on the RDL/1 language [Kier90]). Programs written in or interactively interpreted by these languages use and create relations through the abstract interfaces provided by relation ADTs. A relation instance in the model may be a value or an object. Top-level relations that have identity and persist between program executions will of course be objects. Relations returned as the result of queries will normally be values. Unless they need to be referentially shared, relations included in the state of other ADTs will also normally be values (e.g., a project object might 0730-3157/92 $3.00	abstract data type;data structure;database;interactivity;language primitive;lock (computer science);logic programming;object lifetime;precondition;programming language;relational model;report definition language;sql;type system	Scott Danforth	1992		10.1109/CMPSAC.1992.217564	data structure;computer science;data mining;database;programming language;abstract data type	DB	-30.08528397510676	11.000754393312745	92651
ef01ff80b9ade95ab061125919d90ed4c3b923e7	defining and computing least common subsumers in rdf	selection of rdf triples;least common subsumer;rdf simple entailment;rooted graph;rdf	Several application scenarios in the Web of Data share the need to identify the commonalities between a pair of RDF resources. Motivated by such needs, we propose the definition and the computation of Least Common Subsumers (LCSs) in RDF. To this aim, we provide some original and fundamental reformulations, to deal with the peculiarities of RDF. First, we adapt a few definitions from Graph Theory to paths and connectedness in RDF-graphs. Second, we define rooted RDF-graphs (r-graphs), in order to focus on a particular resource inside an RDF-graph. Third, we change the definitions of LCSs originally set up for Description Logics to r-graphs. According to the above reformulations, we investigate the computational properties of LCS in RDF, and find a polynomial-time characterization using a form of graph composition. This result remarkably distinguishes LCSs from Entailment in RDF, which is an NP-complete graph matching problem. We then devise algorithms for computing an LCS. A prototypical implementation works as a proof-of-concept for the whole approach in three application scenarios, and shows usefulness and feasibility of our proposal. Most of our examples are taken directly from real datasets, and are fully replicable thanks to the fact that the choice about which triples are selected for the computation is made explicit and flexible.	algorithm;cluster analysis;computable function;computation;description logic;graph theory;heuristic (computer science);matching (graph theory);np-completeness;polynomial;prototype;resource description framework;time complexity;word-sense disambiguation;world wide web	Simona Colucci;Francesco M. Donini;Silvia Giannini;Eugenio Di Sciascio	2016	J. Web Sem.	10.1016/j.websem.2016.02.001	computer science;artificial intelligence;theoretical computer science;rdf;data mining;database;rdf query language;world wide web;rdf schema	Web+IR	-22.868504707735635	8.22939196533556	92703
02bc638cc56687f78c2f560bf0f1b7fa90f0dcc9	access control and management in multilevel database models	access control	Description and management of security information are needed in large or complex databases independently of the database itself is physically structured.	access control;database model	U. Bussolati;Giancarlo Martella	1981		10.1007/3-540-10885-8_44	access control;database;conceptual schema;database model;computer science	DB	-32.61101169519717	10.666829454227447	92704
0f0222f9d05dd0ee3e4dad1e3566abe2216ff8a0	form: a flexible data model for integrated case environments	management system;object oriented data model;integrated case;software systems;data model;semantic data model;computer aided software engineering;primitive relationship;object manipulation;meta modeling;version control;meta model;object model	Abstract   An  Integrated Computer-Aided Software Engineering Environment  (ICASE) is a software system that can support the development, maintenance, and use of enterprise-wide information resources. An ICASE requires a data model that can represent heterogeneous types of entities and relationships evolving in time and context. This paper describes a data model called FORM (Flexible Object-Relationship Model) for the object management system of ICASEs. FORM is an extended object-oriented data model adopting concepts in semantic data modeling and meta-modeling. Relationship types as well as entity types, both of which can be defined as objects in an object schema help integrate heterogeneous concepts; and meta-types that hold adaptable semantics of object manipulation help raise the automation level of ICASEs. An object-modeling approach for version control is illustrated to show the effectiveness of the framework.	data model;form	Duk Hyun Kim;Sung Joo Park	1997	Data Knowl. Eng.	10.1016/S0169-023X(96)00042-0	semantic data model;metamodeling;idef1x;data modeling;method;object model;semi-structured model;data model;computer science;revision control;object-oriented design;data mining;management system;database;computer-aided software engineering;logical data model;software system	DB	-33.130171993655274	12.27387318502232	92790
a009e51700b7df7d0e248d7285e001e8f7a90372	validating dynamic properties of rule-based systems	verification;sistema experto;system structure;validacion;rule based system;regle production;rule based;base connaissance;structure systeme;base conocimiento;validation;systeme expert;verificacion;estructura sistema;production rule;regla produccion;dynamic properties;knowledge base;expert system	rules , and generates the set of all paths in the rule base . Path Hunter found 512 abstract rules in the Blackbox Expert’s rule base , which formed 516 paths . A single path found by Path Hunter is shown in Figure 1 . The square nodes represent abstract rules , the ‘‘round’’ nodes represent predicates , and the directed arcs represent data dependencies between the rules . The logical completion that is asserted by this path is GMAP – B ∧ CERTAIN – BALLS ∧ GMAP – CERT – B . The semantics for this path are as follows : upon examining the evidence (beam entry and exit points) currently available , select the actions of placing a ball on the grid and marking the ball as certain ; and the outcome is , a ball is placed and this ball is marked as certain . 2 . 2 . DATA REQUIREMENTS The data requirements of a rule-based system can be determined using our formal model for capturing rule-base structure . We recall that a path is a set of rules that advance the state of the problem being solved from one goal state to another . The facts required by the rules in a path to fire are the precedence constraints for advancing the state of the problem being solved . There are two issues in using the path model to determine the data requirements of a rule-based system : determining the precedence constraints required for a single rule in a path to fire , and determining when all the rules in a path will be able to fire . The initial set of rules that must fire in a path are called start rules . Start rules will then assert facts enabling the other rules in the path . The start rules of a path P k , denoted by SR k , is the set of rules r i P F where the templates in the LHS of r i do not depend upon any other rule r j P F ; SR k 5 h r i u r i P F , ( ; r j P F , r i a / r i ) j . For start rules , W 5 [ . The start set of a path P k is the set of facts required by the start rules of that path to become enabled to fire . A start set of a path is identified by the set of start templates , denoted by ST k , which is the union of the set of templates present on the LHS of each of the start rules in the path ; ST k 5 < r i P SR k ( r i . The set of predicates A . D . PREECE , C . GROSSNER & T . RADHAKRISHNAN 152 used in the start templates for a path is given by SP k 5 h l j u ; k L i , L i l P ST k , 9 (( L i , L i )) 5 l j j . The completion set of a path is the set of facts required for all the rules in that path , except start rules , to become enabled to fire . A completion set of a path is identified by the set of completion templates , and is denoted by CT k . The completion templates is the set of all templates present on the LHS of all the rules of the path , except start rules , which specify a fact that is not asserted by any rule in the path . Formally , CT k 5 ! r i P ( F 2 SR k ) ( ( r i 2 PT i ,k ) . PT i ,k is the set of templates from ( r i that are matched by the facts asserted when the rules that enable r i fire . PT i ,k 5 hk L i , L i l u ( ; r j P W , W › r i )( ' d , k L i , L i l ? d 5 k l i , l i l , k l i , l i l P ! r j ) j . The set of predicates used in the completion templates for a path is given by CP k 5 h l j u ; k L i , L i l P CT k , 9 ( k L i , L i l ) 5 l j j . Now , let us consider the precedence constraints for a rule in a path . Given a path P k , r i P F , r i  ̧ SR k , and W › r i , then the set of completion templates for r i is given by CT i ,k 5 ( ( r i 2 PT i ,k ) . Lemma 1 . If facts matching the completion templates CT i ,k of rule r i in a path P k are present in WM , r i  ̧ SR k , and the ( r j P W , W › r i ) fire , then r i can fire . Proof . After the r j P W fire , WM contains the union of all facts asserted by the r j P W , in addition to the facts matching CT i ,k . These facts match the templates CT i ,k < PT i ,k by definition . This simplifies to ( r i (by putting CT i ,k 5 ( ( r i 2 PT i ,k )) ; thus , the facts in WM match all the LHS templates of r i , and hence r i can fire . Now that we have understood the conditions required for an individual rule in a path to fire , we can determine the conditions for all the rules in a path to become to fire-able . Theorem 1 . Gi y en a path P k , if facts matching the completion templates CT k and start templates ST k are present in WM , then all the rules in the path can fire . Proof . The proof of the theorem follows directly from Lemma 1 by induction . According to Theorem 1 , ST k and CT k identify the data items will be required by the rule-based system to achieve its goals . In Theorem 1 , we have shown that the precedence constraints for each path , ST k and CT k , indicate the facts required for all the rules in that path to fire , achieving a goal ; the goal achieved is identified by the logical completion asserted by the rules in the path . Our Path Hunter tool provides the data requirements for each path ( ST k and CT k ) . For example , the data requirements for our earlier example path are shown in Figure 2 . The set of start rules for the example path shown in Figure 1 is h RA-14-Left%1 j . The data items required by this path to achieve the goal represented by the logical completion it asserts are given by its start predicates h GMAP , SHOT-RECORD , GRIDSIZE j , and its completion predicates h GMAP – CERT , CERTAIN – BALLS j . The start predicates for this path indicate that this path will access facts the indicate the contents of the grid squares , the beams that have been fired . The completion predicates indicate that this path requires access to facts that indicate the certainty of the hypothesis for the contents of the grid squares that have been identified . DYNAMIC VALIDATION OF RULE-BASED SYSTEMS 153	data dependency;dynamic logic (digital electronics);dynamic problem (algorithms);elegant degradation;entry point;formal language;item unique identification;logic programming;mathematical induction;panorama tools;path (graph theory);problem solving;requirement;rule-based system;star catalogue;test case;testbed;tracing (software)	Alun D. Preece;Clifford Grossner;Thiruvengadam Radhakrishnan	1996	Int. J. Hum.-Comput. Stud.	10.1006/ijhc.1996.0008	rule-based system;verification;simulation;computer science;artificial intelligence;validation rule	AI	-23.43893342923857	15.656219807359074	92837
7c8cee0e94ffb227297afbee5b65b1c4dc2b9733	a straightforward formalization of the relational model	relational data model;query language;relational data;formal specification;query optimization;abstract data type;set theory;denotational semantic;relational model	There has been a lot of recent interest in the formalization of the relational data model (RDM). Many approaches may be characterized as ones oriented mainly towards declaring the components of the RDM and their interrelationships. Other approaches provide a tool for manipulating the components of RDM so that research topics on the model can be specified exactly. The latter approaches are based on formal specification methods such as denotational semantics or abstract data types. Some in the data base community experience them quite complex and cumbersome.The goal of the approach of this paper is of the latter kind. However, special attention is being paid to avoid the complexity of the formal specification methods because our notations and definitions are based on set theory. We attempt to provide an exact, convenient and general tool for specifications and proofs concerning various topics like relational query languages, query optimization, relational data base restructuring, data base design, etc.	abstract data type;data model;denotational semantics;mathematical optimization;query language;query optimization;relational model;set theory	Timo Niemi;Kalervo Järvelin	1983	SIGMOD Record	10.1145/984540.984542	domain relational calculus;sargable;query optimization;sql;relational model/tasmania;relational model;codd's theorem;relational calculus;entity–relationship model;data model;relational database;computer science;theoretical computer science;data mining;database;conjunctive query;programming language	DB	-26.225464162475166	10.324126582706603	92843
5484465fd353f1fc371ce91727ecc0c945b6cdb6	kassys: a definition acquisition system in natural language	definition acquisition system;conceptual graph;under-lying formalism;taxinomic hierarchy;natural language;systematic processing;defining statement;initial version;hyperonymous definition	This paper is an introdnction to KASSYS, a system that has been designed to extract information from detining statements in natural language. Only hy-peronymous detinitions are dealt with here, for which systematic processing has been devised and itnl)lenmnted in tlte initial version of the system. The paper describes how KASSYS buiMs a taxinomie hierarchy by extracting the hyperonyms from these definitions. It also explains the way in which the system can answer closed questions (yes/no), thus enabling the user to check very quickly that a definition has been assimilated correctly. The underlying forrnalism is that of conceptual graphs, with which the reader is assumed to be familiar. The mm of KASSYS, the system described here, is to acquire lexicographical definitions expressed in French, to extract from these definitions a carefully chosen conceptual structure, to save this structure in a tile, and to then be able to use it, where appropriate, for the semantic analysis of a text or during the search for an artswer to a question put by the user. + The formalistn which has been adopted for the representation of the detinitions is that o1' conceptual graphs, 2 that the reader is assumed to understand. All the examples will be given in the feral of statements in natural language, and the operations actually pcrfof med by the system on the conceptual stnmtut'es extracted from these statements will not be described. This paper is limited to hyperonymous delinitions. It ;list) shows, very brietly, how KASSYS can answer cet+tain types of question. 2. KNOWLIgl)GE EXTRACTION 2.1-Some points concerning hyperonynmus detinl-lions Ilyperonymy/hyponymy can be delined its follows: term A is said to be a hyperonym of term B, or alternatively that term B is a hyponym of term A, if the set of instances of term B is included in the set of instances of term A. This gives the following corollary: the set of semantic features that make ttp the elements of A is included in the set of sem,'mtic featnres that make up the elements of B; the elelnents of B are saM to inherit the sem,'mtic features that are common to the elements of A. llere wc b For a complete descriplion of the initM version of KASS YS, please refer to (llernert 93). 2Cf. (Sowa 84). have tile notion of inheritance of semantic features that is fundanmntal to tile theory of semantic netwm'ks. Ilyperonymy is thus defined …	conceptual graph;john f. sowa;lexicography;natural language	Patrice Hernert	1994			natural language processing;conceptual graph;computer science;data mining;linguistics;natural language	Web+IR	-22.812986248276072	10.358884652599462	92860
4243a93936617cd2ed939cac2c504736821994c2	framework for query optimization in distributed statistical databases	databases;database management systems;statistical databases;query optimization;distributed statistical databases	Recently, there has been a growing interest in statistical database (SDB) research. When SDBs are dispersed among computing facilities at various sites (e.g., in health-care networks) an additional dimension is added to the already difficult problems faced by the SDB designer. A distributed statistical database management system ( DS-DBMS) consists o f micro data (i.e., raw data) and macro data (i.e., aggregated objects called summary tables), which can be considered essentially as aggregated views o f the raw data in a special format. The first part o f the paper gives an overview o f a model for the representation o f both raw data (micro data) and summary tables (macro data). The model is an extension of the relational model (so that existing distributed database systems can be exploited). Most o f the first part is devoted to defining operations on macro data sets. Based on these operations, a set o f equivalent relational operations is described, as one of the main objectives in defining the micro and macro data sets, and the operations on them, has been to use as much as possible the capabilities that are already offered by most relational DBMSs. The second part o f the paper deals with one o f the important aspects o f performance in a DS-DBMS, namely, the efficient processing o f queries. This is heavily influenced by the performance o f query optimizers. However, to provide query optimization in a DS-DBMS, special issues are raised that manifest themselves in different scenarios. Some of the important issues and problems raised are discussed and solutions proposed. In addition, a set of transformations on macro operations (similar to those in relational algebra) are introduced, which can be used for optimizing queries in DS-DBMSs.	distributed database;mathematical optimization;query optimization;relational algebra;relational model;sdb (debugger);statistical database	Mohammad Hadi Sadreddini;David A. Bell;Sally I. McClean	1992	Information & Software Technology	10.1016/0950-5849(92)90011-D	sargable;query optimization;relational database;computer science;query by example;theoretical computer science;database model;data mining;database;view;database design	DB	-29.69658949619537	6.411875952122999	92906
4b3f12fef87a6fc7ed8b8be9efec3f5b9565d7f2	consistent query answering under key and exclusion dependencies: algorithms and experiments	conjunctive queries;satisfiability;first order;general methods;computational complexity;relative efficiency;integrity constraints;query answering;inconsistency;query rewriting	"""Research in consistent query answering studies the definition and computation of """"meaningful"""" answers to queries posed to inconsistent databases, i.e., databases whose data do not satisfy the integrity constraints (ICs) declared on their schema. Computing consistent answers to conjunctive queries is generally coNP-hard in data complexity, even in the presence of very restricted forms of ICs (single, unary keys). Recent studies on consistent query answering for database schemas containing only key dependencies have analyzed the possibility of identifying classes of queries whose consistent answers can be obtained by a first-order rewriting of the query, which in turn can be easily formulated in SQL and directly evaluated through any relational DBMS. In this paper we study consistent query answering in the presence of key dependencies and exclusion dependencies. We first prove that even in the presence of only exclusion dependencies the problem is coNP-hard in data complexity, and define a general method for consistent answering of conjunctive queries under key and exclusion dependencies, based on the rewriting of the query in Datalog with negation. Then, we identify a subclass of conjunctive queries that can be first-order rewritten in the presence of key and exclusion dependencies, and define an algorithm for computing the first-order rewriting of a query belonging to such a class of queries. Finally, we compare the relative efficiency of the two methods for processing queries in the subclass above mentioned. Experimental results, conducted on a real and large database of the computer science engineering degrees of the University of Rome """"La Sapienza"""", clearly show the computational advantage of the first-order based technique."""	algorithm;co-np;computation;computer science;conjunctive query;data integrity;database schema;datalog;experiment;first-order predicate;key (cryptography);linear algebra;object-relational database;relational database management system;rewriting;sql;unary operation	Luca Grieco;Domenico Lembo;Riccardo Rosati;Marco Ruzzi	2005		10.1145/1099554.1099742	sargable;query optimization;boolean conjunctive query;computer science;theoretical computer science;efficiency;first-order logic;data integrity;data mining;database;conjunctive query;computational complexity theory;query language;satisfiability;spatial query	DB	-25.507678832837335	10.716381462805803	92961
cc7269fc10247fb253aba48bcd8365102803f4ed	a compressed data model for a bitmapped xml structure		Developments on XML processing usually produce tools to formulate both the XML data storage and the associated query processor. PACD is one of such developments that stores the XML structure into a set of n×n bitmap matrices each of which encodes a specific XML structure related to an XPath axis. The amount of space and the complexity of storing uncompressed version of these matrices is large for huge XML databases; and such requirements may go beyond the HW/SW capabilities; this justify the need for the data compression model discussed in this paper.	apache axis;bitmap;cluster analysis;computer data storage;dicom;data compression;data model;requirement;shattered world;sparse matrix;xml database;xml tree;xpath	Mohammed Al-Badawi	2012			streaming xml;xml signature;computer science;world wide web;xml encryption;xml framework;data mining;database;document structure description;simple api for xml;xml database;efficient xml interchange	DB	-32.851347884546975	5.1878348032195	92975
9a299935a861c5166b8cce7523bc3c7fddd7a222	extended query facilities for racer and an application to software-engineering problems	object oriented design;software engineering;query language;abstract syntax	This paper reports on a pragmatic query language for Racer. The abstract syntax and semantics of this query language is defined. Next, the practical relevance of this query language is shown, applying the query answering algorithms to the problem of consistency maintenance between object-oriented design models.	abstract syntax;algorithm;query language;relevance;software engineering	Volker Haarslev;Ralf Möller;Ragnhild Van Der Straeten;Michael Wessel	2004			query expansion;web search query;web query classification;database;query language;rdf query language;sargable;query optimization;query by example;computer science	PL	-31.433786312297887	9.096101529746068	93003
1f82e8bb67b123ad352c2260d5e34a6158c3637b	spatial relational algebra: an extended relational algebra for spatial databases	relation algebra;spatial relation;spatial database		relational algebra;relational database management system	Alberto Belussi;Marco Negri;Giuseppe Pelagatti	1996			relational model;relational algebra;discrete mathematics;spatial database;codd's theorem;spatial relation;relational calculus;relation algebra;mathematics	DB	-29.795082458507338	8.822061862027413	93329
a589f0f59a5fd2687aeb55c56c3d5f557168074d	meta object approach to database schema integration	standards;multidatabase system;database schema integration;client server systems;schema integration;meta object facility;software engineering;repository;standards object oriented databases meta data distributed object management client server systems open systems application program interfaces;bonding;object oriented middleware environment;distributed objects;open interface standard;meta meta information;object oriented;application program interfaces;warehousing;database systems;distributed object management;data warehousing;software engineering computer science australia bonding object oriented databases database systems distributed databases warehousing middleware environmental management;distributed databases;meta data management;middleware;meta data;object oriented databases;unresolved issues;computer science;multidatabase systems;open systems;environmental management;distributed object management meta object approach database schema integration multidatabase systems data warehousing unresolved issues meta data meta meta information schema integration meta object facility repository meta data management object oriented middleware environment open interface standard;australia;meta object approach	Database schema integration is significant not only in building multidatabase systems but also in data warehousing. Metadata, which define schemas, are normally involved in the surrounding issues. And while many of these issues have been addressed in the past, unresolved issues remain. In this paper, we present an approach that not only uses metadata but also uses meta-meta information to make schema integration more possible. Our solution requires meta object facility that serves not only as a repository but also as a more feasible means of managing meta data. We also advocate the use of such a facility as part of an object-oriented middleware environment that provides an open interface standard and several useful services in distributed object management.	database schema;distributed object;meta-object facility;middleware;open interface	Jiahai Tan;Arkady B. Zaslavsky;Andy Bond	2000		10.1109/DOA.2000.874186	computer science;data mining;database;world wide web;database schema	DB	-33.51894761184536	12.626352165273405	93402
b49476184e353f0fa43f9bce4bc82228f1d3625e	maintaining temporal warehouse models		DWT is a tool for the maintenance of data warehouse structures based on the temporal data warehouse model COMET. Data warehouse systems do not provide support for maintaining changes in dimension data. DWT allows keeping track of modifications made in the dimension-structure of multidimensional cubes stored in an OLAP (On-Line Analytical Processing) system. We present the overall structure of the DWT system, which allows to upload and download warehouse models in different modeling notations in a time conscious manner, load edit scripts describing changes between versions of warehouse models and apply these edit scripts. We present the workflows for maintenance of warehouse models and discuss how maintenance can be supported with the various integrated tools of DWT .	comet (programming);discrete wavelet transform;download;olap cube;online analytical processing	Johann Eder;Christian Koncilia;Karl Wiggisser	2006		10.1007/0-387-34456-X_3	dimensional modeling;computer science;data mining;database;world wide web	DB	-31.627545387273383	5.647594018214107	93408
837fd62bb6e6a56a2e7743dac5b0224dfc737cbd	xpath rewriting using multiple views: achieving completeness and efficiency	multiple views;polynomial time	The standard approach for optimization of XPath queries by rewriting using views techniques consists in navigating inside a view’s output, thus allowing the usage of only one view in the rewritten query. Algorithms for richer classes of XPath rewritings, using intersection or joins on node identifiers, have been proposed, but they either lack completeness guarantees, or require additional information about the data. We identify the tightest restrictions under which an XPath can be rewritten in polynomial time using an intersection of views and propose an algorithm that works for any documents or type of identifiers. As an additional contribution, we analyze the complexity of the related problem of deciding if an XPath with intersection can be equivalently rewritten as one without intersection or union.	algorithm;identifier;mathematical optimization;p (complexity);rewriting;time complexity;xpath	Bogdan Cautis;Alin Deutsch;Nicola Onose	2008			time complexity;computer science;data mining;database	DB	-24.496946463966783	10.559958793885077	93548
f50a4205208b3429fd50f0e99867b005e7faf4c5	designing a compression engine for multidimensional raster data	data cube;base donnee;data compression;base donnee temporelle;database;base dato;time series;spatial database;spatio temporal data;object oriented;base donnee spatiale;oriente objet;compresion dato;orientado objeto;temporal data base;compression donnee	Multidimensional raster data appears in many application areas, typically in the form of sampledspatial or spatio-temporal analogue data. Due to the data volume and correlations between neighbouring samples usually encounteredin this kind of data it has high potential for efficient compression, as can be seen by the wealth of specializedcompression techniques developedfor 2D raster images; other examples wouldb e 1D time series, 3D volumetric or spatiotemporal data, 4D spatio-temporal data or 5+D data typically found in OLAP data cubes. Efficiently handling this kind of data often requires compression, be it to reduce storage space requirements or transfer times over low-bandwidth media. In this paper we present the design of the generic, tile-basedcompression engine developed for this purpose and implemented in the multidimensional array DBMS RasDaMan.	raster data	Andreas Dehmel	2001		10.1007/3-540-44759-8_47	data compression;computer science;theoretical computer science;time series;data mining;database;object-oriented programming;spatial database;data cube;statistics	DB	-29.584135727493358	7.9255265764053675	93731
1dc38d6cf0cf7ab46b599440c861bb65054ce3c1	language features for flexible handling of exceptions in information systems	language use;semantic integration;software systems;conceptual model;logic of constraints;exception handling;information system	An exception-handling facility suitable for languages used to implement database-intensive information systems is presented. Such a mechanism facilitates the development and maintenance of more flexible software systems by supporting the abstraction of details concerning special or abnormal occurrences. The type constraints imposed by the schema as well as various semantic integrity assertions are considered to be normalcy conditions, and the key contribution of this work is to allow exceptions to these constraints to persist. To achieve this, solutions are proposed to a range of problems, including sharing and computing with exceptional information, exception handling by users, the logic of constraints with exceptions, and implementation issues. The use of exception handling in dealing with null values, estimates, and measurement is also illustrated.	exception handling;information system;programming language;semantic web;software system	Alexander Borgida	1985	ACM Trans. Database Syst.	10.1145/4879.4995	exception handling;semantic integration;computer science;conceptual model;data mining;database;programming language;information system;software system	DB	-32.2445021286939	12.080566662683443	93838
a69813760e58f01fec63f61124f18b8d464f0559	exploiting hidden semantic clues for heuristic guidance of non-clausal theorem proving.	theorem proving			I. S. Torsun;L. M. Newnham	1999			computer science;artificial intelligence;automated theorem proving;programming language;algorithm	AI	-21.94781828484547	17.05149500212415	93851
3eb91ce9ced1f07342a9acfc4a5d6a989841af80	entity integrity revisited	h 2 3;null;key;h 2 1;relational;cr categories h 2 o;entity		entity integrity	Terry A. Halpin;Peter R. Ritson	1996	Australian Computer Journal		computer science;entity;key;algorithm	DB	-27.164118301659393	13.789890274070068	93925
cb19cf9558c1412e8e84cff829e966240f28847e	the integrity constraints for similarity-based fuzzy relational databases	integrity constraints		data integrity;relational database	Adnan Yazici;Mustafa Ilker Sözat	1998	Int. J. Intell. Syst.	10.1002/(SICI)1098-111X(199807)13:7%3C641::AID-INT4%3E3.0.CO;2-K		DB	-30.859286291896865	9.233067185790466	94110
458690545e7214a8c75ab9c5be04110d70285f76	algebraic query optimization in the cooms structurally object-oriented database system	query optimization		query optimization	Birgit Demuth;Andreas Geppert;Thorsten Gorchs	1991			database tuning;object query language;rdf query language;query language;database;database design;query optimization;sargable;view;computer science	DB	-30.949573439801064	9.246402240174369	94197
407925acd2cf355387a164d3e59ce72c6ded2b18	a hybrid approach for program understanding based on graph parsing and expectation-driven analysis	program understanding;hybrid approach	Program understanding is an important part ofthe domain expertise required for programming language tutoring systems. However, the understanding ofstudent programs by a computer is extremely difficult because ofthe tremendous scope of variability in student solutions for nontrivial tasks. This article aims to handle such variability and improve understanding performance by a hybrid approach based on two complementary methods of graph parsing and expectation-driven analysis. The graph parsing method by Wills is utilized to recognize the programming plans in the code. At the same time, a new expectation-driven analysis is devised to generate expectations about the program design using such knowledge as the programming goals, plans, and information about the problem task. The analysis guides the plan recognition process through confirming, amending, or rejecting the expectations by checking them against the given code. Expectation-driven analysis can recognize the function and implementation level variations...	bottom-up parsing;bottom-up proteomics;code;debugging;em (typography);graph (abstract data type);nl (complexity);program comprehension;programmer;semantics (computer science);software bug;test data;top-down and bottom-up design;word lists by frequency	Seon-Man Kim;Jin H. Kim	1998	Applied Artificial Intelligence	10.1080/088395198117659	computer science;artificial intelligence;machine learning;algorithm	AI	-25.730643363054213	16.573094869762038	94327
d10bbf88802a982359b695c95879437691125445	samstar: an automatic tool for generating star schemas from an entity-relationship diagram	automatic generation;data model;information embedding;data warehouse;transaction processing;entity relationship;general star	While online transaction processing (OLTP) databases are modeled with Entity-Relationship Diagrams (ERDs), data warehouses constructed from these OLTP DBs are usually represented as star schema. Designing data warehouse schemas, however, is very time consuming. We present a prototype system, SAMSTAR, which automatically generates star schemas from an ERD. The system takes an ERD drawn by ERwin Data Modeler as an input and generates star schemas. SAMSTAR uses the Connection Topology Value [1] which is the syntactic structural information embedded in an ERD. SAMSTAR displays the resulting star schemas on a computer screen graphically. With this automatic generation of star schema, this system helps designers reduce their efforts and time in building data warehouse schemas.	diagram;entity–relationship model	Il-Yeol Song;Ritu Khare;Yuan An;Suan Lee;Sang-Pil Kim;Jinho Kim;Yang-Sae Moon	2008		10.1007/978-3-540-87877-3_42	transaction processing;entity–relationship model;data model;computer science;theoretical computer science;data warehouse;data mining;database;programming language	Logic	-32.80981020295862	10.1942178235807	94529
58fa22d9e137c19b3c07e374d829335972789eeb	temporal query processing using spatially-partitioned method	spatial partitioning	In this paper, we propose a general spatiallypartitioned temporal operation model, called 3D-TOM, and partition-based join algorithms for various types of temporal join in the proposed model. The temporal operators addressed in this paper include overlap, precede, follow, and equal. Some factors which a ect the performance of the proposed method are also discussed.	algorithm;join (sql);tom	Duk-Ho Chang;Jong Soo Kim;Myoung-Ho Kim	1996			query expansion;database;query optimization;computer science;spatial query;space partitioning	DB	-28.95081776492217	5.9324949659525545	94564
6c323541d0c006a44da69656c6efaa6af59c9f23	coloured petri nets	petri nets;abstract data types;digital simulation;graph colouring;cpn;design/cpn;standard ml;coloured petri nets;data types;functional programming language;invariants;modularity;state graphs	data base When the user creates a CPN diagram, the editor stores all the semantic information in an abstract data base, from which it can easily be retrieved by the CPN simulator and other analysis programs. The abstract data base was designed as a relational data base, but for efficiency it is implemented by means of a set of list structures making the most commonly used data base operations as efficient as possible. The existence of the abstract data base makes it much easier to integrate new and existing editors and analysis programs with the CPN tools. For this purpose CPN ML provides three sets of functions. The first set reads the information of the abstract data base, e.g., the colour set of a place. The second set creates pages and auxiliary objects (which have a graphical representation, but no representation in the abstract data base). Finally, the third set converts auxiliary objects to CPN objects (which means that they become included in the 176 6 Computer Tools for Coloured Petri Nets abstract data base). The three sets of functions make it easy to write programs which translate CPN diagrams into textual or graphical representations of other Petri net tools, and vice versa.data base). The three sets of functions make it easy to write programs which translate CPN diagrams into textual or graphical representations of other Petri net tools, and vice versa. 6.2 Simulation of CP-nets Simulation of CP-nets can be supported by a computer tool or it can be totally manual, for example, performed on a blackboard or in the head of a modeller. Simulation is similar to the debugging of a program, in the sense that it can reveal errors, but in practice it can never be sufficient to prove the correctness of a system. Some people argue that this makes simulation uninteresting and that the user should instead concentrate on the more formal analysis methods. We do not agree with this conclusion. On the contrary, we consider simulation to be just as important and necessary as the formal analysis methods. In our opinion, all users of CP-nets (and other kinds of Petri nets) are forced to make simulations because it is impossible to construct a CP-net without thinking about the possible effects of the individual transitions. Thus the proper question is not whether the modeller should make simulations or not, but whether he wants computer support for the simulation activity. With this rephrasing the answer becomes trivial. Of course, we want computer support. This means that the simulations can be done much faster and with no errors. Moreover, it means that the modeller can use all his mental capabilities to interpret the simulation results instead of using most of his efforts to calculate the possible occurrence sequences. Simulation is often used in the design phases and the early investigation of a system design, while the more formal analysis methods are used for validation.	abstract data type;ada;algorithm;arbiter (electronics);cp/m;coloured petri net;computer simulation;control system;correctness (computer science);database;de bruijn graph;debugging;denotational semantics;diagram;entity–relationship model;executable;formal verification;graphical user interface;high- and low-level;integrated services digital network;integrated software;intelligent network;international standard book number;jensen's inequality;lecture notes in computer science;modeller;management system;mutual exclusion;online transaction processing;petri nets;radio frequency;simulation;software development;springer (tank);systems design;technical support;test case;theoretical computer science;very-large-scale integration;www	Kurt Jensen	1996		10.1007/978-3-662-03241-1	programming language;data type;coloured petri net;modularity;abstract data type;petri net;functional programming;algorithm;standard ml;computer science;cpn tools	SE	-26.86118826895323	18.052232243926476	94964
57211c6de40c8ba93ce15df93e83e520d6df51e0	asm ground model and refinement for data warehouses	formal method;abstract state machine;data warehouse;model specification	Data Warehouses and on-line analytical processing (OLAP) systems are a promising area for the application of Abstract State Machines (ASMs). In this paper a ground model specification for data warehouses is sketched that is based on the fundamental idea of separating input from operational databases and output to OLAP systems. On this basis we start defining formal refinement rules for such systems. These refinement rules enable a formal method for the design of data warehouses and OLAP systems that can be applied without knowing details of the ASM formalism.	abstract state machines;correctness (computer science);database;formal methods;high- and low-level;multitier architecture;online analytical processing;online and offline;refinement (computing);requirement;semantics (computer science)	Klaus-Dieter Schewe;Jane Zhao	2005			abstract state machines;database;data mining;formal methods;data warehouse;computer science	DB	-30.534345922171855	12.78256118931034	94982
0f4612e80c049a4cd90f8cf6d5c6797441d4cc9a	some practical issues in building a hybrid deductive geographic information system with a dl component	geographic information system;description logic;conjunctive queries;spatial reasoning;software engineering	We report about some preliminary issues from the DFG project “Description Logics and Spatial Reasoning” (“DLS”, DFG Grant NE 279/8-1), one of whose goals is to develop a prototypical deductive hybrid Geographic Information System (GIS) with a DL-component. In this paper we discuss the multi-dimensionality of the space of design decisions from a software engineering perspective. In order to support appropriate representation of spatial and thematic aspects and, considering the different aspects of the geographic data, querying the GIS in a uniform way, we are developing a hybrid representation and reasoning framework, offering support for different description languages (not necessarily being description logics). In order to be applicable to a wide range of representation and reasoning tasks, the exploited description languages are not fixed, but exchangeable. The paper sketches our vision of a deductive GIS and we evaluate how standard description logic systems can be of value in this setting. We also introduce a class of spatio-thematic conjunctive queries which is useful in our setting here and argue that query satisfiability and containment are decidable.	boolean satisfiability problem;conjunctive query;dls format;datalog;description logic;formal system;geographic information system;hybrid system;knowledge representation and reasoning;no silver bullet;recursion;semantic web;serial ata;software engineering	Michael Wessel	2003			spatial intelligence;data mining;conjunctive query;description logic;satisfiability;geographic information system;decidability;thematic map;computer science	AI	-23.936402785041906	9.818231491433703	94994
2316c03dee09773f1e5c793bed3e03fe627891bf	automatic generation of xml from relations: the nested relation approach	starting;document structure;base relacional dato;base donnee;relation algebra;algebra relacional;estructura documental;generacion automatica;structure document;xml language;specification;database;nest algebra;base dato;relational database;automatic generation;demarrage;generation automatique;especificacion;arranque;base donnee relationnelle;xml document;algebre relationnelle;relational algebra;langage xml;lenguaje xml	We propose a method to generate XML documents from relational databases by using nested relational algebra. Starting with a specification of the structure of the XML document, and a description of the database schema, we give an algorithm to build automatically a nested algebra query that will generate a nested view from the relational database; this view is isomorphic to the desired XML document, that can easily be generated from it. We discuss limitations of (and extensions to) the framework.		Antonio Badia	2003		10.1007/978-3-540-39597-3_33	well-formed document;xml catalog;xml validation;xml encryption;simple api for xml;nested set model;xml;relax ng;xml schema;streaming xml;relational database;computer science;document structure description;block nested loop;xml framework;data mining;xml database;xml schema;database;xml signature;programming language;xml schema editor;efficient xml interchange	NLP	-32.22560250431714	9.16132361152118	95005
23388b107cdf6e9ae5375f2eba5b5ad91b297d46	fuzzy discrete-event simulation with ftiplog	automated highways discrete event simulation simulation languages temporal logic logic programming languages fuzzy logic fuzzy systems theorem proving holographic storage content addressable storage intelligent control road traffic traffic control;holographic storage;road traffic;temporal logic;simulation;automated highways;traffic control;advanced vehicle control systems;intelligent control;theorem proving;fuzzy logic;expressive power;robot controlled car ftiplog fuzzy discrete event simulation fuzzy time parameterized temporal logic based simulation language explicit time representation fuzzy time representation fuzzy associative memory fuzzy holographic memory defuzzification process fuzzification process proof procedure simulator dynamic nature expressive power;associative memory;simulation languages;logic programming languages;content addressable storage;fuzzy systems;fuzzy system;discrete event simulation;discrete event simulation fuzzy logic australia fuzzy systems power system modeling holography logic programming associative memory robot control fuzzy control	A Fuzzy Time-parameterized Temporal Logic-based simulation language is presented. The language represents time explicitly and fuzzily. Mixture of Fuzzy Associative Memory and Fuzzy Holographic Memory is adopted. Fuzzification and defuzzification processes are embedded in the language’s proof procedure, thus transparent to the simulator. So the dynamic nature of fuzzy system can be analysed and represented in a natural way. Expressive power of the language is illustrated from a robot controlled car example.	defuzzification;embedded system;expressive power (computer science);fuzzy control system;fuzzy set;holographic data storage;robot;simulation language;temporal logic	Quan Nguyen;Tu Van Le	1996		10.1109/ANZIIS.1996.573932	fuzzy logic;fuzzy electronics;defuzzification;temporal logic;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy classification;computer science;artificial intelligence;fuzzy number;theoretical computer science;discrete event simulation;neuro-fuzzy;machine learning;automated theorem proving;fuzzy associative matrix;logic programming;fuzzy set operations;expressive power;fuzzy control language;algorithm;fuzzy control system	Robotics	-22.62702053428788	16.128340462752405	95317
300334b5a4728f6560b35c7721a25df2730721c5	optimization of join operations in horizontally partitioned database systems	assemblage;base relacional dato;query language;database system;optimisation;base donnee repartie;ensamble;distributed database;optimizacion;limite inferior;base repartida dato;relational database;lenguaje interrogacion;error analysis;distributed database system;particion;data base management system;computer experiment;sensitivity analysis;partition;mathematical model;base donnee relationnelle;communication cost;optimization;langage interrogation;systeme gestion base donnee;joining;inferior bound;sistema gestion base datos;limite inferieure;lower bound;lagrangian relaxation	This paper analyzes the problem of joining two horizontally partitioned relations in a distributed database system. Two types of semijoin strategies are introduced, local and remote. Local semijoins are performed at the site of the restricted relation (or fragment), and remote semijoins can be performed at an arbitrary site. A mathematical model of a semijoin strategy for the case of remote semijoins is developed, and lower bounding and heuristic procedures are proposed. The results of computational experiments are reported. The experiments include an analysis of the heuristics' performance relative to the lower bounds, sensitivity analysis, and error analysis. These results reveal a good performance of the heuristic procedures, and demonstrate the benefit of using semijoin operations to reduce the size of fragments prior to their transmission. The algorithms for the case of remote semijoins were found to be superior to the algorithms for the case of local semijoins. In addition, we found that the estimation accuracy of the selectivity factors has a significant effect on the incurred communication cost.	algorithm;computation;distributed database;error analysis (mathematics);experiment;heuristic (computer science);mathematical model;relational algebra;selectivity (electronic)	Arie Segev	1986	ACM Trans. Database Syst.	10.1145/5236.5241	partition;computer experiment;lagrangian relaxation;relational database;computer science;mathematical model;database;upper and lower bounds;distributed database;sensitivity analysis;algorithm;query language	DB	-26.70011073702887	4.930501514158775	95365
399916bfeea5af5bcc3daba04742cf54c01247e8	an efficient hypothetical reasoning system for predicate-logic knowledge-base	deductive database hypothetical reasoning system predicate logic knowledge base horn clause knowledge;logic;inference mechanisms;predicate logic knowledge base;navigation;deductive database;propositional logic;knowledge acquisition;artificial intelligence;system development;knowledge based systems artificial intelligence deductive databases inference mechanisms;hypothetical reasoning system;knowledge representation;logic deductive databases inference mechanisms knowledge based systems navigation knowledge acquisition knowledge representation fault diagnosis;knowledge based systems;horn clause knowledge;fault diagnosis;deductive databases;knowledge base	The methods of fast hypothetical reasoning systems developed for propositional logic cannot be applicable in a straightforward manner to the predicate-logic case. A fast hypothetical reasoning mechanism that is effective for predicate-logic knowledge (actually for function-free predicate Horn-clause knowledge) is presented. A reasoning method developed in the deductive database area is effectively applied to this mechanism. >	knowledge representation and reasoning;reasoning system	Akiko Kondo;Toshiro Makino;Mitsuru Ishizuka	1991		10.1109/TAI.1991.167116	natural language processing;knowledge representation and reasoning;opportunistic reasoning;knowledge base;navigation;qualitative reasoning;computer science;artificial intelligence;model-based reasoning;knowledge-based systems;data mining;reasoning system;deductive reasoning;logic;algorithm	AI	-19.40987387656136	10.226135857952789	95388
e61ab17b876e04ae19505826ade333ea9c962b1a	qsql: incorporating logic-based retrieval conditions into sql	query language;sql;information retrieval;quantum logic;databasequery language;database query;dbsir	Evaluating a traditional database query against a data tuple yields true on match and false on mismatch. Unfortunately, there are many application scenarios where such an evaluation is not possible or does not adequately meet user expectations about vague and uncertain conditions. Thus, there is a need for incorporating impreciseness and proximity into a logic-based query language. The calculus query language CQQL [24] has been developed for such scenarios by exploiting results from quantum logic. In this work we will show how to integrate underlying ideas and concepts of CQQL into SQL.	sql	Sebastian Lehrack;Ingo Schmitt	2010		10.1007/978-3-642-12026-8_33	sargable;quantum logic;data definition language;query optimization;sql;.ql;query expansion;boolean conjunctive query;data manipulation language;data control language;computer science;query by example;data mining;database;rdf query language;language integrated query;web search query;view;information retrieval;null;query language;object query language;spatial query	NLP	-25.88951907497255	8.132349627748816	95576
da61e59d5c9cc8a6fad99b07d3bfbbb9b2aced25	data model processing	relational data model;data model;network model;database management system	The Data Model Processor (DMP) is an interactive tool for defining and evaluating data models. It is based on Positional Set Notation, a formalism for uniform representation of data modeling objects. The DMP allows the user to enter a set-theoretic description of a data model's structures and a definition of the model's primitive operations based on positional set operations. Based on the data model definition, the DMP then emulates a database management system (DBMS) implementing that data model. It allows the user to play various roles associated with a DBMS, such as database definer and end user.  This paper gives an overview of the DMP and discusses its foundations, namely, Positional Set Notation and a Positional Set Processor. It traces an example showing how the DMP has been used to model the relational data model. (Hierarchical and network models have also been implemented on the DMP.) Future applications of the DMP are considered.	data model;data modeling;database;definition;emulator;relational model;semantics (computer science);set theory;tracing (software)	Matthew B. Koll;W. Terry Hardgrave;Sandra B. Salazar	1982		10.1145/1500774.1500848	data modeling;semi-structured model;data model;computer science;theoretical computer science;data mining;database;logical data model;database design	DB	-29.745830149391004	10.582574561420001	95606
29eb9e82b7a3aa02b9af5f0f1b6d7c6a4c6e2917	query optimization over crowdsourced data	cost-based query optimizer;query optimizer;crowdsourced data;query optimization;query execution;query semantics;data model;traditional query optimization;query language;query execution engine;query plan	Deco is a comprehensive system for answering declarative queries posed over stored relational data together with data obtained ondemand from the crowd. In this paper we describe Deco’s costbased query optimizer, building on Deco’s data model, query language, and query execution engine presented earlier. Deco’s objective in query optimization is to find the best query plan to answer a query, in terms of estimated monetary cost. Deco’s query semantics and plan execution strategies require several fundamental changes to traditional query optimization. Novel techniques incorporated into Deco’s query optimizer include a cost model distinguishing between “free” existing data versus paid new data, a cardinality estimation algorithm coping with changes to the database state during query execution, and a plan enumeration algorithm maximizing reuse of common subplans in a setting that makes reuse challenging. We experimentally evaluate Deco’s query optimizer, focusing on the accuracy of cost estimation and the efficiency of plan enumeration.	algorithm;analysis of algorithms;crowdsourcing;data model;experiment;mathematical optimization;operational semantics;program optimization;query language;query optimization;query plan	Hyunjung Park;Jennifer Widom	2013	PVLDB	10.14778/2536206.2536207	sargable;query optimization;query expansion;web query classification;computer science;query by example;data mining;database;rdf query language;web search query;view;information retrieval;query language	DB	-26.109494867554478	5.199920752516783	96058
130c9e615ecba4afcb7d37e3e0d1b34cefbee451	checking xpath expressions for synchronization, access control and reuse of query results on mobile clients.	access control	The evaluation of XPath expressions plays a central role in accessing XML documents and therefore may be used in XML database systems for different components. We demonstrate that different applications ranging from access control to transaction synchronization to the reuse of query results have very similar requirements to the evaluation of XPath expressions, which can be solved by the same two steps. Firstly, we compute from each XPath expression a regular expression of the selected node paths and right-shuffle predicate filters to the selected nodes. Secondly, we describe the treatment of predicate filters which may be used in XPath expressions for queries, access control, and synchronization, and present a fast predicate evaluator for these predicates. Finally, we introduce the concept of “ fall-back decisions” , which allow us to use an incomplete but efficient theorem prover, which solves most cases in practice and guarantees correct fallback behavior for the other cases.	access control;apache axis;automated theorem proving;interpreter (computing);lock (computer science);path expression;regular expression;requirement;xml database;xpath	Stefan Böttcher;Adelhard Türling	2003			path expression;database;distributed computing;programming language	DB	-27.87816034386646	10.43830114368892	96195
1e7d4b2bd593e76a06722aba6d0b782ef82407e2	improving the efficiency of reasoning through structure-based reformulation	raisonnement probabiliste;representacion conocimientos;learning algorithm;algorithme glouton;algorithme apprentissage;systeme base connaissances;first order;propositional logic;greedy algorithm;algoritmo gloton;information system;knowledge representation;representation connaissances;algoritmo aprendizaje;systeme information;knowledge based systems;probabilistic reasoning;sistema informacion	We investigate the possibility of improving the efficiency o f reasoning through structure-based partitioning of logical theories , combined with partitionbased logical reasoning strategies. To this end, we provide algorithms for reasoning with partitions of axioms in first-order and proposition al logic. We analyze the computational benefit of our algorithms and detect those parameters of a partitioning that influence the efficiency of computation. Thes e parameters are the number of symbols shared by a pair of partitions, the size of e ach partition, and the topology of the partitioning. Finally, we provide a gree dy algorithm that automatically reformulates a given theory into partitions, e xploiting the parameters that influence the efficiency of computation.	algorithm;automated clearing house;automated theorem proving;boolean satisfiability problem;computation;cryptographic service provider;first-order logic;first-order predicate;inference engine;parallel computing;resolution (logic);theory	Eyal Amir;Sheila A. McIlraith	2000		10.1007/3-540-44914-0_15	knowledge representation and reasoning;combinatorics;greedy algorithm;computer science;artificial intelligence;knowledge-based systems;first-order logic;mathematics;propositional calculus;probabilistic logic;information system;algorithm	AI	-19.355654223253033	11.22433415754884	96486
de965f474573cadb073eb4b96e5d63c4b2c18d19	towards a semantic view of an extended entity-relationship model	base relacional dato;entity relationship model;query language;safeness;aggregation function;aggregate function;logic design;modelo entidad velacion;abstract data type;data type;formal semantics;relational database;modele entite relation;lenguaje interrogacion;semantique formelle;semantic data model;conception logique;calculus;relational completeness;base donnee relationnelle;langage interrogation;systeme gestion base donnee;theoretical foundation;concepcion logica;sistema gestion base datos;database management system;extended entity relationship;entity relationship	Nearly all query languages discussed recently for the Entity-Relationship (ER) model do not possess a formal semantics. Languages are often defined by means of examples only. The reason for this phenomenon is the essential gap between features of query languages and theoretical foundations like algebras and calculi. Known languages offer arithmetic capabilities and allow for aggregates, but algebras and calculi defined for ER models do not.  This paper introduces an extended ER model concentrating nearly all concepts of known so-called semantic data models in a few syntactical constructs. Moreover, we provide our extended ER model with a formal mathematical semantics. On this basis a well-founded calculus is developed taking into account data operations on arbitrary user-defined data types and aggregate functions. We pay special attention to arithmetic operations, as well as multivalued terms allowing nested queries, in a uniform and consistent manner. We prove our calculus only allows the formulation of safe terms and queries yielding a finite result, and to be (at least) as expressive as the relational calculi.	aggregate data;aggregate function;denotational semantics;enhanced entity–relationship model;erdős–rényi model;query language;semantic data model;semantics (computer science)	Martin Gogolla;Uwe Hohenstein	1991	ACM Trans. Database Syst.	10.1145/111197.111200	entity–relationship model;computer science;artificial intelligence;database;algorithm	DB	-27.37289097383212	10.459837691127424	96532
da025acf3ad60123e29690bf5f6c46b589c70330	core schema mappings: scalable core computations in data exchange	schema mappings;data exchange;core computation	Research has investigated mappings among data sources under two perspectives. On one side, there are studies of practical tools for schema mapping generation; these focus on algorithms to generate mappings based on visual specifications provided by users. On the other side, we have theoretical researches about data exchange. These study how to generate a solution – i.e., a target instance – given a set of mappings usually specified as tuple generating dependencies. Since the notion of a core solution has been formally identified as an optimal solution, it is very important to efficiently support core computations in mapping systems. In this paper we introduce several new algorithms that contribute to bridge the gap between the practice of mapping generation and the theory of data exchange. We show how, given a mapping scenario, it is possible to generate an executable script that computes core solutions for the corresponding data exchange problem. The algorithms have been implemented and tested using common runtime engines to show that they guarantee very good performances, orders of magnitudes better than those of known algorithms that compute the core as a post-processing step.	algorithm;computation;executable;performance;video post-processing	Giansalvatore Mecca;Paolo Papotti;Salvatore Raunich	2012	Inf. Syst.	10.1016/j.is.2012.03.004	data exchange;computer science;theoretical computer science;operating system;machine learning;data mining;database;programming language;algorithm	DB	-23.61485039522015	12.877086682235833	96557
cbe1d35e2a5bf6774bb612a3051fd536daa14019	scalable data exchange with functional dependencies	data exchange;functional dependency	The recent literature has provided a solid theoretical foundation for the use of schema mappings in data-exchange applications. Following this formalization, new algorithms have been developed to generate optimal solutions for mapping scenarios in a highly scalable way, by relying on SQL. However, these algorithms suffer from a serious drawback: they are not able to handle key constraints and functional dependencies on the target, i.e., equality generating dependencies (egds). While egds play a crucial role in the generation of optimal solutions, handling them with first-order languages is a difficult problem. In fact, we start from a negative result: it is not always possible to compute solutions for scenarios with egds using an SQL script. Then, we identify many practical cases in which this is possible, and develop a best-effort algorithm to do this. Experimental results show that our algorithm produces solutions of better quality with respect to those produced by previous algorithms, and scales nicely to large databases.	algorithm;best-effort delivery;database;first-order predicate;functional dependency;sql;scalability;schema evolution;type class	Bruno Marnette;Giansalvatore Mecca;Paolo Papotti	2010	PVLDB	10.14778/1920841.1920859	data exchange;computer science;theoretical computer science;data mining;database;functional dependency	DB	-24.37253908967596	9.390005950315745	96609
036465aaf3dd152e5c9465e9d6ef9d87121bf5af	mvt: a schema mapping validation tool	road network;spatial search;schema mapping	Schema mappings define relationships between schemas in a declarative way. We demonstrate MVT, a mapping validation tool that allows the designer to ask whether the mapping has certain desirable properties. The answers to these questions will provide information on whether the mapping adequately matches the intended needs and requirements. MVT is able to deal with a highly expressive class of mappings and database schemas, which allows the use of negations, order comparisons and null values. The tool does not only provide a Boolean answer as test result, but also a feedback for that result. Depending on the tested property and on the test result, the provided feedback can be in the form of example schema instances, or in the form of an explanation, that is, highlighting the mapping assertions and schema constraints responsible for getting such a result.	database schema;requirement;xml schema	Guillem Rull;Carles Farré;Ernest Teniente;Toni Urpí	2009		10.1145/1516360.1516492	schema migration;computer science;conceptual schema;star schema;data mining;database;database schema;algorithm	DB	-25.122921757558792	9.520248435516555	96613
10ee23177976e8556e7d60a7dd451366fec65c08	query optimization for xml	query language;search space;query optimization;semistructured data;indexation;world wide web;cost model	XML is an emerging standard for data representation and exchange on the World-Wide Web. Due to the nature of information on the Web and the inherent exibility of XML, we expect that much of the data encoded in XML will be semistructured: the data may be irregular or incomplete, and its structure may change rapidly or unpredictably. This paper describes the query processor of Lore, a DBMS for XML-based data supporting an expressive query language. We focus primarily on Lore's cost-based query optimizer. While all of the usual problems associated with cost-based query optimization apply to XML-based query languages, a number of additional problems arise, such as new kinds of indexing, more complicated notions of database statistics, and vastly di erent query execution strategies for di erent databases. We de ne appropriate logical and physical query plans, database statistics, and a cost model, and we describe plan enumeration including heuristics for reducing the large search space. Our optimizer is fully implemented in Lore and preliminary performance results are reported.	analysis of algorithms;data (computing);database;heuristic (computer science);mathematical optimization;query language;query optimization;world wide web;xml	Jason McHugh;Jennifer Widom	1999			xml validation;online aggregation;sargable;query optimization;query expansion;web query classification;ranking;boolean conjunctive query;streaming xml;computer science;query by example;data mining;xml database;database;rdf query language;web search query;view;information retrieval;query language;efficient xml interchange;object query language;spatial query	DB	-29.539809793392223	5.843907577390604	96741
3a7fd2ecb787cf892025a497c6740de722f03ec1	iterative modification and incremental evaluation of preference queries	iterative method;optimum pareto;ingenierie connaissances;interrogation base donnee;interrogacion base datos;intelligence artificielle;metodo iterativo;refinement method;methode iterative;preferencia;artificial intelligence;preference;inteligencia artificial;information system;methode raffinement;pareto optimum;metodo afinamiento;optimo pareto;database query;systeme information;sistema informacion;knowledge engineering	We present here a formal foundation for an iterative and incremental approach to constructing and evaluating preference queries. Our main focus is on query modification: a query transformation approach which works by revising the preference relation in the query. We provide a detailed analysis of the cases where the order-theoretic properties of the preference relation are preserved by the revision. We consider a number of different revision operators: union, prioritized and Pareto composition. We also formulate algebraic laws that enable incremental evaluation of preference queries.	interplanetary file system;iterative and incremental development;iterative method;linear algebra;mathematical optimization;pareto efficiency;query language;query optimization;semantic query;theory	Jan Chomicki	2006		10.1007/11663881_5	computer science;artificial intelligence;knowledge engineering;data mining;iterative method;information system;algorithm	DB	-27.136068540786344	11.618818113750706	96783
1a39c17c93fbe763f3829826e075c2755c12272f	the vadalog system: datalog-based reasoning for knowledge graphs		Over the past years, there has been a resurgence of Datalog-based systems in the database community as well as in industry. In this context, it has been recognized that to handle the complex knowledge-based scenarios encountered today, such as reasoning over large knowledge graphs, Datalog has to be extended with features such as existential quantification. Yet, Datalog-based reasoning in the presence of existential quantification is in general undecidable. Many efforts have been made to define decidable fragments. Warded Datalog+/- is a very promising one, as it captures PTIME complexity while allowing ontological reasoning. Yet so far, no implementation of Warded Datalog+/- was available. In this paper we present the Vadalog system, a Datalog-based system for performing complex logic reasoning tasks, such as those required in advanced knowledge graphs. The Vadalog system is Oxfordu0027s contribution to the VADA research programme, a joint effort of the universities of Oxford, Manchester and Edinburgh and around 20 industrial partners. As the main contribution of this paper, we illustrate the first implementation of Warded Datalog+/-, a high-performance Datalog+/- system utilizing an aggressive termination control strategy. We also provide a comprehensive experimental evaluation.	algorithm;benchmark (computing);big data;control theory;data store;datalog;existential quantification;expectation propagation;independence day: resurgence;knowledge graph;mathematical optimization;p (complexity);query plan;recursion;synthetic intelligence;undecidable problem	Luigi Bellomarini;Emanuel Sallinger;Georg Gottlob	2018	PVLDB	10.14778/3213880.3213888	undecidable problem;algorithm;data mining;ontology;p;logical reasoning;datalog;decidability;existential quantification;graph;computer science	DB	-23.07052303296615	10.636829703374486	96834
108493541a953f715969584c01580d2d3ce3a122	answering ad-hoc continuous aggregate queries over data streams using dynamic prefix aggregate tree			aggregate data;aggregate function;hoc (programming language)	Ali A. Safaei;Mehdi Mosaferi;Fatemeh Abdi	2016	Intell. Data Anal.	10.3233/IDA-150287	theoretical computer science;data mining;database	DB	-30.751966622068792	7.288800980782268	97368
8a452b4f63f86fc38bd2d50cc0aa798f4d4ad901	a graph grammar approach to geographical databases	graph grammar;geographic database	This paper treats the modeling of an important class of databases, i.e. geographical databases, with emphasis on both structural (data definition) and behavioral (data manipulation) aspects. Geometric objects such as polygons, line segments, and points may have different relations among each other (such as order, adjacency, connectivity) and can he represented in a uniform spatial data structure (structure graph). The dynamic behavior is defined by a finite set of consistency-preserving state transitions (productions) where coincidence problems as well as topological properties have to he solved. Moreover, the graph grammar approach can he used to study the synchronization of several concurrent productions (Church-Rosser properties) and offers a framework for implementing a geographical database. 1. REPRESENTATION OF GEOGRAPHICAL DATA Database modeling is concerned with the static and dynamic behavior of real world applications. As of today, most of the effort to represent data in databases has been invested in commercial applications, e.g. administration. We have studied a few applications dealing with geographical data, e.g. triangulation networks, real estate parcel plans, and earth resource and land use registers. Computerized geographical databases offer advantages over conventional methods: easy updating, fast analysis, and meaningful visual representation of data can be economically accomplished. Of course, in order to be considered official and legal Cjuridical land register), such a system should ensure data integrity (i.e. reliability, consistency, quality, security and protection of data). Geographical entities are classified geometrically as POINTS, LINES and REGIONS. Relationships among these primitives play an important role. Although the relational model adequately describes these relationships, it has been recognized as lacking some semantics because of its uniform and sometimes inconsistent treatment of attributes and relationships. Extensions have been proposed to capture more of the meaning of the data[l]. Furthermore, the modeling of the behavioral properties in a consistent manner has become more and more important especially in the field of geo-processing where modifications can last as long as a year due to legal procedures[2]. As a consequence, data items and topological properties must be analyzed to decide whether or not certain modifications are allowed (context conditions of the manipulation). The goal of this paper is to model structure and behavior of a geographical database in a very consistMuch of this work was done as a Ph.D. thesis research in the Computer Science Department at the ETH Zurich and was supported, in part, by the Zentenarfond of the ETH under grant number 0.330.080.07/8. tent manner. Our approach is based on ideas of the relational model and concepts of graph grammars (originally described in [3]) for the following main	amiga walker;artificial intelligence;church–rosser theorem;computer science;data definition language;data integrity;data structure;entity;graph rewriting;pointer (computer programming);production system (computer science);relational model;spatial database;triangulation (geometry)	Andreas Meier	1985	Inf. Syst.	10.1016/0306-4379(85)90004-3	natural language processing;computer science;pattern recognition;data mining;attribute grammar;graph database	DB	-29.715924541982623	12.471967670519216	97407
def0acd701a5a69d0869de81cd46d75663133f3c	a data processing algorithm in epc internet of things	product codes big data internet internet of things logistics;data processing algorithm;mapreduce logistics center data processing algorithm hadoop;logistics center;system efficiency data processing algorithm epc internet of things sensor nodes logistics center data processing method big data;mapreduce;hadoop;logistics algorithm design and analysis cloud computing internet of things filtering algorithms data analysis	With the rapid growth of data which sensed by the sensor nodes deployed in the logistics center, the current data processing methods have been impossible to meet the requirements of rapid and efficient analysis and processing for big data. In order to satisfy the demand, this paper designs a data processing algorithm which is implemented by Hadoop. The experimental results show that our method can greatly improve the system efficiency of data processing.	algorithm;apache hadoop;big data;cloud computing;electronic product code;google cloud platform;internet of things;logistics;requirement	Huiqun Zhao;Chuancong Huang	2014	2014 International Conference on Cyber-Enabled Distributed Computing and Knowledge Discovery	10.1109/CyberC.2014.30	computer science;data mining;database;world wide web	Robotics	-31.55641769996034	18.158733743002003	97652
c89cee076195649d683728ff7e66172924099ae2	an overview of the iris object-oriented dbms	knowledge based system;programming language;database management systems;knowledge engineering data structures database management systems;data type;iris database systems image storage software prototyping prototypes object oriented databases application software knowledge based systems data engineering design engineering;data model;object oriented;data structures;next generation;tests and measurements;software design;version control;database management system;programming languages iris object oriented dbms database management system office information knowledge based systems engineering test controlled sharing backup recovery rich data modeling constructs uniform access specialized storage subsystems data types images voice text vectors matrices prolonged transactions inference version control;knowledge engineering	The Iris database management system, a research prototype of a next-generation DBMS (database management system) is presented. Iris is intended to meet the needs of new and emerging database applications such as office information and knowledge-based systems, engineering test and measurement, and hardware and software design. In addition to providing for permanence of data, controlled sharing, backup, and recovery, Iris will also provide a number of needed capabilities that include rich data-modeling constructs, uniform access to specialized storage subsystems and to foreign DBMSs, novel data types (images, voice, text, vectors, matrices), prolonged transactions, direct database support for inference, and version control. Iris will also provide sharing of objects across applications and programming languages.<<ETX>>	backup;data modeling;database;knowledge-based systems;prototype;software design;systems engineering;version control	Daniel H. Fishman	1988	Digest of Papers. COMPCON Spring 88 Thirty-Third IEEE Computer Society International Conference	10.1109/CMPCON.1988.4854	computer science;data mining;database;world wide web;component-oriented database	DB	-32.51053074941648	12.174999558078802	97727
24e1c9d8304dba2acd3ca746f8d4e2bb5b3db5c8	description and identification of distributed fragments of recursive relations	distributed system;lattice descriptions;optimal solution;base donnee repartie;systeme reparti;distributed database;relation ordre partiel;empirical analysis;lattices;probleme np complet;storage management;distributed recursive relations;lattice cover;query formulation;distributed computing;base repartida dato;formulacion pregunta;near optimal solutions distributed fragments parallel environments distributed environment lattice structures fragmentation problems distributed recursive relations identification methods distributed databases lattice descriptions np complete problem empirical analysis;formulation question;partial order set;fragments;recursion relation;parallel environments;sistema repartido;marine vehicles;distributed environment;computational complexity;partial ordering;database systems;performance analysis;distributed databases;recursive functions;near optimal solutions;recursive relations;lattice structures;problema np completo;object oriented databases;computational complexity distributed databases storage management database theory recursive functions;relacion orden parcial;systeme parallele;parallel system;recursive relation;distributed fragments;database theory;algorithm design and analysis;fragmentation problems;lattices deductive databases object oriented databases distributed databases database systems marine vehicles distributed computing np complete problem performance analysis algorithm design and analysis;sistema paralelo;identification methods;np complete problem;fragmento;lattice;deductive databases;fragment	In a distributed environment, it is advantageous to fragment a relation and store the fragments at various sites. Based on the concept of lattice structures, we develop a framework to study the fragmentation problems of distributed recursive relations. Two of the fragmentation problems are how to describe and identify fragments. Description and identification methods previously suggested are more suitable in parallel environments than in distributed databases. We propose a method to describe and identify fragments based on lattice structures. Finding lattice descriptions of fragments is shown to be an NP complete problem. We analyze the performance of the lattice approach both theoretically and experimentally. This is done by creating a database of recursive relations. The empirical analysis shows that our proposed algorithms give near optimal solutions.	recursion	Sakti Pramanik;Sungwon Jung	1996	IEEE Trans. Knowl. Data Eng.	10.1109/69.553168	computer science;theoretical computer science;machine learning;lattice;data mining;database;mathematics;distributed computing;distributed database;algorithm	DB	-27.652549599436636	4.8715476039975405	97884
a5a9e20f081ca665c3e6dbe337d57f161bd19eca	performance criteria for relational databases in different normal forms	base relacional dato;experimental method;metodologia;storage access;information retrieval;distributed processing;simulation;interrogation base donnee;forma normal;interrogacion base datos;simulacion;tiempo acceso;dependance multivaluee;relational database;space time;espacio tiempo;methodologie;dependencia multivaluada;client server;multivalued dependency;space use;recherche information;acces memoire;time use;base donnee relationnelle;acceso memoria;normal form;temps acces;forme normale;recuperacion informacion;methodology;efficiency measurement;database query;espace temps;access time	With the increase in client-server and other applications that are dependent on telecommunications possibilities and the use of dispersed and distributed processing, desire for access to organizational databases will increase. The increasing demands on databases make efficient storage space and access time important issues. In this article we illustrate criteria for measuring efficiency of database access as applications expressed in different normal forms. The efficiency measures are (1) actual memory space use, (2) time use, and (3) average number of operations performed. These are tested based on an optimal data search simulation of 25 different database cases for four distinct queries in both third and fourth normal form, and actual space and time use and average number of operations required by 208 subjects who performed the same queries in third and fourth normal form. We find that in almost all studied cases where there are multivalued dependencies, the fourth normal form is more efficient. These techniques, particularly the simulation and the experimental method, may be used as efficiency measures when establishing the ideal form for a new database.		Marion G. Sobol;Albert Kagan;Hirohisa Shimura	1996	Journal of Systems and Software	10.1016/0164-1212(95)00062-3	third normal form;access time;relational database;computer science;space time;methodology;database;engineering drawing;algorithm;multivalued dependency;client–server model	SE	-26.914371064949552	5.090019266855644	98228
eaefcbc08c6d0565aba1e3c17389317b051dddfe	relaxing constraints in enhanced entity-relationship models using fuzzy quantifiers	entity relationship model;fuzzy set;fuzzy quantifiers conceptual database design extended or enhanced entity relationship model eer fuzzy conceptual modeling fuzzy constraints fuzzy databases;database management systems;conceptual database design;conceptual model;indexing terms;database management systems entity relationship modelling fuzzy systems;conceptual design;object oriented modeling erbium fuzzy sets fuzzy systems data models uncertainty database systems fuzzy set theory knowledge management standards publication;extended or enhanced entity relationship model eer;fuzzy quantifiers;entity relationship modelling;fuzzy databases;conceptual database design enhanced entity relationship model fuzzy quantifiers fuzzy query fuzzy databases;database design;fuzzy constraints;fuzzy database;fuzzy conceptual modeling;fuzzy systems;entity relationship;modeling tool	While various articles about fuzzy entity relationship (ER) and enhanced entity relationship (EER) models have recently been published, not all examine how the constraints expressed in the model may be relaxed. In this paper, our aim is to relax the constraints which can be expressed in a conceptual model using the modeling tool, so that these constraints can be made more flexible. We will also study new constraints that are not considered in classic EER models. We use the fuzzy quantifiers which have been widely studied in the context of fuzzy sets and fuzzy query systems for databases. In addition, we shall examine the representation of these constraints in an EER model and their practical repercussions. The following constraints are studied: the fuzzy participation constraint, the fuzzy cardinality constraint, the fuzzy completeness constraint to represent classes and subclasses, the fuzzy cardinality constraint on overlapping specializations, fuzzy disjoint and fuzzy overlapping constraints on specializations, fuzzy attribute-defined specializations, fuzzy constraints in union types or categories and fuzzy constraints in shared subclasses. We shall also demonstrate how fuzzy (min, max) notation can substitute the fuzzy cardinality constraint but not the fuzzy participation constraint. All these fuzzy constraints have a new meaning, they offer greater expressiveness in conceptual design, and are included in the so-called fuzzy EER model.	cardinality (data modeling);category theory;constraint logic programming;data integrity;database trigger;enhanced entity–relationship model;entity;erdős–rényi model;fuzzy concept;fuzzy logic;fuzzy set;information system;knowledge management;maxima and minima;natural language;quantifier (logic);sql;turing completeness;universal quantification;vagueness	José Galindo;Angélica Urrutia;Ramón Alberto Carrasco;Mario Piattini	2004	IEEE Transactions on Fuzzy Systems	10.1109/TFUZZ.2004.836088	fuzzy logic;membership function;defuzzification;entity–relationship model;adaptive neuro fuzzy inference system;fuzzy transportation;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;fuzzy subalgebra;fuzzy number;neuro-fuzzy;machine learning;fuzzy measure theory;data mining;database;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;fuzzy control system	DB	-22.02833436311637	5.288692832872674	99303
40bf57785d1c3ef49b360703ad47f7c3ce2dd056	conceptual schemas and ontologies for database access: myths and challenges	information access;conceptual schema;conceptual modelling;classical logic;description logic;ontology design;entity relationship	First, I will argue that well-founded conceptual modelling and ontology design is required to support intelligent information access. Then, I will show which are the technical consequences of such choices, and how the foundational and computational problems to be faced are non-trivial. The arguments are based on the use of classical logics and description logics as a formal tools for the framework, and I will make use of languages and examples taken from the Entity-Relationship arena.	ontology (information science)	Enrico Franconi	2007		10.1007/978-3-540-75563-0_3	conceptual model;classical logic;description logic;entity–relationship model;computer science;knowledge management;three schema approach;conceptual schema;data mining;database;ontology language	DB	-21.723381847036404	8.795148574883473	99340
ae5087f6980ea88add59632a8defac24b205b89b	ontologies and description logics	description logic	Ontologies and Description Logics (DLs). Why putting them in a single chapter for this document devoted to AI research in Italy? Well, a good reason is that both research areas originated from a single, classic mainstream in AI: logic-based knowledge representation (KR). Indeed, starting from the late 80s, Italian researchers brought a radical contribution to the formal understanding of so-called structured representation languages developed in the previous decade, whose most known example was KL-ONE. Their research took however two different directions. On one hand, after Levesque and Brachman’s logicalization of KL-ONE and their discovery of the fundamental tradeoff between expressivity and tractability [8], a whole Italian school emerged under the direction of Maurizio Lenzerini, focusing mainly on the need to develop KR languages with predictable computational behavior and sound and complete inference algorithms. On the other hand, Nicola Guarino started analyzing the implicit and often ambiguous ontological assumptions made by structured logical formalisms, such as those behind the notions of role and attribute, arguing against the ontological neutrality of representation systems and suggesting formal ontological distinctions to be taken into account. In the meanwhile, in the early 90s, a new hot topic captured the attention of AI researchers: knowledge sharing among heterogeneous, distributed sources (very similar to the issue emerging –roughly at the same time– in the DB community: data integration). It became clear soon that, in order to attack these problems, two complementary aspects needed to be addressed: content and reasoning. Ontological analysis and DLs emerged soon as the proper techniques to deal with these aspects, and became later very popular for their key role in the future Semantic Web. Italian researchers contributed in a peculiar way to both.	algorithm;description logic;hector levesque;kl-one;knowledge representation and reasoning;maurizio lenzerini;ontology (information science);semantic web	Diego Calvanese;Nicola Guarino	2006	Intelligenza Artificiale		natural language processing;world wide web;argument;computer science;web ontology language;t-norm fuzzy logics;knowledge representation and reasoning;classical logic;ontology (information science);description logic;artificial intelligence;ontology language	AI	-21.469330903100964	8.058196628407808	99347
67801f154ad4b553dda27a45ec26311b2596e881	an investigation into owl for concrete syntax specification using uml notations	owl;uml notations;ontology reasoner;postprint article;rcc 8;concrete syntax speci cation;ontology	The Web Ontology Language OWL is a prominent ontology language for specifying ontologies. Although OWL ontologies are wellused for representing and reasoning about knowledge in various domains, they are sparsely studied for visual language specification. The work in this paper, therefore, explores OWL for visual language specification by specifying the concrete syntax of selected UML class diagram notations in an ontology. The selected diagram notations are specified as spatial configurations of primitive elements and qualitative base spatial relationships of Region Connection Calculus-8 (RCC-8). Furthermore, the automated reasoning features of ontology reasoners are investigated to verify the completeness and the correctness of the specification. The verification results indicate that the given specification needs to be revised to support applications to draw the selected notations. The value of such a specification in supporting a semantic diagram interpretation application is demonstrated using the automated instance classification feature of ontology reasoners.		Anitta Thomas;Aurona Gerber;Alta van der Merwe	2016		10.1007/978-3-319-42333-3_15	natural language processing;computer science;applications of uml;database;programming language;owl-s	SE	-28.470391103804953	15.643598868773022	99398
6f80133edc8cba66330bf848368a336ec7504bdd	converting tpc-h query templates to use dsqgen for easy extensibility	databases;software engineering;system performance;database management;automatic generation;operating system;performance analysis;information system;benchmark development	The ability to automatically generate queries that are not known apriory is crucial for ad-hoc benchmarks. TPC-H solves this problem with a query generator, QGEN, which utilizes query templates to generate SQL queries. QGEN’s architecture makes it difficult to maintain, change or adapt to new types of query templates since every modification requires code changes. DSQGEN, a generic query generator, originally written for the TPC-DS benchmark, uses a query template language, which allows for easy modification and extension of existing query templates. In this paper we show how the current set of TPC-H query templates can be migrated to the template language of DSQGEN without any change to comparability of published TPC-H results. The resulting query template model provides opportunities for easier enhancement and extension of the TPC-H workload, which we demonstrate.	benchmark (computing);extensibility;hoc (programming language);ibm tivoli storage productivity center;sql;template processor	John M. Stephens;Meikel Pöss	2009		10.1007/978-3-642-10424-4_8	online aggregation;sargable;query optimization;query expansion;web query classification;ranking;computer science;query by example;data mining;database;rdf query language;programming language;web search query;view;query language;object query language;spatial query	DB	-30.116411955479386	5.774786438109766	99412
eba05ca27a71c1161a22c6b57665a32f1e360da9	06472 abstracts collection - xquery implementation paradigms	xquery xpath xml xquery benchmarking xquery optimization xquery interoperability xquery hard nut compilation benchmarking xmark recursion;004;database system functional programming language transaction management distributed query processing	From 19.11.2006 to 22.11.2006, the Dagstuhl Seminar 06472 ``XQuery Implementation Paradigms'' was held in the International Conference and Research Center (IBFI), Schloss Dagstuhl. During the seminar, several participants presented their current research, and ongoing work and open problems were discussed. Abstracts of the presentations given during the seminar as well as abstracts of seminar results and ideas are put together in this paper. The first section describes the seminar topics and goals in general. Links to extended abstracts or full papers are provided, if available.		Peter A. Boncz;Torsten Grust;Jérôme Siméon;Maurice van Keulen	2006			computer science;data mining;database;information retrieval	NLP	-31.544851070521077	6.734416719217075	99509
794e8abadf1f7bc0fb8fce4a8a33f8a76bbd6f97	modifying the entity relationship modelling notation: towards high quality relational databases from better notated er models		The entity relationship modelling using the original ER notation has been applauded providing a natural view of data in conceptual modelling of information systems. However, the current ER to relational model transformation algorithm is known to be insufficient in providing a complete and accurate representation of the ER model undertaken for transformation. In an effort to derive better transformations from ER models, we have understood that modifications should be introduced to both of the existing transformation algorithm as well as to the ER notation. Introducing some new concepts, we have adapted the original ER notation and developed a new transformation algorithm based on the existing one. This paper presents the modified ER notation with an ER diagram drawn based on the new notation.	algorithm;diagram;display resolution;entity;entity–relationship model;erdős–rényi model;information system;model transformation;relational database;relational model	Dhammika Pieris	2013	CoRR		computer science;data mining;database;set-builder notation;algorithm	DB	-32.38208935197734	13.242697580436126	99693
98bfab6af1ca2cf0559e7050b280618c6e428137	property covering: a powerful construct for schema derivations	modelizacion;architecture systeme;object oriented data model;clase complejidad;semantics;semantica;semantique;modelisation;classe complexite;complexity class;arquitectura sistema;information system;system architecture;modeling;systeme information;sistema informacion	Covering is a well known relationship in semantic and object oriented models that holds when a class is the union of a collection of subclasses Covering has been studied in the past only for entity classes In this paper we study covering for properties and we introduce a new relationship called property covering Property covering holds when a property is the union of a collection of subproperties Property covering allows to i partition a property into subproperties ii express property value re nement and iii express a particular form of negative informa tion We demonstrate that property covering is a powerful conceptual modeling mechanism and we use it to provide a set of inference rules for schema derivations	hereditary property	Anastasia Analyti;Nicolas Spyratos;Panos Constantopoulos	1997		10.1007/3-540-63699-4_22	complexity class;systems modeling;computer science;artificial intelligence;semantics;information system;algorithm	DB	-29.54591835209678	12.75270645954995	99759
d6a61ea0d3d9109cb35e6f3cc2290e7a142a8e66	some specifications for a natural query language: how is it possible to meet the casual user	query language;linguistic analysis	In this paper we define the main characteristics of a possible quasi-natural query language. Some linguistic analysis methods propounded (at the lexical, syntactic and semantic levels) are the results of a previous experience which resulted in a prototype in which only some of the exposed concepts were implemented. The data base model used is CODASYL-like.	codasyl;database;prototype;query language	Alfred Mesguich;Bernard Normier	1977	SIGART Newsletter	10.1145/1045283.1045336	natural language processing;query optimization;query expansion;data control language;computer science;database;rdf query language;programming language;query language;object query language	DB	-32.49846829734299	8.615330803603651	99927
95466f00a92beb717388a713d445757135038e21	detecting and correcting conservativity principle violations in ontology-to-ontology mappings		In order to enable interoperability between ontology-based systems, ontology matching techniques have been proposed. However, when the generated mappings suffer from logical flaws, their usefulness may be diminished. In this paper we present an approximate method to detect and correct violations to the so-called conservativity principle where novel subsumption entailments between named concepts in one of the input ontologies are considered as unwanted. We show that this is indeed the case in our application domain based on the EU Optique project. Additionally, our extensive evaluation conducted with both the Optique use case and the data sets from the Ontology Alignment Evaluation Initiative (OAEI) suggests that our method is both useful and feasible in practice.	application domain;approximation algorithm;directed acyclic graph;entity;gene ontology;graph (abstract data type);interoperability;is-a;multi-agent system;ontology (information science);ontology alignment;ontology-based data integration;peano axioms;propositional calculus;scalability;semantic integration;software deployment;subsumption architecture	Alessandro Solimando;Ernesto Jiménez-Ruiz;Giovanna Guerrini	2014		10.1007/978-3-319-11915-1_1	theoretical computer science;data mining;database;algorithm	AI	-22.805233122395308	7.891233487374184	100128
7d5b6c0491893a15a1dafb64a2a0a7f2f2a20922	t10 data integrity feature (logical block guarding)			data integrity	Martin K. Petersen	2007			data mining;computer science;data integrity;pattern recognition;artificial intelligence	DB	-27.10691079706637	13.55003153304557	100280
d0c5ac8a4464630362405753903720e6b55f1902	the schema evolution and data migration framework of the environmental mass database imis	manuals;change detection;evolutionary computation;nontrivial data migration;information systems;object oriented data model;tool support;particle measurements;radioactivity measurement;multitier system architectures schema evolution environmental mass database object oriented data model evolution relational schemas tool supported object relational mapping nontrivial data migration type evolution transparent data access automatic model change detection mechanism data migration api imis project information system environmental radioactivity measurements;imis project;automatic model change detection mechanism;data migration;object oriented programming;object oriented data model evolution;water storage;environmental science computing;object relational mapping;environmental radioactivity measurements;type evolution;application program interfaces;environmental mass database;entity relationship modelling;data access;tool supported object relational mapping;multitier system architectures;data migration api;predictive models;relational databases;object oriented databases;information system;system architecture;sea measurements information systems water storage object oriented databases data models predictive models particle measurements soil measurements manuals relational databases;relational schemas;schema evolution;entity relationship modelling object oriented databases relational databases object oriented programming application program interfaces environmental science computing radioactivity measurement evolutionary computation;soil measurements;transparent data access;sea measurements;data models	This paper describes a framework that supports the simultaneous evolution of object-oriented data models and relational schemas with respect to a tool-supported object-relational mapping. The proposed framework accounts for non-trivial data migration induced by type evolution from the outset. The support for data migration is offered on the level of transparent data access. The framework consists of the following integrated parts: an automatic model change detection mechanism, a generator for schema evolution code and a generator for data migration APIs. The framework has been concepted in the IMIS project. IMIS is an information system for environmental radioactivity measurements. Though the indicated domain especially demands a solution like the one discussed in this paper, the achievements are of general purpose for multi-tier system architectures with object-relational mapping.	application programming interface;data access;data model;information system;multitier architecture;object-relational database;object-relational mapping;relational model;sql;schema evolution;toplink	Dirk Draheim;Matthias Horn;Ina Schulz	2004	Proceedings. 16th International Conference on Scientific and Statistical Database Management, 2004.	10.1109/SSDBM.2004.69	computer science;theoretical computer science;data mining;database;programming language;information system;evolutionary computation	DB	-33.43368932323476	12.715751431624257	100644
06d7be981f540bd4fd41cb99ead3c2fecec62caa	xml queries and algebra in the enosys integration platform	query plan decomposition challenge;xml query language;query language;query plan composition;xml query;enosys integration platform;application issues xml query;enosys xml integration platform;query processor architecture;underlying xml query algebra;query processor;customer relationship management;relational database;xml;decision support;e commerce;processor architecture;database system;supply chain management	We describe the Enosys XML Integration Platform (EXIP), focusing on the query language, algebra, and architecture of its query processor. The platform enables the development of eBusiness applications in customer relationship management, e-commerce, supply chain management, and decision support. These applications often require that data be integrated dynamically from multiple information sources. The Enosys platform allows one to build (virtual and/or materialized) integrated XML views of multiple sources, using XML queries as view de ̄nitions. During run-time, the application issues XML queries against the views. Queries and views are translated into the XCQL Algebra and are combined into a single algebra expression/plan. Query plan composition and query plan decomposition challenges are faced in this process. Finally, the query processor lazily evaluates the result, using an appropriate adaptation of relational database iterator models to XML. The paper describes the platform architecture and components, the supported XML query language and the query processor architecture. It focuses on the underlying XML query algebra, which di®ers from the algebras that have been considered by W3C in that it is particularly tuned to semistructured data and to optimization and e±cient evaluation in a system that follows the conventional architecture of database systems.	customer relationship management;decision support system;e-commerce;integration platform;iterator;mathematical optimization;query language;query plan;relational database;xml;xquery	Yannis Papakonstantinou;Vinayak R. Borkar;Maxim Orgiyan;Konstantinos Stathatos;Lucian Suta;Vasilis Vassalos;Pavel Velikhov	2003	Data Knowl. Eng.		xml validation;xml encryption;sargable;query optimization;query expansion;web query classification;supply chain management;xml;microarchitecture;streaming xml;relational database;computer science;query by example;document structure description;xml framework;data mining;xml database;xml schema;database;rdf query language;xml signature;view;world wide web;xml schema editor;query language;efficient xml interchange	DB	-32.612189764285226	7.855969009247911	100880
6465c784810d230c43ddafa1e4d7e141d1a4a9b6	test strategy generation using quantified csps	model based diagnosis;quantified csps;test generation;adversarial planning;game playing	Testing is the process of stimulating a system with inputs in order to reveal hidden parts of the system state. We consider a variant of the testing problem that was put forward in the model-based diagnosis literature, and consists of finding input patterns that definitely discriminate between different constraint-based system models. We show that this problem can be framed as a game played between two opponents, and naturally lends itself towards a formulation in terms of quantified CSPs. This QCSP-based formulation is a starting point to extend testing to a new classes of practically relevant applications – namely, systems with limited controllability – where tests consist of stimulation strategies instead of simple input patterns.	autonomous robot;cryptographic service provider;embedded system;test card;test strategy;transition system	Martin Sachenbacher;Paul Maier	2008		10.1007/978-3-540-85958-1_43	mathematical optimization;simulation;artificial intelligence;mathematics;algorithm	AI	-22.01325170228182	13.69213012560142	101144
e810d1b6a6b0ba1ca9162c483aa7ee6860d38db7	the difficulties of using design patterns among novices: an exploratory study	distributed computing conference management technology management file servers prototypes file systems system performance computer applications application software laboratories;directory object identifier and filename hashing;subtree hashing;metadata management;distributed processing;tree data structures;current distribution;tree data structures distributed processing meta data;distributed metadata management scheme;meta data;load balance;subtree partitioning;directory object identifier and filename hashing distributed metadata management scheme subtree partitioning subtree hashing	Elements of design patterns have been incorporated in computer science syllabus. Most of these efforts have been motivated by the benefits offered by patterns as well as positive feedback from the industry and software community. Despite various techniques and approaches suggested by researchers and educators to ensure effective learning of patterns, there is no formal reports on the actual difficulties encountered by these novices when applying patterns. Thus, we have conducted an exploratory study to identify the difficulties they have in using patterns.	computer science;design pattern;exploratory testing;positive feedback	Masita Jalil;Shahrul Azman Mohd. Noah	2007	2007 International Conference on Computational Science and its Applications (ICCSA 2007)	10.1109/ICCSA.2007.32	computer science;load balancing;theoretical computer science;operating system;database;tree;metadata;world wide web;computer security;data element;metadata repository	SE	-33.50068816221397	16.23282444455088	101709
641e9c5a64ddb0650efe716f992c94119a6090ad	a data/knowledge base management testbed and experimental results on data/knowledge base query and update processing	knowledge base management system;knowledge management;design and implementation;relational database system;logic programs;database management system;data structure;knowledge base	This paper presents our experience in designing and implementing a data/knowledge base management testbed. The testbed consists of two layers, the knowledge manager and the database management system, with the former at the top. The testbed is based on the logic programming paradigm, wherein data, knowledge, and queries are all expressed as Horn clauses. The knowledge manager compiles pure, function-free Horn clause queries into embedded-SQL programs, which are executed by the database management system to produce the query results. The database management system is a commercial relational database system and provides storage for both rules and facts. First, the testbed architecture and major data structures and algorithms are described. Then, several preliminary tests conducted using the current version of the testbed and the conclusions from the test results are presented. The principal contributions of this work have been to unify various concepts, both previously published and new ones we developed, into a real system and to present several insights into data/knowledge base management system design gleaned from the test results and our design and implementation experience.	algorithm;data structure;embedded sql;embedded system;horn clause;knowledge base;logic programming;programming paradigm;pure function;relational database management system;systems design;testbed	Raja Ramnarayan;Hongjun Lu	1988		10.1145/50202.50249	knowledge base;relational database management system;data structure;data management;computer science;knowledge management;knowledge-based systems;data mining;database;knowledge extraction;view;database design	DB	-29.980221506823845	10.505567118489598	101833
83c0bc7b55d825faefef46d2c547b30158e354ba	an extendable natural language interface to a consumer service database	natural language interfaces;systeme intelligent;service provider;information retrieval;sistema inteligente;information access;recherche information;intelligent system;acces information;natural language interface;acceso informacion;recuperacion informacion;information system;interface langage naturel;systeme information;sistema informacion	A natural language interface to a consumer service provider database is presented. While natural language interfaces to applications are becoming more common, they are generally not very robust. We present an approach that addresses three specific issues related to robustness. We describe an approach for dealing with underspecified queries, for processing queries that require some limited form of inference, and for handling queries involving synonyms.	extensibility;natural language user interface	Petr Kubon;Fred Popowich;Gordon Tisher	2000		10.1007/3-540-45486-1_7	service provider;natural language processing;natural language user interface;computer science;database;world wide web;information system	DB	-33.657610910252224	9.384360045557512	101834
cf0f79da00fdbc66d33aab34fa74b652ad77ce18	formal semantics-preserving translation from fuzzy er model to fuzzy owl dl ontology	databases;erbium;object recognition;owl;reliability;semantics preserving translation to;computability;iron;fuzzy owl dl ontology;semantics preserving translation to fuzzy er model fuzzy owl dl ontology;reasoning mechanism formal semantics preserving translation fuzzy er model fuzzy owl dl ontology web ontologies semantic web information imprecision information uncertainty domain knowledge fuzzy database model fuzzy ontology development formal definition model theoretic semantics formal translation description logic f shoin d knowledge base satisfiability redundancy;formal semantics;satisfiability;data mining;fuzzy set theory;ontologies artificial intelligence;ontology development;domain knowledge;real world application;cognition;entity relationship modelling;semantic web;erbium owl ontologies fuzzy logic fuzzy reasoning semantic web uncertainty data mining databases automatic logic units;ontologies;profitability;relational databases;semantic web computability data mining entity relationship modelling fuzzy set theory ontologies artificial intelligence relational databases reliability;description logic;fuzzy er model;fuzzy database;knowledge base	How to quickly and cheaply construct Web ontologies has become a key technology to enable the Semantic Web. However, information imprecision and uncertainty exist in many real-world applications. Thus constructing fuzzy ontology by extracting domain knowledge from fuzzy database model such as fuzzy ER model can profitably support fuzzy ontology development. In this paper, firstly, we give the formal definition and semantics of fuzzy ER model. Then, we introduce a kind of fuzzy extension of OWL DL, named fuzzy OWL DL. Furthermore, based on the fuzzy OWL DL, the formal definition and Model-Theoretic semantics of fuzzy OWL DL ontology are given. What’s more, we realize the formal translation from fuzzy ER model to fuzzy OWL DL ontology by a semantics-preserving translation algorithm. Finally, since a fuzzy OWL DL ontology is being equivalent to a description logic f-SHOIN(D) knowledge base, the reasoning problem of satisfiability, subsumption, and redundancy of fuzzy ER model may reason automatically through reasoning mechanism of f-SHOIN(D) is also investigated, which can contribute to constructing fuzzy OWL DL ontologys exactly that meet application’s needs.	algorithm;database model;description logic;entity–relationship model;erdős–rényi model;knowledge base;ontology (information science);semantic web;subsumption architecture	Fu Zhang;Zongmin Ma;Yanhui Lv;Xing Wang	2008	2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology	10.1109/WIIAT.2008.46	fuzzy logic;knowledge base;description logic;erbium;cognition;fuzzy classification;computer science;ontology;artificial intelligence;neuro-fuzzy;semantic web;formal semantics;data mining;reliability;database;fuzzy associative matrix;computability;iron;owl-s;information retrieval;domain knowledge	Web+IR	-22.579409848720722	6.370994445353189	101878
c514b2d1938194608fd57a0f71011a34059d4b5e	pardes - an enhanced active database system (abstract)	language use;active database system;efficient algorithm;active database;expressive power;exception handling	Traditional databases are passive. They do only what isexplicitly requested in the user's query or update operation. Theactive database paradigm states that a database may react in anintelligent way to an external input by creating and executingdatabase operations, which, though not explicitly requested in theinput, are required to preserve invariants associated to the database. This paradigm replaces many operations traditionally implementedby application programs with descriptive definitions that are partof the database schema. Models and research prototypes based onthis paradigm are POST-GRES, HiPAC, SAPIENS, Ariel. Our model extends existing active database models in thefollowing ways: It increases the expressive-power of the language used todefine the database schema by allowing the specification of aricher class of invariants than is currently supported. Example:(SALARY:=BASE-SALARY + BONUS + 1000* count(SUBORDINATES)) The proposed language is compact yet powerful. Itreduces the cost of development and maintenance of database updateapplications and reduces the problems of validation andverification.It provides an efficient algorithm for generating the auxiliaryoperations required to preserve the invariants after an update (Inother models these operations have to be explicitly coded, eitherin the application program or in the database schema.)Our model introduces the notion of Exception-Handling Mode,proposes a number of such modes, and specifies how modes can bedefined in the data base schema. The modes we propose eliminate alarge portion of the exception handling code that currently existsin application programs.	an/cyz-10;active database;algorithm;database model;database schema;exception handling;invariant (computer science);programming paradigm	Opher Etzion	1990		10.1145/100348.100443	exception handling;database theory;intelligent database;semi-structured model;database tuning;computer science;artificial intelligence;theoretical computer science;database model;data mining;database;programming language;view;database schema;expressive power;database testing;database design	DB	-29.685344952895296	11.581587492627277	101915
4056aeb78e6179c5122c2b42413f81fcb6822b5a	distributed reasoning with ontologies and rules in order-sorted logic programming	distributed reasoning;rigidity;ontologies and rules;top down;software agent;order sorted logic;linear resolution system;semantic web;query answering;logic programs;knowledge base	Integrating ontologies and rules on the Semantic Web enables software agents to interoperate between them; however, this leads to two problems. First, reasoning services in SWRL (a combination of OWL and RuleML) are not decidable. Second, no studies have focused on distributed reasoning services for integrating ontologies and rules in multiple knowledge bases. In order to address these problems, we consider distributed reasoning services for ontologies and rules with decidable and effective computation. In this paper, we describe multiple order-sorted logic programming that transfers rigid properties from knowledge bases. Our order-sorted logic contains types (rigid sorts), non-rigid sorts, and unary predicates that distinctly express essential sorts, non-essential sorts, and non-sortal properties. We formalize the order-sorted Horn-clause calculus for such expressions in a single knowledge base. This calculus is extended by embedding rigid-property derivation for multiple knowledge bases, each of which can transfer rigidproperty information from other knowledge bases. In order to enable the reasoning to be effective and decidable, we design a query-answering system that combines order-sorted linear resolution and rigid-property resolution as top-down algorithms. keywords: distributed reasoning, ontologies and rules, order-sorted logic, rigidity, linear resolution system	algorithm;authorization;computation;futures studies;horn clause;interoperability;knowledge base;logic programming;ontology (information science);question answering;recursion;regular expression;ruleml;semantic web rule language;software agent;top-down and bottom-up design;trustworthy computing;unary operation;web ontology language	Ken Kaneiwa;Riichiro Mizoguchi	2009	J. Web Sem.	10.1016/j.websem.2009.05.003	knowledge representation and reasoning;knowledge base;description logic;computer science;artificial intelligence;theoretical computer science;software agent;semantic web;top-down and bottom-up design;data mining;database;reasoning system;deductive reasoning;rigidity;algorithm	AI	-20.87196580233718	9.622224713352592	101919
4be11d8c9e39f81f5ccd491c4e64840ce75a2a37	a semantic approach to keyword search over relational databases	social network data management;semantic approach;keyword search;relational databases	Research in relational keyword search has been focused on the efficient computation of results as well as strategies to rank and output the most relevant ones. However, the challenge to retrieve the intended results remains. Existing relational keyword search techniques suffer from the problem of returning overwhelming number of results, many of which may not be useful. In this work, we adopt a semantic approach to relational keyword search via an Object-Relationship-Mixed data graph. This graph is constructed based on database schema constraints to capture the semantics of objects and relationships in the data. Each node in the ORM data graph represents either an object, or a relationship, or both. We design an algorithm that utilizes the ORM data graph to process keyword queries. Experiment results show our approach returns more informative results compared to existing methods, and is efficient.	computation;database schema;information;relational database;search algorithm	Zhong Zeng;Zhifeng Bao;Mong-Li Lee;Tok Wang Ling	2013		10.1007/978-3-642-41924-9_21	relational calculus;relational database;computer science;data mining;database;graph database;information retrieval	DB	-31.84521133436537	4.474535573062578	102081
5b179e456b088a414e6438c098315899b37a9c1b	using field specifications to determine attribute equivalence in heterogeneous databases	databases;field names;field specifications;design criteria;databases algorithm design and analysis dictionaries object detection inspection;heterogeneous databases;inspection;data structures;data values;dictionaries;distributed databases;distributed databases data structures;synonym dictionaries;attribute equivalence;data values field specifications attribute equivalence heterogeneous databases synonym dictionaries field names design criteria;algorithm design and analysis;object detection	One step in integrating heterogeneous database systems is matching equivalent attributes: Determining which fields in the two databases refer to the same data. We see three (complementary) techniques to automate this process: Synonym dictionaries that compare field n a m e s , design criteria that compare field specifications, and comparison of data values. In this paper we present a technique for using field specifications to compare attributes, and evaluate this technique on a variety of databases.	attribute domain;dictionary;field specification;heterogeneous database system;ibm notes;turing completeness;visual instruction set	Wen-Syan Li;Chris Clifton	1993		10.1109/RIDE.1993.281927	computer science;data mining;database;information retrieval	DB	-28.351727558910888	12.944724280524008	102312
af2b8b6bb5d15fa76f556e9e5a196bd4191430e8	validation sequence optimization: a theoretical approach	dynamic programming;data mining;validation sequences;computational complexity;heuristic algorithms;sequence optimization;validation;validation operators	The need to validate large amounts of data with the help of the domain expert arises naturally in many data-intensive applications, including a variety of data mining, data stream, and database-related applications. This paper presents a general validation approach that generalizes different expert-driven validation methods developed for specialized validation problems. In particular, we model the validation process as a sequence of validation operators, explore various properties of such sequences, and present theoretical results that provide for better understanding of the validation process. We also address the problem of selecting the best validation sequence among the class of equivalent sequence permutations. We demonstrate that this optimization problem is NP-hard and present two heuristic algorithms for improving validation sequences. (Validation; Validation Operators; Validation Sequences; Sequence Optimization; Computational Complexity; Heuristic Algorithms; Data Mining)	algorithm;analysis of algorithms;computation;computational complexity theory;data mining;data-intensive computing;database;heuristic;mathematical optimization;model-driven architecture;np-hardness;optimization problem;optimizing compiler;subject-matter expert	Gediminas Adomavicius;Alexander Tuzhilin	2007	INFORMS Journal on Computing	10.1287/ijoc.1050.0153	mathematical optimization;computer science;bioinformatics;dynamic programming;data mining;mathematics;computational complexity theory;algorithm	DB	-23.552053262569373	12.893773769939372	102364
de7c8a758cb1d7683ac16e41cb639c932ea8dbf6	abstract interleaving semantics for reconfigurable petri nets		Reconfigurable Petri nets are Petri nets together with rules for the dynamic change of the nets. We employ them for the formal modeling in the context of the Living Place Hamburg, a smart home that is an urban apartment serving as  a laboratory for investigating different areas of ambient intelligence. The interaction of the resident and the smart home is modeled using informal descriptions of scenarios. These scenarios provide the resident's procedures together with the smart home's support. A case study using reconfigurable Petri nets for modeling these scenarios has required extensions of the theory and has clearly shown the need for an interleaving semantics for reconfigurable Petri nets. Scenarios are then given by nets, namely decorated place/transition nets that can be adapted to the evolving subgoals by applying rules that change the nets and hence the behavior of the smart home. Decorated place/transition nets are annotated place/transition nets with additional transition labels that may change when the transition is fired. To obtain such reconfigurable Petri nets  we prove that decorated place/transition nets  give rise to an M-adhesive HLR category. The abstract interleaving semantics we introduce is a graph with nodes that consist of an isomorphism class of the net structure and an isomorphism class of the current  marking. Arcs between these nodes represent computation steps being either a transition firing or a direct transformation.	forward error correction;petri net	Julia Padberg	2012	ECEASST	10.14279/tuj.eceasst.51.775	real-time computing;simulation;computer science;process architecture;petri net	Logic	-24.04357147384448	16.856682391511395	102462
62293752a4f433617758d3b698780b66fd596987	the lorel query language for semistructured data	query language;data model;semistructured data;object oriented database management system;structured documents;database management system;path expressions;language design;object model	language, designed for querying semistructured data. Semistructured data is becoming more and more prevalent, e.g., in structured documents such as HTML and when performing simple integration of data from multiple sources. Traditional data models and query languages are inappropriate, since semistructured data often is irregular: some data is missing, similar concepts are represented using different types, heterogeneous sets are present, or object structure is not fully known. Lorel is a user-friendly language in the SQL/OQL style for querying such data effectively. For wide applicability, the simple object model underlying Lorel can be viewed as an extension of the ODMG data model and the Lorel language as an extension of OQL. The main novelties of the Lorel language are: (i) the extensive use of coercion to relieve the user from the strict typing of OQL, which is inappropriate for semistructured data; and (ii) powerful path expressions, which permit a flexible form of declarative navigational access and are particularly suitable when the details of the structure are not known to the user. Lorel also includes a declarative update language. Lorel is implemented as the query language of the Lore prototype database management system at Stanford. Information about Lore can be found at http://www-db.stanford.edu/lore. In addition to presenting the Lorel language in full, this paper briefly describes the Lore system and query processor. We also briefly discuss a second implementation of Lorel on top of a conventional object-oriented database management system, the O2 system.	data model;database;html;object data management group;object query language;path expression;prototype;sql;usability	Serge Abiteboul;Dallan Quass;Jason McHugh;Jennifer Widom;Janet L. Wiener	1997	International Journal on Digital Libraries	10.1007/s007990050005	object model;data model;computer science;data mining;database;programming language;world wide web;query language;object query language	DB	-30.292298471638926	10.945630924290375	102581
33599ecbe36d3419cab6983ecbba73d1ed59cfde	user extensions to the peterlee relational test vehicle	easy writing;user extensions;user extension;sufficient detail;peterlee relational test vehicle;prtv system;result relation;isbl language;normal form;theoretical level;input relation	In the design of the PRTV system we stressed the importance of the easy writing of extensions to the language, ISBL. We give sufficient details of the ISBL language to provide a framework in which to discuss these extensions. They fall into two classes, those which can be thought of as relations, computed when required, rather than being explicitly stored, and those where a tuple in the result relation is, in general, formed from values taken from more than one tuple in the input relation. We discuss these extensions at a theoretical level, how they are defined and used, and how they are implemented. Finally we discuss the operation of 'glumping' or partitioning, in terms of relations not in first normal form. This provides a general way of lookinq at such operations as subtotalling.		Peter Hitchcock	1976			computer science;theoretical computer science;database;algorithm	NLP	-24.408234746339165	13.793023714214245	102689
58fa6a0fdaf417a02ecba4e6bd31a7500db9d31f	approximate retrieval from multimedia databases using relevance feedback	query refinement;fuzzy logic;query refinement approximate retrieval multimedia databases relevance feedback stored multimedia presentations multimedia presentations crisp relational database object oriented database text attribute retrieval by content fuzzy logic;fuzzy logic multimedia databases relevance feedback relational databases object oriented databases content based retrieval;multimedia databases;relational databases;object oriented databases;multimedia presentation;multimedia database;relevance feedback;content based retrieval;information retrieval multimedia databases feedback image retrieval content based retrieval collaborative work image databases matched filters filtering batteries	In this paper we address the problem of retrieving stored multimedia presentations using relevance feedback. We model multimedia presentations using a crisp relational or object-oriented database, augmented with a text attribute. We also introduce a language for retrieval by content from such databases. The language is based on fuzzy logic. We also introduce a method for query refinement that uses relevance feedback provided by the user.	approximation algorithm;content-based image retrieval;data model;database;fuzzy concept;fuzzy logic;online and offline;query language;refinement (computing);relational model;relational operator;relevance feedback;simulation	Ana Lelescu;Ouri Wolfson;Bo Xu	1999		10.1109/SPIRE.1999.796598	fuzzy logic;database theory;relational database;computer science;data mining;database;information retrieval	DB	-31.4840289061936	8.162994869687155	103070
0048b7e329a9bdf6a7fd9e13b0090e4b09f2e9ff	sql query space and time complexity estimation for multidimensional queries	execution time;query processing;time complexity;complexity analysis;sql queries;sql databases;query space	This paper tackles the issue of estimating database query space and time complexities. Initially, queries without joins are considered and classified into five categories in accordance with complexity (type and number of clauses) in a progressive manner. The storage space and execution time complexity measures for each category are then derived after translating the queries into their algebraic representations and then deriving possible relations that accounts for the different factors (i.e., clauses found in the statement). Joins were then considered and similar complexity expressions were derived. Some experiments were carried out against a database of four tables that were populated using a data generation tool, and involved monitoring the execution time with the aid of a performance monitoring software, so as to give insights into the ‘join’ costs. It is shown that the obtained trends exhibit general agreement with the theoretical expressions for both space and time complexity.	database;experiment;linear algebra;population;run time (program lifecycle phase);sql;select (sql);time complexity	Hassan Artail;Hadi El Amine;Fehmi Sakkal	2008	IJIIDS	10.1504/IJIIDS.2008.021448	time complexity;computer science;query by example;theoretical computer science;data mining;database;spatial query	DB	-28.152829474319912	5.159290824286599	103158
762a52e4121038f737445d7ba4d4f329e53b2ecc	the concept difference for el-terminologies using hypergraphs		Ontologies are used to represent and share knowledge. Numerous ontologies have been developed so far, especially in knowledge intensive areas such as the biomedical domain. As the size of ontologies increases, their continued development and maintenance is becoming more challenging as well. Detecting and representing semantic differences between versions of ontologies is an important task for which automated tool support is needed. In this paper we investigate the logical difference problem using a hypergraph representation of EL-terminologies. We focus solely on the concept difference wrt. a signature. For computing this difference it suffices to check the existence of simulations between hypergraphs whereas previous approaches required a combination of different methods.	cobham's thesis;description logic;horn clause;ontology (information science);performance evaluation;real life;sensor;simulation;web ontology language	Andreas Ecke;Michel Ludwig;Dirk Walther	2013			hypergraph;theoretical computer science;ontology (information science);constraint graph;computer science	AI	-23.557105352295128	8.26684199334805	103194
1f6c059fc32485c0bf95e980f5d05c07e15cb52b	correct schema transformations	schema transformation;abstract data type;data model	We develop a formal basis of correct schema transformations. Schemas are formalized as abstract data types, and correct schema transformations are formalized as information-preserving signature interpretations. Our formalism captures transformations of all schema components, making it possible to transform uniformly constraints and queries along with structures. In addition, our formalism captures schema transformations between diierent data models as easily as those within the same data model. Compared with Hull's notion of relative information capacity, our notion of information preservation captures more schema transformations that are natural, and fewer schema transformations that are unnatural. Our work lays the foundation of a transformational framework of schema manipulations.	abstract data type;channel capacity;data model;semantics (computer science);xml schema	Xiaolei Qian	1996		10.1007/BFb0014146	semi-structured model;logical schema;data model;computer science;conceptual schema;document structure description;star schema;database;programming language;abstract data type;database schema;algorithm	DB	-26.27363902488514	10.287853174594533	103268
c54ed52e5b802e19acf2d3b3e8060dae85b0afe3	problem solving by soaking the concept network	concept graph;scenario reasoning;object oriented;cognition;knowledge representation;soak	Because of the complexity and fuzziness of the real world, it’s hard to build a dense knowledge system and reason in it with traditional methods. But man can deal with such tasks freely. Inspired by cognition and linguistics, a solution is advanced for reasoning dense knowledge in this paper. Objects and concepts are organized in the form of concept graph. Soaking the nodes in the graph until the result is represented in the graph the final graph can be the explanation of the scenario. With the naïve algorithm, monotonic scenario reasoning problem can be solved in dense knowledge environment.	algorithm;cognition;graph (discrete mathematics);knowledge-based systems;naivety;problem solving	Xixu Fu;Hui Wei	2011	Comput. Sci. Inf. Syst.	10.2298/CSIS100915027F	knowledge representation and reasoning;cognition;computer science;artificial intelligence;connectivity;machine learning;data mining;graph;programming language;object-oriented programming;algorithm	AI	-19.16329553592884	6.540792164659669	103277
9fffadb7f605f9d3822742a670f40ae9c68aa13f	aquery: query language for ordered data, optimization techniques, and experiments	relational data model;query language;optimization technique;data model;moving average	An order-dependent query is one whose result (interpreted as a multiset) changes if the order of the input records is changed. In a stock-quotes database, for instance, retrieving all quotes concerning a given stock for a given day does not depend on order, because the collection of quotes does not depend on order. By contrast, finding a stock’s fiveprice moving-average in a trades table gives a result that depends on the order of the table. Query languages based on the relational data model can handle order-dependent queries only through add-ons. SQL:1999, for instance, has a new “window” mechanism which can sort data in limited parts of a query. Add-ons make order-dependent queries difficult to write and to optimize. In this paper we show that order can be a natural property of the underlying data model and algebra. We introduce a new query language and algebra, called AQuery, that supports order from-theground-up. New order-related query transformations arise in this setting. We show by experiment that this framework – language plus optimization techniques – brings orders-ofmagnitude improvement over SQL:1999 systems on many natural order-dependent queries. Work supported in part by U.S. NSF grants IIS-9988636 and N2010-0115586. This work was done while this author was visiting NYU Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large Data Base Endowment. To copy otherwise, or to republish, requires a fee and/or special permission from the Endowment. Proceedings of the 29th VLDB Conference, Berlin, Germany, 2003	add-ons for firefox;data model;database;declarative programming;ibm notes;mathematical optimization;query language;query optimization;relational model;sql;sql:1999;vldb	Alberto Lerner;Dennis Shasha	2003		10.1016/B978-012722442-8/50038-0	online aggregation;sargable;query optimization;.ql;query expansion;web query classification;relational model;ranking;boolean conjunctive query;data manipulation language;data control language;data model;computer science;query by example;theoretical computer science;data mining;database;rdf query language;conjunctive query;moving average;programming language;view;range query;query language;object query language;spatial query	DB	-29.809108341492966	6.694721865859412	103382
ca5d3d4a0702e39a52d02546889ae429d01dea09	a data base system for river basin management	query language;command language;river basin management;operating system;large river;fortran;technical report;data retrieval	A data base system for the analysis of a large river basin was developed that permits users to retrieve and process a set of data elements to obtain the desired information pertaining to any gaging station, segment of a river, or a particular river in the basin. The data base management package is written in FORTRAN IV to operate under the NOS operating system. It uses a simple query language to direct the operations of various component programs. The command language specifications are also presented along with sample results of data retrieval and processing that show the usefulness of the data base system for large river basin management.	command language;data retrieval;database;fortran;nos;operating system;query language	Nguyen Duong;Daryl B. Simons;Ruh-Ming Li	1978			computer science;technical report;data mining;database;data retrieval;query language	DB	-33.09980503988687	15.201150711701294	103400
6020ca1c497e2531ea25be11fb3dc90a72c8901e	negation of fuzzy-linguistic concepts in hierarchical domains	pragmatics;semantics;natural languages;thesauri;fuzzy sets;shape;context	The great part of fuzzy-linguistic research assumes the fuzzy complement as a good candidate for representing the meaning of negation. However, linguistic studies show that both acquisition and use of negation are way more complex than that. After reconsidering a way in which the negation is used in the natural language discourse, this paper focuses on reflecting a use of negation in domains in which fuzzy-linguistic concepts are organised into a hierarchy.	cognitive architecture;fuzzy logic;fuzzy set operations;graph (discrete mathematics);natural language;natural language generation;salt (cryptography)	Grzegorz Popek	2016	2016 Third European Network Intelligence Conference (ENIC)	10.1109/ENIC.2016.033	natural language processing;negation as failure;mathematics;negation introduction;linguistics;algorithm	NLP	-19.63917183434311	6.190567341577679	103450
876c981b479895ecbe6e0e0d1a6e083c8cfcc889	incorporating domain specific knowledge into version space search	multiplexor domain specific knowledge version space search inductive concept learning problem conjunctive learning problems associated g set hypothesis s set g set information knowledge based candidate elimination kbce boole problem boolean functions;inductive concept learning problem;boolean functions;prototypes;associated g set hypothesis;space technology computer science prototypes boolean functions merging inference algorithms machine learning polynomials performance analysis;boolean function;inference mechanisms;knowledge based candidate elimination;set theory;s set;polynomials;domain specific knowledge;domain knowledge;machine learning;version space search;knowledge acquisition;conjunctive learning problems;performance analysis;merging;kbce;concept learning;learning problems;inference algorithms;search problems;space technology;g set information;computer science;boole problem;multiplexor;knowledge based systems;domain specificity;knowledge base	While version spaces are useful in the conceptualization of an inductive concept learning problem, they are seldom used in practice. lkis is because the implementation of version spaces can use an amount of space that is exponential in terms of the amount of data presented, even for simple conjunctive learning problems [3]. In this paper, an approach is developed that uses domain knowledge to infer an associated G set hypothesis for each active hypothesis in the S set. lkis allows the use of G set information while concurrently restricting the space required. A prototype, the Knowledge Based Candidate Elimination (KBCE) algorithm, solves the Boole problem using fewer examples than previous approaches, and is extended to a class of boolean functions that subsumes the multiplexor.	algorithm;concept learning;conceptualization (information science);multiplexer;prototype;time complexity;version space learning	William Sverdlik;Robert G. Reynolds	1993		10.1109/TAI.1993.633960	knowledge base;concept learning;computer science;artificial intelligence;theoretical computer science;knowledge-based systems;machine learning;boolean function;algorithm	AI	-20.326927625405496	10.757440631629171	103600
05db4189b7c0cccfc5f020e64d64240460e7d346	on the expressive power of data integration systems	base relacional dato;local as view;conjunctive queries;data integrity;information retrieval;interrogation base donnee;interrogacion base datos;reducibility;relational database;reductibilidad;expressive power;systeme integration donnee;recherche information;integrity constraints;base donnee relationnelle;global as view;recuperacion informacion;database query;reductibilite	There are basically two approaches for designing a data integration system. In the global-as-view (GAV) approach, one maps the concepts in the global schema to views over the sources, whereas in the local-as-view (LAV) approach, one maps the sources into views over the global schema. The goal of this paper is to relate the two approaches with respect to their expressive power. The analysis is carried out in a relational database setting, where both the queries on the global schema, and the views in the mapping are conjunctive queries. We introduce the notion of query-preserving transformation, and query-reducibility between data integration systems, and we show that, when no integrity constraints are allowed in global schema, the LAV and the GAV approaches are incomparable. We then consider the addition of integrity constraints in the global schema, and present techniques for query-preserving transformations in both directions. Finally, we show that our results imply that we can always transform any system following the GLAV approach (a generalization of both LAV and GAV) into a query-preserving GAV	conjunctive query;data integrity;expressive power (computer science);map;relational database	Andrea Calì;Diego Calvanese;Giuseppe De Giacomo;Maurizio Lenzerini	2002		10.1007/3-540-45816-6_33	computer science;star schema;data integrity;data mining;database;database schema;algorithm	DB	-28.343520409088317	10.01311055023418	103706
3a915aa3749019cd21e45e001018b52d999eaa32	uma linguagem para formalização de discursos com base em ontologias	programacao logica;arquitetura da informacao;linguagens formais;ontologia	This research proposes the information architecture of a textual formal language to represent and reason about ontological entities based on foundational ontologies. Through metamodeling, the language is able to deal with heterogeneous ontologies that can be described as instances of one or more foundational ontology. The language provides classic and modal inference mechanisms supported by proof notions based on the (Modal) Logic Programming paradigm. The modalities introduced by the modal framework allow a wide range of interpretations, including multi-agent systems. A systematization of the endurant fragment of the Unified Foundational Ontology (UFO) is produced in order to compose part of the theoretical framework underlying the proposal, and to serve as an example instantiating the developed framework. As complementary results we highlight: a systematization of an extended set of rules for conceptual modeling and a detailed glossary of terms and concepts of UFO-A; functional prototypes implementing the developed systems; translations of the theories described as instances of the framework to diagramatic representations, as extensions of the OntoUML visual language; and discussions regarding the integration of Information Architecture, Conceptual Modeling and Logic Programming within Applied Social Science.		Lauro César Araújo	2015				AI	-21.84388346242868	8.957496360045702	103711
7fe758bf6efee042a6c921fe50f530c91c76c6e6	managing multiple real and simulation business scenarios by means of a multiversion data warehouse	data warehouse	This paper addresses problems of the evolution of data warehouse schema and dimensions. In order to handle the evolution, we apply a multiversion data warehouse (MVDW). In this paper we discuss real world cases illustrating a DW evolution and show how to apply the MVDW in order to handle the cases.	database schema;dreamwidth;evolution;experiment;requirement;simulation;user requirements document	Bartosz Bebel;Zbyszko Królikowski;Robert Wrembel	2006			data mining;database;dimensional modeling;computer science;schema (psychology);data warehouse	DB	-33.33920274883255	11.065043491443225	103858
1902ff4c34c48e1d85aebee8d12c3cd3718172a0	nested intervals tree encoding in sql	hierarchical queries algorithmically;ancestor path;nested sets;nested intervals tree;hierarchy relation;hierarchy reorganization problem;nested intervals	Nested Intervals generalize Nested Sets. They are immune to hierarchy reorganization problem. They allow answering ancestor path hierarchical queries algorithmically - without accessing the stored hierarchy relation.	algorithm;hierarchical and recursive queries in sql	Vadim Tropashko	2005	SIGMOD Record	10.1145/1083784.1083793	nested set model;nested loop join;data mining;database	DB	-28.585362258071672	6.979509341757019	104028
69d88bcf68a89697a3df242ad9a14200d0ad397d	free-link topology navigation on statistical table objects: metadata schema and user interface	user interface;multiple inheritance;navigation system	A table object, defined and described in XML, is treated as the basic unit in a free-link topology that enables multiple inheritances for each node. In this paper, a novel table navigation system called TableHunter is introduced. Each table object is presented in the TableHunter as a node of a map structure, and supports a context+focus view.	user interface;xml	Xiangming Mu;Gary Marchionini	2003			computer science;theoretical computer science;database;single table inheritance;world wide web	DB	-32.26003874525411	8.311638180489009	104047
14eaada5085d3d85f44154cf0f26eb728b85d287	fuzzy type - ahead keyword search in rdf data		Keyword search is the process of identifying relevant data the matches with keyword. If it produces more than one document as a search results, there will be ranking process. This paper explain a novel keyword search method on RDF data model, and use fuzzy technique extract results according to the query we passed. Proposed model can improves the storage and querying characteristics of the underlying RDF store. In addition to the above, the problem of fuzzy search on RDF model while typing key words. The results have supported the objective of the work.	approximate string matching;data model;resource description framework;search algorithm;triplestore	Selvani Deepthi Kavila;Ravi Ravva;Rajesh Bandaru	2013		10.1007/978-3-319-02931-3_9	data mining;database;information retrieval	DB	-31.74268048680975	4.814411739394285	104118
2c98a69265e1f62d20daee107e128d7fc80bb10e	sparql update for materialized triple stores under dl-lite_rdfs entailment		Updates in RDF stores have recently been standardised in the SPARQL 1.1 Update specification. However, computing answers entailed by ontologies in triple stores is usually treated orthogonally to updates. Even W3C’s SPARQL 1.1 Update language and SPARQL 1.1 Entailment Regimes specifications explicitly exclude a standard behaviour for entailment regimes other than simple entailment in the context of updates. In this paper, we take a first step to close this gap. We define a fragment of SPARQL basic graph patterns corresponding to (the RDFS fragment of) DL-Lite and the corresponding SPARQL update language, dealing with updates both of ABox and of TBox statements. We discuss possible semantics along with potential strategies for implementing them. Particularly, we treat materialised RDF stores, which store all entailed triples explicitly, and preservation of materialisation upon ABox and TBox updates.	abox;adobe flash lite;case preservation;informatics;linked data;ontology (information science);rdf schema;resource description framework;sparql;tbox;triplestore;vocabulary	Albin Ahmeti;Diego Calvanese;Axel Polleres	2014			database;sparql;monad (category theory);computer science;logical consequence	Web+IR	-24.331389974261008	8.759751291271682	104462
e84381b2a50b165cc50b10e5f3537fa829b817ce	modeling trajectories: a spatio-temporal data type approach	databases;spatio temporal database;moving objects databases;spatio temporal databases;moving object database;abstract data types;algebraic spatio temporal trajectory data type approach;abstract data type;data type;data mining;conceptual framework;abstract data type modeling;human behavior;semantic model;spatio temporal data;trajectory;design and implementation;data structures geography spatiotemporal phenomena expert systems information retrieval humans geographic information systems database systems spatial databases history;data structures;time geography abstract data type modeling spatio temporal trajectory modeling spatio temporal databases moving objects databases temporal gis;trajectory data interpretation;sui generis data type;trajectory analysis;cities and towns;trajectory retrieval;temporal databases;object trajectory representation;spatio temporal trajectory modeling;visual databases abstract data types temporal databases;trajectory modeling;time geography;data structure;temporal gis;spatio temporal database trajectory modeling trajectory retrieval trajectory analysis human behavior time geography conceptual framework trajectory data interpretation semantic model sui generis data type algebraic spatio temporal trajectory data type approach object trajectory representation abstract data type data structure;data models;visual databases	Retrieving and analyzing trajectories favor the study of human behaviors in space and time. Although Time Geography has long proposed a conceptual framework for the interpretation of trajectory data, there is still a need for database and semantic models that support representation and manipulation. In fact, the concept of trajectory is rarely addressed as a sui generis data type that can be embedded in a database structure. This paper introduces an algebraic Spatio-Temporal Trajectory data type (STT) for the representation of object trajectory. The STT is an abstract data type endowed with a set of operations designed as a way to cover the syntax and semantics of a given trajectory. It is considered as a data structure that can be used for the design and implementation of spatio-temporal databases.	abstract data type;data structure;embedded system;sonic user interface;temporal database;temporal logic;time geography	Ali Frihida;Donia Zheni;Henda Hajjami Ben Ghézala;Christophe Claramunt	2009	2009 20th International Workshop on Database and Expert Systems Application	10.1109/DEXA.2009.70	computer vision;data structure;computer science;data mining;database;programming language;abstract data type	DB	-29.321919620089925	8.540709971108791	104511
147ecd1062167bcdc54f3781daed6ae59443e314	a category theoretic method of temporal data model	temporal data;solid theoretical foundations category theoretic method temporal database technology temporal database system development temporal data models unified concepts systematic formal descriptions temporal database systems development ftdm formal temporal data model data model reuse software reuse category theory convenient formalization theory framework;database;data model;software reuse data model temporal data category theory database;category theory;data models database systems abstracts software finite element analysis data structures;software reusability;temporal databases;temporal databases software reusability;software reuse	Temporal data model is the mainline of trends in temporal database technology, and it is the core and basis of temporal database system development. Existing temporal data models focus on specific implementation among different abstract levels, the lack of unified concepts and systematic formal descriptions, especially the deficiency of solid theoretical foundation for universality, flexibility and extensibility etc., leads to some difficulties to meet actual needs of the temporal database systems development. This paper presented FTDM (Formal Temporal Data Model) at higher abstract level in accordance with the status of quo of temporal data models, proposed the definition of data model reuse by the thinking of software reuse, and further made temporal data models family Mi based on FTDM. By the methods of category theory this paper also demonstrated some categorical properties of Mi, and analyzed the inherent relationship between temporal data models in Mi, which provided an efficient and convenient formalization theory framework for studying temporal data model, also provided solid theoretical foundations for development and design of temporal database system.	angular defect;category theory;code reuse;data model;extensibility;software development process;temporal database;universality probability	Decheng Miao;Jianqing Xi;Jindian Su	2013	2013 Fourth International Conference on Emerging Intelligent Data and Web Technologies	10.1109/EIDWT.2013.24	data modeling;database theory;computer science;theoretical computer science;database model;data mining;database;database design	DB	-31.988948052451715	12.243489324344338	104573
6ff6bf45a30ae0083f6e1f0a1ffe29127f21049c	an extended relational document retrieval model	engineering;working paper university of michigan graduate school of business administration division of research;query language;base donnee;associative relation;relation associative;systeme documentaire;online searching;database management systems;sql;information retrieval;social sciences;database;base dato;user needs information;lenguaje interrogacion;cnf;relacion asociativa;humanities;mathematical models;recherche information;information and library science;sistema recuperacion documental;document retrieval system;information processing;search strategies;langage interrogation;document retrieval;recuperacion informacion;no 394;computer science;systems development;subject index terms	Relational Data Base Management Systems offer a commercially available tool with which to build effective document retrieval systems. The full potential of the relational model for supporting the kind of ad hoc inquiry characteristic of document retrieval has only recently been explored. In addition, commercially available relational DBMS’s also provide effective tools for managing document data bases by providing facilities for, inter alia, concurrency control, data migration and reorganization routines, authorization mechanisms, enforcement of integrity constraints, dynamic data definition, etc. This article will present a relational logical model to support a sophisticated document retrieval system in which flexible forms of inferential and associative searching can be performed. Examples of ad hoc inquiry will be presented in SQL. Several problems of particular importance to document retrieval will be discussed, including the importance of Conjunctive Normal Form in query formulation, unique aspects of document retrieval storage and processing overhead, and techniques for reducing the size of storage without severely impacting retrieval effectiveness.	authorization;concurrency (computer science);concurrency control;conjunctive normal form;data definition language;data integrity;document retrieval;dynamic data;hoc (programming language);inferential theory of learning;object-relational database;overhead (computing);relational database management system;relational model;sql	David C. Blair	1988	Inf. Process. Manage.	10.1016/0306-4573(88)90101-X	well-formed document;document retrieval;conjunctive normal form;sql;document clustering;information processing;computer science;machine learning;mathematical model;data mining;database;world wide web;information retrieval;query language;design document listing	Web+IR	-28.031151442045253	7.031830732019142	104794
ab0b836c6928e3eacd49e044d43af22d29f66bd7	selection of materialized views: a cost-based approach.	workload;optimisation;mise a jour;optimizacion;reutilizacion;interrogation base donnee;interrogacion base datos;reuse;materialized view;actualizacion;charge travail;optimization;carga trabajo;database query;updating;reutilisation;vue materialisee	ÿRecently, multi-query optimization techniques have been considered as beneficial in view selection setting. The main interest of such techniques relies in detecting common sub expressions between the different queries of workload. This feature can be exploited for sharing updates and space storage. However, due to the reuse a query change may entail an important reorganization of the multi query graph. In this paper, we present an approach that is based on multi-query optimization for view selection and that attempts to reduce the drawbacks resulting from these techniques. Finally, we present a performance study using workloads consisting of queries over the schema of the TPC-H benchmark. This study shows that our view selection provides significant benefits over the other approaches.	benchmark (computing);ibm tivoli storage productivity center;materialized view;mathematical optimization;query optimization;sensor	Xavier Baril;Zohra Bellahsene	2003		10.1007/3-540-45017-3_44	materialized view;computer science;data mining;reuse;database	DB	-26.458749475852127	4.224051274029001	104877
23991c517cc13bcce1dd714830578a6e03646f33	join queries in p2p dht systems	p2p	P2P systems based on Distributed Hash Tables are certainly an important category of massively distributed systems. However, it is necessary to improve the query language to use these systems in different applicative contexts. This article presents strategies for evaluating natural and equality joins using the indexes available in the system to reduce the number of peers contacted in the query evaluation process. Several works had included joins. However, their solutions add hypothesis about the P2P system, the objects shared and in some cases, the cost to make the data distribution can affect the performance of the query evaluation process. The ideas presented in this article were evaluated in a theoretical and practical way, using the GRID5000 [1].	algorithm;applicative programming language;distributed computing;distributed hash table;identifier;join (sql);mathematical optimization;peer-to-peer;personal identification number;query language;query optimization	Carlos Prada;María-Del-Pilar Villamil;Claudia Roncancio	2008			hash table;distributed computing;computer science;query language;joins	DB	-27.07081811708868	7.089904336533824	104973
52fbe8658d4f7dcb9252cead575c9e333e1a2735	a method for automatic rule derivation to support semantic query optimization	regle inference;database system;optimisation;base donnee;transformation heuristic;optimizacion;learning;query processing;interrogation base donnee;database;interrogacion base datos;base dato;semantics;data processing;integrite;query optimization;intelligence artificielle;integridad;semantica;semantique;aprendizaje;inference rule;apprentissage;integrity;integrity constraint;integrity constraints;artificial intelligence;optimization;inteligencia artificial;systeme gestion base donnee;semantic query optimization;sistema gestion base datos;database management system;database query;regla inferencia	The use of inference rules to support intelligent data processing is an increasingly important tool in many areas of computer science. In database systems, rules are used in semantic query optimization as a method for reducing query processing costs. The savings is dependent on the ability of experts to supply a set of useful rules and the ability of the optimizer to quickly find the appropriate transformations generated by these rules. Unfortunately, the most useful rules are not always those that would or could be specified by an expert. This paper describes the architecture of a system having two interrelated components: a combined conventional/semantic query optimizer, and an automatic rule deriver. Our automatic rule derivation method uses intermediate results from the optimization process to direct the search for learning new rules. Unlike a system employing only user-specified rules, a system with an automatic capability can derive rules that may be true only in the current state of the database and can modify the rule set to reflect changes in the database and its usage pattern. This system has been implemented as an extension of the EXODUS conventional query optimizer generator. We describe the implementation, and show how semantic query optimization is an extension of conventional optimization in this context.	algorithm;computer science;exodus;mathematical optimization;newton's method;query optimization;semantic query	Michael Siegel;Edward Sciore;Sharon C. Salveter	1992	ACM Trans. Database Syst.	10.1145/146931.146932	query optimization;data processing;computer science;machine learning;data integrity;data mining;database;semantics;view	DB	-25.39346200694998	7.066920211088735	105179
fa35188e56cad2128aab5ddd866f4c3508b6be92	a preliminary system for the design of dbtg data structures (abstract).	automatic programming;non procedural languages;translation;network model of data bases;data base task group;data base design;data structure	"""The functional approach to data base design is introduced. In this approach the goal of design is to derive a data structure which is capable of supporting a set of anticipated queries rather than a structure which """"models the business"""" in some other way. An operational computer program is described which utilizes the functional approach to design data structures conforming to the Data Base Task Group specifications. The automatic programming technology utilized by this program, although typically used to generate procedure, is here used to generate declaratives."""		Rob Gerritsen	1975		10.1145/500080.500104	translation;data structure;computer science;data mining;database;programming language	SE	-27.98049777340198	17.09724777582829	105236
3a29c72bb67d603f8a2e88c7475bb04e81028999	learning implicit and explicit control in model transformations by example		We propose an evolutionary approach that, in addition to learn model transformation rules from examples, allows to capture implicit and explicit control over the transformation rules. The derivation of both transformation and control knowledge is performed through a heuristic search, i.e., a genetic programming algorithm, guided by the conformance with examples of past transformations supplied as pairs of source and target models. Our approach is evaluated on four model transformation problems that require non-trivial control. The obtained results are convincing for three of the four studied problems.		Islem Baki;Houari A. Sahraoui;Quentin Cobbaert;Philippe Masson;Martin Faunes	2014		10.1007/978-3-319-11653-2_39	explicit and implicit methods	Logic	-19.65819289040215	16.10407468638796	105248
92ee9c659f5ee3b12e4d8272093ca2c21d60aa01	hermes: a privacy-preserving approximate search framework for big data		We propose a sampling-based framework for privacy-preserving approximate data search in the context of big data. The framework is designed to bridge multi-target query needs from users and the data platform, including required query accuracy, timeliness, and query privacy constraints. A novel privacy metric,  $(\varepsilon,\delta)$ -approximation, is presented to uniformly measure accuracy, efficiency and privacy breach risk. Based on this, we employ bootstrapping to efficiently produce approximate results that meet the preset query requirements. Moreover, we propose a quick response mechanism to deal with homogeneous queries, and discuss the reusage of results when appending data. Theoretical analyses and experimental results demonstrate that the framework is capable of effectively fulfilling multi-target query requirements with high efficiency and accuracy.	approximation algorithm;big data;bootstrapping (compilers);privacy law;requirement;sampling (signal processing)	Zhigang Zhou;Hongli Zhang;Shang Li;Xiaojiang Du	2018	IEEE Access	10.1109/ACCESS.2017.2788013	data mining;computer science;big data;sampling (statistics);distributed computing;homogeneous;bootstrapping	DB	-24.207910334302483	4.852492469082578	105563
6f080ceb0e588ddb33897ad02cb66183eaaef79f	facilitate effective decision-making by warehousing reduced data: is it feasible?	reduction operators;theorie de l information;experimental assessment;keywords conceptual modeling;multidimensional design;data reduction;recherche d information	The authors' aim is to provide a solution for multidimensional data warehouse's reduction based on analysts' needs which will specify aggregated schema applicable over a period of time as well as retain only useful data for decision support. Firstly, they describe a conceptual modeling for multidimensional data warehouse. A multidimensional data warehouse's schema is composed of a set of states. Each state is defined as a star schema composed of one fact and its related dimensions. The derivation between states is carried out through combination of reduction operators. Secondly, they present a meta-model which allows managing different states of multidimensional data warehouse. The definition of reduced and unreduced multidimensional data warehouse schema can be carried out by instantiating the meta-model. Finally, they describe their experimental assessments and discuss their results. Evaluating their solution implies executing different queries in various contexts: unreduced single fact table, unreduced relational star schema, reduced star schema and reduced snowflake schema. The authors show that queries are more efficiently calculated within a reduced star schema.		Faten Atigui;Franck Ravat;Jiefu Song;Olivier Teste;Gilles Zurfluh	2015	IJDSST	10.4018/ijdsst.2015070103	data reduction;information schema;dimensional modeling;computer science;conceptual schema;data warehouse;star schema;data mining;database;algorithm	DB	-31.97036919353046	5.579102613847928	105686
984cc468c498ff446e275e754254386be4b9bb53	abductive diagnosis of complex active systems with compiled knowledge				Gianfranco Lamperti;Marina Zanella;Xiangfu Zhao	2018			theoretical computer science;finite-state machine;knowledge compilation;computer science	AI	-22.031916125251044	16.965778726497124	105853
66878259b10d2084c8d9aa3121ca87ba7eb63ba6	combining code with specifications. how to document and verify frameworks.				Anna Mikajlova	2000	L'OBJET		computer science;data mining;database;programming language	OS	-31.49949832639951	7.23626746646	106209
17174c389e4405b982cb6a24f696a09e6279b207	abduction guided query relaxation	query relaxation;abductive logic programming	We investigate how to improve cooperative communication between agents by representing knowledge bases as logic programs extended with abduction. In this proposal, agents try to provide explanations whenever they fail to answer a question. Query Relaxation is then employed to search for answers related to the query, characterizing cooperative behavior. Our contributions bring insightful improvements to relaxation attempts and the quality of related answers. We introduce rational explanations and use them to efficiently guide the search for related answers in a relaxation tree.	abductive reasoning;linear programming relaxation	Samy Sá;João Alcântara	2011			query optimization;computer science;artificial intelligence;data mining;algorithm;abductive logic programming	AI	-20.85193706913006	7.59288061249384	106263
6a00c48d13a14612a005ec6ab3c03136b52eab6d	the interaction between functional dependencies and template dependencies	database model;dbtg;data independence;data definition language;functional dependency;inference rule;data dependence;data storage description language;database management system;large classes;codasyl	A large class of dependencies, called template dependencies, was introduced in Sadri and Ullman [1979], and a complete set of inference rules (axioms) was given for it. In this paper, we investigate the interaction between template dependencies and functional dependencies. We develop techniques for partially deciding which template and functional dependencies are logically implied by a set of template and functional dependencies. Since template dependencies include all dependencies, besides functional dependencies, known to us (such as multivalued, join, mutual, generalized mutual, and subset dependencies, and their embedded versions), the results of this paper enable us to apply the techniques from Sadri and Ullman [1979] to any set of data dependencies.	data dependency;embedded system;functional dependency;sethi–ullman algorithm	Fereidoon Sadri;Jeffrey D. Ullman	1980		10.1145/582250.582258	data independence;data definition language;armstrong's axioms;dependency theory;acyclic dependencies principle;computer science;database model;layer;data mining;database;functional dependency;programming language;rule of inference;dependence analysis	DB	-28.045727274016212	10.506613389632651	106346
e86a1f432e32d7956ac29748c4a907f35a22ac0d	a database model of debugging	debugging;mise au point programme;programming environment;user interface;relational database;base donnee relationnelle;relational database system;interface utilisateur;environnement programmation	Debugging a program can be viewed as performing queries and updates on a database that contains program source information as well as the state of the executing program. This approach integrates the facilities of a traditional debugger into a programming environment by providing access to runtime information through normal database query operations. We are building a programming environment in which all program information is stored in a relational database system. This system will include capabilities to provide the programmer a simple yet powerful mechanism for describing debugging requests.	database model;debugging	Michael L. Powell;Mark A. Linton	1983	Journal of Systems and Software	10.1016/0164-1212(83)90015-8	database theory;relational database management system;intelligent database;entity–relationship model;database tuning;relational database;computer science;theoretical computer science;operating system;database model;database;algorithmic program debugging;programming language;user interface;debugging;view;database schema;alias;object-relational impedance mismatch;database design;component-oriented database	Logic	-31.044690957410882	9.984987651478969	106365
a362d8375906e33f48630ea60dc8e1843756c193	a relational-database machine organization for parallel pipelined query execution			relational database	Masahito Hirakawa;Kazuyuki Tsuda;Minoru Tanaka;Tadao Ichikawa	1986				DB	-31.308075207925533	7.4721991050588965	106413
7ebddd9214830376b85541347a2b35c083eafaac	implementation of object-oriented association relationships in relational databases	electrical capacitance tomography;object oriented conceptual models;inverse traversal;database system;data integrity;normalisation;hip;abstract data types;conceptual model;data engineering;relational database;power engineering and energy;relational databases electrical capacitance tomography hip object oriented databases computer science data engineering power engineering and energy australia object oriented modeling power system modeling;arrays;lists;object oriented;collection types;record insertion;relational database system;inverse traversal object oriented association relationships relational databases object relational technology object oriented conceptual models collection types sets lists arrays bags relational tables implementation strategies constraints data integrity normalisation record insertion record deletion;relational tables;relational databases;object oriented databases;object relational technology;computer science;sets;implementation strategies;bags;power system modeling;object oriented association relationships;object relational;object oriented modeling;constraints;record deletion;australia	With the increasing popularity of object-relational technology, it is becoming important to have a methodology which allows designers to exploit the great modelling power of object-oriented conceptual models (OOCMs) and yet which still facilitates implementation on relational database systems. This paper presents a practical solution for the implementation of different types of object-oriented association relationships, which include a wide range of collection types (i.e. sets, lists, arrays and bags), into relational database tables. The implementation strategies raise a number of constraints relating to data integrity in the database system. These constraints are associated with (i) normalisation of the resulting relational tables, and (ii) data integrity after insertion/deletion. In order to ensure completeness of the strategies, some techniques for the implementation of inverse traversal in association relationships are also provided. An example is used throughout this paper to demonstrate and evaluate the proposed method.	relational database management system	J. Wenny Rahayu;Elizabeth Chang;Tharam S. Dillon	1998		10.1109/IDEAS.1998.694385	relational model;information engineering;relational database;computer science;theoretical computer science;database model;data mining;database;programming language;database design	DB	-29.044606608624985	7.705918954981827	106426
a64931acb2de39d9509a44644429ad32cc21cdf7	a system prototype for the evaluation of queries on materialized views.	materialized views			Stefano Basta;Sergio Flesca;Sergio Greco;Ester Zumpano	1999			database;materialized view;computer science	DB	-31.12865058852738	7.946735169165528	106573
81f7f8592202578092b77aed7f47d98efd0dd57a	ixupt: indexing xml using path templates	indexation	The XML format has become the standard for data exchange because it is self-describing and it stores not only information but also the relationships between data. Therefore it is used in very different areas. To find the right information in an XML file, we need to have a fast and an effective access to data. Similar to relational databases, we can create an index in order to speed up the querying for the information. There are several ways of indexing XML data but previous research showed that one of the most effective approaches is to index root-to-leaf paths in the input file. So we took the inspiration from existing path-based indexing concepts, enhanced those ideas, and created a new native XML indexing method derived from the combination of existing approaches in order to improve the evaluation time of regular path expressions in XPath queries.	apevia;database index;hard disk drive;path expression;prototype;relational database;self-documenting code;tree structure;xml;xpath	Tomás Bartos;Ján Kasarda	2010			xml catalog;xml validation;binary xml;xml encryption;simple api for xml;xml schema;streaming xml;computer science;document structure description;xml framework;data mining;xml database;xml schema;database;xml signature;xml schema editor;information retrieval;efficient xml interchange	DB	-30.77071681662546	4.7540947326179275	106587
4ea3a2abfc63474d60741ff0433de9f95805fce4	fully persistent arrays for efficient incremental updates and voluminous reads	functional language;data structure	 The array update problem in a purely functional language is the following: oncean array is updated, both the original array and the newly updated one mustbe preserved to maintain referential transparency. We devise a very simple, fullypersistent data structure to tackle this problem such thatffl each incremental update costs O(1) worst--case time,ffl a voluminous sequence of r reads cost in total O(r) amortized time, andffl the data structure use O(n + u) space,where n is the size ... 		Tyng-Ruey Chuang	1992		10.1007/3-540-55253-7_7	data structure;computer science;data mining;database;programming language;functional programming	Theory	-29.19095790566563	4.901777150841429	106857
2eab6e5dafa72fb9e15765f0a0d31518588d35f7	evaluating a formal kbs specification language	libraries;ml 2;commonkads kbs development method;knowledge based systems formal languages performance evaluation testing frequency logic terminology usability redundancy libraries;information model;knowledge based system;formal specification;commonkads kbs development method formal kbs specification language knowledge based systems ml sup 2 formal language;performance evaluation;formal model;logic;formal specification language;formal languages;testing;specification language;article letter to editor;knowledge modelling;real world application;redundancy;modelling language;specification languages;knowledge acquisition;evaluation criteria;formal kbs specification language;terminology;knowledge representation;frequency;usability;article in monograph or in proceedings;knowledge modeling;knowledge based systems;formal language;knowledge engineering;knowledge representation knowledge acquisition specification languages knowledge based systems	In recent years, the knowledge engineering community has begun to explore formal specification languagesas a tool in the development of knowledgebased systems. These formal knowledge modelling languages have a number of advantages over informal languages, such as their precise meaning and the possibility to derive properties through formal proofs. However, these formal languages also suffer from problems which limit their practical usefulness: they are often not expressive enough to deal with real world applications, formal models are complex and hard to read, and constructing a formal model is a difficult, error prone and expensive process. The goal of the study presented in this paper is to investigate the usability of one such formal KBS modelling language, called (ML) . (ML) is strongly based on the structure of the knowledge-models used in the KADS KBS development method. We first designed a set of evaluation criteria. We then applied (ML) in two case-studies and scored the language on our evaluation criteria. (ML) scored well on most of our criteria. This leads us to conjecture that the close correspondencebetween the informal KADS models and the formal (ML) models avoids some of the problems that traditionally plague formal specification languages.	capacitor plague;cognitive dimensions of notations;formal language;formal proof;formal specification;knowledge engineering;knowledge-based systems;lazy evaluation;modeling language;specification language;usability	Frank van Harmelen;Manfred Aben;Fidel Ruiz;Joke van de Plassche	1996	IEEE Expert	10.1109/64.482959	natural language processing;formal language;computer science;artificial intelligence;knowledge-based systems;data mining;programming language	SE	-22.481550821282976	10.264043322565996	106933
c760db8d4c88a5eec7473356ff8c0d653452ec8e	learning discriminatory and descriptive rules by an inductive logic programming system	inductive logic programming		admissible numbering;inductive logic programming	Maziar Palhang;Arcot Sowmya	1999			concurrent constraint logic programming;inductive bias;statistical relational learning;horn clause;computer science;artificial intelligence;machine learning;functional logic programming;programming paradigm;inductive programming;prolog;logic programming;algorithm	ML	-19.128175004800102	11.644093174520533	107032
22b55200dd1bbbcb909bf0fe2587f9ba93d91c33	a gentle introduction to xcerpt, a rule-based query and transformation language for xml		This articles introduces into Xcerpt, a rule-based query and transformation language for XML. First, the design principles of Xcerpt are given. Then, the essential construct of Xcerpt are explained and illustrated on examples: ”query terms”, i.e. patterns using which Xcerpt queries are posed, ”construct terms”, i.e. pattern re-assembling the data selected in a query term into a new data item, and ”construct-query rule” linking queries with construct terms. Then, Xcerpt and XQuery are compared on examples and the advantages of Xcerpt are discussed. Finally, an outlook into Xcerpt’s declarative and procedural semantics as well as into Xcerpt’s features currently developed are given.	data item;logic programming;microsoft outlook for mac;transformation language;xml;xquery	Sebastian Schaffert;François Bry	2002			xml;data mining;rule-based system;database;transformation language;computer science	DB	-31.66312057065562	8.2535153887968	107576
0035c59e56a27f4d17642df2e561b489b049179a	open oodb: architecture and query processing overview	query processing	This paper covers two main aspects of the Open Object-Oriented Database (Open OODB) system. First, it presents an overview of its architecture which consists of an extensible collection of services including address space management, translation, communications, name and type management, persistence, distribution, transaction management, index management, query processing, and versioning. Second, it describes in more detail the query processing service of the Open OODB system. This service includes an SQL-based object query language called OQL[C++], an extensible object query optimizer, and a data model-independent query execution engine. We conclude with a discussion of the development status and future research plans of the Open OODB project.		José A. Blakeley	1993		10.1007/978-3-642-57939-4_13	sargable;query optimization;query expansion;query by example;database;rdf query language;web search query;information retrieval;query language	DB	-31.580761054240536	6.715231201421077	107837
987916e522fe4db52f69c76ad533ed65d6782f6d	a formal investigation of mapping language for terminological knowledge	point of view	The need to represent mappings between different ontologies has been recognized as a result of the fact that different ontologies may partially overlap, or even represent the same domain from different points of view. Unlike ontology languages, work on languages to represent ontology mappings has not yet reached a state where a common understanding of the basic principles exists. In this paper we propose a formal comparison of existing mapping languages by translating them into distributed first order logic. This allows us to analyze underlying assumptions and differences in the interpretation of ontology mappings.	first-order logic;ontology (information science);programming languages	Luciano Serafini;Heiner Stuckenschmidt;Holger Wache	2005			natural language processing;computer science;ontology;theoretical computer science;data mining;mathematics;ontology language;process ontology	AI	-22.56419782579516	9.440739121147512	107896
dcaf163515973d1987ecfa0ff5771cc9e8cef38d	representing constraint business rules extracted from legacy systems	constraint satisfaction;langage brl;power method;satisfaction contrainte;programa aplicacion;application program;documentacion;programme application;constraint handling;satisfaccion restriccion;information system;legacy system;traitement contrainte;business rules;systeme information;regle gestion contrainte;documentation;sistema informacion	Constraints are a class of business rules that most information systems implement. However, due to staff turnover and lack of documentation, precise knowledge of what constraints are enforced by a system is often not available. This can seriously hinder an organisation's ability to understand the data stored in its systems, and to evolve the systems to implement new business policies. To help the situation, researchers have considered how to extract constraints out of legacy systems. While some powerful methods have been proposed for identifying constraints in application programs, little has been done so far to help users to comprehend the recovered constraints. To step up research in this direction, we study in this paper how the recovered constraints should be represented, so that they can be analysed, processed and then presented to the user in a comprehensible manner. We introduce a representation language that offers a balance between expressiveness, comprehensibility and reasoning power in handling the recovered constraints.	legacy system	Gaihua Fu;Jianhua Shao;Suzanne M. Embury;W. Alex Gray	2002		10.1007/3-540-46146-9_46	simulation;constraint satisfaction;documentation;computer science;artificial intelligence;operating system;constraints accounting;database;legacy system;information system;algorithm	NLP	-33.152278738127784	15.849502662488966	107971
cfd48b6c8190145794f9336a872303e47a7f315a	query optimization in object oriented databases based on signature file hierarchy and sd-tree		Direct query on objects in object-oriented databases costs a lot of data storage during query processing and time to execute query on real data systems. Recently, there are many researches focusing on resolving that problem by indexing on single classes, class hierarchies or nested objects hierarchies. In this paper, we propose a new indexing approach. This approach is based on the technique of using signature files and SD-Trees where signature files are in hierarchical organization to quickly filter irrelevant data and each signature file is stored in the similar structure with SD-Tree to fasten signatures scanning. This technique helps reduce significantly searching space, hence improves significantly time complexity of query.	program optimization;query optimization;secure digital	Tran Minh Bao;Truong Cong Tuan	2015		10.1007/978-3-319-29236-6_30	sargable;query optimization;data mining;database;programming language;query language	DB	-28.94835830808388	4.8690271237225184	108207
df2b2a5469483271654cc58f7f7d6ee31369180e	program perspectives: a relational representation of measurement data	relational data;programming language;relational data profile perspective program measurement;profile;perspective;fortran;program measurement;sampling methods computer languages application software programming profession software systems measurement high level languages organizing computer science digital arithmetic	"""This paper concems the representation and processing of data resulting from the measurement of programs written in high-level programming languages. The value of various """"perspectives"""" of measurement results, especially in languages having many levels of activity, is illustrated. A representation of measurement data based on ideas from relational data bases is presented, in which perspectives can be represented as hierarchically-organized permuted projections of measurement data. Applications described include measurement of Snobol4 and Fortran programs."""	database;fortran;high- and low-level;high-level programming language;snobol	G. David Ripley	1977	IEEE Transactions on Software Engineering	10.1109/TSE.1977.231147	perspective;relational database;computer science;theoretical computer science;third-generation programming language;software engineering;fifth-generation programming language;programming language;second-generation programming language;comparison of multi-paradigm programming languages	Visualization	-29.20155183014478	14.577715899567904	108378
c718a218f93bc94405982fd6d0a91c6f6128d7e6	concept logics with function symbols	kunstliche intelligenz;knowledge representation;domain specificity	Constrained resolution allows t.he incorporat.ion of domain specific problem solving methods into the cla.ssical resolution principle. Firstly, the domain specific knowledge is represen ted by a restriction theory. One then starts with formulas containing so-called rest.ricted quantifiers , written as V X :R F and 3X :R F, where X is a set of variables and the restriction R is used to encode domain specific knowledge by fil tering out some assignments to the variables in X. Formulas with restricted quantifiers can be translated into clauses which consist of a (classical) clause together with a restriction. In order to attain a refutation proceclure which is based on such clauses one needs algorithms to decide satisfiability and validi ty of restrictions w. r.t. the gi ven restriction theory. Recently, concep t logics have been proposed where the restriction theory is defined by terminological logics. However, in this approach problems have been assumed to be given as sets of clauses with restrictions and not in terms of formulas with restricted quantifiers. For this special case algorithms to decide satisfiability and validity of restrictions have been given. In this paper we will show that things become much more complex if problems are given as sets of formu las with restricted quantifiers. The reason for this is due to the fact that Skolem function symbols are introduced when translating such formulas into clauses with restrictions. While we will give a procedure to decide satisfiability of restrictions containing function symbols, validity of such restrictions turns out to be undecidable. Nevertheless, we present an application of concept logics with function sym bois, namely their use for generating (partial) answers to queries.	algorithm;kl-one;knowledge representation and reasoning;problem solving;resolution (logic);skolem normal form;undecidable problem	Hans-Jürgen Bürckert;Bernhard Hollunder;Armin Laux	1994		10.22028/D291-25001	knowledge representation and reasoning;computer science;artificial intelligence;algorithm	AI	-19.52964944792914	9.700803411448327	108483
06680595e4603ad5ebefc918c0073103936dfee9	lossless regular views	data integrity;logic in databases;mobile computer;query optimization;complexity;constraint satisfaction;computational complexity;data warehousing;query answering;deductive databases	If the only information we have on a certain database is through a set of views, the question arises of whether this is sufficient to answer completely a given query. We say that the set of views is lossless with respect to the query, if, no matter what the database is, we can answer the query by solely relying on the content of the views. The question of losslessness has various applications, for example in query optimization, mobile computing, data warehousing, and data integration. We study this problem in a context where the database is semistructured, and both the query and the views are expressed as regular path queries. The form of recursion present in this class prevents us from applying known results to our case.We first address the problem of checking losslessness in the case where the views are materialized. The fact that we have the view extensions available makes this case solvable by extending known techniques. We then study a more complex version of the problem, namely the one where we abstract from the specific view extension. More precisely, we address the problem of checking whether, for every database, the answer to the query over such a database can be obtained by relying only on the view extensions. We show that the problem is solvable by utilizing, via automata-theoretic techniques, the known connection between view-based query answering and constraint satisfaction. We also investigate the computational complexity of both versions of the problem.	automata theory;computational complexity theory;constraint satisfaction;decision problem;lossless compression;mathematical optimization;mobile computing;query optimization;recursion	Diego Calvanese;Giuseppe De Giacomo;Maurizio Lenzerini;Moshe Y. Vardi	2002		10.1145/543613.543646	materialized view;online aggregation;sargable;query optimization;complexity;query expansion;web query classification;boolean conjunctive query;constraint satisfaction;computer science;query by example;theoretical computer science;data warehouse;data integrity;data mining;database;rdf query language;computational complexity theory;mobile computing;view;range query;query language	DB	-26.43164412777698	7.828001688880147	108860
2c5ec04258b5e4526258e52bd3417ea0024a34ff	design and implementation of derivation rules in information systems	developpement logiciel;modelizacion;database trigger;interfase usuario;design tool;sistema experto;formal specification;conceptual modeling;user interface;implementation;software generator;base connaissance;conceptual model;ingenieria logiciel;derivation rule;software engineering;production management;specification formelle;modelisation;graphics system;ejecucion;especificacion formal;design and implementation;desarrollo logicial;software development;integrity constraints;genie logiciel;base conocimiento;interface utilisateur;systeme gestion base donnee;information system;systeme expert;database design;modeling;sistema gestion base datos;database management system;systeme information;information system development;sistema informacion;knowledge base;expert system	Abstract   In contrast to tables and integrity constraints, database triggers have no direct equivalent in conceptual models. To enable CASE generators to create database triggers, therefore, derivation rules for entities and attributes are introduced into conceptual modeling. The necessity of an open, reusable representation of data derivation in information systems is discussed. A systematic approach to abstraction dependencies is presented that is used to specify invariants, and, subsequently, to derive generalised repair rules for most inconsistencies that are caused by deletions and insertions. Based on the proposed system of abstraction dependencies, classes of derivation rules can be identified which imply similar invariants and for which generalised propagations can be derived. Using three basic types of parametrical trigger declarations, the extended system specification is transformed into database triggers which complement the traditional schema implementation comprising tables and integrity constraints. To illustrate the proposed information system development extensions, a subschema for a MRP II-style production management system is developed using a commercial, graphical systems design tool and the proposed trigger generator.		Robert Winter	1998	Data Knowl. Eng.	10.1016/S0169-023X(97)00044-X	knowledge base;computer science;conceptual model;data mining;database;expert system	DB	-32.16370310288938	13.791671873191502	109159
1f76ce209036cceafa2510b8703f00eaddfd6421	optimising performance of object-oriented and object-relational systems by dynamic method materialization	programming language;distributed objects;object oriented programming;object oriented	Efficient executions of object methods have a great impact on application response times. Optimising access to data returned by methods is an important issue in object-oriented programs, object-oriented and objectrelational systems, as well as in distributed object environments. Since methods are written in high-level programming languages, optimising their executions is difficult. In this paper we present a technique of reducing access time to data returned by methods by means of materialising method results. We have developed a prototype system where the software module, called the method analyser and optimiser is responsible for monitoring method access and gathering execution statistics. Based on the statistics, the module selects appropriate methods for materialisation. The experiments that we have conducted show that the overall system's response time decreases while using our optimisation technique.	access time;distributed object;experiment;high- and low-level;high-level programming language;horner's method;identifier;mathematical optimization;object query language;object-relational database;prototype;response time (technology)	Mariusz Masewicz;Robert Wrembel;Juliusz Jezierski	2005			object definition language;portable object;object-oriented design;method;database;distributed object;theoretical computer science;computer science;object model;object (computer science);common object request broker architecture	PL	-30.570809988484083	15.117482880556583	109176
01c723936dea07896fcd031c779a8720bf4869f4	validating constraints for inter and intra relationships in xml structured document	databases;document handling;constraint validation;query processing;application software;database management systems;information technology;xml database;data mining;intra relationships;xml structured document;arrays;incremental constraints checking;document database;static constraints checking;internet;inter relationships;periodic structures;containment relationship;xml;incremental constraints checking constraint validation inter relationships intra relationships xml structured document containment relationship xml database static constraints checking;constraint theory;xml databases data mining periodic structures arrays relational databases data models;xml document;relational databases;computer science;structured documents;xml constraint theory database management systems document handling;buildings;data models;hardware	We propose a method for building a constraint validator from a given set of schema for a structured XML document. The validator checks for various relationships existing between the documents such as containment, intra and inter relationship. Thus generating a set of rules, this is used by the forthcoming operation on the XML database. The constraint validator checks for integrity, referential integrity, relative, absolute and value based constraints on the single, the intra and inter document database. The validator obtained by our method is used not only for checking the correctness of the existing XML document but also for incrementally validating updates over this document. The validator maintains the consistency of XML data after update operations, which can be classified into insertion, deletion and replacement. In this way both the static and incremental constraints checking are performed. The experimental results generated from the real data reveal that the algorithm works well in practice.	algorithm;correctness (computer science);document-oriented database;referential integrity;validator;xml database	Radha Senthilkumar;Arputharaj Kannan;K. Hariharan;N. Harishankar;G. P. Aravindan;R. Muthuraj	2008	2008 First International Conference on Emerging Trends in Engineering and Technology	10.1109/ICETET.2008.218	well-formed document;xml validation;simple api for xml;computer science;document structure description;xml framework;data mining;xml schema;database;information retrieval;validator	DB	-28.604083971881323	9.19954502563133	109261
9501c46acf9992db0e8fa38739f9f48edd8dce36	using bmh algorithm to solve subset of xpath queries	xml document;search algorithm	Boyer-Moore-Horspool (BMH) algorithm is commonly used to solve text searching problems. In this paper is used to solve the constraint subset of XPath queries offering effective algorithm to resolve such queries. XML can be grasp as text file contains tags and its content; that kind of view to XML is used in this work. We constraint XML document content and possible XPath queries. This work focus on key ideas and problems appertaining XPath queries execution using text search algorithm BMH.	boyer–moore string search algorithm;boyer–moore–horspool algorithm;string searching algorithm;xml;xpath	David Toth	2007			xml;xpath;xml schema (w3c);full text search;information retrieval;xpath 2.0;algorithm;xslt;grasp;search algorithm;computer science	DB	-31.750588713383888	4.849147088722723	109609
65f9bd37f9c7f6256a92c04e4fe9bd77f5dee4b0	a complete and efficient algebraic compiler for xquery	databases;algebra xml standardization optimizing compilers databases engines sorting user interfaces navigation query processing;query processing;sorting;optimization technique;navigation;engines;algebra;xml;optimizing compilers;user interfaces;standardization	As XQuery nears standardization, more sophisticated XQuery applications are emerging, which often exploit the entire language and are applied to non-trivial XML sources. We propose an algebra and optimization techniques that are suitable for building an XQuery compiler that is complete, correct, and efficient. We describe the compilation rules for the complete language into that algebra and present novel optimization techniques that address the needs of complex queries. These techniques include new query unnesting rewritings and specialized join algorithms that account for XQuery’s complex predicate semantics. The algebra and optimizations are implemented in the Galax XQuery engine, and yield execution plans that are up to three orders of magnitude faster than earlier versions of Galax.	algorithm;compiler;join (sql);linear algebra;mathematical optimization;query optimization;recursion;xml;xpath;xquery	Christopher Ré;Jérôme Siméon;Mary F. Fernández	2006	22nd International Conference on Data Engineering (ICDE'06)	10.1109/ICDE.2006.6	navigation;xml;computer science;sorting;theoretical computer science;database;programming language;user interface;standardization	DB	-29.487687966964103	6.641403576940818	109637
e6a96ea5637b7983edbe1aa1602a6c7ab7ddefe9	quantification in a three-valued logic for natural language question-answering systems	question answering system;natural language	"""This paper examines quantification in relation to a typed three-valued logical system which was developed to serve as the internal query language for virtual data bases accepting natural language (NL) consultation. Such a system is capable, among, other things, of reflecting certain NL presuppositions, handling relations among sets and coping with certain NL ambiguities, within a simple though natural and fairly vast NL subset.#R##N##R##N#In our approach, quantification is dealt with through a single mechanism by which all NL quantifiers introduce the formula """"those(x,p)"""", denoting the set of all those x's in x's associated domain which satisfy statement p. The meaning of each particular NL quantifier--including any presuppositions it may induce-- is rendered through particular constraints upon the set (e.g., cardinality constraints)."""	natural language;question answering;three-valued logic	Verónica Dahl	1979			natural language processing;computer science;artificial intelligence;mathematics;natural language;programming language;algorithm	NLP	-19.427313876646384	6.3048919783221296	109664
5ea09731adcd07af2bd1a06a1b3649d64b12e3c9	a revision system of circular objects and its applications to dynamic semantics of dialogues	forme circulaire;appartenance;semantics;circular shape;semantica;semantique;systeme revision partielle;descripcion;forma circular;pertenencia;membership;description	Since Peter Aczel’s theory[1] of hypersets, many applications to formalizations of a circular object such as a mutual belief has been proposed [3, 2, 4]. This paper will propose Membership Description Systems (MDSs), a partial revision system of circular objects and their applications to a dynamic semantics of a dialogue in the sense that a dialogue can be considered as a revision process of mutual beliefs between its agents. Although usual dynamic semantics [9] updates a variable assignment or a set of information states, our proposal of a semantics of dialogues directly updates situations, which is specified by MDSs, as dynamic semantics of circular propositions [10] directly updates situations. Furthermore, using MDSs as updated objects in the semantics makes a partial and direct update of circular situations themselves possible. As a result, a dynamic semantics of a language with the ↓-operator, which is introduced by [3] to describe circular propositions, can be provided.	belief revision;denotational semantics;programming language;scope (computer science)	Norihiro Ogata	1998		10.1007/3-540-45738-0_13	artificial intelligence;database;mathematics;semantics;linguistics;operational semantics;algorithm	AI	-24.927594633733207	17.210390080802245	109716
25c96cbb6b0a392d3e1ab07ad0326b13ed0548d5	optimizing continuous queries using update propagation with varying granularities	continuous queries;incremental evaluation;datalog;update propagation;deductive databases	We investigate the possibility to use update propagation methods for optimizing the evaluation of continuous queries. Update propagation allows for the efficient determination of induced changes to derived relations resulting from an explicitly performed base table update. In order to simplify the computation process, we propose the propagation of updates with different degrees of granularity which corresponds to an incremental query evaluation with different levels of accuracy. We show how propagation rules for different update granularities can be systematically derived, combined and further optimized by using Magic Sets. This way, the costly evaluation of certain subqueries within a continuous query can be systematically circumvented allowing for cutting down on the number of pipelined tuples considerably.	blocking (computing);bottom-up parsing;computation;linear algebra;mathematical optimization;optimizing compiler;paq;pipeline (computing);sql;software propagation;stream processing;top-down and bottom-up design;window function	Andreas Behrend;Ulrike Griefahn;Hannes Voigt;Philip Schmiegelt	2015		10.1145/2791347.2791368	computer science;theoretical computer science;data mining;database;datalog;programming language	DB	-29.712077753808636	4.656385654409744	109722
dbe1f16a0695eaabc4c7b0c338237226c281cd21	modeling topological relationships between fuzzy spatio-temporal objects		Topological relationships between spatio-temporal objects are the most fundamental elements in spatio-temporal database systems, GIS, and image database systems. The research issue of modeling topological relationships has increasingly attracted attention, especially for querying of spatio-temporal objects and reasoning of topological relationships. Currently, topological relationship operating on spatio-temporal objects with precisely defined boundaries has been well studied. However, in the real world, spatio-temporal objects are not always crisp but with the nature of fuzziness and imprecision. Therefore, how to model topological relationship between fuzzy spatio-temporal objects is a significant topic and needs more investigations. This paper presents a study on modeling topological relationships between fuzzy spatio-temporal objects. Firstly, we give a model of fuzzy spatio-temporal objects in three-dimensional space and define those objects as moving fuzzy points, moving fuzzy lines, and moving fuzzy regions. On this basis, we propose a model for identifying basic topological relations between fuzzy spatio-temporal objects. Furthermore, in order to describe the evolution of basic topological relations over time, we give a model of complex topological relationships which are the sequences of basic relationships. The benefit of this model is that the complex topological relationships can be used as fuzzy spatio-temporal query operators in query languages. Finally, we provide some query examples to demonstrate fuzzy spatio-temporal queries in spatio-temporal database.	geographic information system;query language;spatiotemporal database;temporal database;temporal logic;utility;vagueness	Haitao Cheng;Fu Zhang	2016	CIT		systems modeling;artificial intelligence;fuzzy number;machine learning;data mining	DB	-28.307902747915634	8.328837884122409	109759
dd0c1bb574395457cc4460b1c829f60d7b7604c3	an entity-relationship approach to modelling of vagueness in databases	entity relationship		database;entity–relationship model;vagueness	R. Vandenberghe;Rita M. M. De Caluwe	1991		10.1007/3-540-54659-6_112	natural language processing;entity–relationship model;computer science;artificial intelligence;data mining	DB	-31.69378842351994	10.046510053959548	109773
53640889cfde9c5b9eb9ead2bd628a838afdae72	preserving xml queries during schema evolution	xml schema;xml query;xml database;schema versioning;xml schema evolution;query evaluation;schema evolution	In XML databases, new schema versions may be released as frequently as once every two weeks. This poster describes a taxonomy of changes for XML schema evolution. It examines the impact of those changes on schema validation and query evaluation. Based on that study, it proposes guidelines for XML schema evolution and for writing queries in such a way that they continue to operate as expected across evolving schemas.	database schema;schema evolution;taxonomy (general);xml database;xml schema	Mirella M. Moro;Susan Malaika;Lipyeow Lim	2007		10.1145/1242572.1242841	xml validation;xml encryption;semi-structured model;relax ng;logical schema;xml schema;streaming xml;computer science;xs3p;document definition markup language;document structure description;star schema;data mining;xml database;xml schema;database;document schema definition languages;schematron;xml signature;database schema;xml schema editor;information retrieval;efficient xml interchange	DB	-32.56576148361284	7.89000391122228	109815
5b7c9be82cb1c56517a32956c8073f44deb260c8	a new and efficient tree pruning method and its applications to workflow-based ppn nets for financial controls	intelligent tree pruning approach;ppns;financial control;intelligent tree pruning;special parallel net;halt analysis;workflow;halts;deadlocks;parallel petri nets	This paper uses an intelligent tree pruning method to identify the halt states of workflow-based parallel Petri nets (PPNs) for financial controls. This effective and efficient approach contributes to PPN net modelling by a systematic study of the necessary conditions utilising modules. Workflowbased PPN nets provide a mathematical structure suitable for modelling and simulating a wide range of concurrent financially controlled systems. Given the examples provided in this paper, the proposed intelligent tree pruning approach has been successful in detecting halts.	halting problem;mathematical structure;os-tan;petri net;sensor;simulation;word lists by frequency	Jason C. H. Chen;Jack H. W. Penm;Richard Deane Terrell	2011	IJIDS	10.1504/IJIDS.2011.040420	workflow;computer science;deadlock;machine learning;data mining;distributed computing	AI	-22.759585657518887	16.957674929995697	109831
94cea0c30ca09e2f283145140df51e653a9e72a2	on top-k retrieval for a family of non-monotonic ranking functions		We presented a top-k algorithm to retrieve tuples according to the order provided by a non-necessarily monotone ranking funtion that belongs to a novel family of functions. The conditions imposed on the ranking functions are related to the values where the maximum score is achieved.	algorithm;computation;non-monotonic logic;ranking (information retrieval);monotone	Nicolás Madrid;Umberto Straccia	2013		10.1007/978-3-642-40769-7_44	data mining;ranking svm;information retrieval	DB	-23.472222290708768	7.082564696574465	110031
15ed7621362f4c07f550a1c954dfee70beaf5f26	extensible grouping and aggregation for data reconciliation	high level language;data integrity;data processing	New applications from the areas of analytical data processing and data integration require powerful features to condense and reconcile available data. Object-relational and other data management systems available today provide only limited concepts to deal with these requirements. The general concept of grouping and aggregation appears to be a fitting paradigm for a number of the mentioned issues, but in its common form of equality based groups and restricted aggregate functions a number of problems remain unsolved. Various extensions to this concept have been introduced over the last years, especially regarding user-defined functions for aggregation and derivation of grouping properties. We propose generic interfaces for user-defined grouping and aggregation as part of a SQL extension, allowing for more complex functions, for instance integration of data mining algorithms. Furthermore, we discuss high-level language primitives for common applications and illustrate the approach by introducing new concepts for similarity-based duplicate detection and elimination.	aggregate data;aggregate function;algorithm;cache (computing);data mining;extensibility;federated search;high- and low-level;high-level programming language;mathematical optimization;object-relational database;parallel computing;programming paradigm;query language;query optimization;rom cartridge;requirement;sql;text-based (computing);unit disk graph;user-defined function;xslt/muenchian grouping	Eike Schallehn;Kai-Uwe Sattler;Gunter Saake	2001			sql;data model;data mining;data integration;data management;data modeling;data warehouse;data integrity;language primitive;computer science	DB	-29.81041849999641	6.720222887593353	110068
e8bd1f4241098e543af4191534494e114febcb55	refining and generalizing p-log - preliminary report		This paper is a preliminary report on the development of a new, improved version of the knowledge representation language P-log. This version clarifies syntax and semantics of the original language and brings informal reading of its main concepts closer to their formal semantics. It also expands P-log with a sophisticated type system and removes some unnecessary restrictions on the occurrences of atoms expressing observations, interventions, and randomness in P-log rules. The generalization simplifies the syntax and allows to formalize reasoning the authors were not able to formalize in the original P-log.	approximation algorithm;correctness (computer science);expressive power (computer science);interaction information;knowledge engineer;knowledge representation and reasoning;monte carlo;question answering;randomness;redo log;sampling (signal processing);semantics (computer science);softening;type system;usability	Evgenii Balai;Michael Gelfond	2017			refining (metallurgy);combinatorics;mathematics;generalization	Logic	-20.689316056997257	14.479777042656737	110142
c5fd99b09165699d0118e49e2d2df90e0a8e4f75	specification and enforcement of classification and inference constraints	mandatory access control;database system;authorisation;database management systems;satisfiability;classification;data labeling classification inference constraints mandatory access control database protected information multilevel systems data repositories model independent framework;computer science protection laboratories labeling data security access control database systems multilevel systems electronic switching systems contracts;data privacy;data privacy authorisation classification database management systems;data classification;historical data	Although mandatory access control in database systems has been extensively studied in recent years, and several models and systems have been proposed, capabilities for enforcement of mandatory constraints remain limited. Lack of support for expressing and combating inference channels that improperly leak protected information remains a major limitation in today’s multilevel systems. Moreover, the working assumption that data are classified at insertion time makes previous approaches inapplicable to the classifi cation of existing, possibly historical, data repositorie s that need to be classified for release. Such a capability would be of great benefit to, and appears to be in demand by, governmental, public, and private institutions. We address the problem of classifying existing data repositories by taking into consideration explicit data cl ssification as well as association and inference constraints . Constraints are expressed in a unified, DBMSand modelindependent framework, making the approach largely applicable. We introduce the concept of minimal classification as a labeling of data elements that, while satisfying th e constraints, ensures that no data element is classified at a level higher than necessary. We also describe a technique and present an algorithm for generating data classification s that are both minimal and preferred according to certain criteria. Our approach is based on preprocessing, or compiling, constraints to produce a set of simple classificatio n assignments that can then be efficiently applied to classify any database instance. This work was supported in part by the National Science Found ation under grant ECS-94-22688 and by DARPA/Rome Laboratory unde r contract F30602-96-C-0337. This work was performed while the author was visiting SRI Int er ational, Computer Science Laboratory, supported in part by t he National Science Foundation under grant ECS-94-22688. On leave from Università di Milano. Author’s permanent add ress: Università di Milano, Polo Didattico e di Ricerca di Crema, Via Bramante 65, 26013 Crema Italy; e-mail: samarati@dsi.unimi.it.	algorithm;compiler;computer science;data element;database;email;gene ontology term enrichment;heuristic (computer science);mandatory access control;preprocessor	Steven Dawson;Sabrina De Capitani di Vimercati;Pierangela Samarati	1999		10.1109/SECPRI.1999.766913	information privacy;biological classification;computer science;data administration;data mining;database;authorization;computer security;database design;satisfiability	DB	-27.554869880607836	13.161086152808194	110160
b2b909edf9d40802d4d67c777b4ed027b74f9b1a	reasoning about schema mappings	004;data exchange data integration schema mappings equivalence optimality normalization schema mapping management	Schema mappings are an important tool in several areas of database research. Recently, the topic of reasoning about schema mappings was given attention, in particular revolving around the central concepts of equivalence and optimality. In this chapter, we survey these results. First, we introduce relaxed notions of logical equivalence and show their potential for finding optimized schema mappings. We then look at applications of these concepts to optimization, normalization, and schema mapping management, as well as the boundaries of computability. We conclude by giving a glimpse at reasoning about schema mappings in a broader sense by looking at how to debug schema mappings. 1998 ACM Subject Classification H.2.5 [Heterogeneous Databases]: Data translation	algorithm;cad data exchange;computability;computation;conjunctive query;database normalization;debugging;extensibility;inferring horizontal gene transfer;mathematical optimization;microsoft outlook for mac;schema evolution;turing completeness;undecidable problem;xml schema	Emanuel Sallinger	2013		10.4230/DFU.Vol5.10452.97	discrete mathematics;schema;logical schema;conceptual schema;star schema;database;mathematics;database schema;algorithm	DB	-24.368243987325595	9.974388958242255	110221
99a721c6805e909177f11151ee31e7cd9ca23d3e	a framework for join pattern indexing in intelligent database systems	database system;relational database join pattern indexing intelligent database systems knowledge directed inference query processing derived data multiple data objects rule derived data query processing costs join pattern relation pattern redundancy reduction methods;base donnee;systeme intelligent;metodo reduccion;query processing;rule based systems;rule processing;redundancia;regle production;sistema inteligente;multiple data objects;database;base dato;relational database;rule derived data;technology management;query processing costs;join pattern indexing;database theory deductive databases relational databases query processing indexing;intelligent database systems;redundancy;indexing;data materialization;pattern redundancy reduction methods;join pattern relation;indexation;indexing deductive databases query processing costs database systems knowledge based systems iris electronics packaging laboratories technology management;database systems;inferencia;intelligent system;intelligent databases;derived data;methode reduction;knowledge directed inference;join indexing;relational databases;iris;electronics packaging;reduction method;database theory;object relational;knowledge based systems;inference;redondance;production rule;regla produccion;deductive databases	In intelligent database systems, knowledge-directed inference often derives large amounts of data, and the efficiency of query processing in these systems depends upon how the derived data are maintained. This paper focuses on situations where the rule is conditional on a join of multiple data objects (relations) and the rule-derived data are materialized to reduce the overall query processing costs. We develop an indexing technique based on a unique construct called join pattern relation. Several pattern redundancy reduction methods are also introduced to minimize the overhead cost of jioin indexing.	intelligent database;join-calculus;overhead (computing)	Arie Segev;J. Leon Zhao	1995	IEEE Trans. Knowl. Data Eng.	10.1109/69.476499	materialized view;hash join;recursive join;relational database;computer science;technology management;data mining;database;sort-merge join;information retrieval	DB	-27.642774811560436	4.824187386467352	110435
f2a0c67287a26882777bd2822df5c2d2b7eb6e76	type hierarchies and semantic data models	programming language;data model;database programming languages;semantic data model;type system	The basic abstraction mechanisms of Semantic Data Models - aggregation, classification and generalization - are considered the essential features to overcome the limitations of traditional data models in terms of semantic expressiveness. An important issue in database programming language design is which features should a programming language have to support the abstraction mechanisms of Semantic Data Models. This paper shows that when using a strongly typed programming language, that language should support the notion of type hierarchies to achieve a full integration of Semantic Data Models abstraction mechanisms within the language's type system. The solution is presented using the language Galileo, a strongly typed, interactive programming language specifically designed for database applications.		Antonio Albano	1983	SIGPLAN Notices	10.1145/872728.806864	semantic data model;natural language processing;fourth-generation programming language;first-generation programming language;data definition language;natural language programming;semantic computing;very high-level programming language;universal networking language;language primitive;type system;data manipulation language;object language;specification language;programming domain;data control language;data model;computer science;third-generation programming language;semantic compression;database;programming paradigm;low-level programming language;fifth-generation programming language;programming language;programming language specification;high-level programming language	SE	-30.005720915548302	11.382479298113138	110491
3dcdfd240bf3ab37bbc227fab1226aba89d8c888	prolog-based meta-rules for relational database representation and manipulation	well defined environment;data restructuring;design automation;relational databases deductive databases knowledge representation database languages logic programming application software data models knowledge based systems environmental management design automation;relational database representation;application software;prolog;deductive databases prolog based meta rules relational database representation prolog based experimental system query optimization data restructuring database design theoretical foundations well defined environment;query optimization;relational database;relational databases database theory deductive databases knowledge based systems prolog;logic programming;relational model;theoretical foundations;prolog based experimental system;relational databases;prolog based meta rules;database design;knowledge representation;theoretical foundation;environmental management;database languages;database theory;knowledge based systems;data models;deductive databases	AbstructSo far, it has been characteristic of Prolog-based representations of relational databases that only the instance level is represented explicitly, whereas the schema (or meta) level is implicitly in the mind of the user. This approach has several serious disadvantages. In this paper we develop a Prologbased experimental system for relational databases. In this system all manipulation is based on such a knowledge representation which binds the instance and schema levels together in a natural, precise, and systematic way. We show that this kind of knowledge representation affords the possibility of defining the essential concepts associated with structures and operations of relational databases generally, i.e., without binding the definition to any sample database which is the usual situation in the approaches based on representing only the instance level explicitly. Because of general definitions in our experimental system, it can be connected flexibly to other rule-based systems. We show that our experimental system can also be used flexibly to define various constraints related to the structures and operations of relational databases. These constraints may concern both the schema and instance levels. Complex structural relationships among data have a central role in database applications. Therefore we need strong general means and principles for structuring knowledge in Prolog. In this paper we propose the use of the structural primitives: tuples, maps, and sets with their primitive operations to manage complex structural modeling in a Prolog environment. Analogously to the abstract data-type mechanism, we can combine these structural primitives with each other. This approach affords also the possibility of defining a set of general primitive operations for each structural primitive. In general, such primitive operations are called meta-rules, and they play a central role in the definition of our experimental system. The framework of this paper is based only on the theoretical foundations of the relational model. This starting point affords the possibility of utilizing the framework both in the context of different approaches to integrate a relational database system with a deductive system, and in the context of different relational database topics such as relational database design, relational database restructuring, relational query languages, etc.	abstract data type;class diagram;database design;database schema;experimental system;formal system;knowledge representation and reasoning;map;prolog;query language;relational database management system;relational model;rule-based system	Timo Niemi;Kalervo Järvelin	1991	IEEE Trans. Software Eng.	10.1109/32.83913	knowledge representation and reasoning;domain relational calculus;sargable;database theory;sql;relational model/tasmania;nested set model;relational model;codd's theorem;statistical relational learning;relational calculus;electronic design automation;entity–relationship model;relational database;computer science;database model;knowledge-based systems;data mining;database;change data capture;conjunctive query;programming language;candidate key;object-relational impedance mismatch;database design	DB	-29.806092136476963	11.482449412973619	110534
3712fbb5fc38f5d79617a973ca33d4e442646ff8	formalization of resilience for constraint-based dynamic systems	resilience;dynamic system;constraint-based system;resistance;recoverability	Many researchers in different fields are interested in building resilient systems that can absorb shocks and recover from damages caused by unexpected large-scale events. Existing approaches mainly focus on the evaluation of the resilience of systems from a qualitative point of view, or pay particular attention to some domain-dependent aspects of the resilience. In this paper, we introduce a very general, abstract computational model rich enough to represent a large class of constraint-based dynamic systems. Taking our inspiration from the literature, we propose a simple parameterized property which captures the main features of resilience independently from a particular application domain, and we show how to assess the resilience of a constraint-based dynamic system through this new resilience property.	analysis of algorithms;application domain;compiler;computation;computational complexity theory;computational model;controller (computing);dynamic bayesian network;dynamical system;intelligent agent;intelligent environment;markov chain;non-deterministic turing machine;partially observable markov decision process;partially observable system;serializability	Nicolas Schwind;Morgan Magnin;Katsumi Inoue;Tenda Okimoto;Taisuke Sato;Kazuhiro Minami;Hiroshi Maruyama	2015	Journal of Reliable Intelligent Environments	10.1007/s40860-015-0016-0	simulation;knowledge management;socio-ecological system	AI	-24.216994591919978	14.976139706062268	110572
dd35b6f9f4edf16faf51d6273222a3a02c2c8e21	domains and data types in era information model	era information model;data types;information model;data type			Joseph J. Tardo	1979			data science;data mining;world wide web	DB	-33.69082073971554	6.084526729437702	110800
8020b77e5278a0dba6ac8ba2260544a31ccf8c17	non-destructive integration of form-based views	regle inference;hierarchized structure;integration information;hierarchical data;structure hierarchisee;intelligence artificielle;inference rule;information integration;integracion informacion;artificial intelligence;modele donnee;inteligencia artificial;information system;systeme information;estructura jerarquizada;regla inferencia;data models;sistema informacion	Form documents or screen forms bring essential information on the data manipulated by an organization. They can be considered as different but often overlapping views of its whole data. This paper presents a non-destructive approach of their integration. The main idea of our approach is to keep the original views intact and to specify constraints between overlapping structures. For reasoning over constraints, we provide a set of inference rules that allows not only to infer implied constraints but also to detect conflicts. These reasoning rules are proved to be sound and complete. Although the form-based views are hierarchical structures, our constraints and reasoning rules can also be used in non-hierarchical data models.	cobham's thesis;constraint (mathematics);data model;hierarchical database model;linked data structure	Jan Hidders;Jan Paredaens;Philippe Thiran;Geert-Jan Houben;Kees M. van Hee	2005		10.1007/11547686_6	data modeling;computer science;artificial intelligence;information integration;machine learning;data mining;database;information system;hierarchical database model;rule of inference	DB	-28.274663468199794	11.524416301913156	110986
5889423c676d965c993816bfed768bd28e9ba404	compensation methods to support cooperative applications: a case study in automated verification of schema requirements for an advanced transaction model	formal methods;formal method;theorem proving;automated verification;theorem prover;workflow system;advanced transaction model;object oriented databases;object oriented database;higher order logic;cooperative work	Compensation plays an important role in advanced transaction models, cooperative work and workflow systems. A schema designer is typically required to supply for each transaction T another transaction T −1 to semantically undo the effects of T . Little attention has been paid to the verification of the desirable properties of such operations, however. This paper demonstrates the use of a higher-order logic theorem prover for verifying that compensating transactions return a database to its original state. It is shown how an OODB schema is translated to the language of the theorem prover so that proofs can be performed on the compensating transactions. Copyright  2001 John Wiley & Sons, Ltd.	automated theorem proving;data integrity;database schema;formal language;formal verification;hol (proof assistant);interactive proof system;isabelle;john d. wiley;late binding;object lifetime;proof assistant;qr code;query language;requirement;run-time type information;undo;verification and validation	David Spelt;Susan Even	2001	Concurrency and Computation: Practice and Experience	10.1002/cpe.610	formal methods;database transaction;transaction processing;distributed transaction;computer science;knowledge management;theoretical computer science;database;distributed computing;online transaction processing;automated theorem proving;programming language;transaction processing system	DB	-28.90677816132863	16.77908253278252	110988
1608965184be3efeef0b4334ac92c56a79c5829e	using heterogeneous equivalences for query rewriting in multidatabase systems		In order to have signiicant practical impact on future information systems, multi-database management systems (MDBMS) must be both exible and eecient. We consider a MDBMS with a common object-oriented model, based on the ODMG standard, and local databases that may be relational or object-oriented. In this context , query rewriting (for optimization) is made diicult by schematic discrepancy, and the need to model mapping information between the multidatabase and local schemas. We address the exibility issue by representing the mappings from a local schema to the multidatabase schema, as a set of heterogeneous object equivalences, in a declarative language. EEciency is obtained by exploiting these equivalences to rewrite multidatabase OQL queries into equivalent, simpliied queries on the local schemas.	database;declarative programming;discrepancy function;information system;mathematical optimization;object data management group;object query language;rewrite (programming);rewriting;schematic	Daniela Florescu;Louiqa Raschid;Patrick Valduriez	1995			distributed computing;query rewriting;database;computer science	DB	-29.620980859367457	10.61808614330097	111221
a069b9e174546f74d4c8619b70e654cf90f8c49e	an alternative characterization of disjunctive logic programs	disjunctive logic programming	\indent We present an alternative characterization of disjunctive logic programs. We first review Inheritance near-Horn Prolog (InH-Prolog), an intuitive and computationally effective procedure that extends Prolog using case-analysis. We then describe a fixpoint characterization of disjunctive logic programs that is similarly based on case-analysis. This fixpoint characterization closely corresponds to the InH-Prolog procedure, and so gives needed insight into the difficult problem of recognizing the minimal disjunctive answers obtainable from the procedure. Due to its case-analysis nature, this characterization maintains a natural and close relationship to the standard characterization of Horn program. It also gives added insight into the role of definite answers for disjunctive programs and enables one to neatly focus attention on the interesting class of definite (single atom) consequences.	disjunctive normal form;logic programming	David W. Reed;Donald W. Loveland;Bruce T. Smith	1991			discrete mathematics;stable model semantics;theoretical computer science;mathematics;algorithm	AI	-20.196994634828755	13.293768276517989	111306
2ef5a65a142a7f287c1912e19c7296d355ec98ca	algebraic query optimization for distributed top-k queries	distributed data;tiempo respuesta;modelizacion;dynamic programming;distributed system;reponse temporelle;largeur bande;optimisation;programacion dinamica;text;base donnee repartie;systeme reparti;distributed database;red www;top k queries;optimizacion;query processing;gestion red;traitement requete;interrogation base donnee;reseau web;base repartida dato;interrogacion base datos;response time;query optimization;dynamic program;texte;almacen dato;temps reponse;modelisation;hierarchical classification;sistema repartido;internet;time response;indexation;anchura banda;retard;programmation dynamique;gestion reseau;classification hierarchique;selectividad;selectivity;bandwidth;world wide web;optimization;network management;tratamiento pregunta;entrepot donnee;selectivite;data warehouse;distributed systems;respuesta temporal;retraso;top k query processing;texto;modeling;clasificacion jerarquizada;database query;cost model	Distributed top-k query processing is increasingly becoming an essential functionality in a large number of emerging application classes. This paper addresses the efficient algebraic optimization of top-k queries in wide-area distributed data repositories where the index lists for the attribute values (or text terms) of a query are distributed across a number of data peers and the computational costs include network latency, bandwidth consumption, and local peer work. We use a dynamic programming approach to find the optimal execution plan using compact data synopses for selectivity estimation that is the basis for our cost model. The optimized query is executed in a hierarchical way involving a small and fixed number of communication phases. We have performed experiments on real web data that show the benefits of distributed top-k query optimization both in network resource consumption and query response time. In dieser Arbeit beschäftigen wir uns mit der Optimierung verteilter top-k Anfragen, bei denen die Daten auf verschiedene Rechner verteilt sind. Die Kosten, die es zu minimieren gilt, umfassen die Netzwerklast, den Verbrauch lokaler Rechenleistung und letztendlich die Zeit der Anfrageausführung. Wir benutzen dynamische Programmierung, um den optimalen Anfrageplan zu finden. Die Kostenschätzung basiert dabei auf kompakten Repräsentationen der eigentlichen Score-Verteilungen. Die optimierte Anfrage wird anschließend in einer hierachischen Weise ausgeführt, bei der nur eine kleine und fest vorgegebene Anzahl von Kommunikationsschritten angewendet wird. Umfassende Experimente mit Daten aus der realen Welt zeigen beachtliche Gewinne sowohl in der Reduktion der Netzwerklast als auch in der Reduktion der Anfragezeit.	algorithm;analysis of algorithms;conjunctive query;dynamic programming;eine and zwei;experiment;information needs;mathematical optimization;query optimization;query plan;response time (technology);run time (program lifecycle phase);selectivity (electronic);unified model;video synopsis	Thomas Neumann;Sebastian Michel	2007	Informatik - Forschung und Entwicklung	10.1007/s00450-007-0024-2	network management;sargable;query optimization;query expansion;web query classification;the internet;selectivity;systems modeling;boolean conjunctive query;computer science;dynamic programming;data warehouse;data mining;database;rdf query language;web search query;world wide web;response time;distributed database;bandwidth;query language;spatial query	DB	-25.849807171482347	4.549209853825713	111505
015d9496f659a0e706a7668ba727b3bc2c723787	on data dependencies in dataspaces	data handling computational complexity;query processing;approximation method;measurement uncertainty;approximation methods measurement uncertainty semantics image color analysis query processing;data dependence;computational complexity;randomized approach data dependencies dataspaces comparable dependencies metric functional dependencies matching dependencies confidence validation problem np hard problem greedy approach;image color analysis;data handling	To study data dependencies over heterogeneous data in dataspaces, we define a general dependency form, namely comparable dependencies (CDs), which specifies constraints on comparable attributes. It covers the semantics of a broad class of dependencies in databases, including functional dependencies (FDs), metric functional dependencies (MFDs), and matching dependencies (MDs). As we illustrated, comparable dependencies are useful in real practice of dataspaces, e.g., semantic query optimization. Due to the heterogeneous data in dataspaces, the first question, known as the validation problem, is to determine whether a dependency (almost) holds in a data instance. Unfortunately, as we proved, the validation problem with certain error or confidence guarantee is generally hard. In fact, the confidence validation problem is also NP-hard to approximate to within any constant factor. Nevertheless, we develop several approaches for efficient approximation computation, including greedy and randomized approaches with an approximation bound on the maximum number of violations that an object may introduce. Finally, through an extensive experimental evaluation on real data, we verify the superiority of our methods.	approximation algorithm;computation;data dependency;database;dataspaces;functional dependency;greedy algorithm;mathematical optimization;np-hardness;query optimization;randomized algorithm;semantic query	Shaoxu Song;Lei Chen;Philip S. Yu	2011	2011 IEEE 27th International Conference on Data Engineering	10.1109/ICDE.2011.5767857	dependency theory;computer science;theoretical computer science;machine learning;group method of data handling;data mining;computational complexity theory;statistics;measurement uncertainty	DB	-24.309065896330623	4.938933626034468	111616
071c2d182060f31adfc5f7aad2c7fbfc18b4641d	solving temporal over-constrained problems using fuzzy techniques	over constrained problems;fuzzy temporal reasoning	The satellite-scheduling problem represents an interesting field to test non-conventional temporal solvers because scheduling-problems are inherently over-constrained and, moreover, the tasks may be known in an imprecise and uncertain manner. In this paper we present an application of our fuzzy temporal reasoning system to the satellite-scheduling problem. First, we describe our model of integration of qualitative and quantitative temporal information affected by vagueness and uncertainty. Then, we show the usefulness of fuzzy constraints when dealing with over-constrained temporal problems.	fuzzy concept	Silvana Badaloni;Marco Falda;Massimiliano Giacomin	2007	Journal of Intelligent and Fuzzy Systems		mathematical optimization;computer science;artificial intelligence;machine learning;mathematics	Robotics	-19.134355618501793	7.460294948725811	111699
709bdb550b6c3824f6f7f8d3727616c617448b9e	hybrid reasoning for ontology classification	resolution;complex ontology;classification;tableau;hybrid reasoning	Ontology classification is an essential reasoning task for ontology based systems. Tableau and resolution are two dominant types of reasoning procedures for ontology reasoning. Complex ontologies are often built on more expressive description logics and are usually highly cyclic. When reasoning complex ontologies, the both approaches may have difficulties in terms of reasoning results and performance, but for different ontology types. In this research, we investigate a hybrid reasoning approach, which will employ well-defined strategies to decompose and modify a complex ontology into subsets of ontologies based on capabilities of different reasoners, process the subsets with suitable individual reasoners, and combine such individual classification results into the overall classification result. The objective of our approach is to detect more subsumption relationships than individual reasoners for complex ontologies, and improve overall reasoning performance.	description logic;method of analytic tableaux;ontology (information science);semantic reasoner;subsumption architecture;web ontology language	Weihong Song;Bruce Spencer;Weichang Du	2011		10.1007/978-3-642-21043-3_44	natural language processing;upper ontology;knowledge representation and reasoning;ontology components;resolution;qualitative reasoning;biological classification;computer science;knowledge management;ontology;model-based reasoning;data mining;reasoning system;ontology-based data integration;process ontology;suggested upper merged ontology	AI	-20.848614760833318	4.958816391345097	111800
6a5246d5ce88178aef13f1d7ae222341156ea150	towards a functional approach to modular ontologies using institutions	query language;functional approach;global language;generalise existing notion;support modularisation;ontology language;functional view;modularity;ontologies;interpolation	We propose a functional view of ontologies that emphasises their role in determining answers to queries, irrespective of the formalism in which they are written. A notion of framework is introduced that captures the situation of a global language into which both an ontology language and a query language can be translated, in an abstract way. We then generalise existing notions of robustness from the literature, and relate these to interpolation properties that support modularisation of ontologies.	abox;antivirus software;description logic;field (computer science);functional approach;interpolation;language-independent specification;ontology (information science);query language;semantics (computer science)	Daniel Pokrywczynski;Grant Malcolm	2014	Studia Logica	10.1007/s11225-012-9466-z	natural language processing;computer science;theoretical computer science;data mining	PL	-22.55916647752947	9.509066005811079	111843
07e4bfc50e759c1fb8efcaa4f3df98c41a1f33a0	formalizing complex task libraries in golog	action language;decision support;domain knowledge;incomplete information;query answering;situation calculus;knowledge base	We present an approach to building libraries of tasks in complex action languages such as Golog, for query answering. Our formalization is based on a situation calculus framework that allows probabilistic, temporal actions. Once a knowledge base is built containing domain knowledge including type information and a library of tasks and the goals they can achieve, we are interested in queries about the achievability of goals. We consider cases where, using domain object type and goal information in the KB, a user is able to get specific answers to a query while leaving some of the specifics for the system to figure out. In some cases where the specifics are missing from the KB, the user is provided with the possible alternative answers that are compatible with the incomplete information in the KB. This approach is being explored in the context of a military operations planning domain for decision support.	business object;correctness (computer science);decision support system;domain-driven design;knowledge base;library (computing);object type (object-oriented programming);programming language;sensitivity and specificity;situation calculus	Alfredo Gabaldon	2006			knowledge base;action language;computer science;knowledge management;artificial intelligence;machine learning;data mining;database;situation calculus;complete information;domain knowledge	AI	-25.232788877070185	9.196146063866381	111970
cca99fd09cd6eb00692d3bbb080a7323da3c6fa7	crowdsensing: a crowd-sourcing based indoor navigation using rfid-based delay tolerant network	indoor navigation;indoor localization;crowd sourcing;rfid;delay tolerant network	As a supporting technology for most pervasive applications, indoor localization and navigation has attracted extensive attention in recent years. Conventional solutions mainly leverage techniques like WiFi and cellular network to effectively locate the user for indoor localization and navigation. In this paper, we investigate the problem of indoor navigation by using the RFID-based delay tolerant network. Different from the previous work, we aim to efficiently locate and navigate to a specified mobile user who is continuously moving within the indoor environment. As the low-cost RFID tags are widely deployed inside the indoor environment and acting as landmarks, the mobile users can actively interrogate the surrounding tags with devices like smart phones and leave messages or traces to the tags. These messages or traces can be carried and forwarded to more tags by other mobile users. In this way, the RFID-based infrastructure forms a delay tolerant network. By using the crowd-sourcing technology in RFID-based delay tolerant network, we respectively propose a framework, namely CrowdSensing, to schedule the tasks and manage the resources in the network. We further propose a navigation algorithm to locate and navigate to the moving target. We verify the performance of proposed framework and navigation algorithm on mobility model built on real-world human traceset. Experiment results show that our solution can efficiently reduce the average searching time for indoor navigation. & 2015 Elsevier Ltd. All rights reserved.	algorithm;crowdsensing;crowdsourcing;dspace;delay-tolerant networking;radio-frequency identification;smartphone;tag (metadata);tracing (software)	Hao Ji;Lei Xie;Chuyu Wang;Yafeng Yin;Sanglu Lu	2015	J. Network and Computer Applications	10.1016/j.jnca.2015.02.010	radio-frequency identification;simulation;telecommunications;computer science;delay-tolerant networking;multimedia;mobile robot navigation;statistics;computer network	Mobile	-28.75157099833352	17.77638744555294	112344
2887fb83beb933a0f5507d17d5e1a6c454e115ab	detecting content changes on ordered xml documents using relational databases	estensibilidad;base relacional dato;change detection;sql;xml language;interrogation base donnee;interrogacion base datos;relational database;base donnee relationnelle;xml document;extensibilite;scalability;systeme gestion base donnee;sistema gestion base datos;database management system;database query;langage xml;lenguaje xml	Previous works in change detection on XML focused on detecting changes to text file using ordered and unordered tree model. These approaches are not suitable for detecting changes to large XML document as it requires a lot of memory to keep the two versions of XML documents in the memory. In this paper, we take a more conservative yet novel approach of using traditional relational database engines for detecting content changes of ordered large XML data. First, we store XML documents in RDBMS. Then, we detect the changes by using a set of SQL queries. Experimental results show that our approach has better scalability, better performance, and comparable result quality compared to the state-of-the-art approaches.	database schema;pervasive psql;relational database management system;sql;scalability;sensor;xml	Erwin Leonardi;Sourav S. Bhowmick;T. S. Dharma;Sanjay Kumar Madria	2004		10.1007/978-3-540-30075-5_56	well-formed document;xml catalog;xml validation;binary xml;xml encryption;simple api for xml;xml;xml schema;streaming xml;computer science;document structure description;xml framework;data mining;xml database;xml schema;database;xml signature;world wide web;xml schema editor;efficient xml interchange	DB	-30.878197556789875	5.528201085580613	112438
2e3dec44d2bc466e15176d543b22d7429fa72285	implementing temporal defeasible logic for modeling legal reasoning	legal reasoning;temporal defeasible logic;efficient implementation;system work;real life example;legal concept	In this paper we briefly present an efficient implementation of temporal defeasible logic, and we argue that it can be used to efficiently capture the the legal concepts of persistence, retroactivity and periodicity. In particular, we illustrate how the system works with a real life example of a regulation.	causality;computation;defeasible logic;defeasible reasoning;deontic logic;event calculus;java;persistence (computer science);quasiperiodicity;real life;situation calculus;the australian;time complexity	Guido Governatori;Antonino Rotolo;Rossella Rubino	2009		10.1007/978-3-642-14888-0_5	computer science;artificial intelligence;theoretical computer science;algorithm	AI	-19.66201140977069	8.68581226909401	112658
399843d4d283fda142adc3f4b510d8e1ad9424d8	handling inconsistencies due to class disjointness in sparql updates		The problem of updating ontologies has received increased attention in recent years. In the approaches proposed so far, either the update language is restricted to sets of ground atoms or, where the full SPARQL update language is allowed, the TBox language is restricted so that no inconsistencies can arise. In this paper we discuss directions to overcome these limitations. Starting from a DLLite fragment covering RDFS and concept disjointness axioms, we define three semantics for SPARQL instance-level (ABox) update: under cautious semantics, inconsistencies are resolved by rejecting updates potentially introducing conflicts; under brave semantics, instead, conflicts are overridden in favor of new information where possible; finally, the fainthearted semantics is a compromise between the former two approaches, designed to accommodate as much of the new information as possible, as long as consistency with the prior knowledge is not violated. We show how these semantics can be implemented in SPARQL via rewritings of polynomial size and draw first conclusions from their practical evaluation.	abox;ontology (information science);polynomial;rdf schema;sparql;tbox	Albin Ahmeti;Diego Calvanese;Axel Polleres;Vadim Savenkov	2016		10.1007/978-3-319-34129-3_24	computer science;data mining;database;algorithm	AI	-23.961920322307364	8.483626199872043	112730
a126b9bd7ceb7b83ebee3c8287d3e99d48e618db	an overview of the object-protocol model (opm) and opm data management tools	databases;base relacional dato;securite;database management systems;specification;data management;scientific database;relational database;automatic generation;data model;data management tools;object protocol model;object data model;especificacion;molecular biology;safety;base donnee relationnelle;relational database management system;protocol specification;analyse transactionnelle;systeme gestion base donnee;seguridad;sistema gestion base datos;database management system;scientific research;models;scientific and technical information;transactional analysis;analisis transaccional	In this paper, we overview the Object-Protocol Model (OPM) and a suite of data management tools based on OPM. OPM is a data model that allows specifying database structures and queries in terms of objects and protocols specific to scientific (e.g., molecular biology laboratory) applications. Thus, scientific experiments and their resources can be described using OPM in a unified way. OPM data management tools provide facilities for specifying and querying relational databases in terms of OPM constructs, and automatically generate database specifications and queries for implementing OPM on top of commercial relational database management systems (DBMSs). OPM tools increase t,he efficiency of developing scientific databases using relational DBMSs, while insulating scientists from the underlying DBMSs.	data model;experiment;relational database management system	I-Min A. Chen;Victor M. Markowitz	1995	Inf. Syst.	10.1016/0306-4379(95)00021-U	relational database management system;scientific method;data model;data management;relational database;computer science;data mining;database;transactional analysis;world wide web;specification	DB	-30.998756980632084	13.966104448581039	112856
ba36101bb7a3af17a2dc1895a76d3229423a385e	object oriented database panel - position statement	object oriented database			Bill Paseman	1989			deep-sky object;computer science;data mining;database;programming language	DB	-31.13870367656384	9.593284421915618	112897
3e0a45f16f9e7d3d50743473ff07d336c785cbb0	keyword query cleaning	quality metric;query processing;dynamic programming algorithm;query optimization;keyword search;natural language;database query	Unlike traditional database queries, keyword queries do not adhere to predefined syntax and are often dirty with irrelevant words from natural languages. This makes accurate and efficient keyword query processing over databases a very challenging task. In this paper, we introduce the problem of query cleaning for keyword search queries in a database context and propose a set of effective and efficient solutions. Query cleaning involves semantic linkage and spelling corrections of database relevant query words, followed by segmentation of nearby query words such that each segment corresponds to a high quality data term. We define a quality metric of a keyword query, and propose a number of algorithms for cleaning keyword queries optimally. It is demonstrated that the basic optimal query cleaning problem can be solved using a dynamic programming algorithm. We further extend the basic algorithm to address incremental query cleaning and top-k optimal query cleaning. The incremental query cleaning is efficient and memory-bounded, hence is ideal for scenarios in which the keywords are streamed. The top-k query cleaning algorithm is guaranteed to return the best k cleaned keyword queries in ranked order. Extensive experiments are conducted on three real-life data sets, and the results confirm the effectiveness and efficiency of the proposed solutions.	dbl-browser;display resolution;dynamic programming;experiment;internet movie database (imdb);linkage (software);list of java keywords;natural language;performance;personalization;plasma cleaning;real life;relevance;scoring functions for docking;search algorithm;semantic translation;sputter cleaning;streaming media;web search query	Ken Q. Pu;Xiaohui Yu	2008	PVLDB	10.14778/1453856.1453955	online aggregation;sargable;query optimization;query expansion;web query classification;ranking;boolean conjunctive query;computer science;query by example;dynamic programming;data mining;database;keyword density;rdf query language;natural language;web search query;view;information retrieval;query language;object query language;spatial query	DB	-32.060949046141886	4.828312160753516	112933
ed3e90b9bdee41e4de789e4052cb6af2e87d506c	a concurrent agent model based on twin-subset semantic	modelizacion;distributed system;multiagent system;systeme reparti;multi agent system;agent modeling;semantics;simultaneidad informatica;intelligence artificielle;semantica;semantique;modelisation;concurrency;sistema repartido;side effect;artificial intelligence;inteligencia artificial;sistema multiagente;modeling;simultaneite informatique;systeme multiagent	Under the multi-agent system (MAS) circumstances, the concurrent behaviors turn out to be very important. Concurrent behaviors can be divided into irrelative and correlative, most of them are correlative and agents should be able to deduce their behaviors. This paper put forward a concurrent Agent model based on twin-subset semantic. This model aims at the characteristics of concurrent behaviors in MAS. Based on twin-subset semantic, the logical omniscience problem and other related problems (such as side-effect problem) can be avoided. This model divides the time set into macro-time and micro-time sets, and describes the concurrency in macro-time level. In this model, parallel actions in macro-time are interleaving in micro-time level. It can be applied to establish the logical foundation for the cooperation and competition based on concurrency in MAS.		Youmin Ke;Shanli Hu	2006		10.1007/11802372_48	simulation;systems modeling;concurrency;computer science;artificial intelligence;multi-agent system;semantics;programming language;side effect;algorithm	NLP	-21.183860221581146	11.35386703124456	113000
eba5d965b62d2b1f17468b21cfde4054f77e258b	user-defined visual query languages	specific applications;object oriented concepts;visual databases query languages visual languages query processing object oriented databases inheritance;query processing;database languages visual databases displays data visualization object oriented databases computer science relational databases layout object oriented modeling application software;application software;predefined visual representations;rule based;visual query languages;user defined pictures;layout;genericity;user defined visual query languages;graphs;bar charts;query languages;user defined visual languages;visual languages;user defined visual languages user defined visual query languages predefined visual representations specific applications meta language object oriented database user defined pictures rule based constraint visual language graphs bar charts pie charts plot charts object oriented concepts inheritance genericity visual query languages;visual representation;displays;data visualization;relational databases;object oriented databases;object oriented database;computer science;pie charts;plot charts;inheritance;database languages;object oriented modeling;constraint visual language;meta language;visual databases;visual query language	Previous research in visual query languages has fo-cused on pre-deened visual representations of data and queries, which are suitable for speciic applications, but diicult to extend and generalize. In this paper we propose a metalanguage to query an object-oriented database with user-deened pictures. By manipulating these pictures, the user can extract information about the data in a purely visual fashion. The proposed metalanguage is rule-based and uses a constraint visual language that allows for complex displays such as graphs, bar charts, pie charts, and plot charts to be speciied. Our approach extends object-oriented concepts, such as inheritance and genericity to visual query languages. We provide examples that illustrate the expressiveness of the user-deened visual languages to display and query data.	chart;generic programming;graph (discrete mathematics);logic programming;query language;visual language	Isabel F. Cruz	1994		10.1109/VL.1994.363614	natural language processing;query optimization;query expansion;web query classification;computer science;database;programming language;query language	DB	-32.20129274709267	8.8172566521125	113161
7c06f9320781e8084315f93e0f086e6989c66e76	maintaining schemata consistency for interoperable database systems	database system	 Many interoperable database systems offer the possibilityof defining integrated schemata on top of heterogeneousdatabases. A very important challenge for these interoperabledatabase systems is to maintain the autonomy ofthe component databases while preserving the correct semanticsof the integrated schemata. This paper presents amechanism that responds automatically to design changesmade in component databases which are relevant to one ormore integrated schemata. Further, this mechanism ... 	interoperability	Arantza Illarramendi;José Miguel Blanco;Eduardo Mena;Alfredo Goñi;J. M. Perez	1995			database theory;database transaction;computer science;data mining;database;database schema;information retrieval;database testing;database design	DB	-33.490072048254326	11.166900288891764	113391
d2edb3cebe8c7b48cc077d6a5b1d01fe6362162f	an executable logic-based model for cutter suction dredging using lps		LPS (Logic-based Production System) is a framework that combines logic programs with reactive rules and a destructivelyupdated database. The logic programs provide proactive behavior and allow definitions of processes, and the reactive rules provide reactive behavior. This paper describes a first attempt in using LPS to model the operations of cutter suction dredging (CSD). It is the result of a year-long consultation with experts from the Dredging Engineering Research Centre at Hohai University. LPS was chosen for this application because its combination of proactivity and reactivity was thought to be a good match for CSD operations. These require processes for normal operations, as well as constant monitoring to identify any operational problems that may be arising and taking reactive correction steps.	cambridge structural database;cutter expansive classification;executable;experiment;lightweight portable security;logic programming;operational semantics;production system (computer science);prolog;reactive programming;sensor;simulation;xsb	Fariba Sadri	2016			database;dredging;suction;executable;computer science	DB	-24.400906240767736	17.309196576440783	113449
a37597d4498ff2df2f46169e2ffc61af56f7c8c1	cardinal relations between regions with a broad boundary	query language;union of regions;spatial relation;spatio temporal relationships	Despite the innovative relevance of the results about spatial relations today available, the expressiveness of spatial query languages needs to be pushed further significantly in order to cope with the complexity of spatial entities. In particular, more research is necessary in order to support queries against data with uncertainty. In connection with cardinal directions, the best method today available is the Boolean, 3 × 3 direction-relation matrix proposed very recently by Goyal and Egenhofer. Our paper extends such a method to the case of regions with a broad boundary by introducing a 4-value, 5 × 5 direction-relation matrix. Furthermore, the present contribution studies the notion of consistency for a direction-relation matrix and proposes a set of conditions which are necessary and sufficient for assessing consistency.	entity;query language;relevance;spatial query	Serafino Cicerone;Paolino Di Felice	2000		10.1145/355274.355539	spatial relation;computer science;data mining;database;mathematics;algorithm;query language;remote sensing	DB	-27.689774071584658	8.300601955059147	113462
fb8847eb47efd8e940aed0d88acba1e6fa354434	skyline with presorting: theory and optimizations	query language;relational database	There has been interest recently in skyline queries, also called Pareto queries, on relational databases. Relational query languages do not support search for “best” tuples, beyond the order by statement. The proposed skyline operator allows one to query for best tuples with respect to any number of attributes as preferences. In this work, we explore what the skyline means, and why skyline queries are useful, particularly for expressing preference. We describe the theoretical aspects and possible optimizations of an efficiant algorithm for computing skyline queries presented in [6].	algorithm;clustered file system;order by;pareto efficiency;query language;relational database;skyline operator	Jan Chomicki;Parke Godfrey;Jarek Gryz;Dongming Liang	2005		10.1007/3-540-32392-9_72	query optimization;relational database;computer science;data mining;database;programming language;query language	DB	-27.315393675984982	5.154474528778144	113463
2bbfa273db3ddf21c58374a0d035f639abb2283b	the impact of logic programming on databases	base relacional dato;optimisation;database semantics;deductibility;well founded approach;stratified databases;optimizacion;implementation;query formulation;cooperative answers;semantics;formulacion pregunta;logical programming;relational database;semantica;semantique;formulation question;deductibilite;ejecucion;programmation logique;disjunctive databases;deducibilidad;update validation;base donnee relationnelle;optimization;base donnee deductive;semantic query optimization;logic programs;programacion logica		database;logic programming	John Grant;Jack Minker	1992	Commun. ACM	10.1145/131295.131297	concurrent constraint logic programming;constraint programming;database theory;declarative programming;programming domain;reactive programming;relational database;computer science;functional logic programming;data mining;database;semantics;programming paradigm;procedural programming;symbolic programming;inductive programming;datalog;fifth-generation programming language;programming language;implementation;prolog;logic programming	DB	-26.978672098657345	11.398079904673997	113552
478b0b0f402e10d8ff631dddd673787f9d224aee	inference control via query restriction vs. data modification: a perspective.	inference control			Norman S. Matloff	1987			pattern recognition;data mining;database	ML	-27.253642001269775	12.875857418825808	113602
03e27b59a06884f1b3f840f1d5a2fa103e3cc8ca	integrity constraints in owl ontologies based on grounded circumscription	grounded circumscription;dantong ouyang xianji cui yuxin ye web本体语言 完整性约束 owl 接地 封闭世界假设 验证方法 知识基础 一致性 integrity constraints in owl ontologies based on grounded circumscription;integrity constraints;semantic web;semantic web description logic ontology integrity constraints grounded circumscription;description logic;ontology	The extensions for logic-based knowledge bases with integrity constraints are rather popular. We put forward an alternative criteria for analysis of integrity constraints in Web ontology language (OWL) ontology under the closed world assumption. According to this criteria, grounded circumscription is applied to define integrity constraints in OWL ontology and the satisfaction of the integrity constraints by minimizing extensions of the predicates in integrity constraints. According to the semantics of integrity constraints, we provide a modified tableau algorithm which is sound and complete for deciding the consistency of an extended ontology. Finally, the integrity constraint validation is converted into the corresponding consistency of the extended ontology. Comparing our approach with existing integrity constraint validation approaches, we show that the results of our approach are more in accordance with user requirements than other approaches in certain cases.	algorithm;circumscription (logic);closed-world assumption;data integrity;description logic;knowledge base;long division;method of analytic tableaux;ontology (information science);ptc integrity;requirement;user requirements document;web ontology language	Dantong Ouyang;Xianji Cui;Yuxin Ye	2013	Frontiers of Computer Science	10.1007/s11704-013-2284-2	description logic;computer science;ontology;artificial intelligence;semantic web;ontology;data integrity;data mining;database;owl-s;process ontology;algorithm	AI	-23.147316065682322	7.3897840264413714	113649
833d6295c9355472451d9aecf98050c50b8be9cd	a new technique for enhancing linked-list data retrieval: reorganize data using artificially synthesized queries	distribution;stochastic automaton;optimisation;dato;dct filters;optimizacion;information retrieval;data;automata estocastico;automate stochastique;donnee;recherche information;filter;estructura datos;filtre;optimization;structure donnee;recuperacion informacion;distribucion;data structure;data retrieval;filtro	Let R = {R 1 , R 2 ,..., R N } be a set of data elements. The elements of R are accessed by the users of the system according to a fixed but unknown distribution S = {s 1 , s 2 ,..., s N }, referred to as the users' query distribution. In this paper we consider the problem of organizing data so as to optimize its retrieval. However, rather than organize the data according to Q, the stream of queries presented by the user, we suggest a scheme by which the data is organized based on a synthesized query stream Q'. This synthesized stream possesses an underlying distribution, S'. Thus, in effect, the data organization is achieved according to the distribution S' and so, in one sense, the user's query distribution is modified without his knowing it	artificial gene synthesis;data retrieval;linked list	B. John Oommen;David T. H. Ng	1994	Comput. J.	10.1093/comjnl/37.7.598	distribution;data structure;filter;computer science;theoretical computer science;data mining;database;programming language;data retrieval;data	DB	-26.491737330267995	5.555393721405045	113736
32772ea2396bcabdd38d076c4e131d896cfafabe	saor: template rule optimisations for distributed reasoning over 1 billion linked data triples	linked data;rule based;indexation	In this paper, we discuss optimisations of rule-based materialisation approaches for reasoning over large static RDF datasets. We generalise and reformalise what we call the “partial-indexing” approach to scalable rule-based materialisation: the approach is based on a separation of terminological data, which has been shown in previous and related works to enable highly scalable and distributable reasoning for specific rulesets; in so doing, we provide some completeness propositions with respect to semi-naı̈ve evaluation. We then show how related work on template rules – T-Box-specific dynamic rulesets created by binding the terminological patterns in the static ruleset – can be incorporated and optimised for the partial-indexing approach. We evaluate our methods using LUBM(10) for RDFS, pD* (OWL Horst) and OWL 2 RL, and thereafter demonstrate pragmatic distributed reasoning over 1.12 billion Linked Data statements for a subset of OWL 2 RL/RDF rules we argue to be suitable for Web reasoning.	algorithm;linked data;logic programming;rdf schema;recursion;rule interchange format;scalability;semiconductor industry;web ontology language	Aidan Hogan;Jeff Z. Pan;Axel Polleres;Stefan Decker	2010		10.1007/978-3-642-17746-0_22	rule-based system;computer science;artificial intelligence;linked data;data mining;database;world wide web;algorithm	AI	-23.55233663550707	8.341115601764747	113744
0076e1a01427e95944be0dc22f72d06a0173891c	document listing on versioned documents		Representing versioned documents, such as Wikipedia history, web archives, genome databases, backups, is challenging when we want to support searching for an exact substring and retrieve the documents that contain the substring. This problem is called document listing. We present an index for the document listing problem on versioned documents. Our index is the first one based on grammar-compression. This allows for good results on repetitive collections, whereas standard techniques cannot achieve competitive space for solving the same problem. Our index can also be addapted to work in a more standard way, allowing users to search for word-based phrase queries and conjunctive queries at the same time. Finally, we discuss extensions that may be possible in the future, for example, supporting ranking capabilities within the index itself.		Francisco Claude;J. Ian Munro	2013		10.1007/978-3-319-02432-5_12	computer science;data mining;database;substring index;world wide web;information retrieval	Web+IR	-31.634136244011067	4.338353697290894	113983
3cb859e47a23c487be6c6236cd828b7a68330956	solving deductive planning problems using program analysis and transformation	reasoning about action;program analysis;logic programs	Two general, problematic aspects of deductive planning, namely, detecting unsolvable planning problems and solving a certain kind of postdiction problem, are investigated. The work is based on a resource oriented approach to reasoning about actions and change using a logic programming paradigm. We show that ordinary resolution methods are insufficient for solving these problems and propose program analysis and transformation as a more promising and successful way to solve them.	deductive database;logic programming;problem solving;program analysis;programming paradigm;sensor	D. Andre de Waal;Michael Thielscher	1995		10.1007/3-540-60939-3_15	program analysis;computer science;theoretical computer science;reasoning system;programming language;deductive reasoning;algorithm	AI	-20.430247674721645	15.116845134561869	114270
53152f9dd1a161d7dd6b9bbf9a7ddd47b8d1a766	metadata propagation in large, multi-layer database systems	database system;query processing;database systems permission humans html security query processing costs programming profession algorithm design and analysis uniform resource locators;html;permission;programming profession;database systems;humans;uniform resource locators;security;algorithm design and analysis	1. The problem Large database systems (e.g., federations, warehouses) have several virtual or physical databases, each derived from other layers. Today, metadata tend to be locked into the schema where it is supplied, and are rarely available through other schemas. Propagating metadata to other schemas is difficult for three reasons. First, syntactically similar metadata values can propagate very differently. For example, the access rights associated with a join view may be the intersection of the source access rights, whereas the completeness of the view may depend on the join domain’s cardinality. Second, a derived table’s metadata may be an order of magnitude larger than its view derivation. This scale precludes any solution that requires humans to extend view definitions to specify metadata transformations. Third, when updates to metadata can be propagated to another schema, it may be necessary to negotiate about whether (or how) the update should be performed. Several administrator roles may be involved (at least one per schema), and each needs to see metadata relative to their schema of interest.	database;federation (information technology);software propagation	Arnon Rosenthal;Edward Sciore	2000		10.1109/ICDE.2000.839395	algorithm design;database theory;html;database tuning;computer science;information security;data mining;database;view;world wide web;database design	DB	-28.498838496761383	8.83808697639222	114430
509265a3083288ad75488cfcd51fe56624aec0cc	terminological logic involving time and evolution: a preliminary report	time dependent;temporal logic;datavetenskap datalogi;computer science;knowledge representation;temporal reasoning	Although terminological logics as well as temporal reasoning has received considerable attention in the knowledge representation community in the last years, few attempts have been made to integrate those fields. We study the combination of the temporal logic LITE and a terminological logic to obtain a temporal terminological logic. We emphasize defining a terminological logic (T-LITE) where the extensions of concepts are time-dependent in the following sense : first, the individuals belonging to a concept are appearances of objects in a temporal context; secondly, we allow concepts to be defined in terms of developments of objects. Formal semantics for T-LITE are provided. This is a preprint of the paper published by Springer: Lambrix P, Rönnquist R, Terminological Logic Involving Time and Evolution: A Preliminary Report, Proceedings of the Seventh International Symposium on Methodologies for Intelligent Systems ISMIS93, LNAI 689, 162-171, Trondheim, Norway, 1993. The final publication is available at www.springerlink.com. doi: 10.1007/3-540-56804-2 16	knowledge representation and reasoning;lecture notes in computer science;springer (tank);temporal logic	Patrick Lambrix;Ralph Rönnquist	1993		10.1007/3-540-56804-2_16	knowledge representation and reasoning;description logic;temporal logic;interval temporal logic;computer science;artificial intelligence;data mining;multimodal logic;algorithm;autoepistemic logic	Logic	-19.723302415752254	8.371804860827272	114904
6eb3063aaa8569b72a142d464f842228a720a887	selectivity estimation using homogeneity measurement	databases;categorial attributes;selectively estimation;parametric statistics;query processing;database management systems;data management;multidimensional data;query optimization;data engineering;trees mathematics;tree search;homogeneity measurement;additives;statistical distributions;databases organizing query processing computer science data engineering decision making additives parametric statistics statistical distributions gaussian distribution;organizing;trees mathematics database management systems database theory information retrieval systems;information retrieval systems;computer science;unknown distribution;multidimensional tree;summary data estimation;database theory;gaussian distribution;homogeneously distributed;categorial attributes homogeneity measurement multidimensional data unknown distribution homogeneously distributed multidimensional tree summary data estimation selectively estimation tree search	In estimating the cardinality of tuples in a database satisfying a query, it is apparent that under the unifm distribution assumption, the more homogeneous the data distribution is, the more accurate the estimation will be. Unfortunately, the distribution of data in a database is not homogeneous, nor is independent among its attributes. in general, which make most of the present estimation models inaccurate. This paper presents a new approach for organizing a large collection of multi-dimensional data with an unknown distribution by partitioning the data such that the data is relatively homogeneously distributed in each block. A multi-dimensional tree is generated according to this partition. After the tree is generated, summary data estimation, such as selectivity estimation, can be performed via a tree search. This approach is applicable to both ordered and categorical attributes. The merits of this method are verified theoretically and by simulation.	organizing (structure);selectivity (electronic);simulation	Meng Chang Chen;Lawrence McNamee;Norman S. Matloff	1990		10.1109/ICDE.1990.113482	normal distribution;probability distribution;query optimization;database theory;data management;food additive;computer science;theoretical computer science;data mining;database;parametric statistics;statistics	DB	-27.70472336283227	4.256797199595054	115043
0b34690df758b7d1fe67265585962efd5cf7022a	vague queries on peer-to-peer xml databases	query language;xml database;satisfiability;semantic mapping;peer to peer	"""We propose a system, named VXPeer, for querying peer-to-peer XML databases. VXPeer ensures high autonomy to participating peers as it does not rely on a global schema or semantic mappings between local schemas. The basic intuition is that of """"vaguely"""" evaluating queries, i.e., computing partial answers that satisfy new queries obtained by transformation of the original ones, then combining these answers, possibly on the basis of limited knowledge about the local schemas used by peers (e.g., key constraints). A specific query language, named VXPeerQL, allows the user to declare constraints on the query transformations applicable. The system retrieves partial answers, using an intelligent routing strategy, then attempts at combining those referring to the same real-world object."""	peer-to-peer;vagueness;xml database	Bettina Fazzinga;Sergio Flesca;Andrea Pugliese	2007		10.1007/978-3-540-74469-6_29	computer science;data mining;xml database;database;information retrieval;query language;satisfiability	DB	-26.314814153099	7.468052494169522	115329
d1a59e34acb4815f156838d06d10337395324989	a proposal for an xml data definition and manipulation language	base relacional dato;langage definition donnee;query language;sql;langage manipulation donnee;xml language;interrogation base donnee;lenguaje manipulacion dato;interrogacion base datos;stockage donnee;relational database;data definition language;lenguaje interrogacion;data storage;lenguaje definicion dato;base donnee relationnelle;almacenamiento datos;langage interrogation;echange donnee informatise;database query;langage xml;lenguaje xml;electronic data interchange;data manipulation language;cambio dato electronico	XML has become a popular data interchange and storage format, which in recent times has precipitated the rise of XML-enabled relational databases as well as native XML databases. This paper outlines a data definition and manipulation language for XML repositories that enables users to perform data management tasks such as creation and deletion of indices, collections and documents. The language proposed also provides the ability to perform queries, transformations and updates on the documents in the XML repository either singly or across an entire collection. A syntax for the language is presented as extensions to the W3C's XML Query language (XQuery) and also as a new language with syntax borrowed heavily from SQL for the relational model and DL/1 of IBM's IMS system for the hierarchical model. A prototype implementation of the language has been partially completed.		Dare Obasanjo;Shamkant B. Navathe	2002		10.1007/3-540-36556-7_1	xml catalog;xml validation;xml encryption;regular language description for xml;data definition language;sql;xml;data manipulation language;data control language;streaming xml;relational database;computer science;document type definition;document definition markup language;document structure description;xml framework;electronic data interchange;computer data storage;xml database;xml schema;database;xml signature;programming language;world wide web;xml schema editor;query language;efficient xml interchange;sgml	DB	-33.3725590018393	8.83750410499835	115364
483de1be98f31584162733ac73a39a5f7a574216	construct queries in sparql	004;rdf sparql query languages	SPARQL has become the most popular language for querying RDF datasets, the standard data model for representing information in the Web. This query language has received a good deal of attention in the last few years: two versions of W3C standards have been issued, several SPARQL query engines have been deployed, and important theoretical foundations have been laid. However, many fundamental aspects of SPARQL queries are not yet fully understood. To this end, it is crucial to understand the correspondence between SPARQL and well-developed frameworks like relational algebra or first order logic. But one of the main obstacles on the way to such understanding is the fact that the well-studied fragments of SPARQL do not produce RDF as output. In this paper we embarrk on the study of SPARQL CONSTRUCT queries, that is, queries which output RDF graphs. This class of queries takes rightful place in the standards and implementations, but contrary to SELECT queries, it has not yet attracted a worth-while theoretical research. Under this framework we are able to establish a strong connection between SPARQL and well-known logical and database formalisms. In particular, the fragment which does not allow for blank nodes in output templates corresponds to first order queries, its well-designed sub-fragment corresponds to positive first order queries, and the general language can be restated as a data exchange setting. These correspondences allow us to conclude that the general language is not composable, but the aforementioned blank-free fragments are. Finally, we enrich SPARQL with a recursion operator and establish fundamental properties of this extension. 1998 ACM Subject Classification H.2.3 Languages – Query languages	first-order logic;query language;recursion;relational algebra;sparql;standard data model;whole earth 'lectronic link;world wide web	Egor V. Kostylev;Juan L. Reutter;Martín Ugarte	2015		10.4230/LIPIcs.ICDT.2015.212	named graph;turtle;computer science;sparql;data mining;database;rdf query language;information retrieval;rdf schema	DB	-23.747565525997935	9.182871186995195	115457
80b8ea1e93ec234bc99e66652d827699f5709987	query processing of geometric objects with free form boundaries in spatial datbases	complex objects;database system;query processing;spatial database;data model;mathematik;mechanical engineering;access method;efficient query processing;database management system;spatial access method;ddc 510	The increasing demand for the use of database systems as an integrating factor in CAD/CAM applications has necessitated the development of database systems with appropriate modelling and retrieval capabilities. One essential problem is the treatment of geometric data which has led to the development of spatial databases. Unfortunately, most proposals only deal with simple geometric objects like multidimensional points and rectangles. On the other hand, there has been a rapid development in the field of representing geometric objects with free form curves or surfaces, initiated by engineering applications such as mechanical engineering, aviation or astronautics. Therefore, we propose a concept for the realization of spatial retrieval operations on geometric objects with free form boundaries, such as B-spline or Bézier curves, which can easily be integrated in a database management system. The key concept is the encapsulation of geometric operations in a so-called query processor. First, this enables the definition of an interface allowing the integration into the data model and the definition of the query language of a database system for complex objects. Second, the approach allows the use of an arbitrary representation of the geometric objects. After a short description of the query processor, we propose some representations for free form objects determined by B-spline or Bézier curves. The goal of efficient query processing in a database environment is achieved using a combination of decomposition techniques and spatial access methods. Finally, we present some experimental results indicating that the performance of decomposition techniques is clearly superior to traditional query processing strategies for geometric objects with free form boundaries.	b-spline;bézier curve;computer-aided design;data model;encapsulation (networking);query language;spatial database	Hans-Peter Kriegel;Stephan Heep;Andreas Fahldiek;Norbert Mysliwitz	1993		10.1007/3-540-57234-1_31	query optimization;object-based spatial database;query expansion;database tuning;data model;computer science;query by example;theoretical computer science;data mining;database;view;access method;spatial database;spatial query	DB	-28.467197751455046	8.161701842422671	115479
9cf71b5c7bd082cf23c99b7b095b753bfb1d283e	an inference system for relationships between spatial granularities	inference system;spatial granularities;spatial relationships;point of view	Spatial granularities allow one to qualify classical data adding them space locations. In order to compare data qualified with different granularities and to associate data to different granularities (e.g., in analysis similar to drill-down and roll-up operations), it is necessary to know how the involved granularities are related. However, the explicit evaluation of these relationships may be heavy from a computational point of view. Moreover, the explicit evaluation of these relationships could not be requested, as relationships can be derived from already established ones. Thus, in this paper, we propose an inference system for deriving spatial relationships that definitely hold, starting from a given set of relationships between spatial granularities, without evaluating them explicitly.	computation;data drilling;inference engine;keyboard technology	Gabriele Pozzani;Carlo Combi	2011		10.1145/2093973.2094040	spatial relation;computer science;theoretical computer science;data mining;database;remote sensing	DB	-27.95456406325349	7.59330920392451	115532
a6c90e43d5c341ab8db6b81360f226104bf69e35	deductive information retrieval based on classifications	relation algebra;mathematical formulas;classification;databases;data management;database management systems;information retrieval;query language;database system	Modern fact databases cont ain abundant data classified through several classifications. Typically users must consult thes e classifications in separate manuals or files thus making their effective use diffi cult. Contemporary database systems do little to support deductive use of classifi cations. In this paper we show how deductive data management techniques can be applied to the utilization of data value classifications. Computation of transitive class relationships is here of primary importance. We define a repres entation of classifi cations which supports transitive computation and present an operation-oriented deuctive query language tailored for classification-ba sed deductive information retr ieval. The operations of this language are on the same abstraction level as rela tional algebra operations and can be integrated with these to form a powerful and flexible query language for deductive information retrieva l. We define the integra tion of the operations and demonstrate the usefulness of the language in t rms of seve ral sample queries. Key-words : fact databases, information retrieval, classi f cations, deduction.	abstraction layer;computation;database;information retrieval;natural deduction;query language;sed	Kalervo Järvelin;Timo Niemi	1993	JASIS	10.1002/(SICI)1097-4571(199312)44:10%3C557::AID-ASI2%3E3.0.CO;2-M	document retrieval;formula;systems modeling;data processing;biological classification;information science;data management;relational database;computer science;data mining;relation algebra;database;algorithm;query language;multiple	DB	-27.86986797955518	7.737605867432517	115647
65e8c7d1161e659847558d843ef2d1a7b53fcaca	magic rewritings for efficiently processing reactivity on web ontologies	bottom up;query processing;active knowledge;expressive power;data dependence;query evaluation;semantic web;query answering;ontology;query rewriting;magic set rewritings	In this paper, we describe an approach aiming at enriching the Semantic Web with active information. We propose ACTION, an ACTIve ONtology formalism to express reactive behavior. In ACTION, events are categorized as concepts of an ontology and, in conjunction with classes, properties and instances, are considered during the query answering and reasoning tasks. We hypothesize that ACTION provides a more expressive solution to the problem of representing and querying active knowledge than existing ECA-based approaches. However, this expressivity power can negatively impact on the complexity of the query processing and reasoning tasks because the number of derived data depends on the number and relationships of the events. The main source of complexity is produced because the number of the derived facts is polynomial with respect to the size of the events, and the same evaluations may be fired by different events. To overcome this problem, we propose optimization strategies to identify Magic Set rewritings where the number of duplicate evaluations is minimized. We present the query rewriting technique called Intersection of Magic Rewritings (IMR), which is based on Magic Sets rewritings that annotate the minimal set of rules that need to be evaluated to process reactive behavior on an ontology. We have conducted an experimental study and have observed that the proposed strategies are able to speed up the tasks of reasoning and query evaluation in two orders of magnitude for small ontologies, and in four orders of magnitude for medium and large ontologies, with respect to the bottom-up strategy.	ontology (information science)	Elsa Liliana Tovar;Maria-Esther Vidal	2008		10.1007/978-3-540-88873-4_29	query optimization;web query classification;computer science;data mining;database;web search query;information retrieval	DB	-24.2829821806833	8.446362196671336	115890
eb58e8414cc05c60e892a8fc2cf1665e501394b4	magic templates: a spellbinding approach to logic programs	base donnee;strategie ascendante;database;base dato;pregunta documental;estrategia;logical programming;question documentaire;strategy;deductive database;programmation logique;base dato deductiva;query;clause horn;evaluation;base donnee deductive;evaluacion;logic programs;programacion logica;strategie	We consider a bottom-up query-evaluation scheme in which facts of relations are allowed to have nonground terms. The Magic Sets query-rewriting technique is generalized to allow arguments of predicates to be treated as bound even though the rules do not provide ground bindings for those arguments. In particular, we regard as ‘‘bound’’ any argument containing a function symbol or a variable that appears more than once in the argument list. Generalized ‘‘magic’’ predicates are thus defined to compute the set of all goals reached in a top-down exploration of the rules, starting from a given query goal; these goals are not facts of constants as in previous versions of the Magic Sets algorithm. The magic predicates are then used to restrict a bottom-up evaluation of the rules so that there are no redundant actions; that is, every step of the bottom-up computation must be performed by any algorithm that uses the same sideways information passing strategy (sips). The price paid, compared to previous versions of Magic Sets, is that we must store relations with nonground facts; and we must perform unifications, rather than equijoins, when evaluating the rules bottom-up. The method is applicable to general Horn clause logic programs.	backtracking;bottom-up parsing;bottom-up proteomics;computation;control theory;datalog;fixed point (mathematics);horn clause;jean;logic programming;prolog;rewriting;sethi–ullman algorithm;software propagation;top-down and bottom-up design	Raghu Ramakrishnan	1988		10.1016/0743-1066(91)90026-L	strategy;computer science;artificial intelligence;evaluation;mathematics;programming language;algorithm	DB	-20.76502639983388	11.986869718777966	116050
a58711ff41603261bf12a32ef8267dd3d8d5b812	a bootstrapping model for children's vocabulary acquisition using inductive logic programming			inductive logic programming;vocabulary	Ikuo Kobayashi;Koichi Furukawa;Tomonobu Ozaki;Mutsumi Imai	2001	Electron. Trans. Artif. Intell.		machine learning;artificial intelligence;inductive programming;natural language processing;functional logic programming;procedural programming;programming language;computer science;logic programming;programming paradigm;inductive logic programming;fifth-generation programming language;prolog	ML	-19.18849391678422	11.653490117749444	116364
5b62ff171b647d54cc4da28beda6d5ebad266878	automatic generation of rule-based solvers for intentionally defined constraints	constraint logic programs;rule based;automatic generation;machine learning;rule based programming;constraint solving	A general approach to implement propagation and simplification of constraints consists of applying rules over these constraints. However, a difficulty that arises frequently when writing a constraint solver is to determine the constraint propagation algorithm. In previous work, different methods for automatic generation of rule-based solvers for constraints defined over finite domains have been proposed1,2,3,4. In this paper, we present a method for generating rule-based solvers for constraint predicates defined by means of a constraint logic program, even when the constraint domain is infinite.		Slim Abdennadher;Christophe Rigotti	2002	International Journal on Artificial Intelligence Tools	10.1142/S0218213002000903	rule-based system;constraint logic programming;concurrent constraint logic programming;mathematical optimization;constraint programming;binary constraint;ac-3 algorithm;constraint satisfaction;constraint learning;computer science;constraint graph;machine learning;constraint satisfaction dual problem;constraint;constraint satisfaction problem;algorithm;hybrid algorithm;local consistency	AI	-19.269864416210083	16.596070231496654	116373
c8e2a7dc20949fafd91bfb0fa48df12bec17eef2	performance analysis of groupby-after-join query processing in parallel database systems	aggregation function;query processing;query optimization;parallel database system;school of engineering and science;groupby queries;parallel databases;respubid18925;performance analysis;parallel query processing;groupby join queries;parallel query optimization;parallel processing;cost model	"""Queries containing aggregate functions often combine multiple tables through join operations. This query is subsequently called """"Groupby-Join"""". There is a special category of this query whereby the group-by operation can only be performed after the join operation. This is known as """"Groupby-After-Join"""" queries--the focus of this paper. In parallel processing of such queries, it must be decided which attribute is used as a partitioning attribute, particularly join attribute or group-by attribute. Based on the partitioning attribute, two parallel processing methods, namely join partition method (JPM) and aggregate partition method (APM) are discussed. The behaviours of these parallelization methods are described in terms of cost models. Experiments are performed based on simulations. The simulation results show that the aggregate partition method performs better than the join partition method."""		David Taniar;Rebecca Boon-Noi Tan;Clement H. C. Leung;Kevin H. Liu	2004	Inf. Sci.	10.1016/j.ins.2003.09.029	hash join;recursive join;online aggregation;parallel processing;query optimization;computer science;theoretical computer science;data mining;database;sort-merge join;query language	DB	-28.031543618071904	4.725761915852931	116437
64c74de3255cd7dfe7622b97108db4588039bfcf	off-line reasoning for on-line efficiency	query language;system approach;multi agent system;social system;intelligent system;knowledge base	The complex i ty of reasoning is a fundamenta l issue in A I . In many cases, the fact tha t an intel l igent system needs to per form reasoning on-l ine contr ibutes to the di f f icul ty of this reasoning. In this paper we investigate a couple of contexts in which an i n i t i a l phase of off-l ine preprocessing and design can improve the on-l ine complexi ty considerably. The first context is one in which an intel l igent system computes whether a query is entai led by the system's knowledge base. We present the not ion of an efficient basts for a query language, and show tha t off-l ine preprocessing can be very effective for query languages tha t have an efficient basis. The usefulness of this not ion is i l lust rated by showing tha t a fair ly expressive language has an efficient basis. The second context is closely related to the ar t i f ic ia l social systems approach introduced in [MT90 ] . We present the design of a social law for a mul t i -agent envi ronment as p r imar i l y an instance of offl ine processing, and study this problem in a par t icu lar mode l . We briefly review the ar t i f ic ia l social systems approach to design of mul t i -agent systems, introduced in [MT90 ] . C o m p u t i n g or coming up w i th a social law is viewed as a p r imar i l y off-l ine act iv i ty tha t has ma jo r impac t on the effectiveness of the on-l ine act i v i t y of the agents. The tradeoff' between the amount of effort invested in comput ing the social law and the cost of the onl ine ac t iv i t y can thus be viewed as an off-l ine vs. on-l ine tradeoff.	knowledge base;preprocessor;query language;social system	Yoram Moses;Moshe Tennenholtz	1993			knowledge base;computer science;artificial intelligence;machine learning;data mining;social system;mathematics;programming language;query language	AI	-21.023438308303778	8.200209267207821	116517
1f3a48e3c1ac68b7adb974b3297105762956995a	a procedure for mediation of queries to sources in disparate contexts	hd28 m414 no 3964 97	This paper discusses the algorithm we are using for the mediation of queries to disparate information sources in a Context Interchange system, where information sources may have di erent interpretations arising from their respective context. Queries are assumed to be formulated without regard for semantic heterogeneity, and are rewritten to corresponding mediated queries by taking into account the semantics of data codi ed in axioms associated with sources and receivers (the corresponding context theories). Our approach draws upon recent advances in abductive logic programming and presents an integration of techniques for query rewriting and semantic query optimization. We also demonstrate how this can be e ciently implemented using the constraint logic programming system ECLiPSe.	abductive logic programming;abductive reasoning;adder (electronics);admissible numbering;algorithm;automated theorem proving;comparator;constraint logic programming;constraint satisfaction problem;deductive database;eclipse;functional dependency;horn clause;input/output;mathematical optimization;network analysis (electrical circuits);prototype;query optimization;query plan;rewriting;semantic heterogeneity;semantic query;solver;theory	Stéphane Bressan;Cheng Hian Goh;Thomas Lee;Stuart E. Madnick;Michael Siegel	1997			data mining;database;information retrieval	DB	-28.81651629781127	9.891906892838323	116875
d3a5930c4d8fbdbd797ed99509006886671dd728	enhancing flexibility and expressivity of contextual hierarchies	biomedical monitoring;context blood pressure integrated circuits data warehouses biomedical monitoring context modeling electronic mail;electronic mail;blood pressure;fuzzy set theory;knowledge engineering data warehouses fuzzy set theory;data warehouses;context modeling;integrated circuits;context;contextual generalization process contextual hierarchies data warehouses expert knowledge knowledge definition process fuzzy based methodology;knowledge engineering	Data warehouses are nowadays extensively used to perform analyses on huge volume of data. This success is partly due to the capacity of considering data at several granularity levels thanks to the use of hierarchies. However, in previous work, we showed that the experts knowledge was not much considered in the generalization process. To overcome this drawback, we introduced a new category of hierarchies, namely the contextual hierarchies. Unfortunately, in contrast to the complexity of expert knowledge that should be considered, the knowledge definition process was too rigid. In this paper, we extend these hierarchies and their related techniques to drastically increase their flexibility and expressivity. To this purpose, we adopt a fuzzy-based methodology which allows to express expert knowledge in a very convenient way. Experiment results obtained on synthetic datasets show that the contextual generalization process is very fast and can thus be used in practice.	aggregate data;authorization;experiment;expressive power (computer science);fits;fuzzy concept;generalization error;personalization;synthetic intelligence	Yoann Pitarch;Cécile Favre;Anne Laurent;Pascal Poncelet	2012	2012 IEEE International Conference on Fuzzy Systems	10.1109/FUZZ-IEEE.2012.6251176	computer science;knowledge management;artificial intelligence;blood pressure;machine learning;knowledge engineering;data mining;database;context model;fuzzy set	DB	-22.415050778342476	5.994222623093975	116878
f650c21452dd0e3f00cd2c091ea6e8a75c0fe84e	formal semantics and automatic verification of hierarchical multimedia scenarios with interactive choices. (sémantique formelle et vérification automatique de scénarios hiérarchiques multimédia avec des choix interactifs)		Formal Semantics and Automatic Verification of Hierarchical Multimedia Scenarios with Interactive Choices Interactive multimedia deals with the computer-based design of scenarios consisting of multimedia content that interacts with external actions and those of the performer (e.g., multimedia live-performance arts, interactive museum installations, and video games). The multimedia content is structured in a spatial and temporal order according to the author’s requirements. Therefore, the potentially high complexity of these scenarios requires adequate specification languages for their complete description and verification. Interactive scores is a formalism which has been proposed as a model for composing and performing interactive multimedia scenarios. In addition, an inter-media sequencer, called I-SCORE, has been developed following the Petri Net semantics proposed by this formalism. During the last years, I-SCORE has been used successfully for the composition and performance of live performances and interactive exhibitions. Nevertheless, these applications and emergent applications such as video games and interactive museum installations, increasingly demand two features that the current stable version of I-SCORE as well as its underlying model do not support: (1) flexible control structures such as conditionals and loops; and (2) mechanisms for the automatic verification of scenarios. In this dissertation we present two formal models for composition and automatic verification of multimedia interactive scenarios with interactive choices, i.e., scenarios where the performer or the system can take decisions about their execution state with a certain degree of freedom defined by the composer. In our first approach, we define a novel programming language called REACTIVEIS. This language extends the full capacity of temporal organization of interactive scenarios by allowing the composer to use a defined logical system for the specification of the starting and stopping conditions of temporal objects (TOs). REACTIVEIS programs are formally defined as tree-like structures representing the hierarchical aspect of interactive scenarios and whose nodes contain the conditions needed to start and stop the TOs. Moreover, we define an operational semantics based on labeled trees, containing in their nodes, the information about the start and stop times of each TO. We show that this operational semantics offers an intuitive yet precise description of the behavior of interactive scenarios. We also endowed REACTIVEIS with a declarative interpretation as formulas in Intuitionistic Linear Logic with Subexponentials (SELL). We shall show that such interpretation is adequate: derivations in the logic correspond to traces of the program and vice-versa. Hence, we can use all the meta-theory of Intuitionistic Linear Logic (ILL) to reason about interactive scenarios and develop tools for the verification and analysis of interactive scenarios. In our second approach, we present a Timed Automata (TA) based framework. In the proposed framework, we model interactive scenarios as a network of timed automata and extend them with interactive points (IPs) guarded by conditions, thus allowing for the specification of branching behaviors. Moreover, we take advantage of the mature and efficient tools for TA to simulate and automatically verify scenarios. In our framework, scenarios can be synthesized into a reconfigurable hardware in order to provide a low-latency and real-time execution by taking advantage of the physical parallelism, low-latency, and high-reliability of these devices. Furthermore, we implemented a tool to systematically construct bottom-up TA models from the composition environment of I-SCORE. Doing that, we provide a friendly and specialized environment for composing and automatic verification of interactive scenarios.		Jaime Arias Almeida	2015				Graphics	-20.179590668840767	16.955551927308424	117209
b94cc028bc572f3a4123558e4e5e1ffdbdf47efb	applying theory revision to the design of distributed databases	domain theory;base donnee repartie;base donnee;distributed database;algorithm analysis;prolog;heuristic method;database;object database;base repartida dato;base dato;inductive logic programming;metodo heuristico;fragmentacion;base donnee orientee objet;analyse algorithme;object oriented databases;methode heuristique;fragmentation;programmation logique inductive;analisis algoritmo	This work presents the application of theory revision to the design of distributed databases to automatically revise a heuristic-based algorithm (called analysis algorithm) through the use of the FORTE system. The analysis algorithm decides the fragmentation technique to be used in each class of the database and its Prolog implementation is provided as the initial domain theory. Fragmentation schemas with previously known performance, obtained from experimental results on top of an object database benchmark, are provided as the set of examples. We show the effectiveness of our approach in finding better fragmentation schemas with improved performance.	algorithm;belief revision;benchmark (computing);branch and bound;computer science;distributed database;domain theory;fragmentation (computing);heuristic (computer science);ip fragmentation;inductive logic programming;inductive reasoning;np-hardness;negation as failure;prolog	Fernanda Araujo Baião;Marta Mattoso;Jude W. Shavlik;Gerson Zaverucha	2003		10.1007/978-3-540-39917-9_6	computer science;artificial intelligence;theoretical computer science;domain theory;database;fragmentation;programming language;prolog;distributed database;algorithm	DB	-26.801023582151334	11.431661251845876	117360
883f1f3aa3d564cc9c925bc8feb1ca2cb79d24d0	time sequence ordering extensions to the entity-relationship model and their application to the automated manufacturing process	engineering;entity relationship model;fabricacion asistida por computador;base donnee;gestion production;coaccion;contrainte;database;base dato;modelo entidad relacion;relational database;modele entite relation;flujo informacion;productique;data model;flux information;fabrication assistee;information flow;constraint;production control;humanities;application industrielle;scheduling;gestion produccion;computer aided manufacturing;philosophy;integrity constraints;robotica;industrial application;ordonamiento;computer science;database design;computer integrated manufacturing;ordonnancement;aplicacion industrial;entity relationship;knowledge engineering	Moyne, J.R., T.J. Teorey and L.C. McAfee, Jr., Time sequence ordering extensions to the entity relationship model and their application to the automated manufacturing process, Data & Knowledge Engineering 6 (1991) 421-443. New extensions to the entity-relationship (E-R) model have been developed to represent time sequencing and ordering aspects of information flow, and to represent the integration of control (programming) information into a database. The model constructs specify an implementation ordering of records in a relational database table. Time sequencing refers to an implementation of process information flow as a result of this ordering. The modeling constructs are needed to more completely model process recipe information flow in a typical automated manufacturing facility. The development of these E-R extensions is pursued using a data model for the factory of the future as a motivation and development vehicle. The extensions are defined formally and possible variations of the constructs are given. Further, the incorporation of the constructs into the existing E-R model semantics and the transformation of these extensions to ordering properties and integrity constraints on a corresponding database implementation is discussed.	data integrity;data model;entity–relationship model;knowledge engineering;relational database;table (database)	James R. Moyne;Toby J. Teorey;Leo C. McAfee	1991	Data Knowl. Eng.	10.1016/0169-023X(91)90011-L	entity–relationship model;computer science;artificial intelligence;knowledge engineering;data mining;database	DB	-30.643397262972456	13.939052373799344	117545
cb4d1e90a10d1df037593d46ba1aa098cb3e4ef8	reasoning with graph constraints	graph constraints;graph transformation;specification and verification;visual modelling;structured documents	Graph constraints were introduced in the area of graph transformation, in connection with the notion of (negative) application conditions, as a form to limit the applicability of transformation rules. However, we believe that graph constraints may also play a significant role in the area of visual software modelling or in the specification and verification of semi-structured documents or websites (i.e. HTML or XML sets of documents). In this sense, after some discussion on these application areas, we concentrate on the problem of how to prove the consistency of specifications based on this kind of constraints. In particular, we present proof rules for two classes of graph constraints and show that our proof rules are sound and (refutationally) complete for each class. In addition, we study clause subsumption in this context as a form to speed up refutation.	anti-grain geometry;attributed graph grammar;constraint (mathematics);constraint logic programming;formal system;gnutella2;graph (discrete mathematics);graph rewriting;html;hidden line removal;modeling language;semiconductor industry;subsumption architecture;well-formed formula;xml	Fernando Orejas;Hartmut Ehrig;Ulrike Golas	2009	Formal Aspects of Computing	10.1007/s00165-009-0116-9	discrete mathematics;null graph;graph property;computer science;constraint graph;clique-width;theoretical computer science;mathematics;graph;moral graph;critical graph;intersection graph;algorithm;graph rewriting	DB	-23.071086218081053	13.663516025908066	117674
7e8a59639c40265261bda04762f9aac31709ed2a	updates, schema updates and validation of xml documents - using abstract state machines with automata-defined states	xml document;abstract state machine;xml	The exact validation of streaming XML documents can be realised by using visibly push-down automata (VPA) that are defined by Extended Document Type Definitions (EDTD). It is straightforward to represent such an automaton as an Abstract State Machine (ASM). In doing so we enable computations on abstract states that are defined by a certain class of automata, in this case VPAs. In this paper we elaborate on this approach by taking also updates of XML documents into account. In this way the ASM-approach combines vertical refinements, which first make states explicit and then instantiate by a specific EDTD, with horizontal refinements, which replace streaming XML documents by stored ones and then add updates. Furthermore, as the EDTD appears as part of the abstract state, updating it is another natural extension by horizontal refinement. In this way we obtain consistently integrated updates and schema updates for XML documents, which can even be extended to become fault-tolerant by taking at most k errors in the document into consideration. It further provides an example of ASM-based computation with automata-defined states.	abstract state machines;computation;fault tolerance;pushdown automaton;refinement (computing);streaming xml;xml schema	Klaus-Dieter Schewe;Bernhard Thalheim;Qing Wang	2009	J. UCS	10.3217/jucs-015-10-2028	well-formed document;xml catalog;xml validation;xml encryption;simple api for xml;xml;relax ng;xml schema;streaming xml;computer science;document type definition;theoretical computer science;document definition markup language;document structure description;xml framework;data mining;xml database;xml schema;database;xml signature;programming language;world wide web;xml schema editor;algorithm;efficient xml interchange;sgml	DB	-27.87903385584838	13.345667497891222	117702
37ed9cabb01cb5fe16535ae7afee50c513eac3d4	tailoring owl for data intensive ontologies	query language;relational data;conjunctive queries;management system;select project join;conceptual model;uml class diagram;description logic;query answering	The idea of using ontologies as a conceptual view over data repositories is becoming more and more popular. In these contexts, data are typically very large (much larger than the intentional level of the ontologies), and query answering becomes the basic reasoning services. In these contexts query answering should be very efficient on the data, and currently the only technology that is available to deal with large amounts of data is the one provided by relational data management systems (RDBMS). In this paper we advocate that for such contexts a suitable fragment of OWL-DL should be devised. Such a fragment must allow forms of query answering that exploit RDBMS when reasoning on the data, while it must include the main modeling features of conceptual models like UML class diagrams and ER diagrams. In particular it must include cyclic assertions, ISA on concepts, inverses of roles, role typing, mandatory participation to roles, and functional restrictions on roles. Also the query language should go beyond the expressive capabilities of concept expressions in description logics, and include at least conjunctive queries (corresponding to the select-project-join fragment of SQL). We discuss this issues by exhibiting a fragment of OWL-DL that includes all such features, namely DL-Lite, and showing that such a fragment is essentially maximal.	abox;adobe flash lite;best, worst and average case;conceptual schema;conjunctive query;data model;data-intensive computing;datalog;description logic;diagram;directed acyclic graph;mathematical optimization;maximal set;ontology (information science);query language;relational database management system;sql;tbox;unified modeling language	Giuseppe De Giacomo;Domenico Lembo;Maurizio Lenzerini;Riccardo Rosati	2005			sargable;query optimization;description logic;boolean conjunctive query;relational database;computer science;artificial intelligence;conceptual model;class diagram;data mining;management system;database;rdf query language;conjunctive query;programming language;query language	AI	-24.62870892736938	8.659916678032433	117708
8462a9fad820ae77d4825df46ae57d9611ffe54b	pttp+glides: guiding linear deductions with semantics	eficacia sistema;architecture systeme;automatic proving;model generation;deduction automatique;performance systeme;demostracion automatica;logical programming;system performance;theorem proving;demonstration automatique;demonstration theoreme;programmation logique;arquitectura sistema;analisis semantico;demostracion teorema;system architecture;analyse semantique;programacion logica;automatic deduction;semantic analysis	Using semantics to guide ATP systems is an under-utilised te chnique [10]. Two systems that have successfully employed semantic guidance are CLIN-S [1 ] and SCOTT [5]. In linear deduction, semantic guidance has received only limited atten tion, e.g., [7, 8]. Our research is developing semantic guidance for linear deduction in the Mode l Elimination (ME) [3] paradigm. Search pruning, at the possible loss of some refutation comp leteness, and search guidance, are being considered. This paper describes PTTP+GLiDeS, a P TTP style prover augmented with a semantic pruning mechanism, GLiDeS. PTTP+GLiDeS com bines modified versions of Stickel’s PTTP style prover [6] and the model generator MACE [4].	automated theorem proving;kronos;natural deduction;psi comp 80;programming paradigm;trusted third party	Marianne Brown;Geoff Sutcliffe	1999		10.1007/3-540-46695-9_21	computer science;artificial intelligence;computer performance;automated theorem proving;algorithm;systems architecture	AI	-20.471769743084227	11.914154288355277	117818
0ec75633ff5402544a671bf7fde2059a8d251079	mdωσ: a modelling language fot build a formal ontology in either description logics or conceptual graphs (short paper)	representacion conocimientos;adquisicion del conocimiento;ingenierie connaissances;acquisition connaissances;specification language;conceptual graph;knowledge acquisition;grafo conceptual;lenguaje especificacion;knowledge representation;representation connaissances;langage specification;graphe conceptuel;knowledge engineering	Md ωζ  is a semi-formal modelling language allowing to model graphically static knowledge of a domain under the form of a conceptual network. It makes easier the model translation into a representation formalism as description logics and conceptual graphs. Md ωζ  is part of a more general method designed to build domain ontologies from French texts. The approach has been proposed in the context of an industrial application concerning the supervision of telecom networks. Using Md ωζ  implies modelling before formal representation in order to avoid the pitfalls of direct representation. Choice of a formalism and its implementation is allowed to be postponed until the representation needs are better known.	conceptual graph;description logic;formal ontology;modeling language	Jérôme Nobécourt;Brigitte Biebow	2000		10.1007/3-540-39967-4_5	natural language processing;conceptual graph;conceptual model;specification language;computer science;artificial intelligence;knowledge engineering;database;algorithm	AI	-21.814866039408873	9.908743553642049	118032
e931d5a3992ce5915da8f9e03e3db1ae895839a9	rule-based integrity enforcement strategy for a distributed database	rule based;distributed database		distributed database	Hamidah Ibrahim;W. Alex Gray;N. J. Fiddian	1998			rule-based system;database tuning;database;data mining;distributed database;database testing;enforcement;database transaction;computer science	DB	-30.730576104849394	9.259288416923546	118089
a6bceeaf0774f2afa88ab2731115233edab2321e	semantic representation of events: building a semantic primes component	grammar;modelizacion;lenguaje natural;representation graphique;linguistique;semantic representation;object oriented design;verbal predicates;langage naturel;semantics;semantica;semantique;role and reference grammar;metalangage;modelisation;large scale;dictionnaire;linguistica;metamodel;metalanguage;object oriented;grammaire;natural language;dictionaries;grafo curva;oriente objet;semantic priming;modeling;orientado objeto;diccionario;gramatica;graphics;event types;metalenguaje;linguistics	This paper describes a system of semantic primes necessary for the large-scale semantic representation of event types, encoded as verbal predicates The system of semantic primes is compiled via mapping modeling elements of the Natural Semantic Metalanguage (NSM), the Semantic Minimum – Dictionary of Bulgarian (SMD), and the Role and Reference Grammar (RRG) The so developed system of semantic primes is a user-defined extension to the metalanguage, adopted in the Unified Eventity Representation (UER), a graphical formalism, introducing the object-oriented design to linguistic semantics.		Milena Slavcheva	2006		10.1007/11846406_31	natural language processing;metamodeling;semantic similarity;semantic computing;semantic integration;systems modeling;semantic search;metalanguage;computer science;graphics;object-oriented design;grammar;semantic compression;semantics;semantic equivalence;semantic property;linguistics;natural language;semantic technology;object-oriented programming;priming;algorithm;semantic gap	NLP	-30.06094515792528	14.791383882123307	118141
863ae492629e0cbe385b4544b3788a1957993afd	query classification, a first step towards a graphical interaction language	query classification		web query classification	Vincent Schenkelaars	1994			query expansion;web query classification;web search query;query by example;query language;information retrieval;rdf query language;sargable;query optimization;computer science	ML	-32.18725586505393	7.858522139109093	118244
55e9625fb73a57e97981c5f8630878fa2969fc4e	sage: geo-distributed streaming data analysis in clouds	service oriented architecture cloud computing distributed processing geographic information systems;service composition;azure cloud sage geodistributed streaming data analysis sensor networks stock exchange climate monitoring scientific applications geographical locations data aggregation real time processing dbms mapreduce service oriented cloud architecture cloud data centers;distributed processing;stream data;geographic information systems;service composition cloud computing stream data geo distributed processing;distributed databases cloud computing data processing scalability economics computational modeling computer architecture;service oriented architecture;geo distributed processing;cloud computing	The continuous growth of sensor networks, stock exchanges, climate monitoring or scientific applications produces new streaming data at increasing rates. Managing and processing such data, sometimes generated from multiple geographical locations, raises important challenges as it requires real-time processing or data aggregation. Conventional solutions like DBMS, MapReduce or dedicated solutions adopting single-located environments fail to meet the demands required for processing the Geo-distributed streaming data. Public clouds like Azure, with data centers spread around the globe, offer the infrastructure which can handle such a processing. Our approach, proposes a service-oriented cloud architecture for performing the stream analysis, by composing services which are distributed among multiple cloud data centers. Hence, the computation is moved towards the multiple data sources exploiting the geographical data locality. The initial results showed good scalability of the approach, reaching 1000 cores in the Azure cloud, and performance improvements compared to single location processing of a factor of 3.3.	cloud computing;computation;data aggregation;data center;locality of reference;mapreduce;microsoft azure;real-time locating system;scalability;service-oriented device architecture;stream (computing);streaming media	Radu Tudoran;Gabriel Antoniu;Luc Bougé	2013	2013 IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum	10.1109/IPDPSW.2013.95	single-chip cloud computer;cloud computing;computer science;database;distributed computing;world wide web;data architecture	Arch	-31.69283195285859	18.04071579337674	118261
5de2b3215c4181ab364e2b07058ae5751d873d70	the nomore++ system	cabeza;programacion conjunto respuesta;representacion conocimientos;answer sets;answer set programming;circonscription;logical programming;circumscription;programmation logique;programmation par ensemble reponse;representation connaissance;head;tete;knowledge representation;programacion logica;circonscripcion	We present a new answer set solver nomore++. Distinguishing features include its treatment of heads and bodies equitably as computational objects and a new hybrid lookahead. nomore++ is close to being competitive with stateof-the-art answer set solvers, as demonstrated by selected experimental results.	parsing;solver;stable model semantics	Christian Anger;Martin Gebser;Thomas Linke;André Neumann;Torsten Schaub	2005		10.1007/11546207_39	knowledge representation and reasoning;computer science;artificial intelligence;answer set programming;mathematics;head;circumscription;algorithm	AI	-19.975843777662003	11.992765299092607	118424
32304b4fb081b6aeffc8b83b71efbcb27eb4620d	a semijoin strategy for distributed query optimization.	query optimization		query optimization;relational algebra	Chin-Wan Chung;Keki B. Irani	1984			sargable;query optimization;query expansion;computer science;web search query	DB	-30.87539925918438	7.692892369597976	118579
b17c3ca89dd66baa5cc769890fce5612e1068b71	making your hands dirty inspires your brain! or how to switch asp into production mode	answer set programming;combinatorial problems;nonmonotonic reasoning;logic programs;theoretical foundation;empirical evaluation;problem solving	Rising from strong theoretical foundations in Logic Programming and Nonmonotonic Reasoning, Answer Set Programming (ASP) came to life as a declarative problem solving paradigm [1,2,3] in the late nineties. The further development of ASP was greatly inspired by the early availability of efficient and robust ASP solvers, like smodels [4] and dlv [5]. The community started modeling with ASP and a first milestone was the conception of TheoryBase [6] providing a systematic and scalable source of benchmarks stemming from combinatorial problems. Although the scalability of such benchmarks is of great value for empirically evaluating systems, the need for applicationoriented benchmarks was early perceived. The demand for systematic benchmarking led to the Dagstuhl initiative and with it the creation of the web-based benchmark archive asparagus [7]. This repository has in the meantime grown significantly, mainly due to the two past ASP competitions [8,9], and contains nowadays a whole variety of different types of benchmarks, although it is still far from being comprehensive. Meanwhile, the prospect of ASP has been demonstrated in numerous application scenarios.1 A highlight among them is arguably the usage of ASP for the high-level control of the space shuttle [10]. What makes this application so special is the fact that it was solving an application problem in a real-world environment. Although we still need many more elaborated proofs of concept, showing how ASP addresses different application scenarios, solving such real(-world) problems is yet another issue. Let me approach this by answering some preliminary questions. What is a real problem? Such a problem could be an unsolved combinatorial or mathematical problem. Also, it could stem from an application that is traditionally solved with different methods. In either case, the problem is not academic anymore, but rather about producing an effective solution. To accomplish this, we have to switch to a production mode,2 that is, the process of organizing the production of a solution to truly challenging problems. This mode of operation goes (currently) quite beyond conceptual modeling and benchmarking that most of us are used to so far. Why should we solve real problems? Apart from the fact that the prospect of doing so partly nourishes our right of existence, real problems are a tremendously fruitful source of new research questions. For instance, concepts like cardinality and weight constraints [11], magic set transformations [12], constraint additions to ASP [13,14], or projection [15], had never been pushed so hard without a real problem driving their development. In other words, making our hands dirty inspires our brain!	answer set programming;archive;benchmark (computing);block cipher mode of operation;dlv;decision problem;high- and low-level;logic programming;non-monotonic logic;organizing (structure);problem solving;programming paradigm;scalability;stable model semantics;stemming;web application;yet another	Torsten Schaub	2009		10.1007/978-3-642-04238-6_73	computer science;artificial intelligence;non-monotonic logic;answer set programming;data mining;programming language;algorithm	AI	-20.910888833242957	13.644971821653073	118981
2f70c8c1cfa95a0e151f14f88f30d1bf9867afaa	aurora: a new model and architecture for data stream management	tratamiento datos;systeme temps reel;operateur humain;base donnee;architecture systeme;continuous queries;operador humano;data stream;continuous query;database;software systems;base dato;data processing;pregunta documental;traitement donnee;systems software;qualite service;question documentaire;data stream management;human operator;query;arquitectura sistema;real time system;modele donnee;database triggers;sistema tiempo real;process model;systeme gestion base donnee;quality of service;system architecture;aurora architecture;sistema gestion base datos;database management system;service quality;logiciel exploitation;data models;calidad servicio;real time systems	This paper describes the basic processing model and architecture of Aurora, a new system to manage data streams for monitoring applications. Monitoring applications differ substantially from conventional business data processing. The fact that a software system must process and react to continual inputs from many sources (e.g., sensors) rather than from human operators requires one to rethink the fundamental architecture of a DBMS for this application area. In this paper, we present Aurora, a new DBMS currently under construction at Brandeis University, Brown University, and M.I.T. We first provide an overview of the basic Aurora model and architecture and then describe in detail a stream-oriented set of operators.	aurora;sensor;software system	Daniel J. Abadi;Donald Carney;Ugur Çetintemel;Mitch Cherniack;Christian Convey;Sangdon Lee;Michael Stonebraker;Nesime Tatbul;Stanley B. Zdonik	2003	The VLDB Journal	10.1007/s00778-003-0095-z	data modeling;simulation;quality of service;data processing;telecommunications;computer science;process modeling;database;service quality;software system	DB	-32.95333332776081	14.683375296646405	119157
a7a763e0244080a1b3679714d88850c261da2ea6	an approach to checking consistency in multimedia synchronization constraints	synchronisation;directed graphs;constraint graph;data integrity;collaboration;directed graph;master slave;computer languages;graph theory;synchronization	This paper proposes a new method for analyzing multimedia synchronization constraints based on the constraint graphand classification, which is essential in developing efficient support too ls for constraintbased authoring systems. We specify the temporal relations between multimedia objects, and use a directed graph to represent the cons traints among the objects in a multimedia scenario. We then analyze synchr onization constraints based on graph theory, solving multiple problems i n a unified theoretical framework: completeness and/or consistency chec king, constraints relaxation and automatic temporal layout generation. We al so discuss the effects of user interactive authoring. Compared to the othe r methods, it is simpler, more efficient, and easier to implement.	constraint logic programming;directed graph;graph theory;linear programming relaxation;synchronization (computer science)	Huadong Ma;Kang G. Shin	2000			synchronization;directed graph;computer science;constraint graph;graph theory;theoretical computer science;database;distributed computing;local consistency	AI	-27.92270033648917	15.901864714942592	119256
533529f5b12c76bffab9bc981639346ebd4c240b	geromesuite: a system for holistic generic model management	xml schema;information systems research;generic model;modeling language;polymorphism;model management	Manipulation of models and mappings is a common task in the design and development of information systems. Research in Model Management aims at supporting these tasks by providing a set of operators to manipulate models and mappings. As a framework, GeRoMeSuite provides an environment to simplify the implementation of model management operators. GeRoMeSuite is based on the generic role based metamodel GeRoMe [10], which represents models from different modeling languages (such as XML Schema, OWL, SQL) in a generic way. Thereby, the management of models in a polymorphic fashion is enabled, i.e. the same operator implementations are used regardless of the original modeling language of the schemas. In addition to providing a framework for model management, GeRoMeSuite implements several fundamental operators such as Match, Merge, and Compose.	holism;information system;metamodeling;modeling language;sql;web ontology language;xml schema	David Kensche;Christoph Quix;Zhenni Li;Yong Li	2007			polymorphism;computer science;knowledge management;data mining;xml schema;database;modeling language;programming language	DB	-33.62043235333576	11.777413918572629	119424
44c0ac5ff44796dbfecea9d20534e64a7566d40b	interactive dynamical visualization of big data arrays in grid	user interfaces big data data visualisation grid computing interactive systems parallel processing;oscillators;simulation;browsers;arrays;3d kuramoto sakaguchi model interactive 3d visualization big data array distributed grid service web interface hadoop mapreduce;three dimensional displays;data visualization;data visualization three dimensional displays simulation oscillators browsers conferences arrays;nonlinear dynamics grid portal webgl map reduce;conferences	Grid service and web interface for dynamical interactive 3D visualization of big data arrays is proposed, implemented and tested. The main requirement of system design is low latency and traffic reduction during data transfer. Distributed grid service is based on Hadoop Map-Reduce implementation. Web interface is implemented using three.js WebGL library. Visualizing of non-linear dynamics simulation results for 3D Kuramoto-Sakaguchi model proved efficiency of proposed approach. Prototype of described system is implemented in Ukrainian National Grid infrastructure.	apache hadoop;big data;dynamical system;kuramoto model;mapreduce;nonlinear system;prototype;simulation;systems design;three.js;user interface;volume rendering;webgl	Yuriy O. Koval;Hlib O. Mendrul;Andrii Salnikov;Ievgen A. Sliusar;Oleksandr Sudakov	2015	2015 IEEE 8th International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications (IDAACS)	10.1109/IDAACS.2015.7340718	computer science;theoretical computer science;operating system;database;oscillation;world wide web;data visualization;grid computing	HPC	-32.539719394839565	17.09807136454997	119466
4f219cb8f7b7236d1866f7d9648447f35adc9e57	on the numbers of shortest keys in relational databases on non-uniform domains	relational database		relational database	O. Selesnjew;Bernhard Thalheim	1988	Acta Cybern.		database theory;nested set model;relational model;relational database;computer science;database normalization;database model;data mining;database;conjunctive query;candidate key;information retrieval;object-relational impedance mismatch;database design	DB	-31.068638653340273	9.000195835161678	119475
9fe63b90226076d8919293891a3739f0742780c9	enhancing keyword search in relational databases using nearly duplicate records	relational database;journal;keyword search	The importance of supporting keyword searches on relations has been widely recognized. Different from the existing keyword search techniques on relations, this paper focuses on nearly duplicate records in relational databases due to abbreviation and typos. As a result, processing keyword searches with duplicate records involves many unique challenges. In this paper we discuss the motivation and present a system, RSEARCH, to show challenges in supporting keyword search using nearly duplicate records and key techniques including identifying nearly duplicate records and generating results efficiently.	association rule learning;expect;precision and recall;relational database management system;search algorithm	Xiaochun Yang;Bin Wang;Guoren Wang;Ge Yu	2010	IEEE Data Eng. Bull.		natural language processing;relational database;computer science;data mining;database;information retrieval	DB	-31.727801332748864	4.339737402539239	119571
7f7b17a212b1550b1ad32e6995f0163975ffc793	induction as a tool for conceptual coherence in computer science	programming language;conceptual understanding;proofs by induction;computer science education;theory of computing;structural induction;undergraduate computer science curricula;data structure;program correctness	"""Induction is central to many areas of computer science (CS) such as data structures, theory of computation, programming languages, program correctness/verification, and program efficiency. In this paper, I discuss the role of induction in the current CS curriculum and its applications in some of the above areas. I also shed some light on """"hidden"""" applications of structural induction, and I demonstrate how induction can be used as a tool for conceptual coherence in CS. Proofs by induction are currently taught independently of their CS applications and as a """"recipe"""" to be followed, without explaining the role of each ingredient. Moreover, there is no unified framework for induction throughout the CS curriculum, which I argue that is necessary for students to gain conceptual understanding."""	computer science;correctness (computer science);data structure;mathematical induction;programming language;structural induction;theory of computation;unified framework	Irene Polycarpou	2008		10.1145/1593105.1593221	computer science;theoretical computer science;algorithm	AI	-21.298194261593945	17.670205751143495	119588
b7bcf43038c71b7cec2499f384118c2ef6b36a91	rdf semantics for web association rules	sets ofrdfs class;rdf semantics;class subsumption;web association rule;vocabulary expansion;language expansion;actual-world semantics;association rule entailment problem;association rules w;complete procedure;instance typing	We present a vocabulary expansion of RDFS, which admits the assertion of rules that hold on associations among sets of RDFS classes, and we give an actual-world semantics to statements in the language expansion that properly captures the meaning commonly given to association rules w.r.t. context, support and confidence, while interacting with class subsumptions and instance typing. In addition, we exhibit a sound and complete procedure for the association rule entailment problem.		Mauricio Minuto Espil	2011		10.1007/978-3-642-23580-1_23	natural language processing;data mining;algorithm	PL	-23.963899521282883	8.816882493863174	120088
a3591c94e81698fe570770d9322a20c6fcb2c53e	sugar: a graph database fuzzy querying system	databases;software;fuzzy set theory sugar graph database fuzzy querying system flexible querying;query processing database management systems fuzzy set theory graph theory;databases sugar ciphers software data models database languages fuzzy sets;fuzzy sets;ciphers;sugar;database languages;data models	Graph databases have aroused a large interest in the last years thanks to their large scope of potential applications. Defining a language allowing a flexible querying of graph databases may greatly improve usability of data. In this paper, we present a system for querying graph databases in a flexible way. The preferences are based on fuzzy set theory and may concern i) the content of the vertices and ii) the structure of the graph.	fuzzy set;graph database;set theory;usability	Olivier Pivert;Olfa Slama;Grégory Smits;Virginie Thion	2016	2016 IEEE Tenth International Conference on Research Challenges in Information Science (RCIS)	10.1109/RCIS.2016.7549366	data modeling;database theory;wait-for graph;computer science;theoretical computer science;data mining;database;graph;fuzzy set;fuzzy set operations;graph database;query language;graph rewriting	DB	-29.288498538226296	7.399722295079002	120121
cff61d9e662aae9dbf9d102630ecb369adcfb37c	partial evaluation, programming methodology, and artificial intelligence	artificial intelligence;programming methodology;partial evaluation;artificial intelligent	This article presents a dual dependency between AI and programming methodologies. AI is an important source of ideas and tools for building sophisticated support facilities which make possible certain programming methodologies. These advanced programming methodologies in turn can have profound effects upon the methodology of AI research. Both of these dependencies are illustrated by the example of anew experimental programming methodology which is based upon current AI ideas about reasoning, representation and control. The manner in which AI systems are designed, developed and tested can be significantly improved in the programming is supported by a sufficiently powerful partial evaluator. In particular, the process of building levels of interpreters and of intertwining generate and test can be partially automated. Finally speculations about a more direct connection between AI and partial evaluation are presented.	artificial intelligence;partial evaluation	Kenneth M. Kahn	1984	AI Magazine		computer science;artificial intelligence;algorithm	AI	-21.4062376747943	14.4522129031369	120153
fbc4ed58ec5ba8cac1cddba8ee6868f420308468	multi-level structures of the dbtg data model for an achievement of physical data independence	data model	Achieving data independence is the main aim in data base systems. This paper proposes a new data model which achieves physical data independence by means of three level structures of the schema in the DBTG data model. And the construction method for the data management program which processes multi-level structures of the schema is considered.	data base task group;data model	Tetsuo Toh;Seiichi Kawazu;Kenji Suzuki	1977			data independence;data modeling;semi-structured model;data model;computer science;data science;data warehouse;data mining;database;logical data model	DB	-32.351394537486904	10.422403544508459	120213
4512a8d194a4629d72f359dedf16abfa4de6e92e	the performance of timeer model by description logics		The relationship between Description Logic (DL) and database is quite close. Indeed, the needs for the building of the systems that can manage both the database and the knowledge representation are really necessary. A description-logic based on the knowledge representation system not only allows the knowledge management, but also provides a standard framework which is considered to be very close to the language used to represent the Entity-Relationship model (ER model). On the other hand, the temporal ER model is used to model the time aspects of the conceptual database schema. Thus, the problem of the use of description logic to express temporal ER models is really useful for modeling the conceptual data models. Based on the temporal DL, Alessandro Artale et al. (2011) presented temporal ER schemas and integrity constraints in the form of complex inclusion dependencies. The paper approaches the representation method of Alessandro Artale and proposes mapping multi-valued attributes in the temporal ER model to DL. Description logic application in TimeER modeling	attribute-value system;conceptual schema;data integrity;data model;database schema;description logic;entity–relationship model;erdős–rényi model;knowledge representation and reasoning;referential integrity	Nguyen Viet Chanh;Quang Hoang	2015		10.1007/978-3-319-29236-6_28	t-norm fuzzy logics	DB	-30.92271538707477	11.780750172761753	120417
aa01c6f985d3b82dc30d90ef27866094ed754737	using an object-oriented database to encapsulate heterogeneous scientific data sources	data encapsulation;data structures;database management systems;distributed databases;natural sciences computing;object-oriented databases;oohdb;data sources;heterogeneity;heterogeneous scientific data sources;object-oriented data model;object-oriented database;object-oriented database management system;object-oriented heterogeneous database;property databases	There are many problems facing scientists in their eeorts to use computers to manage, access, and analyze data. One serious problem is the heterogene-ity of data sources. Previous research into accessing heterogeneous data has not addressed problems with accessing large data sources that are not managed by database management systems. We believe that an object-oriented database management system can be used to implement an object-oriented heterogeneous database (OOHDB) that ee-ciently addresses the problem of heterogeneity among large scientiic property databases. The object-oriented data model provides a powerful modeling paradigm for representing complex scientiic data. Objects within the OOHDB hide heterogeneity among various data sources. EEcient access to external data is achieved by representing external data with a small amount of data stored within the OOHDB.	computer;data model;heterogeneous database system;programming paradigm	David M. Hansen;David Maier	1994			data modeling;data quality;data structure;computer science;data science;heterogeneity;data administration;data warehouse;data mining;management system;database;distributed database;data retrieval;database testing;database design;data	DB	-33.13588447293347	12.316025326816414	120898
5cf8e44857457c070d73a722a84162ab69251d10	document processing in a relational database system	relational database system;document processing	This paper contains a proposal to enhance a relational database manager to support document processing. Basically, it suggests support for data items that are variable-length strings, support for ordered relations, support for substring operations, and support for new operators that concatenate and break apart string fields.	abstract data type;concatenation;document processing;information systems;ingres;relational database;substring	Michael Stonebraker;Heidi Stettner;Nadene Lynn;Joseph Kalash;Antonin Guttman	1983	ACM Trans. Inf. Syst.	10.1145/357431.357433	data definition language;sql;relational database management system;relational model;document processing;entity–relationship model;data model;relational database;computer science;database normalization;database model;database;view;database schema;object-relational impedance mismatch;database design;spatiotemporal database;component-oriented database	DB	-30.91475367272444	9.409569982116803	121064
a74842170885a3af63ec3eee939b40ccf6dea005	guest editorial to the special issue on inductive logic programming, mining and learning in graphs and statistical relational learning	inductive logic programming;statistical relational learning	In 2009, three international conferences/workshops on learning from relational, graph-based and probabilistic data were co-located: ILP-2009, the 19th International Conference on Inductive Logic Programming; MLG-2009, the 7th International Workshop on Mining and Learning with Graphs; and SRL-2009, the International Workshop on Statistical Relational Learning. These events were organized in Leuven, Belgium, on July 2–4, 2009. The ILP conference series has been the premier forum for work on logic-based approaches to learning for almost two decades and has recently reached out to other forms of relational learning and to probabilistic approaches. The MLG workshop series focuses on graph-based approaches to machine learning and data mining while the SRL workshop series focuses on statistical inference and learning with relational and first-order logical	data mining;first-order predicate;inductive logic programming;machine learning;statistical relational learning	Hendrik Blockeel;Karsten M. Borgwardt;Luc De Raedt;Pedro M. Domingos;Kristian Kersting;Xifeng Yan	2010	Machine Learning	10.1007/s10994-010-5195-6	natural language processing;multi-task learning;algorithmic learning theory;statistical relational learning;relational calculus;computer science;theoretical computer science;machine learning;functional logic programming;inductive transfer;inductive programming;logic programming	ML	-21.497001795674002	6.875475333874058	121215
4c3a30879a7bf30699629a86e6ad27ce68fec0fc	a rule-based view of query optimization	query language;rule based;query optimization;query evaluation;relational database management system;database management system	The query optimizer is an important system component of a relational database management system (DBMS). It is the responsibility of this component to translate the user-submitted query - usually written in a non-procedural language - into an efficient query evaluation plan (QEP) which is then executed against the database. The research literature describes a wide variety of optimization strategies for different query languages and implementation environments. However, very little is known about how to design and structure the query optimization component to implement these strategies. This paper proposes a first step towards the design of a modular query optimizer. We describe its operations by transformation rules which generate different QEPs from initial query specifications. As we distinguish different aspects of the query optimization process, our hope is that the approach taken in this paper will contribute to the more general goal of a modular query optimizer as part of an extensible database management system.	mathematical optimization;procedural programming;program optimization;query language;query optimization;relational database management system;scientific literature	Johann-Christoph Freytag	1987		10.1145/38713.38735	rule-based system;search-oriented architecture;online aggregation;sargable;query optimization;.ql;relational database management system;query expansion;web query classification;ranking;boolean conjunctive query;database tuning;data control language;computer science;query by example;data mining;database;rdf query language;web search query;view;information retrieval;alias;query language;object query language;spatial query	DB	-30.022546844953016	6.256241105125518	121260
f742078ff3382c9253895aae3f49f29d8d962fc3	semantics modeling issues for processing natural language database queries	conceptual knowledge;semantic model;data semantics;natural language;semantic description;database query	This work explores data semantics issues necessary for designing an NL interface capable of dealing with incomplete NL queries. First, the notion of incomplete NL queries is clarified to conclude the necessity for enriching the data semantics component of the system. An assumption made by the existing NL interfaces is expounded to argue that the approaches based on this assumption are unable to explore meta-knowledge about the stored data in the query conversion process. A new, unconventional approach is proposed whose tenets underlie the separation between the conceptual interpretation of NL queries and the semantics descriptions of the database constituents. This approach allows explicit descriptions of the conceptual knowledge, referred to herein as epistemological specifications, on the DB constituents.	nl (complexity);natural language;nl (format);semantic data model	D. G. Shin	1990		10.1145/100348.100350	semantic data model;natural language processing;computer science;data mining;database;linguistics;natural language	DB	-27.027790982976732	9.084005053033055	121515
6ec0b9f960d0d83c99384ef0a0e91dfa4de8ef50	on inference control in semantic data models for statistical databases	tratamiento datos;modelizacion;base donnee;confidencialidad;analisis estadistico;analisis datos;securite;interrogation base donnee;database;interrogacion base datos;base dato;data processing;traitement donnee;confidentiality;modelisation;confidentialite;protection;semantic data model;data analysis;inference control;statistical analysis;safety;analyse statistique;inferencia;proteccion;analyse donnee;seguridad;modeling;database query;inference	Abstract   A statistical database (SDB) is a database that is used to provide simple. summary statistics about populations stored in the database and that supports statistical data analysis. When SDB users infer protected information in the SDB from responses to queries, we say that the SDB is compromised. The security problem of SDB is to allow simple summary statistics about protected information in the SDB while preventing compromise.  In this paper, we consider the SDB security problem in the context of the Data Abstraction (D-A) model, and investigate the effectiveness of rounding SUM and COUNT query responses for preventing compromise due to structural, dynamic and pre-knowledge inferences in the generalization hierarchy of the D-A model. We first round only SUM query responses, permit updates in a single population, and investigate the effect of users' a priori knowledge of a single protected value. It is shown that compromise is possible, and a necessary and sufficient condition for compromise is given. We then round both SUM and COUNT query responses, and introduce techniques of choosing rounding bases for populations in tree-organized hierarchies that either eliminate or restrict structural and dynamic inferences. Finally, we propose the range response technique which eliminates structural and dynamic inferences in general generalization hierarchies.	data model	Gultekin Özsoyoglu;Tzong-An Su	1990	J. Comput. Syst. Sci.	10.1016/0022-0000(90)90005-6	semantic data model;systems modeling;confidentiality;data processing;computer science;data mining;database;data analysis;algorithm	DB	-27.07531253591774	12.330409669676687	121669
25b2e38a4056bf2797e7beb8d4491cb69f386b69	similarity of dtds based on edit distance and semantics	semantic similarity;xml schema;edit distance	In this paper we propose a technique for evaluating similarity of XML schema fragments. Contrary to existing works we focus on structural level in combination with semantic similarity of the data. For this purpose we exploit the idea of edit distance utilized to constructs of DTDs which enables to express the structural differences of the given data more precisely. In addition, in combination with the semantic similarity it provides more realistic results. Using various experiments we show the behavior and advantages of the proposed approach.	algorithm;data definition language;edit distance;experiment;semantic similarity;thesaurus;tree (data structure);xml schema	Ales Wojnar;Irena Holubová;Jirí Dokulil	2008		10.1007/978-3-540-85257-5_21	semantic similarity;computer science;data mining;database;information retrieval	DB	-32.36626125432295	4.9514524981538734	121779
5d79f0121f9e4f75ed98683b980829f61b0ee725	updating multidimensional xml documents	extensible markup language;database management systems;spectrum;article δημοσίeυση πeριοδικού;relational database;data structures;xml document;database management system;data structure;design methodology	In Web applications it is often required to manipulate information of semistructured nature, which may present variations according to different circumstances. Multidimensional XML (MXML) is an extension of XML suitable for representing data that assume different facets, having different value and/or structure, under different contexts. In this paper, we consider the problem of updating MXML. Updating must take into account the additional features of MXML compared to XML. Those features stem from the incorporation of context into MXML. We introduce six basic update operations, which are suitable for any possible change in MXML.	emoticon;mxml;norm (social);relational database;sql;xml	Nikolaos Fousteris;Manolis Gergatsoulis;Yannis Stavrakas	2007		10.1108/17440080810882342	xml validation;xml;data structure;computer science;data mining;database;programming language;world wide web;information retrieval	DB	-29.020603798231985	9.060652753964712	121900
29b3222bd66bcb47fba1a20069e0e2085907b2de	towards a categorial data model supporting structured objects and inheritance	data model	We propose a data model in which the data scheme, the data domain and the database are defined using the concepts of graph, category and diagram, respectively, and in which the limit of a diagram plays an essential role. Our model incorporates important concepts of known database models (such as structured objects and inheritance) and provides new insights into these models.	data model	S. Kazem Lellahi;Nicolas Spyratos	1990		10.1007/3-540-54141-1_6	pattern recognition;data mining;database;logical data model	DB	-30.590044131039487	11.825042241013668	121974
684da17ef4d9a8565d72f3ee6d2e47899e75a959	t-sparql: a tsql2-like temporal query language for rdf		In this paper, we present a temporal extension of the SPARQL query language for RDF graphs. The new language is based on a temporal RDF database model employing triple timestamping with temporal elements, which best preserves the scalability property enjoyed by triple storage technologies, especially in a multi-temporal setting. The proposed SPARQL extensions are aimed at embedding several features of the TSQL2 consensual language designed for temporal relational databases.	database model;list of sparql implementations;nondeterministic algorithm;query language;relational database;resource description framework;sparql query results xml format;scalability;temporal database;triplestore;whole earth 'lectronic link	Fabio Grandi	2010			database;rdf;temporal database;sparql;cwm;rdf schema;sargable;rdf query language;rdf/xml;computer science	DB	-32.509037124330604	6.307192554509725	121999
6f79c6e52e529e25889aeb39f66894d2f84f7a24	10412 summary and abstracts collection - qstrlib: a benchmark problem repository for qualitative spatial and temporal reasoning		Reasoning about spatial configurations, temporal constraints, or spatio-temporal dependencies is a major challenge in various application domains of current AI research. One of the research areas concerned with such topics is the field of Qualitative Spatial and Temporal Reasoning (QSTR). More precisely, research topics in QSTR are formalisms and algorithmic methods for processing qualitative information about continuous spatial and/or temporal aspects of physical reality. Thus QSTR forms a distinct subfield within the broader AI community working on knowledge representation and reasoning and has attracted research interest for more than 25 years now. One of the main ideas in QSTR is to describe spatial or temporal phenomena in the world in qualitative formalisms that are tailored to specific reasoning tasks. Qualitative abstraction allows for compact representations of infinite domains and hence is considered key to solve reasoning tasks on such domains efficiently. In view of this motivation of QSTR research, it seems indicated to evaluate the quality of both representation formalisms and reasoning techniques with respect to different criteria including but not limited to expressiveness, efficiency, and cognitive adequacy. In contrast to other established AI communities, the idea of benchmarking formalisms, reasoning procedures, and implemented systems has not played a prominent role so far. For example, performance analyses of QSTR algorithms or systems are often only conducted by empiric evaluations based on random samples of problem instances which in theory are known to be hard to solve. But this gives rise to the question whether evaluations on such problem instances is adequate, since random instances typically do not reflect problem structures that also occur in concrete problem domains. Moreover, little work has been done so far towards a systematic comparison of the variety of qualitative formalisms available in the literature, which seems a crucial deficit for practitioners who want to use QSTR formalisms in applications. The availability of a benchmarking repository is not only useful for system developers, benchmarking is also essential to evaluate new strands of research. This lesson can be learned from other communities in computer science, such as Automated Theorem Proving (ATP), Boolean Satisfiability (SAT), Constraint Satisfaction (CSP), and Automated Planning, where benchmark repositories have proven beneficial to estimate weaknesses or strengths of particular approaches and to identify reoccurring structural properties of problems or tasks. To initiate the development of a problem repository for QSTR, a AAAI Spring Symposium on Benchmarking of Qualitative Spatial and Temporal Reasoning Systems …	algorithm;automated planning and scheduling;automated theorem proving;benchmark (computing);boolean satisfiability problem;computer science;constraint satisfaction;grammar-based code;key (cryptography);knowledge representation and reasoning;problem domain;temporal logic	Stefan Wölfl;Anthony G. Cohn;Jochen Renz;Georg Sutcliffe	2010			data science;data mining;computer science	AI	-20.405782464724087	7.9281273027508945	122080
b9b171d99bcd6b074124e5ec11c73379e19d5427	twig pattern matching with positional predicates in xml queries	query processing;twig pattern matching;pattern matching;positional predicates;xml;following sibling	Twig pattern matching is core operation in XML query processing. Although holistic twig pattern matching algorithms have been studied intensively nowadays, little attention is paid on positional predicates defined on navigation axis. These algorithms cannot be used to implement XPath queries with positional predicates, since the query results fail to be selected from the pattern matching results. For implementation of XPath query with the positional predicates, this paper extends the traditional twig pattern to a new one called ExTwig to deal with both positional predicates and following sibling axis. The corresponding holistic twig pattern matching algorithm TwigPos is also proposed to process ExTwig query. Comparison experiments on large benchmarks show that our algorithm is efficient and practical.	algorithm;apache axis;benchmark (computing);database;experiment;holism;pattern matching;twig;xml namespace;xpath	Fajin Li;Husheng Liao;Hongyu Gao	2013	2013 10th Web Information System and Application Conference	10.1109/WISA.2013.30	computer science;data mining;database;programming language	DB	-29.779885813566587	4.73539918374058	122428
2e8a074811031c0af9f4974fbe80c2d351040c51	multi-data-types interval decision diagrams for xacml evaluation engine	xacml;midd combination;bis business information services;extensible access control mark up language multidata types interval decision diagram xacml evaluation engine xacml policy evaluation efficiency critical attribute setting mandatory property attribute attack data interval partition aggregation decision diagram combination xacml 3 0 feature complex logical expression;authorisation;decision diagram;ts technical sciences;partitioning algorithms data structures engines semantics standards aggregates boolean functions;xml authorisation;universiteitsbibliotheek;communication information;policy evaluation;interval partition processing;xml;information society;informatics;authorization;access control;infrastructures;midd combination access control authorization xacml policy evaluation decision diagram multi data types interval decision diagram midd interval partition processing;multi data types interval decision diagram midd	XACML policy evaluation efficiency is an important factor influencing the overall system performance, especially when the number of policies grows. Some existing approaches on high performance XACML policy evaluation can support simple policies with equality comparisons and handle requests with well defined conditions. Such mechanisms do not provide the semantic correctness of combining algorithms in cases with indeterminate and not-applicable states. They ignore the critical attribute setting, a mandatory property in XACML, leading to potential missing attribute attacks. In this paper, we present a solution using data interval partition aggregation together with new decision diagram combinations, that not only optimizes the performance but also provides correctness and completeness of XACML 3.0 features, including complex logical expressions, correctness in indeterminate states processing, critical attribute setting, obligations and advices as well as complex comparison functions for multiple data types.	algorithm;correctness (computer science);dspace;indeterminacy in concurrent computation;influence diagram;time complexity;tree structure;xacml	Canh Ngo;Marc X. Makkes;Yuri Demchenko;Cees T. A. M. de Laat	2013	2013 Eleventh Annual Conference on Privacy, Security and Trust	10.1109/PST.2013.6596061	computer science;theoretical computer science;data mining;database;authorization;computer security	DB	-27.596954476060912	7.0614640992732305	122453
3b9cc26de5df73ea985fef38a48b2aabe7e5a8c2	efficiently answer top-k queries on typed intervals		Abstract Consider a database consisting of a set of tuples, each of which contains an interval, a type and a weight. These tuples are called typed intervals and used to support applications involving diverse intervals. In this paper, we study top- k queries on typed intervals. The query reports k intervals intersecting the query time, containing a particular type and having the largest weight. The query time can be a point or an interval. Further, we define top- k continuous queries that return qualified intervals at each time point during the query interval. To efficiently answer such queries, a key challenge is to build an index structure to manage typed intervals. Employing the standard interval tree, we build the structure in a compact way to reduce the I/O cost, and provide analytically derived partitioning methods to manage the data. Query algorithms are proposed to support point, interval and continuous queries. An auxiliary main-memory structure is developed to report continuous results. Using large real and synthetic datasets, extensive experiments are performed in a prototype database system to demonstrate the effectiveness, efficiency and scalability. The results show that our method significantly outperforms alternative methods in most settings.	access structure;active database;algorithmica;attribute–value pair;b-tree;berg connector;bioinformatics;cloud computing;computation;computational geometry;computer science;data model;data structure;dual-energy x-ray absorptiometry;dynamic data;dynamization;edmund m. clarke;experiment;expert system;fractal;herbert edelsbrunner;ink (operating system);image scaling;input/output;interval tree;jensen's inequality;lu decomposition;land administration;os-tan;object-based spatial database;object-relational database;overlap–add method;pp (complexity);paris kanellakis award;prototype;r* tree;relational database;ross quinlan;scalability;search algorithm;segment tree;skip list;snapshot (computer storage);space-filling curve;springer (tank);synthetic data;synthetic intelligence;taxonomy (general);transaction time;vldb;valid time;yang;yao graph	Jianqiu Xu;Hua Lu	2017	Inf. Syst.	10.1016/j.is.2017.08.005	tuple;database;computer science;data mining;scalability;interval tree;time point;spatial query	DB	-30.864661077515123	8.32526291371023	122486
70c60ccd2948fca72dd9e4f9ebc3e0155c1a47fd	data analysis for query processing	base relacional dato;systeme intelligent;adquisicion del conocimiento;query reformulation;analisis datos;query processing;sistema inteligente;relational database;acquisition connaissance;data analysis;knowledge acquisition;intelligent system;base donnee relationnelle;analyse donnee;query answering	Data analysis is needed in connection with query processing, to produce data summary information in the form of rules or assertions that allow semantic query optimisation or direct query answering without consulting the data itself. The goal of an intelli gent analyser in this context is to produce robust rules, stable in the presence of data changes, which allow easy rule maintenance as data changes, and provide rapid query reformulation, refutation or answering. It must also limit the rule set to rules useful for query processing.	algorithm;database;mathematical optimization;query optimization;semantic query	Jerome Robinson;Barry G. T. Lowden	1997		10.1007/BFb0052861	sargable;query optimization;query expansion;web query classification;relational database;computer science;query by example;data mining;database;rdf query language;data analysis;view;information retrieval;query language	DB	-26.159920671520517	7.27925546245725	122626
22567572d86043e275b0a4df19dd6edd0595eca1	physical design equivalencies in database conversion	base relacional dato;concepcion sistema;logic design;data processing;physical design;relational database;equivalence relation;conception logique;system design;base donnee relationnelle;systeme gestion base donnee;concepcion logica;sistema gestion base datos;database management system;conception systeme;conversion;database performance	As relational technology becomes increasingly accepted in commercial data processing, conversion of some of the huge number of existing navigational databases to relational databases is inevitable. It is thus important to understand how to recognize physical design modifications and enhancements in the navigational databases and how to convert them to equivalent relational terms as applicable.	physical design (electronics);relational database	Mark L. Gillenson	1990	Commun. ACM	10.1145/79173.79183	physical design;logic synthesis;data processing;database tuning;relational database;computer science;data mining;database;equivalence relation;systems design	DB	-28.575628347631163	12.146908608638567	122766
e9a8eed15499b85de09146aba59b710500ab0071	notes from the logbook of a proof-checker's project	teoria demonstracion;abelian group;system core;preuve programme;ensayo no destructivo;program proof;theorie preuve;proof theory;essai non destructif;grupo abeliano;social decision;software systems;nucleo sistema;program verification;set theory;exactitude programme;noyau systeme;zermelo fraenkel;scenario;software architecture;verificacion programa;exactitud programa;non destructive test;decision procedure;argumento;documentacion;script;prueba programa;real number;decision colectiva;groupe abelien;nombre reel;decision collective;verification programme;architecture logiciel;documentation;cauchy integral;program correctness	We are developing a software system which ingests proofs formalized within Zermelo-Fraenkel set theory and checks their compliance with mathematical rigor. It will accept trivial steps as obvious, without necessarily being clever at discovering how to fill large gaps in a proposed mathematical argument. It will be able to process large proof scripts (say dozens of thousands of proofware lines written on persistent files), without necessarily acting as a highly interactive proof assistant. Over the years, the documentation of our project (entrusted, to a large extent, to electronic correspondence) has come to form a thick logbook, of which we will transcribe three extracts in this paper. The design of a proof checker must face a variety of design issues, partly because proof technology as a whole has not reached, as of today, a plateau on which applications, for example in the areas of program correctness and security protocol verification, can firmly rely. On the other hand, such a wealth of ideas is encompassed in the software of state-of-the-art proof systems as to give us an embarrassingly wide choice: which techniques and methods should we include among our basic inference ingredients, from which reasoning paradigms can we gain deep insights and intellectual stimuli for further research? In any project, a yardstick is needed to judge on virtues and drawbacks of technical solutions and to keep under constant focus the most critical design issues on which ultimate success depends: in our case the yardstick is a huge proof script, our privileged “scenario” file, which evolves in parallel with our prototype proof-checker, supports its functional testing, and brings to light the need for new functions and linguistic features. It was decided from the outset that our scenario should lead from “first principles”, namely the rudiments of set theory, to a most important achievement of classical mathematical analysis, the Cauchy Integral Theorem. The “backbone” of the scenario, consisting only of definitions and theorem statements, became available very early in the project;		Domenico Cantone;Eugenio G. Omodeo;Jacob T. Schwartz;Pietro Ursino	2003		10.1007/978-3-540-39910-0_8	software architecture;documentation;computer science;artificial intelligence;scenario;proof theory;database;mathematics;distributed computing;abelian group;programming language;computer security;algorithm;statistics;real number;set theory	PL	-20.187361241410972	17.509523434061723	122843
83799def03074ff60523cb9fa4f9dcbc935eb1ef	extensible logic program schemata	semantic similarity;programming language;logic programs;higher order logic;large classes	Schema-based transformational systems maintain a library of logic program schemata which capture large classes of logic programs. One of the shortcomings of schema-based transformation approaches is their reliance on a large (possibly incomplete) set of logic program schemata that is required in order to capture all of the minor syntactic differences between semantically similar logic programs. By defining a set of extensible logic program schemata and an associated set of logic program transformations, it is possible to reduce the size of the schema library while maintaining the robustness of the transformational system. In our transformational system, we have defined a set of extensible logic program schemata in Prolog. Because Prolog is a higher-order logic programming language, it can be used as the representation language for both the logic programs and the extensible logic program schemata. In addition to the instantiation of predicate variables, extensible logic program schemata can be extended by applying standard programming techniques (e.g., accumulating results), introducing additional arguments (e.g., a second list to append to the end of the primary list), combining logic program schemata which share a common primary input, and connecting logic program schemata which are connected via a result of one schema being an input to the other schema. These extensions increase the robustness of logic program schemata and enhance traditional schema-based transformational systems.	append;database schema;logic programming;program transformation;programming language;prolog;universal instantiation	Timothy S. Gegg-Harrison	1996		10.1007/3-540-62718-9_15	dynamic logic;semantic similarity;description logic;higher-order logic;horn clause;computer science;theoretical computer science;functional logic programming;computational logic;signature;programming language;prolog;logic programming;multimodal logic;algorithm;autoepistemic logic	PL	-20.9406024375354	16.366729730209745	123079
30313072e94101d860c6887d4cde8d33120f945f	modeling and querying continuous fields with olap cubes	continuous fields;spatial on line analytical processing solap;geographic information systems gis;map algebra;on line analytical processing olap	The notion of SOLAP Spatial On-Line Analytical Processing is aimed at exploring spatial data in the same way as OLAP operates over tables. SOLAP, however, only accounts for discrete spatial data. Current decision support systems are increasingly being needed for handling more complex types of data, like continuous fields, which describe physical phenomena that change continuously in time and/or space e.g., temperature. Although many models have been proposed for adding spatial continuous and discrete information to OLAP tools, no one is general enough to allow users to just perceive data as a cube, and analyze any type of spatial data together with typical alphanumerical discrete OLAP data, using only the classic OLAP operators e.g., Roll-up, Drill-down. In this paper the authors propose a model and an algebra supporting it, that allow operating over data cubes, independently of the underlying data types and physical data representation. That means, in this approach, the final user only sees the typical OLAP operators at the query level, whereas at lower abstraction levels the authors provide discrete and continuous spatial data support as well as different ways of partitioning the space. As far as the authors are aware of, this is the first proposal, which provides such a general framework for spatiotemporal data analysis.	cubes;online analytical processing	Leticia I. Gómez;Silvia A. Gómez;Alejandro A. Vaisman	2013	IJDWM	10.4018/jdwm.2013070102	map algebra;online analytical processing;computer science;theoretical computer science;data mining;database	DB	-29.453349124863482	7.64568713012421	123148
d28648662fc2a5ff833503c82f6ea2eef7625350	a model theoretic approach to update rule programs	system reliability;active database;rule based;well founded semantics;integrity constraints	Semantics of active rules is generally defined by execution models. The lack of a clean declarative semantics threats active system reliability. In this paper, a declarative semantics for update rules based on the well founded semantics of Datalog programs is investigated. The validation of our proposal proceeds by demonstrating it for static and transition integrity constraint enforcement.		Nicole Bidoit;Sofian Maabout	1997		10.1007/3-540-62222-5_44	rule-based system;computer science;theoretical computer science;data integrity;data mining;database	Theory	-25.278566677543093	13.809989734195375	123171
6bfb6fdd3f467a536657463909d31f12ee0bc6c1	an extensible approach to reactive processing in an advanced object modelling environment	modelizacion;base donnee;architecture systeme;database;base dato;modelisation;object oriented;object oriented database systems;oriente objet;arquitectura sistema;information system;system architecture;modeling;orientado objeto;systeme information;production rule;sistema informacion	This paper describes a new approach for supporting reactive capability in an advanced object-oriented database system called ADOME-II. Besides having a rich set of pre-defined composite event expressions and a well-defined execution model, ADOME-II supports an extensible approach to reactive processing so as to be able to gracefully accommodate dynamic applications' requirements. In this approach, production rules combined with methods are used as a unifying mechanism to process rules, to enable incremental detection of composite events, and to allow new composite event expressions to be introduced into the system declaratively. Methods of supporting new composite event expressions are described, and comparisons with other relevant approaches are also conducted. A prototype of ADOME-II has been constructed, which has as its implementation base an ordinary (passive) OODBMS and a production rule base system.		Leung-Chi Chan;Qing Li	1997		10.1007/BFb0022016	real-time computing;systems modeling;computer science;operating system;database;programming language;object-oriented programming;information system;algorithm;systems architecture	SE	-30.674951540371193	13.941869834704438	123286
17246960f3e6653fd676497f0be112c2bb4389e0	on the optimality of scheduling strategies in subsumption-based tabled resolution		Subsumption-based tabled logic programming promotes more aggressive reuse of answer tables over variant-based tabling. However resolving subgoals against answers in tabled logic programming may require accessing incomplete answer tables ( i.e., more answers remain to be added). In subsumption-based tabling it is far more efficient to retrieve from completed tables. Scheduling strategies promote more frequent usage of such tables by exercising control over access to incomplete tables. Different choices in the control can lead to different sets of proof trees in the search forest produced by tabled resolution. The net effect is that depending on the scheduling strategy used, tabled logic programs under subsumption can exhibit substantial variations in performance. In this paper we establish that for subsumption-basedtabled logic programming an optimal scheduling strategy does not exist — i.e., they are all incomparable in terms of time and space performance. Subsumption-based tabled resolution under call abstractionminimizes the set of proof trees constructed. In the presence of call abstraction, we show that there exists a family of scheduling strategies that minimize the number of calls that consume from incomplete answer tables produced by strictly more general calls.	logic programming;memoization;resolution (logic);scheduling (computing);subsumption architecture	Prasad Rao;C. R. Ramakrishnan;I. V. Ramakrishnan	1998				AI	-19.88268459255624	15.123804116451732	123449
11faa1725ca5819dc6be7562eba61b0bcb57e22c	representing and reasoning on xml documents: a description logic approach	document structure;extensible markup language;automated reasoning;polynomial time;xml document;world wide web;description logic;knowledge representation	Recent proposals to improve the quality of interaction with the World Wide Web suggest considering the Web as a huge semistructured database, so that retrieving information can be supported by the task of database querying. Under this view, it is important to represent the form of both the network, and the documents placed in the nodes of the network. However, the current proposals do not pay sufficient attention to represent document structures and reasoning about them. In this paper, we address these problems by providing a framework where Document Type Definitions (DTDs) expressed in the eXtensible Markup Language (XML) are formalized in an expressive Description Logic equipped with sound and complete inference algorithms. We provide methods for verifying conformance of a document to a DTD in polynomial time, and structural equivalence of DTDs in worst case deterministic exponential time, improving known algorithms for this problem which were double exponential. We also deal with parametric versions of conformance and structural equivalence, and investigate other forms of reasoning on DTDs. Finally, we show how to take advantage of the reasoning capabilities of our formalism in order to perform several optimization steps in answering queries posed to a document base.	algorithm;best, worst and average case;conformance testing;description logic;document classification;document-oriented database;exception handling;formal equivalence checking;markup language;mathematical optimization;polynomial;semantics (computer science);time complexity;turing completeness;verification and validation;world wide web;xml	Diego Calvanese;Giuseppe De Giacomo;Maurizio Lenzerini	1999	J. Log. Comput.	10.1093/logcom/9.3.295	well-formed document;knowledge representation and reasoning;xhtml;xml;xml schema;computer science;artificial intelligence;document definition markup language;document structure description;data mining;pcdata;database;programming language;information retrieval;algorithm	AI	-23.90430311544016	8.825679234970776	123466
f17893cd312107fdc8af80da88d5643c27c91aa6	morphological analysis (a formal approach)	morphological analysis	"""Since 1972 a researcher team in Bonn has been working on the automatic syntax-analysis of the german language. The morphoJogical analysis is a part of this work that has already been formalized and programmed by the author. We can consider the following paper as a generalization of this formalized description. In this first chapter some expressions like """" t e x t """" or """"lexicon """" are considered clear by intuition. Later on we'll get to know the exact definitions. The basis o f each description of a text of any language is a morphological analysis of this text. One can easily agree that such a description has to be derived from the words or sentences of the text which is to be described. The expression """"description of a text"""" is understood in a very general sense.. One can imagine a syntactic description or a semantic interpretation or a combination of both of them or any other information. In a natural language the number of the possible texts is not finite. That's easy to prove because of the following sentences:"""	lexicon;natural language;parsing;semantic interpretation	Werner Brecht	1973		10.3115/992532.992535	morphological analysis;computer science	NLP	-25.855117801669806	17.869264177606585	123534
90e2c8d88d8225ec3057eab5bf9a57d0d93e4a7b	confidentiality policies for controlled query evaluation	existing method;controlled query evaluation;information system;modified answer;semantic way;specified confidentiality policy;confidentiality policy;formal definition;incomplete logic databases	Controlled Query Evaluation (CQE) is an approach to enforcing confidentiality in information systems at runtime. At each query, a censor checks whether the answer to that query would enable the user to infer any information he is not allowed to know according to some specified confidentiality policy. If this is the case, the answer is distorted, either by refusing to answer or by returning a modified answer. In this paper, we consider incomplete logic databases and investigate the semantic ways of protecting a piece of information. We give a formal definition of such confidentiality policies, and show how to enforce them by reusing the existing methods for CQE.	censoring (statistics);confidentiality;database;information system;run time (program lifecycle phase)	Joachim Biskup;Torben Weibert	2007			query optimization;computer science;data mining;database;computer security;query language	DB	-26.78173568416005	12.653115061897184	123758
4fb91f4de7be98ebcc2e475b90eed744c6d3e4c4	type theoretic semantics for semantic networks : an application to natural language engineering		Semantic Networks have long been recognised as an important tool for natural language processing. This research has been a formal analysis of a semantic network using constructive type theory. The particular net studied is SemNet, the internal knowledge representation for LOLITA 1 : a large scale natural language engineering system. SemNet has been designed with large scale, efficiency, integration and expressiveness in mind. I t supports many different forms of plausible and valid reasoning, including: epistemic reasoning, causal reasoning and inheritance. The unified theory of types (UTT) integrates two well known type theories, Coquand-Huet's (impredicative) calculus of constructions and Martin-Lof's (predicative) type theory. The result is a strong and expressive language which has been used for formalization of mathematics, program specification and natural language. Motivated by the computational and richly expressive nature of UTT, this research has used it for formalization and semantic analysis of SemNet. Moreover, because of applications to software engineering, type checkers/proof assistants have been built. These tools are ideal for organising and managing the analysis of SemNet. The contribution of the work is twofold. First the semantic model built has led to improved and deeper understanding of SemNet. This is important as many researchers that work on different aspects of LOLITA, now have a clear and unambigious interpertation of the meaning of SemNet constructs. The model has also been used to show soundess of the valid reasoning and to give a reasonable semantic account of epistemic reasoning. Secondly the research contributes to NLE generally, both because it demonstrates that UTT is a useful formalization tool and that the good aspects of SemNet have been formally presented. Large-scale, Object based, Linguistic Interactor, Translator and Analyser		Simon Shiu	1996				AI	-22.41609645811175	9.206603152793843	123978
e4bcb2bd0e7601034ebbcbe0ee2adfcddb56ac4d	optimizing the resource allocation for approximate query processing		Query optimization techniques are a proven tool essential for high perfor- mance of the database management systems. However, in a context of data spaces or new querying paradigms, such as similarity based search, exact query evaluation is neither computationally feasible nor meaningful and approximate query evalua- tion is the only reasonable option. In this paper a problem of resource allocation for approximate evaluation of complex queries is considered and an approximate al- gorithm for an optimal resource allocation is presented, providing the best feasible quality of the output result subject to a limited total cost of a query.	optimizing compiler	Anna Yarygina;Boris Novikov	2012		10.1007/978-3-642-32741-4_27	sargable;query optimization;query expansion;computer science;theoretical computer science;data mining;database	DB	-26.127982605309967	4.645748259081332	124077
2b22c33051a957f0f66237929876b6b73577d154	answer set programming: an introduction to the special issue		FALL 2016 5 What is answer set programming, or ASP for short? Why is it drawing attention and continually gaining in acceptance as a computational problem-solving approach? These are the two key questions the seven articles in this special issue aim to answer. In a nutshell, ASP is a declarative problem solving paradigm — declarative, as all it requires users to do is to describe what the problem is, and not how to solve it. What distinguishes ASP from other declarative paradigms, like satisfiability (SAT) or constraint solving (CSP), is its underlying modeling language and the semantics involved. Problems are specified using logic programminglike rules, with some convenient extensions facilitating compact and readable problem descriptions. Sets of such rules, or answer set programs, come with an intuitive, well-defined and, by now, well-accepted semantics. This semantics has its roots in research in knowledge representation, in particular nonmonotonic reasoning, and avoids the pitfalls of earlier attempts such as the procedural semantics of Prolog based on negation as finite failure. This semantics was originally called the stable-model semantics and was defined for normal logic programs only, that is, programs consisting of rules with a single atom in the head and any finite number of atoms, possibly preceded by default negation, not, in the body. Stable models were later generalized to broader classes of programs, where the semantics can no longer be defined in terms of sets of atoms, which is a natural representation of classical models. Instead, it was defined by means of some sets of literals. For this reason the term answer set was adopted as more adequate (although answer sets also have a straightforward interpretation as models, albeit three-valued ones). Over the last decade or so, ASP has evolved into a vibrant and active research area that produced not only theoretical insights, but also highly effective and useful software tools and interesting and promising applications. The articles collected in this issue discuss these and other related aspects of	answer set programming;atom;boolean satisfiability problem;computation;computational problem;constraint satisfaction problem;declarative programming;human-readable medium;knowledge representation and reasoning;literal (mathematical logic);logic programming;modeling language;non-monotonic logic;problem solving;programming paradigm;prolog;stable model semantics	Gerhard Brewka;Thomas Eiter;Miroslaw Truszczynski	2016	AI Magazine		computer science;theoretical computer science;programming paradigm;inductive programming;algorithm	AI	-20.411988891358458	13.61259755399561	124101
218ca89fbe41b934886a51ef317b0239102cb371	merging conflicting propositional knowledge by similarity	random access memory;knowledge merging;data mining;qa75 electronic computers computer science;consistency conflicting propositional knowledge base knowledge merging similarity;nonvolatile memory;similarity;merging;merging weather forecasting artificial intelligence hamming distance fellows web services clouds;conflicting propositional knowledge base;ferroelectric films;meteorology;consistency;knowledge based systems;conference proceeding;knowledge base;qa76 computer software;knowledge engineering	The paper discusses a new approach to merging conflicting propositional knowledge bases which builds on the idea that consistency can often be restored by interpreting propositions more flexibly, thus enlarging their sets of models.	knowledge base;semantic similarity;stemming	Steven Schockaert;Henri Prade	2009	2009 21st IEEE International Conference on Tools with Artificial Intelligence	10.1109/ICTAI.2009.56	knowledge base;similarity;non-volatile memory;computer science;artificial intelligence;data science;machine learning;knowledge engineering;data mining;consistency	Robotics	-22.290656119104924	5.083072618922372	124412
ed1cd41227f1fcde456144e501645e38f9a27f08	méthodes de vérification de bases de connaissances	verification;methodes logiques;mathematiques appliquees;en francais;metaconnaissances;base connaissance;intelligence artificielle;verification automatique;coherence;informatique;systeme expert;representation connaissances	In this work, we present our approach for automatic checking of rule based systems. Atfer an overview of the related works, this report consists of two distincts parts. The first one deals with the consistency of rules in Knowledge Bases (KB) using the attribute-value formalism with forward chaining. We present the MELOMIDIA system, a consistency checker looking for the specifications of Initial Fact Bases (IFB) such that the deductive closure of IFB and the rule base contains contradictory facts. In the same process, MELOMIDIA also detects the redundant or unfireable rules of the KB. The second one considers the problem of verification of first-order Rule Bases (RB). Our approach consists of matching two kinds of knowledge: on one hand, the KB to be studied, on the other hand, specifications of properties of some predicates used in the KB. This latter core of knowledge is some kind of implicit knowledge, which must be acquired from the KB designer. This is made possible through a descriptive model of predicate we designed for this task. Going further, this model allows us to tackle some methodological aspects of the development of KB Systems.		Philippe Lafon	1991			linguistics;philosophy	SE	-19.6664505989126	7.893911987392352	124417
bd8eb680cc64dd65ad367b67e381603ce522c84a	maintenance of implication integrity constraints under updates to constraints	base donnee;mise a jour;maintenance;redundancia;semantic integration;database;partitioning;base dato;predicate calculus;integrite;integridad;ingenieria logiciel;satisfiability;constraint satisfaction;software engineering;calcul predicat;algorithme;algorithm;satisfaction contrainte;data semantics;integrity;redundancy;particion;calculo predicado;implication integrity constraints;integrity constraints;partition;genie logiciel;mantenimiento;puesta al dia;satisfaccion restriccion;information system;systeme information;updating;redondance;algoritmo;sistema informacion	Semantic integrity constraints are used for enforcing the integrity of the database as well as for improving the efficiency of the database utilization. Although semantic integrity constraints are usually much more static as compared to the data itself, changes in the data semantics may necessitate corresponding changes in the constraint base. In this paper we address the problems related with maintaining a consistent and non-redundant set of constraints satisfied by the database in the case of updates to the constraint base. We consider implication constraints as semantic integrity constraints. The constraints are represented as conjunctions of inequalities. We present a methodology to determine whether a constraint is redundant or contradictory with respect to a set of constraints. The methodology is based on the partitioning of the constraint base which improves the efficiency of algorithms that check whether a constraint is redundant or contradictory with respect to a constraint base.	algorithm;constraint (mathematics);data integrity;insertion sort;semantic web;time complexity	Naci Ishakbeyoglu;Z. Meral Özsoyoglu	1998	The VLDB Journal	10.1007/s007780050054	partition;constraint logic programming;constrained clustering;constraint programming;semantic integration;constraint satisfaction;computer science;theoretical computer science;first-order logic;data integrity;constraint satisfaction dual problem;database;redundancy;constraint;information system;algorithm;satisfiability	DB	-27.103851362381363	11.908005562270171	124521
e7bc7d3f842df40a03e5f7e52ae8501f9a44e195	modelling learners of a control task with inductive logic programming: a case study	learner model;regle inference;commande logique;intelligent tutoring system;apprentissage inductif;model generation;inductive logic programming;logical programming;control logico;inference rule;aprendizaje por induccion;programmation logique;inductive learning;logic control;learning artificial intelligence;programacion logica;programmation logique inductive;production rule;regla inferencia;systeme tutorial intelligent;apprentissage intelligence artificielle	We present results of using inductive logic programming (ILP) to produce learner models by behavioural cloning. Models obtained using a program for supervised induction of production rules (ripper) are compared to models generated using a well-known program for ILP (foil). It is shown that the models produced by foil are either too specific or too general, depending on whether or not auxiliary relations are applied. Three possible explanations for these results are: (1) there is no way of specifying to foil the minimum number of cases each clause must cover; (2) foil requires that all auxiliary relations be defined extensionally; and (3) the application domain (control of a pole on a cart) has continuous attributes. In spite of foil's limitations, the models it produced using auxiliary relations meet one of the goals of our exploration: to obtain more structured learner models which are easier to comprehend.	inductive logic programming	Ma. Guadalupe Quintana;Rafael Morales	2002		10.1007/3-540-46016-0_24	computer science;artificial intelligence;machine learning;programming language;algorithm;rule of inference;logic control	AI	-20.25229752405862	11.634822448821053	124525
39c5107f5cf525291375033567bde0b701701ffd	exchanging owl 2 ql knowledge bases	description logics;computational complexity;ontologies;knowledge representation	Knowledge base exchange is an important problem in the area of data exchange and knowledge representation, where one is interested in exchanging information between a source and a target knowledge base connected through a mapping. In this paper, we study this fundamental problem for knowledge bases and mappings expressed in OWL 2 QL, the profile of OWL 2 based on the description logic DL-LiteR. More specifically, we consider the problem of computing universal solutions, identified as one of the most desirable translations to be materialized, and the problem of computing UCQrepresentations, which optimally capture in a target TBox the information that can be extracted from a source TBox and a mapping by means of unions of conjunctive queries. For the former we provide a novel automata-theoretic technique, and complexity results that range from NP to EXPTIME, while for the latter we show NLOGSPACE-completeness.	abox;automata theory;automaton;computation;conjunctive query;description logic;exptime;knowledge base;knowledge representation and reasoning;np-completeness;ontology (information science);san diego supercomputer center;tbox;web ontology language	Marcelo Arenas;Elena Botoeva;Diego Calvanese;Vladislav Ryzhikov	2013			knowledge representation and reasoning;description logic;computer science;ontology;artificial intelligence;theoretical computer science;data mining;mathematics;computational complexity theory;algorithm	AI	-20.980263766456485	9.48385004402728	124610
4a7f1fea7899a2988fe12b82a45fac35a4c43791	seamless design: impedance mismatch revisited	query language;programming language;design and development;object oriented;information system	A pattern for information system implementation is proposed. The pattern is based on the commonly accepted object-oriented methodologies of design and development oriented toward widely used imperative programming languages. At the same time, this pattern makes it possible to fully use the power of declarative possibilities of the DBMS query language. The presented estimations demonstrate much higher performance of the applications obtained compared to those created by means of commonly accepted techniques.	database;declarative programming;impedance matching;imperative programming;information system;nominal impedance;object code;programming language;programming tool;query language;sql;seamless3d;speedup	P. G. Cherkasova;Boris Novikov	2006	Programming and Computer Software	10.1134/S0361768806050033	module pattern;very high-level programming language;data control language;computer science;theoretical computer science;database;rdf query language;fifth-generation programming language;programming language;object-oriented programming;information system;algorithm;query language	PL	-29.087265662921343	11.507926393727105	124712
1782aa4b075ef115ff3d2fbb1252492eba5463f3	representability of design objects by ancestor-controlled hierarchical specifications	databases;concepcion asistida;hierarchical module;computer aided design;base donnee;conditional expansion;module hierarchise;module alternatives;database;base dato;68p15;ingenieria logiciel;design objects;software engineering;conception objet;controle configuration;object design;versions;genie logiciel;conception assistee;68q25;version;hierarchical modules;expansion conditionnelle;configuration control;68r05	A simple model, called a VDAG, is proposed for succinctly representing hierarchically specified design data in CAD database systems where there are to be alternate expansions of hierarchical modules. The model uses an ancestor-based expansion scheme to control which instances of submodules are to be placed within each instance of a given module.The approach is aimed at reducing storage space in engineering design database systems and providing a means for designers to specify alternate expansions of a module.The expressive power of the VDAG model is investigated, and the set of design forests that are VDAG-generable is characterized. It is shown that there are designs whose representation via VDAGs is exponentially more succinct than is possible when expansion is uncontrolled. The problem of determining whether a given design forest is VDAG-generable is shown to be $NP$-complete, even when the height of the forest is bounded. However, it is shown that determining whether a given forest is VDAG-generable a...		Lin Yu;Daniel J. Rosenkrantz	1992	SIAM J. Comput.	10.1137/0221049	combinatorics;computer science;theoretical computer science;computer aided design;mathematics;configuration management;algorithm	Theory	-29.536585133736576	12.953621556068416	124784
8d65a0f7dc43691e373832a52c8e5f000e8edf6b	promis, a generic product information database system	performance;maintainability;database;metadata;database system;software development	In software development, a conflict exists between flexibility and maintainability of code on one side and performance on the other. In this paper an existing database system is presented. The system uses an approach that significantly improves flexibility and maintainability. A performance model has been developed for the system, making it possible to quantitatively compare the performance reduction with the increased maintainability. Validations using real-world data show that the performance model is very accurate. The model and validation shows that the performance loss of using the flexible approach is substantial. Based on insights gained from the performance model, we sketch how the performance can be substantially improved while keeping the flexibility and maintainability benefits.	database;problem-oriented medical information system	Wolfgang Diestelkamp;Lars Lundberg	1999			computer science;software development;physical data model;database;generic product;data modeling;database design;database testing	DB	-33.319584661209134	14.117628778945237	124901
1d098138eb0369cd2ba0c760d848a2221a0df211	finding top-n pseudo formal concepts with core intents	branch and bound algorithm	We discuss in this paper a method for finding  Top-N    Pseudo Formal Concepts  . A pseudo formal concept (pseudo FC in short) can be viewed as a natural approximation of formal concepts. It covers several formal concepts as its  majorities  and can work as a representative of them. In a word, such a pseudo FC is defined as a triple ( X  ,  Y  ,  S  ), where  X  is a closed set of objects,  Y  a set of  primary features  ,  S  a set of  secondary features  . Then, the concept tells us that 1) all of the objects in  X  are associated with the primary features  Y  and 2) for each secondary feature  y  ***  S  , a majority of  X  is also associated with  y  . Therefore,  X  can be characterized not only  exactly  by  Y  but also  naturally  and  flexibly  by  Y  *** {  y  } for each secondary feature  y  . Our task is formalized as a problem of finding  Top-N   ***-Valid (   ***  ,  ρ  ) -Pseudo Formal Concepts  . The targets can be extracted based on  clique search  . We show several pruning and elimination rules are available in our search. A depth-first branch-and-bound algorithm with the rules is designed. Our experimental result shows that a pseudo FC with a natural conceptual meaning can be efficiently extracted.		Yoshiaki Okubo;Makoto Haraguchi	2009		10.1007/978-3-642-03070-3_36	combinatorics;computer science;machine learning;mathematics;branch and bound;algorithm	DB	-20.74189549480957	5.430652102500633	124975
0172e9caf3dc204175368cab98e34c49def2add1	the network query language noah	query language;query languages;data base systems;path expressions	"""A non-procedural query language for network databases is described. The language is very powerful as compared to other implemented languages. It allows to formulate complex queries which, among other things, include conditions for CODASYL-sets and conditions for n:m-relationships. Cyclic database structures can be processed to some extent.Note: This report is not written for the """"naive"""" user of NOAH, and is not meant to replace a NOAH manual. Instead, this report describes the foundations, the underlying concepts, and the structure of the language in more theoretically oriented, technical terms.This research was performed at the university of Hagen, West Germany, and was partially supported by Triumph Adler AG, Nürnberg, West Germany."""	codasyl;database;query language	Gunter Schlageter;M. Rieskamp;U. Prädel;Rainer Unland	1982		10.1145/582353.582370	natural language processing;query optimization;data control language;computer science;query by example;database;rdf query language;programming language;algorithm;query language	DB	-26.233276935099287	11.632428288278707	125192
280e67b475dc52bc5c367f95c6ba2196495761fd	high-speed access control for xml documents	policy making;extensible markup language;query processing;indexation;xml document;access control;high speed	One of the important tasks of managing eXtensible Markup Language (XML) documents is to uniformly specify and securely maintain  both target documents and authorization policies. However, since existing techniques decouple access authorization from query  processing, the query processing time with access control is not satisfactorily fast. The access control requires the overhead  in addition to the time required for query processing, and the powerful expression of authorization policies makes it impossible  to accelerate the determination of access authorization. In this paper, we propose a bitmap-based access authorization technique  for both fast and secure services of XML documents. The authorization policies are encoded into a bitmap index and further  to be coupled an index of XML documents. User access requests are encoded into bitmap-masks. By applying bit-wise operations  of a bitmap-mask to the authorization bitmap index, authorization can be determined significantly fast. The contribution of  this paper includes 1) the dual goals achieved for fast and secure accesses to XML document collections, and 2) early propagation  of subjects and objects in bitmap indexes.  	access control;xml	Jong P. Yoon	2003		10.1007/1-4020-8070-0_19	well-formed document;xml catalog;ruleml;xml validation;xml encryption;xml base;xhtml;xml;xml schema;streaming xml;computer science;document type definition;document definition markup language;document structure description;xml framework;xml schema;pcdata;database;xml signature;world wide web;xml schema editor;information retrieval;efficient xml interchange;sgml	DB	-31.12042604344407	5.627925491253665	125241
4701f1965be2723a9ac430e2ffec1ab1fa6f3641	how does incoherence affect inconsistency-tolerant semantics for datalog±?	incoherence;inconsistency-tolerant semantics;argumentation;datalog;±;ontologies;68t27;68t30;68t35;68t37	The concept of incoherence naturally arises in ontological settings, specially when integrating knowledge. In the Datalog± literature, however, this is an issue that is yet to be studied more deeply. The main focus of our work is to show how classical inconsistency-tolerant semantics for query answering behaves when dealing with atoms that are relevant to unsatisfiable sets of existential rules, which may hamper the quality of answers and any reasoning task based on those semantics. We also propose a notion of incoherency-tolerant semantics for query answering in Datalog±, and exemplify this notion with a particular semantics based on the transformation of classic Datalog± ontologies into defeasible Datalog± ones, which use argumentation as its reasoning machinery.	acm guide to computing literature;best, worst and average case;co-np;co-np-complete;cobham's thesis;computational complexity theory;datalog;decision problem;defeasible logic;defeasible reasoning;description logic;equality-generating dependency;exemplification;existential quantification;information needs;knowledge base;knowledge representation and reasoning;literal (mathematical logic);logic programming;np (complexity);np-completeness;ontology (information science);p-complete;recommender system;relational database;turing completeness;while	Cristhian A. D. Deagustini;Maria Vanina Martinez;Marcelo A. Falappa;Guillermo Ricardo Simari	2016	Annals of Mathematics and Artificial Intelligence	10.1007/s10472-016-9519-5	natural language processing;mathematics;algorithm	AI	-21.195647180375182	8.3986337216365	125609
c18a5d1f224cb1f96c598ed20aa4707079e4ac95	the control layer in open mechanized reasoning systems: annotations and tactics	computacion informatica;computer algebra system;theorem prover;ciencias basicas y experimentales;grupo a;data structure	We are interested in developing a methodology for integrating mechanized reasoning systems such as Theorem Provers, Computer Algebra Systems, and Model Checkers. Our approach is to provide a framework for specifying mechanized reasoning systems and to use specifications as a starting point for integration. We build on the work presented by Giunchiglia et al.(1994) which introduces the notion of Open Mechanized Reasoning Systems (OMRS) as a specification framework for integrating reasoning systems. An OMRS specification consists of three components: the logic component, the control component, and the interaction component. In this paper we focus on the control level. We propose to specify the control component by first adding control knowledge to the data structures representing the logic by means of annotations and then by specifying proof strategies via tactics. To show the adequacy of the approach we present and discuss a structured specification of constraint contextual rewriting as a set of cooperating specialized reasoning modules. 2001 Academic Press.		Alessandro Armando;Alessandro Coglio;Fausto Giunchiglia;Silvio Ranise	2001	J. Symb. Comput.	10.1006/jsco.2000.0464	data structure;theoretical computer science;mathematics;automated theorem proving;reasoning system;algorithm;algebra	Logic	-24.824026734134222	18.15117842807985	125817
a7fa0b9d56e168b6ced8e98029f4c434203a1f56	unifying framework for rule semantics: application to gene expression data	microarray data;implication;expression profile;armstrong s axiom system;gene expression data;implications;data mining;functional dependency;gene regulatory network;rules;open source	The notion of rules is very popular and appears in different fl avors, for example as association rules in data mining or as functional dependencie s in databases. Their syntax is the same but their semantics widely differs. In the context of gene ex pr ssion data mining, we introduce three typical examples of rule semantics and for each one, we point out that Armstrong’s axioms are sound and complete. In this setting, we propose a unifying fr amework in which any “well-formed” semantics for rules may be integrated. We do not focus on the u nd rlying data mining problems posed by the discovery of rules, rather we prefer to discuss t he expressiveness of our contribution in a particular application domain: the understanding of gene r gulatory networks from gene expression data. The key idea is that biologists have the opportuni ty to choose among some predefined semantics or to define the meaning of their rules which best fi ts into their requirements. Our proposition has been implemented and integrated into an existing open-source system named MeV of the TIGR environment devoted to microarray data interpretatio n. An application has been performed on expression profiles of a sub-sample of genes from breast canc er tumors.	application domain;armstrong's axioms;association rule learning;axiomatic system;data mining;database;microarray;open-source software;requirement;reverse engineering;unsupervised learning;well-formed element;xml	Marie Pailloux;Jean-Marc Petit;Einoshin Suzuki	2007	Fundam. Inform.		microarray analysis techniques;gene regulatory network;computer science;bioinformatics;data mining;database;functional dependency;well-founded semantics;algorithm	ML	-23.19522337027159	11.652205167068798	125823
ddad140c5efe7d1ba55dff9ad3807b8ab8e0ef4d	hpm: a computational formalism for heuristic procedure modification	production system;new products	The HPM (Heuristic Procedure Modification) system is a model of strategy learning and optimization, implemented as a processing environment within the PRISM production system package [4]. This paper describes progress in getting HPM to emulate children's discoveries about basic addition procedures. HPM s goal-trace and production trace formalisms allow it to maintain a history of its actions which is both goal ordered and time ordered. Heuristics about meaningful patterns in these traces guide the construction of new productions, which modify procedures by replacing or circumventing preexisting productions.	heuristic (computer science);mathematical optimization;model of computation;prism (surveillance program);production system (computer science);semantics (computer science);tracing (software)	Robert Neches	1981			simulation;computer science;artificial intelligence;mathematics;production system;algorithm	ML	-19.564630767648307	15.497906715011379	126324
9d11a4c989d12fd896313075600f68b1c8626b61	an exhaustive matching procedure for the improvement of learning efficiency	learning algorithm;systeme apprentissage;educational software program;inductive logic programming;intelligence artificielle;algorithme apprentissage;didacticiel;learning systems;learning system;logique ordre 1;artificial intelligence;programa didactico;inteligencia artificial;algoritmo aprendizaje;deduccion;programmation logique inductive;first order logic;logica orden 1;deduction	Efficiency of the first-order logic proof procedure is a major issue when deduction systems are to be used in real environments, both on their own and as a component of larger systems (e.g., learning systems). Hence, the need of techniques that can speed up such a process. This paper proposes a new algorithm for matching first-order logic descriptions under θ-subsumption that is able to return the set of all substitutions by which such a relation holds between two clauses, and shows experimental results in support of its performance.	algorithm;archive;backtracking;computation;django;first-order logic;first-order predicate;heuristic (computer science);inductive reasoning;literal (mathematical logic);natural deduction;prolog;sld resolution	Nicola Di Mauro;Teresa Maria Altomare Basile;Stefano Ferilli;Floriana Esposito;Nicola Fanizzi	2003		10.1007/978-3-540-39917-9_9	computer science;artificial intelligence;machine learning;first-order logic;mathematics;algorithm	AI	-20.129442120233765	11.47990826122207	126435
9230d279a9c6a8379c94798fd7671454b9f3586e	a knowledge assimilation method for logic databases	database system;relational database;question answering system;logic programs	In this paper we consider a deductive question-answering system for relational databases as a logic database system, and propose a knowledge assimilation method suitable for such a system. The concept of knowledge assimilation for deductive logic is constructed in an implementable form based on the notion of amalgamating object language and metalanguage. This concept calls for checks to be conducted on four subconcepts, provability, contradiction, redundancy, independency, and their corresponding internal database updates. We have implemented this logic database knowledge assimilation program in PROLOG, a logic programming language, and have found PROLOG suitable for knowledge assimilation implementation.	data assimilation;object language;programming language;prolog;question answering;redundancy (engineering);relational database	Taizo Miyachi;Susumu Kunifuji;Hajime Kitakami;Koichi Furukawa;Akikazu Takeuchi;Haruo Yokota	1984	New Generation Computing	10.1007/BF03037329	database theory;description logic;question answering;horn clause;relational database;computer science;data mining;database;datalog;programming language;logic programming;algorithm;database design;autoepistemic logic	DB	-20.566253594418605	10.556519888947893	126576
407304a53c612e764fa917cca886b25c02b3e16e	delivering visual pertinent information services for commuters	mobile devices;location aware mobile application;information asymmetry;probe vehicle;real-time traffic flow;traffic flow prediction;traffic engineering computing;advanced traffic management systems;mobility environment;back end services;traffic management ecosystem;spatio-temporal;visual pertinent information services;2-level indexing scheme;urban environment;information services;matching mechanism;road traffic;mobile computing;grid cells;mobile services;traffic congestion problem;decision maker;indexation;spatial information;traffic flow;indexes;traffic management;advanced traffic management system;mobile device;mobile communication;data mining;real time systems	One of the major objectives of Advanced Traffic Management Systems (ATMS) is to reduce traffic congestion in urban environments by improving the efficiency of utilization of existing infrastructures. Many creative and efficient technologies have been developed over the years. Although, commuters especially drivers take a critical part in containing traffic congestion problems, they are playing a passive role in the traffic-management ecosystem. Considerably, this is due to the information asymmetry between ATMS decision makers and commuters; what is missing is a matching mechanism to create a bridge between information providers and information consumers in a mobility environment. We solve this dilemma through implementing visual pertinent information services for commuters. We use probe vehicles to estimate the real-time traffic flow and disseminate this information effectively to users' mobile devices. We propose a 2-level indexing scheme to effectively index the grid cells which contain the spatial information. Processed information is disseminated to users through wireless means and presented in a user friendly interface on users' mobile devices. We have implemented a location-aware mobile application and back-end services. Experimental results show that our system is effective and scalable.	ecosystem;location awareness;mobile app;mobile device;network congestion;real-time clock;relevance;scalability;usability	Wee Siong Ng;Justin Cheng	2009	2009 IEEE Asia-Pacific Services Computing Conference (APSCC)		simulation;computer science;operating system;mobile device;mobile computing;computer security;computer network	Mobile	-31.121375963745624	17.142955155629025	126648
204d766d6e8eb16569ef3966a285919ec51a0b3a	discovering associations between spatial objects: an ilp application	extraction information;inductive logic;base relacional dato;hierarchical structure;base donnee;analisis datos;information extraction;database;base dato;inductive logic programming;prior knowledge;logical programming;relational database;data mining;logique inductive;spatial database;association rule mining;data analysis;deductive database;programmation logique;base dato deductiva;fouille donnee;base donnee spatiale;decouverte connaissance;base donnee relationnelle;descubrimiento conocimiento;analyse donnee;base donnee deductive;knowledge discovery in database;programacion logica;busca dato;knowledge discovery;extraction informacion	In recent times, there is a growing interest in both the extension of data mining methods and techniques to spatial databases and the application of inductive logic programming (ILP) to knowledge discovery in databases (KDD). In this paper, an ILP application to association rule mining in spatial databases is presented. The discovery method has been implemented into the ILP system SPADA, which benefits from the available prior knowledge on the spatial domain, systematically explores the hierarchical structure of taskrelevant geographic layers and deals with numerical aspatial properties of spatial objects. It operates on a deductive relational database set up by selecting and transforming data stored in the underlying spatial database. Preliminary experimental results have been obtained by running SPADA on geo-referenced census data of Manchester Stockport, UK.	association rule learning;data mining;inductive logic programming;inductive reasoning;numerical analysis;relational database;spatial database	Donato Malerba;Francesca A. Lisi	2001		10.1007/3-540-44797-0_13	association rule learning;relational database;computer science;artificial intelligence;data science;machine learning;data mining;database;data analysis;information extraction;spatial database	ML	-30.121437626088408	7.88940095986042	126960
a41cf7254903c367d93f50f782153a4c0180d910	pxml: a probabilistic semistructured data model and algebra	relation algebra;query processing;efficient algorithm;tree data structures pxml probabilistic semistructured data model relational algebra semistructured instances query processing relational databases;tree data structures;data model;semistructured data;xml;relational databases;data models algebra data engineering;query processing xml data models relational algebra relational databases tree data structures;relational algebra;data models	Despite the recent proliferation of work on semistructured data models, there has been little work to date on supporting uncertainty in these models. In this paper, we propose a model for probabilistic semistructured data (PSD). The advantage of our approach is that it supports a flexible representation that allows the specification of a wide class of distributions over semistructured instances. We provide two semantics for the model and show that the semantics are probabilistically coherent. Next, we develop an extension of the relational algebra to handle probabilistic semistructured data and describe efficient algorithms for answering queries that use this algebra. Finally, we present experimental results showing the efficiency of our algorithms.	algorithm;coherence (physics);cycle (graph theory);data model;ibm notes;relational algebra;signal-to-noise ratio;xml	Edward Hung;Lise Getoor;V. S. Subrahmanian	2003		10.1109/ICDE.2003.1260814	data modeling;xml;codd's theorem;relational calculus;data model;relational algebra;relational database;computer science;theoretical computer science;data mining;relation algebra;database;conjunctive query;tree	DB	-27.077882038838805	8.506668384126272	127134
451273b6fe7db15204bc644418bded2d31bb034e	kappa-join: efficient execution of existential quantification in xml query languages	correlacion;query language;base donnee;execution time;lenguaje xpath;xml language;interrogation base donnee;004 informatik;database;xpath language;interrogacion base datos;base dato;lenguaje interrogacion;langage expression chemin;temps execution;decorrelation;langage interrogation;experimental evaluation;correlation;tiempo ejecucion;database query;langage xml;lenguaje xml	XML query languages feature powerful primitives for formul ating queries, involving comparison expressions which are exist ntially quantified. If such comparisons involve several scopes, they are correlat ed and, thus, become difficult to evaluate efficiently. In this paper, we develop a new ternary operator, called Kapp a-Join, for efficiently evaluating queries with existential quantificatio n. In XML queries, a correlation predicate can occur conjunctively and disjunctiv ely. Our decorrelation approach not only improves performance in the conjunctive c ase, but also allows decorrelation of the disjunctive case. The latter is not pos sible with any known technique. In an experimental evaluation, we compare the qu ery execution times of the Kappa-Join with existing XPath evaluation technique s to demonstrate the effectiveness of our new operator.	decorrelation;disjunctive normal form;existential quantification;fits;linear algebra;mathematical optimization;perf (linux);point of sale;query language;xml;xpath	Matthias Brantner;Sven Helmer;Carl-Christian Kanne;Guido Moerkotte	2006		10.1007/11841920_1	xml;decorrelation;computer science;theoretical computer science;database;programming language;correlation;algorithm;query language	DB	-26.974799555913307	10.385343291668493	127138
6f92d255570e7572fd15587f497f1bd418a909ae	temporal query processing and optimization in object-oriented databases	join algorithms;query output;optimisation;time reference associations;query processing;temporal data;temporal object oriented databases;enhanced path;ordering information;cost analysis;data model;cost analysis temporal query processing temporal query optimization temporal object oriented databases temporal object data model temporal object algebra decomposition strategy aggregation hierarchy associations time reference associations enhanced path sub paths time stamped class ordering information query output join algorithms stream processing techniques;query processing object oriented databases data models algebra costs algorithm design and analysis proposals object oriented modeling relational databases;algebra;data structures;decomposition strategy;stream processing techniques;database theory temporal databases object oriented databases query processing process algebra optimisation data structures;aggregation hierarchy associations;time stamped class;temporal databases;temporal object algebra;relational databases;object oriented databases;object oriented database;sub paths;stream processing;process algebra;temporal query optimization;proposals;database theory;algorithm design and analysis;object oriented modeling;temporal object data model;temporal query processing;data models	This paper investigates temporal query processing a d optimization in the context of object-oriented aktabases. Based on our temporal object data model and algebra, a strategy of decomposition is presented to process temporal queries that involve associations of both aggregation hierarchy and timereference. That is, evaluation of an enhanced path, which is a!ejined to e.xtend a path with tirnt-refrence, is decomposed by initially dividing the path into hvo sub-paths: one contairls the time-stamped class that can be optimized by making use of the ordering irlfornlarion of temporal &a and another is an ordirlaq sub-parlz which can be further decomposed and evaluated ining dizerent algorithms. The intermediate results of traversed two sub-paths are then joined together to create the q u q output. Algorithm of time-reluted operations and four basic join algorithms have been implemented with stream processing techniques and presented with cost analysis. Optimization issires have been discussed Temporal p r o p t m play an essential role in many real world applications. Most objectdented database (OODB) proposals and pt-relational products include constructors for complex types like lists and arrays that allow a time-stamped entity to be represented as a ‘’blob” (which is managed by the system, but intqreted solely by the application program); no facilities for temporal queries are provided [14,16]. Research on temporal databases (TDBs) has mainly focused on defining temporal data models by extending existing models, and developing access structures for temporal data [ll,I3,16,17, 201. Little work has k e n reported on temporal query processing and optimization. In this paper, we investigate historical data management in the context of OODBs. We explore an extensible approach to processing temporal queries within a uniform framework. By the uniform kamework we mean that temporal queries can be processed and optimized within an object-oriented query processing fiamework that is expanded from the relational kamework, by smoothly extending the existing query processing techniques. The exploration is based on our previously defined ternpal object data model and algebra (18,191. Due to the hierarchical structure of OUT temporal data model and the reducibility of the query algebra, we propose a decomposition strategy for processing t empa l queries that involve the associations of both aggregation hierarchy and time-reference. This sort of query is represented by an enhanced path that is defined to extend the path with time-references. Applying the strategy, the enhanced path is decomposed into two subpaths: one contains time-references and another is an ordinary path (involving no time-referenced class) that can be further demnpased. The intermediate results of traversed two subpaths are then joined togetha to c r d e the query output. We provide algorithms to process these sub-compnents of the query. optimization issues will also be discussed. As relational databases (RDBs) always require the users to explicitly join two relations, the temporal query processing in the context of RDBs has paid attention to specific temporal join algorithms [4,12,17, following the boaom up apprwh. OODBs significantly reduce the need for joins of classes. OODBs replace this explicit join with an implicit join (select operator). So path optimization becomes a difficult and central issue in object query processing that distinguishes objecturiented from relational query processing [1,9,10]. To the best of my knowledge, this paper is the first to put temporal query processing within a formal objectoriented query processing hmework that exploits the timedimension into path optimization. The remainder of this paper is organised as follows. Section 2 reviews OUT temporal object data model and algebra An extensible approach to precessing tempopal queries will be presented in Section 3. Section 4 provides algorithms to process the temporal queries. Optimization issues will be discussed in Section 5. Section 6 includes concluding remarks and future Work 2 A temporal object data model and algebra 0-8186-8147-0197 $10.00	algorithm;data model;join (sql);mathematical optimization;program optimization;relational database;smoothing;stream processing;temporal database	Lichun Wang;Mike Wing;Christopher W. Davis;Norman Revell	1997		10.1109/DEXA.1997.617334	query optimization;data structure;computer science;theoretical computer science;data mining;database;temporal database	DB	-29.084027590445675	7.4305880952403776	127202
96b58b7bd6b1977dd9b25536d84870bc2bb7d631	sequence datalog: declarative string manipulation in databases	query language	We investigate logic-based query languages for sequence databases , that is, databases in which strings of symbols over a xed alphabet can occur. We discuss diierent approaches to querying strings, including Prolog and Datalog with function symbols, and argue that all of them have important limitations. We then present the semantics of Sequence Datalog, a logic for querying sequence databases, and show how this language can be used to perform structural recursion over sequences.	datalog;declarative programming;prolog;query language;recursion;sequence database;string (computer science);structural induction	Anthony J. Bonner;Giansalvatore Mecca	1996		10.1007/BFb0031753	.ql;computer science;database;rdf query language;datalog;programming language;query language	DB	-24.671536880578287	12.406225317993734	127247
3be60ca1d272b3a24fb637738fba28e2c73a5bb2	constraint-generating dependencies	decision problem;compact representation;decision procedure;dependencies;data basesd;constraints	Traditionally, dependency theory has been developed for uninterpreted data. Specifically, the only assumption that is made about the data domains is that data values can be compared for equality. However, data is often interpreted and there can be advantages in considering it as such, for instance obtaining more compact representations as done in constraint databases. This paper considers dependency theory in the context of interpreted data. Specifically, it studies constraintgenerating dependencies. These are a generalization of equality-generating dependencies where equality requirements are replaced by constraints on an interpreted domain. The main technical results in the paper are a general decision procedure for the implication and consistency problems for constraint-generating dependencies, and complexity results for specific classes of such dependencies over given domains. The decision procedure proceeds by reducing the dependency problem to a decision problem for the constraint theory of interest, and is applicable as soon as the underlying constraint theory is decidable. The complexity results are, in some This work was supported by NATO Collaborative Research Grant CRG 940110 and by NSF Grant IRI-9110581. Dept. of Computer Science, Howard Hall, West Long Branch, NJ 07764, USA, Email: chomicki@moncol.monmouth.edu. Institut Montefiore, B28, 4000 Liège Sart-Tilman, Belgium, Email: pw@montefiore.ulg.ac.be.	classification research group;computer science;constraint logic programming;database;decision problem;dependency theory (database theory);email;grammar-based code;ibm notes;requirement;weatherstar	Marianne Baudinet;Jan Chomicki;Pierre Wolper	1999	J. Comput. Syst. Sci.	10.1006/jcss.1999.1632	mathematical optimization;discrete mathematics;dependency theory;computer science;join dependency;decision problem;mathematics;functional dependency;algorithm	DB	-25.591737201870536	10.720420447234906	127424
61fabe7304007717cb2df76f5828abc8c7b96cfb	the functional dependency model for logical database design	high level design evaluation;transaction specification language;functional dependency model;alternative design;solution space;irrelevant detail;data model;effective logical database design;minimal cost;high level;application requirement;database design;data models;process design;arithmetic;functional dependency	In order to have an effective logical database design tool, a designer should have facilities for specifying his data and application requirements at a high level. These specifications should be invariant with respect to a given realization, and should not require irrelevant details. Also, one should be able to compare alternative designs at a high level in order to reduce the solution space at minimal cost. This paper presents several concepts which are aimed toward the above objectives. Specifically, we define a data model, a transaction specification language, and an approach for high level design evaluation.	basis (linear algebra);data model;database design;database schema;design tool;feasible region;finite difference method;foremost;functional dependency;high- and low-level;high-level programming language;level design;process specification language;relevance;requirement	Barron C. Housel;Vance E. Waddle;S. Bing Yao	1979	Fifth International Conference on Very Large Data Bases, 1979.		process design;data modeling;computer science;theoretical computer science;data mining;database;functional dependency;programming language;database design	DB	-29.928168298315207	11.481380806918022	127622
5e80941eaabacb992b3b5d7668c7bd22d4acd7b2	a kernel method for measuring structural similarity between xml documents	information compatibility analysis;web service discovery;xml mining;kernel methods;data mining;schema matching;string kernel;xml document;kernel method;structural similarity	Measuring structural similarity between XML documents has become a key component in various applications, including XML data mining, schema matching, web service discovery, among others. The paper presents a novel structural similarity measure between XML documents using kernel methods. Results on preliminary simulations show that this outperforms conventional ones.	kernel method;structural similarity;xml	Buhwan Jeong;Daewon Lee;Hyunbo Cho;Boonserm Kulvatunyou	2007		10.1007/978-3-540-73325-6_57	xml catalog;xml validation;binary xml;xml encryption;kernel method;simple api for xml;xml;xml schema;streaming xml;computer science;document structure description;machine learning;xml framework;data mining;xml database;xml schema;database;xml signature;xml schema editor;information retrieval;efficient xml interchange	DB	-32.90662806583667	5.710076590258909	127698
1495c79a71172d16437f316a7a34ad15c4487c43	potminer: mining ordered, unordered, and partially-ordered trees	partial order;data mining;data structure	Non-linear data structures are becoming more and more common in data mining problems. Trees, in particular, are amenable to efficient mining techniques. In this paper, we introduce a scalable and parallelizable algorithm to mine partially-ordered trees. Our algorithm, POTMiner, is able to identify both induced and embedded subtrees in such trees. As special cases, it can also handle both completely ordered and completely unordered trees.	application domain;approximation algorithm;cluster analysis;data structure;embedded system;list of data structures;relational data mining;scalability;software design;software engineering;software maintenance;software mining;software repository;software system;structure mining;tree (data structure);xml	Aída Jiménez;Fernando Berzal Galiano;Juan C. Cubero	2009	Knowledge and Information Systems	10.1007/s10115-009-0213-3	partially ordered set;combinatorics;discrete mathematics;data structure;computer science;data mining;mathematics;weight-balanced tree;programming language	ML	-28.43452857437814	13.603269572247823	127711
7d02ab930dfcac8d04c7f39f5efc4e12f06b241f	distributed query processing a multiple database system	data transmission;database system;group by attribute semantic information distributed query processing multiple database system mermaid testbed system integrated access query optimization semijoin algorithm dynamic network functional dependency fragment attribute aggregate attribute;systems science and cybernetics heuristic programming;database management systems;information retrieval;distributed processing;informing science;query optimization;functional dependency;distributed query processing;semantic information;database systems;distributed;information retrieval database management systems database theory;processing speed;efficient query processing;query processing database systems aggregates system testing data communication cost function parallel processing spatial databases partitioning algorithms computer network management;database theory;dynamic networks	Mermaid is a testbed system which provides integrated access to multiple databases. Two query optimization algorithms have been developed for Mermaid. The semijoin algorithm tends to reduce the data transmission cost, while the replicate algorithm reduces the processing cost. An algorithm that integrates the features of these two algorithms to optimize the processing cost as well as the transmission cost is presented. A dynamic network environment is considered where processing speeds at each site and transmission speeds at each link can be variable. Moreover, distributed processing of aggregates is considered based on the functional dependency among the fragment attribute, the aggregate attribute, and the group-by attribute. Semantic information is utilized to obtain efficient query processing. >	database	Arbee L. P. Chen;David Brill;Marjorie Templeton;Clement T. Yu	1989	IEEE Journal on Selected Areas in Communications	10.1109/49.16871	query optimization;database theory;computer science;data mining;database;functional dependency;information retrieval;data transmission	DB	-27.671828219726436	4.531756663657518	127902
24378c4f99c410cada7f2474d22e79f5f3943abe	ltix: a compact level-based tree to index xml databases	experimental tests;labeling scheme;xml indexes;query processing;index structure;xml database;query optimization;data model;path indexes;indexation;structural indexes;labeling schemes;data models	Indexing XML data is essential for XML query optimization. Most of the existing approaches that combine a labeling scheme with a path index use labeling schemes that reflect the structure of the indexed data. In addition, the labeling rules do not depend on the combined path indexes. By designing a labeling scheme that does not reflect the structure of XML data, since it is available in the accompanied path index; and by aligning the data nodes' labels with the path index nodes' labels, we can support the join process more efficiently. We propose a novel index structure called LTIX (Level-based Tree Index for XML databases). This index structure is based on Level-based Labeling Scheme (LLS) that not only minimizes the number of joins and matches required to evaluate twig queries, if it is used with path indexes, but also facilitates effective query optimization through early pruning of the space search. Experimental tests show the performance benefits of our proposed approach.	mathematical optimization;query optimization;twig;xml database	Samir Mohammad;Patrick Martin	2010		10.1145/1866480.1866484	data modeling;query optimization;data model;computer science;data mining;xml database;database;information retrieval	DB	-30.256118752780896	4.417030709369694	128309
5acb1140455c5a5d52967d05bc488189b610a965	a multimedia presentation algebra	query language;relation algebra;beleif assertion;query optimization;interactive multimedia;inheritance and overriding;mls database;reasoning;multimedia presentation;deductive databases	Over the last few years, there has been a tremendous increase in the number of interactive multimedia presentations prepared by different individuals and organizations. In this paper, we present an algebra for querying multimedia presentation databases. In contrast to the relational algebra, an algebra for interactive multimedia presentations must operate on trees whose branches reflect different possible playouts of a family of presentations. The query language supports selection type operations for locating objects and presentation paths that are of interest to the user, join type operations for combining presentations from multiple databases into a single presentation, and finally set theoretic operations for comparing different databases. The algebra operations can be used to locate presentations with specific properties and also for creating new presentations by borrowing different components from existing ones. We prove a host of equivalence results for queries in this algebra which may be used to build query optimizers for interactive presentation databases.	computer;ibm notes;interpupillary distance;linear algebra;mathematical optimization;query language;query optimization;relational algebra;relational database management system;rewrite (programming);rewriting;scalability;set theory;toolbook;turing completeness	Sibel Adali;Maria Luisa Sapino;V. S. Subrahmanian	1999		10.1145/304182.304193	query optimization;computer science;theoretical computer science;relation algebra;database;interactive media;programming language;reason;query language	DB	-29.6879565472342	9.44306213093856	128367
4dfcac52a30d06b77639ecd80d0c6465ba543b40	an agent language with destructive assignment and model-theoretic semantics	theory and modeling;operational semantics;model theoretic semantics;agent languages;integrity constraints;logic programs;abductive logic programming;language model	In this paper we present an agent language that combines agent functionality with an action theory and model-theoretic semantics. The language is based on abductive logic programming (ALP), but employs a simplified state-free syntax, with an operational semantics that uses destructive assignment to manipulate a database, which represents the current state of the environment. The language builds upon the ALP combination of logic programs, to represent an agent’s beliefs, and integrity constraints, to represent the agent’s goals. Logic programs are used to define macro-actions, intensional predicates, and plans to reduce goals to sub-goals including actions. Integrity constraints are used to represent reactive rules, which are triggered by the current state of the database and recent agent actions and external events. The execution of actions and the assimilation of observations generate a sequence of database states. In the case of the successful solution of all goals, this sequence, taken as a whole, determines a model that makes the agent’s goals and beliefs all true.	abductive logic programming;abductive reasoning;action theory (philosophy);assignment (computer science);data assimilation;data integrity;intensional logic;operational semantics;ptc integrity	Robert A. Kowalski;Fariba Sadri	2010		10.1007/978-3-642-14977-1_16	computer science;theoretical computer science;programming language;well-founded semantics;operational semantics;algorithm;abductive logic programming	AI	-19.52408442754065	8.812068128663581	128382
258e8f5df96e23d40b628c010bc12d8e1cf59f16	computability and complexity issues of extended rdf	erdf stable model semantics;complexity issue;complexity issues;extended rdf;faithful extension;general undecidable;rdf graph;erdf ontology;rdfs semantics	ERDF stable model semantics is a recently proposed semantics for ERDF ontologies and a faithful extension of RDFS semantics on RDF graphs. Unfortunately, ERDF stable model semantics is in general undecidable. In this paper, we elaborate on the computability and complexity issues of the ERDF stable model semantics.	complexity;computability;ontology (information science);rdf schema;stable model semantics;undecidable problem	Anastasia Analyti;Grigoris Antoniou;Carlos Viegas Damásio;Gerd Wagner	2008		10.3233/978-1-58603-891-5-733	theoretical computer science;operational semantics;algorithm	AI	-22.505000063310153	8.689925402030395	128482
4fd71529001287bcf022b03cbe913d59afada6e6	index structures for preference database queries		Preference queries enable satisfying search results by delivering best matches, even when no tuple in a dataset fulfills all preferences perfectly. Several methods were developed for preference query processing, such as window-based, distributed, divide-and-conquer, and indexbased algorithms. In particular, all index-based algorithms were designed to evaluate Pareto preferences, where the participating preferences are all equally important. In this paper we present index structures for base preferences. Our comprehensive experiments show how indexing data for preference database queries enable faster access of the data tuples and therefore lead to performance advantages when evaluating preferences.	algorithm;cyclic redundancy check;data structure;database;database index;experiment;p (complexity);pareto efficiency;range query (data structures);range searching;sql;skyline operator;symposium on foundations of computer science;vldb	Markus Endres;Felix Weichmann	2017		10.1007/978-3-319-59692-1_12	tuple;data mining;information retrieval;pareto principle;graph database;database;search engine indexing;range query (data structures);computer science	DB	-27.496356932839195	5.1425784738578075	128490
18fbc94882661f152570407a0e6e9bfe0683e922	fuzzy ontologies for specialized knowledge representation in wordnet		Despite the undisputed success of ontologies, they are not appropriate to represent and reason with vague knowledge. WordNet, a widely used ontology for general/natural language tasks, presents some of these problems when it is applied to specialized discourse. In this paper, we propose to use fuzzy ontologies, which combine Fuzzy Logic theory and Description Logics, to represent the imprecise notions of prototypicality and representativeness inside a synonym set, semantic similarity, and hyponymic degree in WordNet. We show that this approach is particularly appropiate to combine WordNet with other specific terminological resources, such as the environmental knowledge base EcoLexicon.	definition;description logic;fuzzy logic;fuzzy set;hoc (programming language);knowledge base;knowledge representation and reasoning;natural language;ontology (information science);semantic network;semantic similarity;synonym ring;theory (mathematical logic);vagueness;wordnet	Fernando Bobillo;Juan Gómez-Romero;Pilar León Araúz	2012		10.1007/978-3-642-31709-5_44	natural language processing;computer science;data mining;information retrieval	AI	-21.13358914523837	5.456599248984811	128515
62134f4e6d32f7786d1a03d1163b8ed7232c3f37	the (almost) complete guide to tree pattern containment	dtd;complexity;containment;satisfiability;graphs;schema;trees;validity;xpath;xml;optimization;tree patterns	Tree pattern queries are being investigated in database theory for more than a decade. They are a fundamental and flexible query mechanism and have been considered in the context of querying tree structured as well as graph structured data. We revisit their containment, validity, and satisfiability problem, both with and without schema information. We present a comprehensive overview of what is known about the complexity of containment and develop new techniques which allow us to obtain tractability- and hardness results for cases that have been open since the early work on tree pattern containment. For the tree pattern queries we consider in this paper, it is known that the containment problem does not depend on whether patterns are evaluated on trees or on graphs. This means that our results also shed new light on tree pattern queries on graphs.	boolean satisfiability problem;database theory;graph (abstract data type)	Wojciech Czerwinski;Wim Martens;Pawel Parys;Marcin Przybylko	2015		10.1145/2745754.2745766	complexity;xml;computer science;document type definition;theoretical computer science;incremental decision tree;k-ary tree;schema;database;containment;fractal tree index;tree structure;search tree;tree;graph;tree traversal;algorithm;validity;satisfiability	DB	-24.211496658135676	10.686431154975065	128642
8a1760ed09d3a7fd950bdcd2a53377948df5a0ed	answer set programs with queries over subprograms		Answer-Set Programming (ASP) is a declarative programming paradigm. In this paper we discuss two related restrictions and present a novel modeling technique to overcome them: (1) Meta-reasoning about the collection of answer sets of a program is in general only possible by external postprocessing, but not within the program. This prohibits the direct continuation of reasoning based on the answer to the query over a (sub)program’s answer sets. (2) The saturation programming technique exploits the minimality criterion for answer sets of a disjunctive ASP program to solve co-NP-hard problems, which typically involve checking if a property holds for all objects in a certain domain. However, the technique is advanced and not easily applicable by average ASP users; moreover, the use of default-negation within saturation encodings is limited. In this paper, we present an approach which allows for brave and cautious query answering over normal subprograms within a disjunctive program in order to address restriction (1). The query answer is represented by a dedicated atom within each answer set of the overall program, which paves the way also for a more intuitive alternative to saturation encodings and allows also using defaultnegation within such encodings, which addresses restriction (2).	answer set programming;atom;character encoding;co-np;continuation;declarative programming;disjunctive normal form;hex;microsoft outlook for mac;np-hardness;programming paradigm;rewriting;saturation arithmetic;semantic reasoner;stable model semantics;subroutine	Christoph Redl	2017		10.1007/978-3-319-61660-5_15	computer science;theoretical computer science;declarative programming;continuation;metaprogramming;exploit;answer set programming;non-monotonic logic	AI	-22.758961442159816	11.597595448296424	128647
c4081f99a6f4874ae9bfcdfcca3db2be5c92c83b	ontological meta-properties of derived object types	derived object types;ontological foundations;meta properties	In this paper, we revisit a number of classical formal meta-properties that have been used in the conceptual modeling and ontology engineering literature to provide finer-grained distinctions among the category of Object Types. These distinctions constitute an essential part of relevant existing approaches, in particular, the ontology-driven conceptual modeling language OntoUML, and the ontology and taxonomy evaluation methodology OntoClean. The idea in this paper is to investigate the interaction between these meta-properties and Derived Object Types, i.e., Object Types which extensions are dynamically inferred via Derivation Rules. The contributions here are two-fold: firstly, we revisit two classical Derivation Patterns and prove a number of results that can be used to infer the modal meta-properties of Derived Types from those of the types participating in the associated derivation rules; secondly, we demonstrate how these results can be applied in the automated support for model construction in OntoUML.	derived object;modal logic;modeling language;object type (object-oriented programming);ontouml;ontology engineering	Giancarlo Guizzardi	2012		10.1007/978-3-642-31095-9_21	artificial intelligence;data mining;algorithm	SE	-22.443812348914648	9.145383634918046	128758
e3dca0c3d80dc7cac1d53a6f25b216357be3036d	incrementally maintaining materializations of ontologies stored in logic databases	ontologie;base donnee;maintenance;web semantique;interrogation base donnee;database;logica descripcion;interrogacion base datos;base dato;vista materializada;materialized view;deductive database;datalog;base dato deductiva;web semantica;semantic web;mantenimiento;ontologia;base donnee deductive;description logic;logic programs;ontology;database query;logique description;deductive databases;vue materialisee	This article presents a technique to incrementally maintain materializations of ontological entailments. Materialization consists in precomputing and storing a set of implicit entailments, such that frequent and/or crucial queries to the ontology can be solved more efficiently. The central problem that arises with materialization is its maintenance when axioms change, viz. the process of propagating changes in explicit axioms to the stored implicit entailments. When considering rule-enabled ontology languages that are operationalized in logic databases, we can distinguish two types of changes. Changes to the ontology will typically manifest themselves in changes to the rules of the logic program, whereas changes to facts will typically lead to changes in the extensions of logical predicates. The incremental maintenance of the latter type of changes has been studied extensively in the deductive database context and we apply the technique proposed in [31] for our purpose. The former type of changes has, however, not been tackled before. In this article we elaborate on our previous papers [34,35], which extend the approach of [31] to deal with changes in the logic program. Our approach is not limited to a particular ontology language but can be generally applied to arbitrary ontology languages that can be translated to Datalog programs, i.e. such as O-Telos, F-Logic [17] RDF(S), or Description Logic Programs [36].	datalog;deductive database;description logic;f-logic;logic programming;ontology (information science);precomputation;viz: the computer game	Raphael Volz;Steffen Staab;Boris Motik	2005	J. Data Semantics	10.1007/978-3-540-30567-5_1	materialized view;description logic;computer science;artificial intelligence;semantic web;ontology;data mining;database;ontology language;datalog;programming language;algorithm	AI	-24.921849381496347	8.851394320755665	128781
88d676331d0287770a088c4e870491fa540378b6	modeling and management of spatio-temporal objects within temporal gis application framework	application framework;object oriented model;object oriented methods;geographic information;storage management;object database;geographic information systems object oriented modeling data models spatial databases object oriented databases relational databases database systems kernel application software computer graphics;generic framework spatio temporal object management temporal gis application framework spatiotemporal thematic data object oriented gis application framework object oriented modeling concepts temporal geographic information conceptual spatio temporal object model standard uml class diagram notation object oriented application domain object relational database domain spatio temporal object database kernel spatio temporal object storage manager temporal gis application functionality functional components geographic information internet based temporal gis frameworks;uml class diagram;object relational databases;object oriented;specification languages;geographic information systems;temporal databases;relational databases;object oriented databases;object model;relational databases geographic information systems object oriented databases specification languages object oriented methods temporal databases visual databases;visual databases	In this paper, modeling and management of spatiotemporal-thematic data within an object-oriented GIs application framework, are presented. The objectoriented modeling concepts have been applied in integration of spatial, thematic and temporal geographic information in the conceptual spatio-temporal object model presented using standard UML class diagram I notation. By its implementation in object-oriented application domain and (object-) relational database domain, spatio-temporal object database kernel has been developed as a spatio-temporal object storage manager. Based on spatio-temporal object database kernel and development of appropriate components on top of it for temporal GIs application functionality, a temporal GIS application framework has been developed. Description of its architecture and functional components dedicated to management of temporal aspect of geographic information is given. Desktopand internet-based temporal GIs frameworks are further refined from generic framework.	application domain;application framework;class diagram;geographic information system;internet;kernel (operating system);object storage;relational database;unified modeling language	Dragan Stojanovic;Slobodanka Djordjevic-Kajan;Zoran Stojanovic	2001		10.1109/IDEAS.2001.938092	enterprise gis;object model;relational database;computer science;theoretical computer science;class diagram;data mining;database;geographic information system;temporal database;programming language;object-oriented programming	DB	-33.38419416215363	12.54873530476238	128865
4ea871de48eefbb16172abf2b102f2db56b57b57	approximate query answering in inconsistent databases		Classical algorithms for query optimization presuppose th e absence of inconsistencies or uncertainties in the database and exploit onl y valid semantic knowledge provided, e.g., by integrity constraints. Data incons istency or uncertainty, however, is a widespread critical issue in ordinary databas es: total integrity is often, in fact, an unrealistic assumption and violations to in egrity constraints may be introduced in several ways. In this report we present an approach for semantic query opti mization that, differently from the traditional ones, relies on not necessari ly valid semantic knowledge, e.g., provided by violated or soft integrity constrai nts, or induced by applying data mining techniques. Query optimization that leverages invalid semantic knowledge cannot guarantee the semantic equivalence between the original user’s query and its rewriting: thus a query optimized by our approach yie lds approximate answers that can be provided to the users whenever fast but poss ibly partial responses are required. Also, we evaluate the impact of use of invalid s emantic knowledge in the rewriting of a query by computing a measure of the quali ty of the answer returned to the user, and we rely on the recent theory of Belie f Logic Programming to deal with the presence of possible correlation in the sema ntic knowledge used in the rewriting.	approximation algorithm;cisco ios;data integrity;data mining;experiment;logic programming;mathematical optimization;predicate (mathematical logic);query optimization;rewrite (programming);rewriting;semantic query;turing completeness	Federica Panella	2014	CoRR		sargable;query optimization;query expansion;web query classification;boolean conjunctive query;computer science;data mining;database;information retrieval;query language	DB	-25.51397000444103	8.133845011866548	128886
3bb462f18906aec36c1178f375e9cee29e22f313	replicaiton in mobile information systems	mobile information system	Replication techniques are already used in distributed database systems for increasing performance and availability. But also mobile information systems (MIS) need replication for making required data available at any time at every place. In this paper we discuss what replication in MIS means and which additional requirements are caused by mobility.	distributed database;information system;microsoft outlook for mac;peer-to-peer;preprocessor;requirement	Hagen Höpfner	2002			mobile identification number;mobile search;mobile web;public land mobile network;gsm services;mobile database;mobile technology;location-based service;mobile deep linking;mobile station;mobile computing	DB	-31.103698398328973	16.793338023198267	128955
04085f0094e6bebf3ba7e60d6793e78d9d68308b	a simple and expressive semantic framework for policy composition in access control	query language;multi valued logic;bilattices;conference paper;access control policy;access control policy languages;access control	In defining large, complex access control policies, one would like to compose sub-policies, perhaps authored by different organizations, into a single global policy. Existing policy composition approaches tend to be ad-hoc, and do not explain whether too many or too few policy combinators have been defined. We define an access controlpolicy as a four-valued predicate that maps accesses to either grant, deny, conflict, or unspecified. These correspond to the four elements of the Belnap bilattice. Functions on this bilattice are then extended to policies to serve as policy combinators. We argue that this approach provides a simple andnatural semantic framework for policy composition, with a minimal but functionally complete set of policy combinators. We define derived, higher-level operators that are convenient for the specification of access control policies, and enable the decoupling of conflict resolution from policy composition. Finally, we propose a basic query language and show that it can reduce important analyses (e.g., conflict analysis) to checks of policy refinement.	access control;combinatory logic;coupling (computer programming);functional completeness;hoc (programming language);query language;refinement (computing)	Glenn Bruns;Daniel S. Dantas;Michael Huth	2007		10.1145/1314436.1314439	computer science;access control;data mining;database;programming language;query language	Security	-25.044566191234217	13.314985616038536	129004
1e6283695c534beda576e272c24438847cc0b320	querying and updating probabilistic information in xml	modelizacion;tratamiento transaccion;base donnee;confiance;mise a jour;structure arborescente;xml language;web semantique;logique floue;interrogation base donnee;database;service web;xml database;interrogacion base datos;base dato;logica difusa;almacen dato;probabilistic approach;web service;fuzzy logic;actualizacion;expressive power;modelisation;semistructured data;confidence;dato semi estructurado;confianza;estructura arborescente;enfoque probabilista;approche probabiliste;web semantica;tree structure;semantic web;entrepot donnee;data warehouse;transaction processing;modeling;database query;langage xml;lenguaje xml;updating;traitement transaction;servicio web;donnee semistructuree	We present in this paper a new model for representing probabilistic information in a semi-structured (XML) database, based on the use of probabilistic event variables. This work is motivated by the need of keeping track of both confidence and lineage of the information stored in a semi-structured warehouse. For instance, the modules of a (Hidden Web) content warehouse may derive information concerning the semantics of discovered Web services that is by nature not certain. Our model, namely the fuzzy tree model, supports both querying (tree pattern queries with join) and updating (transactions containing an arbitrary set of insertions and deletions) over probabilistic tree data. We highlight its expressive power and discuss implementation issues.	expressive power (computer science);fuzzy logic;level of detail;lineage (evolution);mathematical optimization;query optimization;semi-structured model;semiconductor industry;web service;xml database	Serge Abiteboul;Pierre Senellart	2006		10.1007/11687238_62	fuzzy logic;web service;xml;systems modeling;transaction processing;computer science;data warehouse;semantic web;data mining;xml database;database;tree structure;confidence;world wide web;expressive power;divergence-from-randomness model	DB	-27.982197006811834	11.281849944024524	129039
5ef04dc250d8deb8ad5b412de43e6454750606e7	oracle regular expressions - pocket reference: tutorial and quick reference: new in oracle database 10g	regular expression		handbook;oracle database;regular expression	Jonathan Gennick;Peter Linsley	2003			pl/sql;computer science;theoretical computer science;database;programming language	DB	-31.31298928083094	6.569767211517947	129069
4f4a67507164429c311ab62866cfbc00c9b0fccd	implementing general purpose applications with the rule-based approach	rule-based approach;formal property;certain knowledge representation;example knowledge representation;layer architecture;extended tabular trees;rbs approach;example application;general purpose application;rbs architecture;prototype rbs	Using Rule Based Systems (RBS) for implementing general purpose applications makes verification of their formal properties feasible - especially conformance of such applications to their design. To make the RBS approach suitable for general purpose applications an RBS architecture and a certain knowledge representation should be engineered. The paper proposes both the architecture (the Four Layer Architecture - FLA) and an example knowledge representation (Extended Tabular Trees - XTT2). A prototype RBS and an example application which acknowledges the approach are also discussed.		Igor Wojnicki	2011		10.1007/978-3-642-22546-8_29	simulation;computer science;database;algorithm	Crypto	-21.89816269378637	15.332289081824298	129276
f611bb85293f6e8e3efef7e93b59057df4805ed7	cooperative relational database querying using multiple knowledge bases	query formulation;relational database;artificial intelligent;knowledge base	Copyright © 1999, American Association for Artificial Intelligence (www.aaai.org). All rights reserved. +On sabbatical leave at University of Florida, IFAS-IT Abstract We present in this paper an approach for providing cooperativeness in database querying using artificial intelligence techniques. The main focus is a cooperative interf ac that assists non-experienced and casual users in extracting useful answers from a relational database. We propose an architecture for our approach that comprises two knowledge bases which store rules that describe the application domain and that guide the process of query formulation and answering. A subset of SQL is used for expressing queries, and the cooperative interface relieves the user from knowing its full syntax and also the database schema.	application domain;artificial intelligence;database schema;knowledge base;query language;relational database;sql	José Luís Braga;Alberto H. F. Laender;Claudiney Vander Ramos	1999			data definition language;knowledge base;query optimization;relational model;intelligent database;entity–relationship model;relational database;computer science;knowledge management;artificial intelligence;query by example;database model;data mining;database;conjunctive query;view;database schema;alias;database design	AI	-31.36257716057756	8.768353629559591	129559
eb2718d8253ed07d984ea562edb0c533be87955d	roster of programming languages for 1974-75	programming language;high level language	"""This roster contains a list of currently existing high-level languages which (a) have been develcped or reported in the United States; (b) have been implemented on at least one general-purpose computer; and (c) are believed to ~e in use in the United States by someone other than the developer. In a few cases of exceptional interest, these basic ground rules have been violated; in p~rticular, some new languages have been included even though their actual broad usage is somewhat less certain. The reason for excluding non-U.S, activities is definitely n o~ because they are deemed inferior; the reasons are (a) the time and effort to include them would be too great and (b) the multiple sources on non-~.S, developments are not generally available to th~ author and a partial list is deemed worse than no list. while every effort has b~en made to make this list both complete and accurate, inevitably some errors of omission or commission have occurred. The author would appreciate notice of these errors for cbr~ection in futur~ rosters, subject to the constraints in the following paragraph. No attempt has been made to decide whether a language is """"good"""": if it satisfies the criteria indicated above and below, it is included. However, since a listing of this type inevitably represents certain technical value judgments, it should be made clear that the opinions stated or implied are (only) the personal views of the author."""	computer;emoticon;general-purpose modeling;high- and low-level	Jean E. Sammet	1976	Commun. ACM	10.1145/953777.953783	fourth-generation programming language;first-generation programming language;natural language programming;very high-level programming language;language primitive;programming domain;computer science;programming language generations;programming language implementation;third-generation programming language;functional logic programming;compiled language;programming paradigm;low-level programming language;fifth-generation programming language;programming language theory;programming language;programming language specification;second-generation programming language;high-level programming language;comparison of multi-paradigm programming languages	PL	-23.84861095261956	18.193615071445834	129615
672f919858c25a87941d0cda61fd3f8df64b4799	on the intersection of xpath expressions	information systems;finite automata xpath expressions xml document semistructured data xml databases indexed data modifying database operation;xpath expressions;xml database;modifying database operation;xml indexes management information systems navigation database systems data structures software design automata computational modeling information systems;semistructured data;automata;indexes;navigation;computational modeling;data structures;indexation;database systems;finite automata;xml;xml document;management information systems;software design;path expressions;xml databases;indexed data	XPath is a common language for selecting nodes in an XML document. XPath uses so called path expressions which describe a navigation path through semistructured data. In the last years some of the characteristics of XPath have been discussed. Examples include the containment of two XPath expressions p and p' (p /spl sube/ p'). To the best of our knowledge the intersection of two XPath expressions (p /spl cap/ p') has not been treated yet. The intersection of p and p' is the set that contains all XML nodes that are selected both by p and p'. In the context of indexes in XML databases the emptiness of the intersection of p and p' is a major issue when updating the index. In order to keep the index consistent to the indexed data, it has to be detected if an index that is defined upon p is affected by a modifying database operation with the path expression p'. In this paper, we introduce the intersection problem for XPath and give a motivation for its relevance. We present an efficient intersection algorithm for XPath expressions without the NOT operator that is based on finite automata. For expressions that contain the NOT operator the intersection problem becomes NP-complete leading to exponential computations in general. With an average case simulation we show that the NP-completeness is no significant limitation for most real-world database operations.	automata theory;best, worst and average case;computation;finite-state machine;intersection algorithm;np-completeness;path expression;relevance;simulation;time complexity;xml database;xpath	Beda Christoph Hammerschmidt;Martin Kempa;Volker Linnemann	2005	9th International Database Engineering & Application Symposium (IDEAS'05)	10.1109/IDEAS.2005.39	xml;data structure;computer science;theoretical computer science;path expression;management information systems;data mining;xml database;database;finite-state machine	DB	-28.887405646833713	6.992636574098404	129983
d4c64a1097d2f1331e648767e44fd6c40caf702c	record matching in data warehouses: a decision model for data consolidation	computers;decision support systems record matching programming;decision models;computers databases data warehousing data consolidation;heuristic systems;databases data warehousing;data warehouse;programming integer algorithms heuristic systems;information systems decision support systems record matching;integer algorithms;data consolidation information systems	The notion of a data warehouse for integrating operational data into a single repository is rapidly becoming popular in modern organizations. An important issue in the integration process is how to deal with the identifier mismatch problem when combining similar data from disparate sources. A real-world entity may be represented using different identifiers in different operational data sources, and matching them may often be difficult using simple database operations expressed, say, as an SQL query. A record-by-record manual matching is also not practical because the databases may be large. A decision model is presented that combines probability-based automated matching with manual matching in a cost minimization formulation. A heuristic approach is proposed for solving the decision model. Both the model and the heuristic solution approach have been tested on real data. The results from the testing indicate that the model can be effectively used in real-world situations.	semiconductor consolidation	Debabrata Dey	2003	Operations Research	10.1287/opre.51.2.240.12779	decision model;computer science;theoretical computer science;data warehouse;data mining;database	DB	-26.22476645590675	5.557977273132444	130080
3f31123cde4873000a77072cbf0d412ce45ae653	interpretation of natural language database queries using optimization methods	databases;lenguaje natural;evaluation performance;optimisation;impedance;computer languages;base donnee;performance evaluation;constraint optimization;optimizacion;evaluacion prestacion;optimal method;langage naturel;database;formal languages;pregunta documental;natural languages;operations research;question documentaire;interpretacion;anglais;mathematical programming;natural languages databases optimization methods constraint optimization computer languages program processors artificial intelligence formal languages impedance operations research;natural language;query;interpretation;artificial intelligence;optimization;english;base datos;ingles;programmation mathematique;program processors;programacion matematica;database query;optimization methods	The automatic interpretation of natural language (English) database questions formulated by a user untrained in the technical aspects of database querying is an established problem in the field of artificial intelligence. State-of-the-art approaches involve the analysis of queries with syntactic and semantic grammars expressed in phrase structure grammer or transition network formalisms. With such methods difficulties exist with the detection and resolution of ambiguity, with the misinterpretation possibilities inherent with finite length look-ahead, and with the modification and extension of a mechanism for other sources of semantic knowledge. The potential of optimization techniques to solve these problems and interpret natural language database queries is examined.	artificial intelligence;database;mathematical optimization;natural language;phrase structure rules	William Ernest Leigh;James Evans	1986	IEEE Transactions on Systems, Man, and Cybernetics	10.1109/TSMC.1986.289280	natural language processing;constrained optimization;computer science;artificial intelligence;theoretical computer science;machine learning;natural language;graph database;algorithm	DB	-25.171914750027252	12.603194332836294	130408
d4ef90e766cc64eafede72b1901f70274469dbf9	modeling and querying shapes in an image database system				Vincent Oria;M. Tamer Özsu;L. Irene Cheng;Paul Iglinski;Yuri Leontiev	1999			automatic image annotation;database;information retrieval;data mining;database design;spatiotemporal database;computer science	DB	-31.285627970102794	8.351140779550658	130638
3234c4a2ba26742fe7a41fc6144e3cad07324f48	relational database constraints as counterexamples	search and retrieval;relational database;first order;normal form;first order logic	 First order static database constraints are expressed as counterexamples, i.e., examples that violate the integrity of the database. Examples are data and as such they can be specified and stored as data, and structured into database files for efficient search and retrieval. To express all first order constraints as counterexamples, a new normal form for first order logic is created which, after some syntactic transformation, is amenable to storage in flat files, and efficient search and retrieval. The critical contribution is the ability to manage a large number constraints on secondary storage devices, and eliminate the requirement to transport all constraints to main memory for testing.	automated theorem proving;computer data storage;conjunctive normal form;first-order logic;relational database	Levent V. Orman	1997	Acta Informatica	10.1007/s002360050078	relational model;relational database;computer science;theoretical computer science;database normalization;first-order logic;data mining;database;mathematics;programming language;database schema;algorithm;database design	DB	-26.81795893499582	10.45283923222608	130816
f66a097d046ef9ad687a099c1b59ab688a792c4b	constraint-based automation of multimedia presentation assembly	base donnee;multimedia;satisfiabilite;database;priorite;base dato;automatisation;automatizacion;satisfiability;consistencia;contrainte presentation;upper bound;contrainte inclusion exclusion;prototipo;assembly;multimedia data;consistance;montage;montaje;multimedia presentation;priority;prioridad;prototype;segment selection problem;consistency;multimedia database system;automation	In this paper, we present a methodology for automated construction of multimedia presentations. Semantic coherency of a multimedia presentation is expressed in terms of presentation inclusion and exclusion constraints. When a user specifies a set of segments for a presentation, the multimedia database system adds segments into and/or deletes segments from the set in order to satisfy the inclusion and exclusion constraints. We discuss the consistency and the satisfiability of inclusion and exclusion constraints when exclusion is allowed. Users express a presentation query by (a) pointing and clicking to an initial set of desired multimedia segments to be included into the presentation, and (b) specifying an upper bound on the time length of the presentation. The multimedia database system then finds the set of segments satisfying the inclusion-exclusion constraints and the time bound. Using priorities for segments and inclusion constraints, we give two algorithms for automated presentation assembly and discuss their complexity. To automate the assembly of a presentation with concurrent presentation streams, we introduce presentation organization constraints that are incorporated into the multimedia data model, independent of any presentation. We define four types of presentation organization constraints that, together with an underlying database ordering, allow us to obtain a unique presentation graph for a given set of multimedia segments. We briefly summarize a prototype system that fully incorporates the algorithms for the segment selection problem.	data model;database;emoticon;prototype;selection algorithm	Veli Hakkoymaz;Joel Kraft;Gultekin Özsoyoglu	1999	Multimedia Systems	10.1007/s005300050150	presentation logic;computer science;theoretical computer science;operating system;automation;database;assembly;prototype;multimedia;upper and lower bounds;consistency;world wide web;algorithm;satisfiability	DB	-27.96824189975019	15.707269249375303	130902
d27391eb92c51bb588e2b828a4299f4e4139c1bb	attribute grammars for the heuristic translation of query languages	query language;interfase usuario;linguistique;base donnee;probability;user interface;accesibilidad;relacion hombre maquina;heuristic method;interrogation base donnee;database;interrogacion base datos;base dato;man machine relation;metodo heuristico;attribute grammar;lenguaje interrogacion;user assistance;linguistica;assistance utilisateur;accessibility;probabilidad;asistencia usuario;probabilite;interface utilisateur;langage interrogation;relation homme machine;methode heuristique;database query;accessibilite;linguistics	This work is motivated by the necessity of providing a high-level user-interface for database query languages to improve the accessibility of information to the nonexpert users. The goal is to achieve portable, adaptable query languages which are supported by mechanisms that allow one to change the query language without changing the implementation. An attribute grammar is used for semantic description of the language. Imprecise terms which are hedges (e.g. very, almost) or nonstandard quantifiers (e.g. many, most, few) are permitted. The structure of the database is transparent to the language because a rule-based approach is taken to determine which relations a query implicitly references. The attributes in the database are treated as linguistic variables for which there exists an abstract description in the metadatabase. These heuristic measures encompass a large class of high-level query languages.		Bill P. Buckles;Frederick E. Petry;Yuet-Ying Cheung	1989	Inf. Syst.	10.1016/0306-4379(89)90019-7	natural language processing;sargable;query optimization;query expansion;web query classification;boolean conjunctive query;data control language;computer science;artificial intelligence;query by example;accessibility;machine learning;probability;data mining;database;rdf query language;programming language;attribute grammar;user interface;view;query language;object query language	DB	-21.80333393228764	11.110233306087938	130928
61848d0c3f856aa3559e1682faca1e17ce8051a5	keyword search on rdf graphs - a query graph assembly approach		Keyword search provides ordinary users an easy-to-use interface for querying RDF data. Given the input keywords, in this paper, we study how to assemble a query graph that is to represent useru0027s query intention accurately and efficiently. Based on the input keywords, we first obtain the elementary query graph building blocks, such as entity/class vertices and predicate edges. Then, we formally define the query graph assembly (QGA) problem. Unfortunately, we prove theoretically that QGA is a NP-complete problem. In order to solve that, we design some heuristic lower bounds and propose a bipartite graph matching-based best-first search algorithm. The algorithmu0027s time complexity is O(k 2l ... l 3l ), where l is the number of the keywords and k is a tunable parameter, i.e., the maximum number of candidate entity/class vertices and predicate edges allowed to match each keyword. Although QGA is intractable, both l and k are small in practice. Furthermore, the algorithmu0027s time complexity does not depend on the RDF graph size, which guarantees the good scalability of our system in large RDF graphs. Experiments on DBpedia and Freebase confirm the superiority of our system on both effectiveness and efficiency.	best-first search;dbpedia;entity–relationship model;freebase;graph embedding;heuristic;np-completeness;query language;resource description framework;scalability;search algorithm;time complexity;two-phase locking;vertex (geometry);vertex (graph theory);word-sense disambiguation	Shuo Han;Lei Zou;Jeffrey Xu Yu;Dongyan Zhao	2017		10.1145/3132847.3132957	time complexity;data mining;web search query;rdf schema;theoretical computer science;sparql;computer science;artificial intelligence;rdf query language;graph product;clique-width;bipartite graph;pattern recognition	DB	-32.68695882401456	4.5669685815932795	131153
94293c95736eb8fcd1832a94595c53c76f9ab05a	skyline-join in distributed databases	databases;distributed database;iterative algorithms;query processing;multi criteria decision making;sorting;web databases;database table skyline join query operator distributed database multicriteria decision making web database;distributed computing;multicriteria decision making;vanadium;controller area networks;distributed databases iterative algorithms sorting decision making query processing sun computer science distributed computing costs educational institutions;artificial neural networks;distributed environment;database table;sun;distributed databases;web database;computer science;peer to peer computing;indium;query processing distributed databases;iodine;arsenic;beryllium;skyline join query operator	The database research community has recently recognized the usefulness of skyline query. As an extension of existing database operator, the skyline query is valuable for multi-criteria decision making. However, current research tends to assume that the skyline operator is applied to one table which is not true for many applications on Web databases. In Web databases, tables are distributed in different sites, and a skyline query may involve attributes of multiple tables. In this paper, we address the problem of processing skyline queries on multiple tables in a distributed environment. We call the new operator skyline-join, as it is a hybrid of skyline and join operations. We propose two efficient approaches to process skyline-join queries which can significantly reduce the communication cost and processing time. Experiments are conducted and results show that our approaches are efficient for distributed skyline-join queries.	algorithm;distributed database;experiment;iterative method;pareto efficiency;scalability;skyline operator;world wide web	Dalie Sun;Sai Wu;Jianzhong Li;Anthony K. H. Tung	2008	2008 IEEE 24th International Conference on Data Engineering Workshop	10.1109/ICDEW.2008.4498313	arsenic;vanadium;beryllium;computer science;sorting;data mining;database;iodine;indium;distributed database;information retrieval	DB	-27.141649035036988	4.8061909980052215	131235
7e83bbb85a2e30dc7cab8d201b39b63074fe8f2d	towards an odmg-compliant visual object query language	object query language;front end;query language;user evaluation;aggregation function;object database;object data management group;visual representation;similarity function;visual query language	We describe the design, implementation and user evaluation of QUIVER, a graph-based visual query language for object databases. The design goals of QUIVER include compliance to standards, comprehensive representational power, and consistency of visual representation. Compliance to standards is achieved through QUIVER queries being translated to OQL, the standard query language proposed by the Object Data Management Group (ODMG). Comprehensive representational power is gained by QUIVER supporting a significant number of object database constructs, including objects, literals, attributes, relationships, structures, collections, operations, (aggregate) functions, and subqueries. Consistency of visual representation is pursued by assigning similar visual representations to constructs with similar functionality, as well as by minimising the use of text in QUIVER queries. The language is implemented as a visual front-end to the O2 object database system. Results of a user evaluation suggest that users find it easier to formulate correct queries in QUIVER than in OQL.	aggregate data;database;hierarchical and recursive queries in sql;object data management group;object query language;quiver;recursion;regular expression;sql	Manoj Chavda;Peter T. Wood	1997			computer science;theoretical computer science;front and back ends;database;rdf query language;programming language;query language;object query language;object definition language	DB	-31.79788380620564	8.143544128224294	131305
af150947708a54826dafa11173c81d2fcfe2caf2	a performance comparison of the rete and treat algorithms for testing database rule conditions	performance evaluation;update patterns performance simulation comparison rete algorithm treat algorithm data patterns database rule conditions production rule condition testing algorithms database rule system join conditions rule definition;performance comparison;system testing production systems design optimization context modeling relational databases statistical analysis process design computer science military computing application software;performance evaluation database theory deductive databases digital simulation knowledge based systems;database theory;knowledge based systems;digital simulation;production rule;deductive databases	The authors present the results of a simulation comparing the performance of the two most widely used production rule condition testing algorithms, Rete and TREAT, in the context of a database rule system. The results show that TREAT almost always outperforms Rete. TREAT requires less storage than Rete, and is less sensitive to optimization decisions than Rete. Based on these results, it is concluded that TREAT is the preferred algorithm for testing join conditions of database rules. Since Rete does outperform TREAT in some cases, this study suggests a next step which would be to develop a hybrid version of Rete and TREAT with an optimizer that would decide which strategy to use based on the rule definition and statistics about the data and update patterns. >	rete algorithm	Yu-Wang Wang;Eric N. Hanson	1992		10.1109/ICDE.1992.213202	database theory;computer science;theoretical computer science;knowledge-based systems;rete algorithm;data mining;database	DB	-28.163661859293104	7.580889512356957	131357
06742a8498ef96c16eff7b91c4d624f58febc43d	the data complexity of the syllogistic fragments of english	semantic representation;common sense reasoning;complex i;computational complexity;query answering;knowledge base	Pratt and Third’s syllogistic fragments of English can be used to capture, in addition to syllogistic reasoning, many other kinds of common sense reasoning, and, in particular (i) knowledge base consistency and (ii) knowledge base query answering, modulo their FO semantic representations. We show how difficult, in terms of semantic (computational) complexity and data complexity (i.e., computational complexity w.r.t. the number of instances declared in a knowledge base), such reasoning problems are. In doing so, we pinpoint also those fragments for which the reasoning problems are tractable (in PTime) or intractable (NP-hard or coNP-hard).	approximation algorithm;boolean satisfiability problem;c object processor;co-np;cobham's thesis;commonsense reasoning;computation;computational complexity theory;decision problem;encode;knowledge base;knuth–morris–pratt algorithm;modo (software);model checking;modulo operation;np-hardness;question answering;software quality assurance;viz: the computer game	Camilo Thorne;Diego Calvanese	2009		10.1007/978-3-642-14287-1_12	natural language processing;artificial intelligence;mathematics;algorithm	AI	-19.822467288298466	9.733846399357091	131431
2041b3e72cac59cc24c2b7b425618cd05a428c1e	inconsistency-tolerant ontology-based data access revisited: taking mappings into account		Inconsistency-tolerant query answering in the presence of ontologies has received considerable attention in recent years. However, existing work assumes that the data is expressed using the vocabulary of the ontology and is therefore not directly applicable to ontology-based data access (OBDA), where relational data is connected to the ontology via mappings. This motivates us to revisit existing results in the wider context of OBDA with mappings. After formalizing the problem, we perform a detailed analysis of the data complexity of inconsistency-tolerant OBDA for ontologies formulated in DL-Lite and other data-tractable description logics, considering three different semantics (AR, IAR, and brave), two notions of repairs (subset and symmetric difference), and two classes of global-as-view (GAV) mappings. We show that adding plain GAV mappings does not affect data complexity, but there is a jump in complexity if mappings with negated atoms are considered.	adobe flash lite;cobham's thesis;data access;description logic;iar systems;ontology (information science);vocabulary	Meghyn Bienvenu	2018		10.24963/ijcai.2018/238	machine learning;artificial intelligence;ontology;data access;computer science	AI	-24.39395614633443	8.766687509400859	131490
483135a524d616d3e8e74e0d6aa8a31d421ec76b	designing a dynamic integrity constraint checker with nonmonotonic logic	logic design databases specification languages inference mechanisms economic indicators kernel monitoring fluctuations;data integrity;integrable model;nonmonotonic logic;integrable system;temporal logic;temporal logic data integrity deductive databases formal logic inference mechanisms;inference mechanisms;partial information;specification language;body of knowledge;knowledge base deductive databases dynamic integrity constraint checker nonmonotonic logic nonmonotonic interval logic unless operator temporal intervals ada isl interval specification language icarus integrity system inference mechanism refutation faulty beliefs partial information;integrity constraints;formal logic;deductive databases;knowledge base	This paper reports on the design of a dynam integrity constraint checker with a nonmontonic Interval Logic IL. Central to this logic is the inclusion of the unless operator which captures the nonmonotonicity of dynamic integrity constraints over temporal intervals. IL is embedded in the AdallSL Interval Specification Language which has been utilized in the design of the lcarus Integrity System. In this system, the answer to a query depends on the availability of an explicit body of knowledge (knowledge base) about the integrity of data items contained in the query. The principal inference mechanism in this system is the refutation of faulty beliefs about data based on partial information and intuition, with respect to facts in the knowledge base. A technique for the verification of the lcarus kernel is presented. A family of assertions about the completeness, correctness, and confidence integrity characteristics about data is included in IM.	correctness (computer science);data integrity;embedded system;instant messaging;interval temporal logic;knowledge base;non-monotonic logic;specification language	Sheela Ramanna;James F. Peters;Elizabeth A. Unger;K. W. Glander	1990		10.1109/CMPSAC.1990.139342	knowledge base;computer science;artificial intelligence;theoretical computer science;data integrity;database;programming language;algorithm	DB	-23.118021661058442	15.129238513335622	131525
0a8015709360f566bb3be3f5f6531234c6f50408	the functional data model and its uses for interaction with databases		During the last decade, much research has been done in the field of data abstraction techniques in programming languages (see the chapter by Shaw), and higher-level data models for databases [CODD70] [CHEN76] [SS77b]. It is thus not surprising that researchers have begun to attempt to integrate databases and programming languages (i.e., by providing definitional and manipulative primitives for databases in the programming language that is consistent in philosophy with its existing data and control structures). The work in PASCAL/R (see the chapter by Mall, Reimer, and Schmidt) and PS-ALGOL [ACC82] are examples of such attempts made in the context of compiled, imperative (ALGOL-like) languages. This chapter describes our ideas on integrating databases into an interactive, applicative (Lisp-like) programming system.	data model;database	Peter Buneman;Rishiyur S. Nikhil	1982			relational algebra;data model;database;abstract data type;query language;data modeling;abstraction;computer science	DB	-30.352451210431504	11.038056607851232	131543
c5f582ded87aa7f1740ad366e878bf6d85cd852a	the meaning of of and have in the usl system	relational data;natural language;information system;natural language processing	This paper shows how the transformational relationship between HAVE-sentences and OF=phrases is used to represent data contained in sentences with HAVE as the main verb in the context of an information system using natural language to access a relational data base. An overview of the system first establishes the framework in which natural language processing is done. Then ways of representing HAVE are discussed with emphasis on the relation between HAVE and OF. The interpretation proposed and the interpretation process are illustrated by a list of representative queries and phrases against a small data base. In conclusion, this interpretation is extended to prepositional attributes with WITH and W I T H O U T , and problems are discussed.	database;information system;natural language processing;unix system laboratories	Magdalena Zoeppritz	1981	American Journal of Computational Linguistics		natural language processing;language identification;natural language programming;relational database;computer science;linguistics;natural language;information system	NLP	-30.050492710458535	9.948319809133768	131753
82e679fe2cdd60e7bfcdd11f4d8b52c4007d653b	application of supersql query language for the migration from a relational to object-oriented database	query language;object oriented database	As the object-oriented programming proliferates and the new application areas like CAD emerge, shifting of the database technology to the object-orientation accelerates toward either object-oriented or object-relational databases. Consequently, the legacy data currently stored in a relational database need to be migrated by adding more structures onto them.SuperSQL is a database publishing/presentation extension of SQL to yield a query result presented as a document in any of several target media, for example, HTML, Java, LaTeX and Excel work sheets. When a Web (HTML) document is the target medium, SuperSQL produces intra-page and inter-page (hyper-link) structures in arbitrary size and complexity.In this paper, we further extend the SuperSQL so that it could also generate O2C language source code as the result of a query in order to migrate data to O2. A commercial OODB of Ardent Software. As in case of database publishing, a user specify various forms of target object structures in a declarative fashion, in order to obtain the best O-O schema for the intended application.The superimposing technique is developed to realize the important aspects of the migration like object sharing and recursive reference topology including bidirectional references.		Shiro Udoguchi;Tadashi Iijima;Motomichi Toyama	2000		10.1109/IDEAS.2000.10004	data definition language;query optimization;computer science;query by example;database model;data mining;database;programming language;view;world wide web;database schema;alias;database design;query language	DB	-33.21497305739618	10.154352039174784	131866
809b8a24075dc206065486122611c5a4a9611ba4	knowledge-based query processing in object bases	query processing;combinatorial problems;parallel programming;query optimization;data model;monitoring;parallel programming environment;performance debugging;parallel processing;knowledge base;expert system	"""It is generally accepted that object-based systems provide """"a simple and elegant paradigm for general-purpose programming that meshed well with data models. In this paper we consider the problem of query optimization in logicoriented object bases. A logic-oriented object base is a database constructed based on object data model and augmented by mathematical logic. Problems with query processing in such systems are identified and an expert system approach is proposed. Advantages of such approach are illustrated by examples of combinatorial problems for general graph objects."""	data model;database;expert system;general-purpose modeling;mathematical optimization;object-based language;programming paradigm;query optimization	Phillip C.-Y. Sheu	1986		10.1145/324634.325393	parallel processing;sargable;knowledge base;query optimization;query expansion;data model;computer science;artificial intelligence;theoretical computer science;database;rdf query language;programming language;web search query;expert system;query language	DB	-30.60113998889364	9.865732416686548	131934
318cd82e14558e3ca983b33ec32597f2d556b091	a general procedure to test containment of conjunctive queries	conjunctive queries;generic algorithm	 Equivalence and containment of queries is a crucial issueof database query optimization theory. Among thedierent perspectives used to study this problem, weconsider in this work two orthogonal ones: the presenceof inequalities in the queries, and the underlyingsemantics, that can be set or bag theoretic. This workpresents a general algorithmic skeleton, QCC (Querycontainment Checker), that can be used to check thecontainment of queries with or without inequalities, underset or bag... 	conjunctive query	Miguel R. Penabad;Nieves R. Brisaboa;José R. Paramá;Hendrik Decker	2001			database;genetic algorithm;data mining;conjunctive query;optimality theory;computer science;boolean conjunctive query	DB	-25.22238594990269	11.420139195111638	132024
13b55dfb568050d86191b755c4d9d6e5bd09ac78	a multi-level organization for problem solving using many, diverse, cooperating sources of knowledge	question answering systems;data processing;systems analysis;speech recognition;artificial intelligence;computational linguistics;models;problem solving	An organization is presented for implementing solutions to Knowledge-based AI problems. The hypothesize-and-test paradigm is used as the basis for cooperation among many diverse and independent Knowledge sources (KS's). The KS's are assumed individually to be errorfui and incomplete. A uniform and integrated muiti-level structure, the blacKboard. holds the current state of the system. Knowledge sources cooperate by creating, accessing, and modifying elements in the blacKboard, The activation of a KS is data-driven, based on the occurrence of patterns in the blackboard which match templates specified the Knowledge oource. Each level in the blacKboard specifies a different representation of the problem space; the sequence of levels forms a loose hierarchy ir which the element» at each level can approximately be described as abitractions of elements at the ne>.t lower level. This decompostion can be thought of as an a priori frameworK of a plan for solving the problem; ea-h level is a generic stage in the plan. The elements at each level in the blackboard are hypotheses about some aspect of that level. The internal structure of an hypothesis consists of a fixed set of attributes; this set is the same for hypotheses at all levels of representation in the blackboard. These attributes are selected to serve as mechanisms for implementing the data-directed hypothesize-and-test paradigm and for efficient goal-directed scheduling of KS's. Knowledge sources may create networKs of structural relationships among hypotheses. These relationships, which are explicit in the blacKboard, serve to represent Tiferences and deductions made by the KS's about the hypotheses; they also allow competing and overlapping partial solutions to be handled in an integrated manner. The Hearsayll speech-understanding system is an implementation of this organization; it is used here as an example for descriptive purposes. * This research was supported in part by the Defense Advanced Research Proiects Agency under contract no. F44520-73-C-0074md monitored by the Air Force Ofice'of Scientific Research.	goal programming;knowledge-based systems;level structure;problem domain;problem solving;programming paradigm;scheduling (computing);windows legacy audio components	Lee D. Erman;Victor R. Lesser	1975			blackboard system;systems analysis;data processing;computer science;artificial intelligence;computational linguistics;machine learning;mathematics;algorithm	AI	-28.003560171515105	17.08281082043666	132055
e27ad4bad2868465877aa26b336008acbf987832	mining for attribute definitions in a distributed two-layered db system	operational semantics;distributed knowledge systems;knowledge interchange;knowledge systems;two layered databases;knowledge exchange;information system;query answering;theoretical foundation;knowledge discovery	Empirical equations are an important class of regularities that can be discovered in databases. We concentrate on the role of equations as definitions of attribute values. Such definitions can be used in many ways in a single database and for transfer of knowledge between databases. We present a quest for equations that can be used as definitions of an attribute in a given database. That quest triggers a discovery mechanism that specializes in searching recursively a system of databases and returns a set of partial definitions. We introduce the notion of shared operational semantics. It is founded on an equation-based system of partial definitions and it gives necessary foundations for designing local query answering systems in a distributed two-layered information system (D2LIS). The knowledge exchange between two sites of D2LIS may only improve an equation-based system of partial definitions at each of these sites. At the same time the shared operational semantics will better interpret user queries. Operational semantics augments the earlier developed semantics for rules used as attribute definitions. To put the shared operational semantics on a firm theoretical foundation we give a formal interpretation of queries which justifies empirical equations in their definitional role.	database;definition;information system;interpretation (logic);operational semantics;recursion	Zbigniew W. Ras;Jan M. Zytkow	2000	Journal of Intelligent Information Systems	10.1023/A:1008779617939	computer science;knowledge management;artificial intelligence;knowledge-based systems;data mining;database;knowledge extraction;operational semantics;information system	DB	-27.88996216220727	9.83919634768334	132283
2d9a6a1e224bf0b9e248de1303731667b4ac10c2	temporal and evolving data warehouse design		The data model of the classical data warehouse (formally, dimensional model) does not offer comprehensive support for temporal data management. The underlying reason is that it requires consideration of several temporal aspects, which involve various time stamps. Also, transactional systems, which serves as a data source for data warehouse, have the tendency to change themselves due to changing business requirements. The classical dimensional model is deficient in handling changes to transaction sources. This has led to the development of various schemes, including evolution of data and evolution of data model and versioning of dimensional model. These models have their own strengths and limitations, but none fully satisfies the above-stated broad range of aspects, making it difficult to compare the proposed schemes with one another. This paper analyses the schemes that satisfy such challenging aspects faced by a data warehouse and proposes taxonomy for characterizing the existing models to temporal data management in data warehouse. The paper also discusses some open challenges.		Sidra Faisal;Mansoor Sarwar;Khurram Shahzad;Shahzad Sarwar;Waqar Jaffry;Muhammad Murtaza Yousaf	2017	Scientific Programming	10.1155/2017/7392349	temporal database;business requirements;logical data model;computer science;data mining;transactional leadership;data model;dimensional modeling;data warehouse;database transaction	DB	-33.348523652953624	11.238614136269083	132349
16fb1211cd5a7a7b5f21baf494d15494c07e167e	updating recursive xml views of relations	relational data;efficient algorithm;xml views;side effect;view update;xml;xml publishing;heuristic algorithm	This paper investigates the view update problem for XML views published from relational data. We consider XML views defined in terms of mappings directed by possibly recursive DTDs compressed into DAGs and stored in relations. We provide new techniques to efficiently support XML view updates specified in terms of XPath expressions with recursion and complex filters. The interaction between XPath recursion and DAG compression of XML views makes the analysis of the XML view update problem rather intriguing. Furthermore, many issues are still open even for relational view updates, and need to be explored. In response to these, on the XML side, we revise the notion of side effects and update semantics based on the semantics of XML views, and present effecient algorithms to translate XML updates to relational view updates. On the relational side, we propose a mild condition on SPJ views, and show that under this condition the analysis of deletions on relational views becomes PTIME while the insertion analysis is NP-complete. We develop an efficient algorithm to process relational view deletions, and a heuristic algorithm to handle view insertions. Finally, we present an experimental study to verify the effectiveness of our techniques.	algorithm;binary prefix;centralized computing;computer networks (journal);computer data storage;computer science;conjunctive query;data integrity;data mining;data quality;database;directed acyclic graph;distributed computing;dummy variable (statistics);encode;emoticon;exception handling;existential quantification;experiment;heuristic (computer science);informatics;kuso;mathematical optimization;missing data;np-completeness;needham–schroeder protocol;p (complexity);preprocessor;query optimization;recursion;rename (relational algebra);rewrite (programming);rewriting;sql;side effect (computer science);social inequality;substitution (logic);trusted computer system evaluation criteria;universal instantiation;vldb;web service;xml;xpath;xfig	Byron Choi;Gao Cong;Wenfei Fan;Stratis Viglas	2007	2007 IEEE 23rd International Conference on Data Engineering	10.1007/s11390-008-9150-y	well-formed document;heuristic;xml validation;xml encryption;simple api for xml;xml;relax ng;xml schema;streaming xml;relational database;computer science;document structure description;xml framework;data mining;xml database;xml schema;database;xml signature;programming language;xml schema editor;side effect;information retrieval;efficient xml interchange	DB	-27.011674468222786	8.570138957865401	132352
de15bb44034d2e88f73668aed665f230a3ae3762	versioned relations: support for conditional schema changes and schema versioning	relational data model;schema versioning	We introduce the versioned relational data model, which allows a user to apply conditional schema changes to a populated database without breaking applications compiled against an existing schema, and without loss of existing data. Our model is based on keeping a history of conditional schema changes, and converting tuples on demand to fit the correct schema in any schema version.#R##N##R##N#We provide a concrete definition of schema versioning: The ability to specify an operator on any schema version, such that the tuples in the result are unaffected by schema versions created after the specified schema version. Finally, we show that our model supports schema versioning.	software versioning	Peter Sune Jørgensen;Michael H. Böhlen	2007		10.1007/978-3-540-71703-4_104	schema migration;information schema;relational model;semi-structured model;relax ng;logical schema;computer science;conceptual schema;document definition markup language;document structure description;star schema;data mining;xml schema;database;document schema definition languages;programming language;superkey;database schema;xml schema editor	DB	-31.70871616576548	11.115661845144503	132623
69823efae9e02f43cf332650d740d7400dbbc5e9	concept adjustment for description logics	biomedical ontologies;description logics;natural language processing;constraints	There exist a handful of natural language processing and machine learning approaches for extracting Description Logic concept definitions from natural language texts. Typically, for a single target concept several textual sentences are used, from which candidate concept descriptions are obtained. These candidate descriptions may have confidence values associated with them. In a final step, the candidates need to be combined into a single concept, in the easiest case by selecting a relevant subset and taking its conjunction. However, concept descriptions generated in this manner can contain false information, which is harmful when added to a formal knowledge base. In this paper, we claim that this can be improved by considering formal constraints that the target concept needs to satisfy. We first formalize a reasoning problem for the selection of relevant candidates and examine its computational complexity. Then, we show how it can be reduced to SAT, yielding a practical algorithm for its solution. Furthermore, we describe two ways to construct formal constraints, one is automatic and the other interactive. Applying this approach to the SNOMED CT ontology construction scenario, we show that the proposed framework brings a visible benefit for SNOMED CT development.	algorithm;computational complexity theory;description logic;information;knowledge base;machine learning;natural language processing;systematized nomenclature of medicine	Yue Ma;Felix Distel	2013		10.1145/2479832.2479851	natural language processing;open biomedical ontologies;description logic;computer science;artificial intelligence;data mining;algorithm	AI	-23.206028244902914	7.7514494691784765	132631
2058558b111fba68e85a63feed3761510f84a25f	applying p-calculus to dynamically-changing plan for agent model	dynamic change;agent model;agent modeling;plan;dynamically changing environment;planning;π calculus	In this paper, we apply 7r-calculus x ' to the planning for agents. First, we have proposed a new language for describing agent plans based on 7r-calculus, called PDL (Plan Description Language), and the model to execute them. The plans described in PDL can be changed dynamically while they are executing, because 7r-calculus provides dynamically-changing structures. By using this property, agents can change their plans to adapt to the environment around them by themselves in executing their plans. This property called reflection is very important to agents. We state the properties as theorems and prove them. Second, we have implemented an interpreter for PDL. In order to implement the system, we propose a primitive language, called PiL (Pi-calculus Language). PiL can be used on computers more easily than using the mathematical notations of 7r-calculus. We have shown that this system executes programs correctly and a plan written in PDL can be executed on the system. Finally, we have proved that the PDL is useful in the dynamically-changing environment by experiments.	computer;experiment;perl data language (pdl);π-calculus	Kazunori Iwata;Nobuhiro Ito;Xiaoyong Du;Naohiro Ishii	2000	International Journal on Artificial Intelligence Tools	10.1142/S0218213000000240	planning;simulation;π-calculus;computer science;artificial intelligence;plan;algorithm	AI	-24.635409249927264	17.398429665613467	132709
ebe3647db6e5be11f879df733eca4367e93a61f5	declarative approach to information systems requirements	information system	Abstract   The specification of requirements is among the most critical activities of information system development, and yet it is very prone to errors. A number of actions are proposed here to improve the situation: (1) shifting the concern from the target system towards its environment; (2) using an expressive language close to the natural way future users of the system express their requirements; and (3) using a formal language to ensure unambiguity, and to provide the deductive capabilities that will help in validating the requirements. A requirements expression language called ERAE is presented, which is inspired by some of the knowledge representation techniques in artificial intelligence, and by the so-called ‘declarative’ conceptual modelling languages in databases. The author also illustrates how the accompanying methodology makes the best use of the language's characteristics.	declarative programming;information system;requirement	Jacques Hagelstein	1988	Knowl.-Based Syst.	10.1016/0950-7051(88)90031-7	natural language processing;requirements analysis;universal networking language;specification language;computer science;system requirements specification;data mining;information system	OS	-32.397397550106945	14.207891753187331	132909
b7b2effb7a066c032016f5ba0016d87377c2aa1c	interactive query expansion in meta search engines.				Wolfgang Köhler;Daniel T. J. Backhausen;Claus-Peter Klas;Matthias Hemmje	2013			natural language processing;sargable;query optimization;query expansion;web query classification;web search query;information retrieval;search engine	Vision	-33.494108465693564	6.323686885038412	133417
11fcec17cf98de7a14e0de2c1858a23c5cd19385	object-oriented reengineering of information systems in a heterogeneous distributed environment	information systems business process re engineering investments object oriented modeling globalization acceleration isolation technology history middleware size control;information systems;repository instances object oriented re engineering information systems heterogeneous distributed environment highly flexible business processes;object oriented programming;object oriented programming information systems systems re engineering;distributed objects;distributed environment;object oriented;information system;business process;systems re engineering	Distributed object-oriented environments are the architecture of choice for supporting modern, highly flexible business processes. Existing host-based information systems (IS) must be integrated into these environments in order to preserve their investments in a changing environment. In this paper we present a strategy that establishes a cooperative co-existence of legacy and new IS in a distributed object-oriented environment. The cooperation is based on the approximation of the semantics of the participating systems. The approximated semantics allows for a mapping between corresponding entities of the different models. This mapping forms the basis of an implementation translating and replicating the repository instances of the systems taking part in the cooperation.	code refactoring;information system	Ulrike Kölsch	1998		10.1109/WCRE.1998.723180	information engineering;computer science;systems engineering;knowledge management;distributed computing;distributed object;programming language;object-oriented programming;information system	HPC	-33.61212436398996	12.531214716903357	133550
357d9269f8272dae81899ccfacced837749f738e	the manipulation of schematic correspondences with the quantification of uncertainty in dataspaces	schematic correspondence;schema mapping;dataspace	Dataspaces aim to remove upfront cost in the generation of the schema mappings that reconcile schematic heterogeneities, and to incrementally improve the generated mappings based on user feedback. The reconciliation of schematic heterogeneities is a crucial step for translating queries between a mediating schema and data sources. The generation of schema mappings depends on the elicitation of conceptually equivalent schema constructs and information on schematic heterogeneities. Furthermore, many dataspace operations manipulate associations between schemas, for example for generating a global schema to mediate user queries. With a view to minimizing upfront costs associated with understanding the relationships between schemas, many schema matching algorithms and tools have been developed for postulating equivalent schema constructs. However, they derive simple associations between schema constructs, and do not provide rich information on schematic heterogeneities. Without manual refinement, the elicitation of conceptually equivalent schema constructs and schematic heterogeneities may create uncertainties that must be managed.The schematic correspondences captures a wide range of one-to-one and many-to-many schematic heterogeneities. This thesis investigates the use of schematic correspondences as a central component in a dataspace management system. To support query evaluation in a dataspace in which relationships between schemas are represented using schematic correspondences, we propose a mechanism for automatically generating schema mappings from the schematic correspondences. We then characterise model management operators, which can underpin the bootstraping and maintenance of dataspaces, over schematic correspondences. To support the management of uncertainty in dataspaces, we propose techniques for quantifying uncertainty in the equivalence of schema constructs from evidence in the form of similarity scores and user feedback, and provide a flexible framework for incrementally updating the uncertainties in the light of new evidence.	dataspaces;schematic	Lu Mao	2013			logical schema;computer science;theoretical computer science;star schema;data mining;database	Vision	-28.567980296664924	10.612157203321901	133577
2f131379fe42bbf51f1e04cbb4a2b942a82313db	a redundancy free 4nf for xml	base relacional dato;base donnee;eliminacion;redundancia;xml language;database;forma normal;base dato;semantics;swinburne;dependance multivaluee;constraint integrity;relational database;semantica;semantique;dependencia multivaluada;multivalued dependency;redundancy;dependance fonctionnelle;integrity constraints;base donnee relationnelle;normal form;integrite contrainte;forme normale;dependencia funcional;elimination;integridad constrenimiento;langage xml;lenguaje xml;redondance;functional dependence	While providing syntactic exibility, XML provides little semantic content and so the study of integrity constraints in XML plays an important role in helping to improve the semantic expressiveness of XML. Functional dependencies (FDs) and multivalued dependencies (MVDs) play a fundamental role in relational databases where they provide semantics for the data and at the same time are the foundation for database design. In some previous work, we deened the notion of multivalued dependencies in XML (called XMVDs) and deened a normal form for a restricted class of XMVDs, called hierarchical XMVDs. In this paper we generalise this previous work and deene a normal form for arbitrary XMVDs. We then justify our deenition by proving that it guarantees the elimination of redundancy in XML documents.	a-normal form;data integrity;database design;fourth normal form;functional dependency;multivalued dependency;relational database;syntactic predicate;xml	Millist W. Vincent;Jixue Liu;Chengfei Liu	2003		10.1007/978-3-540-39429-7_17	xml validation;xml encryption;xml;relax ng;dependency theory;relational database;computer science;document structure description;xml framework;data integrity;data mining;xml schema;database;semantics;xml signature;redundancy;xml schema editor;algorithm;multivalued dependency;elimination	DB	-27.191375310729992	10.813538756205975	133612
4bd68a02159442c976f5bac15b5bea66b7c5e238	parallel storing and querying xml documents using relational dbms	tratamiento paralelo;multidisciplinaire;traitement parallele;machine unique;xml language;interrogation base donnee;data management;interrogacion base datos;stockage donnee;data model;data storage;single machine;maquina unica;indexing;indexation;indizacion;xml document;almacenamiento datos;multidisciplinary;modele donnee;multidisciplinar;systeme gestion base donnee;sistema gestion base datos;database management system;database query;data placement;langage xml;lenguaje xml;parallel processing;data models	The widespread adoption of XML is creating a new set of data management requirements, such as the need to store and query XML documents. Traditional methods of storing and querying XML documents are in single processor environment. This paper proposes a parallel processing technology based on multi-processor. The key issues such as XML data model, storage model, data placement strategy, paralleling query, indexing etc. are also discussed.	data model;multiprocessing;parallel computing;relational database management system;requirement;storage model;xml	Jie Qin;Shuqiang Yang;Wenhua Dou	2003		10.1007/978-3-540-39425-9_73	xml catalog;xml validation;binary xml;xml encryption;parallel processing;xml base;xml namespace;simple api for xml;xml;xml schema;data management;streaming xml;computer science;document structure description;xml framework;data mining;xml database;xml schema;database;xml signature;world wide web;xml schema editor;cxml;efficient xml interchange	DB	-27.885723129892895	5.815029397705311	133787
1cfbde1d9a1608ca9cf35302239f28706bdb4116	redd: an algorithm for redundancy detection in rdf models	estensibilidad;modelizacion;metadata;redundancia;web semantique;semantics;resource description framework;semantica;semantique;modelisation;design technique;redundancy;space use;web semantica;metadonnee;specification rdf;semantic web;extensibilite;metadatos;scalability;modeling;redondance;rdf;object model;knowledge base	The base of Semantic Web specifications is Resource Description Framework (RDF) as a standard for expressing metadata. RDF has a simple object model, allowing for easy design of knowledge bases. This implies that the size of knowledge bases can dramatically increase; therefore, it is necessary to take into account both scalability and space consumption when storing such bases. Some theoretical results related to blank node semantics can be exploited in order to design techniques that optimize, among others, space requirements in storing RDF descriptions. We present an algorithm, called REDD, that exploits these theoretical results and optimizes the space used by a RDF description.	algorithm;blank node;cobham's thesis;graph (discrete mathematics);knowledge base;polynomial;prototype;pseudocode;requirement;resource description framework;scalability;semantic web;vocabulary	Floriana Esposito;Luigi Iannone;Ignazio Palmisano;Domenico Redavid;Giovanni Semeraro	2005		10.1007/11431053_10	rdf/xml;cwm;knowledge base;computer science;artificial intelligence;rdf;data mining;database;semantics;rdf query language;world wide web;information retrieval;rdf schema	AI	-32.037156713109226	6.924480521826537	133941
bca5beca8740904acf9e67fb5a1fc5d507abbd96	modelling and indexing fuzzy complex shapes	and fuzzy database.;complex shapes;fuzzy solid modelling;fuzzy cad system;indexing fuzzy;fuzzy shape	At the conceptual stage of design, designers have only vague ideas of initial shapes that they gradually refine. A tool that supports conceptual design should capture such imprecise features but current CAD systems that are based on precise geometry and topology information cannot satisfy this requirement. The fuzzy set approach is particularly suitable for handling imprecise information by providing a set of solutions with different, user-specified preference degrees. We therefore choose this approach to address the imprecise design problem in a solid modelling system. Design data need to be stored in a database and accessed in later design process because of the iterative nature of design. This paper presents the representation, construction and display approaches for fuzzy shapes. We also discuss techniques for indexing and retrieving of fuzzy shapes in an object relational database with a fuzzy processing module.	computation;computer-aided design;constructive solid geometry;database model;fuzzy set;information retrieval;iteration;matlab;object-relational database;prototype;relational database;resultant;solid modeling;superquadrics;vagueness	Jinglan Zhang;Binh Pham;Yi-Ping Phoebe Chen	2002			computer science;theoretical computer science;neuro-fuzzy;data mining;database	EDA	-28.341396868446687	8.170747025490748	133998
6e896a6dcb97f01d1c079c8bd00ced97fe155bf1	variants of validity and their impact on the overall test space	overall test space;inductive reasoning	Dealing with t e question whether or not a given system does sulllce some interesting property one is confronted with t e problem, how to navigate appropriately through the available knowledge space to prove or torefute that property under investigation. Moreover it s d sireable toget an algorithm w ich solves that task efficiently. Applied to the area of system validation, we will propose some solution allowing for the reduction of test cases, i.e. of the scale of the knowledge space to be investigated, wh n validating some target system by testing. The key idea for test case r duction is to exploit certain inheritance prop rties of th underlying space of input data. In its right perspective, inheritance is induction. Due to the impossibility to create any general induction scheme for deductive justification of inductive reasoning, there arises the necessity of domain-dependent variants.	algorithm;hindley–milner type system;i/o controller hub;inductive reasoning;knowledge space;mathematical induction;norm (social);simplicial complex;test case	Jörg Herrmann;Klaus P. Jantke;Rainer Knauf	1998			concurrent validity	ML	-20.526914519399803	5.5078028475197005	134002
71dc5620a4efff0714eceec7f7cbd70596b89a4b	a control-data-mapping entity-relationship model for internal controls construction in database design	information systems;control data mapping entity relationship cdmer model;internal controls construction;database design;internal control deficiency	The internal controls construction of a transaction system is important to management, operation and auditing. In the environment of manual operation, the internal controls of the transaction process are all done by manual mechanism. However, after the transaction processing environment has been changed from manual operation to computerized operation, the internal control techniques have been gradually transformed from manual mechanisms to computerized methods. The essence of internal controls in operational activities is the data expressions or constraints. The adoption of information systems often results in internal control deficiencies and operating risks due to the data unavailable in database for the data expressions of internal controls. Hence, how to design database schema to support internal controls mechanism is becoming a crucial issue for a computerized enterprise. Therefore, this paper referred Entity-Relationship model (ER model) in order to propose a Control-Data-Mapping Entity-relationship (CDMER) model by manipulating the required fields of tables to design database to support internal controls construction. Finally, a simple simulated case is prepared for illustration of the CDMER model. The contribution of this paper is to enhance the reliability of information systems through internal controls construction by applying the model to design databases. A Control-Data-Mapping Entity-Relationship Model for Internal Controls Construction in Database Design	constraint (mathematics);database design;database schema;entity–relationship model;information system;transaction processing	Jason Chen;Ming-Hsien Yang;Tian-Lih Koo	2014	IJKBO	10.4018/ijkbo.2014040102	engineering;operations management;data mining;database	DB	-33.57743852102571	11.75669299749399	134015
303f530b706e7c09a48e0b66ec4b8f741bbd2356	a generic querying algorithm for greedy sets of existential rules	existential rules;datalog;tgds	Answering queries in information systems that allow for expressive inferencing is currently a field of intense research. This problem is often referred to as ontology-based data access (OBDA). We focus on conjunctive query entailment under logical rules known as tuple-generating dependencies, existential rules or Datalog+/-. One of the most expressive decidable classes of existential rules known today is that of greedy bounded treewidth sets (gbts). We propose an algorithm for this class, which is worst-case optimal for data and combined complexities, with or without bound on the predicate arity. A beneficial feature of this algorithm is that it allows for separation between offline and online processing steps: the knowledge base can be compiled independently from queries, which are evaluated against the compiled form. Moreover, very simple adaptations of the algorithm lead to worst-case-optimal complexities for specific subclasses of gbts which have lower complexities, such as guarded rules.	2-exptime;best, worst and average case;blocking (computing);compiler;conjunctive query;data access;data structure;datalog;decision tree model;description logic;expressive power (computer science);greedy algorithm;information system;jack lutz;knowledge base;on the fly;online and offline;polynomial;treewidth;worst-case complexity	Michaël Thomazo;Jean-François Baget;Marie-Laure Mugnier;Sebastian Rudolph	2012			computer science;datalog;programming language;algorithm	AI	-24.415282983889995	8.549785785819399	134023
1a5faeba6badf00f9330f702c2911fc1ba10d817	schema and database evolution in the o2 object database system	object database;object oriented database	When the schema of an object-oriented database system is modified, the database needs to be changed in such a way that the schema and the database remain consistent with each other. This paper describes the algorithm implemented in the new forthcoming release of the 02 object database for automatically bringing the database to a consistent state after a schema update has been performed. The algorithm, which uses a deferred strategy to update the database, is a revised and extended version of the screening algorithm first sketched in [7].	algorithm;benchmark (computing);class hierarchy;data structure;database;program transformation	Fabrizio Ferrandina;Thorsten Meyer;Roberto V. Zicari;Guy Ferran;Joëlle Madec	1995			database theory;object-based spatial database;intelligent database;semi-structured model;computer science;data access object;database model;database;database catalog;view;database schema;database design;spatiotemporal database;component-oriented database	DB	-30.72057825923029	9.997535277026534	134134
343f489855496a045daa7ad4d50524a4ec650d22	visual semantics - or: what you see is what you compute	turing machines;computer languages;programming environments;abstract graph syntax;geometric figures;magnetic heads;visual semantics;geometric relationships;turing machine;functional programming;visual programming;visualization;visual languages;abstract syntax;visual programming visual languages;turing machines equations magnetic heads functional programming computer languages computer interfaces visualization programming environments;computer interfaces;turing machines visual semantics visual graphs concrete visual syntax abstract graph syntax geometric figures geometric relationships;concrete visual syntax;visual graphs	"""We introduce visual graphs as an intermediate representation between concrete visual syntax and abstract graph syntax. In a visual graph some nodes are shown as geometric figures, and some edges are represented by geometric relationships between these figures. By carefully designing visual graphs and corresponding mappings to abstract syntax graphs, semantics definitions can, at least partially, employ a visual notation while still based on abstract syntax. Visual semantics thus offers the """" best of both worlds """" by integrating abstract syntax and visual notation. These concepts can also be used to give visual semantics for traditional textual formalisms. As an example we provide a visual definition of Turing machines. Language semantics are conveniently expressed based on abstract syntax. This allows to abstract from details of concrete syntax and leads to more succinct semantics definitions , and in some cases it makes semantics definitions even tractable at all. In [5] we have presented a framework for defining semantics of visual languages. The approach is fundamentally based on an abstract graph syntax for visual languages. Thus, it is essentially a textual formalism which is unfortunate (to a certain degree) for two reasons: first, visual relationships, such as inside or adjacent, are represented in a topological way by appropriately labeled edges. By this transition from a visual to a textual representation , many characteristics of the visual language under consideration are difficult to grasp in the abstract representation, they might even get lost. Second, the tex-tual treatment of a visual language presents in a sense a """" modality mismatch """" , more precisely, it means a retrogres-sion from visual to textual. This might cause psychologically grounded reluctances to using the formalism – just because it is not visual. These drawbacks might be obstacles for a widespread use of the formalism in the VL community. Therefore, we extend the semantics formalism to stay, to a large degree, with visual notation when defining semantics. The main idea is to re-visualize some relationships of abstract syntax that have been translated more or less directly from geometric relationships. This means that some nodes are visualized as geometric figures, and some of the edges between these nodes are represented by relationships that hold between the figures. Since this will not cover, in general, all relationships, we arrive at a semi-visual notation, that is, a mixture of graphs and pictures, called visual graphs, in which complex relationships are still …"""	abstract syntax;cobham's thesis;degree (graph theory);geometric median;intermediate representation;modality (human–computer interaction);norm (social);parse tree;semantics (computer science);semiconductor industry;sion's minimax theorem;turing machine;visual language	Martin Erwig	1998		10.1109/VL.1998.706151	natural language processing;abstract syntax;computer science;theoretical computer science;abstract semantic graph;visual programming language;programming language;abstract syntax tree	AI	-22.970614166428547	13.854124293201481	134161
1bc2ec32123cfec04e0517583703f82b49b592a3	lims: a suite of database tools for laboratory organization	report generation;independent functionality;query processing laboratory techniques scientific information systems object oriented databases relational databases graphical user interfaces report generators;user friendly mechanism;mice;laboratory organization;query processing;data entry;querying;ease of use laboratory information management system database tools laboratory organization user friendly mechanism data collection information retrieval data validation report generation object oriented databases querying data entry automated tasks graphical user interface object oriented screens customized solution relational databases independent functionality;information retrieval;data collection;customized solution;graphical user interface;relational database;cloning;ease of use;laboratories relational databases information retrieval costs information management cloning mice object oriented databases software tools network servers;network servers;graphical user interfaces;complex data;object oriented;laboratory techniques;information management;graphic user interface;database tools;software tools;relational databases;object oriented databases;object oriented database;object oriented screens;data retrieval;data validation;laboratory information management system;scientific information systems;automated tasks;report generators	Our motivation is to create an efficient and user-friendly mechanism for data collection, retrieval, validation and report generation of laboratory information. Described in this paper are novel object-oriented databases which are being used for the collection, querying, retrieval and simplistic report generation of laboratory information. The program provides users with a friendly interface that breaks complex data entry, data retrieval and report generation into separate automated tasks utilizing graphical user interface (GUI) object-oriented screens. This customized solution of a laboratory information management system (LIMS) was designed utilizing five relational databases for both ease of use and independent functionality.	laboratory information management system	M. S. Hilliard;D. L. Larson;Martina J Rosenberg	2001		10.1109/CBMS.2001.941714	computer science;data mining;graphical user interface;database;information management;information retrieval	DB	-33.478447466723	16.350838668132845	134245
309a61c034dc490897dc3de9574386e5b82d5aa9	logic-based xpath optimization	efficiency;axiomatization;containment;xpath;xml;query;xml document;optimization;static analysis;high performance	XPath [5] was introduced by the W3C as a standard language for specifying node selection, matching conditions, and for computing values from an XML document. XPath is now used in many XML standards such as XSLT [4] and the forthcoming XQuery [10] database access language. Since efficient XML content querying is crucial for the performance of almost all XML processing architectures, a growing need for studying high performance XPath-based querying has emerged. Our approach aims at optimizing XPath performance through static analysis and syntactic transformation of XPath expressions.	mathematical optimization;static program analysis;xml;xpath;xquery;xslt	Pierre Genevès;Jean-Yves Vion-Dury	2004		10.1145/1030397.1030437	xml validation;xml encryption;simple api for xml;xml;processing instruction;xslt;xml schema;streaming xml;computer science;xpath 2.0;document structure description;identity transform;xml framework;path expression;xml database;xml schema;database;schematron;xml signature;programming language;xml schema editor;information retrieval;efficient xml interchange	DB	-32.73810299179982	7.153219289188089	134311
68cbddd8e06f2ce16c6c4bdf31ca211d66181cf5	how to exploit the device diversity and database interaction to propose a generic cost model?	physical design;generic cost model;ontologies;devices	Cost models have been following the life cycle of databases. In the first generation, they have been used by query optimizers, where the cost-based optimization paradigm has been developed and supported by most of important optimizers. The spectacular development of complex decision queries amplifies the interest of the physical design phase (PhD), where cost models are used to select the relevant optimization techniques such as indexes, materialized views, etc. Most of these cost models are usually developed for one storage device (usually disk) with a well identified storage model and ignore the interaction between the different components of databases: interaction between optimization techniques, interaction between queries, interaction between devices, etc. In this paper, we propose a generic cost model for the physical design that can be instantiated for each need. We contribute an ontology describing storage devices. Furthermore, we provide an instantiation of our meta model for two interdependent problems: query scheduling and buffer management. The evaluation results show the applicability of our model as well as its effectiveness.	analysis of algorithms;database;index (publishing);interdependence;materialized view;mathematical optimization;metamodeling;ontology (information science);physical design (electronics);programming paradigm;scheduling (computing);storage model;universal instantiation	Ladjel Bellatreche;Salmi Cheikh;Sebastian Breß;Amira Kerkad;Ahcène Boukorca;Jalil Boukhobza	2013		10.1145/2513591.2513660	physical design;simulation;computer science;ontology;data mining;database	DB	-27.003688119974203	6.274723645379453	134520
956e26ac593718edacf178b926a0196b0d25ecc2	efficient algorithms and performance results for multi-user knowledge bases	efficient algorithm;multi user;knowledge base	The paper describes research efforts to develop efficient implementation techniques for large, shared knowledge bases, focusing on efficient concurrent access of large knowledge bases by multiple users. We present an algorithm, called the Dynamic Directed Graph policy, originally proposed in [Chaudhri et a/., 1992], which al­ lows efficient interleaved execution of transac­ tions against a large knowledge base with the intent of optimizing transaction throughput. The implementation of the policy and exper­ imental evaluation results are also presented and discussed. The paper concludes with dis­ cussion on l essons learnt from this research. Large knowledge bases containing millions of facts will soon be here, thanks to research efforts such as the Knowledge Sharing initiative [Patil et a/., 1992] and the CYC project [Guha and Lenat, 1994]. However, tools for building knowledge bases do not scal e up to accom­ modate such large knowledge bases. Our efforts are fo­ cusing on the adoption of database techniques to build knowledge base building tools that do scale up. One of the requirements of such tools is that they ac­ commodate efficient multiuser access of a single, large knowledge base by maximizing throughput, i.e., the number of user-defined transactions that are executed against the knowledge base per time unit. A compara­ ble requirement for databases is addressed by concur­ rency control mechanisms that are routinely offered by database management systems, which have been shown to improve throughput by as much as an order of mag­ nitude or more. A comparable concurrency control algorithm, specif­ ically designed for knowledge bases and called the Dy­ namic Directed Graph policy was proposed in [Chaudhri et a/., 1992]. The main purpose of this paper is to de­ scribe an implementation of that policy and to present performance results which compare the performance of the proposed policy against the performance of off-the-shelve concurrency control mechanisms designed for databases. The paper summarizes some of our findings and concludes with research directions. The outline of this paper is as follows. In Section 1, we begin by motivating the problem. In Section 2, we present the Dynamic Directed Graph (DDG) policy. In Section 3, we briefly describe some problems that were faced while implementing the policy. In Sections 4-5, we present the evaluation of the algorithm for knowledge base applications, and in Section 6, discuss some related work. In Section 7, we discuss the lessons learnt from our research and conclude in …	computer multitasking;concurrency (computer science);concurrency control;control system;cyc;database;directed graph;duckduckgo;fault tolerance;hybrid algorithm;information system;intelligent agent;knowledge base;multi-user;programming paradigm;reason maintenance;refinement (computing);requirement;response time (technology);telecommunications network;throughput	Vinay K. Chaudhri;John Mylopoulos	1995			knowledge base;computer science;knowledge management;artificial intelligence;data mining;database	DB	-27.5005622515564	15.046687176657976	134523
539632ed85b3b6f7cc1978983bf0cd01a64387ed	reactive consistency control in deductive databases	eficacia sistema;interfase usuario;deductibility;base donnee;architecture systeme;nested scan;user interface;sistema informatico;coaccion;performance systeme;contrainte;database;base dato;computer system;system performance;consistencia;transaction;deductibilite;theorem proving;join;demonstration theoreme;hashing;constraint;deducibilidad;consistance;arquitectura sistema;interface utilisateur;systeme informatique;buffer;merging scan;systeme gestion base donnee;information system;demostracion teorema;system architecture;sistema gestion base datos;database management system;consistency;systeme information;sort;sistema informacion;deductive databases	Classical treatment of consistency violations is to back out a database operation or transaction. In applications with large numbers of fairly complex consistency constraints this clearly is an unsatisfactory solution. Instead, if a violation is detected the user should be given a diagnosis of the constraints that failed, a line of reasoning on the cause that could have led to the violation, and suggestions for a repair. The problem is particularly complicated in a deductive database system where failures may be due to an inferred condition rather than simply a stored fact, but the repair can only be applied to the underlying facts. The paper presents a system which provides automated support in such situations. It concentrates on the concepts and ideas underlying the approach and an appropriate system architecture and user guidance, and sketches some of the heuristics used to gain in performance.	database transaction;deductive database;heuristic (computer science);systems architecture	Guido Moerkotte;Peter C. Lockemann	1991	ACM Trans. Database Syst.	10.1145/115302.115298	embedded system;hash function;buffer;computer science;sort;database;computer performance;automated theorem proving;constraint;consistency;user interface;information system;algorithm;systems architecture	DB	-27.478761467580252	12.26381954704195	134939
16f05f02d9866ad3fe9b2e9f841b54d5f4e58dab	building temporal structures in a layered multimedia data model	radiological diagnosis;multimedia;multimedia data;object oriented database;reusable component;integrated hospital information systems	The Layered Multimedia Data Model (LMDM) aids in the specification of multimedia compositions by dividing the problem into smaller, more manageable pieces. In this paper we describe the lower two layers of the LMDM, the Data Definition Layer, which allows the specification of multimedia objects in a database, and the Data Manipulation Layer, which allows the specification of temporal structures built from those objects. Several examples demonstrate the advantages of the layered paradigm: simple specifications, and modular, reusable components.	data definition language;data model;programming paradigm;wrapper function	Gerhard A. Schloss;Michael J. Wynblatt	1994		10.1145/192593.192674	real-time computing;computer science;operating system;database;multimedia;world wide web	DB	-33.21737432074995	13.29530568276206	134985
5242442c3b41c8b9a9ef3716a49916f4db1ece7a	an entity-relationship algebra and its semantic description capabilities	query language;semantics;relational database;modele entite relation;semantique;algebre;algebra;semantic description;base donnee relationnelle;langage interrogation;entity relationship	The entity-relationship model (ERM) has been defined without a manipulative part. Since a link between the ERM and the relational model may be established by choosing the relation to be the structural unit of the data-representational level of the ERM, this model could benefit by somehow inheriting the relational algebra. This paper proposes as manipulative part for the entity-relationship model a reshaped relational algebra (RRA) . It is shown that just as the entity-relationship model concepts are based on the way people perceive information, the RRA operators bear analogies to the way people communicate, i.e., natural language, and are therefore convenient in describing the semantics of query languages within ERM.	aggregate function;computer science;dos;entity–relationship model;mental representation;natural language;query language;relational algebra;relational calculus;relational model;software engineering;system analysis;turing completeness	Victor M. Markowitz;Yoav Raz	1984	Journal of Systems and Software	10.1016/0164-1212(84)90005-0	natural language processing;codd's theorem;relational calculus;entity–relationship model;relational database;computer science;database;semantics;query language	DB	-29.77219655041883	9.598362044400488	135036
dcd1dbcd0d14bbe7044b99f68218f527e4b44ac0	mining association rules from semi-structured data	structured data;semi structured data;knowledge discovery;association rule	Despitethe growing popularity of semi-structur ed data suchasWebdocuments, mostknowledgediscoveryresear ch hasfocusedon databasescontainingwell structured data. In this paper, we try to find usefulinformationfrom semistructured data. In our approach, we begin by representing semi-structur ed data in a prototype-basedapproach. We thendetectthe mosttypical commonstructure of semistructureddataandre-store thedatainto thisstructure. We can considerthis commonstructure as a structured layer. Thestructured layer filters out uselesspropertiesof semistructured data. Next, we apply the algorithm of mining associationrules to the structured layer by usingthe idea of concepthierarchy. Concepthierarchy maintainsrelationshipsbetweenconcepts.Theuseof concepthierarchy allows us to generate extra rules in addition to originally generatedrules. Theextra rules contain relatedconcepts with the conceptsin the original rules. Theseextra rules are oftenmore informativeandusefulfor findingpatterns. In this way, somekind of knowledge canbeextractedfrom semi-structur eddata.	algorithm;data model;function prototype;javaserver pages;semi-structured data;semiconductor industry	Kohei Maruyama;Kuniaki Uehara	2000			data mining;data science;knowledge extraction;semi-structured data;data model;association rule learning;computer science	DB	-33.33473752178107	6.68979883748936	135050
e2775770c7e4672ff57ed89ca5509ddb899b10e8	comparing and synthesizing integrity checking methods for deductive databases	data integrity;database management systems;deductive databases transaction databases spatial databases constraint theory relational databases tin convergence database systems logic proposals;convergence method integrity checking methods deductive databases generation phase implementation scheme metaprogram;database management systems deductive databases data integrity;integrity checking;deductive databases	We compare and synthesize different methods for integrity checking in deductive databases. First, we state simplified integrity checking for deductive databases independently of the particular strategy used by different methods found in the literature. In accordance with this statement, we classify integrity checking methods into two main groups: methods with a generation phase without fact access and methods with a generation phase with fact access. Then, we propose an implementation scheme (a metaprogram) where the differences and similarities among the methods can be pointed out. In this common implementation framework, we compare the methods; this comparison is based on the number of facts accessed by each of them during integrity checking. Finally and from the analysis of the results, we define a convergence method which synthesizes some different features from several methods. >		Matilde Celma;Carlos García;Laura Mota-Herranz;Hendrik Decker	1994		10.1109/ICDE.1994.283033	database theory;computer science;theoretical computer science;data integrity;data mining;database	DB	-27.497923134224568	8.15104866292537	135055
04c2f39ad3f489a9bd1e7ebb1bf483a2245501d1	pr-owl 2.0 - bridging the gap to owl semantics	owl;pr owl;uncertainty reasoning;probabilistic ontology;random variable;compatibility;semantic web;multi entity bayesian networks;mebn;use case	The past few years have witnessed an increasingly mature body of research on the Semantic Web, with new standards being developed and more complex use cases being proposed and explored. As complexity increases in SW applications, so does the need for principled means to cope with uncertainty inherent to real world SW applications. Not surprisingly, several approaches addressing uncertainty representation and reasoning on the Semantic Web have emerged [3, 4, 6, 7, 10, 11, 13, 14]. For example, PR-OWL [3] provides OWL constructs for representing Multi-Entity Bayesian Network (MEBN) [8] theories. This paper reviews some shortcomings of PR-OWL 1 [2] and describes how they will be addressed in PR-OWL 2. A method is presented for mapping back and forth from triples into random variables (RV). The method applies to triples representing both predicates and functions. A complex example is given for mapping an n-ary relation using the proposed schematic.	algorithm;bayesian network;bridging (networking);dos;schematic;semantic web;shattered world;symmetric multiprocessing;web ontology language	Rommel N. Carvalho;Kathryn B. Laskey;Paulo Cesar G. da Costa	2010		10.1007/978-3-642-35975-0_1	computer science;theoretical computer science;data mining;owl-s;algorithm	AI	-21.702661874539942	7.614778722961759	135060
81d2526e46be7ef5d43ed66b7d8a7a186e67ea5c	fragmentation and query decomposition in the ecr model	irrigation switches	Fragmentation and query decomposition are considered important issues in a distributed database environment. For a semantic data model (ECR data model), the types of fragmentation and the criteria they should follow is provided along with how global queries in a high-level language, GORDAS, are decomposed into local queries on the fragments. This technique may be used in a heterogeneous distributed database, where the global schema presented to the user is in the high-level ECR model.	distributed database;fragmentation (computing);high- and low-level;high-level programming language;semantic data model	Ramez Elmasri;P. Srinivas;Gomer Thomas	1987	1987 IEEE Third International Conference on Data Engineering	10.1109/ICDE.1987.7272413	semi-structured model;computer science;data mining;database;world wide web	DB	-31.447698519704417	8.937580895148432	135101
a9b04c7e43fbeac3cb46c5493181f84ace76fe83	rewriting systems as a tool for relational data base design	relational data;design process;functional dependency;rewrite systems	Methodologies for relational data base design have been extensively studied in the literature, mainly with regard to functional dependencies. Rewriting systems may be used as a formal tool to express transformations between refinements of the model in the design process.	relational database management system;rewriting	Carlo Batini;Alessandro D'Atri	1978		10.1007/BFb0025717	domain relational calculus;sql;relational model;codd's theorem;statistical relational learning;relational calculus;relational database;computer science;database;conjunctive query;programming language;algorithm	EDA	-30.062418476102994	11.284215608679016	135416
0b10851a82006a116c2c9a83ea3be817ba585707	rule-based activity recognition in ambient intelligence	lower duration;ambient intelligence;account complex event;bigger duration;rule-based activity recognition;complex event;activity recognition;confidence value;activity definition rule;optional event;rule-based activity recognition system;possible complex event	Activity recognition is an important, multi-faceted problem with a broad application scope. In this talk we will present a rule-based activity recognition system with the option of using confidence values in our rules and facts. Each activity definition rule has some primary and some optional events and the absence of any of the optional events, only decreases the confidence value of the recognized complex event. We recognize all possible complex events (activities) based on predefined rules, which express temporal and spatial combinations of atomic and complex events. Then we detect all conflicted events (recognized events that overlap but use common resources). The optimal solution is found with an optimization function that takes into account complex eventu0027s confidence, temporal duration and number of used atomic events. Adjusting this function, results in higher or lower abstraction levels in our results (more generic events with bigger duration / more specific events with lower duration).rnrnAs application domain we use ambient assisted living (e.g. for elderly persons). The approach has been implemented, and tested in a real ambient intelligence environment hosted by the FORTH Institute of Computer Science.	activity recognition;ambient intelligence	Grigoris Antoniou	2011		10.1007/978-3-642-22546-8_1	artificial intelligence;data mining	AI	-25.020800584649184	15.173425293448034	135662
dae7c146b3dd040c74261defcaeed499512a39f6	horizontal partitioning of multimedia databases using hierarchical agglomerative clustering		"""Horizontal partitioning is a database design technique widely used in relational databases in order to achieve query optimization. Re- cently, this technique has been applied in multimedia databases to im- prove query execution cost in these databases. Nevertheless, current al- gorithms are based on affinity between predicates to obtain an horizontal partitioning scheme (HPS). Affinity measures how a pair of predicates is accessed by the queries (""""togetherness""""). The main disadvantage of this measure is that it only involves two predicates, and hence does not show the """"togetherness"""" of more than two predicates. In this paper we pro- pose an horizontal partitioning method for multimedia databases which is based on a hierarchical agglomerative clustering algorithm. The main advantage of our method is that it does not use affinity to create the HPS. We present experimental results to clarify the soundness of the proposed method."""	binary space partitioning;hierarchical clustering;partition (database)	Lisbeth Rodríguez-Mazahua;Giner Alor-Hernández;María Antonieta Abud-Figueroa;Silvestre Gustavo Peláez-Camarena	2014		10.1007/978-3-319-13650-9_27	computer science;theoretical computer science;machine learning;data mining;database	DB	-27.7078695772676	6.878504625817829	135846
8b077e984e0e5a49f6dd2ab0244898306ebc5357	schemata transformation of object-oriented conceptual models to xml	object oriented;conceptual model		xml	Ling Feng;Elizabeth Chang;Tharam S. Dillon	2003	Comput. Syst. Sci. Eng.		conceptual model;database;xml;natural language processing;conceptual model (computer science);object-oriented programming;schema (psychology);conceptual schema;computer science;artificial intelligence	DB	-31.47464025411401	10.714140808335443	135936
5f1953f20467dfe025dbd1799e92e31dfad21ca5	preliminary report on (active) view materialization in gui programming			graphical user interface;materialized view	Nita Goyal;Charles Hoch;Ravi Krishnamurthy;Brian Meckler;Michael Suckow	1996			programming language;database;computer science	HCI	-31.504131385601337	9.31660188670919	136011
2f72e690f2ed290942f12be5466d8db1cd2513f9	slugtransit: a location-based public transportation management system	transportation networks;log files;management system;public transport;real time;base station;location based;cost effectiveness;user satisfaction;transportation management system	In this paper, we describe SlugTransit, an on-line location-based system which allows public transportation systems to be managed in a cost-effective manner while improving overall system's usability and user satisfaction. SlugTransit vehicles report their location in real-time to SlugTransit base stations; vehicle location information is then uploaded and stored in a database. Software updates and log file retrievals are done through SlugTransit gateway's. Through a visual, Web-based interface, system operators as well as users have access to current location information in real-time. This capability enables operators to better manage the transportation network, improving efficiency and, at the same time, user satisfaction.	online and offline;patch (computing);real-time clock;real-time locating system;sysop;usability	James Koshimoto;Matt Bromage;Vladislav Petkov;Katia Obraczka	2009		10.1145/1710035.1710069	simulation;cost-effectiveness analysis;computer science;base station;operating system;management system;public transport;computer security;advanced traffic management system	HCI	-31.175289734380364	17.224716324470535	136080
3b2c49da30046153b667daa8bdc23afcb62e06f1	do rule-based approaches still make sense in logical data warehouse design?		As any product design, data warehouse applications follow a well-known life-cycle. Historically, it included only the physical phase, and had been gradually extended to include the conceptual and the logical phases. The management of phases either internally or intranally is dominated by rule-based approaches. More recently, a cost-based approach has been proposed to substitute rule-based approaches in the physical design phase in order to optimize queries. Unlike the traditional rule-based approach, it explores a huge search space of solutions (e.g., query execution plans), and then based on a cost-model, it selects the most suitable one(s). On the other hand, the logical design phase is still managed by rule-based approaches applied on the conceptual schema. In this paper, we propose to propagate the cost-based vision on the logical phase. As a consequence, the selection of a logical design of a given data warehouse schema becomes an optimization problem with a huge space search generated thanks to correlations (e.g. hierarchies) between data warehouse concepts. By the means of a cost model estimating the overall query processing cost, the best logical schema is selected. Finally, a case study using the Star Schema Benchmark is presented to show the effectiveness of our proposal.		Selma Bouarar;Ladjel Bellatreche;Stéphane Jean;Mickaël Baron	2014		10.1007/978-3-319-10933-6_7	dimensional modeling;data mining;database	Logic	-29.29824268637293	6.039261979928045	136159
8d3ce437ea590087bb4002ce482f2a527f41b4a3	the knowledge acquisition and representation language karl	prototipificacion rapida;program interpreters logic programming languages knowledge acquisition knowledge representation formal logic inference mechanisms computational linguistics formal specification;debugging;puesta a punto programa;regle inference;representacion conocimientos;sistema experto;knowledge based system;formal specification;adquisicion del conocimiento;program interpreters;operational semantics;semantics;base connaissance;inference mechanisms;ingenieria logiciel;logical programming;acquisition connaissance;semantica;semantique;software engineering;specification language;debogage;specification formelle;domain knowledge;inference rule;dynamic logic;especificacion formal;deductive database;prototyping;rapid prototyping;logic programming;programmation logique;base dato deductiva;specification languages;knowledge acquisition;karl model knowledge acquisition and representation language knowledge based system expertise model executable level domain knowledge inference knowledge procedural control knowledge modeling primitives frame logic dynamic logic declarative semantics operational definition fixpoint approach karl interpreter karl specifications;formal logic;genie logiciel;base conocimiento;logique dynamique;knowledge acquisition knowledge based systems specification languages software engineering logic programming natural languages computer society testing knowledge engineering knowledge representation;base donnee deductive;lenguaje especificacion;computational linguistics;systeme expert;logic programming languages;knowledge representation;representation connaissances;programacion logica;langage specification;knowledge based systems;prototypage rapide;regla inferencia;deductive databases;knowledge base;expert system;knowledge engineering	The Knowledge Acquisition and Representation Language (KARL) combines a description of a knowledge based system at the conceptual level (a so called model of expertise) with a description at a formal and executable level. Thus, KARL allows the precise and unique specification of the functionality of a knowledge based system independent of any implementation details. A KARL model of expertise contains the description of domain knowledge, inference knowledge, and procedural control knowledge. For capturing these different types of knowledge, KARL provides corresponding modeling primitives based on Frame Logic and Dynamic Logic. A declarative semantics for a complete KARL model of expertise is given by a combination of these two types of logic. In addition, an operational definition of this semantics, which relies on a fixpoint approach, is given. This operational semantics defines the basis for the implementation of the KARL interpreter, which includes appropriate algorithms for efficiently executing KARL specifications. This enables the evaluation of KARL specifications by means of testing.	knowledge acquisition;knowledge representation and reasoning	Dieter Fensel;Jürgen Angele;Rudi Studer	1998	IEEE Trans. Knowl. Data Eng.	10.1109/69.706055	natural language processing;computer science;artificial intelligence;knowledge-based systems;database;programming language;logic programming;algorithm	DB	-27.546113764923742	17.371171169808797	136235
01538cba7e35da5c58d9e8dac235c1a2d5d453fe	ajar: aggregations and joins over annotated relations	generalized hypertree decomposition;aggregation;semiring;fractional hypertreewidth;join query	We study a class of aggregate-join queries with multiple aggregation operators evaluated over annotated relations. We show that straightforward extensions of standard multiway join algorithms and generalized hypertree decompositions (GHDs) provide best-known runtime guarantees. In contrast, prior work uses bespoke algorithms and data structures and does not match these guarantees. We extend the standard techniques by providing a complete characterization of (1) the set of orderings equivalent to a given ordering and (2) the set of GHDs valid with respect to the given ordering, i.e., GHDs that correctly answer a given aggregate-join query when provided to (simple variants of) standard join algorithms. We show by example that previous approaches are incomplete. The key technical consequence of our characterizations is a decomposition of a valid GHD into a set of (smaller) unconstrained GHDs, i.e., into a set of GHDs of sub-queries without aggregations. Since this decomposition is comprised of unconstrained GHDs, we are able to connect to the wide literature on GHDs for join query processing, thereby obtaining improved runtime bounds, MapReduce variants, and an efficient method to find approximately optimal GHDs.	aggregate data;aggregate function;algorithm;bespoke;data structure;database;join (sql);mapreduce	Manas R. Joglekar;Rohan Puttagunta;Christopher Ré	2016		10.1145/2902251.2902293	theoretical computer science;semiring;database;algorithm	DB	-24.073635975628612	11.899153264964125	136376
f141ebe3202b2833ccf7e26d976b1cc9968b295e	using sql with object-oriented databases	object oriented database models;use;modelizacion;base relacional dato;base donnee;syntax;langage requete;sql;database;base dato;semantics;relational database;syntaxe;semantica;semantique;utilizacion;query languages;modelisation;utilisation;langage sql;object oriented;base donnee orientee objet;base donnee relationnelle;oriente objet;object oriented database;extension;sintaxis;modeling;orientado objeto;requete	We investigate how the standard database query language SQL can be extended to deal with the newly emerging trends of complex objects and object orientation. Our main concern is to extend SQL as naturally as possible, rather than to redesign SQL into “yet another” object-oriented query language. We achieve this goal through a faithful mapping from a complete object-oriented database model, compatible with recent proposals in the field, to the nested relational database model, which is widely accepted as a natural extension of the relational database model on which standard SQL is based. We provide formal definitions of syntax and semantics. We also review related research and situate our work into it.		Jan Van den Bussche;Andreas Heuer	1993	Inf. Syst.	10.1016/0306-4379(93)90003-J	data definition language;sql;.ql;nested set model;systems modeling;syntax;stored procedure;data manipulation language;relational database;computer science;query by example;in-memory processing;database model;data mining;autocommit;database;semantics;sql/psm;temporal database;language integrated query;programming language;object-oriented programming;view;null;object-relational impedance mismatch;query language	DB	-30.242737447080465	11.290872374709643	136538
cccd7eca47256d5237d1baa6190398c885e72020	the cost model for xml documents in relational database systems	hypermedia markup languages;information resources;hierarchical structure;query processing;software performance evaluation;costs xml relational databases web sites web server information analysis;storage models cost model xml documents relational database data representation world wide web data server query processing hierarchical document structure xml dtd performance evaluation;xml document;relational database system;world wide web;relational databases;cost model;visual databases hypermedia markup languages relational databases information resources query processing software performance evaluation;visual databases	XML is emerging as the standard for representing data in the World Wide Web. If a data server can efficiently store and query the XML documents, it will allow users to manage volumes of data effectively. We describe new storage strategies and a cost model for XML documents. We suggest two storage techniques for storing XML documents in a relational database system. Our storage techniques preserve the attribute of XML documents such as the hierarchical structure of documents by considering both XML DTD and XML instances. The cost model is extended to estimate the queries on the XML documents in a relational database system. We have evaluated the performance of queries for four different storage techniques including suggested storage models and existing storage models by our cost model.	relational database management system;xml	JiSim Kim;Wol-Young Lee;Kiho Lee	2001		10.1109/AICCSA.2001.933973	xml catalog;xml validation;binary xml;xml encryption;xml base;simple api for xml;relational database management system;xml;xml schema;streaming xml;relational database;computer science;document structure description;database model;xml framework;data mining;xml database;xml schema;database;xml signature;xml schema editor;cxml;information retrieval;efficient xml interchange;sgml	DB	-31.16828145459117	5.712662886739724	136746
0442845100020f8d2e3970a3180ce918908d57ae	the expressive power of complex values in object-based data models	total order;valeur complexe;modele base objet;systeme base donnee;data model;query languages;expressive power;grafo;puissance expressive;doubles;first order;data structures;graph;graphe;database systems;structure donnee;langage interrogation;dobles;duplicate	In object based data models complex values such as tuples or sets have no special status and must therefore be represented by objects As a consequence di erent objects may represent the same value i e duplicates may occur This paper contains a study of the precise expressive power required for the representation of complex values in typical object based data models supporting rst order queries object creation and while loops Such models are su ciently powerful to ex press any reasonable collection of complex values provided duplicates are allowed It is shown that in general the presence of such dupli cates is unavoidable in the case of set values In contrast duplicates of tuple values can easily be eliminated A fundamental operation for duplicate elimination of set values called abstraction is considered and shown to be a tractable alternative to explicit powerset construc tion Other means of avoiding duplicates such as total order equality axioms or copy elimination are also discussed Extended and revised version of a paper presented at the Tenth ACM Symposium on Principles of Database Systems VP Research Assistant of the Belgian National Fund for Scienti c Research	cobham's thesis;data model;object lifetime;object-based language;symposium on principles of database systems	Jan Van den Bussche;Jan Paredaens	1995	Inf. Comput.	10.1006/inco.1995.1110	discrete mathematics;data structure;data model;computer science;first-order logic;mathematics;graph;programming language;expressive power;total order;algorithm;query language	DB	-25.23493488581751	11.470406098608214	137353
aced767da974011e5f6dd1b823f40be23f0bbf3f	the implementation of dynamic engineering document based on xml technology.	dynamic engineering document;implementation;xml		xml	Jianjun Zhai;Wenliang Chen;Xiaoping Yan	2004			well-formed document;xml validation;binary xml;simple api for xml;xml;xml schema;streaming xml;document structure description;xml framework;database;document schema definition languages;world wide web;xml schema editor;information retrieval;efficient xml interchange	DB	-33.51903495644331	7.569462202956154	137580
8095163e0469c1ffa23f493f22f9dab5dafda323	on the tractability and intractability of consistent conjunctive query answering	conjunctive queries;answer sets;generic algorithm;inconsistent databases;first order;database repairs;polynomial time;query answering;consistent query answering	The consistent query answering framework has received considerable attention since it was first introduced as an alternative to coping with inconsistent databases. The framework was defined based on two notions: repairs and consistent query answers. Informally, a repair is a consistent database that minimally differs from the inconsistent database. The consistent answers to a query are those tuples that appear in the intersection of the answer sets of the query when evaluated over all possible repairs. Here we study the complexity of the problem of consistent query answering for the class of acyclic conjunctive queries without self-joins, under primary key constraints. The problem is known to be coNP-complete in general for this class. Our goal is to determine the boundary between tractability and intractability, by establishing a dichotomy to the effect that, every conjunctive query in this class is either in PTIME or coNP-complete. In the PTIME direction, previous work has identified the queries for which consistent answers can be computed via first-order rewriting. In fact, for the class of acyclic conjunctive queries without self-joins, under primary key constraints, the boundary between first-order rewritable and not first-order rewritable queries has already been determined. Hence, our focus is on queries for which there is no first-order rewriting. We present a technique for computing in polynomial time the consistent query answers to several not first-order rewritable queries. We hope this technique may lay the foundations for a more general algorithm that handles all PTIME not first-order rewritable queries. In the hardness direction, we identify several representative queries of the class, for which we show that the problem is coNP-hard.  This work is done under the supervision of Prof. Phokion G. Kolaitis and Prof. Wang-Chiew Tan.	algorithm;co-np;computational complexity theory;conjunctive query;database;directed acyclic graph;first-order predicate;os-tan;p (complexity);rewriting;time complexity;unique key	Enela Pema;Phokion G. Kolaitis;Wang Chiew Tan	2011		10.1145/1966874.1966881	query optimization;web query classification;boolean conjunctive query;theoretical computer science;data mining;database;mathematics;conjunctive query;range query;query language;spatial query	DB	-25.300159557099033	10.765001098896786	137779
8e51652ee5f0906bd7443694a2adbf4ea29bd8cd	the systems integration problem	integrable system;reference model;system integration;federated databases	Abstract The systems integration problem, defined as coupling GIS and other systems, is analysed by the use of two models based on database and federated database reference models. A three-schema model of a system enables the classification of incompatibilities between systems in terms of differences in external, conceptual and internal schemas. A systems reference model identifies transformation, constructor and accessor operations as the basic mechanisms to overcome these differences. The possible architectures of integrated systems are defined in terms of the presence of these operations and their general strengths and weaknesses are assessed.		David J. Abel;P. J. Kilby;J. Richard Davis	1994	International Journal of Geographical Information Systems	10.1080/02693799408901984	integrable system;reference model;computer science;theoretical computer science;data mining;database;system integration	DB	-32.58908265008121	11.774488033991423	137797
c1db0bac92137fdb2509c5734c603bf6a425ea60	a holistic and principled approach for the empty-answer problem	database usability;database;empty answer problem;query modification	We propose a principled optimization-based interactive query relaxation framework for queries that return no answers. Given an initial query that returns an empty-answer set, our framework dynamically computes and suggests alternative queries with fewer conditions than those the user has initially requested, in order to help the user arrive at a query with a non-empty-answer, or at a query for which no matter how many additional conditions are ignored, the answer will still be empty. Our proposed approach for suggesting query relaxations is driven by a novel probabilistic framework based on optimizing a wide variety of application-dependent objective functions. We describe optimal and approximate solutions of different optimization problems using the framework. Moreover, we discuss two important extensions to the base framework: the specification of a minimum size on the number of results returned by a relaxed query and the possibility of proposing multiple conditions at the same time. We analyze the proposed solutions, experimentally verify their efficiency and effectiveness, and illustrate their advantages over the existing approaches.	approximation algorithm;cardinality (data modeling);experiment;holism;interactivity;linear programming relaxation;mathematical optimization;query optimization;stable model semantics;usability testing;user (computing)	Davide Mottin;Alice Marascu;Senjuti Basu Roy;Gautam Das;Themis Palpanas;Yannis Velegrakis	2016	The VLDB Journal	10.1007/s00778-016-0431-8	online aggregation;sargable;query optimization;query expansion;web query classification;boolean conjunctive query;computer science;query by example;theoretical computer science;data mining;database;view;spatial query	DB	-25.168851511844462	4.530407046467135	138152
9c0c0ed637ae25883e60eb50195e31c86e7094a0	generating ontologies from relational data with fuzzy-syllogistic reasoning		Existing standards for crisp description logics facilitate information exchange between systems that reason with crisp ontologies. Applications with probabilistic or possibilistic extensions of ontologies and reasoners promise to capture more information, because they can deal with more uncertainties or vagueness of information. However, since there are no standards for either extension, information exchange between such applications is not generic. Fuzzy-syllogistic reasoning with the fuzzy-syllogistic system S provides 2048 possible fuzzy inference schema for every possible triple concept relationship of an ontology. Since the inference schema are the result of all possible set-theoretic relationships between three sets with three out of 8 possible fuzzy-quantifiers, the whole set of 2048 possible fuzzy inferences can be used as one generic fuzzy reasoner for quantified ontologies. In that sense, a fuzzy syllogistic reasoner can be employed as a generic reasoner that combines possibilistic inferencing with probabilistic ontologies, thus facilitating knowledge exchange between ontology applications of different domains as well as information fusion over them.	common logic;description logic;fuzzy logic;information exchange;ontology (information science);ontology learning;semantic web;semantic reasoner;set theory;sugeno integral;vagueness	Bora I. Kumova	2015		10.1007/978-3-319-18422-7_2	semantic reasoner;artificial intelligence;data mining;mathematics;algorithm	AI	-21.812564881517215	6.537913325808536	138255
5096b08925bd78cc0f56ecb6341751bf519ac73f	on the integration of data and mathematical modeling languages	embedding;embedded language;data management;integration;data model;decision support system;mathematical models;decision support systems;mathematical model;model management;difference set;technical report;language;management;models	This paper examines Ways in which the addition of data modeling features can enhance the capabilities of mathematical modeling languages, and demonstrates how such integration might be achieved as an application of the embedded languages technique proposed by Bhargava and Kimbr-ougMDecision'making, and decision support systems, require the representation and manipulation of both data and mathematical models. Several data modeling languages as well as several mathematical modeling languages exist, but they have differences sets of capabilities. We motivate with a detailed example the need for the integration of these languages. We describe the benefits that might result, and claim that this could lead to a significant improvement in the functionality of model management systems. Then we present our approach for the integration of these languages, and specify how the claimed benefits are realized. *Tifs author's work on this paper was performed in conjunction with research funded by the Naval Postgraduate School.	data modeling;decision support system;embedded system;mathematical model;modeling language	Hemant K. Bhargava;Ramayya Krishnan;Sumitra Mukherjee	1992	Annals OR	10.1007/BF02283651	decision support system;algebraic modeling language;computer science;theoretical computer science;mathematical model;data mining;mathematics;statistics	DB	-32.98368318372645	13.623842975996135	138422
dabef86a61ab317366911980e5a23aaa0345a129	biorl: an xml-based active rule language for biological database constraint management	biology computing;genomics;programming language semantics;sequences;xquery;management system;data integrity;database management systems;time inconsistency;conference management;biology;relational databases biology bioinformatics sequences genomics database systems biomedical engineering biomedical informatics conference management engineering management;query languages;biorule system biorl xml based active rule language biological database constraint management biological data integrity database management system dbms database constraints biological semantics xquery expressions xquery functions;biological semantics;active rules;biomedical engineering;xquery functions;engineering management;biorl;biological data integrity;database systems;xml;xquery active rules constraint management biological database;constraint handling;biological database constraint management;relational databases;dbms;biological data;high throughput;database constraints;constraint management;biological database;biomedical informatics;database management system;xml biology computing constraint handling data integrity database management systems knowledge based systems programming language semantics query languages;knowledge based systems;xml based active rule language;biorule system;bioinformatics;xquery expressions	High throughput biological experiments produce a large amount of biological data that can be updated by time. Inconsistent data stored in the same or different databases may lead to serious biological research problems. Constraint management is important to ensure biological data integrity. Some existing databases were built for the easy use for biologists; however the database management system (DBMS) may not have a sufficient constraint management system. In this paper, we propose an XML-based active rule language, named BioRL, to enforce constraints on existing databases without changing the original system structures or the underlying databases. In BioRL language, biological semantics are represented declaratively in active rules by using XQuery expressions and functions. This paper presents the syntax of the BioRL language, with examples that illustrate the use of BioRL in an application environment. This paper also presents the BioRule system to support the specification and execution of BioRL, focusing on the architectural components of the BioRL parser and the BioRL repository.	biological database;data integrity;event condition action;experiment;throughput;xml;xquery	Huaqin Xu;Ying Jin	2008	2008 International Conference on BioMedical Engineering and Informatics	10.1109/BMEI.2008.133	theory of constraints;high-throughput screening;genomics;xml;biological database;biological data;relational database;computer science;dynamic inconsistency;data integrity;data mining;sequence;xml database;management system;database;information retrieval;query language	DB	-32.76826098150069	10.667011659170855	138452
1433fe1e36895eb098d5ebeaa5f53f29951ce46b	toward heterogeneous cardinal direction calculus	composition;cardinal direction relations;spatial relation;converse;heterogeneous spatial calculi	Cardinal direction relations are binary spatial relations determined under an extrinsically-defined direction system (e.g., north of). We already have point-based and region-based cardinal direction calculi, but for the relations between other combinations of objects we have only a model. We are, therefore, developing heterogeneous cardinal direction calculus, which allows reasoning on cardinal direction relations without regard to object types. In this initial report, we reformulate the definition of cardinal direction relations, identify the sets of relations between various pairs of objects, and develop the methods for deriving upper approximation of converse and composition.	approximation;bitmap;object type (object-oriented programming);spatial–temporal reasoning	Yohei Kurata;Hui Shi	2009		10.1007/978-3-642-04617-9_57	calculus;mathematics;geometry	AI	-27.73036382392302	8.404829171305973	138635
b321508a6358f4d23112eb62da464eef4709d1b2	mobile client-server system for real-time continuous query of moving objects	real time continuous query;mobile client server system;mcqps mobile continuous query processing system	In this paper, a Mobile Continuous Query Processing System (MCQPS) is designed to solve problems related to database hoarding, maintenance of shared data consistency, and optimization of logging. These problems are caused by weak connectivity and disconnection of wireless networks inherent in mobile database systems under mobile client-server environments. We show the superiority of the proposed MCQPS by comparing its performance to the Client-Intercept-Server (C-I-S) model. In addition, several experimental results show the effectiveness of our proposed indexing structure and methodology for real-time continuous queries.	client–server model;database;mathematical optimization;real-time clock;real-time transcription;server (computing)	Young-Choon Kim;Hae-Jong Joo;Young-Baek Kim;Sang-Yong Rhee	2011	Int. J. Fuzzy Logic and Intelligent Systems	10.5391/IJFIS.2011.11.2.095	query optimization;mobile search;mobile database;computer science;database;distributed computing;web search query;world wide web	DB	-31.002434485288877	16.910090415636482	138688
5942b888898d5d3fcc7a82e3f419a52e72bd0af2	modelling evolvable component systems: part i: a logical framework	logical frameworks;revision based logic;logical framework;evolvable systems;minimum models;reconfigurable component systems	We develop a logical modelling approach to describe evolvable computational systems. In this account, evolvable systems are built hierarchically from components where each component may have an associated supervisory process. The supervisor’s purpose is to monitor and possibly change its associated component. Evolutionary change may be determined purely internally from observations made by the supervisor or may be in response to external change. Supervisory processes may be present at any level in the component hierarchy allowing us to use evolutionary behaviour as an integral part of system design. We model such systems in a revision-based first-order logical framework in which supervisors are modelled as theories which are at a logical meta-level to the theories of their components. This enables evolutionary change of the component to be induced by revision-based changes of the supervisor at the meta-level. In this way, the intervention required in evolutionary change is modelled purely logically. The hierarchical component-based structure is fairly intricate so we present the basic ideas firstly in a simple setting, the well-known blocks world, before introducing tree-based structures to represent component hierarchies. We also introduce some techniques for establishing the behaviour of evolvable systems specified in this logical framework. The ideas and concepts are driven by example throughout. We conclude with a more substantial example, that of a simple model of an evolvable system of automated bank teller machines.	blocks world;component-based software engineering;first-order predicate;floor and ceiling functions;logical data model;logical framework;systems design;theory;tree (data structure);whole earth 'lectronic link	Howard Barringer;Dov M. Gabbay;David E. Rydeheard	2009	Logic Journal of the IGPL	10.1093/jigpal/jzp026	logical framework;computer science;artificial intelligence;theoretical computer science;mathematics;programming language;algorithm	Logic	-23.16587660725447	17.06698383246039	139103
ee41fd015c2d572c535f097062d8a265bfbf3ab9	a context sensitive tabular parsing algorithm	context sensitive tabular	"""Tlie parsing algorithm due to Younger is applica­ ble to any context free language but requires that the grammar be in Chomsky normal form. Resides es­ tablishing an 0 (n'*) upper bound to time complex­ ity needed to parse context free languages, Younger’s algorithm has the advantages that it docs not re­ quire the grammar to be unambiguous and that it degrades gracefully while parsing """"ungrammatical” strings. These are advantages desirable in processing nnlurnl language, if not coupled with the restriction that the grammar be in Chomsky normal form. In this article, a general tabular algorithm is de­ veloped which uses the tabular structure employed by Younger’s algorithm. Instead of nonterminal charac­ ters (or syntactic types) as table entries, a construct is used which is similar to the LR(0) items employed by Knulh’s LR(k) parsing algorithm, and by the Gra­ ham, Harrison, R u e e o algorithm. Graham, Harrison and Russo use these items as table entries, obviating the need to employ Chomsky normal form. The algo­ rithm presented in this article employs such items for not only the replacement phrase, but also the left and right context strings in context sensitive rules. The resulting algorithm is applicable to context sensitive grammars without erasure rules and is suitable as a platform for semantic case frame analysis of natural"""	algorithm;chomsky normal form;context-free language;context-sensitive grammar;docs (software);emoticon;fault tolerance;frame language;graham scan;parsing;table (information);terminal and nonterminal symbols	Frank Hadlock	1990		10.1145/98949.99012	parser combinator;bottom-up parsing;pattern recognition;database;programming language;top-down parsing	NLP	-24.935899745226756	16.396642359534933	139120
e899ff8626899bbb2afe8e38bc03da92511bca68	implementing semantic frames as typed feature structures with xmg	typed feature structures;grammar engineering;frame semantics	This work presents results on the integration of frame-based representations into the framework of eXtensible MetaGrammar (XMG). Originally XMG allowed for the description of tree-based syntactic structures and underspecified representations of predicate-logical formulae, but the representation of frames as a sort of typed feature structure, particularly type unification, was not supported. Therefore, we introduce an extension that is capable of handling frame representations directly by means of a novel  -dimension. The aim is not only to make possible a straightforward specification of frame descriptions, but also to offer various ways to specify constraints on types, be it as a contiguous type hierarchy or a loose set of feature structure constraints. The presented extensions to XMG are fully operational in a new prototype.		Timm Lichte;Simon Petitjean	2015	J. Language Modelling	10.15398/jlm.v3i1.96	computer science;theoretical computer science;programming language;algorithm	NLP	-21.34645882762865	16.333516431017195	139213
5dbed4fdbd93726764777fe2c0804f95647dd411	inconsistency tolerance in p2p data integration: an epistemic logic approach	distributed system;multimodal logic;systeme reparti;data integrity;query processing;complexite calcul;nonmonotonic logic;autonomous system;integration information;traitement requete;logica multimodal;circonscription;interrogation base donnee;interrogacion base datos;p2p;circumscription;sistema autonomo;rational agent;autonomic system;complejidad computacion;information integration;sistema repartido;complex i;computational complexity;systeme autonome;integracion informacion;logique epistemique;epistemic logic;tratamiento pregunta;query answering;integration donnee poste a poste;logique multimodale;peer to peer;database query;logica epistemica;circonscripcion	We study peer-to-peer data integration, where each peer models an autonomous system that exports data in terms of its own schema, and data interoperation is achieved by means of mappings among the peer schemas, rather than through a global schema. We propose a multi-modal epistemic semantics based on the idea that each peer is conceived as a rational agent that exchanges knowledge/belief with other peers, thus nicely modeling the modular structure of the system. We then address the issue of dealing with possible inconsistencies, and distinguish between two types of inconsistencies, called local and P2P, respectively. We define a nonmonotonic extension of our logic that is able to reason on the beliefs of peers under inconsistency tolerance. Tolerance to local inconsistency essentially means that the presence of inconsistency within one peer does not affect the consistency of the whole system. Tolerance to P2P inconsistency means being able to resolve inconsistencies arising from the interaction between peers. We study query answering and its data complexity in this setting, and we present an algorithm that is sound and complete with respect to the proposed semantics, and optimal with respect to worst-case complexity.	epistemic modal logic	Diego Calvanese;Giuseppe De Giacomo;Domenico Lembo;Maurizio Lenzerini;Riccardo Rosati	2005		10.1007/11601524_6	rational agent;epistemic modal logic;computer science;autonomous system;artificial intelligence;information integration;non-monotonic logic;peer-to-peer;data integrity;data mining;database;mathematics;computational complexity theory;multimodal logic;circumscription;algorithm	AI	-21.18659061085832	10.798582048863292	139461
140f868c385cac4ff354d8f70b1e09dac3fc0e05	a semantic feature language for sculptured object modelling	modelizacion;sculptured object;escultura;semantica formal;morfoscopia;auxiliary information;shape analysis;semantics;rectangular shape;formal semantics;semantica;semantique;objet sculpte;bound form;semantique formelle;modelisation;features;morphoscopie;forme liee;representation forme;sculpture;shaping;faconnage;feature modelling;modeling;forma ligada;lenguaje formal;hechura;formal language;forma rectangular;feature semantics;forme rectangulaire;langage formel	Feature technology is a promising approach for modelling an object since it provides a systematic representation scheme for the object semantics in various engineering application domains. A semantic oriented feature language is proposed in this paper to cover the modelling of the sculptured object class. The language consists of two components: vocabularies and grammar. Features and feature relationships are vocabularies of the language manipulated by the grammar. Although geometry is the main characteristic that differentiates the sculptured object from the regular shaped object, it is just a set of auxiliary information in the feature vocabulary class. The relationships between various features within a sculptured object reflecting its functionality are the major concerns. Parametric modelling of sculptured objects is also discussed in the feature language. q 2000 Elsevier Science Ltd. All rights reserved.	vocabulary	Chikit K. Au;Matthew Ming-Fai Yuen	2000	Computer-Aided Design	10.1016/S0010-4485(99)00085-8	natural language processing;formal language;method;shaping;systems modeling;sculpture;computer science;formal semantics;shape analysis;semantics;engineering drawing;object definition language	AI	-29.786428304717152	15.714972701095292	139531
606ad31f57d58ed67791f698f9cc94600c10c99f	private inference control for aggregate database queries	image storage;image databases;query processing;real time;image database;algebra remote sensing image databases query processing multidimensional systems image storage data mining conferences jacobian matrices data engineering;data engineering;data mining;algebra;remote sensing;jacobian matrices;data structure;multidimensional systems;conferences	We study private inference control for aggregate queries, such as those provided by statistical databases or modern database languages, to a database in a way that satisfies privacy requirements and inference control requirements. For each query, the client learns the value of the function for that query if and only if the query passes a specified in- ference control rule. The server learns nothing about the queries, and the client learns nothing other than the query output for passing queries. We present general protocols for aggregate queries with private inference control.	aggregate data;aggregate function;database;requirement;sql;server (computing)	Geetha Jagannathan;Rebecca N. Wright	2007	Seventh IEEE International Conference on Data Mining Workshops (ICDMW 2007)	10.1109/ICDMW.2007.66	data structure;multidimensional systems;computer science;theoretical computer science;data mining;database	DB	-28.377323084454613	6.281516612540597	139610
216b56e57f49db2576d092eb5cd3d20f6aecb0fc	attraction - a global affinity measure for database vertical partitioning		Vertical partitioning clusters attributes of a relation to generate fragments suitable for subsequent allocation over a distributed platform. Partitioning is also important for centralized and web databases because data items that are mostly accessed together should reside in the same fragment. The target is to minimize the execution time of user applications. Most previous algorithms for intuitive database partitioning design use the Attribute Affinity Matrix (AAM) directly. This paper proposes an attraction measure to enable global evaluation of attribute togetherness in a given relation. An attribute attraction matrix (AAM*) can also evolve from the AAM by selecting an appropriate attraction function. We further propose the connection-based partitioning approach to take advantage of the global comparability of the AAM*. Two such algorithms are presented in this paper. The Preliminary Connection-Based Partitioning Algorithm (PCBPA) finds the fragments with a complexity of O(m), where m is the number of edges found in the corresponding undirected connection graph of the AAM*. The Connection-Based Partitioning Algorithm (CBPA) is capable of eliminating trivial fragments; it has a complexity measure of O(n), where n is the number of attributes. Finally, we argue that the partitioning obtained based on the attraction measure is at least as good as the result from the algorithms that use the conventional affinity measure.	active appearance model;automatic acoustic management;binary space partitioning;blum axioms;centralized computing;computational complexity theory;database normalization;dijkstra's algorithm;graph (discrete mathematics);partition (database);processor affinity;run time (program lifecycle phase);software incompatibility	Jun Du;Ken Barker;Reda Alhajj	2003			database;attraction;computer science	DB	-27.506331493715898	6.841865759126105	139669
87f8e7ad1c4f1384930ee00eb1f3604097318fed	performance analysis of several algorithms for processing joins between textual attributes	document handling;query processing;information retrieval systems software performance evaluation document handling distributed databases query processing very large databases;performance analysis computer science algorithm design and analysis query processing statistics cost function analytical models relational databases information retrieval nasa;software performance evaluation;performance analysis;distributed databases;information retrieval systems;very large databases;input query performance analysis textual attributes join processing algorithms large document collections inverted files simulation system characteristics	Three algorithms for processing joins on attributes of a textual type are presented and analyzed in this paper. Since such joins often involve document collections of very large size, it is very important to find efficient algorithms to process them. The three algorithms differ according to whether the documents themselves or the inverted files on the documents are used to process the join. Our analysis and simulation results indicate that the relative performance of these algorithms depends on the input document collections, the system characteristics and the input query. For each algorithm, the type of input document collection with which the algorithm is likely to perform well is identified.	algorithm;profiling (computer programming)	Weiyi Meng;Clement T. Yu;Wei Wang;Naphtali Rishe	1996		10.1109/ICDE.1996.492214	query expansion;computer science;data mining;database;distributed database;information retrieval;query language	DB	-27.967743086070044	4.3347287733632385	139820
635623cfc473141133155593d7bc73b14d91c536	an ontology design pattern for material transformation		In this work we discuss an ontology design pattern for material transformations. It models the relation between products, resources, and catalysts in the transformation process. Our axiomatization goes beyond a mere surface semantics. While we focus on the construction domain, the pattern can also be applied to chemistry and other domains.	axiomatic system;software design pattern	Charles Vardeman;Adila Krisnadhi;Michelle Cheatham;Krzysztof Janowicz;Holly Ferguson;Pascal Hitzler;Aimee Buccellato;Krishnaprasad Thirunarayan;Gary Berg-Cross;Torsten Hahmann	2014			data mining;mathematics;algorithm	AI	-19.819354744982117	5.511439687097676	140026
e788f08148b4b06f4e537b27b51338f1c88ccea8	fact++ description logic reasoner: system description	modelizacion;representacion conocimientos;ontologie;tool support;array mathematics;relacion orden;heuristic method;web semantique;tablero;service web;tipo dato;chaine caractere;logica descripcion;ordering;prise de decision;metodo heuristico;logical programming;data type;web service;classification;modelisation;relation ordre;decision procedure;programmation logique;web semantica;cadena caracter;ontology engineering;representation connaissance;semantic web;ontologia;methode heuristique;description logic;knowledge representation;type donnee;toma decision;programacion logica;modeling;ontology;clasificacion;servicio web;character string;logique description;tableau matriciel	This is a system description of the Description Logic reasoner FaCT++. The reasoner implements a tableaux decision procedure for the well known SHOIQ description logic, with additional support for datatypes, including strings and integers. The system employs a wide range of performance enhancing optimisations, including both standard techniques (such as absorption and model merging) and newly developed ones (such as ordering heuristics and taxonomic classification). FaCT++ can, via the standard DIG interface, be used to provide reasoning services for ontology engineering tools supporting the OWL DL ontology language.	decision problem;description logic;heuristic (computer science);ontology (information science);ontology engineering;semantic reasoner;web ontology language;whole earth 'lectronic link	Dmitry Tsarkov;Ian Horrocks	2006		10.1007/11814771_26	web service;knowledge representation and reasoning;description logic;systems modeling;string;data type;biological classification;order theory;computer science;semantic reasoner;artificial intelligence;semantic web;ontology;database;programming language;algorithm	AI	-26.757094551955827	11.333706531980877	140130
28df24d39735ca35483174235436b86f7a19c5eb	on supporting interactive association rule mining	query language;filtering;evaluation performance;logica booleana;filtrage;base donnee;performance evaluation;evaluacion prestacion;filtrado;coaccion;contrainte;database;base dato;mode conversationnel;interactive mode;data mining;lenguaje interrogacion;association rule mining;algorithme;algorithm;constraint;fouille donnee;modo conversacional;logique booleenne;association;langage interrogation;boolean logic;asociacion;algoritmo	We investigate ways to support interactive mining sessions, in the setting of association rule mining. In such sessions, users specify conditions (filters) on the associations to be generated. Our approach is a combination of the incorporation of filtering conditions inside the mining phase, and the filtering of already generated associations. We present several concrete algorithms and compare their performance.	algorithm;association rule learning;filter (signal processing)	Bart Goethals;Jan Van den Bussche	2000		10.1007/3-540-44466-1_31	filter;boolean algebra;association;association rule learning;computer science;artificial intelligence;data mining;database;constraint;algorithm;query language	ML	-27.157155347661657	11.218125927829544	140243
e3c7a6e53a06af17a41ac8e1ad2eaf26bf84119c	on the completion of the most specific hypothesis computation in inverse entailment for mutual recursion	representacion conocimientos;learning algorithm;adquisicion del conocimiento;algorithm analysis;algorithme apprentissage;logical programming;acquisition connaissances;first order;inductive reasoning;programmation logique;knowledge acquisition;background knowledge;analyse algorithme;knowledge representation;representation connaissances;algoritmo aprendizaje;programacion logica;analisis algoritmo;raisonnement inductif	In this paper, we introduce a complete algorithm for computing the most specific hypothesis (MSH) in Inverse Entailment when the background knowledge is a set of definite clauses and the positive example is a ground atom having the same predicate symbol as that of the target predicate to be learned.#R##N##R##N#Muggleton showed that for any first order theory (background knowledge) B and a single clause (a positive example) E, the MSH can be computed by first computing all ground (positive and negative) literals which logically follow from B∧¬E and negating their conjunction. However, Yamamoto gave a counter example and indicated that Muggleton's proof contains error. Furukawa gave a sufficient condition to guarantee the above algorithm to compute the MSH. Yamamoto defined a class of problems where the algorithm computes the MSH. In this paper, we extend the MSH computation algorithm to ensure that it computes the MSH correctly under the condition described above.	computation;mutual recursion	Koichi Furukawa	1998		10.1007/3-540-49292-5_28	knowledge representation and reasoning;computer science;artificial intelligence;inductive reasoning;mathematics;algorithm	NLP	-19.884093803852693	11.414336558666934	140297
0cba354180ff2909fac8ecad2291e256079604be	peer-to-peer query answering with inconsistent knowledge	query answering;peer to peer	Decentralized reasoning is receiving increasing attention due to the distributed nature of knowledge on the Web. We address the problem of answering queries to distributed propositional reasoners which may be mutually inconsistent. This paper provides a formal characterization of a prioritized peerto-peer query answering framework that exploits a priority ordering over the peers, as well as a distributed entailment relation as an extension to established work on argumentation frameworks. We develop decentralized algorithms for computing query answers according to distributed entailment and prove their soundness and completeness. To improve the efficiency of query answering, we propose an ordering heuristic that exploits the peers’ priority ordering and empirically evaluate its effectiveness.	algorithm;argumentation framework;heuristic;peer-to-peer;world wide web	Arnold Binas;Sheila A. McIlraith	2008			computer science;data mining;database;information retrieval	AI	-23.085869670081344	6.915717324932404	140471
b40629546c57b49441398a47ed7c3d6b8a8772be	dynamic magic sets for super-consistent answer set programs	answer sets;data integrity;answer set programming;search space;query optimization;logic in computer science;query answering	For many practical applications of ASP, for instance data integration or planning, query answering is important, and therefore query optimization techniques for ASP are of great interest. Magic Sets are one of these techniques, originally defined for Datalog queries (ASP without disjunction and negation). Dynamic Magic Sets (DMS) are an extension of this technique, which has been proved to be sound and complete for query answering over ASP programs with stratified negation. A distinguishing feature of DMS is that the optimization can be exploited also during the nondeterministic phase of ASP engines. In particular, after some assumptions have been made during the computation, parts of the program may become irrelevant to a query under these assumptions. This allows for dynamic pruning of the search space, which may result in exponential performance gains. In this paper, the correctness of DMS is formally established and proved for brave and cautious reasoning over the class of super-consistent ASP programs (ASP programs). ASP programs guarantee consistency (i.e., have answer sets) when an arbitrary set of facts is added to them. This result generalizes the applicability of DMS, since the class of ASP programs is richer than ASP programs with stratified negation, and in particular includes all odd-cycle-free programs. DMS has been implemented as an extension of DLV, and the effectiveness of DMS for ASP programs is empirically confirmed by experimental results with this system.	automated planning and scheduling;computation;correctness (computer science);dlv;datalog;exponential time hypothesis;field (computer science);mathematical optimization;prototype;query optimization;recursion;relevance;stable model semantics;time complexity	Mario Alviano;Wolfgang Faber	2010	CoRR		query optimization;computer science;artificial intelligence;answer set programming;data integrity;data mining;database;mathematics;programming language;algorithm	DB	-23.326581576352407	11.173820284990244	140535
0865c0d851f55bc3b1568f6579fa9a55bbe58bfa	symbolic gray code as a data allocation scheme for two-disc systems	disque;hachage;distributed system;gray code;disk;systeme reparti;storage system;sistema informatico;stockage donnee;computer system;disco;data storage;hashing;sistema repartido;systeme memoire;code gray;almacenamiento datos;systeme informatique;information system;codigo gray;sistema memoria;systeme information;sistema informacion	In this paper, we are concerned with the multi-disc data allocation problem. We shall show that the symbolic Gray code can be used as a data allocation scheme for two-disc systems. We then show that this allocation method is a strictly optimal one for all possible partial match queries. There are many other interesting properties of this allocation method. For example, there is a disc-to-record transformation which can be used to determine the records stored on certain disc. In addition, all records are uniformly stored on discs if this allocation method is used. At the end of this paper, we shall point out that the resulting allocation is good for nearest neighbour searching.		Chin-Chen Chang;H. Y. Chen;C. Y. Chen	1992	Comput. J.	10.1093/comjnl/35.3.299	gray code;computer science;artificial intelligence;operating system;computer data storage;static memory allocation;information system;algorithm	Theory	-26.884547663109007	5.683258973214863	140637
69a5c73c063e674dd832e3fd172a47f7ddf4adc5	a minimal attribute set-oriented data provenance method		In view of data provenance in ETL, to improve the efficiency of provenance tracking, this paper analyzes the common transformation and attribute mapping of ETL, focuses on the key attributes in key attribute mapping, summarizes its characteristics, puts forward the concept of minimal attribute set, and designs the data provenance method based on minimal attribute set. In the reverse tracking, using this method construct the reverse transformation sequence whose input and output patterns are dynamically transformed, the number of attributes is decreasing, the space - time costs is reduced and the provenance efficiency is improved.	input/output	Chaofan Dai;Ran Zhang;Pei Li;Wenqian Wang;Zewen Cao	2017		10.1145/3175684.3175686	data mining;artificial intelligence;machine learning;computer science;provenance;input/output	DB	-32.01605895801434	5.457358338374126	140720
f73d726095137ecf94e7b4f73a32ef39b9d9396c	extending datatype support for tractable reasoning with owl 2 el ontologies		It was mentioned on multiple occasions that datatype expressions are a necessary component of any production quality knowledge base and will surely play a major role in the upcoming Semantic Web. This paper briefly summarizes the work on improving the support for tractable reasoning with datatype expressions in ELK a highly efficient reasoner. Tests have shown an exceptional speed of ontology classification of great size which opens up new perspectives for applying ontologies with datatype expressions in practice.	cobham's thesis;knowledge base;ontology (information science);semantic web;semantic reasoner;web ontology language	Pospishnyi Oleksandr	2013			semantic web;ontology;knowledge base;ontology (information science);web ontology language;expression (mathematics);machine learning;artificial intelligence;mathematics;semantic reasoner	AI	-21.077252292774293	8.604716990149557	140812
fb7034d886b528f8eda8aa6190389d3faf316759	towards a geometric-object-oriented language	lenguaje programacion;symbolic computation;automatic proving;object oriented language;visualizacion;programming language;geometrie algorithmique;computational geometry;demostracion automatica;calculo simbolico;demonstration automatique;visualization;langage symbolique;visualisation;object oriented;comportement utilisateur;symbolic language;lenguaje simbolico;langage programmation;oriente objet;geometria computacional;user behavior;orientado objeto;calcul symbolique;comportamiento usuario	This paper proposes a geometric-object-oriented language for symbolic geometric computation, reasoning, and visualization. In this language, geometric objects are constructed with indefinite parametric data. Modifications and basic operations on these objects are enabled. Degeneracy and uncertainty are handled effectively by means of imposing conditions and assumptions and geometric statements are formulated by declaring relations among different objects. A system implemented on the basis of this language will allow the user to perform geometric computation and reasoning rigorously and to prove geometric theorems and generate geometric diagrams and interactive documents automatically. We present the overall design of the language, explain the capabilities, features, main components of the proposed system, provide specifications for some of its functors, report our experiments with a preliminary implementation of the system, and discuss some encountered difficulties and research problems.	computation;computational geometry;degeneracy (biology);diagram;experiment	Tielin Liang;Dongming Wang	2004		10.1007/11615798_9	symbolic computation;visualization;computational geometry;computer science;artificial intelligence;theoretical computer science;mathematics;programming language;object-oriented programming;algorithm	PL	-29.65057875981883	13.757636218024416	140936
cd78577934635d943cc6864a9845bc3885d3b0ca	consistency and orderability: semantics-based correctness criteria for databases	database system;base donnee;database;base dato;semantics;simultaneidad informatica;abstract data type;semantica;semantique;data model;serializability theory;concurrency;object oriented;type abstrait;concurrency control;integrity constraints;base donnee orientee objet;oriente objet;tipo abstracto;modele donnee;object oriented databases;object oriented database;transaction processing;simultaneite informatique;orientado objeto;traitement transaction	The semantics of objects and transactions in database systems are investigated. User-defined predicates called consistency assertions are used to specify user programs. Three new correctness criteria are proposed. The first correctness criterion consistency is based solely on the users' specifications and admit nonserializable executions that are acceptable to the users. Integrity constraints of the database are maintained through consistency assertions. The second correctness criterion orderability is a generalization of view serializability and represents a weak notion of equivalence to a serial schedule. Finally, the third correctness criterion strong order-ability is introduced as a generalization of conflict serializability. Unlike consistency, the notions of orderability allow users to operate an isolation as maintenance of the integrity constrainst now becomes the responsibility of the database system.	consistency model;correctness (computer science);data integrity;database;serializability;turing completeness	Divyakant Agrawal;Amr El Abbadi;Ambuj K. Singh	1993	ACM Trans. Database Syst.	10.1145/155271.155276	weak consistency;concurrency;transaction processing;data model;computer science;concurrency control;data integrity;database;distributed computing;semantics;programming language;object-oriented programming;abstract data type;strong consistency	DB	-26.691238197847834	12.617617029263032	141066
1746a1f92f97e4d15bbcbca627b8e21ef001adf4	tiresias: the database oracle for how-to queries	constrained optimization;query language;performance evaluation;data management;data driven optimization;system performance;specification language;tiresias;data analysis;mixed integer program;tiql;relational database system;query answering;constrained optimization problem;divide and conquer;possible worlds	"""How-To queries answer fundamental data analysis questions of the form: """"How should the input change in order to achieve the desired output"""". As a Reverse Data Management problem, the evaluation of how-to queries is harder than their """"forward"""" counterpart: hypothetical, or what-if queries.  In this paper, we present Tiresias, the first system that provides support for how-to queries, allowing the definition and integrated evaluation of a large set of constrained optimization problems, specifically Mixed Integer Programming problems, on top of a relational database system. Tiresias generates the problem variables, constraints and objectives by issuing standard SQL statements, allowing for its integration with any RDBMS.  The contributions of this work are the following: (a) we define how-to queries using possible world semantics, and propose the specification language TiQL (for Tiresias Query Language) based on simple extensions to standard Datalog. (b) We define translation rules that generate a Mixed Integer Program (MIP) from TiQL specifications, which can be solved using existing tools. (c) Tiresias implements powerful """"data-aware"""" optimizations that are beyond the capabilities of modern MIP solvers, dramatically improving the system performance. (d) Finally, an extensive performance evaluation on the TPC-H dataset demonstrates the effectiveness of these optimizations, particularly highlighting the ability to apply divide-and-conquer methods to break MIP problems into smaller instances."""	aggregate data;constrained optimization;datalog;declarative programming;emoticon;ibm tivoli storage productivity center;integer programming;linear programming;mathematical optimization;parallel computing;performance evaluation;possible world;query language;relational database management system;sql;scalability;semiconductor industry;solver;specification language;user interface	Alexandra Meliou;Dan Suciu	2012		10.1145/2213836.2213875	constrained optimization;relational database management system;divide and conquer algorithms;specification language;data management;computer science;theoretical computer science;data mining;database;possible world;data analysis;programming language;query language	DB	-26.189991664128286	5.834260460899776	141172
104993ce9cafaafdf22d7b60465a716688aa24c7	querying xml in timber	xml database;system design;semi structured data;rule based;query optimization	In this paper, we describe the TIMBER XML database system implemented at University of Michigan. TIMBER was one of the first native XML database systems, designed from the ground up to store and query semi-structured data. A distinctive principle of TIMBER is its algebraic underpinning. Central contributions of the TIMBER project include: (1) tree algebras that capture the structural nature of XML queries; (2) the stack-based family of algorithms to evaluate structural joins; (3) new rule-based query optimization techniques that take care of the heterogeneous nature of the intermediate results and take the schema information into consideration; (4) cost-based query optimization techniques and summary structures for result cardinality estimation; and (5) a family of structural indices for more efficient query evaluation. In this paper, we describe not only the architecture of TIMBER, its storage model, and engineering choices we made, but also present in hindsight, our retrospective on what went well and not so well with our design and engineering choices. Figure 1: TIMBER Architecture: XML documents are parsed and nodes stored individually in the back-end store. Parsed queries, from multiple supported interfaces, go through a query optimizer to the query evaluator in a relatively standard overall database system architecture. The TIMBER system [10, 16] was developed at the University of Michigan, Ann Arbor, beginning 1999. It was an early native XML data management system. In this retrospective, we take stock of our work over the past nine years. Figure 1 provides an overview of the major system components. Secs. 1 through 4 describe the underlying algebra, query evaluation methods, query optimization, and indices, respectively. Sec. 5 mentions aspects of TIMBER not included in this article. Sec. 6 concludes with a retrospective view. 1 Algebra Relational algebra has been a crucial foundation for relational database systems, and has played a large role in enabling their success. A corresponding XML algebra for XML query processing has been more elusive, due to the comparative complexity of XML, and its history. In the relational model, a tuple is the basic unit of operation and a relation is a set of tuples. In XML, a database is often described as a forest of rooted node-labeled trees. Hence, for the basic unit and central construct of our algebra, we chose an XML query pattern (or Copyright 2008 IEEE. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works must be obtained from the IEEE. Bulletin of the IEEE Computer Society Technical Committee on Data Engineering	algorithm;care-of address;information engineering;interpreter (computing);linear algebra;logic programming;mathematical optimization;parsing;query optimization;relational algebra;relational database;relational model;semi-structured data;semiconductor industry;server (computing);stack-oriented programming language;storage model;systems architecture;xml database	Yuqing Wu;Stelios Paparizos;H. V. Jagadish	2008	IEEE Data Eng. Bull.		database;data mining;streaming xml;xml framework;xml validation;computer science;query optimization;xml schema editor;xml database;xml base;efficient xml interchange	DB	-30.401700367944322	6.890676112797287	141269
b1b6f18b9851cdf45a0b5d5047effd7d2eac0273	database skeleton and its application to logical database synthesis	database management systems;logical database design;relational database;data model;skeleton relational databases data models laboratories knowledge based systems knowledge engineering cancer councils dictionaries;relational database database management systems database skeleton logical database design;database design;database management system;database skeleton	This paper presents the concept of a model of database skeleton which is used to represent user's knowledge on the meaning of the data. The logical database synthesis pfoblem is then formulated as a problem of convetting a partially specified database skeleton to a completely specified-one, which corresponds to one of several data models. Approaches to optimal logical database synthesis for the relational database are then considered.	data model;relational database	Shi-Kuo Chang;Wu-Haung Cheng	1978	IEEE Transactions on Software Engineering	10.1109/TSE.1978.231463	database theory;relational model;intelligent database;database transaction;surrogate key;entity–relationship model;database tuning;data model;relational database;computer science;probabilistic database;database model;data mining;database;view;database schema;physical data model;information retrieval;alias;database testing;database design;spatiotemporal database;component-oriented database	DB	-31.64384146407395	10.772500939220492	141288
8ab4e17ea170b0eaf525f6e924814e0f10fed099	improving sql with generalized quantifiers	natural language interfaces;operator;generalized quantifiers;query processing;relational databases sql natural language interfaces query processing;sql;sql interface generalized quantifiers operator sets natural language phrases reasoning sql users;sql users;natural language;natural language phrases;relational databases;sets;reasoning;sql interface;databases computer science natural languages writing	A generalized quantifier is a particular kind of operator on sets. Coming under increasing attention recently by linguists and logicians, they correspond to many useful natural language phrases, including phrases like: three, Chamberlzn’s three, more than three, fewer than three, at most three, all but three, no more than three, not more than half the, at least two and not more than three, no student’s, most male and all female, etc. Reasoning about quantifiers is a source of recurring problems for most SQL users, and leads to both confusion and incorrect expression of queries. By adopting a more modern and natural model of quantification these problems can be alleviated. In this paper we show how generalized quantifiers can be used to improve the SQL interface.	natural language;quantifier (logic);sql	Ping-Yu Hsu;Douglas Stott Parker	1995		10.1109/ICDE.1995.380381	natural language processing;data definition language;sql;nested set model;sql injection;stored procedure;relational database;computer science;query by example;operator;user-defined function;database;language integrated query;natural language;programming language;null;reason	Theory	-25.072673744080145	12.45411796707948	141507
f55a76fc3f66341e099f9184a5c6e1482a74abb5	automated synthesis of constrained generators	top down;satisfiability;explicit knowledge;entire solution;knowledge compilation	"""Knowledge compilation is an emerging research area that focuses on """"compi l ing"""" a problem solver's inefficient, explicit knowledge representat ion into more efficient, impl ic i t forms. This paper presents a technique that transforms a declarative problem description (specifying the problem but not how to solve i t ) into a reasonably efficient, generate-and-test problem solver. Our technique performs constraint incorporation, modifying the parameter generators so they only generate values that satisfy the problem constraints. Successful constraint incorporation depends upon choosing the right solut ion representation (i.e., the set of parameters). Having expressed a constraint in terms of a particular set of parameters, incorporation fails if the constraint is not factorable into constraints on the individual parameter generators. R ICK, a Refinement-based constraint Incorporator for Compil ing Knowledge, is a prototype program that compiles a problem specification into a problem solver using least commitment, topdown refinement to achieve constraint incorporat ion. R ICK refines an abstract solution representation to avoid premature commitment to representations that hinder constraint incorporat ion. R I C K is able to incorporate local constraints that constrain relatively small portions of the entire solution. We have tested these ideas by having RICK automatical ly construct a house floor planning problem solver. *The research reported here was supported in part by the Defense Advanced Research Projects Agency (DARPA) under Contract Number N00014-85-K-0116, in part by the National Science Foundation (NSF) under Grant Number DMC-8610507, and in part by the Center for Computer Aids to Industrial Productivity (CAIP), Rutgers University, with funds provided by the New Jersey Commission on Science and Technology and by CAIP's industrial members. The first author has also received support from IBM. The opinions expressed in this paper are those of the authors and do not reflect any policies, either expressed or implied, of any granting agency. * currently on leave at IBM Watson Research Center. 1 I n t r o d u c t i o n Many AI systems perform run time evaluation of explicitly represented domain knowledge to find a solution to a problem. Such systems make a particular tradeoff between the """"explicitness of the representation and the efficiency of the computat ion"""" [Dietterich (ed.), 1986]. Expl ic i t ly represented knowledge is easier to gather, but more costly to evaluate. This observation has spawned a research area called knowledge compilation, which includes methods for """"compil ing"""" an inefficient, explicit knowledge representation into more efficient, impl ic i t forms. The work described in this paper is being conducted wi th in the context of the KBSDE project [Tong, 1986]. The purpose of the project is to develop compilat ion techniques whose input is a declarative problem description that does not specify how to solve the problem. The problem description includes a set of problem constraints that must be satisfied by a solution. The compilation techniques we are developing produce a generateand-test problem solver. Such a problem solver must generate a solution that passes a set of tests, one for each problem constraint. We have attempted to develop a compilation method that places as few requirements as possible on the domain, so that the method is broadly applicable. Thus, we have made the very weak requirement that the problem be solvable using a generate-andtest algori thm. In addit ion to insisting that our compiler be general, we have also required that it produce reasonably efficient algorithms. Like computer language compilers, knowledge compilers can use optimization techniques to improve the performance of the resulting problem-solving system. One optimization technique for a generate-and-test architecture is test incorporation [Dietterich and Bennett, 1986]. Test incorporation involves test movement, constraint incorporation, or both. Test movement regresses tests back into the generator process to achieve early pruning wi thout affecting the correctness of the problem solver. Constraint incorporation [Tappel, 1980] modifies the generator so that it enumerates only those values which satisfy a particular problem constraint; we wi l l call such a generator a constrained generator. The test corresponding to the incorporated constraint can be removed from the generate-and-test problem solver. Constraint incorporation reduces the size of the problem solver's search space and results in a more efficient problem solver. ConBraudaway and Tong 583 s t ra in t i n c o r p o r a t i o n makes use o f e x p l i c i t l y represented d o m a i n knowledge (e.g. , spec i f ica t ions for sys tem c o m ponen ts ) , t hus d i s t i n g u i s h i n g i t f r o m more conven t i ona l code o p t i m i z a t i o n approaches w h i c h re ly solely on syn tac t i c know ledge (e .g . , d a t a dependencies a m o n g sys tem componen t s ) . T h i s paper focuses on c o n s t r u c t i n g cons t ra ined genera tors of hierarchical so lu t ions , i.e., so lu t ions t h a t have par ts (e.g. , a house w i t h r o o m s ) . H ie ra rch i ca l so lu t ions can be genera ted us ing co r respond ing l y h ie ra rch ica l genera tors (e.g. , F i g u r e 3) . T h a t is , so l u t i on pa r t s are genera ted b y sub-genera to rs , a n d p r i m i t i v e so lu t i on p a r a m eters are assigned values by pa rame te r genera tors . A p a r t i c u l a r solution representation is def ined by a set of p r i m i t i v e pa rame te rs . To i n c o r p o r a t e a cons t ra in t i n t o a h ie ra rch ica l genera tor requi res t h a t i t be loca l izab le to a n d i n c o r p o r a t e d i n one o r more o f the ( p r i m i t i v e ) parame te r genera tors . I n c o r p o r a t i n g cons t ra in t s i s d i f f i cu l t w h e n the so lu t i on rep resen ta t i on is """" i n a p p r o p r i a t e """" . A cons t ra in t expressed in te rms of a p a r t i c u l a r so lu t i on represen ta t ion can have a s t r u c t u r e t h a t does no t """" m a t c h """" t h a t o f the genera to r ; t h a t is , the cons t ra i n t m a y no t be fac to rab le i n t o cons t ra in t s on the i n d i v i d u a l pa rameter generators . We w i l l ca l l th i s d i f f i cu l t y the structure mismatch problem. T h i s paper demons t ra tes a sys tem wh i ch i nco rpo ra tes the cons t ra in t s wh i l e a v o i d i n g th i s s t r u c t u r e m i s m a t c h p r o b l e m . R I C K , a Ref inement -based cons t ra in t I n c o r p o r a t o r for C o m p i l i n g K n o w l e d g e , is a p r o t o t y p e p r o g r a m t h a t compi les a p r o b l e m solver us ing least c o m m i t m e n t , top-down refinement to achieve cons t r a i n t i n c o r p o r a t i o n . T h e least c o m m i t m e n t app roach helps to avo id a p r e m a t u r e c o m m i t m e n t to a representat i o n t h a t m a y lead t o the s t r u c t u r e m i s m a t c h p r o b l e m . Sect ion 2 of th is paper defines the class of doma ins for w h i c h our m e t h o d appl ies and i l l us t ra tes an example f r o m th i s class: the house f loor p l a n n i n g d o m a i n . Sect i o n 3 i l l us t ra tes t he s t r u c t u r e m i s m a t c h p r o b l e m in the house f l oo r p l a n n i n g d o m a i n . T h e f o u r t h sect ion discusses the concepts b e h i n d our app roach and i l l us t ra tes the i r i m p l e m e n t a t i o n i n R I C K . W e compare our wo rk w i t h re la ted research in Sect ion 5. F i na l l y , Sect ion 6 summar izes t he paper and some o f the l i m i t a t i o n s o f our app roach . 2 T h e p r o b l e m doma in P a r a m e t e r i n s t a n t i a t i o n d e s i g n p r o b l e m s . T h e K B S D E p ro jec t has focused on design doma ins where the p r o b l e m solver cons t ruc ts an a r t i f a c t t h a t satisfies a set o f p r o b l e m cons t ra in t s . M a n y design p rob lems can be v iewed as parameter instantiation p rob lems . T h e hierarchical structure of the a r t i f a c t is usua l l y p re -de te rm ined for a class of pa rame te r i n s t a n t i a t i o n p rob lems ; for i n s tance, a l l house f loor p lans consist o f rec tangu la r r ooms , w h i c h , i n t u r n , consist o f sides and corners . T h e design task r e m a i n i n g for the p r o b l e m solver i s to """" f i l l i n """" and """" i n t e r c o n n e c t """" the s t r u c t u r e by ass igning values to the unspeci f ied a r t i f a c t pa ramete rs in a w a y t h a t is cons istent w i t h the p r o b l e m cons t ra i n t s . R I C K cons t ruc ts a p r o b l e m solver for pa rame te r i n s t a n t i a t i o n p rob lems whose so lu t ions are compos i te ob jec ts (e.g. , house f loor p lans ) . T h e p r o b l e m cons t ra in t s are p resumed to be h a r d cons t ra in t s t h a t def ine feasible so lu t ions ( ra ther t h a n soft cons t ra in t s t h a t def ine the re la t ive o p t i m a l i t y o f feasible so lu t i ons ) . We also presume t h a t the design p r o b l e m is no t ove r -cons t ra ined : a so lu t i on t h a t satisfies a l l the cons t ra in t s can be f o u n d . A h o u s e f l o o r p l a n n i n g d o m a i n . T o i l l u s t r a te our ideas, we w i l l use the pa rame te r i n s t a n t i a t i o n p r o b l e m of c o n s t r u c t i n g """"house floor p l a n s """" . A house floor p lan is a t w o d i m e n s i o n a l , rec tangu la r house p laced a t the o r ig in o f an x y g r i d and h a v i n g rec tangu la r rooms as par ts ; such a f loor p lan is a b s t r a c t l y dep ic ted in F igu re 1. A l l leng ths a n d coord ina tes are m u l t i p l e s o f 1 f oo t . T h e solut i o n is a f loor p l an t h a t con ta ins a p rob lem-spec i f i c n u m ber o f r o o m s , and satisf ies a l l o f the p r o b l e m cons t ra in ts shown in F igu re 2 . C o n s t r a i n t s S C I , SC2 an"""	algorithm;artificial intelligence;compiler;computer language;correctness (computer science);decision problem;declarative programming;floorplan (microelectronics);genera;ibm notes;knowledge compilation;knowledge representation and reasoning;lan messenger;loic;lu decomposition;linear algebra;mathematical optimization;naruto shippuden: clash of ninja revolution 3;numerical aperture;numerically controlled oscillator;original net animation;problem solving;prototype;re-order buffer;refinement (computing);requirement;run time (program lifecycle phase);scarab of ra;solver;spec#;thomas j. watson research center;top-down and bottom-up design;tor messenger;universal instantiation;watson (computer)	Wesley Braudaway;Chris Tong	1989			mathematical optimization;computer science;artificial intelligence;explicit knowledge;theoretical computer science;machine learning;top-down and bottom-up design;constraint satisfaction dual problem;mathematics;constraint;programming language;algorithm;satisfiability	AI	-21.150475751206546	16.988793868211797	142270
cea0007431e5b32cd1b81d19c93b7b7a7dec60a7	efficient querying and animation of periodic spatio-temporal databases	query language;relation algebra;spatio temporal databases;data model;animation;relational algebra	We propose a representation of spatio-temporal objects with continuous and cyclic or acyclic periodic movements. We also describe an extended relational algebra query language for databases with such objects. We show that the new spatio-temporal databases are closed under the extended relational algebra queries, and each fixed relational algebra query can be evaluated in PTIME in the size of the input database.	data model;directed acyclic graph;expressive power (computer science);p (complexity);quasiperiodicity;query language;r* tree;relational algebra;spatiotemporal database;stationary process;temporal database	Peter Z. Revesz;Mengchu Cai	2002	Annals of Mathematics and Artificial Intelligence	10.1023/A:1016399308282	anime;domain relational calculus;sargable;query optimization;discrete mathematics;relational model;codd's theorem;boolean conjunctive query;relational calculus;data model;relational algebra;relational database;computer science;query by example;theoretical computer science;relation algebra;database;rdf query language;conjunctive query;query language	DB	-29.406032663674083	8.590881446713814	142373
99996bf1fd4e8466ba6737505845eb02aa8f20c2	the definability abduction problem for data exchange - (abstract)	universiteitsbibliotheek	Data exchange is the problem of transforming data structured according to a source schema into data structured according to a target schema, via a mapping specified by means of rules in the form of source-to-targettuplegeneratingdependencies --- rules whose body is a conjunction of atoms over the source schema and the head is a conjunction of atoms over the target schema, with possibly existential variables in the head. With this formalization, given a fixed source database, there might be more than one target databases satisfying a given mapping. That is, the target database is actually an incompletedatabase represented by a set of possible databases. Therefore, the problem of query answering the target data is inherently complex for general (non-positive) relational or aggregate queries.		Enrico Franconi;Nhung Ngo;Evgeny Sherkhonov	2012		10.1007/978-3-642-33203-6_18	data exchange;information schema;semi-structured model;computer science;conceptual schema;star schema;data mining;database;database schema;algorithm	DB	-25.918207911728057	9.625503530993715	142581
0fa1fe584895765e97c22ce3673525920eca19b6	possibilistic testing of owl axioms against rdf data		We develop the theory of a possibilistic framework for OWL 2 axiom testing against RDF datasets, as an alternative to statistics-based heuristics. The intuition behind it is to evaluate the credibility of OWL 2 axioms based on the evidence available in the form of a set of facts contained in a chosen RDF dataset. To achieve it, we first define the notions of development, content, support, confirmation and counterexample of an axiom. Then we use these notions to define the possibility and necessity of an axiom and its acceptance/rejection index combining both of them. Finally, we report a practical application of the proposed framework to test SubClassOf axioms against the DBpedia RDF dataset.	dbpedia;heuristic (computer science);rejection sampling;web ontology language	Andrea Tettamanzi;Catherine Faron-Zucker;Fabien L. Gandon	2017	Int. J. Approx. Reasoning	10.1016/j.ijar.2017.08.012	web ontology language;machine learning;linked data;mathematics;counterexample;artificial intelligence;rdf;possibility theory;axiom;rdf schema;heuristics;data mining	Web+IR	-22.716851436610668	7.185334413585975	142607
d8ae856e059a55690c37f7ffc30ee6b39d2f82af	deriving data base specifications from user queries			database	James L. Weiner	1977			theoretical computer science;database;computer science	DB	-31.223829386101286	7.672760240951094	142743
4ffd38b36027a81440e998c4ac73f4fda5ca9b5a	intelligent query answering		One way to make query answering system (QAS) intelligent is to assume a hierarchical structure of its attributes. Such systems have been investigated by Cuppens & Demolombe (1988), Gal & Minker (1988), and Gaasterland et al. (1992), and they are called cooperative. Any attribute value listed in a query, submitted to cooperative QAS, is seen as a node of the tree representing that attribute. If QAS retrieves an empty set of objects, which match query q in a target information system S, then any attribute value listed in q can be generalized and the same the number of objects that possibly can match q in S can increase. In cooperative systems, these generalizations are usually controlled by users. Another way to make QAS intelligent is to use knowledge discovery methods to increase the number of queries which QAS can answer: knowledge discovery module of QAS extracts rules from a local system S and requests their extraction from remote sites (if system is distributed). These rules are used to construct new attributes and/or impute null or hidden values of attributes in S. By enlarging the set of attributes from which queries are built and by making information systems less incomplete, we not only increase the number of queries which QAS can handle but also increase the number of retrieved objects. So, QAS based on knowledge discovery has two classical scenarios that need to be considered:	consensus dynamics;generalized automation language;information system;question answering	Zbigniew W. Ras;Agnieszka Dardzinska	2009			natural language processing;ontology;knowledge base;semantics;artificial intelligence;computer science	DB	-26.694907062748747	7.057877125204903	142880
870184084925505fb30e61dd1dfe67b6f1aed13a	metaschemas for er, orm and uml data models: a comparison	information engineering;data model;database research;unified modeling language;conceptual modeling language;object role modeling;data structure	This paper provides metaschemas for some of the main database modeling notations used in industry. Two Entity Relationship (ER) notations (Information Engineering, and Barker) are examined in detail, as well as Object Role Modeling (ORM) conceptual schema diagrams. The discussion of optionality, cardinality and multiplicity is widened to include Unified Modeling Language (UML) class diagrams. Issues addressed in the metamodel analysis include the normalization impact of non-derived constraints on derived associations, the influence of orthogonality on language transparency, and trade-offs between simplicity and expressibility. To facilitate comparision, the same modeling notation is used to display each metaschema. For this purpose, ORM is used because of its greater expressibility and clarity.	class diagram;conceptual schema;entity–relationship model;information engineering;metamodeling;object-role modeling;unified modeling language	Terry A. Halpin	2002	J. Database Manag.	10.4018/jdm.2002040102	unified modeling language;data structure;information engineering;data model;systems modeling language;computer science;data mining;object-role modeling;database;modeling language;programming language;story-driven modeling;algorithm	DB	-31.82398495446347	12.885061568035406	143036
2c0698de7ed0ac1c01c3c4f65fb56cf6fee5f654	deriving relational database programs from formal specifications	formal specification;tool support;generalized derivation;relational database;satisfiability	The derivation of database programs directly from formal specii-cations is a well known and unsolved problem. Most of the previous work on the area either tried to solve the problem too generally or was restricted to some trivial aspects, for example deriving the database structure and/or simple operations. However diicult in general, deriving relational database applications directly from Z speciications satisfying a certain set of rules (the method) is not arduous. This paper describes a set of rules on how to map such speciications to database programs. If appropriate tool support is provided, writing formal speciications according to the method and deriving the corresponding relational database programs should be straightforward. Moreover it should produce code which is standardized and thus easier to understand and maintain.	database design;relational database	Roberto Souto Maior de Barros	1994		10.1007/3-540-58555-9_123	domain relational calculus;sql;relational model;codd's theorem;relational calculus;entity–relationship model;relational database;computer science;database model;formal specification;database;conjunctive query;programming language;database schema;program derivation;database design;satisfiability	DB	-25.9779958628832	10.535852762623469	143068
485af0292af87e1660b9e9eb2dc327e50da8bb74	disaggregations in databases	algebraic approach;recovery strategy;query processing;system log;database;transaction;resource consistency;relational model;system failure;point of view;database management system;process synchronization point	An algebraic foundation of database schema design is presented. A new database operator, namely, disaggregation, is introduced. Beginning with “free” families, repeated applications of disaggregation and three other operators (matching function, Cartesian product, and selection) yield families of increasingly elaborate structure. In particular, families defined by one join dependency and several “embedded” functional dependencies can be obtained in this manner.	cartesian closed category;database schema;embedded system;functional dependency;join dependency;linear algebra	Serge Abiteboul	1983		10.1145/588058.588104	query optimization;database theory;relational model;database transaction;rollback;database tuning;relational database;computer science;theoretical computer science;database model;transaction log;data mining;database;view;database schema;consistency;transaction processing system;database testing;database design	DB	-30.56381816349828	10.14877003108298	143206
acd3a9547a7db7273cac37046ccbe040619a912b	optimizations for the role-depth bounded least common subsumer in el+		Computing the least common subsumer (lcs) yields a generalization of a collection of concepts, computing such generalizations is a useful reasoning task for many ontology-based applications. Since the lcs need not exist, if computed w.r.t. general TBoxes, an approximative approach, the role-depth bounded lcs, has been proposed. Recently, this approach has been extended to the Description logic EL+, which covers most of the OWL 2 EL profile. In this paper we present two kinds of optimizations for the computation of such approximative lcs: one to obtain succinct rewritings of EL+concepts and the other to speed-up the k-lcs computation. The evaluation of these optimizations give evidence that they can improve the computation of the role-depth bounded lcs by orders of magnitude.	computation;description logic;level of detail;lowest common ancestor;mathematical optimization;web ontology language	Andreas Ecke;Anni-Yasmin Turhan	2012			description logic;data mining;ontology;computer science;discrete mathematics;web ontology language;computation;bounded function;generalization	AI	-23.551522844077564	8.380251473016605	143288
c88157134441f09bbb94a97d9fdf7dde5d5860a8	exploring concepts of metric space theory in similarity queries over complex data				Ives Rene Venturini Pola	2010				DB	-30.684952385236922	7.721759568398635	143493
01d0eed4c16246bd2830badd17921fa0d020b283	tuning the ims data base management system	information management system;minicomputers;data base management system;real time scheduling;modular design;data acquisition	"""For the purposes of this discussion, I would like to define """"tuning"""" as the adjustment of the variables in a generalized system to optimize it for a particular environment. The generalized systems we are talking about are called Generalized Data Base Management Systems or GDBMS, and I am going to talk about one of these: IBM's Information Management System, or IMS."""	database;information management system (ims);performance tuning	W. P. Grafton	1974		10.1145/1408800.1408899	embedded system;real-time computing;computer science;database;structure of management information	DB	-31.963298961122657	15.162862572364679	143585
52fda9af47772bf090dc734a5a0affb3ace66652	compression as data transformation	lempel ziv;data compression;context modeling spatial databases data processing compression algorithms cryptography standardization hardware software standards application software stability;vcodex data transformation platform data compression conventional compression technique lempel ziv technique huffman technique statistical model string matching burrows wheeler transform database table;statistical model;conventional compression technique;database table;vcodex data transformation platform;data transformation;burrows wheeler transform;string matching;lempel ziv technique;huffman technique	Summary form only given. Conventional compression techniques exploit general redundancy features in data to compress them. For example, Huffman or Lempel-Ziv techniques compresses data by statistical modeling or string matching while the Burrows-Wheeler Transform simply sorts data by context to improve compressibility. On the other hand, data can often be compressed better by exploiting their specific features. For example, columns or fields in a database table tend to be sparse, but not rows. Techniques have been developed to either group related table columns or compute dependency among them to transform data and enhance compressibility. The Vcodex data transformation platform provides a framework to develop and use such data transforms. That is, it treats compression techniques as invertible data transforms that can be composed together for specific tasks. In this way, data transformation remains general and can include techniques for encryption and others.	burrows–wheeler transform;column (database);data compression;encryption;huffman coding;lempel–ziv–welch;sparse matrix;statistical model;string searching algorithm;table (database)	Kiem-Phong Vo	2007	2007 Data Compression Conference (DCC'07)	10.1109/DCC.2007.23	data compression;statistical model;computer science;theoretical computer science;table;burrows–wheeler transform;data mining;mathematics;lossless compression;data transformation;algorithm;statistics;string searching algorithm	DB	-32.671443639861174	4.999117252503127	143747
255432f055125d4aba87a551d072f8f007e6bdd3	interrogation efficace de ressources distribues dans des systmes de mdiation	base donnee repartie;reseau pair;distributed database;integration information;interrogation base donnee;base repartida dato;interrogacion base datos;traitement de requetes distribuees;peer to peer p2p;information integration;integracion informacion;database query;restrictions d acces;integration de donnees et programmes	With the recent developments of the Web, we witness a shift from data integration systems to data and program integration applications, allowing communities of users to share their resources: data, programs, and computing power. This work investigates data and program integration in a fully distributed peer-to-peer mediation architecture. The challenge in making such a system succeed at a large scale is twofold. First, we need a simple concept for modeling resources. Second, we need efficient operators for distributed query execution, capable of handling well costly computations and large data transfers. To model heterogeneous resources, we use the model of table with binding patterns, simple yet powerful enough to capture data and programs. To exploit a resource with restricted binding patterns, we propose an efficient BindJoin operator, in which we build optimization techniques for minimizing large data transfers and costly computations. Furthermore, the proposed BindJoin operator delivers most of its output in the early stages of the execution, which is an important asset in a system meant for human interaction. Our experimental evaluation validates the proposed BindJoin algorithm on queries involving expensive programs. It shows that our BindJoin delivers the major part of the result early on during query execution. 2 Nom de la revue. Volume X – n° X/2001	acm computing surveys;acm transactions on database systems;algorithm;bibliothèque des ecoles françaises d'athènes et de rome;computation;computer data storage;council for educational technology;dance dance revolution extreme;dataflow;dynamic programming;e-services;erdős–rényi model;franklin electronic publishers;information system;interactive computing;journal of the acm;large eddy simulation;linear algebra;mathematical optimization;middleware;name binding;online aggregation;optimizing compiler;peer-to-peer;performance;phil bernstein;precedence effect;predicate (mathematical logic);query optimization;raman scattering;ripple;sigmod edgar f. codd innovations award;semiconductor industry;sethi–ullman algorithm;shim (computing);symposium on principles of database systems;vldb;wavelet scalar quantization;world wide web	Ioana Manolescu;Luc Bouganim;Françoise Fabret;Eric Simon	2003	Technique et Science Informatiques	10.3166/tsi.22.1271-1296	computer science;artificial intelligence;information integration;data mining;database;distributed database;computer security	DB	-32.025819161199216	7.24972377230573	143860
bf751d9e6c8f93fd5952632b91186b792cb33ec5	computing a deterministic semantics for p2p deductive databases		This paper proposes a logic based framework for data integration and query answering for deductive databases in a P2P environment. It is based on a special interpretation of mapping rules that leads to a declarative semantics for P2P systems defined in terms of preferred weak models. Under this semantics, only facts not making the local databases inconsistent can be imported, and the preferred weak models are the consistent scenarios in which peers import, by means of mapping rules, maximal sets of facts not violating (directly or indirectly) integrity constraints. The preferred weak models can be computed by means of a rewriting technique allowing to model a P2P system as a unique logic program whose stable models correspond to its preferred weak models. In the general case a P2P system may admit many preferred weak models and it has been shown that the complexity of their computation is prohibitive. Therefore, the paper looks for a more pragmatic solution assigning to a P2P system a new and more suitable semantics: the Well Founded Model Semantics. It allows to obtain a deterministic model whose computation is polynomial time. This model is a (partial) stable model obtained by evaluating with a three-value semantics the normal version of the rewriting of the P2P system. Finally, a distributed algorithm for the computation of the well founded model is proposed.	ccir system a;computation;consistency model;data integrity;deductive database;distributed algorithm;logic programming;maximal set;peer-to-peer;rewriting;stable model semantics;time complexity;value semantics;whole earth 'lectronic link	Luciano Caroprese;Ester Zumpano	2017		10.1145/3105831.3105837	database;time complexity;data mining;distributed algorithm;computer science;deterministic system;semantics;theoretical computer science;computation;well-founded semantics;data integrity;rewriting	DB	-21.316824586870723	10.687683853192015	144186
c351b054761298eeae1f9e7091f889dc636d1219	special issue on program development, guest editors' introduction	program development	This special issue marks the tenth anniversary of the LOPSTR workshop. LOPSTR started in 1991 as a workshop on Logic Program Synthesis and Transformation, but later it broadened its scope to logic-based Program Development in general.The motivating force behind LOPSTR has been a belief that declarative paradigms such as logic programming are better suited to program development tasks than traditional non-declarative ones such as the imperative paradigm. Specification, synthesis, transformation or specialisation, analysis, verification and debugging can all be given logical foundations, thus providing a unifying framework for the whole development process.In the past ten years or so, such a theoretical framework has indeed begun to emerge. Even tools have been implemented for analysis, verification and specialisation. However, it is fair to say that so far the focus has largely been on programming-in-the-small. So the future challenge is to apply or extend these techniques to programming-in-the-large, in order to tackle software engineering in the real world.		Maurice Bruynooghe;Kung-Kiu Lau	2002	TPLP	10.1017/S1471068402001424	computer science;artificial intelligence;programming language;algorithm	AI	-22.205131504501754	14.714930527865228	144273
93404c93cc7b109e2766445178c846bb649fe3af	tableau reasoning for description logics and its extension to probabilities	description logics;tableau;prolog;semantic web;68t30;68t37;68t27;68n17	The increasing popularity of the Semantic Web drove to a widespread adoption of Description Logics (DLs) for modeling real world domains. To help the diffusion of DLs, a large number of reasoning algorithms have been developed. Usually these algorithms are implemented in procedural languages such as Java or C++. Most of the reasoners exploit the tableau algorithm which features non-determinism, that is not easily handled by those languages. Prolog directly manages non-determinism, thus is a good candidate for dealing with the tableau’s non-deterministic expansion rules. We present TRILL, for “Tableau Reasoner for descrIption Logics in proLog”, that implements a tableau algorithm and is able to return explanations for queries and their corresponding probability, and TRILL P , for “TRILL powered by Pinpointing formulas”, which is able to compute a Boolean formula representing the set of explanations for a query. Reasoning on real world domains also requires the capability of managing probabilistic and uncertain information. We show how TRILL and TRILL P can be used to compute the probability of queries to knowledge bases following DISPONTE semantics. Experiments comparing these with other systems show the feasibility of the approach.	c++;computation;description logic;experiment;java;logic programming;long division;method of analytic tableaux;nondeterministic algorithm;peano axioms;performance;prolog;rdf/xml;semantic web;semantic reasoner;server (computing);web ontology language;web application;web framework;web page;world wide web;xml	Riccardo Zese;Elena Bellodi;Fabrizio Riguzzi;Giuseppe Cota;Evelina Lamma	2016	Annals of Mathematics and Artificial Intelligence	10.1007/s10472-016-9529-3	artificial intelligence;theoretical computer science;machine learning;mathematics;programming language;algorithm	AI	-23.442022734913827	7.947280028215178	144341
7c679ca2acb4ebb48de5bcfa967b2793ddeaaebe	tool to evaluate performance in distributed heterogeneous processing	middleware measurement internet information systems computer architecture data mining data analysis performance analysis data visualization workstations;performance evaluation;data visualisation performance evaluation distributed heterogeneous processing next generation computing architectures performance visualisation heterogeneous interoperating systems middleware information systems processing performance metrics user market study raw performance data extraction performance presentation;heterogeneous systems;distributed processing;performance tool;performance metric;conference paper;computer architecture;data extraction;next generation;performance evaluation open systems distributed processing;middleware;information system;open systems	The next generation computing architectures will consist of a fusion of heterogeneous systems interoperating. This concept is gaining popularity, yet there are currently few tools available for performance visualisation and management of heterogeneous interoperating systems using middleware. This paper looks at the design of middleware performance tools, using our current project as an example. focusing on information systenis processing. We discuss the issues such as performance metrics (obtained from a user market study), raw performance data extraction, its analysis, and finally performance presentation and visualisation. 1. Interoperable Environments and Middleware Today's enterprises consist of a decentralised collection of heterogeneous systems. In an attempt to upgrade older systems they have introduced more modern (e.g. Relational or 00) DBMS running on nonmainframe; workstation and PC based architectures. Business critical legacy systems have been a heavy investment and are practically impossible to rewrite. Consequently, there is a fundamental business need for interoperability as a cost effective way to use these systems, or migrate from legacy systems to the more modern. Michael Brodie's defines interoperability as the ability to interact effectively to achieve shared goals [7]. There has been a notable increase in interest in interoperability over the past few years, with a significant number of London financial institutions investing in some form of middleware. This interest will increase as more applications use the internet aided by implementations of CORBA incorporating internet hooks. Consequently, many researchers and industry counterparts share our vision of Co-operative Information Systems which they deem as being the next generation of information systems, emerging over the next few years [7]. The integration of heterogeneous systems are not new to the research community. Work on Federated DBMS have been around for some time [20]. The issues concerning interoperability are not the focus of this paper, but for clarity the reader may reference the many papers which discuss these issues [7,8]; and some which discuss different interoperability mechanisms: Object Equivalencing [14]. Mediation [15], KBS Assist [19], Orb based Intelligent Co-operating Systems [ 16,22,4,1,18] and Com based object systems [9]. However, this paper will concentrate on the design and development of performance tools for interoperable systems using our present project as a case study. The Iriss project is a continuing Eureka project which has evolved from a project which focused on distributed parallel database systems simultaneously running decision support and on-line transaction processing processes [23]. The distributed systems in the previous project made use of the middleware framework, Clearinghouse, to provide interoperability. It is for this reason that this paper will focus more on Clearinghouse; a message based framework, rather than CORBA object based technology, to describe the issues concerning the design of performance tools. Having said this, the authors are familiar with CORBA and bear it in mind when drawing conclusions. Therefore, it is our belief that the techniques used provides useful insight into performance tool design for all middleware systems. In this paper we state the case for middleware performance tools which is backed up by a user market study. We then explain the general issues of tool design, focusing on performance metrics and performance tools embedded in middleware. Finally, we briefly outline implementation issues and discuss the question of performance visualisation. 180 0-8186-8332-5/98 $10.00	backup;column-oriented dbms;common object request broker architecture;decision support system;distributed computing;embedded system;information systems;information system;interoperability;knowledge-based systems;legacy system;middleware;mind;next-generation network;object-based language;online and offline;online transaction processing;parallel database;rewrite (programming);workstation	Julie A. McCann;K. J. Manning	1998		10.1109/EMPDP.1998.647196	middleware;real-time computing;computer science;operating system;database	HPC	-32.53629923349012	15.586034085571539	144602
24f3f73c7421bebf208ba5fd686b51f84c5a785f	spar-key: processing sparql-fulltext queries to solve jeopardy! clues		We describe our SPAR-Key query engine that implements indexing, ranking, and query processing techniques to run a new kind of SPARQL-fulltext queries that were provided in the context of the INEX 2013 Jeopardy Task.	database;database schema;entity;machine translation;nl (complexity);relational database management system;resource description framework;sparql;sql;storage model	Arunav Mishra;Sairam Gurajada;Martin Theobald	2013			computer science;data mining;database;world wide web	DB	-33.1048638243188	6.2709738946052145	144608
d409d4dbf782c638a8addd93241d42907c9660f6	m4 - a metamodel for data preprocessing	metadata management;data mining;design and implementation;object relational data warehouse;method materialisation;data warehousing;object oriented view;data preprocessing;view materialisation	Metadata-driven tools store control information in repositories that are outside of programs and applications. At runtime, this control information (i.e., metadata) is read, interpreted and dynamically bound into software execution. If new requirements arise, metadata may be changed without affecting the programs sharing it and without requiring re-compilation of these programs. Repositories store metadata according to a metadata structure, called a metamodel. M4 is the metamodel used by Mining Mart, a system for supporting data preprocessing for data mining. The aim of this paper is twofold. First, we introduce M4 (the MetaModel of Mining Mart) and present some ideas underlying the design and implementation. Second, we discuss on the basis of M4 issues related to metadata-driven software: advantages of building and using such software, its weaknesses and the role it plays for metadata management, especially within data warehousing environments.	compiler;data mart;data mining;data pre-processing;interpreted language;metamodeling;preprocessor;requirement	Anca Vaduva;Jörg-Uwe Kietz;Regina Zücker	2001		10.1145/512236.512248	software mining;computer science;data science;data warehouse;data mining;database;data pre-processing;metadata;data element;meta data services;metadata repository	PL	-33.36417558566886	10.715059815199025	145049
4cc3bfc8d2ec27b958c06466bedeb0de72d16224	forgetting-based abduction in alc		This paper presents a method for abduction in description logic ontologies. The method is based on forgetting and contrapositive reasoning and can produce semantically minimal hypotheses for TBox and ABox abduction in the description logic ALC. The method is not restricted to Horn clauses or atomic observations and hypotheses. Key considerations when using forgetting for abduction are addressed. A Java prototype has been implemented, making use of the resolution-based forgetting method implemented in the tool LETHE. Experimental results over a corpus of ontologies show the practicality of the method across a number of scenarios.	abductive reasoning;abox;description logic;horn clause;iteration;java;ontology (information science);prototype;tbox;text corpus	Warren Del-Pinto;Renate A. Schmidt	2017			cognitive psychology;forgetting;mathematics	AI	-20.40858896262799	9.3078441281113	145083
280b29926f6eb3625f2da1213039f04c6ff91b54	inconsistency-tolerant semantics for description logics	conjunctive queries;expressive power;description logic;query answering;knowledge base	We address the problem of dealing with inconsistencies in Description Logic (DL) knowledge bases. Our general goal is both to study DL semantical frameworks which are inconsistency-tolerant, and to devise techniques for answering unions of conjunctive queries posed to DL knowledge bases under such inconsistency-tolerant semantics. Our work is inspired by the approaches to consistent query answering in databases, which are based on the idea of living with inconsistencies in the database, but trying to obtain only consistent information during query answering, by relying on the notion of database repair. We show that, if we use the notion of repair studied in databases, inconsistency-tolerant query answering is intractable, even for the simplest form of queries. Therefore, we study different variants of the repair-based semantics, with the goal of reaching a good compromise between expressive power of the semantics and computational complexity of inconsistency-tolerant query answering.	abox;adobe flash lite;algorithm;apple electric car project;computational complexity theory;conjunctive query;database;description logic;first-order logic;iar systems;rewriting;tbox	Domenico Lembo;Maurizio Lenzerini;Riccardo Rosati;Marco Ruzzi;Domenico Fabio Savo	2010		10.1007/978-3-642-15918-3_9	natural language processing;knowledge base;description logic;boolean conjunctive query;computer science;artificial intelligence;data mining;database;conjunctive query;expressive power	AI	-22.030657235709636	8.543625811468573	145097
79e9fcd25f43e4edfcf246fa6d6fff9b69d75b64	yet another query algebra for xml data	relational optimization;hypermedia markup languages;query language;hierarchical structure;xml data;digital movies;motion pictures;query processing;algebra xml database languages relational databases database systems motion pictures query processing runtime object oriented modeling power system modeling;runtime;query algebra;data model;query languages;expressive power;algebra;database systems;xml;object oriented algebra;relational algebra query processing hypermedia markup languages query languages data models relational databases object oriented databases;path expression evaluation;optimization;xtasy database management system;relational databases;object oriented databases;xml query processing;document order preservation;power system modeling;nested query resolution;database management system;optimization query algebra xml data relational databases digital movies query processing path expression evaluation data model nested query resolution document order preservation relational optimization query languages xtasy database management system object oriented algebra;database languages;path expressions;relational algebra;object oriented modeling;data models	XML has reached a widespread diffusion as a language for representing nearly any kind of data source, from relational databases to digital movies. Due to the growing interest toward XML, many tools for storing, processing, and querying XML data have appeared in the last two years. Three main problems affect XML query processing: path expression evaluation, nested query resolution, and preservation of document order. These issues, which are related to the hierarchical structure of XML and to the features of current XML query languages, require compile-time as well as run-time solutions. This paper describes a query algebra for XML data. The main purpose of this algebra, which forms the basis for the Xtasy database management system, is to combine good optimization properties with a good expressive power that allows it to model significant fragments of current XML query languages; in particular, explicit support is given to efficient path expression evaluation, nested query resolution, and order preservation.	compile time;compiler;mathematical optimization;path expression;query language;relational database;sql;subject reduction;xml;yet another	Carlo Sartiani;Antonio Albano	2002		10.1109/IDEAS.2002.1029662	xml catalog;xml validation;xml encryption;xml namespace;query optimization;simple api for xml;xml;relax ng;xml schema;streaming xml;computer science;document structure description;xml framework;xml database;xml schema;database;xml signature;programming language;xml schema editor;information retrieval;query language;efficient xml interchange	DB	-28.905499865558255	7.733560126350536	145211
d61606a732fda289e059f6c70d74c4b4de5da4a6	bucket-based query rewriting with disjunctive data source	conjunctive queries;mathematics;rule based;simulation;greedydual;algorithm;cache replacement;mathematics computer science;computer science;web caching;query rewriting	Many algorithms for query rewriting using views have been proposed.Most of them are used for rewriting conjunctive queries with conjunctive views only.Only one inverse rule-based rewriting algorithm has been presented to deal with query rewriting with disjunctive views, in which a set of disjunctive inverse rules are created and used to generate the query rewritings.However, there has no been bucket-based algorithm for this issue.In this paper we apply the ideas of the previous bucket-based algorithms to deal with query rewriting using views in the presence of disjunctions in view definitions.We create a set of buckets over either a conjunctive view or a disjunctive view.Specially, when a bucket is over a disjunctive view, we present a method to remove the tuples that are in the view but not required by a given query.The rewritings obtained by our algorithm are contained in the original query.	algorithm;conjunctive query;disjunctive normal form;logic programming;rewriting	Qingyuan Bai;Jun Hong;Michael F. McTear;Hui Wang	2004	IEEE/WIC/ACM International Conference on Web Intelligence (WI'04)	10.1109/WI.2004.36	rule-based system;query optimization;web query classification;boolean conjunctive query;computer science;artificial intelligence;theoretical computer science;database;conjunctive query;algorithm;query language	DB	-26.972497379010893	8.094491376240164	145257
9860ff7487d0e992014ba029cb124d1e875218e0	integrity constraint checking in stratified databases	integrity constraints	This paper continues the development of a foundation for deductive database systems begun in [6], [7], and [8]. The result of this paper is a simplification theorem for static integrity constraint checking in stratified databases. This result extends an earlier version of the theorem in [7] for definite databases. Integrity constraint checking is an important issue in database systems. From a theoretical viewpoint, it is highly desirable for the database to satisfy its integrity constraints at all times. However, from a practical viewpoint, there are serious difficulties in finding efficient ways of checking the integrity constraints after each update. The problem is especially difficult for deductive databases, since the addition of a single fact can have a substantial impact on the logical consequences of the database because of the presence of rules. In spite of these difficulties, it is possible to reduce the amount of computation if advantage is taken of the fact that, before the update was made, the database was known to satisfy its integrity constraints. The simplification theorem in this paper shows that it is only necessary to check certain instances of each integrity constraint. For a very large database, this can lead to a dramatic reduction in the amount of computation required. This idea is originally due to Nicolas [lo] in the context of relational database systems. The setting for the theorem in this paper is the class of stratified databases. This class is essentially an extension of the class of stratified general programs introduced	computation;data integrity;deductive database;level of detail;relational database	John W. Lloyd;Liz Sonenberg;Rodney W. Topor	1987	J. Log. Program.	10.1016/0743-1066(87)90009-4	computer science;data integrity;database;programming language	DB	-25.492769214859983	10.629100319733153	145360
c21172405ac7c67ec303f8ed48a300cee87a72cc	difficulties in integrating multiview development systems	multiple perspectives;programming environments;view oriented databases;database management systems;software tools database management systems development systems integrated software programming environments;integration mechanisms multiview development systems multiple perspectives development environment systems integration shared file systems selective broadcasting simple databases view oriented databases canonical representation;integration mechanisms;development environment;file system;system integration;multiview development systems;integrated software;shared file systems;selective broadcasting;development systems;software tools;systems integration;simple databases;software systems system testing system analysis and design handicapped aids petri nets visualization control systems writing programming environments user interfaces;canonical representation	Drawbacks of current approaches to integrating multiple perspectives in a development environment are discussed. An integrated environment is defined as one in which a dynamic collection of tools can work together on a single system so that changes made to the system by one tool can be seen by other tools, and integration criteria are set forth. Five representative approaches to systems integration-shared file systems, selective broadcasting, simple databases, view-oriented databases, and canonical representation-are examined, and their relative strengths and weaknesses are summarized. None of the integration mechanisms is shown to be uniformly superior to the others. The issue of environment evolution and its effect on integration is addressed. >		Scott Myers	1991	IEEE Software	10.1109/52.62932	computer science;engineering;theoretical computer science;database;distributed computing;system integration	Embedded	-32.46556316075956	12.338568328693057	145659
1b96cbb8edff2c477af69d3590ef07f21f09a9e1	systematic testing of database engines using a relational constraint solver	grammar;alloy tool set;dbms engine;database system;generators;oracle 11g database systematic testing database engine relational constraint solver systematic black box testing database management system automated database testing automated sql query generator dbms engine alloy tool set sql grammar constraint alloy constraint adusa;metals testing grammar generators aggregates database systems;metals;query processing;sql;sql constraint handling grammars program testing query processing relational databases;automatic testing;database;oracle 11g database;testing;constraint solver sql alloy database automatic testing dbms;database engine;sql grammar constraint;alloy;automated sql query generator;grammars;automated database testing;program testing;aggregates;database systems;constraint handling;relational databases;dbms;systematic black box testing;constraint solver;database management system;relational constraint solver;systematic testing;adusa;alloy constraint;black box testing	We describe an automated approach for systematic black-box testing of database management systems (DBMS) using a relational constraint solver. We reduce the problem of automated database testing into generating three artifacts: (1) SQL queries for testing, (2) meaningful input data to populate test databases, and (3) expected results of executing the queries on the generated data. We leverage our previous work on ADUSA and the Automated SQL Query Generator to form high-quality test suites for testing DBMS engines. This paper presents a detailed description of our framework for Automated SQL Query Generation using the Alloy tool-set, and experimental results of testing database engines using our framework. We show how the main SQL grammar constraints can be solved by translating them to Alloy constraints to generate semantically and syntactically correct SQL queries. We also present experimental results of combining ADUSA and the Automated SQL Query Generator, and applying our framework to test the Oracle 11g database. Our framework generated 5 new queries, which reveal erroneous behavior of Oracle 11g.	alloy (specification language);black box;black-box testing;constraint programming;database testing;population;sql;select (sql);software bug;solver	Shadi Abdul Khalek;Sarfraz Khurshid	2011	2011 Fourth IEEE International Conference on Software Testing, Verification and Validation	10.1109/ICST.2011.21	sargable;data definition language;pl/sql;sql;nested set model;stored procedure;white-box testing;relational database;computer science;query by example;in-memory processing;database model;data mining;grammar;database;software testing;conjunctive query;language integrated query;programming language;view;constraint satisfaction problem;alias;null;spatial query	SE	-30.107782917627247	16.050852754823282	145705
a337a86c43e76f7e2d58aec6305ea73d44321d6f	decomposing relationship types by pivoting and schema equivalence	schema transformation;relational data model;decomposition;pivoting;functional dependency;data model;semantic data model;constraint preservation;cardinality constraints;schema equivalence	In the relational data model, the problem of data redundancy has been successfully tackled via decomposition. In advanced data models, decomposition by pivoting provides a similar concept. Pivoting has been introduced by Biskup, Menzel and Polle, and used for decomposing relationship types according to a unary non-key functional dependency. Our objective is to study pivoting in the presence of cardinality constraints which are commonly used in semantic data models. For this, we generalize the notion of pivoting such that decomposing relationship types does no longer require the existence of a given unary functional dependency. In order to ensure the equivalence of the given schema and its image under pivoting, the original application-dependent constraints have to be preserved. We discuss this problem for sets of participation and co-occurrence constraints. In particular, we prove the necessity of path cardinality constraints, and give an appropriate foundation for this concept.	pivot table;turing completeness;xml schema	Sven Hartmann	2001	Data Knowl. Eng.	10.1016/S0169-023X(01)00031-3	semantic data model;relational model;data model;computer science;database;functional dependency;decomposition;algorithm	DB	-27.128638863204028	9.798979890378122	146026
36273d8cc39be159f6d5a2a113e48c64e97ec3f5	gac via unit propagation	generalized arc consistency	In this paper we argue that an attractive and potentially very general way of achieving generalized arc consistency (GAC) on a constraint is by using unit propagation (UP) over a CNF encoding of the constraint. This approach to GAC offers a number of advantages over traditional constraint specific algorithms (propagators): it is easier to implement, it automatically provides incrementality and decrementality in a backtracking context, and it can provide clausal reasons to support learning and non-chronological backtracking. Although UP on standard CNF encodings of a constraint fails to achieve GAC, we show here that alternate CNF encodings can be used on which UP does achieve GAC. We provide a generic encoding applicable to any constraint. We also give structure specific encodings for the regular, among, and gen-sequence constraints on which UP can achieve GAC with the same run time bounds as previously presented propagators. Finally, we explain how a UP engine can be added to a CSP solver to achieve a seamless integration of constraints encoded in CNF and propagated via UP and those propagated via traditional constraint specific propagators.	algorithm;backtracking;conjunctive normal form;context-free grammar;context-free language;dynamic programming;encode;inferential theory of learning;local consistency;parsing;production (computer science);propagator;regular language;run time (program lifecycle phase);schema (genetic algorithms);seamless3d;software propagation;solver;time complexity;unit propagation	Fahiem Bacchus	2007		10.1007/978-3-540-74970-7_12	mathematical optimization;discrete mathematics;computer science;mathematics;algorithm	AI	-20.50038831747901	16.294658591024437	146130
12b70c9c3db46d0d97e4e59cbe8be24f0884cf6d	recent advances in unification for the el family		Unification in Description Logics (DLs) has been proposed as an inference service that can, for example, be used to detect redundancies in ontologies. For the DL EL, which is used to define several large biomedical ontologies, unification is NP-complete. Several algorithms that solve unification in EL have previously been presented. In this paper, we summarize recent extensions of these algorithms that can deal with general concept inclusion axioms (GCIs), role hierarchies (H), and transitive roles (R). For the algorithms to be complete, however, the ontology consisting of the GCIs and role axioms needs to satisfy a certain cycle restriction.	algorithm;description logic;han unification;np-completeness;ontology (information science);unification (computer science)	Franz Baader;Stefan Borgwardt;Barbara Morawska	2012			evolutionary biology;political science;unification	AI	-20.3319454782993	8.701611871913912	146268
440af66187334b486952b09a468ff9a000c1f0c2	querying historical data in ibm db2 c/s dbms using recursive sql	querying historical data;ibm db2 c;recursive sql	Many applications handle time-varying data. In this paper, we discuss how existing features of IBM DB2 for AIX Version 2 can be used to support this type of applications efficiently, in terms of both storage and query evaluation1. We discuss several temporal data models, the tradeoffs among these models, and how one can formulate so-called “as of” or “time-slice” queries in these models. We also show how “time-coalesce” operation can be expressed directly in SQL. In all these features, the recursive SQL capability supported by DB2 plays a crucial role. While DB2 lacks many temporal features proposed in the literature, we show to what extent existing features of a commercial DBMS, such as DB2, can be used to provide some advanced supports for time-varying data.	recursion;sql	T. Y. Cliff Leung;Hamid Pirahesh	1995			computer science;data mining;database;programming language	DB	-30.502054583485652	6.86241598987563	146354
d510023f25801109417462a11510e5d21226cfd7	formal transformation from fuzzy object-oriented databases to fuzzy xml	schema;mapping;object oriented databases;fuzzy xml	XML has become the standard for publishing and exchanging data on the Web. Since most of the business data nowadays are stored in structured databases including relational and object-oriented databases (OODB), it is of significance to automate the transformation process and generate the XML data containing information from existing databases. At the same time, information imprecision and uncertainty exist in many practical applications, and for this reason, fuzzy data modeling has been extensively investigated in various data models. As such, there is an increasing need to effectively publish fuzzy structured data as fuzzy XML documents for Web-based applications. In this paper, we take a significant step in a fundamental consolidation of fuzzy XML. In particular, we are interested in finding an XML schema that best describes the existing fuzzy object-oriented schema. To accomplish this, we first offer mapping formalisms to capture the semantics of fuzzy XML Schema and fuzzy object-oriented schema. To allow for better and platform independent sharing of data stored in an object-oriented format, we investigate the formal transformation from fuzzy OODB to fuzzy XML and develop a set of rules to assist in the transformation process.	data model;data modeling;database;fuzzy logic;prototype;semiconductor consolidation;world wide web;xml schema	Jian Liu;Zongmin Ma	2013	Applied Intelligence	10.1007/s10489-013-0438-4	data exchange;xml validation;xml encryption;xml;xml schema;streaming xml;computer science;document structure description;xml framework;data mining;schema;xml database;xml schema;database;xml signature;xml schema editor;information retrieval;efficient xml interchange	DB	-33.545412450403035	10.139826687405899	146414
063dc634115273782916365ae110d9959f2f7451	a proposal for syntactic data integration math protocols	data integrity;functional programming;automatic program transformation;partial evaluation	The problem of providing connectivity for a collection of applications is largely one of data integration: the communicating parties must agree on the syntax and semantics of the data being exchanged. The Multi Protocol (MP) is an ongoing effort whose goaf is to solve this problem in the context of exchanging mathematical data. In earlier papers [5, 16], it was shown that dictionaries of definitions for operators, functions, and symbolic constants can effectively solve the problem of semantic data integration. In this paper we extend that earlier work and address the problem of syntactic data integration. A set of solutions is proposed that is both general, supporting a wide ramge of data objects with typing information, and efficient, supporting fast transmission and parsing.	dictionary;parsing	Olaf Bachmann;Hans Schönemann;Simon Gray	1997		10.1145/266670.266724	computer science;data integrity;database;programming language;functional programming;partial evaluation;algorithm	DB	-20.94608285189355	16.608346554249334	146472
f2162f76f215d96c6e7725b90c2dd7e097dff416	an extensible kernel object management system	database system;data model;layered architecture;object model;management system;storage system;satisfiability;semantic model;system architecture	Traditional monolithic database system architectures have been found to be inadequate for supporting end-user data modeling paradigms needed in complex application domains. Instead, database system architectures which are open, multi-layered, and extensible must be developed. In this paper, we explore the concept of a multilayer, extensible architecture for building new generation knowledge/data base systems. A Kernel Object Management System (KOMS) which serves as an intermediate layer in a multi-layer architecture is described. The aim is to build a generalized and extensible system which can be (a)upwardly extended into a high-level, end-user, semantic model and (b)downwardly extended to interface with various storage systems. To satisfy the generic or kernel requirement, the system uses an extensible Kernel Object Model (XKOM) which consists of a set of generalized or core object modeling constructs. Model Extensibility is facilitated by reflexively modeling the constructs of XKOM as objects, classes and associations. An object-based specification and implementation of the system architecture is used to achieve System Extensibility. By modifying and extending these model and system schemata, the model and system can be tailored or customized to suit various application domains. Various implementation issues and techniques related to the development of KOMS are also described. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. @ 1991 ACM 89791-446-5/9110010/0247...$1.50	application domain;data modeling;database;extensibility;high- and low-level;kernel (operating system);layer (electronics);management system;object-based language;systems architecture	Rahim Yaseen;Stanley Y. W. Su;Herman Lam	1991		10.1145/117954.117973	semantic data model;object model;semi-structured model;data model;system model;computer science;theoretical computer science;multitier architecture;management system;database;programming language;database design;systems architecture;satisfiability	DB	-33.11276239042916	13.084477627212022	146665
3a305df087d6682547e803df2bdf2011edf79ba0	using inductive learning to generate rules for semantic query optimization	user needs;learning;reduction;cost reduction;semantics;data bases;interrogation;inductive learning;algorithms;optimization;semantic query optimization;database query;problem solving	Semantic query optimization can dramatically speed up database query answering by knowledge intensive reformulation. But the problem of how to learn the required semantic rules has not been previously solved. This chapter presents a learning approach to solving this problem. In our approach, the learning is triggered by user queries. Then the system uses an inductive learning algorithm to generate semantic rules. This inductive learning algorithm can automatically select useful join paths and attributes to construct rules from a database with many relations. The learned semantic rules are effective for optimization because they will match query patterns and reflect data regularities. Experimental results show that this approach learns sufficient rules for optimization that produces a substantial cost reduction.	algorithm;database;inductive reasoning;mathematical optimization;program optimization;query optimization;semantic query	Chun-Nan Hsu;Craig A. Knoblock	1996			multi-task learning;query optimization;reduction;computer science;machine learning;data mining;database;semantics	DB	-25.214669197090874	7.011724314080131	146803
5148e3f3c42b2ff1167dee286e01486ff8dbce4a	specifying admissibility of dynamic database behaviour using temporal logic	temporal logic	This work uses temporal logic as a calculus for expressing integrity constraints that specify admissibility of dynamic database behaviour. Formulas are interpreted in state sequences representing dynamic behaviour. Our approach incorporates temporal quantification by 11 always 11 , 11 sometime 11 , and quanti-fiers bounded by intervals in state sequences. Moreover, dynamically changing domains of database values are considered. We then use special kinds of formulas as a language for dyna-rnie constraints and give some hints how to specify in typical situations. For such formulas, a frame for monitoring constraints during runtime of a database is discussed which allows to characterize admissibility operationally.	admissible heuristic;data integrity;temporal logic	Udo W. Lipeck;Hans-Dieter Ehrich;Martin Gogolla	1985			multimodal logic;interval temporal logic;linear temporal logic;theoretical computer science;temporal logic;computation tree logic;temporal logic of actions;computer science	DB	-23.21949090802979	15.831101471306223	147049
949b10ea53d0920a2cf0304eccf99b314e1339ce	when conceptual model meets grammar: a dual approach to xml data modeling	conceptual to xml schema transformation;xml schema;conceptual modeling;conceptual model;software engineering;data model;model driven development;expressive power;conceptual schema;regular tree grammars;reverse engineering	Article history: Received 8 December 2010 Received in revised form 7 September 2011 Accepted 9 September 2011 Available online 22 September 2011 In this paper we introduce a novel approach to conceptual modeling for XML schemas. Compared to other approaches, it allows for modeling of a whole family of XML schemas related to a particular application domain. It is integrated in a well-established way of softwareengineering, namely Model-Driven Development (MDD). It allows software-engineers to naturally model their application domain using a conceptual schema at the platform-independent level of the MDD hierarchy. From there they can design the desired XML schemas in a form of conceptual schemas at the platform-specific level of MDD hierarchy. Schemas at the platformspecific level are then automatically translated to particular XML schemas. Beside this forwardengineering direction, reverse-engineering direction integrating existing XML schemas into the MDD hierarchy is supported as well. We provide several theoretical results which ensure correctness of the introduced approach. We exploit regular tree grammars to formalize XML schemas. We formalize the bindings between the schemas at the two MDD levels and between schemas at the platform-specific level and XML schemas. We prove that conceptual schemas specify the target XML schemas unambiguously. We also prove the expressive power of the conceptual schemas. And, finally, we prove correctness of the introduced translation algorithms between platform-specific and XML schema levels. © 2011 Elsevier B.V. All rights reserved.	algorithm;application domain;conceptual schema;correctness (computer science);data integrity;data modeling;expressive power (computer science);information system;interpretation (logic);language binding;level of detail;mathematical optimization;model-driven engineering;model-driven integration;platform-independent model;platform-specific model;regular tree grammar;reverse engineering;schema evolution;semantics (computer science);xml namespace;xml schema	Martin Necaský;Irena Holubová;Jakub Klímek;Jakub Malý	2012	Data Knowl. Eng.	10.1016/j.datak.2011.09.002	natural language processing;conceptual graph;xml validation;relax ng;xml schema;computer science;conceptual model;document structure description;xml framework;xml schema;database;xml signature;programming language;xml schema editor	DB	-31.876452899733952	13.568109581422851	147133
00831387e546e70c2acb5fff3e8aaef66988fda3	cohesive keyword search on tree data.		Keyword search is the most popular querying technique on semistructured data. Keyword queries are simple and convenient. However, as a consequence of their imprecision, there is usually a huge number of candidate results of which only very few match the user’s intent. Unfortunately, the existing semantics for keyword queries are ad-hoc and they generally fail to “guess” the user intent. Therefore, the quality of their answers is poor and the existing algorithms do not scale satisfactorily. In this paper, we introduce the novel concept of cohesive keyword queries for tree data. Intuitively, a cohesiveness relationship on keywords indicates that they should form a cohesive whole in a query result. Cohesive keyword queries allow term nesting and keyword repetition. Cohesive keyword queries bridge the gap between flat keyword queries and structured queries. Although more expressive, they are as simple as flat keyword queries and not require any schema knowledge. We provide formal semantics for cohesive keyword queries and rank query results on the proximity of the keyword instances. We design a stack based algorithm which efficiently evaluates cohesive keyword queries. Our experiments demonstrate that our approach outperforms in quality previous filtering semantics and our algorithm scales smoothly on queries of even 20 keywords on large datasets.	algorithm;experiment;group cohesiveness;hoc (programming language);information needs;semantics (computer science);smoothing	Aggeliki Dimitriou;Ananya Dass;Dimitri Theodoratos;Yannis Vassiliou	2016		10.5441/002/edbt.2016.15	keyword density	DB	-32.87063313104114	4.6022150882362025	147387
7e162a5c4982ef915cdf6c1a0f72371283861c87	efficiency increase of fuzzy query languages by using indexes for selected operations	query language;management system;fuzzy query language;fuzzy equijoin;fuzzy data;sorting;iron;database languages fuzzy sets indexing database systems upper bound distributed computing fuzzy systems indexes project management multidimensional systems;fuzzy database management systems;fuzzy equality indicator efficiency increase fuzzy query language selected operation relatively weak performance fuzzy database management systems fuzzy data management fuzzy equijoin fuzzy projection;data mining;fuzzy set theory;query languages;selected operation;relatively weak performance;indexing;fuzzy projection;efficiency increase;indexation;fuzzy equality indicator;query languages fuzzy set theory;database management system;fuzzy database;database languages;fuzzy data management;algorithm design and analysis	The relatively weak performance of fuzzy database management systems is a major reason for the clear lack of commercial variants. A very important component of a database management system is the query language whose performance is improved amongst others by using indexes. Consequently, the applicability of indexes, designed for the management of fuzzy data, for several operations of a fuzzy query language, in particular a fuzzy equi-join and a fuzzy projection, is examined in this paper. In doing so, the advantages achieved with the integration of a specific index are compared with the benefits resulting from the utilization of a fuzzy equality indicator which is another approach for the efficiency increase of fuzzy query languages. The results will demonstrate that the use of an index can definitely enhance the performance of a fuzzy query language.	algorithm;fuzzy logic;fuzzy set operations;precondition;query language;relational database	Marcel Shirvanian;Wolfram-Manfred Lippe	2009	2009 IEEE International Conference on Fuzzy Systems	10.1109/FUZZY.2009.5277125	query optimization;defuzzification;adaptive neuro fuzzy inference system;fuzzy transportation;type-2 fuzzy sets and systems;fuzzy classification;computer science;fuzzy number;theoretical computer science;data mining;database;fuzzy associative matrix;fuzzy set operations;fuzzy control language;query language	DB	-28.465235330380665	7.398116830931035	147458
9eae1185d7a79fb3efcccf1ca1f15d0f6b74f45f	causality and temporal dependencies in the design of fault management systems		Reasoning about causes and effects naturally arises in the engineering of safety-critical systems. A classical example is Fault Tree Analysis, a deductive technique used for system safety assessment, whereby an undesired state is reduced to the set of its immediate causes. The design of fault management systems also requires reasoning on causality relationships. In particular, a fail-operational system needs to ensure timely detection and identification of faults, i.e. recognize the occurrence of run-time faults through their observable effects on the system. Even more complex scenarios arise when multiple faults are involved and may interact in subtle ways. In this work, we propose a formal approach to fault management for complex systems. We first introduce the notions of fault tree and minimal cut sets. We then present a formal framework for the specification and analysis of diagnosability, and for the design of fault detection and identification (FDI) components. Finally, we review recent advances in fault propagation analysis, based on the Timed Failure Propagation Graphs (TFPG) formalism.	causality;complex systems;fault detection and isolation;fault tree analysis;management system;max-flow min-cut theorem;observable;operational system;semantics (computer science);software propagation;system safety	Marco Bozzano	2017		10.4204/EPTCS.259.4	complex system;fault tree analysis;stuck-at fault;system safety;theoretical computer science;causality;fault detection and isolation;fault management;fault model;reliability engineering;computer science	SE	-24.129640581859995	15.08696435624803	147507
f9c34f596a88a437377185f53dff5af02d3f6af8	a progressive transmission scheme for vector maps in low-bandwidth environments based on device rendering	developpement logiciel;largeur bande;metodo vectorial;informatique mobile;effet echelle;shared memory;systeme information geographique;scale effect;red www;geographic information system;memoria compartida;conceptual analysis;reseau web;mobile computer;analisis conceptual;effet dimensionnel;spatial database;internet;progressive transmission;desarrollo logicial;size effect;efecto escala;base donnee spatiale;vector method;anchura banda;software development;bandwidth;world wide web;methode vectorielle;web gis;base dato especial;efecto dimensional;analyse conceptuelle;mobile computing;geospatial data;sistema informacion geografica;memoire partagee;scale dependence	The Internet has created an interesting environment for geospatial data sharing, so that users may transfer, visualize, manipulate and interact the data sets. This environment not only provides new opportunities to geospatial data usage, but also introduces new problems that should be addressed in order to provide an efficient and effective use of these datasets. One of such problems is related to the use of these spatial datasets in a low-bandwidth environment, such as those involving mobile computing. This paper presents a progressive transmission method for vector maps on the Web. The proposed method anticipates the map rasterization process to be performed at the server side, so that the amount of information transmitted may be reduced. We combine scaledependent transmission techniques with simplification and progressive ones, in order to maximize the overall performance of a Web GIS environment.	discretization;display resolution;experiment;geographic information system;internet;level of detail;map;mobile computing;online and offline;rasterisation;response time (technology);server (computing);server-side;vector graphics;web mapping;web service	David Cavassana Costa;Anselmo Cardoso de Paiva;Mário Meireles Teixeira;Cláudio de Souza Baptista;Elvis Rodrigues da Silva	2006		10.1007/11908883_18	shared memory;the internet;telecommunications;computer science;software development;operating system;mobile computing;spatial database;bandwidth	Visualization	-29.549947276253167	16.752238345679284	147601
7238976ef7450263deba5c473d53dbd6394ed0f1	datalog rewritings of regular path queries using views		We consider query answering using views on graph databases, i.e. databases structured as edge-labeled graphs. We mainly consider views and queries specified by Regular Path Queries (RPQ). These are queries selecting pairs of nodes in a graph database that are connected via a path whose sequence of edge labels belongs to some regular language. We say that a view V determines a query Q if for all graph databases D, the view image V(D) always contains enough information to answer Q on D. In other words, there is a well defined function from V(D) to Q(D). Our main result shows that when this function is monotone, there exists a rewriting of Q as a Datalog query over the view instance V(D). In particular the rewriting query can be evaluated in time polynomial in the size of V(D). Moreover this implies that it is decidable whether an RPQ query can be rewritten in Datalog using RPQ views.	continuation;datalog;graph database;polynomial;query language;regular expression;regular language;rewriting;transitive closure;monotone	Nadime Francis;Luc Segoufin;Cristina Sirangelo	2014	Logical Methods in Computer Science	10.2168/LMCS-11(4:14)2015	discrete mathematics;theoretical computer science;database;mathematics	DB	-24.29166833367199	10.764580552673328	147728
cda765ca96c26e755457955f64795e3f4c3faf07	location-based grid-index for spatial query processing	spatial index;moving objects;mobile computing	Researchers have studied the various index structures and query processing algorithms to support efficient spatial queries in wireless communication environments. However, most previous studies have focused on server-side methods. In a server-side method, the server executes queries from individual clients and then relays the results. In our proposed method, the server transmits an index called DGI (Distributed Grid Index), and the client examines the received index to process spatial queries. The proposed index structure and search algorithm support efficient spatial queries in a wireless broadcast environment, shortening query search times.		Kwangjin Park	2014	Expert Syst. Appl.	10.1016/j.eswa.2013.08.027	computer science;data mining;database;mobile computing;world wide web;spatial database;spatial query	DB	-30.994564263622724	16.863129081674987	147809
e755e15ac7ccc8292e09ee6e96b30b4a4362378c	probabilistic programs: contextuality and relational database theory		[6] have introduced a contextual probability theory called Contextuality-by-Default (C-b-D) which is based on three principles. The first of these principles states that each random variable should be automatically labeled by all condition under which it is recorded. The aim of this article is to relate this principle to block structured computer programming languages where variables are declared local to a construct called a “scope”. In this way a variable declared in two scopes can be safely overloaded meaning that they can have the same label but preserve two distinct identities without the need for the modeller to label each variable in each condition as advocated by C-b-D. A core issue addressed is how to construct a single probabilistic model from the various interim probability distributions returned by each syntactic scope. For this purpose, a probabilistic variant of the natural join operator of relational algebra is used to “glue” together interim distributions into a single distribution. The semantics of this join operator are related to contextual semantics [1].	database theory;randomized algorithm;relational database	Peter Bruza;Samson Abramsky	2016		10.1007/978-3-319-52289-0_13	database theory;relational model;relational theory;relational calculus;probabilistic database	Logic	-24.12498753505788	12.602360626386096	147925
b6059f5464b8c03708a06b1aa3007d6825bbc9d0	towards a robust query optimizer: a principled and practical approach	metadata;information retrieval;query optimization;multi dimensional;semistructured data;navigation;searching browsing	Research on query optimization has focused almost exclusively on reducing query execution time, while important qualities such as consistency and predictability have largely been ignored, even though most database users consider these qualities to be at least as important as raw performance. In this paper, we explore how the query optimization process can be made more robust, focusing on the important subproblem of cardinality estimation. The robust cardinality estimation technique that we propose allows for a user- or application-specified trade-off between performance and predictability, and it captures multi-dimensional correlations while remaining space- and time-efficient.	mathematical optimization;query optimization;run time (program lifecycle phase);selectivity (electronic)	Brian Babcock;Surajit Chaudhuri	2005		10.1145/1066157.1066172	sargable;query optimization;navigation;query expansion;web query classification;computer science;data mining;database;web search query;metadata;information retrieval;query language	DB	-25.15952310248974	4.410795868115579	147948
a2917a9bee434d806df78a31380cc7ee80de2635	a parameterized algorithm for exploring concept lattices	ordered set;association rule;concept lattice;parameterized algorithm;formal concept analysis	Formal Concept Analysis (FCA) is a natural framework for learning from positive and negative examples. Indeed, learning from examples results in sets of frequent concepts whose extent contains only these examples. In terms of association rules, the above learning strategy can be seen as searching the premises of exact rules where the consequence is fixed. In its most classical setting, FCA considers attributes as a non-ordered set. When attributes of the context are ordered, Conceptual Scaling allows the related taxonomy to be taken into account by producing a context completed with all attributes deduced from the taxonomy. The drawback, however, is that concept intents contain redundant information. In this article, we propose a parameterized generalization of a previously proposed algorithm, in order to learn rules in the presence of a taxonomy. The taxonomy is taken into account during the computation so as to remove all redundancies from intents. Simply changing one component, this parameterized algorithm can compute various kinds of concept-based rules. We present instantiations of the parameterized algorithm for learning positive and negative rules.	2.5d;algorithm;association rule learning;computation;experiment;formal concept analysis;redundancy (engineering);taxonomy (general)	Peggy Cellier;Sébastien Ferré;Olivier Ridoux;Mireille Ducassé	2007		10.1007/978-3-540-70901-5_8	discrete mathematics;association rule learning;computer science;formal concept analysis;machine learning;data mining;mathematics;algorithm	ML	-20.849814920708265	5.431894831496625	148187
ca67d08b515a9b1d6d55d1a40e4bc8c9a402a394	physical schemas for large multidimensional arrays in scientific computing applications	libraries;scientific application;scientific computing applications;application software;high performance computing;database management systems;physical storage large multidimensional arrays scientific computing applications physical schemas i o library abstract interface;database management systems data structures;scattering;multidimensional systems scientific computing database systems high performance computing workstations application software libraries nasa scattering computer science;i o library;abstract interface;physical schemas;data structures;workstations;database systems;scientific computing;physical storage;computer science;large multidimensional arrays;nasa;multidimensional systems	We describe physical schemas for storing multidimensional arrays on disk. We have developed an i/o library supporting these schemas that provides an abstract interface shielding scientific application developers from physical storage details. Our library has resulted in simplified programming and improved i/o performance in the applications we have studied. >	computational science	Kent E. Seamons;Marianne Winslett	1994		10.1109/SSDM.1994.336945	application software;workstation;data structure;multidimensional systems;computer science;data science;theoretical computer science;database;scattering	HPC	-32.253079194387404	16.56008791652501	148323
e914c4d9496523b6acc202c0a8f6544afd42339e	learning multiple predicates	representacion conocimientos;systeme intelligent;learning algorithm;sistema inteligente;inductive logic programming;algorithme apprentissage;logical programming;induccion;induction;programmation logique;intelligent system;information system;logic programs;knowledge representation;representation connaissances;algoritmo aprendizaje;programacion logica;systeme information;sistema informacion	We present an approach for solving some of the problems of top-down Inductive Logic Programming systems when learning multiple predicates. The approach is based on an algorithm for learning abduc-tive logic programs. Abduction is used to generate additional information that is useful for solving the problem of global inconsistency when learning multiple predicates.	algorithm;inductive logic programming;inductive reasoning;top-down and bottom-up design	Antonis C. Kakas;Evelina Lamma;Fabrizio Riguzzi	1998		10.1007/BFb0057454	knowledge representation and reasoning;computer science;artificial intelligence;machine learning;mathematics;information system;algorithm;abductive logic programming	AI	-20.173776788984753	11.116818222139043	148435
75d67ab66f19120dbb80f2b2442d20938233caad	automatic importation of relational schemas in pegasus	object oriented data model;multidatabase system;object oriented data models relational schemas pegasus external relational schema multidatabase system data model functional object model specialization object identity nested aggregation import algorithm primary keys referential integrity constraints;data model;integrity constraints;distributed databases;relational databases distributed databases;relational databases;data models object oriented databases information retrieval relational databases laboratories database systems multimedia databases milling machines object oriented modeling prototypes;object model	This paper describes a technique for importing automatically an external relational schema into Pegasus, a heterogeneous, multidatabase system. The Pegasus data model is a functional object model which supports specialization, object-identity, and nested aggregation. The import algorithm generates the definition of a Pegasus schema which captures the semantics of a relational schema containing information about primary keys and referential integrity constraints. The algorithm described can be adapted easily for other semantic and objectoriented data models.	algorithm;data integrity;data model;database schema;partial template specialization;pegasus;referential integrity;unique key	Joseph Albert;Rafi Ahmed;Mohammad A. Ketabchi;William Kent;Ming-Chien Shan	1993		10.1109/RIDE.1993.281938	information schema;nested set model;relational model;relational calculus;semi-structured model;data model;relational database;computer science;theoretical computer science;database model;data mining;database;candidate key;database design	DB	-31.525406953824188	10.52038531960945	148588
09a358ae50e509dd6952b7ab0b65d0e08b3efd87	the l* data language	codasyl report;data base;relations;sets;sigfidet;high level language;data structure;sigplan	L* is a high-level language for defining and manipulating data structures and data bases. Its formalism derives from the concepts of sets and relations on sets. Unlike some recent proposals L* is a complete language in itself containing procedural facilities and does not therefore rely on being embedded in some host. Preliminary studies have shown it to be well suited to its field of application.	data structure;database;embedded system;high- and low-level;high-level programming language;semantics (computer science)	J. C. Gray;J. C. Tomlinson	1973		10.1145/800192.805699	natural language processing;data manipulation language;computer science;data mining;algorithm	AI	-28.80376416198683	12.63691902261875	148826
4c2d77b10b69800686cf2efa290d2eed9e72b2f9	a global perspective of schema modification management for object-oriented databases	schema modification management;object-oriented databases;global perspective;change management	Schema Modification Management (SMM) is concerned about how schema changes should affect the object base in order to make database objects be compatible with specifications after change. However, a particular problem with existing SMM approaches is the lack of concern for schema-level changes (which may involve multiple classes), or semantic dependencies which applies between non-consecutive versions of the schema. The presented approach adopts a schema versioning approach to SMM (allowing multiple schema versions to coexist), and promotes a global perspective of change management: A powerful means is provided for specifying the presence and maintenance of arbitrary semantic relationships between classes and properties as defined for any schema version in the derivation hierarchy. The fundamental assumption is that semantic dependencies between schema versions do not only follows the derivation relationship, but may go in any directions. To ensure the database behaves consistently as regarded within any schema version context, it is essential that all semantic dependencies are maintained. The approach is able to handle schema version derivations in arbitrary directions, including complex modifications to the class hierarchy, and in accordance with the inherently unpredictable nature of change.	acid;class hierarchy;coexist (image);complexity;database schema;prototype;specification language	Erik Odberg	1994			schema migration;information schema;logical schema;knowledge management;three schema approach;conceptual schema;data mining;database;database schema	DB	-33.625800301395564	11.624087382208579	148839
d106ffdd89a27bca5edfbd7a3ff802b0a355f37f	xml type checking with macro tree transducers	anomaly;query language;snapshot isolation;theoretical framework;top down;two phase locking;type checking;pattern matching;concurrency control;serializability;theoretical foundation;type inference;consistency	"""MSO logic on unranked trees has been identified as a convenient theoretical framework for reasoning about expressiveness and implementations of practical XML query languages. As a corresponding theoretical foundation of XML transformation languages, the """"transformation language"""" TL is proposed. This language is based on the """"document transformation language"""" DTL of Maneth and Neven which incorporates full MSO pattern matching, arbitrary navigation in the input tree using also MSO patterns, and named procedures. The new language generalizes DTL by additionally allowing procedures to accumulate intermediate results in parameters. It is proved that TL -- and thus in particular DTL - despite their expressiveness still allow for effective inverse type inference. This result is obtained by means of a translation of TL programs into compositions of top-down finite state tree transductions with parameters, also called (stay) macro tree transducers."""	diode–transistor logic;hartmut neven;pattern matching;query language;top-down and bottom-up design;transducer;transform, clipping, and lighting;transformation language;type inference;type system;xml	Sebastian Maneth;Alexandru Berlea;Thomas Perst;Helmut Seidl	2005		10.1145/1065167.1065203	anomaly;computer science;two-phase locking;theoretical computer science;type inference;concurrency control;pattern matching;top-down and bottom-up design;database;consistency;programming language;serializability;algorithm;query language;snapshot isolation	DB	-23.96352828633996	13.403973034334731	148952
6df56b02149b7e0f8279f9fe1a02817ed672b929	extensible index technique for storing and retrieving xml documents	content management;containment relationship query extensible index technique document storage document retrieval xml documents coordinate based methods update operation structural relations;document handling;xml computer science degradation sgml web sites internet information retrieval content based retrieval databases costs;index structure;information storage;performance improvement;indexing;content management content based retrieval information storage document handling xml indexing;indexation;xml;xml document;content based retrieval	Because most researches that were studied so far on XML documents used a method to express coordinate-based methods in most of the index techniques, the update operation makes a large burden. Due to the expression of structural relations between elements, attributes and texts, we must reconstruct the structure of coordinate. As the reconstruction process proceeds in the index structure overall the XML document without limitation of the number of node in a cascade manner, it is caused a serious performance problem by the frequent update operations. In this paper, we propose an index technique based on an extensible index that does not cause serious performance degradations. It can limit the number of node to participate in reconstruction process and improve lots of performance capacities on the whole. Extensible index with deferred update shows additional performance improvement specially. In addition, the extensible index technique contains the advantage to perform efficiently the containment relationship query through the simple expression, therefore it can maximize the performance of the system.	monomial;xml	Jungsuk Song;Woosaeng Kim	2004	The Fourth International Conference onComputer and Information Technology, 2004. CIT '04.	10.1109/CIT.2004.1357209	well-formed document;xml catalog;xml validation;xml encryption;simple api for xml;xml;processing instruction;xml schema;streaming xml;computer science;document type definition;document structure description;xml framework;xml database;xml schema;database;xml signature;world wide web;xml schema editor;information retrieval;efficient xml interchange	DB	-30.24188452898058	5.009948288204539	148964
5feaafaa4103a751dc39b1be389a56004536c218	conceptual level design of semi-structured database system: graph-semantic based approach	xml document;rule based;conceptual model;xml schema;semi structured data;database system	This paper has proposed a Graph – semantic based conceptual model for semi-structured database system, called GOOSSDM, to conceptualize the different facets of such system in object oriented paradigm. The model defines a set of graph based formal constructs, variety of relationship types with participation constraints and rich set of graphical notations to specify the conceptual level design of semi-structured database system. The proposed design approach facilitates modeling of irregular, heterogeneous, hierarchical and nonhierarchical semi-structured data at the conceptual level. Moreover, the proposed GOOSSDM is capable to model XML document at conceptual level with the facility of document-centric design, ordering and disjunction characteristic. A rule based transformation mechanism of GOOSSDM schema into the equivalent XML Schema Definition (XSD) also has been proposed in this paper. The concepts of the proposed conceptual model have been implemented using Generic Modeling Environment (GME). KeywordsSemi-structured Data; XML; XSD; Conceptual Modeling; Semi-structured Data Modeling; XML Modeling.	algorithm;computer-aided software engineering;data model;data modeling;database schema;gme of deutscher wetterdienst;generic modeling environment;graphical model;graphical user interface;level design;metamodeling;programming paradigm;query language;referential integrity;semi-structured data;semiconductor industry;unified modeling language;xml schema	Anirban Sarkar	2011	CoRR		rule-based system;idef1x;xml validation;data modeling;conceptual model;xml;semi-structured model;xml schema;data model;system model;computer science;knowledge management;three schema approach;conceptual schema;conceptual model;document structure description;conceptual system;domain model;data mining;xml schema;database;database schema;xml schema editor	DB	-32.28457120190234	12.476788143240908	148971
e9c146ec2fca406004e4d88fa53da0eabfda2008	about selections and joins in possibilistic queries addressed to possibilistic databases	information retrieval;informacion incompleta;logique floue;interrogation base donnee;interrogacion base datos;logica difusa;pregunta documental;stockage donnee;imperfect information;question documentaire;fuzzy logic;incomplete information;data storage;recherche information;information incomplete;query;informacion imperfecta;almacenamiento datos;recuperacion informacion;fuzzy database;database query;information imparfaite	"""This paper is situated in the area of the so-called fuzzy databases, i.e., databases containing imprecise information. It is now recognized that querying databases containing imperfect information raises several problems, including complexity. In this paper, we consider a specific kind of queries, called possibilistic queries, of the form """"to what extent is it possible that a given tuple t belongs to the answer of Q (a regular relational query)."""" The major objective is to show that a reasonable complexity can be expected for a specific (although not too restricted) subset of possibilistic queries."""	database	Patrick Bosc;Laurence Duval;Olivier Pivert	2002		10.1007/3-540-46146-9_59	fuzzy logic;computer science;artificial intelligence;perfect information;computer data storage;data mining;database;complete information;information retrieval	DB	-26.064767393676316	7.172570891326799	148973
367608997fec0e9ac3e5fd4651bdb2715ab1e51b	omt-g: an object-oriented data model for geographic applications	multiple representation;software;topology;networks;base donnee;object oriented data model;systeme information geographique;resolution spatiale;spatial data;logiciel;topologie;geometry;base dato;geometrie;reseau;cartographie;data bases;data model;multiple views;topologia;geographic software design;modelo;cartografia;geographic information systems;geographic data modeling;cartography;database modeling;geometria;modele;spatial relationships;geografia;geographie;models;spatial resolution;geography	Semantic and object-oriented data models, such as ER, OMT, IFO, and others, have been extensively used for modeling geographic applications. Despite their semantic expressiveness, such models present limitations to adequately model those applications, since they do not provide appropriate primitives for representing spatial data. This paper presents OMT-G, an object oriented data model for geographic applications. OMT-G provides primitives for modeling the geometry and the topology of spatial data, supporting different topological structures, multiple views of objects, and spatial relationships. OMT-G also includes tools to specify transformation processes and presentation alternatives, that allow, among many other possibilities, modeling for multiple representations and multiple presentations. In this way, it overcomes the main limitations of the existing models, thus providing more adequate tools for modeling geographic applications. A comparison with other data models is also presented in order to stress the main advantages of OMT-G.	data model;language primitive	Karla A. V. Borges;Clodoveu A. Davis;Alberto H. F. Laender	2001	GeoInformatica	10.1023/A:1011482030093	spatial relation;data modeling;image resolution;geography;data model;computer science;theoretical computer science;data mining;database;spatial analysis;geographic information system;algorithm	DB	-30.320998174737685	12.955752313082565	148990
c4ab70f24b8b32b6cf5142a6431d1a0ab2a7e0cc	databases in telecommunications		This paper first presents a short overview of application areas for database technology in telecommunications. The main result of this brief survey is that telecommunications has recently broadened to a degree that almost all fashionable fields of database research are becoming relevant; nevertheless, relatively little research focus has been set on this application domain. In the second part of the paper, we look specifically at the performance challenges posed to distributed and wireless database technologies by novel applications such as Intelligent Networks, mobile telephony and very large numbers of database caches embedded in telephones. A brief overview of two of our own research efforts is presented, one concerning the design of replication mechanisms, the other concerning analytic performance models in distributed and wireless information systems.	application domain;database;embedded system;information system	Jan van Leeuwen;Willem Jonker	1999		10.1007/10721056	online aggregation;database search engine;intelligent database;database transaction;database tuning;computer science;remote database access;database model;data mining;database;view;world wide web;database schema;alias;database testing;database design	DB	-31.120614479696044	16.62522993775558	149230
a87234ae594594c047d124485adfa56bf6cd2760	initial results on the f-logic to owl bi-directional translation on a tabled prolog engine	functional properties;software engineering;artificial intelligent;inference rule;directional data	In this paper, we show our results on the bi-directional data exchange between the F-logic language supported by the Flora2 system and the OWL language. Most of the TBox and ABox axioms are translated preserving the semantics between the two representations, such as: proper inclusion, individual definition, functional properties, while some axioms and restrictions require a change in the semantics, such as: numbered and qualified cardinality restrictions. For the second case, we translate the OWL definite style inference rules into F-logic style constraints. We also describe a set of reasoning examples using the above translation, including the reasoning in Flora2 of a variety of ABox queries.	abox;bi-directional text;data integrity;directional statistics;f-logic;prolog;semantic web service;tbox;web ontology language	Paul Fodor	2008	CoRR		computer science;artificial intelligence;machine learning;database;mathematics;programming language;algorithm;rule of inference	AI	-21.964011295790378	9.85040884906328	149537
ed59d9c09e24f0089310093c3566ba4e3c146ef1	logical foundations of conceptual modelling using hit data model	data model		data model	Marie Duzí	2000			machine learning;mathematics;conceptual model (computer science);discrete mathematics;domain model;three schema approach;logical data model;data modeling;data model;artificial intelligence	AI	-31.249428645949322	9.825234154418785	149551
270e21af847fbe37f02b401640c1bc8da2b3178c	a reasoner for simple conceptual logic programs	open answer set programming;rule based;satisfiability;stable model semantics;logic programs;knowledge base	Open Answer Set Programming (OASP) can be seen as a framework to represent tightly integrated combined knowledge bases of ontologies and rules that are not necessarily DL-safe. The framework makes the open-domain assumption and has a rule-based syntax supporting negation under a stable model semantics. Although decidability of different fragments of OASP has been identified, reasoning and effective algorithms remained largely unexplored. In this paper, we describe an algorithm for satisfiability checking of the fragment of simple Conceptual Logic Programs and provide a BProlog implementation. To the best of our knowledge, this is the first implementation of a (fragment) of a framework that can tightly integrate ontologies and non-DL-safe rules under an expressive nonmonotonic semantics.	algorithm;answer set programming;b-prolog;logic programming;ontology (information science);semantic reasoner;stable model semantics	Stijn Heymans;Cristina Feier;Thomas Eiter	2009		10.1007/978-3-642-05082-4_5	rule-based system;knowledge base;stable model semantics;computer science;artificial intelligence;theoretical computer science;answer set programming;programming language;well-founded semantics;algorithm;satisfiability	AI	-20.33445508619822	9.420321756221265	149596
aaf08312c731878b28103eb99a8a9e98722dedfb	queries with guarded negation (full version)		A well-established and fundamental insight in database theory is that negation (also known as complementation) tends to make queries difficult to process and difficult to reason about. Many basic problems are decidable and admit practical algorithms in the case of unions of conjunctive queries, but become difficult or even undecidable when queries are allowed to contain negation. Inspired by recent results in finite model theory, we consider a restricted form of negation, guarded negation. We introduce a fragment of SQL, called GN-SQL, as well as a fragment of Datalog with stratified negation, called GN-Datalog, that allow only guarded negation, and we show that these query languages are computationally well behaved, in terms of testing query containment, query evaluation, open-world query answering, and boundedness. GN-SQL and GN-Datalog subsume a number of well known query languages and constraint languages, such as unions of conjunctive queries, monadic Datalog, and frontier-guarded tgds. In addition, an analysis of standard benchmark workloads shows that most usage of negation in SQL in practice is guarded negation.	algorithm;benchmark (computing);conjunctive query;database theory;datalog;grid north;monadic predicate calculus;open world;petri net;query language;sql server compact;undecidable problem	Vince Bárány;Balder ten Cate;Martin Otto	2012	CoRR		negation as failure;negation introduction;negation normal form;conjunctive query;programming language;algorithm	DB	-23.839743439231754	10.707014402037235	149953
ae91df6d77e770420a895aaf19124a2d7d17ef73	the theory of joins in relational data bases (extended abstract)	databases;silicon;finite element methods;relational data;complexity theory;thyristors;efficient algorithm;relational database;companies;sufficient conditions;time factors;data dependence;writing;artificial intelligence;inference algorithms;relational databases;proposals;algorithm design and analysis;data models;relational databases proposals	Answering queries in a relational database often requires that the natural join of two or more relations be computed. However, not all joins are semantically meaningful. This paper gives an efficient algorithm to determine whether the join of several relations is semantically meaningful (lossless) and an efficient algorithm to determine whether a set of relations has a subset with a lossy join. These algorithms assume that all data dependencies are functional. Similar techniques also apply to the case where data dependencies are multivalued.	database	Alfred V. Aho;Catriel Beeri;Jeffrey D. Ullman	1977		10.1109/SFCS.1977.33	hash join;recursive join;relational database;computer science;theoretical computer science;join dependency;data mining;database;sort-merge join	Theory	-27.264940217386382	8.981993379680434	150149
c9797ffed80146467e85732c5d425e656e0115dc	gnet: a generalized network model and its applications in qualitative spatial reasoning	hybrid rcc8 network;qualitative spatial reasoning;generalized network;data model;network model;binary relation;extended bellman ford algorithm;rcc8 network;knowledge base	A data model, named generalized network (GNet), is proposed to perform various network-tracing tasks, especially tracing conceptual proposition networks in qualitative spatial reasoning (QSR). The GNet model can be defined as a 6-tuple: (V,A,q,@?,~,L). By specifying each element in the 6-tuple, a GNet can function as a conventional network, or an activity on edge (AOE) network, etc. The algorithm for searching for the generalized optimum path weight (GOPW) between two vertices in a GNet is developed by extending the Bellman-Ford algorithm (EBFA). Based on the GNet model, this paper focuses on representing spatial knowledge, which consists of a set of binary relations. We present two applications of GNets, namely the RCC8 network and the hybrid RCC8 network involving cardinal direction relations. Both can be traced to infer new spatial knowledge using EBFA. The applications demonstrate that the GNet model provides a promising approach to dealing with proposition-based geospatial knowledge based on weak composition. We also point out that EBFA can check whether a network is algebraically closed, or path-consistent when the corresponding composition table is extensional.	flow network;gnet;network model;spatial–temporal reasoning	Yu Liu;Yi Zhang;Yong Gao	2008	Inf. Sci.	10.1016/j.ins.2008.01.002	mathematical optimization;knowledge base;data model;computer science;artificial intelligence;network model;machine learning;binary relation;database;mathematics;algorithm;statistics	AI	-21.793027068439162	4.565477168215129	150186
dd59d49a7852629f6449216d0fde804d04893e75	building maintainable knowledge bases with knowledge objects	maintenance;knowledge based systems;conference proceeding;knowledge base	Knowledge 'objects' are constructors for building chunks of knowledge that enable the hidden links in the knowledge to be identified. A single operation for objects enables these hidden links to be removed from the knowledge thus simplifying maintenance. Analysis of the maintenance links shows that they are of four different types. The density of the maintenance links is reduced by transforming that set into an equivalent set. In this way the knowledge base maintenance problem is analysed and simplified. A side benefit of knowledge items as a formalism is that they contain knowledge constraints that protect the knowledge from unforeseen modification.		John K. Debenham	2007		10.1007/978-3-540-74819-9_35	knowledge base;computer science;knowledge management;artificial intelligence;explicit knowledge;body of knowledge;knowledge-based systems;open knowledge base connectivity;data mining;knowledge extraction;knowledge value chain;domain knowledge	AI	-33.41678621926279	11.624148651940166	150364
78b37e52dffcca4ff486d13e6170341074919d6d	parallel processing and trusted database management systems	management system;database security;parallel databases;parallel architecture;database management system;parallel processing;multilevel security	This paper applies parallel processing technology to database security technology and vice versa. We first describe the issues involved in incorporating multilevel security into parallel database management systems. In particular, we describe how multilevel security could be incorporated into the GAMMA architecture. Then we describe the use of parallel architectures to perform trusted database management system functions. In particular, we describe security constraint processing in trusted database management systems and show how parallel processing could enhance the performance of this function.	database security;multilevel security;parallel computing;parallel database;parallel processing (dsp implementation)	Bhavani M. Thuraisingham;William R. Ford	1993		10.1145/170791.170824	parallel processing;database theory;database tuning;computer science;data mining;management system;database;distributed computing;database schema;distributed database;database testing;database design	DB	-32.26414404894861	10.846192601434165	150404
0b1c6a4d13b8c4f92aacce50411fc999a8a0c03f	maintaining behavioral consistency during schema evolution	schema transformation;object oriented;software reuse;schema evolution;language model	We examine the problem of how to ensure behavioral consistency of an object oriented system after its schema has been updated The problem is viewed from the perspective of both the strongly typed and the untyped language model Solutions are compared in both models using C and CLOS as examples	clos network;language model;programming language;schema evolution	Paul L. Bergstein;Walter L. Hürsch	1993		10.1007/3-540-57342-9_73	schema migration;logical schema;knowledge management;three schema approach;conceptual schema;document structure description;xml schema;database;document schema definition languages;programming language;database schema	SE	-31.23980787915782	10.998665138755177	150416
f5d4b1f50bbd7ff52af56a3763744616af19e52b	an efficient graphical algorithm for incremental conceptual schema evolution	conceptual schema		algorithm;conceptual schema;schema evolution	Catherine A. Ewald;Maria E. Orlowska	1994			schema migration;star schema;document structure description;database;three schema approach;database schema;data mining;conceptual schema;logical schema;computer science	Vision	-31.787776428123717	9.784540751583346	150487
30f6179f186781a072f806a08cda7f792177e0ed	qualifying objects in classical relational database querying	relational database	Database querying by various selection criteria can often confront a major limitation: the difficulty to realize and express precise criteria for locating the information. This happens because people do not always think and speak in precise terms, or they do not have details on the data range. The research community recently proposed a new way to query databases, more expressive and flexible than the classical one. It is about vague queries, for example: “retrieve the persons well paid which live not too far from the office”, of course, formulated in an adequate query language. The main reason to use the vague predicates well paid and not too far is to express more flexibly the user’s preferences and at the same time to rank the selected tuples by a degree of criteria satisfaction. When a precise criterion, like “salary > 500 and distance home-office < 200” is required, it may return an empty list, even if there are a lot of persons having attribute values very close to the specified ones. As well, the same precise criterion may return a complete list of all persons, without any helpful ordering. So, it would be useful to provide intelligent interfaces to databases, able to interpret and evaluate imprecise criteria in queries. AbstrAct	database;query language;small office/home office;vagueness	Cornelia Tudorie	2008			domain relational calculus;relational model;conjunctive query;relational database;database;relational calculus;object-relational impedance mismatch;database model;database design;computer science	DB	-26.06015662070447	9.962091921624028	150527
799a670bc74b469bcfb388ab99c77251798dcfd8	the notions of consistency and predicate locks in a database system	database system;database;satisfiability;transaction;lock;concurrancy;consistency	In database systems, users access shared data under the assumption that the data satisfies certain consistency constraints. This paper defines the concepts of transaction, consistency and schedule and shows that consistency requires that a transaction cannot request new locks after releasing a lock. Then it is argued that a transaction needs to lock a logical rather than a physical subset of the database. These subsets may be specified by predicates. An implementation of predicate locks which satisfies the consistency condition is suggested.	consistency (database systems);content-addressable memory;data model;database;database transaction;imaging phantom;lock (computer science);naruto shippuden: clash of ninja revolution 3;predicate (mathematical logic);relational model;two-phase locking;well-formed document	Kapali P. Eswaran;Jim Gray;Raymond A. Lorie;Irving L. Traiger	1976	Commun. ACM	10.1145/360363.360369	lock;database transaction;transaction processing;rollback;distributed transaction;computer science;two-phase locking;consistency model;data mining;database;consistency;programming language;data consistency;acid;consistency;atomicity;database testing;local consistency;satisfiability	DB	-26.686071331779395	12.591721954742905	150529
a1d324a036649fc02b379b58a02dc9152507008d	comparison of different access strategies in a distributed database	distributed database		distributed database	Giuseppe Pelagatti;Fabio A. Schreiber	1978			database tuning;distributed database;database server;database;database testing;spatiotemporal database;computer science	DB	-30.98916447040035	8.406356769809477	150649
d0a615cc615e498376ed007ecc431b13ad279aad	an ontological account of action in processes and plans	constraint satisfaction;process models;planning;process model;ontology;model theory	This paper formalises the constraints governing the relationship between actions and their preconditions and effects in processes and plans. By providing axiomatisations and a model theory, we establish a sound basis for both deductive and constraint satisfaction-based reasoning. The constraints we present are expressed in a common ontology of classes and relations that is the basis of process and plan representations.		J. Stuart Aitken	2005	Knowl.-Based Syst.	10.1016/j.knosys.2005.04.001	knowledge management;artificial intelligence;ontology;process modeling;data mining;process ontology	ML	-19.879425522571157	7.524031784759632	150703
d547a07e8499dd0643cc043d359f3a9aa642982b	non-recursive approach for sort-merge join operation		Several algorithms have been developed over the years to per- form join operation which is executed frequently and affects the efficiency of the database system. Some of these efforts prove that join performance mainly depends on the sequences of execution of relations in addition to the hardware architecture. In this paper, we present a method that processes a many-to-many multi join operation by using a non-recursive reverse polish notation tree for sort-merge join. Precisely, this paper sheds more light on main memory join operation of two types of sort-merge join sequences: sequential join sequences (linear tree) and general join sequences (wide bushy tree, also known as composite inner) and also tests their performance and functionality. We will also provide the algorithm of the proposed system that shows the implementation steps.	recursion;sort-merge join	Norah Asiri;Rasha Alsulim	2016		10.1007/978-3-319-34099-9_16	hash join;recursive join;real-time computing;computer science;block nested loop;join dependency;database;sort-merge join;algorithm	DB	-29.574563963574192	4.685244603739536	150742
820f238c036d20933aa78820fa90fd1584cf6c01	torus: a step towards bridging the gap between data bases and the casual user		Ahstraet-This paper describes TORUS, a natural language understanding system that serves as a front end to a data base management system in order to facilitate communication with casual users. The system employs a semantic network to store knowledge about a data base of student files. This knowledge is used to find the meaning of each input statement, to decide what action to take with respect to the data base, and to select information that must be output in response to the input statement. A prototype version of TORUS has been implemented.	bridging (networking);database;natural language understanding;prototype;semantic network	John Mylopoulos;Alexander Borgida;P. Cohen;Nick Roussopoulos;John K. Tsotsos;Harry K. T. Wong	1976	Inf. Syst.	10.1016/0306-4379(76)90009-0	simulation;computer science;artificial intelligence;theoretical computer science;operating system;data mining;database;distributed computing;programming language;computer security	DB	-32.54470435182486	8.356589014637953	150743
6ab05c5c7f1741c8baffb0f6ac970058763656c3	embedding vague spatial objects into spatial databases using the vaguegeometry abstract data type	conferenceobject	Spatial vagueness has been required by geoscientists to handle vague spatial objects, i.e., spatial objects that do not have exact locations, strict boundaries, or sharp interiors. However, there is a gap in the literature in how to handle these objects in spatial database management systems since they mainly provide support to crisp spatial objects, i.e., objects that have well-defined locations, boundaries, and interiors. In this paper, we fill this gap by proposing VagueGeometry, a novel abstract data type that handles vague spatial objects, includes an expressive set of vague spatial operations, and its implementation is open source. Experimental results show that VagueGeometry improved the performance of spatial queries with vague topological predicates from 23% up to 84% if compared with functionalities available in current spatial databases.	abstract data type;algorithm;join (sql);numerical analysis;open-source software;postgis;postgresql;sql;spatial database;spatial query;vagueness	Anderson Chaves Carniel;Ricardo Rodrigues Ciferri;Cristina Dutra de Aguiar Ciferri	2015			mathematical analysis;object-based spatial database;discrete mathematics;topology;computer science;mathematics;spatial database	DB	-27.935716993079776	8.402981362324246	150859
3c35b2364729e8faa3a00560a7a440c37f3aca68	on relational support for xml publishing: beyond sorting and tagging	performance evaluation;image database;classification;data model;relational model;middleware;cluster merging;content based image retrieval;relevance feedback	In this paper, we study whether the need for efficient XML publishing brings any new requirements for relational query engines, or if sorting query results in the relational engine and tagging them in middleware is sufficient. We observe that the mismatch between the XML data model and the relational model requires relational engines to be enhanced for efficiency. Specifically, they need to support relation valued variables. We discuss how such support can be provided through the addition of an operator, GApply, with minimal extensions to existing relational engines. We discuss how the operator may be exposed in SQL syntax and provide a comprehensive study of optimization rules that govern this operator. We report the results of a preliminary performance evaluation showing the speedup obtained through our approach and the effectiveness of our optimization rules.	data model;mathematical optimization;middleware;performance evaluation;query optimization;relational database;relational model;relational operator;requirement;sql;seamless3d;sorting;speedup;xml	Surajit Chaudhuri;Raghav Kaushik;Jeffrey F. Naughton	2003		10.1145/872757.872831	sargable;query optimization;sql;nested set model;relational model;statistical relational learning;relational calculus;entity–relationship model;biological classification;data model;relational database;computer science;database model;middleware;data mining;database;conjunctive query;information retrieval	DB	-31.901342755765118	7.868006005745673	150875
6ebe38c780fe09de966baf8cbb11814bfec839e1	incremental evaluation of natural semantics specification	natural semantics specification;incremental evaluation	Natural Semantics is a logical formalism used to specify semantic aspects of a language by sets of logical rules (called a Typol program) where a query is proved using Prolog. In a previous paper, we have shown how to replace, under certain hypotheses, the Prolog engine by a functional evaluator; this is possible because uniication is no longer required and can be replaced by pattern matching. Starting from this previous work, we now add incremental facilities to our evaluator. That is to say, after some modiication of a term whose semantic value has already been evaluated, we do not need to re-evaluate everything from scratch as it is the case with a Prolog engine.	interpreter (computing);operational semantics;pattern matching;prolog;semantics (computer science)	Isabelle Attali;Jacques Chazarain;Serge Gilette	1992		10.1007/3-540-55844-6_129	natural language processing;formal semantics;database;programming language;operational semantics;language of temporal ordering specification	PL	-21.01850020079027	16.579697024486077	150987
12c224fac1dbf5ff465eaf5ca0c72bff18466e3c	the query management facility	description systeme;system description;programming language;relational database;base donnee relationnelle;langage programmation;systeme qmf	"""Data from a relational data base can be displayed in reports, changed, and otherwise controlled using a program called Query Management Facility (QMF). An overview of this program is presented and is followed by a discussion comparing equivalent forms of various queries expressed in two distinctly different languages. Both languages are designed for use with relational data and are supported by QMF. T he Query Management Facility (QMF)'-* provides a full-screen, interactive environment to query, define, and control relational data contained in either the IBM Database 2 (~ 8 2) ~ or the Structured Query Language/Data System (SQLIDS)~ data base systems. QMF is used from a visual display terminal and is functional only when installed with either D B ~ or SQL/DS. Relational data contained in D B ~ and in SQL/DS is perceived to exist in table form as shown in Table 1. A considerable understanding of QMF can be obtained knowing no more than that about relational data; however, an understanding of some of the more complex applications of QMF requires some additional background. Other papers in this issue of the IBM Systems Journal address the D B ~ data base system and provide some of that background. G. Sand-berg's """" A primer on relational data base concepts """" ' and C. J. Date's book, An Introduction to Database Systems,6 are other good sources. QMF provides a means of retrieving and displaying data and of inserting, deleting, and updating data using either a form of the Query-by-Example (QBE) language """" lo conceived by M. M. Zloof at IBM'S Thomas J. Watson Research Center or the Struc-tured Query Language (SQL)334*'1 """" 3 developed at IBM'S San Jose Research Center. Although both QBE and SQL are designed specifically for use with relational data, they offer the QMF user a distinct choice between two languages that are very different in form and syntax. This paper provides a QMF overview and discusses the types of queries that may be created using either QBE or SQL. It also shows the equivalent QBE and SQL syntax that may be used to create such queries. See References 1 and 2 for more detailed descriptions of QMF. Every QMF session begins (and ends) with the QMF home panel shown in Figure 1. QMF items can be viewed by pressing the associated PF keys shown at the bottom of the display. Messages from QMF such as """" …"""	berg connector;computer monitor;data system;ibm query management facility;ibm sql/ds;microsoft query;primer;quadrature mirror filter;query by example;query language;relational database management system;sql;thomas j. watson research center	Joseph J. Sordi	1984	IBM Systems Journal	10.1147/sj.232.0126	relational database;computer science;theoretical computer science;database;programming language;algorithm	DB	-32.70100499033215	15.198785286835987	151090
67062905a46abfd3cbe04f85620f8bec94c891ca	algres: an advanced system for complex applications	complex objects;database system;computer aided design;relational databases logic programming programming environments;programming environments;knowledge bases;programming environment;database systems relational databases prototypes data structures algebra costs application software art software prototyping data models;algres;abstract machine;data model;office automation systems;logic programming;software engineering systems;ra abstract machine;computer aided design databases;relational model;knowledge based software engineering;c relational algebra computer aided design databases algres programming environment relational model complex objects logic programming knowledge bases software engineering systems office automation systems manufacturing databases data model ra abstract machine user languages;relational databases;logic programs;c;manufacturing databases;relational algebra;office automation;user languages	A description is given of Algres, a programming environment that extends the relational model to handle complex objects and operations and integrates the logic programming paradigm. Algres is designed to be used to develop knowledge bases, software-engineering systems, office-automation systems, and computer-aided design and manufacturing databases. The discussion covers the Algres components, its data model, the RA abstract machine, and user languages. The system is written entirely in C. >		Stefano Ceri;Stefano Crespi-Reghizzi;Roberto V. Zicari;Gianfranco Lamperti;Luigi Lavazza	1990	IEEE Software	10.1109/52.56451	relational model;data model;relational algebra;relational database;computer science;theoretical computer science;data mining;database;abstract machine;programming paradigm;event-driven programming;fifth-generation programming language;programming language;logic programming	Embedded	-31.464125775326075	11.477122379536695	151124
2083c16e848c8e3db77418b170488b49c8e52119	a performance analysis of view materialization strategies	performance analysis;materialized views	The conventional way to process commands for relational views is to use query modification to translate the commands into ones on the base relations. An alternative approach has been proposed recently, whereby materialized copies of views are kept, and incrementally updated immediately after each modification of the database. A related scheme exists, in which update of materialized views is deferred until just before data is retrieved from the view. A performance analysis is presented comparing the cost of query modification, immediate view maintenance, and deferred view maintenance. Three different models of the structure of views are given a simple selection and projection of one relation, the natural join of two relations, and an aggregate (e.g. the sum of values in a column) over a selection-projection view. The results show that the choice of the most efficient view maintenance method depends heavily on the structure of the database, the view definition, and the type of query and update activity present.	aggregate data;materialized view;profiling (computer programming);relational algebra	Eric N. Hanson	1987		10.1145/38713.38759	materialized view;computer science;data mining;database;view;information retrieval	DB	-28.52778346796009	4.754340851806065	151194
2050b93e6d56902aea5107d63f460e14be4333df	grass: an efficient method for rdf subgraph matching	graph based index;rdf;subgraph isomorphism	Resource Description Framework RDF is a standard data model of the Semantic Web, and it has been widely adopted in various domains in recent years for data and knowledge representation. Unlike queries on relational databases, most of queries applied on RDF data are known as graph queries, expressed in the SPARQL language. Subgraph matching, a basic SPARQL operation, is known to be NP-complete. Coupled with the rapidly increasing volumes of RDF data, it makes efficient graph query processing a very challenging problem. This paper primarily focuses on providing an index scheme and corresponding algorithms that support the efficient solution of such queries. We present a subgraph matching query engine based on the FFD-index which is an indexing mechanism encoding a star subgraph into a bit string. A SPARQL query graph is decomposed into several star query subgraphs which can be efficiently processed benefiting from succinct FFD-index data structure. Extensive evaluation shows that our approach outperforms RDF-3X and gStore on solving subgraph matching.	subgraph isomorphism problem	Xuedong Lyu;Xin Wang;Yuan-Fang Li;Zhiyong Feng;Junhu Wang	2015		10.1007/978-3-319-26190-4_8	rdf/xml;factor-critical graph;computer science;sparql;theoretical computer science;rdf;data mining;subgraph isomorphism problem;database;rdf query language;induced subgraph isomorphism problem;world wide web;blank node;rdf schema	Vision	-32.35099035071031	4.890029295744151	151280
8351dfafb6b71821e2778e6f345cc54680b8ca2b	schema mapping for data transformation and integration	schema mapping user interface;target schema;semantic function;source schema;user input;schema mapping;data transformation;data source;total user effort;critical point	Finding semantically correct mappings between the schemas of two data sources is a fundamental problem in many important database applications. But it cannot be fully automated, so user input is necessary. This dissertation presents SCIA, a system that assists users in creating executable mappings between a source schema and a target schema by automating simple matches and finding critical points where user input is necessary and maximally useful. SCIA handles complex mappings involving n-to-m matches with semantic functions and conditions, which often exist in practice, but are ignored in most related systems. It outputs mappings in both correspondence format and executable view format that can transform source data into target instances. Formal experiments show SCIA significantly reduces total user effort by using path contexts and combination of multiple matching algorithms to find critical points, and help users at those points by asking them specific questions with sufficient context and suggestions for adding semantic information for data transformation, such as join and grouping conditions. Moreover, SCIA has semantic models for schemas and schema mappings using algebra. This dissertation also gives principles for schema mapping user interface design, based on the ideas of minimal model for schema and schema mapping, and of optimal semiotic morphism from this model into the interface design.		Guilian Wang	2006			schema migration;logical schema;computer science;theoretical computer science;star schema;data mining;database	DB	-25.365611421441447	9.489156929761775	151333
b53a31551f1d19e19155eb2264fa8e6df0dd7d49	a suite of metamodels as a basis for a classification of visual languages	visual language classification;classification;language independent service layers;language independent service layers metamodels classification visual language classification visual environments visual entity;visual languages;design and implementation;metamodels;visual language;visual environments;concrete unified modeling language computer science operations research metamodeling environmental management visualization computational complexity humans computer languages;visual entity;domain specificity;meta model	Metamodeling frameworks for the definition and management of visual languages allow the implementation of visual environments based on some abstract notion of visual entity and of relations among them. We propose a suite of metamodels able to accommodate most commonly used visual paradigms, built as progressive specialisation of a root meta-meta model. This approach facilitates the design and implementation of new, general purpose as well as domain specific, visual languages by allowing the progressive construction of language-independent service layers	emergence;language-independent specification;metamodeling;programming language;software incompatibility;visual language	Paolo Bottoni;Antonio Grau	2004	2004 IEEE Symposium on Visual Languages - Human Centric Computing	10.1109/VLHCC.2004.5	natural language processing;visual analytics;computer science;theoretical computer science;human visual system model;programming language	Visualization	-33.19422675774833	14.90691389424812	151454
163872f6bfca5353bec43a124c19c4cabf0c2d4c	innovations in tape storage automation at ibm	form factor;data access;human error;hard disk drive;random access	In the mid-1980s, tape storage appeared to be heading toward the graveyard. Storing information on large reels of tape that had to be manually mounted was expensive, inefficient, and prone to human error. In addition, competition from hard disk drives (HDDs) was growing. Data access on HDD was faster because the data was stored in a manner for rapid random access, not linearly as on tape, and HDD capacity was rapidly growing. Then, in 1984, IBM introduced the half-inch tape cartridge (3480), which was novel in its small form factor. In 1986, an automatic loading mechanism followed. In 1987 and 1988, competitors introduced the first automated tape libraries to use the half-inch tape cartridge. Suddenly, tape storage became considerably easier to use and more efficient. With automation, it also represented the least expensive way to store data. In 1993, IBM introduced the 3495 tape library, and now offers several different automated libraries ranging from the smaller 3575 MP and 3581/3583 models to very large libraries such as the 3494 and 3584 models. This paper examines the history of IBM tape storage and automation products, including the engineering challenges that were met in response to users’ pressing requirements; it also examines the future of automated tape storage.	automation;course (navigation);data access;hard disk drive;history of ibm;human error;library (computing);magnetic tape data storage;random access;requirement;small form factor;tape drive;tape library	Diana J. Hellman;Raymond Yardy;Perry E. Abbott	2003	IBM Journal of Research and Development	10.1147/rd.474.0445	data access;embedded system;human error;form factor;computer hardware;magnetic tape data storage;computer science;engineering;electrical engineering;operating system;virtual tape library;computer data storage;programming language;access method;mass storage;engineering drawing;random access;quantum mechanics	DB	-33.59311474523896	17.192749936945603	151586
7900cf0ba1844b7c1518ebd2cc9b6b4e952dba16	managing xml documents in an integrated digital library	xml document;digital library		digital library;xml	David A. Smith;Anne Mahoney;Jeffrey A. Rydberg-Cox	2000	Markup Languages		well-formed document;xml catalog;binary xml;xml base;simple api for xml;digital library;xml;xml schema;streaming xml;computer science;document type definition;document structure description;xml framework;xml database;database;xml signature;world wide web;xml schema editor;cxml;information retrieval;efficient xml interchange	DB	-33.60056207079891	7.612194217199306	151619
9a8f8ce6402feeea3c7d60a1bf7a2140b37d3240	updateable views for an xml query language	query language	We present an approach to updateable XML views, which is based on the Stack-Based Approach to query languages. Its novelty consists in introducing procedures overloading generic updating operations on virtual objects into a view definition. This overloading is implicit and users update virtual objects like stored objects. Due to procedures we support full algorithmic power of view definitions, which is essential for writing sophisticated wrappers and mediators. The approach is also relevant to general object-oriented data models.	data model;function overloading;query language;wrapper function;xml;xquery	Hanna Kozankiewicz;Jacek Leszczylowski;Kazimierz Subieta	2003			web query classification;web search query;data mining;query language;computer science;data control language;sargable;query by example;rdf query language;query optimization	DB	-30.941088129172055	9.497975027480107	151620
f80d42df1ccd3af01fd0c79069d00557d6174116	the mutual exclusion problem in reasoning about action and change	mutual exclusion		mutual exclusion	Jan M. Broersen;John-Jules Ch. Meyer;Roel Wieringa	2002			machine learning;computer science;mutual exclusion;artificial intelligence	AI	-20.881287630723214	13.128452488080086	151749
c16e638f83b81c70aa9d151527f3c0818b1f42e8	an axiomatic approach to deciding query safety in deductive databases	decision procedure;database query;deductive databases	A database query is safe if its result consists of a finite set of tuples. If a query is expressed using a set of pure Horn Clauses, the problem of determining query safety is, in general, undecidable. In this paper we consider a slightly stronger notion of safety, called supersafety, for Horn databases in which function symbols are replaced by the abstraction of infinite relations with finiteness constraints [Ramarkrishman et. al 87] We show that the supersafety problem is not only decidable, but also axiomatizable, and the axiomatization yields an effective decision procedure. Although there are safe queries which are not supersafe, we demonstrate that the latter represent quite a large and nontrivial portion of the safe of all safe queries	axiomatic system;decision problem;deductive database;horn clause;undecidable problem	Michael Kifer;Raghu Ramakrishnan;Abraham Silberschatz	1988		10.1145/308386.308412	online aggregation;sargable;query optimization;boolean conjunctive query;query by example;data mining;database;mathematics;view;range query;algorithm	DB	-25.114561833998476	11.062529195820975	151798
c28a803de15894291606d33ad2a7c9e143da3615	extracting schema from an oem database	data integrity;time complexity;heterogeneous databases;data model;semi structured data;query evaluation;data warehouse;database query	While the schema-less feature of the OEM (Object Exchange Model) gives flexibility in representing semi-structured data, it brings difficulty in formulating database queries. Extracting schema from an OEM database then becomes an important research topic. This paper presents a new approach to this topic with the following features. (1) In addition to representing the nested label structure of an OEM database, the proposed OEM schema keeps up-to-date information about instance objects of the database. The object-level information is useful in speeding up query evaluation. (2) The OEM schema is explicitly represented as a label-set, which is easy to construct and update. (3) The OEM schema of a database is statically built and dynamically updated. The time complexity of building the OEM schema is linear in the size of the OEM database. (4) The approach is applicable to a wide range of areas where the underlying schema is much smaller than the database itself (e.g. data warehouses that are made from a set of heterogeneous databases).	database schema;object exchange model;semi-structured data;semiconductor industry;time complexity	Yidong Shen	1998	Journal of Computer Science and Technology	10.1007/BF02946619	time complexity;schema migration;semi-structured data;semi-structured model;data model;computer science;conceptual schema;database model;data warehouse;star schema;data integrity;data mining;database;view;world wide web;database schema;database design	DB	-33.20075341900404	8.322816081428092	151809
589db8af75b7e69ea47e460b256d284f4d320d2a	strategic term rewriting and its application to a vdmsl to sql conversion	developpement logiciel;base relacional dato;relational data model;strategic term rewriting;new product;sql;metodo formal;interrogation base donnee;methode formelle;tipo dato;interrogacion base datos;producto nuevo;data type;relational database;formal method;programm calculation;refinement method;marcador;pointer;rewriting systems;desarrollo logicial;software development;base donnee relationnelle;produit nouveau;pointeur;modele donnee;vdm;strateguc term rewriting;methode raffinement;type donnee;term rewriting;metodo afinamiento;systeme reecriture;database query;program calculation;data models	We constructed a tool, called VooDooM, which converts datatypes in VDM-SL into SQL relational data models. The conversion involves transformation of algebraic types to maps and products, and pointer introduction. The conversion is specified as a theory of refinement by calculation. The implementation technology is strategic term rewriting in Haskell, as supported by the Strafunski bundle. Due to these choices of theory and technology, the road from theory to practise is straightforward.	algebraic data type;data model;haskell;map;pointer (computer programming);refinement (computing);relational database;rewriting;sl (complexity);sql;strafunski;theory	Tiago L. Alves;Paulo F. Silva;Joost Visser;José Nuno Oliveira	2005		10.1007/11526841_27	data modeling;sql;relational model;pointer;formal methods;vienna development method;data type;relational database;computer science;software development;database;mathematics;programming language;algorithm;new product development	PL	-29.07000369773596	14.414545460015866	151858
82cc738a817a5031d660aaa3f8bf647e2ece9909	a temporal relational model and a query language	base relacional dato;query language;base donnee temporelle;interrogation base donnee;interrogacion base datos;base dato temporal;relational database;lenguaje interrogacion;modelo;temporal database;tsql;relational model;base donnee relationnelle;modele;langage interrogation;systeme gestion base donnee;sistema gestion base datos;database management system;models;database query	This paper proposes an extension of the relational data model for incorporating temporal semantics of the real world into a database. In this temporal relational model (TRM), the attributes are categorized as synchronous and asynchronous. This categorization leads to the notions of temporal dependency and time normal form. The imposition of time normal form avoids redundancy and retrieval and update anomalies. New relational algebra operations for this model are also discussed. A temporal query language called TSQL, which is a superset of SQL, has been proposed to retrieve information from time-varying relations, TSQL has several new features, which provide extremely powerful query processing capabilities for time-varying relations. The model and the language have numerous applications, where data in their historical form are important.	algorithm;categorization;coherence (physics);data compression;data model;database trigger;emoticon;federal enterprise architecture;goto;imperative programming;linear algebra;modal logic;nl (complexity);query language;r language;relational algebra;relational model;sql;temporal database;temporal logic;test engineer;transact-sql;trusted timestamping;usability;web slice	Shamkant B. Navathe;Rafi Ahmed	1989	Inf. Sci.	10.1016/0020-0255(89)90026-1	sargable;query optimization;sql;relational model;codd's theorem;relational calculus;relational database;computer science;query by example;data mining;database;temporal database;conjunctive query;algorithm;query language	DB	-30.26843905762721	8.739662267407763	151860
802570dd56f4c3734fe35629f0f20d7a76cd0abd	graphical query mechanism for historical data warehouse within mdd	databases;query tools;historical multidimensional data structure;query processing;computational modeling databases adaptation model data warehouses data structures proposals data models;olap tools;multidimensional data structure;mdd historical data warehouse graphical query;data mining;graphical query mechanism;software engineering;software engineering data mining data structures data warehouses query processing;visual query mechanism;computational modeling;adaptation model;temporal queries;graphical query;data structures;historical data warehouse;olap tools graphical query mechanism historical data warehouse mdd model driven software development historical multidimensional data structure temporal queries decision making queries query tools visual query mechanism;model driven software development;decision making queries;cognitive load;data warehouses;proposals;mdd;historical data;data models	Query tools that depend on the ability of programmers, impose a cognitive load that could reduce the user's productivity. Within MDD, the proposal of our work is the creation of a visual query mechanism derived from a historical multidimensional data structure. This mechanism facilitates (and partially automates) the formulation of temporal and decision making queries.	data structure;graphical user interface;model-driven engineering;programmer	Carlos G. Neil;Jerónimo Irazábal;Marcelo De Vincenzi;Claudia Pons	2010	2010 XXIX International Conference of the Chilean Computer Science Society	10.1109/SCCC.2010.49	query optimization;computer science;data science;data mining;database;query language	DB	-29.712539317174322	8.379336861948977	151904
9f52918258e240f173cca1c042cb272e8be0d349	xml update processing for incremental refresh of xml cache			xml	Seungchul Han;Dae Hyun Hwang;Hyunchul Kang	2004			xml;world wide web;streaming xml;processing instruction;database;xml encryption;simple api for xml;xml framework;xml database;efficient xml interchange;computer science	DB	-32.33432363282417	7.217929150766296	152413
e206576d0f98961aebec1da922ba25f04c0ae8d9	a heuristic approach to distributed query processing	optimal solution;data transmission;distributed database;query processing;distributed query processing	In a distributed database environment, finding the optimal strategy which fully reduces all relations referenced by a general tree query, may take exponential time. Furthermore, since reduced relations are to be moved to the final site, the optimal strategy which fully reduces all relations does not give an optimal solution to the problem of minimizing the total transmission cost. For a general query, even with only one join attribute, the problem of finding an optimal strategy to reduce the total data transmission cost has been shown to be NP-hard. In this paper, a heuristic approach is taken to the distributed query processing problem. Different cost benefit functions are defined based on the nature of the relations involved in the semijoin. The proposed algorithm will produce a sequence of cost beneficial semijoin operations to reduce the total data transmission cost involved in answering a general query. For each join attribute, a two phase reduction process is used. The order in which the semijoins are performed is controlled by the projected size of the join attribute. This algorithm produces optimal sequence of semijoins for simple queries. For general queries, The experimental results, obtained by simulation, indicate a substantial improvement over the SDD-1 query processing algorithm.	algorithm;distributed database;heuristic;relational algebra;simulation;time complexity	Jo-Mei Chang	1982			online aggregation;distributed algorithm;sargable;query optimization;query expansion;computer science;query by example;theoretical computer science;database;rdf query language;web search query;view;distributed database;information retrieval;query language;data transmission;spatial query	DB	-26.778876859315364	4.92118356437203	152449
e983c26ff4d2cd6473dd89e0a3bca980b1622375	generation of text from logical formulae	semantic representation;generic algorithm;efficient algorithm;target language;indexation;normal form;natural language processing;categorial grammar;logical form;machine translation	The paper addresses the problem of generating sentences from logical formulae. It describes a simple and efficient algorithm for generating text which has been developed for use in machine translation, but will have wider application in natural language processing. An important property of the algorithm is that the logical form used to generate a sentence need not be one which could have been produced by parsing the sentence: formal equivalence between logical forms is allowed for. This is necessary for a machine translation system, such as the one envisaged in this paper, which uses single declarative grammars of individual languages, and declarative statements of translation equivalences for transfer. In such a system, it cannot be guaranteed that transfer will produce a logical form in the same order as would have been produced by parsing some target-language sentence, and it is not practicable to define a normal form for the logical forms. The algorithm is demonstrated using a categorial grammar and a simple indexed logic, as this allows a particularly clear and elegant formulation. It is shown that the algorithm can be adapted to phrase-structure grammars, and to more complex semantic representations than that used here.	a-normal form;algorithm;categorial grammar;declarative programming;machine translation;natural language processing;parsing;turing completeness	John D. Phillips	1993	Machine Translation	10.1007/BF00981757	logical nor;natural language processing;logical equivalence;non-classical logic;synchronous context-free grammar;categorial grammar;transfer-based machine translation;genetic algorithm;logical form;logical consequence;computer science;truth table;linguistics;machine translation;rule-based machine translation;atomic sentence;algorithm	NLP	-21.089215620858347	16.016311740389348	152705
725e10455cf5f8bc234c24cb66f16a2f00f66918	analysis of schemas with access restrictions	verification;categories and subject descriptors h35 information systems online information services web based services;computacion informatica;hidden web;access methods;f41 mathematical logic and formal languages mathematical logic temporal logic general terms theory;languages additional key words and phrases access methods;ciencias basicas y experimentales;optimization;grupo a	We study verification of systems whose transitions consist of accesses to a Web-based data source. An access is a lookup on a relation within a relational database, fixing values for a set of positions in the relation. For example, a transition can represent access to a Web form, where the user is restricted to filling in values for a particular set of fields. We look at verifying properties of a schema describing the possible accesses of such a system. We present a language where one can describe the properties of an access path and also specify additional restrictions on accesses that are enforced by the schema. Our main property language, AccessLTL, is based on a first-order extension of linear-time temporal logic, interpreting access paths as sequences of relational structures. We also present a lower-level automaton model, A-automata, into which AccessLTL specifications can compile. We show that AccessLTL and A-automata can express static analysis problems related to “querying with limited access patterns” that have been studied in the database literature in the past, such as whether an access is relevant to answering a query and whether two queries are equivalent in the accessible data they can return. We prove decidability and complexity results for several restrictions and variants of AccessLTL and explain which properties of paths can be expressed in each restriction.	automaton;compiler;database schema;first-order predicate;form (html);linear temporal logic;lookup table;relational database;static program analysis;verification and validation;world wide web	Michael Benedikt;Pierre Bourhis;Clemens Ley	2015	ACM Trans. Database Syst.	10.1145/2699500	verification;computer science;theoretical computer science;data mining;database;programming language;access method;deep web	DB	-24.797994045424844	11.987154056780577	152714
b6d842814ddc079e1184ae9234a970517aad7c12	the dades/gp approach to automatic generation of information system prototypes from a deductive conceptual model	optimisation;deductibility;base donnee;architecture systeme;conceptualization;information systems;optimizacion;prototypes;sistema informatico;exigence usager;exigencia usuario;database;base dato;conceptual model;computer system;data bases;specification language;automatic generation;deductive conceptual model;deductibilite;conceptualizacion;prototipo;langage dades;deducibilidad;user requirement;arquitectura sistema;optimization;systeme informatique;lenguaje especificacion;information system;system architecture;langage specification;prototype;conceptualisation;systeme information;sistema informacion	We describe the main issues we faced in the design and construction of the DADES/GP system. This system generates prototypes of Information Systems (IS). The input to the system is a deductive conceptual model of the IS written in the DADES language, which is also described in this paper. Several alternatives exist in the architecture of the prototype. Our system uses forward inference architecture, which allows an efficient production of outputs and is able to incorporate a number of optimization procedures.	dicom;information system;mathematical optimization;prototype	Jaume Sistac	1992	Inf. Syst.	10.1016/0306-4379(92)90013-D	computer science;artificial intelligence;database;prototype;information system;algorithm;systems architecture	DB	-30.339070346580986	14.101839068475698	153397
81ac79e2ba9f01f37c5648380d53565ba54ee9d1	semantic transformation approach with schema constraints for xpath query axes	semantic xml query optimization;xml schema;query processing;query optimization;xpath;xml;semantic query optimization;empirical evaluation	XPath queries are essentially composed of a succession of axes defining the navigation from a current context node. Among the XPath query axes family, child, descendant, parent can be optionally specified using the path notations {/,//,..} which have been commonly used. Axes such as following-sibling and preceding-sibling have unique functionalities which provide different required information that cannot be achieved by others. However, XPath query optimization using schema constraints does not yet consider these axes family.#R##N##R##N#The performance of queries denoting the same result by means of different axes may significantly differ. The difference in performance can be affected by some axes, but this can be avoided. In this paper, we propose a semantic transformation typology and algorithms that transform XPath queries using axes, with no optional path operators, into semantic equivalent XPath queries in the presence of an XML schema. The goal of the transformation is to replace whenever possible the axes that unnecessarily impact upon performance. We show how, by using our semantic transformation, the accessing of the database using such queries can be avoided in order to boost performance. We implement the proposed algorithms and empirically evaluate their efficiency and effectiveness as semantic query optimization devices.	xpath	Dung Xuan Thi Le;Stéphane Bressan;Eric Pardede;J. Wenny Rahayu;David Taniar	2010		10.1007/978-3-642-17616-6_41	query optimization;xml;xml schema;computer science;path expression;data mining;xml schema;database;information retrieval	DB	-28.884543172544515	6.5955089379428795	153561
aa4edd45dff48f59886cac94966b1306b331bd5f	file-level operations on network data structures	management system;software systems;data processing;network structure;data structure	The time and cost of implementing data processing applications can be greatly reduced through the use of software systems which provide language for the expression of file-level operations on data, i.e., operations whose operands are sets of records or entire files, such as report generation and sorting. Such systems, which have been referred to as self-contained systems or generalized file management systems, have characteristically been restricted to logical data structure classes no richer than hierarchies. This paper explores the extension of the concept of file-level operations to network structures, as exemplified by the CODASYL DDLC data structure class, on the assumption that such a facility will prove useful to certain types of users (e.g., data administrators) for certain types of data manipulation (e.g., data base creation and updating). The paper first outlines the general requirements which must be met in such a facility, and then describes a specific approach to the development of a language for such a facility.	codasyl;data structure;database;operand;requirement;software system;sorting	William C. McGee	1975		10.1145/500080.500085	data structure;data processing;computer science;theoretical computer science;data mining;management system;database;programming language;software system	DB	-29.214919277515023	12.44945225491725	153632
604b767ad66c229ce1a30a925c9751d93b84f008	learning rewrite rules versus search control rules to improve plan quality	rewrite rule;representacion conocimientos;systeme intelligent;adquisicion del conocimiento;systeme apprentissage;sistema inteligente;acquisition connaissances;learning systems;planificacion;knowledge acquisition;intelligent system;planning;planification;knowledge representation;representation connaissances;domain specificity;partial order	Domain independent planners can produce better-quality plans through the use of domain-speci c knowledge, typically encoded as search control rules. The planning-by-rewriting approach has been proposed as an alternative technique for improving plan quality. We present a system that automatically learns plan rewriting rules and compare it with a system that automatically learns search control rules for partial order planners. Our results indicate that learning search control rules is a better choice than learning rewrite rules.	algorithm;emoticon;rewrite (programming);rewriting	Muhammad Afzal Upal;Renée Elio	2000		10.1007/3-540-45486-1_20	partially ordered set;planning;knowledge representation and reasoning;computer science;artificial intelligence;machine learning;algorithm	AI	-20.221761106541727	11.464009049450208	153665
6b886dc874d80546a7d6df108f61ff8ea7865c1c	software engineering for mathematics (keynote)	theorem proving	Since Turing, we have wanted to use computers to store, process, and check mathematics. However even with the assistance of modern software tools, the formalization of research-level mathematics remains a daunting task, not least because of the talent with which working mathematicians combine diverse theories to achieve their ends. By drawing on tools and techniques from type theory, language design, and software engineering we captured enough of these practices to formalize the proof of the Odd Order theorem, a landmark result in Group Theory, which ultimately lead to the monumental Classification of finite simple groups. This involved recasting the software component concept in the setting of higher-order, higher-kinded Type Theory to create a library of mathematical components covering most of the undergraduate Algebra and graduate Group Theory syllabus. This library then allowed us to write a formal proof comparable in size and abstraction level to the 250-page textbook proof of the Odd Order theorem.	abstraction layer;component-based software engineering;computer;formal proof;kind (type theory);turing;type theory	Georges Gonthier	2013		10.1145/2491411.2505429	computer science;engineering;software engineering;mathematical proof;automated theorem proving;programming language;algorithm	PL	-19.954477723255206	18.214159511649598	153913
8a7c49677816280263f60d810521826455abda7e	merging of xml documents	document structure;entity relationship model;relation algebra;algebra relacional;estructura documental;structure document;integration information;xml language;conceptual analysis;modelo entidad relacion;modele entite relation;analisis conceptual;information integration;integracion informacion;xml document;algebre relationnelle;information system;analyse conceptuelle;relational algebra;systeme information;langage xml;lenguaje xml;sistema informacion	How to deal with the heterogeneous structures of XML documents, identify XML data instances, solve conflicts, and effectively merge XML documents to obtain complete information is a challenge. In this paper, we define a merging operation over XML documents that can merge two XML documents with different structures. It is similar to a full outer join in relational algebra. We design an algorithm for this operation. In addition, we propose a method for merging XML elements and handling typical conflicts. Finally, we present a merge template XML file that can support recursive processing and merging of XML elements.	algorithm;erdős–rényi model;join (sql);prototype;recursion;relational algebra;springer (tank);vldb;world wide web;xml	Wanxia Wei;Mengchi Liu;Shijun Li	2004		10.1007/978-3-540-30464-7_22	xml catalog;xml validation;binary xml;xml encryption;xml namespace;simple api for xml;xml;relax ng;streaming xml;computer science;document structure description;xml framework;data mining;xml database;xml schema;database;xml signature;world wide web;xml schema editor;cxml;efficient xml interchange;sgml	DB	-32.14202825787316	9.205353698242584	154014
b3064ba95898fe5512d129fede153c6ed5e662ef	query translation in a heterogeneous distributed database based on hypergraph models models(abstract)	distributed database;first order predicate logic;prolog;logic data base;data base view;query translation;query by example;group		distributed database	O. Mehdi Owrang M.MehdiOwrang;Leslie L. Miller	1986		10.1145/324634.325014	sargable;query optimization;query expansion;web query classification;boolean conjunctive query;computer science;query by example;theoretical computer science;first-order logic;database;group;programming language;web search query;prolog;view;distributed database;query language	DB	-30.978865036587052	8.56262369066544	154179
18d23a1b3192d7c292534bd1dd323775edd13f34	a decidable very expressive description logic for databases		We introduce (mathcal{DLR}^+), an extension of the n-ary propositionally closed description logic (mathcal{DLR}) to deal with attribute-labelled tuples (generalising the positional notation), projections of relations, and global and local objectification of relations, able to express inclusion, functional, key, and external uniqueness dependencies. The logic is equipped with both TBox and ABox axioms. We show how a simple syntactic restriction on the appearance of projections sharing common attributes in a (mathcal{DLR}^+) knowledge base makes reasoning in the language decidable with the same computational complexity as (mathcal{DLR}). The obtained (mathcal{DLR}^{pm }) n-ary description logic is able to encode more thoroughly conceptual data models such as EER, UML, and ORM.	abox;challenge-handshake authentication protocol;cobham's thesis;computational complexity theory;conceptual schema;data model;database theory;datalog;description logic;dynamic language runtime;encode;exptime;enhanced entity–relationship model;handbook;iswc;international joint conference on artificial intelligence;knowledge base;lecture notes in computer science;map;norma;object-relational mapping;object-role modeling;ontology (information science);p (complexity);peano axioms;perrin number;relational database;semantic web;semantic reasoner;springer (tank);tbox;theoretical computer science;undecidable problem;unified modeling language;web ontology language;world wide web	Alessandro Artale;Enrico Franconi;Rafael Peñaloza;Francesco Sportelli	2017		10.1007/978-3-319-68288-4_3	tuple;database;syntax;computer science;abox;axiom;computational complexity theory;description logic;decidability;positional notation	AI	-22.833896434550514	10.019153064047302	154300
7f31d45be4bc077c9e70ef38fbf65b7be2fa4943	indexing dif/2		Many Prolog programs are unnecessarily impure because of inadequate means to express syntactic inequality. While the frequently provided built-in dif/2 is able to correctly describe expected answers, its direct use in programs often leads to overly complex and inefficient definitions — mainly due to the lack of adequate indexing mechanisms. We propose to overcome these problems by using a new predicate that subsumes both equality and inequality via reification. Code complexity is reduced with a monotonic, higher-order if-then-else construct based on call/N. For comparable correct uses of impure definitions, our approach is as determinate and similarly efficient as its impure counterparts.	benchmark (computing);conditional (computer programming);iso/iec 42010;programming complexity;prolog;property (philosophy);reification (knowledge representation);social inequality;stack overflow	Ulrich Neumerkel;Stefan Kral	2015	CoRR		algorithm	DB	-20.342576018247254	15.765658014836152	154339
058b54f848af6611ec6a1ed2749bbb4f8cbb692b	sat-based decision procedures for automated reasoning: a unifying perspective	automated reasoning;qa076 computer software;decision procedure;sat solver;qa075 electronic computers computer science	Propositional reasoning (SAT) is an essential part of many reasoning tasks. Many problems in computer science can be compiled to SAT and then effectively decided using state-of-the-art solvers. Alternatively, if reduction to SAT is not feasible, the ideas and technology of state-of-the-art SAT solvers can be useful in deciding the propositional component of the reasoning task being considered. This last approach has been used in different contexts by different authors, many times by authors of this paper. Because of the essential role played by the SAT solver, these decision procedures have been called “SAT-based”. SATbased decision procedures have been proposed for various logics, but also in other areas such as planning. In this paper we present a unifying perspective on the various SAT-based approaches to these different reasoning tasks.	automated reasoning;boolean satisfiability problem;compiler;computer science;decision problem;first-order logic;first-order predicate;modal logic;quantifier (logic);solver	Alessandro Armando;Claudio Castellini;Enrico Giunchiglia;Fausto Giunchiglia;Armando Tacchella	2005		10.1007/978-3-540-32254-2_4	computer science;theoretical computer science;algorithm	AI	-19.22760220905913	14.132364376825757	154491
71fcfaa7a38cf9d6afff6ee98c548dd70d76ff6b	rewriting queries using views	databases computer society prototypes data warehouses mobile computing software libraries;query processing;views;functional dependency;information integration;database theory query processing relational databases;system development;relational databases;materialized views;database theory;query rewriting query answering materialized views negative goals positive knowledge negative knowledge conjunctive views negation as failure disjunctive views functional dependencies;queries	ÐIn this paper, we consider the problem of answering queries using materialized views in the presence of negative goals. The solution is carried out by ainvertingo views and deriving both positive and negative knowledge. In order to derive negative knowledge, we invert conjunctive views with negation into a set of (extended) views which may also have, in addition to negation-asfailure, a different form of negation called classical (or strong) negation. We also consider the case of disjunctive views and present a technique which permits us to infer both positive and negative knowledge. Furthermore, we extend previous techniques for inferring knowledge from views based on relations with functional dependencies. Finally, we present a prototype of a system developed at the University of Calabria. Index TermsÐRelational databases, queries, views, information integration.	algorithm;bi-directional text;bus (computing);database;datalog;disjunctive normal form;functional dependency;inductive reasoning;literal (mathematical logic);materialized view;prototype;quantum information;recursion;rewriting;unfolding (dsp implementation)	Sergio Flesca;Sergio Greco	2001	IEEE Trans. Knowl. Data Eng.	10.1109/69.971191	materialized view;database theory;information schema;relational database;computer science;information integration;data mining;database;functional dependency;conjunctive query;view;information retrieval	DB	-29.18505151486426	9.600624559670655	154685
132318382373ea06312c5c812193c99c38697efb	mesh data management in large-scale scientific computing	natural sciences computing data analysis internet;data sharing;distributed mesh data mesh data management large scale scientific computing numerical simulations multidimension multivariables numerical simulation programs data exchange data sharing hierarchical data model high level data access data analysis tools scientific data management system meta data web pages;range query;data analysis tools;web pages;multidimension;metadata;large scale scientific computing;high level data access;complex structure;data management;hierarchical data;data exchange;data model;scientific data management;metadata scientific computation numerical simulation data model data management;data analysis;large scale;hierarchical data model;computational modeling;large scale simulation;internet;data models numerical models computational modeling numerical simulation data visualization object oriented modeling mathematical model;data visualization;data access;mesh data management;scientific computing;mathematical model;distributed mesh data;meta data;numerical simulations;multivariables numerical simulation programs;numerical models;scientific computation;natural sciences computing;object oriented modeling;scientific data management system;data models;numerical simulation	In many research fields of numerical simulations, programs often produce a large amount of mesh data with complex structure. It is a fatal bottleneck for scientists to manage such large-scale simulation data. In allusion to typical data characteristics of multi-dimension and multi-variables numerical simulation programs, the relativity between time and space, and practical problems of data exchange and sharing, this paper brings forward a hierarchical data model to organize mesh data. We use meta-data to efficiently organize and manage mesh data, and have developed a unified interface for high-level data access, supporting scientific computing, visualization and other data analysis tools to store mesh data with a unified method. Meanwhile, we have completed a scientific data management system where users can browse and search meta-data on Web pages. In addition, users can browse and analyze remote distributed mesh data, and do spatial or value range queries to their mesh data.	browsing;computational science;computer simulation;data access;data model;hierarchical database model;high- and low-level;numerical analysis;numerical relativity;range query (data structures);web page	Hong Chen;Winmin Zheng	2008	The Third ChinaGrid Annual Conference (chinagrid 2008)	10.1109/ChinaGrid.2008.55	computer science;data science;data warehouse;data mining;database;order one network protocol	HPC	-32.269947622213195	16.59181855223901	154903
5a3dd8de1ae8d99824ccf63bc48aa1758bb9f80b	cardinality constraints for uncertain data	data semantics;conference item;possibility theory;qualitative reasoning;cardinality constraint;uncertain data;armstrong database	Modern applications require advanced techniques and tools to process large volumes of uncertain data. For that purpose we introduce cardinality constraints as a principled tool to control the occurrences of uncertain data. Uncertainty is modeled qualitatively by assigning to each object a degree of possibility by which the object occurs in an uncertain instance. Cardinality constraints are assigned a degree of certainty that stipulates on which objects they hold. Our framework empowers users to model uncertainty in an intuitive way, without the requirement to put a precise value on it. Our class of cardinality constraints enjoys a natural possible world semantics, which is exploited to establish several tools to reason about them. We characterize the associated implication problem axiomatically and algorithmically in linear input time. Furthermore, we show how to visualize any given set of our cardinality constraints in the form of an Armstrong instance, whenever possible. Even though the problem of finding an Armstrong instance is precisely exponential, our algorithm computes an Armstrong instance with conservative use of time and space. Data engineers and domain experts can jointly inspect Armstrong instances in order to consolidate the certainty by which a cardinality constraint shall hold in the underlying application domain.		Henning Köhler;Sebastian Link;Henri Prade;Xiaofang Zhou	2014		10.1007/978-3-319-12206-9_9	possibility theory;cardinality;qualitative reasoning;computer science;artificial intelligence;data mining;database;algorithm	DB	-25.238513861460262	9.34716840586377	155211
1557bd271aab9346bc6a786b69f1889dccad3979	towards reconciling sparql and certain answers	certain answers;dl lite;complexity;sparql;query answering;query rewriting	SPARQL entailment regimes are strongly influenced by the big body of works on ontology-based query answering, notably in the area of Description Logics (DLs). However, the semantics of query answering under SPARQL entailment regimes is defined in a more naive and much less expressive way than the certain answer semantics usually adopted in DLs. The goal of this work is to introduce an intuitive certain answer semantics also for SPARQL and to show the feasibility of this approach. For OWL 2 QL entailment, we present algorithms for the evaluation of an interesting fragment of SPARQL (the so-called well-designed SPARQL). Moreover, we show that the complexity of the most fundamental query analysis tasks (such as query containment and equivalence testing) is not negatively affected by the presence of OWL 2 QL entailment under the proposed semantics.	algorithm;description logic;sparql;turing completeness;web ontology language	Shqiponja Ahmetaj;Wolfgang Fischl;Reinhard Pichler;Mantas Simkus;Sebastian Skritek	2015		10.1145/2736277.2741636	complexity;named graph;computer science;sparql;data mining;database;rdf query language;information retrieval;rdf schema	Web+IR	-24.107016195859885	8.676562475721479	155316
33a5e2857afd3ae67c1da1ec7d87798c3b7c698e	a distributed database server for continuous media	distributed databases streaming media feature extraction buffer storage mpeg 7 standard web server samarium query processing data mining aggregates;query plan execution distributed database server continuous media video data handling data type predator open source object relational dbms shore storage manager video operations video storage searching by content video streaming query types query by example multi feature similarity searching buffer manager real time constraints query plan optimization;video databases;file servers;base donnee repartie;distributed database;red www;query processing;continuous media;storage management;real time;reseau web;buffer management;base repartida dato;base donnee video;data type;serveur video;serveur reseau;network servers;multimedia databases;distributed databases;world wide web;video servers;relational databases;object oriented databases;systeme gestion base donnee;query by example;sistema gestion base datos;database management system;object relational;similarity search;open source;real time systems;real time systems multimedia databases video databases distributed databases file servers object oriented databases relational databases storage management query processing	In our project, we adopt a new approach for handling video data. We view the video as a well-defined data type with its own description, parameters, and applicable methods. The system is based on PREDATOR, the open source object relational DBMS. PREDATOR uses Shore as the underlying storage manager (SM). Supporting video operations (storing, searching by content, and streaming) and new query types (query by examples and multi-features similarity search) requires major changes in many of the traditional system components. More specifically, the storage and buffer manager will have to deal with huge volumes of data with real time constraints. Query processing has to consider the video methods and operators in generating, optimizing and executing query plans. Our system includes the following features:	database server;distributed database;image processing;open-source software;relational database management system;similarity search;streaming media	Walid G. Aref;Ann Christine Catlin;Ahmed K. Elmagarmid;Jianping Fan;Jishun Guo;Moustafa A. Hammad;Ihab F. Ilyas;Mirette S. Marzouk;Sunil Prabhakar;Abdelmounaam Rezgui;S. Teoh;Evimaria Terzi;Yi-Cheng Tu;Athena Vakali;Xingquan Zhu	2002		10.1109/ICDE.2002.994764	file server;sargable;query optimization;data type;relational database;computer science;query by example;video tracking;data mining;database;world wide web;distributed database	DB	-31.19979307111901	5.538685669942387	155343
67e9e48b3e3442b134deb175e984a89fae6c7ebf	schema versioning for multitemporal relational databases	valid time;transaction time;user interface;database management systems;design requirements;information technology;transaciton time;computer system design;relational database;multitemporal database;temporal database;schema versioning;schematic studies;database design;database management system;computer oriented programs;models;schema evolution;systems approach	In order to follow the evolution of application needs, a database management system is easily expected to undergo changes involving database structure after implementation. Schema evolution concerns the ability of maintaining extant data in response to changes in database structure. Schema versioning enables the use of extensional data through multiple schema interface as created by a history of schema changes. However, schema versioning has been considered only to a limited extent in current literature. Also in the field of temporal databases, whereas a great deal of work has been done concerning temporal versioning of extensional data, a thorough investigation of schema versioning potentialities has not yet been made. In this paper we consider schema versioning in a broader perspective and introduce new design options whose distinct semantic properties and functionalities will be discussed. First of all, we consider solutions for schema versioning along  transaction time  but also along  valid time . Moreover, the support of schema versioning implies operations both at intensional and extensional level. Two distinct design solutions (namely  single-  and  multi-pool ) are presented for the management of extensional data in a system supporting schema versioning. Finally, a further distinction is introduced to define  synchronous  and  asynchronous  management of versioned data and schemata. The proposed solutions differ in their semantics and in the possible operations they support. The mechanisms for the selection of data through a schema version are in many cases strictly related to the particular schema versioning solution adopted, that also affects the data definition and manipulation language at user-interface level. In particular, we show how the temporal language TSQL2, originally designed to support basic functionalities of transaction-time schema versioning, can accordingly be extended. ©1997 Elsevier Science Ltd	relational database	Cristina De Castro;Fabio Grandi;Maria Rita Scalas	1997	Inf. Syst.	10.1016/S0306-4379(97)00017-3	schema migration;information schema;semi-structured model;relational database;computer science;conceptual schema;star schema;transaction time;data mining;database;temporal database;user interface;information technology;world wide web;database schema;valid time;database design;systems thinking	DB	-32.18696542446803	11.68814075375637	155709
1839c97a677a4eadaf88590a22257e367b73b2ed	krypton: integrating terminology and assertion	theorem prover;first order;semantic net;knowledge representation;first order logic;domain specificity	The demands placed on a knowledge representation scheme by a knowledge-based system are generally not all met by any of today’s candidates. Representation languages based on frames or semantic networks have intuitive appeal for forming descriptions but tend to have severely limited assertional power, and are often fraught with ambiguous readings. Those based on first-order logic are less limited assertionally, but are restricted to primitive, unrelated terms. We have attempted to overcome these limitations in a new, hybrid knowledge representation system, called “KRYPTON”. KRYPTON has two representation languages, a frame-based one for forming domain-specific descriptive terms and a logic-based one for making statements about the world. We here summarize the two languages, a functional interface to the system, and an implementation in terms of a taxonomy of frames and its interaction with a first-order theorem prover.	attribute-value system;automated theorem proving;first-order logic;first-order predicate;first-order reduction;knowledge representation and reasoning;knowledge-based systems;krypton (programming language);semantic network;taxonomy (general)	Ronald J. Brachman;Hector J. Levesque;Richard Fikes	1983			knowledge representation and reasoning;computer science;artificial intelligence;theoretical computer science;machine learning;first-order logic;algorithm	AI	-22.030854173802165	9.399176962501036	155838
286daefd837bf761e04f0ac20f1114e1f4bc79b8	the semantics of an extended referential integrity for a multilevel secure relational data model	relational data model;polyinstantiation;referential integrity;data model;multilevel secure relational data model;security;information leakage;multilevel security	To prevent information leakage in multilevel secure data models, the concept of polyinstantiation was inevitably introduced. Unfortunately, when it comes to references through foreign key in multilevel relational data models, the polyinstantiation causes referential ambiguities. To resolve this problem, this paper proposes an extended referential integrity semantics for a multilevel relational data model, Multilevel Secure Referential Integrity Semantics (MLS-RIS). The MLS-RIS distinguishes foreign key into two types of references, i.e. value-based and entity-based reference. For each type, it defines the referential integrity to be held between two multilevel relations, and provides resolution rules for the referential ambiguities. In addition, the MLS-RIS specifies the semantics of referential actions of the SQL update operations so as to preserve the referential integrity. 2003 Elsevier B.V. All rights reserved.	data model;foreign key;information leakage;multilevel security;polyinstantiation;referential integrity;relational interface system;relational database;relational model;sql;spectral leakage;update (sql)	Sang-Won Lee;Yong-Han Kim;Hyoung-Joo Kim	2004	Data Knowl. Eng.	10.1016/S0169-023X(03)00122-8	referential integrity;relational model;data model;computer science;information security;theoretical computer science;data mining;database	Security	-27.262738679126016	12.598934808083078	155938
e70777a06c2e08c74b84083f0a355dd9f71667ce	high performance absorption algorithms for terminological reasoning.	experimental tests;optimization technique;perforation;experimental evaluation;description logic;high performance;knowledge base	When reasoning with description logic (DL) knowledge bases (KBs), performance is of critical concern in real applications, especially when these KBs contain a large number of axioms. To improve the performance, axiom absorption has been proven to be one of the most effective optimization techniques. The well-known algorithms for axiom absorption, however, still heavily depend on the order and the format of the axioms occurring in KBs. In addition, in many cases, there exist some restrictions in these algorithms which prevent axioms from being absorbed. The design of absorption algorithms for optimal reasoning is still an open problem. In this paper, we propose some new algorithms to absorb axioms in a KB to improve the reasoning performance. The experimental tests we conducted are mostly based on synthetic benchmarks derived from common cases found in real KBs. The experimental evaluation demonstrates a significant runtime improvement.	algorithm;automated reasoning;benchmark (computing);description logic;existential quantification;heuristic (computer science);kilobyte;knowledge base;mathematical optimization;semantic reasoner;synthetic intelligence;whole earth 'lectronic link	Ming Zuo;Volker Haarslev	2006			computer science;data mining;engineering drawing;algorithm	AI	-23.46956869943955	8.106614221516198	156025
1129483f3e706ddf3a3df116e2534e7310d14949	query answering in bayesian description logics		The Bayesian Description Logic (BDL) BEL is a probabilistic DL, which extends the lightweight DL EL by defining a joint probability distribution over EL axioms with the help of a Bayesian network (BN). In the recent work, extensions of standard logical reasoning tasks in BEL are shown to be reducible to inferences in BNs. This work concentrates on a more general reasoning task, namely on conjunctive query answering in BEL where every query is associated to a probability leading to different reasoning problems. In particular, we study the probabilistic query entailment, top-k answers, and top-k contexts as reasoning problems. Our complexity analysis suggests that all of these problems are tractable under certain assumptions.	analysis of algorithms;bayesian network;cobham's thesis;computational complexity theory;conjunctive query;database;description logic;directed acyclic graph;logic programming	Ismail Ilkan Ceylan	2015			description logic;machine learning;conjunctive query;logical reasoning;probabilistic logic;mathematics;logical consequence;artificial intelligence;query optimization;bayesian network;bayesian probability	AI	-21.55140202613784	7.726818961546585	156060
ebee13b0a623a978b05380bed5929d6943ee7143	dynamic complexity under definable changes		In the setting of dynamic complexity, the goal of a dynamic program is to maintain the result of a fixed query for an input database that is subject to changes, possibly using additional auxiliary relations. In other words, a dynamic program updates a materialized view whenever a base relation is changed. The update of query result and auxiliary relations is specified using first-order logic or, equivalently, relational algebra.rnrnThe original framework by Patnaik and Immerman only considers changes to the database that insert or delete single tuples. This article extends the setting to definable changes, also specified by first-order queries on the database, and generalizes previous maintenance results to these more expressive change operations. More specifically, it is shown that the undirected reachability query is first-order maintainable under single-tuple changes and first-order defined insertions, likewise the directed reachability query for directed acyclic graphs is first-order maintainable under insertions defined by quantifier-free first-order queries.rnrnThese results rely on bounded bridge properties, which basically say that, after an insertion of a defined set of edges, for each connected pair of nodes there is some path with a bounded number of new edges. While this bound can be huge, in general, it is shown to be small for insertion queries defined by unions of conjunctive queries. To illustrate that the results for this restricted setting could be practically relevant, they are complemented by an experimental study that compares the performance of dynamic programs with complex changes, dynamic programs with single changes, and with recomputation from scratch.rnrnThe positive results are complemented by several inexpressibility results. For example, it is shown that—unlike for single-tuple insertions—dynamic programs that maintain the reachability query under definable, quantifier-free changes strictly need update formulas with quantifiers.rnrnFinally, further positive results unrelated to reachability are presented: it is shown that for changes definable by parameter-free first-order formulas, all LOGSPACE-definable (and even AC1-definable) queries can be maintained by first-order dynamic programs.	directed acyclic graph;first-order predicate;graph (discrete mathematics);quantifier (logic);reachability	Thomas Schwentick;Nils Vortmeier;Thomas Zeume	2017	ACM Trans. Database Syst.	10.4230/LIPIcs.ICDT.2017.19	directed acyclic graph;database;tuple;relational algebra;discrete mathematics;materialized view;conjunctive query;theoretical computer science;scratch;computer science;reachability;bounded function	DB	-23.592135354848544	11.647899047040795	156175
444ef3f1457fea4066e738a88412877510fcad7a	distribution and modularity in nonmonotonic logic programming.		In the recent years, there has been a trend towards considering computation in a distributed setting, due to the fact that increasingly not only data is linked via media such as the internet, but also computational entities which process and exchange data and knowledge. This leads to the formation of (possibly complex) systems of interlinked entities, based on possibly heterogenous formalisms, posing challenging issues on semantics and computation. The concept of modularity, which in computer science and engineering is a key to structured program development, naturally links to this as a tool for defining semantics of distributed systems, and has been widely studied, e.g., in the area of ontologies. In line with the general development, distribution and modularity have been also been receiving increased attention in logic programming, at several levels of language expressiveness, from distributed (plain) datalog to advanced nonmonotonic logic programming semantics. In this talk, we shall address the issue of distribution and modularity for logic programming under the answer set semantics, which is one of the most widely used semantics for nonmontonic logic programs do date and at the heart of the Answer Set Programming paradigm for declarative problem solving. It appeared that the issue of modularity for answer set semantics is nontrivial, due to its nonmonotonicity. For the same reason, also the issue of efficient distributed evaluation, assuming a reasonable behavior of the semantics for a program composed of distributed modules, is a challenging problem. We shall discuss these issues, pointing out that modularity and distribution admit different solutions for semantics, depending on the underlying view of a system of logic programs. We then illustrate this view on particular formalisms that have been developed at the Vienna University of Technology in the last years, including modular nonmonotonic logic programs (Modular ASP) and nonmonotonic multi-context systems (MCS). For these formalisms, various semantics have been developed, as well as experimental prototype implementations that take local or distributed evaluation into account, adopting different realization schemes. While considerable progress has been achieved, further work is needed to arrive at highly efficient solvers.	answer set programming;computation;computer science;datalog;distributed computing;entity;formal system;internet;logic programming;modularity (networks);non-monotonic logic;ontology (information science);problem solving;programming paradigm;prototype;stable model semantics	Thomas Eiter	2012			theoretical computer science;implementation;datalog;semantics;modular design;modularity;non-monotonic logic;answer set programming;logic programming;computer science	AI	-21.294847502119556	8.672478187978369	156261
e5d6f9f737af5d4d4d05477da51975755b5854d0	representation and reasoning on rbac: a description logic approach	modelizacion;controle acces;representacion conocimientos;decision function;securite informatique;logica descripcion;fonction decision;role based access control;commande repartie;computer security;modelisation;large scale;lenguaje descripcion;funcion decision;seguridad informatica;representation connaissance;access control;control repartido;description logic;knowledge representation;modeling;distributed control;langage description;logique description;description language	Role-based access control (RBAC) is recognized as an excellent model for access control in large-scale networked applications. Formalization of RBAC in a logical approach makes it feasible to reason about a specified policy and verify its correctness. We propose a formalization of RBAC by the description logic language ALCQ. We also show that the RBAC constraints can be captured by ALCQ. Furthermore, we demonstrate how to make access control decision, perform the RBAC functions as well as check the consistency of RBAC via the description logic reasoner RACER.	authorization;cardinality (data modeling);correctness (computer science);description logic;jing;role-based access control;semantic reasoner;semantics (computer science);united nations university institute on computing and society	Chen Zhao;NuerMaimaiti Heilili;Shengping Liu;Zuoquan Lin	2005		10.1007/11560647_25	knowledge representation and reasoning;description logic;systems modeling;computer science;artificial intelligence;access control;role-based access control;database;computer security;algorithm	Security	-21.879133520981643	11.469391613589725	156437
5fb31120e220096696f5e99b48184c89a6db727d	reasoning with concept diagrams about antipatterns in ontologies		Ontologies are notoriously hard to define, express and reason about. Many tools have been developed to ease the ontology debugging and reasoning, however they often lack accessibility and formalisation. A visual representation language, concept diagrams, was developed for expressing ontologies, which has been empirically proven to be cognitively more accessible to ontology users. In this paper we answer the question of “How can concept diagrams be used to reason about inconsistencies and incoherence of ontologies?”. We do so by formalising a set of inference rules for concept diagrams that enables stepwise verification of the inconsistency and incoherence of a set of ontology axioms. The design of inference rules is driven by empirical evidence that concise (merged) diagrams are easier to comprehend for users than a set of lower level diagrams that are a one-to-one translation from OWL ontology axioms. We prove that our inference rules are sound, and exemplify how they can be used to reason about inconsistencies and incoherence.		Zohreh Shams;Mateja Jamnik;Gem Stapleton;Yuri Sato	2017		10.1007/978-3-319-62075-6_18	debugging;rule of inference;web ontology language;data mining;natural language processing;axiom;ontology;ontology (information science);empirical evidence;artificial intelligence;computer science	AI	-22.76007938131392	9.594925366794609	156445
743f6367ce28402beb4314fcfe7cae6c41b57198	data mining query languages	query language;integrable system;association rules;data mining;association rule mining;query languages;association rule;inductive databases;data mining algorithm;knowledge discovery process	Many Data Mining algorithms enable to extract different types of patterns from data (e.g., local patterns like itemsets and association rules, models like classifiers). To support the whole knowledge discovery process, we need for integrated systems which can deal either with patterns and data. The inductive database approach has emerged as an unifying framework for such systems. Following this database perspective, knowledge discovery processes become querying processes for which query languages have to be designed. In the prolific field of association rule mining, different proposals of query languages have been made to support the more or less declarative specification of both data and pattern manipulations. In this chapter, we survey some of these proposals. It enables to identify nowadays shortcomings and to point out some promising directions of research in this area.	algorithm;application domain;association rule learning;data mining;database;general-purpose modeling;generic programming;han unification;hoc (programming language);inductive reasoning;interoperability;predictive model markup language;query language;word lists by frequency;xml	Jean-François Boulicaut;Cyrille Masson	2005		10.1007/978-0-387-09823-4_33	query optimization;association rule learning;computer science;data science;data mining;database;data stream mining;k-optimal pattern discovery;query language	ML	-33.01565443862355	6.604291661944787	156614
4db8e45dd00d4ae91bc4ca4d65ea2c3e48911580	extending a natural language interface with geospatial queries	lenguaje natural;natural language interfaces;query language;grammar extensions;menu based natural language interface;query processing;relational database management systems architectural perspectives natural language geospatial queries sql;sql;grammar extensions architectural perspectives menu based natural language interface geospatial queries sql relational database query language database schemas spatial operators spatial indexes inexperienced users;inexperienced users;langage naturel;distributed computing;spatial index;relational database;lenguaje interrogacion;architectural perspectives;spatial indexes;grammars;internet;natural languages relational databases cities and towns database languages spatial databases design automation geographic information systems spatial indexes buildings roads;geographic information systems;natural language;sql relational database query language;spatial operators;relational database management system;calculo repartido;natural language interface;langage interrogation;computational linguistics;visual databases computational linguistics geographic information systems grammars natural language interfaces query processing;database schemas;calcul reparti;geospatial queries;relational database management systems;visual databases	In this installation of architectural perspectives, we describe an extension of a menu-based natural language interface (MBNLI) to support geospatial queries. Our extension makes it easier for application analysts and even inexperienced users to phrase complex queries without knowing the relational database query language SQL, database schemas (table structures), spatial operators, or spatial indexes.	database schema;experience;natural language user interface;query language;relational database;sql	Vinitha Reddy;Kyle Neumeier;Joshua McFarlane;Jackson D. Cothren;Craig W. Thompson	2007	IEEE Internet Computing	10.1109/MIC.2007.124	natural language processing;data definition language;relational database management system;computer science;computational linguistics;database;conjunctive query;programming language;world wide web;spatial database;spatial query	DB	-31.731479466640295	8.715667185836551	156719
63587a1595c80d98123bd4d806bc319af8323c7e	a cloud computing based framework for storage and processing of meteorological data		This document shows an analysis of emerging technology for the recovery of meteorological data and its cost-benefit using GPRS (General Packet Radio Service) data transfer in automatic meteorological stations to improve the monitoring and the prediction of the atmosphere and inland water behavior in Ecuador. In different areas of study comparisons between data or generated registers coming from Automatic Weather Station (AWS) and Conventional Weather Station (CWS) have been made. Therefore, here the authors mainly underline the importance of storing meteorological information using cloud computing. Among the benefits of cloud computing there are high data availability access and high efficiency in technical/scientific studies at lower cost due to the decrease of local investment in technological infrastructure, upgrades, maintenance of equipment and applications.	cloud computing	Maritza Aguirre-Munizaga;Raquel Gómez-Chabla;María Aviles;Mitchell Vásquez;G. Cristina Recalde-Coronel	2016		10.1007/978-3-319-48024-4_8	data mining;database	HPC	-29.46179872706006	17.59831168207986	156974
78b911361eee378ba7adb3608f0f1df433322a58	using contextually closed queries for local closed-world reasoning in rough knowledge databases	computer science	Representing internal models of aspects of an autonomous ag ent’s surrounding environment or of its own epistemic state and developing que ry mechanisms for these models based on efficient forms of inference are fundamental compon ents in any deliberative/reactive system architecture used by an agent in achieving task goals . The problem is complicated by the fact that the models in question necessarily have to be in complete due to the complexity of the environments in which such agents are intended to oper ate. Consequently, the querying mechanisms must be framed in the context of an open-world assumption . We propose an architecture for such a system that involves generalizing cla ssic l deductive databases to rough knowledge databases (RKDB), where relations in the databas e are defined as rough sets. We also propose the use of contextually closed queries (CCQs) where a context for a query and a local minimization policy are provided in terms of integrit y constraints and techniques from circumscription. The concept of a contextually closed quer y is a generalization of querying in the context of a local closed-world assumption ( LCW) previously proposed in the literature.CCQs have the effect of dynamically reducing the boundary regio ns of relations relative to a particular set of integrity constraints associated wit h the query before actually querying the RKDB. The general problem of querying the RKDB using CCQs is co-NPTIME complete, but we isolate a number of important practical cases w here polynomial time and space complexity is achieved.	aerial photography;autonomous robot;belief revision;boolean satisfiability problem;bus (computing);circumscription (logic);cisco ios;closed-world assumption;complete (complexity);dspace;data integrity;deductive database;experiment;global optimization;intensional logic;knowledge base;online and offline;open world;open-world assumption;precondition;prototype;question answering;real-time computing;real-time transcription;relational database;rough set;systems architecture;time complexity;unmanned aerial vehicle	Patrick Doherty;Jaroslaw Kachniarz;Andrzej Szalas	2004			computer science;artificial intelligence;machine learning;data mining	AI	-20.678128525349848	10.090719459452256	157058
9fd773d1a7f17a464867563455af3a38de95b5d5	designing an extensible query language for data mining	query language;data mining	Aryl and heteroaryl sulfonimidamides which also contain a pyrimidine or triazine heterocycle are useful as general or selective preemergent and postemergent herbicides.	data mining;query language	Afshin Salajegheh;Mohammad Reza Kangavari	2008			query expansion;web query classification;web search query;rdf query language;data control language;object query language;query language;data mining;query optimization;computer science	ML	-32.3857261542965	7.276215711164321	157121
6a4da11c8e53b57b3d7ad3eda85bc7f29d9a386e	safe computation of the well-founded semantics of datalog queries	base relacional dato;regle inference;semantica formal;funcion logica;interrogation base donnee;interrogacion base datos;negation;enregistreur donnee;logical programming;formal semantics;relational database;logical function;fonction logique;semantique formelle;inference rule;well founded semantics;programmation logique;query evaluation;base donnee relationnelle;registrador datos;negacion;programacion logica;database query;regla inferencia;data logger;deductive databases	A Datalog program (abbreviated to “program” hereafter) is a set of function-free logic rules allowing negation in their bodies. It is customary to see a program as composed of two parts: a set of rules (the Intentional Database, IDB for short) and a set of facts stored as tuples of a relational database (the Extentional Database, EDB for short). The well-founded semantics assigns to every program a (possibly partial) model to be thought of as the meaning of the program. Although the well-founded computation is effective for (function-free) programs, it is worth noting that the construction of the greatest unfounded set results in a very demanding task when large EDBs are considered (data-intensive applications). In this paper we describe a method for the evaluation of the well-founded semantics of Datalog queries that allows us to construct answers without having to compute the whole greatest unfounded set. The key idea is that of restricting the computation of false facts to only those that are “relevant” to draw positive conclusions. We call this set of facts the Greatest Useful Unfounded Set (GUUS), from which the choice of the name GUUS method. The method is shown to be sound and complete w.r.t. any Datalog query.#R##N##R##N#A main advantage of the proposed approach is that of resulting efficiently implementable for a meaningful class of programs. The syntactic structure of such programs guarantees that the greatest useful unfounded set can be safely (and efficiently) computed as the least fixpoint of the immediate consequence operator applied to a suitable rewritten program.	computation;datalog;well-founded semantics;whole earth 'lectronic link	Nicola Leone;Pasquale Rullo	1992	Inf. Syst.	10.1016/0306-4379(92)90003-6	relational database;computer science;data logger;negation;formal semantics;data mining;database;programming language;algorithm;rule of inference	DB	-21.541309178417052	11.911639788481944	157179
dfdab93ce9dc12a6de73970854e7861357177a01	applying an extended relational model to indefinite deductive databases	relational model;deductive databases		relational database management system;relational model	Ken-Chih Liu;Rajshekhar Sunderraman	1987			relational model;codd's theorem;relational theory;relational calculus;relational database;computer science;conjunctive query	DB	-30.16984334198179	9.109460489149699	157399
4027ccdd0c9593cdfafa1259cca3e86fba70dfa1	updating xml	feature transformation;data mining;high dimensional space;similarity join;similarity search;knowledge discovery	As XML has developed over the past few years, its role has expanded beyond its original domain as a semantics-preserving markup language for online documents, and it is now also the de facto format for interchanging data between heterogeneous systems. Data sources expert XML “views” over their data, and other system can directly import or query these views. As a result, there has been great interest in languages and systems for expressing queries over XML data, whether the XML is stored in a repository or generated as a view over some other data storage format. Clearly, in order to fully evolve XML into a universal data representation and sharing format, we must allow users to specify updates to XML documents and must develop techniques to process them efficiently. Update capabilities are important not only for modifying XML documents, but also for propagating changes through XML view and for expressing and transmitting changes to documents. This paper begins by proposing a set of basic update operations for both ordered and unordered XML data. We next describe extensions to the proposed standard XML query language, XQuery, to incorporate the update operations. We then consider alternative methods for implementing update operations when the XML data is mapped into a relational database. Finally, we describe an experimental evaluation of the alternative techniques for implementing our extensions.	computer data storage;data (computing);markup language;query language;relational database;transmitter;xml;xquery	Igor Tatarinov;Zachary G. Ives;Alon Y. Halevy;Daniel S. Weld	2001		10.1145/375663.375720	xml catalog;xml validation;binary xml;xml encryption;simple api for xml;xml;xml schema;streaming xml;computer science;document type definition;document structure description;xml framework;data mining;xml database;xml schema;database;knowledge extraction;nearest neighbor search;xml signature;xml schema editor;cxml;information retrieval;efficient xml interchange;sgml	DB	-33.156077980820314	5.23503471851303	157454
011d109c18dae433887ec6123bf7e34810291a40	representation and reasoning on role-based access control policies with conceptual graphs	modelizacion;graph theory;controle acces;regle inference;representation graphique;representacion conocimientos;teoria grafo;conceptualization;securite informatique;conceptual analysis;role based access control;theorie graphe;analisis conceptual;conceptualizacion;inference rule;computer security;modelisation;access control policy;conceptual graph;graphical representation;seguridad informatica;access control models;representation connaissance;grafo conceptual;grafo curva;access control;analyse conceptuelle;knowledge representation;modeling;graphe conceptuel;conceptualisation;graphics;regla inferencia	This paper focused on two aspects of access control: graphical representation and reasoning. Access control policies describe which operations on resources are granted to users. Role-based access control is the model which introduces the concept of role to design user’ permissions. Actually, there is a lack of tools allowing security officers to describe and reason on their policies graphically. Thanks to conceptual graphs, we can provide a consistent graphical formalism for role-based access control policies, able to deal with specificities of this model such as role hierarchy and constraints. Moreover, once a policy modelled within CGs, graph rules and chainings can be used to reason on it. Thus, allowing SOs to understand why (through wich role assignment) user’ permissions are granted and to find constraints violated by assignments.	conceptual graph;formal system;graphical user interface;knowledge representation and reasoning;role hierarchy;role-based access control;visual programming language	Romuald Thion;Stéphane Coulondre	2006		10.1007/11787181_31	conceptual graph;conceptualization;systems modeling;computer science;graphics;artificial intelligence;access control;graph theory;theoretical computer science;role-based access control;data mining;database;computer security;algorithm;rule of inference	Security	-31.877919337981375	15.534114457281031	157728
1600e8c878a2f42c71c753a353b8554b37ecf093	answering order-based queries over xml data	order based queries;holistic algorithms;xml	Order-based queries over XML data include XPath navigation axes such as following-sibling and following. In this paper, we present holistic algorithms that evaluate such order-based queries. An experimental comparison with previous approaches shows the performance benefits of our algorithms.	algorithm;holism;xml;xpath	Zografoula Vagena;Nick Koudas;Divesh Srivastava;Vassilis J. Tsotras	2005		10.1145/1062745.1062919	xml validation;xml;streaming xml;computer science;xml framework;data mining;xml database;xml schema;database;xml signature;information retrieval;efficient xml interchange	DB	-32.59818971130489	5.593040786543504	157856
1c7e8338ba0a586c91bc149cf0aa553dc338afba	external sources of knowledge and value invention in logic programming	artificial intelligent;logic programming;answer set semantics;object oriented;value invention;polynomial time;artificial intelligence;logic programs;68t30;68t27	The issue of value invention in logic programming embraces many scenarios, such as logic programming with function symbols, object oriented logic languages, inter-operability with external sources of knowledge, or set unification. This work introduces a framework embedding value invention in a general context. The class of programs having a suitable (but, in general, not decidable) ‘finite grounding property’ is identified, and the class of ‘value invention restricted’ programs is introduced. Value invention restricted programs have the finite grounding property and can be decided in polynomial time. They are a very large polynomially decidable class having this property, when no assumption can be made about the nature of invented values (while this latter is the case in the specific literature about logic programming with function symbols). Relationships with existing formalisms are eventually discussed, and the implementation of a system supporting the class of such programs is described.	algorithm;answer set programming;antistatic device;dlv;disjunctive normal form;interoperability;knowledge representation and reasoning;logic programming;mathematical optimization;operability;semantic web;time complexity;unification (computer science)	Francesco Calimeri;Susanna Cozza;Giovambattista Ianni	2007	Annals of Mathematics and Artificial Intelligence	10.1007/s10472-007-9076-z	dynamic logic;time complexity;description logic;logic optimization;horn clause;stable model semantics;computer science;artificial intelligence;theoretical computer science;machine learning;functional logic programming;computational logic;mathematics;signature;inductive programming;programming language;object-oriented programming;prolog;logic programming;algorithm;autoepistemic logic	AI	-20.779045018171804	9.837748844043587	157886
1a932cb702fceef17cb7fbf25d2aafe2197f49c9	super-recursive algorithms and modes of computation	inductive computation;super recursive algorithms;limit computation;periodic computation;recursive algorithm	According to the contemporary computer science, working in the functional recursive mode, computers and computer networks function as recursive algorithms. At the same time, working in the functional super-recursive mode, such as inductive or limit modes, computers and computer networks function as super-recursive algorithms (Burgin, 2005). While one group of notable researchers claims that interactive computation is more powerful than Turing machines (cf., for example, (Wegner, 1997)), others argue that the Church-Turing Thesis, which equates algorithms with Turing machines, still holds and that interaction does not add anything new (cf., for example, (Prasse and Rittgen, 1998) and (Van Leeuwen and Wiedermann, 2000). The cause of this misunderstanding is that the standard computability theory does not take into account time and space where real computers and networks function. Under such artificial conditions, interacting abstract automata and algorithms cannot achieve super-recursive power if they are all recursive as it is proved in (Burgin, 2006). In contrast to this, even a finite system of interacting recursive automata or algorithms functioning in the real time and space can become super-recursive (Burgin, 2007).In this paper, we study modes of information processing in abstract automata, material, e.g., physical or biological, computers and networks. Computational and networking practice shows that taking into account modes of information processing is important for efficient design of distributed hardware and software systems. Modes of computation studied here for abstract automata are actually directions and explanations of how to utilize computers and network computations.	automata theory;automaton;church–turing thesis;computability theory;computer science;information processing;interaction;interactive computation;software system;super-recursive algorithm;turing machine	Mark Burgin	2015		10.1145/2797433.2797443	model of computation;theory of computation;computer science;theoretical computer science;recursive language;distributed computing;programming language;algorithm;recursion;super-recursive algorithm	PL	-20.924083977573172	14.050215433199025	158024
301bfbddb616e61aac50bd9338b1368596f78b9f	representing sequences in description logics using suffix trees	suffix tree;description logic	This paper describes an approach for representing and manipulating sequences in description logics (DLs). The key idea is to represent sequences u ing suffix trees, then represent the resulting trees in a DL using traditional (tractable) concept and role operators. This approach supports the representation of a range of types of information about a sequence, such as the locations and numbers of occurrences of all subsequences of the sequence. Moreover, subsequence t sting and pattern matching reduces to subsumption checking in this representation, and computing the least common subsumer of two terms supports the application of inductive learning to sequences. Finally, we describe a simple addition to our approach, using the same set of DL operators, that extends our representation to handle additional types of information, such as sequence l ngths and the existence and number of occurrences of palindromes in a sequence.	cobham's thesis;description logic;lowest common ancestor;sting;subsumption architecture;suffix tree	Daniel Kudenko;Haym Hirsh	1996			combinatorics;discrete mathematics;mathematics;algorithm	AI	-21.30573764517955	12.410837931335509	158135
1fabd706cdbca6eed7d652f7aba28a23f0748f11	a systematic approach to selecting maintenance policies in a data warehouse environment	tiempo respuesta;base donnee;exigence usager;exigencia usuario;database;base dato;response time;stockage donnee;qualite service;temps reponse;data storage;specification donnee;user requirement;data warehousing;especificacion datos;user requirements;almacenamiento datos;data warehouse;service quality;cost model;data specification;calidad servicio	Most work on data warehousing addresses aspects related to the internal operation of a data warehouse server, such as selection of views to materialise, maintenance of aggregate views and performance of OLAP queries. Issues related to data warehouse maintenance, i.e. how changes to autonomous sources should be detected and propagated to a warehouse, have been addressed in a fragmented manner. Although data propagation policies, source database capabilities, and user requirements have been addressed individually, their co-dependencies and relationships have not been explored. In this paper, we present a comprehensive framework for evaluating data propagation policies against data warehouse requirements and source capabilities. We formalize data warehouse specification along the dimensions of staleness, response time, storage, and computation cost, and classify source databases according to their data propagation capabilities. A detailed cost-model is presented for a representative set of policies. A prototype tool has been developed to allow an exploration of the various trade-offs.	aggregate data;autonomous robot;benchmark (computing);computation;database;hoc (programming language);mathematical optimization;online analytical processing;prototype;quality of service;requirement;response time (technology);selection algorithm;server (computing);software propagation;testbed;user requirements document	Henrik Engström;Sharma Chakravarthy;Brian Lings	2002		10.1007/3-540-45876-X_22	dimensional modeling;computer science;user requirements document;data warehouse;data mining;database;world wide web	DB	-26.29832350703532	4.37749259199478	158364
36d92fbfc67f13dca61d0b81b670aa4d5b08d86c	set-term matching in a logic database language	logic programming;matching;unification	Existing set-term matching algorithms [l] for logic-based database languages, of which set terms have the commutative and idempotent properties, have several problems: Given a pair of set terms, matchers can only be computed uequentiollyand duplicatedmatchers are generated. Hence, these algorithms cannot take the advantage of existing multiple processors for computing all matchers in parallel. Further, duplicated matchers are redundant and undesirable. In order to overcome these shortcomings, we propose an improved set-term matching approach for LDL/NR, a logic database language for nested relations, which generates non-redundant matchers for a given pair of set terms in parallel. The approach thus solves the sequential and redundant problems in [I].	algorithm;central processing unit;cholesky decomposition;formal language;idempotence;noise reduction;numerical recipes;query language	Seung Jin Lim;Yiu-Kai Ng	1995			natural language processing;data definition language;database theory;description logic;data manipulation language;object language;data control language;database;signature;programming language;database schema;database design	DB	-28.28924513195024	8.798918489584327	158390
7024bcbf0f4a77d45268343fad4a8aa29d54b331	improving efficiency of xpath-based xml querying		XML is becoming the de facto standard for information exchange. XML querying is a key component for structured information processing and plays a central role in the next generation world wide web, information management systems and databases. Applications relying on XML processing notably depend on XPath, the standard language for adressing parts of XML documents. Besides its fundamental functionality, reasons behind XPath success include being widely accepted by programmers and well-suited for formal treatments. With the growing volume of XML content and XML processing applications, our research is oriented toward efficiency of XML querying. Our approach relies on analysis and transformation of XPath expressions for optimization. This note presents current open issues with XPath, and introduces our preliminary results applied to streaming XPath processing. Moreover, it describes our methodology, which includes XPath modelisation using the Coq proof assistant, and future directions envisioned toward high performance XML querying.	coq (software);database;information exchange;information management;information processing;logical framework;mathematical optimization;programmer;proof assistant;world wide web;xml;xpath	Pierre Genevès	2004			xml validation;binary xml;xml encryption;simple api for xml;xml;processing instruction;xslt;xml schema;streaming xml;computer science;xpath 2.0;document structure description;xml framework;data mining;xml database;xml schema;database;schematron;xml signature;xml schema editor;information retrieval;efficient xml interchange	DB	-32.23319254492381	7.593010797910727	158423
e43c9e0c079810a013f10ade3b3219ae9b980999	text / relational database management systems: harmonizing sql and sgml	relational database;relational database management system	"""Combined text and relational database support is increasingly recognized as an emerging need of industry, spanning applications requiring text elds as parts of their data (e.g., for customer support) to those augmenting primary text resources by conventional relational data (e.g., for publication control). In this paper, we propose extensions to SQL that provide exible and eecient access to structured text described by SGML. We also propose an architecture to support a text/relational database management system as a federated database environment , where component databases are accessed via \agents"""": SQL agents that translate standard or extended SQL queries into vendor-speciic dialects, and text agents that process text sub-queries on full-text search engines."""	customer support;federated database system;file spanning;primary source;relational database management system;sql;standard generalized markup language;structured text;web search engine	G. Elizabeth Blake;Mariano P. Consens;Pekka Kilpeläinen;Per-Åke Larson;T. Snider;Frank Wm. Tompa	1994		10.1007/3-540-58183-9_54	data definition language;sql;relational database management system;nested set model;relational model;stored procedure;entity–relationship model;relational database;database model;data mining;database;alpha;database schema;information retrieval;object-relational impedance mismatch;database design	DB	-33.089603425079936	8.598845411802287	158493
195381951bfce89f2e26d24a5cf8eef4920b1329	reasoning with concept diagrams about antipatterns		Ontologies are notoriously hard to define, express and reason about. Many tools have been developed to ease the debugging and the reasoning process with ontologies, however they often lack accessibility and formalisation. A visual representation language, concept diagrams, was developed for expressing and reasoning about ontologies in an accessible way. Indeed, empirical studies show that concept diagrams are cognitively more accessible to users in ontology debugging tasks. In this paper we answer the question of “ How can concept diagrams be used to reason about inconsistencies and incoherence of ontologies?”. We do so by formalising a set of inference rules for concept diagrams that enables stepwise verification of the inconsistency and/or incoherence of a set of ontology axioms. The design of inference rules is driven by empirical evidence that concise (merged) diagrams are easier to comprehend for users than a set of lower level diagrams that offer a one-to-one translation of OWL ontology axioms into concept diagrams. We prove that our inference rules are sound, and exemplify how they can be used to reason about inconsistencies and incoherence. Finally, we indicate how our rules can serve as a foundation for new rules required when representing ontologies in diverse new domains.		Zohreh Shams;Mateja Jamnik;Gem Stapleton;Yuri Sato	2017			computer science;theoretical computer science;systems engineering	AI	-22.6887274590429	9.63023166879298	158523
31208d26bc01e0baba1ba200336d9cb90e19a1df	ictrl: intensional conformal text representation language	information retrieval system;information retrieval;first order;knowledge acquisition;natural language;profitability;classical logic;knowledge representation;knowledge base	Abstract   A new compact and homogeneous symbolism is introduced to achieve a more general and exact representation of natural language texts. Traditional first-order and intensional logic cannot cope with numerous natural language phenomena such as the large variety of modalities, satisfactory interpretation of iterative application of modal operators or certain modelling problems like one-to-one sentence–formula mapping. The CTRL/iCTRL formalism can model them successfully and they are able to control many other different shades of meaning by applying only a minimal number of syntactic tools.  The most profitable and beneficial AI application of the presented natural language syntax consistent knowledge representation technique is automated knowledge acquisition: computer-aided textual data base generation and logical inference based information retrieval. CTRL/iCTRL applicability is demonstrated by various illustrative examples including a transparent graphical interpretation analogous to Frege's graph language that help clarify new concepts and exemplify partial inappropriateness of traditional logical language.  The CTRL/iCTRL paradigm is based on a novel and interesting synthesis of the two traditional logic schools, the Stoic and the Peripatetic school, refuting a century long scientific prejudice against the latter stated to be completely outworn. An interesting issue of this analysis points out that expressing subordination unconsciously and simply by co-ordination causes a typical restriction of meaning in classical logic.	intensional logic	Gábor Rédey	1999	Artif. Intell.	10.1016/S0004-3702(99)00016-8	natural language processing;knowledge representation and reasoning;knowledge base;classical logic;universal networking language;question answering;computer science;artificial intelligence;first-order logic;mathematics;natural language;algorithm;profitability index	NLP	-19.671442038839757	6.16063829253067	158580
90bbeb85cee275b0e1224dbdcbbdce50be290859	a declarative approach to the dynamics of information systems	information system		declarative programming;information system	Frans Van Assche;Pericles Loucopoulos;G. Speltincx;Raf Venken	1988			information system;natural language processing;computer science;artificial intelligence	Robotics	-31.454421222423882	10.251631191349434	158750
66c3a46fcf811e24ddbc1792d9b893e7028c6a0d	an adaptive join strategy in distributed data stream management system	distributed data;management system;delta sigma modulation;information security;data stream;computational intelligence;continuous query;conference management;data model;delta sigma modulation data models conference management engineering management costs h infinity control computational intelligence data security information security educational institutions;engineering management;h infinity control;cost model;data models;data security	As data stream springs up in various areas, distributed data stream management systems are being paid more and more attention. In DSMS, join is one of the most common but complicated operators, and the efficiency of continuous queries may be influenced by join directly. This paper mainly studies the join operation over data stream located on two different sites in WAN. Firstly, data model, query model and cost model are defined, and then a simple direct-join strategy and a semi-join strategy are proposed, which are respectively ideal under certain conditions but can't adapt to the variation of data stream. Finally, based on the analysis of the two strategies, an optimized adaptive join strategy is presented. The efficiency and flexibility of our strategy is proved by extensive experiments. Key words: distributed data stream management, continuous query, adaptive join.		Tianjie Cao;Shi Huang;Hui Cui;Yipeng Wu;Qihan Luo	2007		10.1109/CIS.2007.79	data modeling;data model;computer science;artificial intelligence;information security;theoretical computer science;delta-sigma modulation;machine learning;computational intelligence;data mining;management system;database;data security;data stream mining;computer security;algorithm	DB	-28.527942938389895	6.039776921152818	158958
1f30d486d6421de177669f1fd40c48878209a7ee	geographic information systems: are they decision support systems?	decision support;decision aiding;information systems;geographic information system;decision aid;spatial data;earth;resource management;model management component;dss components model;spatially dependent decisions;spatial data structures geographic information systems decision support systems;decision support system;mainstream business applications;geographic information systems decision support systems resource management information systems power system management earth power system modeling management information systems costs workstations;geographic information systems;power system management;temporally dependent decisions;workstations;model management component geographic information systems decision support systems spatial data descriptive data mainstream business applications spatially dependent decisions temporally dependent decisions dss components model decision aiding;decision support systems;descriptive data;management information systems;component model;spatial data structures;model management;information system;power system modeling	Geographic Information Systems (GIS) are a special case of information systems with a capability to integrate spatial and descriptive data. As GIS enter into mainstream business applications, understanding how they are similar and different from more familiar information systems such as DSS can be usefil. An attempt is made to converge on a definition of GIS. The potential for many decisions to be viewed as spatially (and temporally) dependent is noted. Special analysis issues and problems associated with spatial data are discussed. DSS are deBned relative to Sprague”s cktssic 1980 DSS components model; it is concluded that specialized decision support applications exist for decision aiding but that the model management component for true DSS is missing. Cross-fertilization from DSS & GIS research is discussed along with implications for IS management of	converge;decision support system;geographic information system;temporal logic	Lisa D. Murphy	1995		10.1109/HICSS.1995.375736	enterprise gis;decision support system;computer science;knowledge management;artificial intelligence;gis and public health;data mining;management science;information system;statistics	DB	-33.27936978095309	13.79607633931927	159122
8a9c7e51efbed0bf0f3c2c4b3d5a84ca8fcfd073	invited paper: consistent query answering: opportunities and limitations	databases;expert systems;remuneration;special issues and sections;special issues and sections databases remuneration computer science data engineering lead data mining expert systems context modeling;data engineering;data mining;lead;computer science;context modeling	This paper briefly reviews the recent literature on consistent query answering, an approach to handle database inconsistency in a systematic and logical manner based on the notion of repair. It discusses some computational and semantic limitations of consistent query answering, and summarizes selected research directions in this area.		Jan Chomicki	2006	17th International Workshop on Database and Expert Systems Applications (DEXA'06)	10.1109/DEXA.2006.76	lead;information engineering;computer science;data mining;database;context model;expert system;information retrieval	DB	-22.028761940621845	7.171178654687484	159298
5d018a99e8b69f75e0bccc69ce1946f3231f9bc1	deductive spreadsheets using tabled logic programming	controle acces;confiance;sistema experto;psychologie sociale;spreadsheet;rule based;securite informatique;trust management;tipo dato;gestion configuration;base connaissance;logical programming;data type;prise decision;vista materializada;materialized view;computer security;confidence;datalog;metafora;confianza;programmation logique;design and implementation;seguridad informatica;psicologia social;base conocimiento;social psychology;access control;systeme expert;logic programs;type donnee;toma decision;metaphor;programacion logica;tableur;configuration management;metaphore;knowledge base;expert system;vue materialisee	Rule-based specifications in Datalog are used in a number of application areas, such as configuration management, access control and trust management, decision making, etc. However, rules sets are typically hard to maintain; the rules often interact in subtle ways, making them difficult to understand and reason about. This has impeded the wide-spread adoption of rule-based computing. This paper describes the design and implementation of XcelLog, a deductive spreadsheet system (DSS), that permits users to specify and maintain Datalog rules using the popular and easy-to-use spreadsheet interface. The driving idea underlying the system is to treat sets as the fundamental data type and rules as specifying relationships among sets, and use the spreadsheet metaphor to create and view the materialized sets. The fundamental feature that makes XcelLog suitable even for non-programmers is that the user mainly sees the effect of the rules; when rules or basic facts change, the user sees the impact of the change immediately. This enables the user to gain confidence in the rules and their modification, and also experiment with what-if scenarios without any programming. XcelLog is implemented as an add-in to Excel with XSB serving as the rule engine for evaluating Datalog specifications. Preliminary experience with using XcelLog indicates that it is indeed feasible to combine the power of rule-based computing and the elegance and simplicity of the spreadsheet metaphor, so that end users can encode and maintain rule bases with little or no programming.	access control;business rules engine;configuration management;datalog;deductive database;encode;logic programming;plug-in (computing);programmer;software maintenance;spreadsheet;trust management (information system);xsb	C. R. Ramakrishnan;I. V. Ramakrishnan;David Scott Warren	2006		10.1007/11799573_29	materialized view;knowledge base;data type;computer science;artificial intelligence;access control;data mining;configuration management;confidence;datalog;programming language;expert system;algorithm	HCI	-33.05737786425081	15.950337871582699	159344
f9ff0d1a55f51e0abca041fbcf6a5225d053b0b6	a methodology for interpreting tree queries into optimal semi-join expressions	databases;microprocessors;multiple language;multiple indexes;vsam;distributed database system;files;concurrent update;indexed sequential;portable;isam	In this paper we outline a general methodology illustrated by specific examples for the optimal interpretation of a class of semi-join queries in distributed database systems.	distributed database;join (sql);regular expression;semiconductor industry	D. M. Chiu;Yu-Chi Ho	1980		10.1145/582250.582277	isam;virtual storage access method;computer file;database storage structures;computer science;data mining;database;programming language	DB	-30.433274336558533	9.539192628513018	159539
a044a7a0adbb2ce8b6e05a7eef82173e2919c9fb	efficient keyword search in fuzzy xml	slca;keyword queries;fuzzy xml	Evaluation of keyword queries over XML documents is one of the most fundamental tasks for XML data management. Previous methods have focused on the processing of deterministic XML data. However, uncertain data are inherent in practical applications, and how to support efficient keyword search over fuzzy XML data remains at large an open problem. In this paper, we tackle the problem of efficiently producing SLCA (smallest lowest common ancestor) results for keyword queries in fuzzy XML documents. We propose an efficient approach that can find all SLCA results for a given keyword query over fuzzy XML data. In particular, we introduce an effective method to transform a simple keyword query into a segmented keyword query that captures the original query requirements and conforms to the underlying fuzzy XML data. The proposed approach could help us eliminate irrelevant SLCA results and speed up the query processing. The final experiments show the effectiveness and efficiency of our proposed approach in generating SLCA results.	search algorithm;xml	Jian Liu;X. X. Zhang	2017	Fuzzy Sets and Systems	10.1016/j.fss.2016.05.015	xml validation;computer science;document structure description;xml framework;data mining;xml database;xml schema;database;keyword density;information retrieval;efficient xml interchange	DB	-31.528623333120905	4.667297189432854	159557
097c038aae62706664480466f9c309b9f0dac9e8	interaction between path and type constraints	type constraint;integrity constraints;world wide web;query optimization;data exchange;type system;data definition language	Path constraints are capable of expressing inclusion and inverse relationships and have proved useful in modeling and querying semistructured data [Abiteboul and Vianu 1999; Buneman et al. 2000]. Types also constrain the structure of data and are commonly found in traditional databases. There has also been work on imposing structure or a type system on semistructured data for, e.g., storing and querying semistructured data in a traditional database system [Alon et al. 2001; Deutsch et al. 1999; Florescu and Kossmann 1999; Shanmugasundaram et al. 1999]. One wants to know whether complexity results for reasoning about path constraints established in the untyped (semistructured) context could carry over to traditional databases, and vice versa. It is therefore appropriate to understand the interaction between types and path constraints. In addition, XML [Bray et al. 1998], which may involve both an optional schema (e.g., DTDs or XML Schema [Thompson et al. 2001]) and integrity constraints, highlights the importance of the study of the interaction. This paper investigates that interaction. In particular it studies constraint implication problems, which are important both in understanding the semantics of type/constraint systems and in query optimization. It shows that path constraints interact with types in a highly intricate way. For that purpose a number of results on path constraint implication are established in the presence and absence of type systems. These results demonstrate that adding a type system may in some cases simplify reasoning about path constraints and in other cases make it harder. For example, it is shown that there is a path constraint implication problem that is decidable in PTIME in the untyped context, but that becomes undecidable when a type system is added. On the other hand, there is an implication problem that is undecidable in the untyped context, but becomes not only decidable in cubic time but also nitely axiomatizable when a type system is imposed.	cubic function;data integrity;database;emoticon;mathematical optimization;mediawiki;model m keyboard;naruto shippuden: clash of ninja revolution 3;p (complexity);query optimization;time complexity;type system;typeof;undecidable problem;victor vianu;word lists by frequency;xml schema	Peter Buneman;Wenfei Fan;Scott Weinstein	1999		10.1145/303976.303982	library science;computer science;operations research	DB	-25.292279659047445	9.939371826823498	159608
14a1dc914f75404a65dea3e0be9cad964af61de5	scalable web reasoning using logic programming techniques	owl;abstract machine;conference paper;large scale;parallelism;logic programming;deri;semantic web;scalability;description logic;logic programs;generic programming;knowledge base;dl	One of the key issues for the uptake of the Semantic Web idea is the availability of reasoning techniques that are usable on a large scale and that offer rich modelling capabilities by providing comprehensive coverage of the OWL language. In this paper we present a scalable extension of our ABox reasoning framework called DLog. DLog performs query-driven execution whereby the terminological part of the description logic knowledge base is converted into a Logic Program and the assertional facts are accessed dynamically from a database. The problem of instance retrieval is reduced to a series of instance checks over a set of individuals containing all solutions for the query. Such a superset is calculated by using static-code analysis on the generated program. We identify two kinds of parallelism within DLog execution: (1) the instances in the superset can be independently checked in parallel and (2) a specific instance check can be executed in parallel by specialising well-established techniques from Logic Programming. Moreover, for efficiency reasons, we propose to use a specialised abstract machine rather than relying on the more generic WAM execution model. We describe the architecture of a distributed framework in which the above mentioned techniques are integrated. We compare our results to existing approaches.	abox;conjunctive query;description logic;discrete logarithm;distributed database;knowledge base;logic programming;parallel computing;peer-to-peer;performance evaluation;reasoning system;scalability;semantic web;single-instance storage;static program analysis;tbox;top-down and bottom-up design;triplestore;warren abstract machine	Gergely Lukácsy;Péter Szeredi	2009		10.1007/978-3-642-05082-4_8	knowledge base;description logic;scalability;computer science;artificial intelligence;theoretical computer science;semantic web;database;abstract machine;programming language;generic programming;logic programming	AI	-24.680055942868233	8.257898764449958	159655
32b0a9bad3938db2a64f92059c55dafc6928d80c	ontology adaptation upon updates		Ontologies, like any other model, change over time due to modifications in the modeled domain, deeper understanding of the domain by the modeler, error corrections, simple refactoring or shift of modeling granularity level. Local changes usually impact the remainder of the ontology as well as any other data and metadata defined over it. The massive size of ontologies and their possible fast update rate requires automatic adaptation methods for relieving ontology engineers from a manual intervention, in order to allow them to focus mainly on high-level inspection. This paper, in spirit of the Principle of minimal change, proposes a fully automatic ontology adaptation approach that reacts to ontology updates and computes sound reformulations of ontological axioms triggered by the presence of certain preconditions. The rule-based adaptation algorithm covers up to SROIQ DL.	algorithm;code refactoring;high- and low-level;logic programming;ontology (information science);precondition	Alessandro Solimando;Giovanna Guerrini	2013		10.1007/978-3-642-41242-4_4	belief revision;metadata;ontology;ontology (information science);code refactoring;theoretical computer science;description logic;granularity;remainder;computer science	AI	-22.947477235036363	13.232909671555587	159828
c413a0639fcfd6099a015d32253dd691c96462c9	index splitting for complex objects in parallel environments	indexation	Many indexing techniques for complex objects have been developed, but most of them are intended for a single-machine environment. By dividing a large index into subindexes and placing each of them on a separate machine, we can get good efficiency of index operations through parallelism. In this paper, we propose an optimizing scheme for horizontal and vertical index splitting by considering parallel processing, assuming a wide variety of multi-processor environments. The optimizing method gives good retrieval query efficiency in the case where an attribute value of a nested object is specified, and it also improves retrieval throughput, that is, the average number of retrieval queries processed within a constant time. In our method, the multi-indexing scheme is used, in which index updating can be performed with minimal cost and index elements can be easily moved across machines.		Kazuhiro Ogura;Tatsuo Tsuji;Teruhisa Hochin	1996	Systems and Computers in Japan	10.1002/(SICI)1520-684X(199708)28:9%3C1::AID-SCJ1%3E3.0.CO;2-I	computer simulation;parallel processing;search engine indexing;relational database;computer science;artificial intelligence;object-oriented programming;information system;algorithm	HPC	-28.31070622946921	4.374386221619135	159944
373940e65cf74e264c56fcdc33474e9f49c0174e	the design and implementation of modularized wrappers/ monitors in a data warehouse	base relacional dato;gestion informacion;information sources;almacenamiento informacion;information source;source information;relational database;information organization;organizacion informacion;information storage;design and implementation;information management;base donnee relationnelle;organisation information;modular design;stockage information;information system;gestion information;data warehouse;object relational;systeme information;fuente informacion;sistema informacion	To simplify the task of constructing wrapper/monitor for the information sources in data warehouse systems, we provide a modularized design method to reuse the code. By substituting some parts of wrapper modules, we can reuse the wrapper on a different information source. For each information source, we also deveIop a toolkit to generate a corresponding monitor. By the method, we can reduce much effort to code the monitor component. We also develop a method to map the object-relational schema into relational one. The mapping method helps us make an uniform interface between a wrapper and an integrator.		Jorng-Tzong Horng;Jye Lu;Baw-Jhiune Liu;Ren-Dar Yang	1999		10.1007/3-540-48298-9_3	relational database;computer science;data warehouse;data mining;database;information management;world wide web;modular design;information system	DB	-33.69190375219187	10.975490152715647	159978
333ec47205b860b38b25b16205e8fecd31776f3c	automatic normalization and enity-relationship generation through attributes and roles	entity relationship	The opinions expressed in this paper are those of the authors and do not represent the opinions, statements of policies of their respective employers and of the DDSWP.	automatic control;conceptual schema;data dictionary;data model;data modeling;diagram;documentation;enquire;entity;icl;information system;john d. wiley;lattice boltzmann methods;mad;management system;newton;relational database;samuel newman;software quality assurance;system analysis;transcription (software)	Ken Meyer;John Doughty	1984	SIGMOD Record	10.1145/984549.984553	entity–relationship model;computer science;database;weak entity	NLP	-26.280978404129236	17.34074975327218	160039
abdebefd467621f3be3e320f0776c71e2f1510b7	oql: a query language for manipulating object-oriented databases	query language;object oriented database		object query language	Abdallah M. Alashqur;Stanley Y. W. Su;Herman Lam	1989			sargable;query optimization;query expansion;web query classification;data control language;computer science;query by example;database;rdf query language;web search query;view;query language;object query language	DB	-31.39731505022037	8.958261337503965	160046
c8383db6d241048aaf6b8150be70bccf0b157c7b	an adaptive clustering algorithm of complex objects in object-oriented database			algorithm;cluster analysis	Su-Shang Huang;I-Heng Meng;Wei-Pang Yang	1999			object-oriented design;object-based spatial database;flame clustering;database;cluster analysis;object-oriented programming;computer science	DB	-31.068940272816757	9.131047583310123	160115
594e0b776bbc5f7f80a56137af0f2cc1d6f27d28	how to specify a graph transformation approach - a meta model for fujaba	graph transformation;multiple constraints;side effect;object oriented;graph grammar;meta model	Application-oriented approaches to graph transformation provide structural features beyond vertices and edges, like composition in hierarchical graphs, inheritance in object-oriented graphs, multiplicity constraints, etc. Often, these features have a specific dynamic interpretation which requires complex embedding mechanisms and context conditions. For example, the deletion of a compound node usually implies the deletion of its components. In this paper, we propose the use of a meta graph grammar for the definition of such a complex graph transformation approach. A meta graph grammar is a typed graph grammar whose type graph provides a static description of the structure of graphs, rules, and transformations of the approach. This static meta model, which is comparable to the meta model in the Uml specification, is extended by a specification of the rule application operator by means of graphical embedding rules, i.e., the productions of the meta graph grammar. These embedding rules allow a concise visual description of the admissible context embeddings of a rule and of the side effects of the rule application on the context. As a case-study, a meta graph grammar for selected features of the object-oriented graph transformation approach Fujaba is given.	denotational semantics;diagram;geographical operations system;graph (discrete mathematics);graph rewriting;java;metamodeling;natural deduction;operational semantics;orientation (graph theory);rewrite (programming);unified modeling language	Reiko Heckel;Albert Zündorf	2001	Electr. Notes Theor. Comput. Sci.	10.1016/S1571-0661(04)80942-3	metamodeling;lattice graph;topological graph theory;combinatorics;discrete mathematics;directed graph;null graph;graph property;graph labeling;computer science;clique-width;machine learning;mathematics;voltage graph;graph;programming language;object-oriented programming;complement graph;side effect;graph rewriting	AI	-29.321518375539465	16.051849120190262	160245
231abcf6db2a2f080de0b5c7856ca467631fe6bb	structural types for systems of equations - type refinements for structurally dynamic first-class modular systems of equations		Characterising a problem in terms of a system of equations is common to many branches of science and engineering. Due to their size, such systems are often described in a modular fashion by composition of individual equation system fragments. Checking the balance between the number of variables (unknowns) and equations is a common approach to early detection of mistakes that might render such a system unsolvable. However, current approaches to modular balance checking have a number of limitations. This paper investigates a more flexible approach that makes it possible to treat equation system fragments as true first-class entities. Furthermore, the approach handles so-called structurally dynamic systems, systems whose behaviour changes discretely and abruptly over time. The central idea is to record balance information in the type of an equation fragment. This information can then be used to determine if individual fragments are well formed, and if composing fragments preserves this property. The type system presented in this paper is developed in the context of Functional Hybrid Modelling (FHM). However, the key ideas are in no way specific to FHM, but should be applicable to any language featuring a notion of modular systems of equations, including systems with first-class components and structural dynamism.	dynamical system;entity;local variable;operational semantics;sensor;total functional programming;type system	John Capper;Henrik Nilsson	2012	Higher-Order and Symbolic Computation	10.1007/s10990-013-9099-6	pure mathematics;mathematics;algorithm	Logic	-23.137620911548836	17.08276785582235	160512
3e36ea385e9209c47c22145a9551698a994eec1e	tractable reasoning with quality guarantee for expressive description logics	description logics;semantic web;knowledge representation information theory;reasoning;ontology		cobham's thesis;description logic	Yuan Ren	2014			natural language processing;t-norm fuzzy logics;knowledge representation and reasoning;knowledge management;theoretical computer science;non-monotonic logic;model-based reasoning;mathematics;reasoning system	AI	-20.496795412788764	8.874114595429322	160627
da037921207683626c905d05aed5e2b41a381d1a	a pattern representation of indexed languages	indexation	. The Ai are auxiliary symbois. Eachfcan be rpreted “‘ as a homomorphism from Ri to W. j I . with f(t) = g(t) = t for all t E T and f(,4 t) = a, f~~2)=b)f~~3)=BandgiAr)=s/i1,~A2)= bA2A3A3, g(A3) = bA3. Set p(T, W = CL1 (T’, &), L#‘, &)S with L*(T’*&) = CI(L1, h I), (19 Q, 2)v t&t v; L& m and L#“, 4) = {{(La, )i, 3), (3, A l-9, (4,A2,% (5, At, Ul, U2, A, 61, (6s k L3), JO}}. P(T, N) in graphic notation:	indexed language;linear algebra	Jürgen Duske;Rainer Parchmann;H. Schumacher	1976	Inf. Process. Lett.	10.1016/0020-0190(76)90063-6	indexed language;computer science;mathematics	DB	-26.88165131490113	14.581656895387258	160685
0134c555ad55017eccc52009457c492bbc672883	coco: a web-based data tracking architecture for challenged network environments	model view controller;data collection;offline;low latency;model view controller mvc;network connectivity;analytical method;ngo;content management system;low bandwidth;intermittent network;rural development;ict4d	Data tracking and analytics methods for organizations operating in rural locations are cumbersome -- particularly for the many that continue to use traditional, non-electronic methods. To evaluate effectiveness and productivity, these organizations are being asked to adopt data and analytics methodologies that are technology oriented in nature. Today, network connectivity is increasing even in the most remote locations and electronic data input is gradually becoming less of an impediment to data collection. To be sure, uninterrupted bandwidth availability still eludes these locations, making it difficult for traditional web applications to function. In cognizance of intermittent to zero bandwidth availability in rural locations, we propose COCO (connect-online, connect-offline), a data input framework designed to seamlessly enable online as well as offline connection capabilities for applications. By building on the COCO framework, applications can continue operating by going offline. Preliminary results show that applications built on the COCO framework achieve low latency and high execution speeds by virtue of the framework's browser-client architecture. We also demonstrate the effectiveness of a COCO framework-enabled application over a traditional web application when run in a lab, and in rural locations.	online and offline;web application	Saureen Shah;Apurva Joshi	2010		10.1145/1926180.1926189	real-time computing;simulation;engineering;data mining	Web+IR	-33.40108084548021	17.89680864229081	160956
2f0d3f80966cfeadaaed5abd3bf564a3e89a2bb2	integrating quantitative and qualitative discovery: the abacus system	graph search;learning by discovery;search space;inductive learning;concept learning;inductive inference;heuristic evaluation;technical report;quantitative discovery	Most research on inductive learning has been concerned with qualitative learning that induces conceptual, logic-style descriptions from the given facts. In contrast, quantitative learning deals with discovering numerical laws characterizing empirical data. This research attempts to integrate both types of learning by combining newly developed heuristics for formulating equations with the previously developed concept learning method embodied in the inductive learning program AQ11. The resulting system, ABACUS, formulates equations that bind subsets of observed data, and derives explicit, logic-style descriptions stating the applicability conditions for these equations. In addition, several new techniques for quantitative learning are introduced. Units analysis reduces the search space of equations by examining the compatibility of variables' units. Proportionality graph search addresses the problem of identifying relevant variables that should enter equations. Suspension search focusses the search space through heuristic evaluation. The capabilities of ABACUS are demonstrated by several examples from physics and chemistry.	concept learning;graph traversal;heuristic evaluation;inductive reasoning;numerical analysis	Brian Falkenhainer;Ryszard S. Michalski	1986	Machine Learning	10.1023/A:1022866732136	multi-task learning;instance-based learning;algorithmic learning theory;inductive bias;concept learning;computer science;artificial intelligence;technical report;inductive reasoning;machine learning;inductive transfer;mathematics;heuristic evaluation;hyper-heuristic;algorithm	AI	-23.03205634633012	5.39569243221108	161063
be17c2f80e99262811714315ab907a700eff40e6	kgrl: an owl2 rl reasoning system for large scale knowledge graph	resource description framework;cognition;sparks;ontologies;inference algorithms;optimization;algorithm design and analysis	Currently knowledge graph has been widely applied to various fields. Although the data scale is large, there is still a lot of useful but implicit information in it. Thus, a powerful reasoning system is required to derive these data. However, current reasoning systems cannot accomplish this task very well. On the one hand, stand-alone reasoning systems cannot meet the demand of large data. On the other hand, the reasoning ability of existing distributed reasoning systems is limited because of the lack of expressive inference rules. In this paper, we propose and implement a distributed reasoning system KGRL for knowledge graph based on OWL2 RL. It has a more powerful reasoning ability due to more expressive rules. It also supports optimization for redundant data. Besides, a rule-based algorithm is designed to find the inconsistent data. Experimental results show that KGRL can derive more implicit information efficiently compared to other reasoning systems. Moreover, KGRL is capable of eliminating redundant data, which can reduce the storage of knowledge graph by an average of 42%. Finally, KGRL also performs well for the detection of inconsistencies in knowledge graph.	algorithm;experiment;knowledge graph;logic programming;mathematical optimization;reasoning system;redundancy (engineering);scalability	Yuze Wei;Jie Luo;Huiyuan Xie	2016	2016 12th International Conference on Semantics, Knowledge and Grids (SKG)	10.1109/SKG.2016.020	knowledge representation and reasoning;opportunistic reasoning;algorithm design;cognition;qualitative reasoning;computer science;ontology;artificial intelligence;adaptive reasoning;theoretical computer science;non-monotonic logic;machine learning;rdf;data mining;database;distributed computing;reasoning system	AI	-24.137879261709433	7.225948058700733	161159
2cdcceeac3bc7012ec01d4e1c717f8c7ce56783b	domain-specific languages in a finite domain constraint programming system	finite domain constraints;artificial intelligent;domain specific language	In this paper, we present domain-specific languages (DSLs) that we devised for their use in the implementation of a finite domain constraint programming system, available as library(clpfd) in SWI-Prolog and YAP-Prolog. These DSLs are used in propagator selection and constraint reification. In these areas, they lead to concise specifications that are easy to read and reason about. At compilation time, these specifications are translated to Prolog code, reducing interpretative run-time overheads. The devised languages can be used in the implementation of other finite domain constraint solvers as well and may contribute to their correctness, conciseness and efficiency.	admissible numbering;algorithmic efficiency;compiler;constraint logic programming;constraint programming;correctness (computer science);domain-specific language;prolog;propagator;reification (knowledge representation);swi-prolog	Markus Triska	2011	CoRR		constraint logic programming;constraint programming;constraint satisfaction;programming domain;computer science;domain-specific language;artificial intelligence;theoretical computer science;fifth-generation programming language;programming language;algorithm	PL	-19.881650090540482	15.33180020068846	161265
f2ac3e92983bd0ad57fba31e732fa794dfe97924	presto authorization: a bitmap indexing scheme for high-speed access control to xml documents	database indexing;controle acces;intercambio informacion;document access;extensible markup language;licence procedure;information security;xml access control information security authorization enforcement bitmap indexing;authorisation;information retrieval;xml access control;xml language;securite informatique;autorizacion;electronic data interchange xml authorisation database indexing information retrieval;acces document;autorisation;control velocidad;internet presto authorization bitmap indexing scheme high speed access control extensible markup language xml document information exchange;computer security;recherche documentaire;internet;indexing;grande vitesse;bitmap indexing;echange information;busqueda documental;information exchange;seguridad informatica;indexation;authorization enforcement;indizacion;xml;xml document;authorization indexing access control xml information retrieval internet transaction databases information security database languages;procesador oleoducto;gran velocidad;document retrieval;access control;processeur pipeline;speed control;acceso documento;commande vitesse;high speed;langage xml;lenguaje xml;electronic data interchange;pipeline processor	"""XML (extensible markup language) is fast becoming the de facto standard for information exchange over the Internet. As more and more sensitive information gets stored in the form of XML, proper access control to the XML documents becomes increasingly important. However, traditional access control methodologies that have been adapted for XML documents do not address the performance issue of access control. This paper proposes a bitmap-indexing scheme in which access control decisions can be sped up. Authorization policies of the form (subject, object, and action) are encoded as bitmaps in the same manner as XML document indexes are constructed. These two are then efficiently pipelined and manipulated for """"fast"""" access control and """"secure"""" retrieval of XML documents."""	access control;authorization;bitmap;information exchange;information sensitivity;internet;markup language;pipeline (computing);presto;xml	Jong P. Yoon	2006	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2006.113	well-formed document;xml catalog;document retrieval;xml validation;binary xml;xml encryption;simple api for xml;xml;streaming xml;computer science;information security;document type definition;document structure description;xml framework;data mining;xml database;xml schema;database;xml signature;world wide web;xml schema editor;cxml;information retrieval;efficient xml interchange;sgml	DB	-31.267650187994718	5.750384825088346	161272
7a53b57943da598cbf2605390ca33e4de2a53d20	xml prefiltering as a string matching problem	xml projection xml prefiltering string matching xml document xml streams keyword search tree structured data xml data management;memory management;xml prefiltering;xml data management;information filtering;low complexity;tree data structures;runtime;acceleration;navigation;engines;keyword search;tree structure;xml runtime navigation algorithm design and analysis databases face detection pattern matching keyword search project management filtering;xml;xml projection;xml document;xml streams;tree structured data;string matching;xml information filtering string matching tree data structures;algorithm design;algorithm design and analysis	We propose a new technique for the efficient search and navigation in XML documents and streams. This technique takes string matching algorithms designed for efficient keyword search in flat strings into the second dimension, to navigate in tree structured data. We consider the important XML data management task of prefiltering XML documents (also called XML projection) as an application for our approach. Different from existing prefiltering schemes, we usually process only fractions of the input and get by with very economical consumption of both main memory and processing time. Our experiments reveal that, already on low-complexity problems such as XPath filtering, in-memory query engines can experience speed-ups by two orders of magnitude.	computer data storage;experiment;in-memory database;streams;search algorithm;string searching algorithm;xml schema;xpath	Christoph Koch;Stefanie Scherzinger;Michael Schmidt	2008	2008 IEEE 24th International Conference on Data Engineering	10.1109/ICDE.2008.4497471	xml validation;algorithm design;simple api for xml;xml;computer science;xml framework;xml schema;database;world wide web;information retrieval	DB	-29.889504927491323	5.164525430564941	161399
91572e46365fedb8bd10010fbe0aaa6d54f96654	generalized query-by-rule: a heterogeneous database query language	query language;base donnee repartie;distributed database;implementation;heterogeneous databases;conception;design;langage interrogation	Abstract   This paper describes the design and implementation of a high-level query language called Generalized Query-By-Rule (GQBR) which supports retrieval, insertion, deletion and update operations. This language, based on the formalism of database logic, enables the users to access each database in a distributed heterogeneous environment, without having to learn all the different data manipulation languages. The compiler has been implemented on a DEC 1090 system in Pascal.		Lalit M. Patnaik;D. M. Chowdhary	1985	Comput. Lang.	10.1016/0096-0551(85)90014-1	design;data definition language;query optimization;database theory;data manipulation language;database tuning;data control language;computer science;query by example;theoretical computer science;database;rdf query language;programming language;implementation;view;database schema;distributed database;database testing;database design;query language	DB	-30.875377688184283	9.709244060815305	161464
8e6a7f64c5a9aea317bfd1a3ba4908e00359c418	computers, justification, and mathematical knowledge	proof assistant;proofs;justification;certificates;proof search;four color theorem;a priori;mathematical knowledge	The original proof of the four-color theorem by Appel and Haken sparked a controversy when Tymoczko used it to argue that the justification provided by unsurveyable proofs carried out by computers cannot be a priori. It also created a lingering impression to the effect that such proofs depend heavily for their soundness on large amounts of computation-intensive custom-built software. Contra Tymoczko, we argue that the justification provided by certain computerized mathematical proofs is not fundamentally different from that provided by surveyable proofs, and can be sensibly regarded as a priori. We also show that the aforementioned impression is mistaken because it fails to distinguish between proof search (the context of discovery) and proof checking (the context of justification). By using mechanized proof assistants capable of producing certificates that can be independently checked, it is possible to carry out complex proofs without the need to trust arbitrary custom-written code. We only need to trust one fixed, small, and simple piece of software: the proof checker. This is not only possible in principle, but is in fact becoming a viable methodology for performing complicated mathematical reasoning. This is evinced by a new proof of the four-color theorem that appeared in 2005, and which was developed and checked in its entirety by a mechanical proof system.	automated proof checking;computation;computer;formal proof;four color theorem;proof assistant;proof calculus	Konstantine Arkoudas;Selmer Bringsjord	2007	Minds and Machines	10.1007/s11023-007-9063-5	proof by contradiction;computer-assisted proof;a priori and a posteriori;probabilistically checkable proof;epistemology;computer science;artificial intelligence;four color theorem;automated proof checking;analytic proof;proof theory;mathematical proof;mathematical fallacy;proof assistant;programming language;structural proof theory;proof complexity;algorithm;statistical proof	Theory	-20.149368242399753	17.43090396256662	161657
6fcf642b9437009e17b5750a8863dfd57b4f1906	intensional concept graphs as patterns over power context families		Intension graphs are introduced as an intensional variant of Wille’s concept graphs. Windowed intension graphs are then introduced as formalizations of conjunctive queries. Realizations describe pattern matching over power context families, which have been introduced with concept graphs as representations of relational data using a sequence of formal contexts. Using windowed intension graphs as patterns within pattern structures, we can define concept lattices, where power context families take the role of formal contexts. Relational Context Families, used in Relational Concept Analysis (RCA), correspond to power context families using sorts and only binary relations, and the lattices generated by the RCA algorithm (using wide scaling) can be represented using rooted trees as intents, which are introduced as a subclass of windowed intension graphs. Consequently, projections of the previously introduced pattern structure can be used as an alternative to the RCA algorithm.	algorithm;conjunctive query;database theory;formal concept analysis;image scaling;intension;intensional logic;iteration;pattern matching;window function	Jens Kötters	2016			machine learning;computer science;artificial intelligence;graph	DB	-23.907436971279626	5.7585185715347915	161813
725776120e2fcca6d6cf3033969a52a15168e6a5	abstract data types and data bases	adt approach;data base;data bases;data base area;abstract data type;different specific problem	Several researchers have been working, recently, at the attempt of using the ADT approach in the Data Base area. Different specific problems have attacked and we will review some of them.	abstract data type;database	Paolo Paolini	1980		10.1145/800227.806912	computer science;data mining;database;programming language;abstract data type;algorithm	ML	-31.28852949190394	10.157667839336357	161829
0618d82a4126d5914274f29f7e0f0d4b77886d6a	a semantic-head-driven generation algorithm for unification-based formalisms	topdown method;semantic-head-driven fashion;semantic-head-driven generation algorithm;implicit traversal;semantically nonmonotonic grammar;enabling design feature;analysis tree;earley deduction generator;logical form encodings;previous algorithm;unification-based formalisms;fewer restriction	We present an algorithm for generating strings from logical form encodings that improves upon previous algorithms in that it places fewer restrictions on the class of grammars to which it is applicable. In particular, unlike an Earley deduction generator (Shieber, 1988), it allows use of semantically nonmonotonic grammars, yet unlike top-down methods, it also permits left-recursion. The enabling design feature of the algorithm is its implicit traversal of the analysis tree for the string being generated in a semantic-head-driven fashion.	algorithm;left recursion;natural deduction;string generation;top-down and bottom-up design;tree traversal;unification (computer science)	Stuart M. Shieber;Gertjan van Noord;Robert C. Moore;Fernando Pereira	1989			genetic algorithm;logical form;computer science;theoretical computer science;machine learning;linguistics;algorithm	NLP	-21.321029753843252	16.05656690634645	162038
3d2945ea2eb85fd6e4064ae0c74743e2df333b79	schema and data integration for relational and object-oriented data sources	data integrity			Vincenza Carchiolo;Alessandro Longheu;Michele Malgeri	2002			ontology-based data integration;idef1x;database;information schema;logical data model;semi-structured model;data mining;change data capture;database model;data warehouse;computer science	DB	-31.83093372492726	9.725010974490242	162054
0010742af02e338043aa0229ace7a8eaaba84f24	probabilistic logical approach for testing diagnosability of stochastic discrete event systems	logical diagnoser;aa diagnosability;a diagnosability;logical diagnosability;stochastic discrete event system;fault diagnosis	Fault diagnosis plays an important role in the prevention of harmful events in discrete event systems (DESs). Stochastic DES (SDES) is a more precise formulation of DES. In this paper, a novel approach that uses probabilistic logic to diagnose SDES is investigated. SDES is formalized as a set of probabilistic logical formulas. Moreover, a logical diagnoser is presented. Fault diagnosis of SDES has two properties: A-diagnosability and AA-diagnosability. On the basis of resolution principle, an algorithm is proposed to test A-diagnosability and AA-diagnosability of the SDES. Experimental results demonstrate that our algorithm improves the accuracy and efficiency of verifying diagnosability of SDES. Graphical abstractDisplay Omitted HighlightsUsing logical formulas to describe the stochastic discrete event system.Using probabilistic logical approach to verify the diagnosability of the stochastic discrete event system.Constructing a logical diagnoser.The algorithms do not need any synchronization operation.The complexity of the algorithms is polynomial.		Xuena Geng;Dantong Ouyang;Xiangfu Zhao;Shuang Hao	2016	Eng. Appl. of AI	10.1016/j.engappai.2016.03.008	algorithm	AI	-22.714838078533944	15.828673151184173	162106
02d1b417b4f4c1aa3cc4bd72372765c7641f1ab9	simplifying schema mappings	query language;graph databases;schema mappings;conjunctive queries;formal specification;relational database;lav;schema mapping;gav	A schema mapping is a formal specification of the relationship holding between the databases conforming to two given schemas, called source and target, respectively. While in the general case a schema mapping is specified in terms of assertions relating two queries in some given language, various simplified forms of mappings, in particular LAV and GAV, have been considered, based on desirable properties that these forms enjoy. Recent works propose methods for transforming schema mappings to logically equivalent ones of a simplified form. In many cases, this transformation is impossible, and one might be interested in finding simplifications based on a weaker notion, namely logical implication, rather than equivalence. More precisely, given a schema mapping M, find a simplified (LAV, or GAV) schema mapping M' such that M' logically implies M. In this paper we formally introduce this problem, and study it in a variety of cases, providing techniques and complexity bounds. The various cases we consider depend on three parameters: the simplified form to achieve (LAV, or GAV), the type of schema mapping considered (sound, or exact), and the query language used in the schema mapping specification (conjunctive queries and variants over relational databases, or regular path queries and variants over graph databases). Notably, this is the first work on comparing schema mappings for graph databases.	conjunctive query;formal specification;graph database;query language;relational database;schema (genetic algorithms);turing completeness;xml schema	Diego Calvanese;Giuseppe De Giacomo;Maurizio Lenzerini;Moshe Y. Vardi	2011		10.1145/1938551.1938568	information schema;logical schema;relational database;computer science;conceptual schema;star schema;data mining;formal specification;database;mathematics;conjunctive query;programming language;database schema;graph database;algorithm;query language	DB	-25.658150318366317	10.350099259288724	162129
0a5cd384b11d1e31c74233769c66075202f101f0	meshsql: the query language for simulation mesh data	query language;simulation mesh data;sql99;efficient implementation;object relational databases	Mesh data has been a common form of data produced and searched in scientific simulations, and has been growing rapidly in the size thanks to the increasing computing power. Today, there are visualization tools that assist scientists to explore and examine the data, but their query capabilities are limited to a small set of fixed visualization operations, which is far too short to meet the needs of most users. Thus, it is imperative to provide ad hoc query tools for them. In this paper, we propose an ad hoc query language MeshSQL, which has been extended from ANSI SQL99 to support the features unique to simulation mesh data, such as temporality, spatial regions, statistics, and similarity. After classifying MeshSQL queries based on three criteria related to efficient implementations of the queries, we present the syntax and semantics of MeshSQL, and support them with examples. We also discuss implementing MeshSQL queries in SQL99 in an object-relational database system that allows incorporating user-defined types and functions. To our knowledge, MeshSQL is the first and the only query language for simulation mesh data. 2003 Elsevier Inc. All rights reserved.	hoc (programming language);imperative programming;object-relational database;query language;relational database management system;simulation	Byung Suk Lee;Ron Musick	2004	Inf. Sci.	10.1016/j.ins.2003.02.002	sargable;query optimization;query expansion;web query classification;data manipulation language;data control language;computer science;query by example;theoretical computer science;data mining;database;rdf query language;web search query;view;query language;object query language;spatial query	DB	-30.115955109964997	6.27579801138382	162288
624f060d773c11facdf0d9f7fabe229fbc67c825	relational database: selected writings	relational database;mathematical model;information system	By reading, you can know the knowledge and things more, not only about what you get from people to people. Book will be more trusted. As this relational database selected writings, it will really give you the good idea to be successful. It is not only for you to be success in certain life you can be successful in everything. The success can be started by knowing the basic knowledge and do actions.	relational database	C. J. Date	1986			database theory;sql;relational model;semi-structured model;entity–relationship model;data model;relational database;computer science;database normalization;database model;data mining;database;database schema;information retrieval;object-relational impedance mismatch;database design	AI	-30.722707086272134	9.359308478547488	162329
287b12f77581460eb057a2060e19b39ad772ef2c	on the properties of metamodeling in owl1	owl;description logics;conceptual model;decidabilty;semantic web;description logic;knowledge representation;metamodeling	A common practice in conceptual modeling is to separate the conceptual from the data model. Although very intuitive, this approach is inadequate for many complex domains, in which the borderline between the two models is not clear-cut. Therefore, OWL Full, the most expressive of the Semantic Web ontology languages, allows us to combine the conceptual and the data model by a feature we refer to as metamodeling. In this article, we show that the semantics of metamodeling adopted in OWL Full leads to the undecidability of basic inference problems due to the free usage of the built-in vocabulary. Based on this result, we propose two alternative semantics for metamodeling: the contextual and the HiLog semantics. We present several examples showing how to use the latter semantics to axiomatize the interaction between concepts and metaconcepts. Finally, we show that SHOIQ(D)—the description logic underlying OWL DL—is still decidable when extended with metamodeling under either semantics.		Boris Motik	2007	J. Log. Comput.	10.1093/logcom/exm027	natural language processing;knowledge representation and reasoning;description logic;computer science;artificial intelligence;database;programming language	Logic	-22.88841306679081	9.35586722167859	162685
f8ce3329bf037d69a130e26f6d13042f0123f00f	chimera: a model and language for active dood systems	object oriented	"""Chimera is a novel database model and language which has been designed as a joint conceptual interface of the IDEA project, a major Euro-pean cooperation initiative aiming at the integration of object-oriented, active and deductive database technology. In this paper, we present a view of the main features of Chimera and discuss the design choices made. The most remarkable characteristic of Chimera is the fact that fully developed rule languages for both active and deductive rules have been integrated in an object-oriented context. Rules are in the center of interest of the IDEA project, which aims at developing prototypical components of a future \intelligent"""" DBMS."""	chimera;database model;deductive database	Stefano Ceri;Rainer Manthey	1994			deductive database;natural language processing;object-oriented programming;active database;artificial intelligence;database model;computer science	DB	-30.585619145607705	11.475211737068944	162740
f60b06bb719ded67d5bd1db0184e1496de6c2200	approximately common patterns in shared-forests	experimental tests;metadata;database schema integration;federated data base systems;federated information system fis;relation al algebra;multidatabase;database integration;query languages;metaquery;interoperability;schema transparency	We present a proposal intended to demonstrate the applicability of tabulation techniques for detecting approximately common patterns when dealing with structures sharing some common parts. This sharing saves on the space needed to represent the structures and also on their later processing, by factorizing the filtering of substructure matching. As a consequence, preliminary experimental tests indicate a reduction of the running time.	sensor;table (information);time complexity	Manuel Vilares Ferro;Francisco J. Ribadas;Jorge Graña Gil	2001		10.1145/502585.502599	interoperability;computer science;data integration;data mining;database;metadata;world wide web;information retrieval;query language	AI	-32.6407694529927	5.365321646429802	162799
2e3ea26326ed286ec193d3b28e15ec65ba0151db	on the role of abstraction in case-based reasoning	empirical study;case base reasoning;abstraction;case based planning;levels of abstraction;case based reasoning	Cases Conrete Cases level of abstraction high low Abstraction hierarchy C C 1 2 Fig. 2. Abstraction hierarchy for indexing cases This approach to indexing, however, makes an assumption concerning the similarity assessment. It requires that a problem cannot be similar to a concrete case unless it is at least similar to this case at a higher level of abstraction. This assumption holds particularly, if similarity is de ned based on the level of abstraction, which can be done as follows: A problem p is more similar to the concrete case C1 than to the concrete case C2 if the lowest level of abstraction on which p matches C2 is higher (more abstract) than the lowest level of abstraction on which p matches C1. 2.4 Reuse of Abstract Cases There are di erent ways of using the information provided in abstract cases for solving the current problem. No reuse of abstract solutions Abstract cases are only used as indexes to concrete cases. For problem solving, concrete cases are used exclusively. Abstract solutions as result The CBR system retrieves and reuses abstract cases. The abstract solutions contained in the abstract cases are not re ned to more concrete levels but are directly returned as output. The interpretation of abstract solutions is up to the user. Re nement of abstract cases The CBR system retrieves and reuses abstract cases and re nes abstract solutions to the concrete level. The re ned solution is then presented to the user. For her/him it is transparent, whether the solution presented by the system stems directly from a matching concrete cases or whether the solution is obtained through the re nement of an abstract case.solutions as result The CBR system retrieves and reuses abstract cases. The abstract solutions contained in the abstract cases are not re ned to more concrete levels but are directly returned as output. The interpretation of abstract solutions is up to the user. Re nement of abstract cases The CBR system retrieves and reuses abstract cases and re nes abstract solutions to the concrete level. The re ned solution is then presented to the user. For her/him it is transparent, whether the solution presented by the system stems directly from a matching concrete cases or whether the solution is obtained through the re nement of an abstract case. available concrete case new solution 1 new solution 2 (concrete level) Level of Abstraction : 0 1 2 n abstract cases case abstraction refinement Level of Abstraction : Level of Abstraction : Level of Abstraction : Fig. 3. Adaptation by abstraction and re nement Please note that abstraction and re nement is already a technique for solution adaptation (see Figure 3). If available concrete cases are abstracted (e.g., automatically) to abstract cases and then getting retrieved and re ned, a new solution to a new problem will be constructed. The higher the level of abstraction of the reused abstract case, the more may the newly re ned solution di er from the solution contained in the original case. We can distinguish di erent methods for realizing such a re nement: Generative re nement of abstract cases This re nement is done by generative problem solving methods, e.g. hierarchical problem solving. For automatically performing this re nement task, additional general domain knowledge is usually required. Case-based re nement of abstract cases This re nement itself is done in a case-based way, avoiding partially the need for additional general knowledge. However, case-based renement requires cases that describe how the individual elements, the abstract solutions are built of, can be re ned at a more concrete level. 2.5 Adaptation of Abstract Cases Besides the possibility to realize adaptation by re ning abstract cases, adaptation can also be done on a single level of abstraction (see Fig. 4). The spectrum of known methods for solution adaptation in CBR (for an overview see e.g., [8; 14; 34]) can also be applied to abstract cases prior the re nement. Thereby, the exibility of reuse can be increased, i.e., a concrete case covers a larger area in the solution space. available concrete case new solution 1 new solution 2 (concrete level) Level of Abstraction : 0 1 2 n abstract cases case abstraction refinement Level of Abstraction : Level of Abstraction : Level of Abstraction : adaptation adaptation Fig. 4. Adaptation of abstract cases 2.6 Forgetting cases The reuse of cases at several levels of abstraction also provides a frame for realizing case deletion policies [31]. Cases deletion is particularly important to avoid the utility or swamping problem [32; 21; 12] that occurs when case bases grow very large. When reusing abstract cases for indexing and reuse, case deletion can e ciently be realized through a pruning of the abstraction hierarchy. If certain concrete cases are removed from the case base, the abstract cases that remain accessible, can still cover the set of target problems previously covered by the deleted concrete case. However, this requires e ective ways of re nement such as generative or case-based re nement. For selecting cases to forget, the savings due to the reduced retrieval e ort must outweigh the additional e ort for re ning more abstract cases. 2.7 Summary of the Framework The following Table 1 summarizes the various facets of the framework for reusing abstract cases. Existing approaches can be described and analyzed and new approaches can be designed using this framework. kind of s tored cases : abs tract cases for indexing: reuse of abs tract solutions : adaptation of abs tract solutions : case deletion policy: abs tract abs tract & concrete hierarchical abs tract result case based refine. yes no yes yes no no no generative refine. acquis ition of abs tract cases: available manual generation automatic generation cases Table 1. Framework for reusing abstract cases 3 PARIS: Using abstraction in case-based planning Now, we describe a concrete case-based reasoning system, called Paris2 [4; 5; 3] that uses abstraction for case-based planning.Paris can be characterized according to this framework as follows: { abstract and concrete cases are stored in the case base { abstract cases are generated automatically from concrete cases { abstract cases are used for indexing { generative re nement of abstract solutions is realized { adaptation of abstract solutions is possible { case deletion policy is realized. We now explain the approach in more detail. 3.1 Requirements Paris was designed as a generic (i.e., domain independent) case-based planning system but with a particular area of application domains in mind: process planning in mechanical engineering. From this application area, a set of CBR speci c requirements have been identi ed: { ability to cope with vast space of solution plans { construction of correct solutions { exible reuse due to large spectrum of target problems { processing of highly complex cases { only concrete planning cases available (e.g. in archives of a company) 3.2 Abstract Planning Cases We now summarize the formal de nition of concrete and abstract planning cases already presented in detail in [5]. Representation of domains Following a STRIPS-oriented representation [11], a planning domain D de nes a (possibly in nite) set of states S and a set of operators O which describe transitions from one state to a successor state. A state s 2 S is described by a nite set of prepositions. A problem hsI ; sGi in a domain is given by an initial state sI and a goal state sG and a plan (a solution to the problem) is a totally ordered sequence of operators ho1; : : : ; oni that transform the initial state into the goal state. A case is a problem together with a solution to this problem. Di erent levels of abstraction In Paris, di erent levels of abstraction are realized by di erent planning domains. In the following we assume two planning domains: a concrete domain Dc and an abstract domain Da. Concrete and abstract planning domains may represent di erent languages with completely di erent states and operators. A concrete case Cc = hhsc0; scni; (oc1; : : : ; ocn)i is a case given in the concrete planning domain, and an abstract case Ca = hhsa0 ; sami; (oa1; : : : ; oam)i is a case in the abstract planning domain. However, not every abstract case is an abstraction of a concrete case. Additional requirements must be met to call an abstract case abstraction of a concrete case. These requirements can be expressed by the existence of two independent mappings: a state abstraction mapping , and a sequence abstraction mapping [2] (see Fig. 5). 2 Paris stands for plan abstraction and re nement in an integrated system. I 1 2 3 s c s c sc s c s c I s a s a s a s a s G G 1 j i concrete domain: abstract domain: α n i+1 i 4 3 2 1 1 2 m Da Dc α α α j j+1 β(0) = 0 β(1) = 3 β(j) = i β(m) = n Oc Oc Oc Oc Oc Oc Oc Oa Oa Oa Oa Oa Fig. 5. Formalization of case abstraction in planning State Abstraction A state abstraction mapping : Sc ! Sa translates states of the concrete domain into the abstract domain. For this translation, we require additional general domain knowledge about how an abstract state description relates to a concrete state description. We assume that this kind of knowledge can be provided in terms of a domain speci c generic abstraction theory A. Such a generic abstraction theory (expressed by horn clauses) de nes each proposition, abstract states can be composed of, in terms of propositions that can occur in the concrete states. Sequence abstraction The solution to a problem consists of a sequence of operators and a corresponding sequence of states. To relate an abstract solution to a concrete solution, the relationship between the abstract states (or operators) and the concrete states (or operators) must be captured. Each abstract state must have a corresponding concrete state but not every concrete state must have an associated abstract state. The sequence abstraction mapping : N! N selects those states of the concrete problem solution that	abstraction layer;archive;case-based reasoning;feasible region;horn clause;open road tolling;principle of abstraction;problem solving;reasoning system;refinement (computing);requirement;strips;state (computer science);tract (literature);xfig	Ralph Bergmann;Wolfgang Wilke	1996		10.1007/BFb0020600	abstraction inversion;computer science;knowledge management;artificial intelligence;data mining	Logic	-23.17863383894191	10.112329213068538	163043
ee3f6f658a84de5f732ece74848b61ce369f5728	physical structures design for relational databases	base relacional dato;sistema operativo;gestion informacion;base donnee tres grande;relational database;operating system;information management;base donnee relationnelle;systeme exploitation;systeme gestion base donnee;information system;gestion information;very large databases;sistema gestion base datos;database management system;structural design;systeme information;sistema informacion	This paper contains a description of the optimisation method for large databases with heterogeneous applications. This method makes it possible to automatically define the physical structure of a relational database. The input to the optimisation algorithm are the SQL queries stored in text files which are executed on the database server in a real life applications. Our tool recognises the classes of SQL queries and their frequency, analyses semantics and as a result prepares suggestions for the following parameters of physical definition of the database: the key and physical structure of a table (from amongst Btree, Isam, Hash and Heap), set of optimal indexes (this means their key as well as physical structure). The principal information which the system uses within the database optimisation process are: frequency and classes of SQL queries in an application and the data value distributions. The selection of the optimal configuration of the database relies a Electre Method which is a multicriteria optimal choice method. The use of these techniques increases the efficiency of the system first of all because it reduces the number of pages which are read and written to disks as a result of the optimal choice of the physical structures of tables and indexes. The system supports Oracle, Informix, Sybase and CA-Ingres.	algorithm;b-tree;database server;heterogeneous computing;ibm informix;ingres;mathematical optimization;oracle database;real life;relational database;sql;server (computing);table (database)	Janusz Charczuk	1998		10.1007/BFb0057747	database index;data definition language;stored procedure;relational database;database storage structures;computer science;database model;data mining;database;conjunctive query;foreign key;information management;view;world wide web;physical data model;information system;database testing;database design	DB	-27.50397176471026	5.293632126906195	163079
2fe67de7b63fb65b163020a9ee6b92eb5b478070	research on parallel visualization in large-scale scientific computing	scientific information systems client server systems data visualisation;client server systems;data visualisation;large scale;client server;large scale oil simulation parallel visualization large scale scientific computing middleware client server structure client commware server structure pc cluster large scale display technology visual system model parallel reservoir integrated simulator;scientific computing;pc cluster;middleware;visual system;scientific information systems;visualization large scale systems scientific computing visual system computational modeling middleware displays paper technology hydrocarbon reservoirs large scale integration	In order to solve the visual problems of large-scale scientific computing, the project imports the middleware for communication, and extends the classic client-server structure into client-commware-server structure. The paper proposes a visual system model for large-scale scientific computing based on client-commware-server structure, PC-cluster and large-scale display technology. The visual system model has the virtues of generality and excellently interactive capacity, and has successfully been applied to PRIS (parallel reservoir integrated simulator), a visual system of large-scale oil simulation. Applied results prove that it has a widely applicable prospective	client–server model;computational science;display device;middleware;prospective search;server (computing);simulation	Jiaquan Gao;Duanyang Zhao	2006	16th International Conference on Artificial Reality and Telexistence--Workshops (ICAT'06)	10.1109/ICAT.2006.107	computer science;database;distributed computing;world wide web	HPC	-32.685304530353534	16.913849886162613	163108
97ba59b98cafe99d4c6a9f43283fa5a0c2135812	a rule-based data manipulation language for olap systems	base donnee;langage manipulation donnee;rule based;lenguaje manipulacion dato;database;base dato;data manipulation language;on line analytical processing	On-Line Analytical Processing (OLAP) 4] has emerged to support multidi-mensional data analysis, by providing manipulations and aggregations of data according to multiple dimensions. Foundations of OLAP languages are now a growing eld of interest in the database research community. We introduce an extension of Datalog devoted to the manipulation of data organized in multidimensional cubes. In our data model, data are organized in cells. Our language is based on the point of view that a Datalog fact represents an entry (called cell reference) in a cube. Associations of cells contents with cells references are represented by ground atoms of the form: and the tuple hN p+1 ; : : : ; N p+q i denotes the cell contents. A cube is simply a set of ground atoms having a common cube name, in which the same reference does not appear more than once to ensure cell monovaluation, and a multidimensional database is a set of ground atoms in which the same reference does not appear more than once. Consider the rule p(X) ? q(X; Y); r(Y): The standard (Datalog) informal meaning of this rule is if q(X,Y) holds and r(Y) holds, then p(X) holds. The basic intuition of our extension is to read such a rule in the following way: if there are two cells of references q(X,Y) and r(Y), then there is a cell of reference p(X). We also add the handling of cell contents, and then a typical rule will be: p(X) : hWi ? q(X; Y) : hWi; r(Y) : hXi: This rule will be informally read: if there exists a cell of reference q(X,Y) containing W, and there exists a cell of reference r(Y) containing X, then there exists a cell of reference p(X) containing W. This language provides a declarative and concise way to specify the basic standard restructuring and summarizing operations used in OLAP systems. It can be used for example to specify the restructuring from one of the two representations below to the other.	data manipulation language;data model;datalog;olap cube;online analytical processing;spreadsheet	Mohand-Said Hacid;Patrick Marcel;Christophe Rigotti	1997		10.1007/3-540-63792-3_29	computer science;data mining;database;programming language	DB	-29.791638859953913	7.460224822660765	163123
f335ca034e07b843e6acfa9a13d13af777bc95dc	searching for composite queries in a main memory database	base relacional dato;eficacia sistema;core storage;data base query;base donnee;information retrieval;interrogation base donnee;performance systeme;database;interrogacion base datos;base dato;memoria central;metodo secuencial;relational database;sequential method;memoire centrale;system performance;recherche binaire;recherche information;base donnee relationnelle;methode sequentielle;binary search;recuperacion informacion		in-memory database	Yannis Manolopoulos;Loukas Petrou;Dimitris Kleftouris	1987	Angewandte Informatik		relational database;computer science;database;computer performance;graph database;binary search algorithm	DB	-27.491022773687654	5.816189900884585	163243
f945d6c89817560f47a7ac85eb46111f85f1f69d	block splitting indexing for supporting containment queries	base relacional dato;xml language;nested loops;relational database;inclusion expression;indexing;numerotation;indexation;indizacion;base donnee relationnelle;xml document;numbering;numerotacion;langage xml;lenguaje xml;query containment	There is structural discrepancy between XML documents and relational databases. The structural information should be represented additionally when XML documents are stored in relational databases. Numbering schemes have been proposed for representing the structural information of XML documents. However, the numbers in these schemes should be updated whenever deletion or insertion of an object occurs. If an XML document is modified frequently, the overhead of redefining numbers can't be ignorable. Hence, in this paper, we propose a mechanism for representing structural information of XML documents using grouping, which can reduce the overhead of redefining numbers when deletion or insertion of objects occurs. We also develop algorithms for processing containment queries of XML documents using our representation scheme. We implement our algorithms and compare the performance of them with the performance of standard join method and non-grouping join method. The experiment shows that the performance of our scheme is better than that of standard join method, and is similar to that of index nested loop join method.		Min Jin;Jong-Myoung Kim	2003		10.1007/3-540-45036-X_76	xml validation;recursive join;xml encryption;simple api for xml;xml;computer science;document structure description;xml framework;data mining;xml database;xml schema;database;xml signature;sort-merge join;information retrieval;efficient xml interchange	DB	-30.13800972481863	4.958319207084962	163296
11e67b93338529f0d628e1e759fea31df0d013a3	set-theoretic problems of null completion in relational databases	base relacional dato;theorems;information systems;informacion incompleta;teoria conjunto;theorie ensemble;relational database;data bases;set theory;integration;incomplete information;information incomplete;base donnee relationnelle;complement vide;models	Abstract   When considering using databases to represent incomplete information, the relationship between two facts where one may imply the other needs to be addressed. In relational databases, this question becomes whether null completion is assumed. That is, does a (possibly partially-defined) tuple imply the existence of tuples that are ‘less informative’ than the original tuple. We show that no relational algebra, that assumes equivalence under null completion, can include set-theoretic operators that are compatible with ordinary set theory. Thus, the approach of x-relations is incompatible with the axioms of a boolean algebra.		Arthur M. Keller	1986	Inf. Process. Lett.	10.1016/0020-0190(86)90104-3	combinatorics;discrete mathematics;theorem;relational model;relational calculus;relational database;computer science;mathematics;complete information;information system;algorithm;set theory	DB	-26.347755294781464	10.25666511249395	163566
8983c970a0a58d5d74c9f63547698220438c2833	tax-pq: dynamic taxonomy probing and query modification for topic-focused web search	employment;web search interfaces;internet query formulation web sites search engines decision trees;data set;decision tree;search engines;topic focused web search;query formulation;data mining;probes;web search interfaces tax pq dynamic taxonomy probing query modification topic focused web search boolean web search interfaces data set taxonomy based search facility sampling decision tree user requirements web sites;sampling;taxonomy based search facility;boolean web search interfaces;internet;database systems;web sites;taxonomy;user requirements;taxonomy web search decision trees search engines sampling methods data mining employment robustness probes database systems;query modification;robustness;web search;sampling methods;decision trees;tax pq;dynamic taxonomy probing	We propose a novel Web search scheme TAX-PQ. TAX-PQ enables taxonomy-based topic-focused Web search on ordinary Boolean Web search interfaces. TAX-PQ utilizes a taxonomy and the data set maintained in an existing taxonomy-based search facility for this purpose. The search is initiated by designating an initial query and a context category in the taxonomy. The data set in the taxonomy-based search facility is probed with a technique combining the initial query with sampling, and a decision tree is constructed from the sampled query result. A query modifier is then derived from the decision tree to focus the initial query on the selected context category. To adapt TAX-PQ to different user requirements on search result effectiveness and properties of target Web search interfaces, we have developed a new decision tree construction algorithm. Experiments involving real Web sites show that TAX-PQ can significantly improve the Web search process and result. The results comply with user requirements under constraints of the target Web search interfaces.	algorithm;decision tree;experiment;modifier key;online search;relevance;requirement;response time (technology);sampling (signal processing);taxonomy (general);user requirements document;web search engine;world wide web	Said Mirza Pahlevi;Hiroyuki Kitagawa	2003	Eighth International Conference on Database Systems for Advanced Applications, 2003. (DASFAA 2003). Proceedings.	10.1109/DASFAA.2003.1192372	beam search;sampling;sargable;query optimization;query expansion;web modeling;web query classification;semantic search;computer science;phrase search;decision tree;data mining;database;best-first search;search analytics;web search query;world wide web;information retrieval;search engine;taxonomy;statistics	DB	-33.17108073785225	5.9999508611608325	163614
2b4d1d60fc5237d1f3c881cd5c98416c91fb05a2	axioms, algebras and topology		Axiomatic theories provide a very general means for specifying the logical properties of formal concepts. From the axiomatic point of view, it is symbolic formulae and the logical relations between them — especially the entailment relation — that form the primary subject of interest. The vocabulary of concepts of any theory can be interpreted in terms of a domain of entities, which exemplify properties, relations and functional mappings corresponding to the formal symbols of the theory. Moreover, by interpreting logical operations as functions of these seman-	axiomatic semantics;entity;exemplification;logical connective;logical relations;theory;vocabulary	Brandon Bennett;Ivo Düntsch	2007		10.1007/978-1-4020-5587-4_3	separation axiom;eilenberg–steenrod axioms;topological space;interior algebra;euclidean topology;nest algebra;initial topology;general topology;kuratowski closure axioms	Logic	-19.923154680787512	5.851271693638838	163616
c727c00ca2e011084971accf27200348378ce10a	on equivalence of relational and network database models	modele relationnel;dependance multivaluee;relational database;equivalence;functional dependency;multivalued dependency;dependance fonctionnelle;diagramme bachman;base donnee relationnelle	On montre que certaines structures relationnelles +1 composees a la fois de dependances fonctionnelles (FD) et de dependances multivaluees (MVD) ne sont pas equivalentes a un diagramme de Bachman a boucle libre si on suit l'approche de Lien et on remplace chaque X→A des FD par sa contrepartie X→A de MVD	database model;turing completeness	Sushil Jajodia	1985	Inf. Process. Lett.	10.1016/0020-0190(85)90130-9	equivalence;relational database;computer science;artificial intelligence;mathematics;functional dependency;algorithm;multivalued dependency	DB	-28.91898984748212	14.65069958807558	163714
7d007e2a962e2401204225c8b3bb7be54d6a41c8	preface to the special issue on inductive logic programming		We are pleased to present this special issue of Machine Learning on inductive logic programming (ILP). The papers contained in this volume are extended and thoroughly revised versions of shorter papers which were first presented at the 26th international conference on inductive logic programming (ILP’16). ILP 2016 was held in London, during September 4–6 2016, at the Warren House ConferenceCentre. Since its first edition in 1991, the annual ILP conference has served as the premier international forum for learning from structured relational data. Originally focusing on the induction of logic programs, over the years it has expanded its research horizon significantly and welcomed contributions on all aspects of learning in logic, multi-relational data mining, statistical relational learning, graph and tree mining, learning in other (non-propositional) logic-based knowledge representation frameworks, exploring intersections with statistical learning and other probabilistic approaches. Theoretical advances in all these areas have also been accompanied by challenging applications of these techniques to important problems in fields like bioinformatics, medicine, and text mining. ILP’16 received 35 submissions in total: 10 long papers, 19 short papers, and 6 alreadypublished papers. Of these 35 submissions only 4 out of the unpublished papers have made it into this special issue. Our first paper Meta-Interpretive Learning from noisy images by Muggleton et al. provides a novel approach to image classification called Logical Vision. An approach to ILP called Meta-Interpretative Learning is used to learn an accurate classifier based on as few as a one example, due to the use of rich (learnable) background knowledge. The second paper Ultra-strong machine learning: Comprehensibility of programs learned with ILP, also by Muggleton et al, focuses on what has long been a key ‘selling point’ of ILP: the comprehensibility (for humans) of what is learned. A key issue addressed by this	bioinformatics;computer vision;inductive logic programming;inductive reasoning;knowledge representation and reasoning;machine learning;propositional calculus;relational data mining;statistical classification;statistical relational learning;structure mining;text mining;warren abstract machine	James Cussens;Alessandra Russo	2018	Machine Learning	10.1007/s10994-018-5720-6	mathematics;machine learning;artificial intelligence;inductive logic programming	ML	-21.450633163925946	6.827442610588821	163812
45b1ed8ae127e7cd5952313477ec7dc42721e224	the power-method: a comprehensive estimation technique for multi-dimensional queries	voicexml;information retrieval;performance estimation;query optimization;power method;data distribution;multi dimensional;speech user interfaces;distance metric;spoken query retrieval	Existing estimation approaches for multi-dimensional databases often rely on the assumption that data distribution in a small region is uniform, which seldom holds in practice. Moreover, their applicability is limited to specific estimation tasks under certain distance metric. This paper develops the Power-method, a comprehensive technique applicable to a wide range of query optimization problems under various metrics. The Power-method eliminates the local uniformity assumption and is accurate even in scenarios where existing approaches completely fail. Furthermore, it performs estimation by evaluating only one simple formula with minimal computational overhead. Extensive experiments confirm that the Power-method outperforms previous techniques in terms of accuracy and applicability to various optimization scenarios.	circuit complexity;database;experiment;mathematical optimization;monte carlo method;overhead (computing);power iteration;query optimization	Yufei Tao;Christos Faloutsos;Dimitris Papadias	2003		10.1145/956863.956881	query optimization;metric;power iteration;computer science;machine learning;data mining;database;world wide web;information retrieval	DB	-24.81725626810132	4.212331799031045	163877
8ee93ebe416eef50654433b6c0180c77a0116061	an intelligent user interface to cim multiple data bases	intelligent user interface;knowledge based system;user interface;integrable system;semantic interpretation;definite clause grammar;natural language;computer integrated manufacturing	This paper presents an intelligent, natural language-based system to the four major CIM (Computer Integrated manufacturing) data bases in the manufacturing environment: design, planning, manufacturing and accounting data bases. Such a novel integrated framework consists of four databases, four knowledge-based systems, a central manager, and a natural language (NL) user interface. The paper begins with a discussion of the issues of database heterogeneity and integration. Then it gives an overview of the architecture of the integrated framework to link the four CIM database islands and places the position of the NL user interface in the integrated system. Following this, the detailed design of the NL user interface is presented, which is composed of the DCG (Definite Clause Grammar) Parser, Semantic Interpreter and Query Generator. 1.0 INTRODUCTION To design, plan and manufacture a product in the computer integrated manufacturing environment (CIM) requires the utilization of large amount of diverse data. These data are often distributed among many heterogeneous databases. The design data, concentrating on the geometric models and physical characteristics of the product, generally are stored and maintained in a computer aided design (CAD) data base. The planning data, mainly dealing with resource capacity, allocation, and scheduling aspects Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its dale appear, and notice is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission.	computer-aided design;computer-integrated manufacturing;database;definite clause grammar;intelligent user interface;knowledge-based systems;language-based system;nl (complexity);natural language;parser;scheduling (computing);semantic interpretation	David M. Dilts;Wenhua Wu	1990		10.1145/97709.97744	user interface design;natural language processing;user;interface metaphor;natural language user interface;computer science;theoretical computer science;natural user interface;programming language;user interface;graphical user interface testing	DB	-32.485231629953816	10.193972784032029	163930
01e5f74f20251ede7b6b16e2ac4e846ba2dc9dab	set operations in object-based data models	data models object oriented modeling database systems object oriented databases set theory database languages data systems computer science data mining;object based data models;complex objects;query language;database system;type;heritage;sistema informatico;teoria conjunto;semantics;abstraction;theorie ensemble;computer system;set theory;abstraccion;semantica;semantique;set membership;data model;inheritance rules;semantic data model;property characteristics;set theoretic operations;property inheritance object based data models semantics set operations database systems complex objects set type set theoretic operations set difference union intersection symmetric difference set membership inheritance rules property characteristics single valued multivalued;object oriented;multivalued;relacion;database systems;relational model;oriente objet;systeme informatique;modele donnee;object oriented databases;property inheritance;single valued;systeme gestion base donnee;information system;union;set difference;intersection;inheritance;set;sistema gestion base datos;orientado objeto;database management system;relation;symmetric difference;systeme information;objet complexe;set operations;sistema informacion	Query languages designed for traditional database systems, such as the relational model, generally support set operations. However, the semantics of these set operations are not adequate for richer data models of newly developed object-based database systems that include object-oriented and semantic data modeling concepts. The reason is that precise semantics of set operations on complex objects require a clear distinction between the dual notions of a set and a type, both of which are present in the class construct found in object-based data models. In fact, class creation by set operations has largely been ignored in the literature. Our paper lls this gap by presenting a framework for executing set-theoretic operations on the class construct. The proposed set operations, including set diierence, union, intersection and symmetric diierence, determine both the type description of the derived class as well as its set membership. For the former, we develop inheritance rules for property characteristics such as single-versus multi-valued and required versus optional. For the latter, we borrow the object identity concept from data modeling research. Our framework allows for property inheritance among classes that are not necessarily is-a related.	data modeling;database;is-a;object-based language;personal knowledge base;relational model;semantic data model;set theory	Elke A. Rundensteiner;Lubomir F. Bic	1992	IEEE Trans. Knowl. Data Eng.	10.1109/69.149933	semantic data model;complement;set;universal set;relational model;symmetric difference;data model;computer science;relation;artificial intelligence;theoretical computer science;machine learning;intersection;data mining;database;mathematics;semantics;abstraction;index set;programming language;object-oriented programming;type;information system;algorithm;query language;set theory	DB	-29.52320302464385	12.610940904102515	163956
ef878bc9e7ae5e4c9397a1a4443a1b6c272aba7a	interpretation of statistical queries to relational databases	relation algebra;relational database;statistical query	A research in progress concerning the problem of extending the logical independence approach to statistical query answering is described. The main ideas and limitations of a system which assists a user in obtaining a statistical table from a relational database by describing only elements that compose the table are discussed. The proposed system assists the user in formulating statistical queries by means of a universal relation interface, i.e., independently from the underlying database structure. To generate the result, an extended relational algebra is needed to process aggregations in queries function; heuristics for the interpretation of queries and their translation in terms of database primitives are the basis of the proposed approach. 1 . I N T R O D U C T I O N Statistical/scientific databases (SSDBs) are different from traditional databases in several aspects.Usually, they are more static (they represent consolidated events), i.e., update operations are infrequent and are applied to a small percentage of the data; on the other hand, a statistical query is, in general, more complex and involves a large amount of data [Shoshani 85].Tipically, it is harder for the user to be formulate and more expensive for the system to evaluate. There are two kinds of SSDB: micro and macro SSDBs [Wong 84]. Micro-SSDBs handle	heuristic (computer science);micro-operation;relational algebra;relational database	Alessandro D'Atri;Fabrizio L. Ricci	1988		10.1007/BFb0027517	domain relational calculus;sargable;query optimization;database theory;nested set model;relational model;codd's theorem;relational calculus;entity–relationship model;relational algebra;relational database;computer science;query by example;theoretical computer science;database model;relation algebra;database;conjunctive query;candidate key;view;database design	DB	-29.598426049362732	7.118186769378305	164059
2c4a486acbf10c870629d9b980600c0492e6126e	accessing structured health information through english queries and automatic deduction	deductive databases hiv database;theorem proving;deductive question answering;natural language understanding;drug resistance	While much health data is available online, patients who are not technically astute may be unable to access it because they may not know the relevant resources, they may be reluctant to confront an unfamiliar interface, and they may not know how to compose an answer from information provided by multiple heterogeneous resources. We describe ongoing research in using natural English text queries and automated deduction to obtain answers based on multiple structured data sources in a specific subject domain. Each English query is transformed using natural language technology into an unambiguous logical form; this is submitted to a theorem prover that operates over an axiomatic theory of the subject domain. Symbols in the theory are linked to relations in external databases known to the system. An answer is obtained from the proof, along with an English language explanation of how the answer was obtained. Answers need not be present explicitly in any of the databases, but rather may be deduced or computed from the information they provide. Although English is highly ambiguous, the natural language technology is informed by subject domain knowledge, so that readings of the query that are syntactically plausible but semantically impossible are discarded. When a question is still ambiguous, the system can interrogate the patient to determine what meaning was intended. Additional queries can clarify earlier ones or ask questions referring to previously computed answers. We describe a prototype system, Quadri, which answers questions about HIV treatment using the Stanford HIV Drug Resistance Database and other resources. Natural language processing is provided by PARC’s Bridge, and the deductive mechanism is SRI’s SNARK theorem prover. We discuss some of the problems that must be faced to make this approach work, and some of our solutions.	automated theorem proving;axiomatic system;database;language technology;natural deduction;natural language processing;prototype;snark (graph theory)	Richard J. Waldinger;Daniel G. Bobrow;Cleo Condoravdi;Kyle Richardson;Amar Das	2011			natural language processing;drug resistance;computer science;artificial intelligence;machine learning;data mining;automated theorem proving;algorithm	AI	-24.450139937805357	7.392851959660024	164099
d303622a00bfb5d3cf7b27e26b85b0f38601b522	towards topological consistency and similarity of multiresolution geographical maps	topology;query processing;gis;system integration;spatial databases;similarity;multiresolution;multi resolution;similarity function;topological relations;consistency;topological relation	Several application contexts require the ability to use together and compare different geographic datasets (maps) concerning the same or overlapping areas. This is for example the case of mediator systems, integrating distinct data sources for query processing, and GISs dealing with multi-resolution maps. In both cases, distinct maps may represent the same geographic feature with different geometry type (a road can be a region in one map and a line in another one). An important issue is therefore determining whether two multi-resolution maps are consistent, i.e., they represent the same area without contradictions, and, if not, if they are at least similar. In this paper we consider consistency and similarity of multi-resolution maps with respect to topological information. Existing approaches do not take feature geometry type into account. In this paper, we extend them with two notions of topological consistency, the first requiring the same topological relation between pairs of common features, the second `relaxing' the first one by considering similarity between topological relations. A similarity function for multi-resolution maps is then provided, taking into account both feature geometry types and topological relations of map objects. We finally discuss how the proposed consistency and similarity concepts can be significantly used in GIS applications. Some experimental results are also reported to show the effectiveness of the proposed approach.	database;gis applications;geographic information system;line level;map;multiresolution analysis;similarity measure	Alberto Belussi;Barbara Catania;Paola Podestà	2005		10.1145/1097064.1097096	generalized map;discrete mathematics;geomatics;topology;similarity;quasi-open map;data mining;mathematics;geometry;consistency;system integration	DB	-27.7230977682078	7.786135697488339	164339
8d8489028e9c034e267eb26d703e0819ed1f4b25	methods in query languages of object oriented databases.	query language;object oriented database			Karol Gilarski	2004			natural language processing;sargable;query optimization;query expansion;web query classification;data control language;query by example;database;rdf query language;programming language;web search query;view;query language;object query language;spatial query;object definition language	DB	-31.486605848324746	8.769000918825792	164479
dbe73d0bfa10265015059b9b5ecf2af56c48a705	reducing redundancy of xpath query over networks by transmitting xml views	optimization technique;redundancy elimination;suffix tree;xpath;xml;communication cost;xml document;query rewrite;query rewriting	XPath is a language to retrieve information in the XML documents, which is used to traverse their elements and attributes. At the present time there are a large number of optimization techniques on XML applications at home and abroad, including axis join queries, query rewriting, the semantic cache using in the client, redundancy elimination based on marked suffix tree (MST) and so on. However, these techniques are lack of research to eliminate redundancy in the network environments. In this paper, we reduce communication costs and computing costs over networks by rewriting the queries submitted by the client. Given a set of queries, we compute a minimal XML view that can answer all the original queries and eliminate possible redundancy.	xml;xpath	Yanjun Xu;Shicheng Xu;Chengbao Peng;Zhang Xia	2010		10.1007/978-3-642-16720-1_9	xml validation;xml encryption;simple api for xml;xml;xml schema;streaming xml;computer science;xpath 2.0;document structure description;xml framework;xml database;xml schema;database;xml signature;programming language;information retrieval;efficient xml interchange	DB	-30.987813920017413	4.786310553039449	164503
691c2827994389502853790bb448134f3c13e61b	a tool for ramification reasoning over temporal owl knowledge bases	knowledge base	In this paper we study the ramification problem in the settin g of time-owl. Standard solutions from the literature on reasoning about action are inadequate because they rely o n the assumption that fluents persist, and actions have effec ts on the next situation only. In this paper we provide a solution t theramification problembased on an extension of the situation calculus and the work of McCain and Turner. In our approach li es the use of static and dynamic rules which capture the indir ect and direct effects of actions. Also our tool has the ability t o address the frame problemwhich refers to the identification of fluents that remain unchanged as result of actions.	fluent (artificial intelligence);java;ontology (information science);ramification problem;semantic reasoner;situation calculus;web ontology language	Nikos Papadakis;Stavros Boutzas	2010	KES Journal	10.3233/KES-2010-0199	computer science;artificial intelligence;algorithm	AI	-19.532063662744214	8.15654870755297	164642
ae24e644823a8ac3e5a5649eeb4e27c16bef83b0	identifying a forest hierarchy in an oodb specification hierarchy satisfying disciplined modeling	manufacturing systems;database system;federated system;information retrieval;vocabulary;partitioning;oodb specialization hierarchy;satisfiability;large schemas;skeleton;large vocabularies;medical services;disciplined modeling;object oriented modeling vocabulary object oriented databases skeleton manufacturing systems terminology medical services dictionaries computer aided manufacturing database systems;computer aided manufacturing;database systems;dictionaries;informational thinning;distributed databases;partitioning object oriented databases oodb specialization hierarchy disciplined modeling large schemas large vocabularies forest hierarchy federated system information retrieval informational thinning;terminology;object oriented databases;object oriented database;very large databases;forest hierarchy;database theory;object oriented modeling	Our work is motivated by the desire to develop methods to comprehend large vocabularies and large schemas of Object-Oriented Databases. The ability of a user of a database participating in a federated system to retrieve information from the other database systems will be greatly enhanced by acquiring a better comprehension of these systems. We are trying to develop both a theoretical paradigm and a methodology to analyze existing large schemas. Our approach to achieve comprehension is based on combining two concepts: informational thinning (i.e. concentration on the specialization hierarchy of the schema) and partitioning. In this paper we present a new technique for modeling which is called disciplined modeling. Based on the rules of disciplined modeling we develop a theoretical paradigm to support the existence of a meaningful forest hierarchy within the specialization hierarchy. Such a hierarchy functions as a skeleton of the schema and supports comprehension and partitioning eeorts. 1 Motivation One of the problems in cooperative information systems is that the user of each system is familiar with his own applications, databases and terminologies, but he is much less familiar with those of the other systems. Therefore, the cooperation suuers from the lack of comprehension of the schemas and terminologies used in the other federated systems. Our work is motivated by the desire to comprehend large vocabularies and schemas of Object-Oriented Databases (OODB) and to develop methods to make them understandable for users. We are developing a theoretical framework and a methodology which will enable the design of tools to aid comprehension of existing OODB systems. Graphical presentation is a very important tool to support schema comprehension. Thus, we assume that schemas and vocabularies are represented with a graphical schema language and manipulated with a graphical editor. Our approach to achieve comprehension of graphical schemas is based on two concepts: informational thinning (i.e. concentration on the specialization hierarchy) and partitioning. In this paper we present a new technique for mod-eling called disciplined modeling. Based on the rules of disciplined modeling we develop a theoretical paradigm to support the existence of a meaningful forest within a specialization hierarchy. This forest represents a partitioning of the specialization hierarchy into trees and functions as a skeleton of the schema that supports comprehension eeorts. In 1] we will present a methodology based on our paradigm for nding a forest hierarchy for a given schema. This research is part of our current OOHVR …	graphical user interface;information system;mod database;partial template specialization;programming paradigm;schema evolution;thinning;vocabulary	Yehoshua Perl;James Geller;Huanying Gu	1996		10.1109/COOPIS.1996.555010	database theory;computer science;knowledge management;data mining;database;distributed computing;federated architecture;programming language;terminology;skeleton;world wide web;distributed database;computer-aided manufacturing;satisfiability	DB	-32.21036855494119	11.87078459132266	164702
e68687cbaf4ee56ab20c939224c1cea97092a065	state-based modelling in hazard identification	model based reasoning;simulation;batch hazop;directed graph;qualitative modelling;batch process	The signed directed graph (SDG) is the most commonly used type of model for automated hazard identification in chemical plants. Although SDG models are efficient in simulating the plant, they have some weaknesses, which are discussed here in relation to typical process industry examples. Ways to tackle these problems are suggested, and the view is taken that a state-based formalism is needed, to take account of the discrete components in the system, their connection together, and their behaviour over time. A strong representation for operations and actions is also needed, to make the models appropriate for modelling batch processes. A research prototype for HAZOP studies on batch plants (CHECKOP) is also presented, as an illustration of the suggested approach to modelling.		Stephen McCoy;Dingfeng Zhou;Paul W. H. Chung	2006	Applied Intelligence	10.1007/s10489-006-8517-4	directed graph;computer science;model-based reasoning;machine learning;algorithm;batch processing	SE	-24.154542448982216	15.867204996952193	164806
15ab1e195bd49b5d0d1b41bd03ad85f280c1eb75	a generalised relational database model	intelligent database;first order probabilistic language;generalised relational database model;image databases;uncertainty;uncertain knowledge processing;probabilistic logic deductive databases relational databases generalisation artificial intelligence problem solving uncertainty handling database theory inference mechanisms;inference mechanisms;uncertainty handling;data engineering;relational database;two sorted language;uncertain relational database;urd;first order;engineering management;solid modeling;formal logic;retrieval mechanism;relational databases deductive databases probabilistic logic uncertainty image databases solid modeling computer science data engineering australia engineering management;relational databases;generalisation artificial intelligence;computer science;probabilistic logic;reasoning;database management system;reasoning with uncertainty;database theory;problem solving;australia;formal logic generalised relational database model decision making problem solving intelligent database uncertain knowledge processing reasoning uncertain relational database urd two sorted language first order probabilistic language retrieval mechanism;deductive databases	Information provided by database management systems is frequently used as an input to decision making and problem solving in uncertain circumstances. Accordingly, an intelligent database which has uncertain knowledge processing reflects the user's perception of real world requirements. This paper aims to describe a theoretical basis of an intelligent database which is able to represent, manipulate and reason with uncertainty in actual life. The model called Uncertain Relational Database (URD), a generalised relational database model, is formalised by means of logic theories based on a two-sorted first-order probabilistic language in order to have a solid, powerful and unifying retrieval mechanism.	database model;relational database;relational model	Hairong Yu;Arthur Ramer	1998		10.1109/ICSMC.1998.728112	data definition language;database theory;relational model;intelligent database;semi-structured model;entity–relationship model;data model;relational database;computer science;probabilistic database;database model;machine learning;data mining;database;view;database schema;object-relational impedance mismatch;database testing;database design	DB	-27.88422279373648	8.966570731369185	164860
4b97eeb496dc268e3e6631e09171356a5fd72f36	evaluation of rule processing strategies in expert databases	rule processing strategies;pattern relations;database system;rule relation;performance evaluation;expert systems;pattern relations performance evaluation rule processing strategies expert databases join results base relations rule relation clustered b trees;query processing;expert databases;database systems firing costs material storage delay expert systems query processing decision support systems production systems production planning;database management systems;trees mathematics;join results;firing;trees mathematics database management systems expert systems performance evaluation;material storage;database systems;decision support systems;production systems;production planning;clustered b trees;base relations	This paper studies rule processing strategies in expert database systems which involve rules conditional on join results of base relations. In particular, we are concemed with those rules that require very fast response time in their evaluation. We propose to materialize the results of firing a rule in a relation. the rule relation. Performance evaluation of several strategies is presented.	database;performance evaluation;response time (technology)	Arie Segev;J. Leon Zhao	1991		10.1109/ICDE.1991.131489	conflict resolution strategy;decision support system;computer science;machine learning;data mining;database;production system;expert system	DB	-27.776962025349434	4.490070855396792	164870
200ba0c4218547b256115a560c0de8ef07bc93ef	or-sml: a functional database programming language for disjunctive information and its applications	relation algebra;standard ml;database programming languages;database query	We describe a functional database language OR-SML for handling disjunctive information in database queries, and its implementation on top of Standard ML. The core language has the power of the nested relational algebra, and it is augmented with or-sets which are used to deal with disjunctive information. Sets, or-sets and tuples can be freely combined to create objects, which gives the language a greater flexibility. We give examples of queries which require disjunctive information (such as querying incomplete or independent databases) and show how to use the language to answer these queries. Since the system runs on top of Standard ML and all database objects are values in the latter, the system benefits from combining a sophisticated query language with the full power of a programming language. OR-SML includes a number of primitives that deal with bags and aggregate functions. It is also configurable by user-defined base types. The language has been implemented as a library of modules in Standard ML. This allows the user to build just the database language as an independent system, or to interface it to other systems built in Standard ML. We give an example of connecting OR-SML with an already existing interactive theorem prover.	apl;aggregate data;automated theorem proving;database;disjunctive normal form;programming language;proof assistant;query language;relational algebra;standard ml	Elsa L. Gunter;Leonid Libkin	1994		10.1007/3-540-58435-8_230	fourth-generation programming language;first-generation programming language;data definition language;query optimization;database theory;declarative programming;data manipulation language;programming domain;data control language;computer science;theoretical computer science;database model;functional logic programming;relation algebra;database;programming paradigm;fifth-generation programming language;programming language;view;database schema;second-generation programming language;database design;query language;component-oriented database	DB	-29.762314516944244	10.629020732483276	164936
c36d515c099094c5181f84a439f6236056d8081a	the design and implementation of a digital music library	busqueda informacion;query language;information structure;database system;digital music libraries;structure information;music database systems music index structures;information retrieval;musica;index structure;database;content based music information retrieval;estructura informacion;base dato;musical instruments;data model;digital music;digital music libraries content based music information retrieval;biblioteca electronica;musique;design and implementation;recherche information;busqueda por contenido;feature extraction;music information retrieval;base de donnees;music index structures;electronic library;theoretical foundation;content based retrieval;music;recherche par contenu;bibliotheque electronique;music database systems	The design and implementation of Harbin Institute of Technology—Digital Music Library (HIT-DML) is presented in this paper. Firstly, a novel framework, a music data model, and a query language are proposed as the theoretical foundation of the library. Secondly, music computing algorithms used in the library for feature extracting and matching are described. In addition, indices are introduced for both mining themes of music objects and accelerating content-based information retrieval. Finally, experimental results on the indices and the current development of the library are provided. HIT-DML is distinguished by the following points. First, it is inherently based on database systems, and combines database technologies with multimedia technologies seamlessly. Musical data are structurally stored. Second, it has a solid theoretical foundation, from framework and data model to query language. Last, it can retrieve musical information based on content against different kinds of musical instruments. The indices used, also power the library.	algorithm;approximation algorithm;correctness (computer science);data manipulation language;data model;database;digital library;entity–relationship model;feedback;goto;hit-or-miss transform;ibm notes;information retrieval;library (computing);pattern matching;peer-to-peer;prototype;query language;sound and music computing;zhao youqin's π algorithm	Changping Wang;Jianzhong Li;Shengfei Shi	2005	International Journal on Digital Libraries	10.1007/s00799-005-0125-0	speech recognition;feature extraction;data model;computer science;music;database;multimedia;world wide web;query language	DB	-31.023654385479468	5.296183514718134	164964
50a6dea99abc3067309c9c52f5daf64dc1b93bf3	the relational model with relation-valued attributes	base relacional dato;expression imbriquee;journal_article;transformacion;relational database;data base management system;non first normal form;relacion;relational model;rested expression;base donnee relationnelle;algebre relationnel;transformation;systeme gestion base donnee;sistema gestion base datos;relation;relational algebra	Motivated by new applications of database systems we introduce relations which may have relation-valued attributes and propose a related algebra. Formal definitions for this extended relational model can be given by applying usual notions recursively. The main problem considered in this paper is the formal definition of an appropriate relational algebra for these non-first-normal-form relations. We allow the application of the basic relational operators to any relation-valued attribute u;ithin a relation. This leads to a (hierarchically) nested relational algebra.	database;first normal form;recursion;relational algebra;relational model;relational operator	Hans-Jörg Schek;Marc H. Scholl	1986	Inf. Syst.	10.1016/0306-4379(86)90003-7	transformation;domain relational calculus;relational model/tasmania;nested set model;relational model;codd's theorem;statistical relational learning;relational theory;relational calculus;entity–relationship model;relational algebra;relational database;computer science;relation;data mining;database;projection;conjunctive query;candidate key;superkey;algorithm;tuple relational calculus	DB	-27.41660718205812	10.409774246472725	165169
