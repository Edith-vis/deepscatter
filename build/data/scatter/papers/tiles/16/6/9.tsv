id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
7a6d012b352d503070a9b91df79e1326da821e5c	prototyping a tool environment for run-time assertion checking in jml with communication histories	attribute grammars;runtime assertion checking;generic model;tool support;java programming;java modeling language;attribute grammar;run time assertion checking;meta programming;interface specification	In this paper we present prototype tool-support for the runtime assertion checking of the Java Modeling Language (JML) extended with communication histories specified by attribute grammars. Our tool suite integrates Rascal, a meta programming language and ANTLR, a popular parser generator. Rascal instantiates a generic model of history updates for a given Java program annotated with history specifications. ANTLR is used for the actual evaluation of history assertions.	antlr;assertion (software development);attribute grammar;compiler-compiler;java modeling language;metaprogramming;programming language;prototype;rascal	Frank S. de Boer;Stijn de Gouw;Jurgen J. Vinju	2010		10.1145/1924520.1924526	computer science;theoretical computer science;java modeling language;database;real time java;programming language;generics in java	PL	-26.225189708291314	28.53649189502079	40889
ba1cc504f6d3c43b3b9f36e818d310c65cc9c0e0	dynamic dispatch for method contracts through abstract predicates	abstract predicates;modular specification;jml;design by contract;dynamic dispatch	Dynamic method dispatch is a core feature of object-oriented programming by which the executed implementation for a polymorphic method is only chosen at runtime. In this paper, we present a specification and verification methodology which extends the concept of dynamic dispatch to design-by-contract specifications. The formal specification language JML has only rudimentary means for polymorphic abstraction in expressions. We promote these to fully flexible specification-only query methods called model methods that can, like ordinary methods, be overridden to give specifications a new semantics in subclasses in a transparent and modular fashion. Moreover, we allow them to refer to more than one program state which give us the possibility to fully abstract and encapsulate two-state specification contexts, i.e., history constraints and method postconditions. We provide the semantics for model methods by giving a translation into a first order logic and according proof obligations. We fully implemented this framework in the KeY program verifier and successfully verified relevant examples.	denotational semantics;design by contract;dynamic dispatch;first-order logic;formal specification;java modeling language;key;postcondition;run time (program lifecycle phase);specification language;state (computer science)	Wojciech Mostowski;Mattias Ulbrich	2015		10.1145/2724525.2724574	computer science;database;programming language;algorithm	PL	-26.10414088452548	28.66580838675312	40900
581219360e53302636c88ae55edad6597d813677	code analysis for temporal predictability	computacion informatica;real time;graph transformation;embedded systems;worst case execution time;hard real time system;data dependence;ciencias basicas y experimentales;real time languages;compiler optimization;code transformation;worst case execution time analysis;inbaddad systemteknik;abstract interpretation;grupo a;compiler optimizations;article;hard real time	The execution time of software for hard real-time systems must be predictable. Further, safe and not overly pessimistic bounds for the worst-case execution time (WCET) must be computable. We conceived a programming strategy called WCET-oriented programming and a code transformation strategy, the single-path conversion, that aid programmers in producing code that meets these requirements. These strategies avoid and eliminate input-data dependencies in the code. The paper describes the formal analysis, based on abstract interpretation, that identifies input-data dependencies in the code and thus forms the basis for the strategies provided for hard real-time code development.	abstract interpretation;best, worst and average case;branch (computer science);compiler;computable function;control flow;data dependency;dependence analysis;imperative programming;programmer;programming language;real-time computing;real-time locating system;real-time transcription;requirement;run time (program lifecycle phase);worst-case execution time	Jan Gustafsson;Björn Lisper;Raimund Kirner;Peter P. Puschner	2005	Real-Time Systems	10.1007/s11241-005-4683-4	code bloat;parallel computing;real-time computing;code review;computer science;operating system;redundant code;optimizing compiler;programming language;code generation;static program analysis	Embedded	-19.954763371939865	32.2590346726814	41231
419e1ab146d0b8ab9c63f6431357cc82ee84bfeb	discrete math with programming: better together	object oriented programming;discrete mathatics	This paper proposes a Discrete Mathatics course that is integrated with programming. The course consists of a sequence of Math modules with coordinated programming projects. Advantages of this approach are presented, and a methodology for developing the course is shown. A sample list of Math modules and brief project descriptions are included.		Kirby McMaster;Nicole Anderson;Brian Rague	2007		10.1145/1227310.1227348	simulation;programming domain;reactive programming;functional reactive programming;computer science;theoretical computer science;functional logic programming;programming paradigm;procedural programming;symbolic programming;inductive programming;programming language;object-oriented programming	PL	-29.712935827352567	24.327064957042513	41372
0e23475d3b93b4af01ebbfadf30dafbd757e3757	scrub-systematically clean and renumber users basic		Abstract#R##N##R##N#SCRUB is an aid for programmers using BASIC. It lists programs in a uniform layout and provides programmers with a concordance of all simple variables, arrays, functions, subroutines and line numbers used as labels. Full syntax checks are made so that SCRUB may be used as an aid to program development and standardization.	basic;clean;data scrubbing;line number	A. R. Lawrence	1978	Softw., Pract. Exper.	10.1002/spe.4380080211	simulation;computer science;operating system;programming language	HCI	-27.732289854489476	25.794818747976063	41491
77d8f8c07ed0d7311982570e034c295d9d53748b	writing truly efficient smalltalk	visualworks;performance;polymorphism;smalltalk;optimization	This paper shows how to substantially increase the performance of Smalltalk programs by creating more classes to take advantage of polymorphism. An improved implementation of the well-known message match:, using this and other techniques, can run up to twice as fast as the current inlined implementation VisualWorks Smalltalk includes. In this particular case, creating more classes is shown to be so powerful as to become preferable to heavy use of identity checks on immediate objects by a margin of up to 20% on average. In addition, non-inlined implementations compare quite well to the existing inlined implementation of match:. While they can run faster in some cases, their overall performance falls behind by no more than a factor of 2.This is a quick summary of chapter 3 from the book currently being written by the author. It is due to be published in 2007.	inline expansion;smalltalk;visualworks;whole earth 'lectronic link	Andrés Valloud	2006		10.1145/1176617.1176675	polymorphism;performance;computer science;programming language	PL	-24.903748605896876	29.1217244726499	41681
6af2efe9b96421eba62093edb576a8cf0e5246c1	alloy*: a general-purpose higher-order relational constraint solver	metals;boolean functions;semantics;higher order;alloy;alloy analyzer engine general purpose higher order relational constraint solver software analysis runtime analysis first order solver domain specific algorithm kodkod model relational logic;engines;syntactics;data structures;constraint handling;constraint solving;program analysis;static analysis;metals syntactics semantics data structures boolean functions concrete engines;higher order alloy constraint solving;concrete	The last decade has seen a dramatic growth in the use of constraint solvers as a computational mechanism, not only for analysis of software, but also at runtime. Solvers are available for a variety of logics but are generally restricted to first-order formulas. Some tasks, however, most notably those involving synthesis, are inherently higher order; these are typically handled by embedding a first-order solver (such as a SAT or SMT solver) in a domain-specific algorithm.  Using strategies similar to those used in such algorithms, we show how to extend a first-order solver (in this case Kodkod, a model finder for relational logic used as the engine of the Alloy Analyzer) so that it can handle quantifications over higher-order structures. The resulting solver is sufficiently general that it can be applied to a range of problems; it is higher order, so that it can be applied directly, without embedding in another algorithm; and it performs well enough to be competitive with specialized tools. Just as the identification of first-order solvers as reusable backends advanced the performance of specialized tools and simplified their architecture, factoring out higher-order solvers may bring similar benefits to a new class of tools.	algorithm;alloy analyzer;boolean satisfiability problem;domain-specific language;first-order logic;first-order predicate;first-order reduction;general-purpose markup language;integer factorization;model of computation;relational algebra;run time (program lifecycle phase);satisfiability modulo theories;solver	Aleksandar Milicevic;Joseph P. Near;Eunsuk Kang;Daniel O Jackson	2015	2015 IEEE/ACM 37th IEEE International Conference on Software Engineering	10.1109/ICSE.2015.77	program analysis;higher-order logic;concrete;computer science;theoretical computer science;semantics;boolean function;programming language;static analysis;algorithm	SE	-21.73986820102764	18.739838219065906	41727
03091802b0d8f947802c3085fe492aef46d2cfac	implementing synchronous models on loosely time triggered architectures	software architecture formal specification programming language semantics;programming language semantics;models of computation;loosely time triggered architecture synchronous models semantic equivalence;formal specification;semantics preserving implementation;code generation;system architectures;design space;embedded system;synchronous system;embedded systems;software architecture;integration and modeling real time systems and embedded systems models of computation system architectures;semantic equivalence;time triggered architecture;model of computation;integration and modeling;distributed systems;system architecture;synchronous models;real time systems and embedded systems;loosely time triggered architecture;real time systems	Synchronous systems offer a clean semantics and an easy verification path at the expense of often inefficient implementations. Capturing design specifications as synchronous models and then implementing the specifications in a less restrictive platform allow to address a much larger design space. The key issue in this approach is maintaining semantic equivalence between the synchronous model and its implementation. We address this problem by showing how to map a synchronous model onto a loosely time-triggered architecture that is fairly straightforward to implement as it does not require global synchronization or blocking communication. We show how to maintain semantic equivalence between specification and implementation using an intermediate model (similar to a Kahn process network but with finite queues) that helps in defining the transformation. Performance of the semantic preserving implementation is studied for the general case as well as for a few special cases.	blocking (computing);kahn process networks;tcp global synchronization;time-triggered architecture;turing completeness	Stavros Tripakis;Claudio Pinello;Albert Benveniste;Alberto L. Sangiovanni-Vincentelli;Paul Caspi;Marco Di Natale	2008	IEEE Transactions on Computers	10.1109/TC.2008.81	model of computation;embedded system;parallel computing;real-time computing;computer science;theoretical computer science;operating system;distributed computing;programming language;systems architecture	Embedded	-28.948220535512668	32.16654264237239	41753
e09b44db24f2e15f2ea25b03f956b1732a5d4b5d	automated exercises for constraint programming.		We describe the design, implementation, and empirical evaluation of some automated exercises that we are using in a lecture on Constraint Programming. Topics are propositional satisfiability, resolution, the DPLL algorithm, with extension to DPLL(T), and FD solving with arc consistency. The automation consists of a program for grading student answers, and in most cases also a program for generating random problem instances. The exercises are part of the autotool Eassessment framework. The implementation language is Haskell. You can try them at https://autotool.imn.htwk-leipzig.de/cgi-bin/Trial. cgi?lecture=199.	boolean satisfiability problem;constraint programming;dpll algorithm;gnu build system;haskell;local consistency;object language;resolution (logic)	Johannes Waldmann	2014			concurrent constraint logic programming;constraint programming;inductive programming	AI	-21.401579279210164	19.73051521322464	41864
2158e71b313ef376e5c33415873c89a7054d59a3	type-based object immutability with flexible initialization	polymorphism;ownership types;article in monograph or in proceedings;type system	We present a type system for checking object immutability, read-only references, and class immutability in an open or closed world. To allow object initialization outside object constructors (which is often needed in practice), im ­ mutable objects are initialized in lexically scoped regions. The system is simple and direct; its only type qualifiers specify immutability properties. No auxiliary annotations, e.g., ownership types, are needed, yet good support for deep im ­ mutability is provided. To express object confinement, as required for class im ­ mutability in an open world, we use qualifier polymorphism. The system has two versions: one with explicit specification commands that delimit the object initialization phase, and one where such commands are implicit and inferred. In the latter version, all annotations are compatible with Java’s extended annotation syntax, as proposed in JSR 308.	immutable object;java community process;open world;read-only memory;scope (computer science);type inference;type system	Christian Haack;Erik Poll	2009		10.1007/978-3-642-03013-0_24	polymorphism;type system;computer science;artificial intelligence;database;programming language;algorithm	PL	-24.179076778874524	26.87317260914225	41953
10c5a3bf9f99f97f05ed8dbd413d584c41fdca9a	an overview of life	programming language;functional programming;functional equation;logic programs;data structure	LIFE (Logic, Inheritance, Functions, Equations) is an expe rim ntal programming language with a powerful facility for structured type inher itance. LIFE reconciles styles from Functional Programming and Logic Programming by impli citly delegating control to an automatic suspension mechanism. This allows interleavi ng interpretation of relational and functional expressions which specify abstract structu al dependencies on objects. Together, these features provide a convenient and versatil e power of abstraction for very high-level expression of constrained data structures. Quelle est la vie du mathématicien ? Quels sentiments exprime son langage ? [...] Calembours, jeux de mots, associations fortuites, la piste est chaude pour l’analyst e. Là où le rapport logique, conscient, est flottant, le rappo rt inconscient peut être fécond. Pierre Berloquin Un souvenir d’enfance d’Evariste Galois	computer programming;computer-aided design;data structure;functional programming;gramática de la lengua castellana;high- and low-level;linear algebra;logic programming;multiple inheritance;natural language processing;programming language;prototype;public-domain software;real life;structured type;type system	Hassan Aït-Kaci	1990		10.1007/3-540-54141-1_4	first-generation programming language;constraint programming;declarative programming;very high-level programming language;language primitive;programming domain;reactive programming;functional reactive programming;computer science;programming language implementation;theoretical computer science;extensible programming;functional logic programming;programming paradigm;procedural programming;symbolic programming;inductive programming;fifth-generation programming language;programming language theory;programming language;prolog;logic programming;high-level programming language;algorithm	PL	-23.175005106265203	20.738382665538282	42010
0776863604639e07727c158bfb7a0630e95b4456	nsedit: a syntax-directed editor and testing tool based on nassi-shneiderman charts	pascal language;text editor;programming environment;assisted programming;ingenieria logiciel;software engineering;systeme conversationnel;medio ambiente programacion;nassi shnciderman charts;interactive system;test coverage;sistema conversacional;editor texto;genie logiciel;editeur texte;syntax directed editor;ayuda programacion;pascal;aide programmation;environnement programmation	Abstract#R##N##R##N#This paper describes a programming environment for Pascal, called NSEDIT, which runs under VAX/VMS1 . NSEDIT is based upon the notion of Nassi-Shneiderman charts (abbreviated to N-S charts in this paper) and is designed for ease of use by novice programmers in a teaching situation. A key component of the system is the editor, which enables structural features of Pascal to be manipulated in terms of their N-S chart box representations. Simultaneous editing of more than one program or module is permitted. Another key feature of the system is a semiautomatic interface to the VAX/VMS Pascal compiler, with subsequent display of execution counts as part of the N-S charts. In this way users can obtain immediate feedback on the coverage of program elements by test runs, while still inside an editing session.	chart;nassi–shneiderman diagram	Keith Halewood;Martin R. Woodward	1988	Softw., Pract. Exper.	10.1002/spe.4380181005	pascal;computer science;engineering;operating system;code coverage;programming language;algorithm;pascal;computer graphics (images)	SE	-27.501660177683146	25.644470870224733	42198
87dc964895f09eaa8ba4ab094223311bde41c6a9	a knowledge-based system to synthesize fp programs from examples	knowledge based system;program synthesis;functional programming;input output	This paper describes a knowledge-based system to synthesize functional programs of Backus' FP system [1,2] from input / output instances. Based on a theory of orthogonal expansion of programs [3,4], the task of program synthesis is expressed in program equations, and fulfilled by solving them according to the knowledge about the equivalence between programs. Examples are given in the paper.	knowledge-based systems	Hong Zhu;Lingzi Jin	1989		10.1007/3-540-51665-4_89	input/output;real-time computing;computer science;artificial intelligence;theoretical computer science;functional programming;algorithm	Logic	-26.58761824827544	20.724049142884706	42366
20e386145bd2e5a4c70afd71ddd806a1349989ef	game semantics for a polymorphic programming language	model checking techniques;system f;programming language semantics;computer languages;game theory;observational equivalence;programming language;general references;game semantics;semantics;genericity;finite element;law;model checking techniques game semantics polymorphic programming language system f question answer labelling;formal verification;full abstraction;model checking;general references game semantics polymorphism genericity;question answer labelling;polymorphism;games;polymorphic programming language;semantics games object oriented modeling law context computer languages;localized state;programming language semantics formal verification game theory;context;object oriented modeling;question answering	Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact J Laird. 2013 Copyright is held by the author/owner(s): J Laird	blueprint;call-by-push-value;computation;copycat (software);game semantics;generic programming;intensional logic;lawrence a. hyland;model checking;name binding;polymorphic programming language;programmer;system f;turing completeness;value (ethics)	James Laird	2010		10.1109/LICS.2010.32	system f;model checking;games;game theory;polymorphism;question answering;formal verification;computer science;game semantics;theoretical computer science;finite element method;mathematics;semantics;programming language;algorithm	Logic	-20.782255330703126	19.992221280901028	42666
27a2f3da2f1a29198805e5481da804e902494d74	symbolic execution: a semantic approach	semantica operacional;programmation;semantica denotacional;specification;operational semantics;ejecucion programa;program execution;programacion;symbolic execution;semantique operationnelle;especificacion;informatique theorique;execution programme;denotational semantics;execution symbolique;programming;semantique denotationnelle;computer theory;informatica teorica	This paper discusses symbolic execution from a semantic point of view, covering both programs and specifications. It defines the denotational semantics of symbolic execution of specifications and programs, and thus introduces a notion of correctness of symbolic execution which applies not just to an individual language but to a wide class of languages, namely those whose semantics can be described in terms of states and state transformations. Also described are the operational semantics of a language as used for symbolic execution.#R##N##R##N#This work also provided the basis of the prototype symbolic execution system SYMBEX which was developed at the University of Manchester as part of the mural project. However, this paper only covers the theoretical foundations used by SYMBEX, but not the system itself.	symbolic execution	Ralf Kneuper	1991	Sci. Comput. Program.	10.1016/0167-6423(91)90008-L	programming;computer science;theoretical computer science;symbolic data analysis;programming language;operational semantics;specification;concolic testing;denotational semantics;symbolic trajectory evaluation;algorithm	PL	-19.38078859384671	21.99812224202122	42730
cc6d7a08b8035b22cf16e56938b3186331557ed8	a proof system for ada tasks	proceso secuencial comunicante;lenguaje de programacion;detection erreur;comportement rendez vous;programmation;semantica denotacional;programming language;gestion labor;ada;estudio comparativo;deteccion de error;communicating sequential process;concurrent program;programacion;etude comparative;gestion tâche;denotational semantics;programa competidor;comparative study;langage programmation;deadlock;interbloqueo;error detection;dating behavior;interblocage;task scheduling;ada language;processus sequentiel communiquant;programming;semantique denotationnelle;programme concurrent;systeme preuve		ada;proof calculus	Howard Barringer;I. Mearns	1986	Comput. J.	10.1093/comjnl/29.5.404	programming;error detection and correction;ada;computer science;artificial intelligence;deadlock;comparative research;programming language;denotational semantics;algorithm	Robotics	-23.97213886640756	31.619568665067806	42832
704dedb98f1d4bdeb270ec75955858fd273df1f4	on open arrays and variable number of parameters	programming language design;automatic parameter conversion;variable number of parameters;open arrays	Two facilities for programming languages are described: open arrays (an extension of Pascal conformant arrays) and automatic parameter conversion. As a result of combining these two mechanisms, it is possible to give compile-time verifiable specification fo procedures with a variable number of parameters and varying types. This ability is very useful in many applications, and in particular in specifiying I/O procedures within a programming language itself.		Claudio Sergio Da Rós de Carvalho;Tomasz Kowaltowski	1992	Comput. Lang.	10.1016/0096-0551(92)90023-G	computer science;theoretical computer science;algorithm	NLP	-24.337983586151196	26.319956152764817	42859
153d4aa7f5c20e1bbd4d464e1a92a4f5724eaac9	towards intuitive interaction for end-user programming	programming by example;programming by demonstration;html wrapper generation;generic programming;end user programming	Communication between a user and a programming system is a crucial aspect of any programming activity. Even if the programmer knows a step by step description of how to solve a problem, it is often difficult to express this description in a form that can be understood by the system. The reason for this communication problem [1] is the gap between the programmer’s knowledge and problem domain representation on the one hand, and the knowledge built into the programming system on the other hand: The programmer has to translate her problem-specific, concrete representation into the abstract, detailed, and technical language of the system. This translation is difficult and error-prone even for expert programmers, but for casual users it is an unsurmountable hurdle. Simplifying programming is obviously a worthwhile endeavor, especially when it empowers casual users (constituting the vast majority of computer users) to adapt and extend the capabilities of their applications in order to solve their specific problems more efficiently. Programming by Demonstration (PbD) is a subbranch of end-user programming that aims to bridge the knowledge gap between user and system by allowing the user to “program in the user interface”, that is, the user demonstrates to the system how to solve a task	admissible numbering;cognitive dimensions of notations;computer programming;end-user development;jargon;problem domain;programmer;programming by demonstration;user (computing);user interface	Eric Schwarzkopf;Mathias Bauer;Dietmar Dengler	2003		10.1145/604045.604101	first-generation programming language;constraint programming;protocol;declarative programming;very high-level programming language;delegation;programming domain;reactive programming;functional reactive programming;computer science;theoretical computer science;extensible programming;functional logic programming;database;programming paradigm;event-driven programming;procedural programming;symbolic programming;inductive programming;fifth-generation programming language;programming language;generic programming;system programming;concurrent object-oriented programming	HCI	-28.500441529544503	24.47981502981069	42941
b62772e0121ad2658c7e30e4cc4f50fbbe522d66	tree buffers		In runtime verification, the central problem is to decide if a given program execution violates a given property. In online runtime verification, a monitor observes a program’s execution as it happens. If the program being observed has hard real-time constraints, then the monitor inherits them. In the presence of hard real-time constraints it becomes a challenge to maintain enough information to produce error traces, should a property violation be observed. In this paper we introduce a data structure, called tree buffer, that solves this problem in the context of automata-based monitors: If the monitor itself respects hard real-time constraints, then enriching it by tree buffers makes it possible to provide error traces, which are essential for diagnosing defects. We show that tree buffers are also useful in other application domains. For example, they can be used to implement functionality of capturing groups in regular expressions. We prove optimal asymptotic bounds for our data structure, and validate them using empirical data from two sources: regular expression searching through Wikipedia, and runtime verification of execution traces obtained from the DaCapo test suite.	automata theory;data structure;real-time clock;real-time computing;regular expression;runtime verification;test suite;tracing (software);wikipedia	Radu Grigore;Stefan Kiefer	2015		10.1007/978-3-319-21690-4_17		Logic	-19.9020354276363	31.950517981369405	43107
59dd4f43b61d2f823af4a2a00f59a2146bfa1056	compiling and executing declarative modeling languages to gecode	combinatorial problems;modeling language;declarative languages	We developed a compiler from SICStus Prolog CLP(FD) to Gecode as well as a compiler from MiniZinc to Gecode. We compared the running times of the executions of (standard) codes directly written in the three languages and of the compiled codes for a series of classical problems. Performances of the compiled codes in Gecode improve those in the original languages and are comparable with running time of native Gecode code. This is a first step towards the definition of a unified declarative modeling language for combinatorial problems that will allow the user to define problem instances and solving meta-algorithms using high-level declarative language, and then to automatically translate the specification into a low-level encoding, that allows the solving algorithms to run efficiently.	algorithm;code;compiler;constraint programming;declarative programming;gecode;high- and low-level;modeling language;performance;sicstus prolog;time complexity	Raffaele Cipriano;Agostino Dovier;Jacopo Mauro	2008		10.1007/978-3-540-89982-2_69	computer architecture;parallel computing;computer science;modeling language;programming language	PL	-23.77791617832926	24.452446363579586	43487
6781a89596998c3e2be2b2d86537239755e1325c	event algebra for transition systems composition application to timed automata		Formal specification languages have a lot of notions in common. They all introduce entities usually called processes, offer similar operators, and most importantly define their operational semantics based on labelled transition systems (LTS). However, each language defines specific synchronizing and/or memory structures. For instance, in CSP, the synchronization is defined between identical events, while in CCS and in synchronization vectors-based views it is defined respectively between complementary events or between possibly different events. In this paper, we aim at capturing some similarities of specification languages by defining a label-based formal framework for reasoning on LTS, their semantics and related properties. Firstly, we define a high-level synchronization mechanism in the form of an abstract label structure and identify some properties. Then, we introduce operators for composing and transforming label structures, study their intrinsic properties and explore how label structure properties propagate. Secondly, we introduce a LTS-based behavioral framework. We then lift the label structure composition and transformation operators to the LTS level and establish LTS-related properties derived from those of their underlying labelled structures. Thirdly, we consider extended transition systems, more specifically timed automata, as LTS built on top of specific labelled structures. Their semantics is reconstructed by applying operators of our framework on the syntactic LTS, which allows the direct proof of some semantic properties such as compositionality.	automata theory;bip-8;bisimulation;calculus of communicating systems;coq (software);entity;formal specification;high- and low-level;idempotence;netware file system;operational semantics;process calculus;proof assistant;reachability;refinement (computing);scott continuity;semantics (computer science);set theory;simultaneous multithreading;solver;specification language;timed automaton;type theory;uppaal;z3 (computer)	Elie Fares;Jean-Paul Bodeveix;Mamoun Filali	2013	2013 20th International Symposium on Temporal Representation and Reasoning	10.1007/s00236-017-0302-9	principle of compositionality;semantic property;formal specification;operator (computer programming);discrete mathematics;mathematics;syntax;theoretical computer science;semantics;operational semantics;direct proof	Logic	-20.05929172751238	28.663536237770575	43671
b2f927201b603898a994b3c659606059e136362e	program development: from enumeration to backtracking	program development		backtracking	Manfred Broy;Martin Wirsing	1980	Inf. Process. Lett.	10.1016/0020-0190(80)90138-6	computer science;mathematics	DB	-20.836000566795644	19.355121019834087	43691
ffe98fb5df4a92699a0f0b13d3216b32f6cf6a14	automatic run-time code generation in object-oriented languages	lenguaje programacion;object oriented language;programming language;generation code;generacion codigo;code generation;run time code generation;object oriented;langage programmation;oriente objet;orientado objeto	l~un-time code generation improves programs by generating machine code specific to values that are unknown until run time. it is becoming a mature technique, but in the existing systems, the programmer must rewrite the source code to initiate run-time code generation~ to invoke the generated machine code~ and to maintain consistency between the embedded values in the'code and the actual values. This paper shows that object-oriented languages provide useful information for run-time code generation. It also proposes a system for automatic run-time code generation, code invocation, and management. This is an extention of the already proposed automatic run-time code generation system [Fuj95]. The goals of the run-time code generation system proposed in this paper are automation and efficiency. The programmer is freed from annotating programs and from providing suitable parameters to the system to preserve consistency. The output run-time code generator and the code it generates are both optimized.	code generation (compiler);embedded system;machine code;programmer;rewrite (programming);run time (program lifecycle phase);self-modifying code	Nobuhisa Fujinami	1997		10.1007/BFb0033861	parallel computing;computer science;programming language;object-oriented programming;algorithm	PL	-24.095970313843914	28.352073699307585	43745
a61e783d85f6604b55d3da90da53aa344b8807f7	vdm semantics of programming languages: combinators and monads	basic concept;main feature;denotational semantics;formal specification;major programming language;programming language;vdm specification language;vdm semantics;monadic style;vdm semantic description;specification language	Although VDM semantic descriptions of programming language are denotational, they can be read quite operationally. After recalling  the main features of denotational semantics, this paper examines the combinators of the VDM specification language, and relates  them to the use of monads in the monadic style of denotational semantics. It also provides an overview of published VDM semantic  descriptions of major programming languages. Familiarity is assumed with the basic concepts of formal specification.  	combinatory logic;monad (functional programming);semantics (computer science);vienna development method	Peter D. Mosses	2007		10.1007/978-3-540-75221-9_23	natural language processing;normalisation by evaluation;combinatory logic;action semantics;specification language;vienna development method;computer science;domain theory;formal specification;programming language theory;programming language;denotational semantics of the actor model;operational semantics;monad;denotational semantics;algorithm	PL	-24.369573664311915	21.459083115954286	43880
796b062d936b5067116a6fb3c70afcf0218d1ae1	project imperion: new semantics, façade and command design patterns for swing	image processing;selected works;design pattern;bepress	"""Project Imperion consists of a library of GUI elements that combine the command and facade design patterns to add new semantic meaning to some common Swing components. This simplifies the synthesis and maintenance of the code. It also shortens the code, while improving readability, speeding synthesis and easing maintenance. It also promotes better code organization, including the separation of business logic from GUI code and a logical grouping of the code along the same lines as its GUI’s appearance. Started in the late ‘90’s, in the DocJava, Inc. Skunkworks, the Imperion project was meant only for menu items using AWT, targeting the single application of image processing programs. It has since been reworked and extended to support the use of 16 Swing components with built-in keyboard shortcuts. The Imperion project was named for the Latin root, imperium, meaning the power to command. 1 WHAT’S WRONG WITH THE CURRENT EVENT MODEL? When programmers use the current event model, they typically jump around in the source code. For example, it is normal for programmers to create a graphic user interface using a 5 step process: 1. Make a new button (or Component) 2. Set the keyboard shortcuts. PROJECT IMPERION: NEW SEMANTICS, FAÇADE AND COMMAND DESIGN PATTERNS FOR SWING 52 JOURNAL OF OBJECT TECHNOLOGY VOL. 3, NO. 5 3. Add the button to the container. 4. Add an ActionListener to the button 5. Create a long dispatch in an actionPerformed method. Visiting code in several places in order to add a feature is a fruitful source of errors. The code is also hard to maintain, and less readable in that the programmer must jump back and forth in the code file to see declarations, installation of components into GUI containers and implementations of event listeners. Such an approach does not scale well. When the number of different components gets large, these jump points can be many lines of code away from one another. To illustrate the programmers’ jumping around in the code, consider the following example: package gui.run.examples; import javax.swing.JButton; import javax.swing.JFrame; import java.awt.Container; import java.awt.FlowLayout; import java.awt.event.ActionEvent; import java.awt.event.ActionListener; public class CommandFrame extends JFrame implements ActionListener { //1. Make the new buttons JButton okButton = new JButton(""""OK""""); JButton cancelButton = new JButton(""""cancel""""); public CommandFrame() { Container c = getContentPane(); //2. Set the shortcuts okButton.setMnemonic('o'); cancelButton.setMnemonic('c'); //3. Add the components to the // container c.add(okButton); c.add(cancelButton); //4. Set up the ActionListeners okButton.addActionListener(this); cancelButton.addActionListener(this); c.setLayout(new FlowLayout()); setSize(200, 200); show(); } public void actionPerformed(ActionEvent e) { WHAT’S WRONG WITH THE CURRENT EVENT MODEL? VOL. 3, NO. 5 JOURNAL OF OBJECT TECHNOLOGY 53 Object o = e.getSource(); // 5. Add to a if-else dispatch if (o == okButton) System.out.println(""""ok""""); else if (o == cancelButton) System.out.println(""""cancel""""); } public static void main(String[] args) { new CommandFrame(); } } Thus, the typical procedure is to put an instance of a button into a class member variable. During the initialization phase for the GUI, the shortcuts are established, components are added to the container and the listeners are added. During the event handling dispatch we check the event to see if it the button was selected. As a result, we have created a situation where we have to visit the code in several places. This complicated procedure for adding code also complicates maintenance and decreases reliability of the program. It also adds more code. The output of the CommandFrame is shown in Figure 1. Figure 1. OK-Cancel Dialog. In addition, using the above formulated actionPerformed method, one can expect a rather long dispatch. As an example, consider the following code, taken from [Lyon 1999]: public void actionPerformed(ActionEvent e) { if (match(e, mandelbrot_mi)) { mandelbrot(); return; } if (match(e, goslab_mi)) { goslab(); return; } // .... 17 if statements later.... if (match(e, systemInfo_mi)) { systemInfo(); return; } super.actionPerformed(e); } PROJECT IMPERION: NEW SEMANTICS, FAÇADE AND COMMAND DESIGN PATTERNS FOR SWING 54 JOURNAL OF OBJECT TECHNOLOGY VOL. 3, NO. 5 Such long dispatches can be exceedingly difficult to maintain. And GUI elements are removed from their definitions by hundreds of lines of code (in programs of only modest complexity). Even more insidious is the invocation: super.actionPerformed(e); This can make reference to an unknown number of concrete super-classes that improve implementation reuse at a cost of increased fragility. In short, such a programming style does not scale well because it does not encapsulate complexity at the right level. The following section shows that adding function points in this way is a waste of time (i.e., unnecessarily difficult and wrong-headed). If you don’t have time to do it right the first time, when will you have time to do it again?"""	abstract window toolkit;business logic;conditional (computer programming);dos;design pattern;dynamic dispatch;entry point;event (computing);function point;graphical user interface;human-readable medium;image processing;keyboard shortcut;mandelbrot set;observer pattern;programmer;programming style;scalability;source lines of code;swing (java);the journal of object technology;dialog	Douglas A. Lyon	2004	Journal of Object Technology	10.5381/jot.2004.3.5.c6	simulation;image processing;computer science;engineering;software engineering;database;design pattern;programming language;engineering drawing	SE	-27.95551701055727	26.25199684040289	44344
1b2d1fd42f296b860bd3181c32c054b23d5844e5	integrated methods for protocol specification and verification	protocol specification	1 ABSTRACT A methodology for the formal specification and deductive verification of protocols is presented. Protocol modules are modeled as sequential processes and abstract behavioral specifications are given for each. In concurrent systems such processes are connected by buffers and communicate by passing messages, as in the Gypsy model of concurrency. Methods similar to those of Gypsy for verifying safety properties are discussed and illustrated with an example, the data transfer protocol introduced by Stenning. The methodology is geared toward the use of automated verification technology. A fully mechanical proof of the example in this paper was obtained using the Boyer-Moore theorem prover. 2 1 Introduction Recently, the importance of applying formal methods to the problem of communication protocol design has been recognized. A number of approaches to protocol specification and verification have been put forth, using a large variety of models and techniques. Surveys of this work are readily available [Sunshine79, Bochmann80]. Several distinct lines of research have subsequently emerged, although it is safe to say that we have not yet heard the final word. In this paper, we report on a new methodology that has been developed to address this problem. It is based on techniques for verifying systems of concurrent processes that communicate by message passing. Only verification of safety properties are considered at this time; we hope to consider liveness properties in the future. Included is a novel method for stating abstract behavioral specifications of protocol modules. It avoids the highly procedural forms of specification that are in common use today. Also, a significant aspect of the methodology is the use of mechanical theorem proving tools to actually carry out the proofs. We view this as important not so much for the obvious advantages of automation but rather for the elimination of errors that can result from doing tedious proofs by hand. Overall we would characterize the methodology as being an integration of many tools and techniques from a diversity of sources. Among them are verification of concurrent and sequential processes, functional language design, state transition models, decision table techniques, deductive theory building and mechanical theorem proving. Particularly important is the integration of techniques for modeling concurrent processes and state transition systems into a unified framework. The manner in which these ideas have been forged together reflect our belief that an effective methodology is the result of a delicate balance between theoretical and practical considerations. Much …	automated theorem proving;boyer–moore string search algorithm;communications protocol;concurrency (computer science);decision table;formal methods;formal specification;formal verification;functional programming;liveness;message passing;nqthm;procedural programming;state transition table;theory (mathematical logic);unified framework;verification and validation	Benedetto L. DiVito	1982			verification;language of temporal ordering specification	Logic	-20.622604169946765	25.70397924736043	44408
695099b45050866a720930747b6a9c6cdff381f0	ada, as seen from simula	lenguaje programacion;simula;categorisation;programming language;ada;simula language;estudio comparativo;etude comparative;categorizacion;comparative study;langage programmation;ada language;categorization;lenguaje simula		ada;simula	Stein Krogdahl;K. A. Olsen	1986	Softw., Pract. Exper.		simula;ada;computer science;comparative research;programming language;algorithm;categorization	SE	-25.81502010708092	24.16222498690653	44866
c3cc15a63e9b6cc0e1a4a2a834bfb0a3d17a417a	a study of evaluation order semantics in expressions with side effects	unspecified evaluation order;evaluation order;different evaluation strategy;accurate semantics;specific order;evaluation order semantics;possible evaluation strategy;unspecified order;ansi c programming language;side effect;simple language	The presence of side effects in even a very simple language of expressions gives rise to a number of semantic questions. The issue of evaluation order becomes a crucial one and, unless a specific order is enforced, the language becomes non-deterministic. In this paper we study the denotational semantics of such a language under a variety of possible evaluation strategies, from simpler to more complex, concluding with unspecified evaluation order, unspecified order of side effects and the mechanism of sequence points that is particular to the ANSI C programming language. In doing so, we adopt a dialect of Haskell as a metalanguage, instead of mathematical notation, and use monads and monad transformers to improve modularity. In this way, only small modifications are required for each transition. The result is a better understanding of different evaluation strategies and a unified way of specifying their semantics. Furthermore, a significant step is achieved towards a complete and accurate semantics for ANSI C.	ansi c;aliasing;computation;continuation;denotational semantics;emoticon;exception handling;expression (computer science);functional programming;haskell;iteration;monad (functional programming);monad transformer;non-deterministic turing machine;operand;printing;programming language;sequence point;side effect (computer science);transformers	Nikolaos S. Papaspyrou;Dragan Macos	2000	J. Funct. Program.		natural language processing;normalisation by evaluation;computer science;programming language;operational semantics;side effect;algorithm	PL	-23.605003836569537	23.369768063673483	44943
3a197e832784b503820a9eb25d09df9dc87624ae	diagen: a generator for diagram editors providing direct manipulation and execution of diagrams	programming environments;formal specification;garnets;formal model;direct manipulation;specification;hypergraph grammar diagram editor generator diagen direct manipulation diagram execution flowcharts trees hierarchical structures graphs finite state machines visual language systems user interfaces application specific specification;diagen;diagrams;graphs;formal specification graphical user interfaces diagrams visual languages visual programming programming environments software tools graph grammars;diagram execution;visual programming;application specific;tree graphs;automata;hierarchical structures;trees;finite state machines;visual languages;graphical user interfaces;hypergraph grammar;programming profession;animation;visual language;graph grammars;software tools;diagram editor generator;graphical user interfaces garnets flowcharts tree graphs animation automata graphics programming profession context modeling multidimensional systems;context modeling;flowcharts;user interfaces;finite state machine;graphics;multidimensional systems;visual language systems	Diagrams (e.g., flowcharts, trees for hierarchical structures, or graphs for finite state machines) are often needed as part of visual language systems and advanced user interfaces, and are frequently application specific. The implementation of editors for diagrams should be supported by a tool and based on a formal model. This paper gives an overview ofDiaGen, our generator for diagram editors. An editor for a certain kind of diagrams is generated from a specification, which includes a hypergraph grammar to describe the structure of diagrams. The user of a diagram editor does not have to be concerned with the grammar, but can manipulate diagrams very conveniently by direct manipulation. As an additional and important feature in the context of visual languages editors generated by DiaGen can not only be used for editing, but also for executing, i.e., animating diagrams.	algorithm;c++;diagram;direct manipulation interface;finite-state machine;flowchart;formal language;graphical user interface;high-level programming language;hypergraph grammar;mathematical model;microsoft windows;motif;parsing;programmer;prototype;source lines of code;vector graphics editor;visual language;visual programming language;x window system	Mark Minas;Gerhard Viehstaedt	1995		10.1109/VL.1995.520810	block diagram;natural language processing;interaction overview diagram;computer science;theoretical computer science;programming language	HCI	-31.78350939823115	23.96917898426098	44955
bafd702a68d6749883ceab33d525d2e7ba67eec5	sf1: introduction to ada	ada;syntax rule;software;ada programming construct;object-oriented programming;concurrent programming;reliability;programming language;concurrent-programming feature;software engineering;downloadable ada programming environment;high-level programming language;object oriented programming	Level - Beginner, but attendees should have some experience with a high-level programming language.  This tutorial is designed for those who have some familiarity with a programming language, but who are new to Ada. In the morning, we will discuss the basics of programming in Ada, including types, packages, syntax rules, and other Ada programming constructs. In the afternoon, we will cover Ada's object-oriented programming and concurrent-programming features. Many examples will be shown; freely downloadable Ada programming environments and tools will be demonstrated.	apl;ada;concurrent computing;high- and low-level;high-level programming language	Michael B. Feldman	2008		10.1145/1454474.1454476	first-generation programming language;computer architecture;real-time computing;declarative programming;very high-level programming language;ada;concurrent computing;programming domain;reactive programming;functional reactive programming;computer science;programming language generations;extensible programming;functional logic programming;reliability;computer programming;programming paradigm;procedural programming;symbolic programming;inductive programming;fifth-generation programming language;programming language theory;programming language;object-oriented programming;system programming;concurrent object-oriented programming	PL	-28.288613582632976	24.225026597648974	45004
ecb3f05fc73ec8a9dc1a64fb0e1caa3bd9194a1e	axiomatic semantics of communicating sequential processes	axiomatic;communicating sequential process;system programming;formal semantics;semantique formelle;synchronisation;programmation systeme;synchronization;sincronizacion;axiomatique;processus sequentiel communiquant	A simple definition of the axiomatic semantics of Communicating Sequential Processes is presented. The most important aspect of the approach is that it allows dealing with the individual processes of a program in isolation from the other processes. The axiomatic semantics is used to prove the correctness of a program for partitioning sets.	axiomatic semantics;communicating sequential processes;correctness (computer science)	Neelam Soundararajan	1984	ACM Trans. Program. Lang. Syst.	10.1145/1780.1805	synchronization;computer science;artificial intelligence;algorithm	PL	-24.327463924447855	31.34522136788394	45279
4a05d76d1c582d0e1cab117542424941f9d43372	type inference, haskell and dependent types		xi I Foundations of type inference 1	dependent type;haskell;type inference	Adam Michael Gundry	2013			parametric polymorphism;type inference;hindley–milner type system;programming language;generalized algebraic data type	PL	-22.28295178089188	21.21166386877076	45391
ac1c47f128d35af39fe85149883574dd94de09f7	verified bytecode verification and type-certifying compilation	proof assistant;java bytecode;theorem proving;type system	This article presents a type certifying compiler for a subset of Java and proves the type correctness of the bytecode it generates in the proof assistant Isabelle. The proof is performed by defining a type compiler that emits a type certificate and by showing a correspondence between bytecode and the certificate which entails welltyping. The basis for this work is an extensive formalization of the Java bytecode type system, which is first presented in an abstract, lattice-theoretic setting and then instantiated to Java types.	compiler;correctness (computer science);isabelle;java bytecode;proof assistant;theory;type system	Gerwin Klein;Martin Strecker	2004	J. Log. Algebr. Program.	10.1016/j.jlap.2003.07.004	type system;computer science;theoretical computer science;automated theorem proving;proof assistant;programming language;algorithm	PL	-21.181708536747	27.254908355794274	45504
335338d521237035da9770c5d7807353a1171902	traits: composable units of behaviour	herencia;lenguaje programacion;programming language;building block;hierarchized structure;bepress selected works;heritage;reutilizacion;structure hierarchisee;object oriented programming;reuse;code reuse;object oriented;object oriented programming languages;multiple inheritance;smalltalk;langage programmation;oriente objet;inheritance;orientado objeto;estructura jerarquizada;reutilisation;inheritance mixins multiple inheritance traits reuse smalltalk	Inheritance is the fundamental reuse mechanism in object-oriented programming languages; its most prominent variants are single inheritance, multiple inheritance, and mixin inheritance. In the first part of this paper, we identify and illustrate the conceptual and practical reusability problems that arise with these forms of inheritance. We then present a simple compositional model for structuring object-oriented programs, which we call traits. Traits are essentially groups of methods that serve as building blocks for classes and are primitive units of code reuse. In this model, classes are composedfrom a set of traits by specifying glue codethat connects the traits together and accesses the necessary state. We demonstrate how traits overcome the problems arising with the different variants of inheritance, we discuss how traits can be implemented effectively, and we summarize our experience applying traits to refactor an existing class hierarchy.	aliasing;backward compatibility;class hierarchy;code refactoring;code reuse;constructor (object-oriented programming);ecoop;encapsulation (networking);java;mixin;multiple inheritance;programmer;programming language;programming tool;smalltalk;squeak;type system;usability	Nathanael Schärli;Stéphane Ducasse;Oscar Nierstrasz;Andrew P. Black	2003		10.1007/978-3-540-45070-2_12	computer science;theoretical computer science;database;composition over inheritance;programming language;object-oriented programming;algorithm	PL	-25.9157635407494	27.66596755329726	45667
ef9531525676c5843fc8a1ec752ba63961110da4	funz designs a bridge between z specifications and haskell implementations	translation process funz designs z specifications haskell implementations intermediate specification language purely functional programs notational conventions object oriented variants purely functional languages concrete predicates;software prototyping;haskell implementations;translation process;prototypes;intermediate specification language;program interpreters;bridges;functional programming;specification language;funz designs;concrete predicates;purely functional languages;intermediate language;bridges software prototyping prototypes functional programming animation software design computer science specification languages design methodology concrete;object oriented;specification languages;object oriented variants;functional languages;animation;computer science;program interpreters functional languages functional programming specification languages;software design;notational conventions;functional language;z specifications;concrete;purely functional programs;design methodology	FunZ, an intermediate specification language, is part of a complete methodology designed to facilitate the derivation of purely functional programs from Z specifications. FunZ is actually an extension of Haskell, yet the language also retains a Z-like flavor in that it contains notational conventions similar to those of standard Z or several object oriented variants. By combining features from both Z and Haskell, FunZ provides a bridge between Z specifications and Haskell implementations. Although the intermediate language and associated methodology target Haskell, the approach itself is general and can also be applied to other purely functional languages. The paper highlights one facet of the methodology, namely the procedure to derive concrete predicates written in FunZ from their abstract Z counterparts. In addition, by means of an example, the paper gives an overview of the translation process from Z specifications to FunZ designs.	haskell;z notation	Linda B. Sherrell;Doris L. Carver	1995		10.1109/CMPSAC.1995.524752	anime;concrete;design methods;specification language;computer science;software design;theoretical computer science;prototype;programming language;intermediate language;object-oriented programming;functional programming;algorithm	PL	-26.02649140377211	23.864956099462873	45764
fc4006ee0292dfd33d1ea397e56020a3887f32cf	vips: a visual debugger	debugging;poles and towers;testing;debugging testing displays poles and towers programming profession electronics packaging laboratories data structures error correction force control;data structures;programming profession;error correction;displays;electronics packaging;force control	Testing Ada programs is easier with this visual debugger that graphically depicts what the program as doing—and how it is being done.	ada;debugger;vips (software)	Sadahiro Isoda;Takao Shimomura;Yuji Ono	1987	IEEE Software	10.1109/MS.1987.230394	embedded system;real-time computing;error detection and correction;data structure;computer hardware;computer science;software testing;electronic packaging;programming language;debugging	SE	-30.983139116442864	24.553747372229285	45881
88e4f5182bb9a781c3e461e9653bc11cde2a76aa	program specialization for execution monitoring	program specialization	Execution monitoring is a proven tool for securing program execution and to enforce safety properties on applets and mobile code, in particular. Inlining monitoring tools perform their task by inserting certain run-time checks into the monitored application before executing it. For efficiency reasons, they attempt to insert as few checks as possible using techniques ranging from simple ad hoc optimizations to theorem proving. Partial evaluation is a powerful tool for specifying and implementing program transformations. The present work demonstrates that standard partial evaluation techniques are sufficient to transform an interpreter equipped with monitoring code into a non-standard compiler. This compiler generates application code, which contains the inlined monitoring code. If the monitor is enforcing a security policy, then the result is a secured application code. If the policy is defined using a security automaton, then the transformation can elide many run-time checks by using abstract interpretation. Our approach relies on proper staging of the monitoring interpreter. The transformation runs in linear time, produces code linear in the size of the original program, and is guaranteed not to duplicate incoming code.	abstract interpretation;applet;automata theory;automated theorem proving;automaton;cobham's thesis;code generation (compiler);code mobility;combinatory logic;compile time;compiler;data-flow analysis;disk staging;duplicate code;experiment;hoc (programming language);hugo thiemann;inline expansion;interpreter (computing);jones calculus;just-in-time compilation;line code;online and offline;partial evaluation;partial template specialization;program transformation;self-modifying code;time complexity;trusted computing base	Peter Thiemann	2003	J. Funct. Program.	10.1017/S0956796802004586	dead code;real-time computing;computer science;operating system;dead code elimination;redundant code;code coverage;programming language;code generation;program animation;unreachable code;source code	PL	-21.713943676269267	31.735067246441997	46116
032d4ad644c8d4e65a6231e1b929817e325be384	correspondent computing	correspondent computing;correspondent operation;primary operation	In the context of the preliminary study reported here, correspondent computing is loosely defined as the generation of effects (results or outcomes) which are correspondent to the effect of an operation in a computer program. The operation whose effect is the target of correspondent computing is called the primary operation. Operations generate correspondent effects are called correspondent operations. The philosophy of correspondence is that if executing an operation can produce a significant effect, an equivalently significant effect may be generated by another semantically correspondent operation. In general, correspondent operations can be categorized as the following three types. 1) Reciprocal type. The behaviour of a reciprocal operation is semantically the inverse of the primary operation. 2) Duplicate type. The behaviour of a duplicate operation is semantically equivalent to the primary operation. 3) Residual (or complementary) type. An operation that fulfills the definition of correspondent operations, but is neither a duplicate, nor a reciprocal is termed as residual operation. All these three types of correspondent operations can be easily derived from the primary operation on the basis of correspondence. We believe that Correspondent computing has the potential of becoming a powerful tool and can be used in the areas such as Fault Tolerance, Artificial Intelligence, Graphics, Database systems, etc. In the following paragraphs, we discuss the use of correspondent computing for Error Detection. From the viewpoint of software fault tolerance, the correct behavior of some operations are more critical than the others although every operation is a constituent part of the program. Therefore, the effect generated by these operations must be examined, and hence they are the primary operations - the targets of correspondent computing. Using correspondent computing, the effect of a primary operation can then be checked by comparing with the effects generated by its correspondent operations. The specification of the comparative test is base on the precise relationship between the primary and correspondent operations. If an error occurs, the relationship between the two effects would not match that of the two operations. The greatest advantage of this error detection scheme is the simple and concise specification for the comparative test. As an example, suppose the program SUM will sum up an array of N integer numbers. Two correspondent operations are implemented for the purpose of error detection. Array pa contains the original data of N integer numbers.	artificial intelligence;categorization;computer program;error detection and correction;graphics;software fault tolerance	Pen-Nan Lee	1988		10.1145/322609.323197	computer science;theoretical computer science;engineering drawing;algorithm	PL	-28.646978965594798	21.17803880388583	46144
8608295d2c1b78178e95ae3cf8ff931fef3712eb	the transformational derivation of parallel programs using data distribution algebras and skeletons				Mario Südholt	1997			transformational leadership;derivation;algebra;mathematics	HPC	-22.828386307492945	21.606287685773456	46343
54dabd99844608f8528ea7a2ead62b77545316da	extending dynamic constraint detection with disjunctive constraints	disjunctive constraint;dynamic constraint inference;state space;behavioral specification;program specification	The languages of current dynamic constraint detection techniques are often specified by fixed grammars of universal properties. These properties may not be sufficient to express more subtle facts that describe the essential behavior of a given program. In an effort to make the dynamically recovered specification more expressive and program-specific we propose the state space partitioning technique as a solution which effectively adds program-specific disjunctive properties to the language of dynamic constraint detection. In this paper we present ContExt, a prototype implementation of the state space partitioning technique which relies on Daikon for dynamic constraint inference tasks.  In order to evaluate recovered specifications produced by ContExt, we develop a methodology which allows us to measure quantitatively how well a particular recovered specification approximates the essential specification of a program's behavior. The proposed methodology is then used to comparatively evaluate the specifications recovered by ContExt and Daikon on two examples.	constraint inference;disjunctive normal form;prototype;space partitioning;state space	Nadya Kuzmina;John Paul;Ruben Gamboa;James Caldwell	2008		10.1145/1401827.1401839	real-time computing;computer science;state space;theoretical computer science;programming language;algorithm	SE	-19.584298912712104	27.69245816789612	46376
2c855cf254268f4d0a39eac23765a4a0a1d51575	a rule-based approach for animating java algorithms	visualization java animation indexes heuristic algorithms data structures unified modeling language;java programs rule based approach java algorithm animation program visualization programming language visual trace utility constraint handling rule programs chr programs dynamic methodology;constraint handling rules;program visualisation computer animation constraint handling java;algorithm animation;visual language;algorithm animation java programs visual language constraint handling rules;java programs	Over the past years, visualization of programs has been widely applied. Algorithm animation was proven to aid in teaching and learning. It provides a convenient medium for beginners to a programming language by giving them the ability to visually discover how their programs are running. It also provides experts of a language with a means to have a visual trace utility. Lately, a new approach for adding visualization features into Constraint Handling Rules (CHR) programs was proposed. The new methodology was a dynamic one able to animate different types of algorithms. The work in this paper aims at introducing a revised extension that is able to embed visualization features into Java programs. With the new extension, Java algorithms could be animated without the need of doing any modifications to the code. In addition, the provided technique is still a general one able to animate different kinds of algorithms.	apl;algorithm;constraint handling rules;constructor (object-oriented programming);java;logic programming;programming language;prototype;source transformation;visual objects;visual language	Nada Sharaf;Slim Abdennadher;Thom W. Frühwirth	2016	2016 20th International Conference Information Visualisation (IV)	10.1109/IV.2016.55	java concurrency;computer science;theoretical computer science;java modeling language;strictfp;real time java;programming language;java;algorithm;generics in java;scala;java annotation	SE	-30.82459587948063	24.792749849032486	46445
95c6c49d5bc26791cdcf04754187027f4f151b95	a design and prototyping of an object-oriented program debugger	performance evaluation;object oriented programming;asymptotic analysis;dynamic binding;routing problems;object oriented;object oriented programming languages;high level language;tree structure networks;combinatorial analysis	This paper introduces our design and prototyping of a debugger for an object-oriented programming language Object-Oriented CHILL. CHILL is a high level language recommended by CCITT(ITU-T). CHILL has been extended to Object-Oriented CHILL for adoption of object-orientedness. Besides basic facilities for general debugging, Object-Oriented CHILL Debugger(OOCD) provides facilities for showing information of object types(modes), inheritance, visibilities, dynamic binding. Since our implementation is based on the translator, therefore, the additional considerations such as investigating original line numbers for source level debugging are supported.	chill;debugger;debugging;high-level programming language;late binding;line number;prototype	Chang-Hyun Jo;Phil Sun Kim;Hyeung Sik Im;Eui Hyun Paik;Byung-Sun Lee	1997		10.1145/331697.331708	method;asymptotic analysis;computer science;object;theoretical computer science;common object request broker architecture;distributed computing;fifth-generation programming language;programming language;object-oriented programming	PL	-27.33096033470341	29.986625796898924	46612
83591e21a6bb86f4cce2675d4c7f2d1758c362d8	two implementation models of abstract data types	lenguaje programacion;compilacion;execution time;programming language;ada;abstract data type;type abstrait;langage programmation;compilation;temps execution;tipo abstracto;ada language;tiempo ejecucion	JOHN D. GANNON a n d MARVIN V. ZELKOWITZ Department of Computer Science and Institute for Advanced Computer Studies, University of Maryland, College Park, MD 20742, U.S.A. (Received March 1986; revision received July 1986) A ~ T h i s paper compares two implementation models for abstract data types: direct and indirect implementations. Direct implementations offer relatively cheap execution and expensive compilation costs while indirect implementations result in relatively expensive implementations and cheap compilation costs. These two models are both accommodated by Ada, and a small experiment compares their costs for a particular data type. Keyworda: abstract data type ada run-time implementation 1. I N T R O D U C T I O N The development of user-defined types in ALGOL 68 and Pascal provided a clear break with the earlier generation of languages such as FORTRAN and Algol which allowed users only a fixed set of types. Programmers using these newer languages can declare new types which in turn can be used in declaring new data objects. Objects with user-defined typescorrespond more closely with problem-domain objects than to programming-language objects. Pascal and Algol 68 type definitions are not ideal methods for defining new types since there is no way to declare the operations of a newly declared type. Any program unit within the reach of the type definition can access the components of objects with user-defined types. Programmers define new operations with functions and procedures, and turned to methodologies like information hiding [1] to remedy the shortcomings of the type definition features of these languages. Only the few routines that actually manipulated the components of objects need to know the details of an object's representation; in the remainder of a program components can be manipulated indirectly by calling one of these routines. During the 1970s, several languages were designed to extend type definitions that encapsulated the representation of objects and permitted users to define new operations on objects. Among the better known languages from this era are CLU [2], Alphard [3], Simula [4], Mesa [51, Euclid [6] and Ada [7]. Of these languages, Ada is certain to become the most widely used. The remainder of this paper discusses the implementation of encapsulated objects in Ada, although we believe similar results can be obtained from other languages. 2. T W O I M P L E M E N T A T I O N M O D E L S In languages which support abstract data types two basic implementation models have been used; for purposes of discussion we call them: direct implementations and indirect implementations. Storage for directly implemented objects is allocated in the activation record of the procedure in which they are declared. This implies that the compiler knows the underlying representation of objects during compilation of all modules that declare objects and can (possibly) generate inline code to manipulate the objects. In contrast, indirectly implemented objects are represented in the activation record of their declaration as pointers to the actual (heap) storage for the objects. The compiler generating code for operations on these objects need not know thdr structure, but must generally generate less efficient code. For example, if an abstract type Stack was declared as follows type Values is array (1 .. 100) of integer; type Stack is record Top: integer; A: Values; end record;	algol 68;abstract data type;abstract type;ada;clu;call stack;compiler;computer science;declaration (computer programming);domain-driven design;embedded system;euclid;fortran;inline expansion;memory management;mesa;need to know;pascal;programmer;run time (program lifecycle phase);simula;type system	John D. Gannon;Marvin V. Zelkowitz	1987	Comput. Lang.	10.1016/0096-0551(87)90009-9	parallel computing;real-time computing;ada;computer science;programming language;abstract data type	PL	-24.67221771226437	28.262974225181708	46744
8e1d3cc74f4c6319bb6bc4ae5414b0caebd402bf	interpreter prototypes from formal language definitions	programming language;denotational semantic;thesis;language development;computer software programming computer software;formal language	Denotational semantics is now used widely for the formal definition of programming languages but there is a lack of appropriate tools to support language development. General purpose language implementation systems are oriented to syntax with poor support for semantics. Specialised denotational semantics based systems correspond closely to the formalism but are rendered inflexible for language experimentation by their monolithic multiple stages Exploratory language development with formal definitions is better served by a unitary notation, encompassing syntax and semantics, which is close to but simpler than denotational semantics. Interactive implementation of the notation then facilitates language investigation through the direct execution of a formal definition as an interpreter for the defined language. This thesis presents Navel, a run-time typed, applicative order, pure functional programming language with integrated context free grammar rules. Navel has been used to develop prototype implementations from substantial formal language definitions, including for Navel itself. The Navel implementation achieves a performance which enables interactive language experimentation and compares well with that of contemporaneous declarative language implementations. Denotational semantics does not address concrete syntax issues and denotational semantics based systems either ignore or have ad-hoc provision for context sensitivity. In Navel, rules are full values. Abstraction over rules enables the concise expression of context sensitivity in a style similar to dynamic syntax.	adaptive grammar;applicative programming language;context-free grammar;declarative programming;denotational semantics;exploratory testing;formal language;functional programming;hoc (programming language);interpreter (computing);parse tree;prototype;semantics (computer science)	Greg J. Michaelson	1993			natural language processing;fourth-generation programming language;first-generation programming language;natural language programming;very high-level programming language;formal methods;picture language;universal networking language;language primitive;object language;specification language;programming domain;data control language;computer science;programming language implementation;theoretical computer science;software development;low-level programming language;fifth-generation programming language;programming language theory;programming language;programming language specification;high-level programming language	PL	-26.33838273755531	22.462476635388825	46818
568f10291bf4be5d658991c2e90f79a2949f3af5	editing large programs using a structure-oriented text editor	tree structure;program development	This paper will describe how a structure-oriented text editor, named ED3, is used as a practical and efficient tool for program development and maintenance. Unlike syntax-directed editors this editor does not use its tree structure to represent the parse tree of the program. Instead the user is free to build any tree structure of text nodes he wants. For a block structured language the tree can be built the same way procedures are defined inside procedures.	text editor	Ola Strömfors	1986		10.1007/3-540-17189-4_86	computer science;database;programming language	NLP	-27.947611795896844	26.018605329413056	46948
f1a63ca0a73fefdf95c4c284a5c09d83ab3ac1f1	a flexible environment for program development based on a symbolic interpreter	program development	The paper describes an interactive programming system which provides an integrated collection of tools for dealing with the whole process of program development. The pivot tool, the symbolic interpreter, may cover a broad range of applications, from testing to correctness Proving. The aspects in which the symbolic interpreter differs from a conventional interpreter, i.e. the possibility of handling nondeterministic branching at choice points and the presence of a system for manipulating symbolic expressions, are described. Furthermore, the main features of a programming language, around which the programming system is built, are presented.		Patrizia Asirelli;Pierpaolo Degano;Giorgio Levi;Alberto Martelli;Ugo Montanari;Giuliano Pacini;Franco Sirovich;Franco Turini	1979			computer science;theoretical computer science;symbolic programming;programming language;algorithm	SE	-24.496518992377883	25.749034203120793	47323
38e9ec37c298738f9d96316ab53610f929aba3db	machine translation using isomorphic ucgs	unification categorial grammar;mt system design;machine translation;monolingual ucg;translation equivalence;tl expression;target language;isomorphic grammars approach;translation relation;isomorphic ucgs;isomorphic grammars;system design;categorial grammar	"""This paper discusses the application of Unification Categorial Grammar (UCG) to the framework of Isomorphic Grammars for Machine Translation pioneered by Landsbergen. The Isomorphic Grammars approach to MT involves developing the grammars of the Source and Target languages in parallel, in order to ensure that SL and TL expressions which stand in the translation relation have isomorphic derivations. The principle advantage of this approach is that knowledge concerning translation equivalence of expressions may be directly exploited, obviating the need for answers to semantic questions that we do not yet have. Semantic and other information may still be incorporated, but as constraints on the translation relation, not as levels of textual representation. After introducing this approach to MT system design, and the basics of monolingual UCG, we will show how the two can be integrated, and present an example from an implemented bidirectional Engllsh-Spanish fragment. Finally we will present some outstanding problems with the approach. 1 Background and Introduct ion The aim of this paper is to explore how the linguistic theory known as Unification Categorial Grammar can be adapted to the general methodology of Machine Translation using Isomorphic Grammars, as pioneered by Landsbergen and others in the ROSETTA team [Landsbergen 87a, b]. UCG is one of several recent grammar formalisms [Calder et al. 86, Kar t tunen 86, Pollard 85] which are highly lexicalist, i.e. rules of syntactic combination are not a language-specific component of the grammar, but are very general in character, and combinatory information is primarily associated with lexical items. Lexical items are represented by sets of feature-value pairs (where the values may be themselves sets of such pairs}, and are combined by unification into objects of the same type. The language defined is thus the closure of the lexicon under the combinatory rules. Landsbergen's work on Isomorphic Grammars follows Montague's approach of having a one-to-one correspondence between syntactic and semantic rules. A syntactic rule Rs/, in the Source Language corresponds to a syntactic rule RTL in the Target Language if and only if they are both associated with the same semantic operation Rsem. The translation relation is then defined in a precise manner and it can be guaranteed that well-formed expressions in the Source Language are translatable, as there will be an expression in the Target Language that is derived in a corresponding way, and can therefore be considered as a possible translation of it. *Supported by a studentship from the Science and Engineering Research Council. According to Landsbergen, writing isomorphic grammars is a way of being explicit about the """"tuning"""" of SL and TL grarmnars that is essential for reliable MT. The present paper is an a t tempt to adapt this approach to a type-driven mapping between syntax and semantics. 2 I s o m o r p h i c G r a m m a r s We can recognise two basic relations of relevance in translation. namely, """"possible translation"""" (which is symmetric}, and """"best translation"""" given the current context and much extra-linguistic knowledge (which is not symmetric}. We take the task of the lin.~ guistic component of an MT system to be a correct and complete characterisation of the former, and will have nothing further to say about the latter. An important problem that arises in an interlingual translation system is what Landsbergen [Landsbergen 87b] calls the """"subset problem"""". If the analysis component generates a set L of interlin° gum expressions, and the generation component accepts a set L I of them, the only sentences that can be translated are those that correspond to expressions in the intersection L N L ~. If the gram-. mars of the source and target languages are written independently, there is no way of guaranteeing that they map the languages into the same subset. The problem arises because a sufficiently powerful system of"""" interlingual representation will contain an infinite number of logically equivalent expressions that represent a meaning of a given Source Language expression. Of course, the Source Language grammar will only associate a single one of these with a given SL expression. However, in the absence of specific tuning, this is not guaranteed to be the same one that the Target Language grammar associates with any of the translation equivalents. Therefore, SL and TL grammars must be tuned to each other. This is not a problem specific to interlingual translation: in the transfer approach to MT system design, this tuning is effected by an explicit transfer module. The use of Isomorphic Grammars is another way of being explicit about this, tuning the grammars themselves rather than their inputs /outputs , which offers a greater possibility of bi-directionality than the transfer approach. Landsbergen assumes the existence of compositional grammars for two languages, that is, grammars in which i) basic expressions correspond to semantic primitives and ii) each syntactic rule that builds up a complex linguistic expreaqion from simpler ones is paired with a semantic rule that builds the meaning of the complex expression from the meanings of the simpler ones. The tuning of grammars consists in ensuring that there it~ a basic expression in one grammar corresponding to each basic ex-~ pression in the other, and that for each semantic rule there is a corresponding syntactic rule in each grammar. Two expressions are then considered possible translations of each other if they can be derived from corresponding basic expressions by applying cor~ responding syntactic rules. In other words, they are possible transo lations of each other if they are built from expressions having the same rneaning, by using syntactic rules that perform the same semantic oper,tions. Note the lack of directional specificity in this definition of the """"possible translation"""" relation. / v 8 The ~monohngual) UCG formalis~n Many recent grarmnar formalisms [Shieber 86] represent linguistic objects as t~ts of attribute-.value pairs. Values taken by these attr ibutes may be atomic, variables, or they may thenmelves be sets of attribate-value pairs, so these objects *nay be thought of as Directed Acyclic Graphs (DAGs), in which directed arcs represent feature% and the nodes at the end of these represent values. Such formalisms t~pically support re-entrancy, that is, they provide a mechanism 5)r specifying that object~s at the end of different paths are the same object. Unification Gategorinl Grarimaar is such a formalism, which combines a categorial t reatment of syntax with semantics similar to Kamp 's :Vliscourse Representation [Kamp 81]. Each linguistic expression licensed by the grammar corresponds to what is called a sign. A sigt~ consists of four main entries or features, which are explained below: 1. p h o n o l o g y (orthography in the present cruse)"""	combinatory categorial grammar;compiler;context-free grammar;directed acyclic graph;formal system;han unification;lexicon;montague grammar;one-to-one (data model);phrase structure rules;regular expression;relevance;sl (complexity);sensitivity and specificity;statistical machine translation;systems design;transfer-based machine translation;transform, clipping, and lighting;turing completeness;unification (computer science);well-formed formula	John L. Beaven;Pete Whitelock	1988			dynamic and formal equivalence;natural language processing;synchronous context-free grammar;categorial grammar;transfer-based machine translation;example-based machine translation;computer science;linguistics;machine translation;rule-based machine translation;algorithm;systems design	NLP	-25.656401061759198	18.571725269072598	47340
aa531d32b66304b2fc20eebf217a832d16f9ef1b	nondeterministic data flow programs: how to avoid the merge anomaly	lenguaje programacion;representation graphique;programming language;asynchrone;representacion grafica;communicating process;fusion anormale;flot donnee;concurrent program;flujo datos;systeme non deterministe;grafico;fusion anormal;interpretacion;non deterministic system;graph;graphe;merge anomaly;processus communicant;programa competidor;langage programmation;interpretation;sistema no determinista;data flow;graphics;asincrono;asynchronous;programme concurrent	A simple programming language for the description of networks of loosely coupled, communicating, nondeterministic agents is introduced. Two possible graphical interpretations are discussed: finite cyclic and infinite acyclic, tree-like graphs. Operational semantics for such graphs is defined by computation sequences. The merge anomaly is described, analysed and explained. Two fixed-point semantics are defined in a denotational style, one that avoids the merge anomaly, and another one that includes the merge anomaly, and they are proved to be consistent with the resp. operational definitions. Both definitions are compared and analysed.	anomaly detection;dataflow architecture	Manfred Broy	1988	Sci. Comput. Program.	10.1016/0167-6423(88)90016-0	data flow diagram;interpretation;computer science;graphics;theoretical computer science;asynchronous communication;graph;programming language;algorithm	PL	-21.06443855010897	22.28520346741456	47400
6aa13777069a7956618d50c1df482511d7432bd0	extending lustre with timeout automata	attribute grammars;synchronous languages;synchronous language;attribute grammar;extensible languages;asynchronous communication;language extension;composable language extensions	This paper describes an extension to Lustre to support the analysis of globally asynchronous, locally synchronous (GALS) architectures. This extension consists of constructs for directly specifying the timeout automata used to describe asynchronous communication between processes represented by Lustre nodes. It is implemented using an extensible language framework based on attribute grammars that allows such extensions to be modularly defined so that they may be more easily composed with other language extensions.	asynchronous circuit;attribute grammar;automata theory;automaton;extensible programming;globally asynchronous locally synchronous;lustre;modular programming	Jimin Gao;Mike Whalen;Eric Van Wyk	2008	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2008.05.014	lustre;computer science;theoretical computer science;asynchronous communication;programming language;attribute grammar;algorithm	PL	-27.464863900505804	31.737607266320726	47423
1d5ad4f5e60cadf76d967ea943359cc455fa79f2	foundation of object-oriented languages, 2nd workshop report	algebraic approach;object oriented language;operational semantics;data type;denotational semantic;semantics of programming languages	A report on the workshop Foundations of Object-Oriented Languages, Paris, July 1994.		Giuseppe Castagna;Gary T. Leavens	1995	SIGPLAN Notices	10.1145/199873.607638	natural language processing;fourth-generation programming language;action semantics;data type;computer science;theoretical computer science;third-generation programming language;abstract family of languages;fifth-generation programming language;programming language theory;programming language;object-oriented programming;operational semantics;second-generation programming language;comparison of multi-paradigm programming languages;denotational semantics	PL	-24.501057709804186	21.89512930111118	47810
a3a018e37bc27d67edaee4725231ee3921bf915f	banzai+tatoo: using cutting-edge parsers for implementing high-performance servers	server;software engineering;parsing;non blocking io;protocol	This paper presents how the Tatoo parser generator enables the implementation of Java high-performance servers using the Banzai generic server shell. The performance of these servers relies on the ability of Tatoo to produce push non-blocking parsers with a fixed memory footprint during parsing and on the generic and efficient server architecture of Banzai. This approach reconciles the use of formally defined grammars for protocol parsing and the efficiency of the implementation. We argue that the use of the formal grammars simplifies the implementation of the protocol and we show that an HTTP server built using the Banzai+Tatoo is as efficient as several existing specially tuned high-performance HTTP servers.	parsing	Julien Cervelle;Rémi Forax;Gautier Loyauté;Gilles Roussel	2012	Sci. Comput. Program.	10.1016/j.scico.2011.01.005	asynchronous i/o;protocol;computer science;operating system;parsing;database;programming language;server	PL	-28.081594224210846	30.119976853398654	47978
cf50f10abe15ef8b57a3caba9f27d36722645b03	extensible host language for domain-specific languages	generic syntax;functional composition;language composition;domain specific language;metaprogramming;concept composition	Programming languages greatly influence the way how programs are created and evolved. This means that the use of appropriate language for solved problem can greatly increase developer productivity. Composition of languages can provide great help in construction of a new language from existing components and for integration of several languages that may be needed to effectively solve a complex problem. In this paper we analyze the composition problem on the two levels: composition of languages and composition of concepts in a language. Possibilities of transition from language composition to concepts composition are also presented. Based on that, we propose a framework of languages construction based on concept composition that aims to support reusability of language elements and tools. It uses common host syntax for developed languages. Their semantics is defined in a general-purpose language. Proposed approach is demonstrated on example languages developed using prototype implementation.	database schema;digital subscriber line;domain-specific language;evaluation function;expressive power (computer science);gene ontology term enrichment;general-purpose language;general-purpose markup language;library (computing);metaprogramming;object language;programming language;prototype;software development	Sergej Chodarev;Ján Kollár	2016	Computing and Informatics		metaprogramming;natural language processing;fourth-generation programming language;language primitive;object-based language;object language;computer science;domain-specific language;third-generation programming language;syntax;linguistics;compiled language;ontology language;low-level programming language;fifth-generation programming language;programming language;second-generation programming language;high-level programming language;comparison of multi-paradigm programming languages;algorithm;query language	PL	-28.034383031089185	27.321515848108884	48092
2454922772995140f563869d6ac095c734ea65b1	overcoming the multiplicity of languages and technologies for web-based development using a multi-paradigm approach	institutional repositories;web based applications;fedora;programming language;language technology;functional programming;vital;object oriented;vtls;data structure;ils	In this paper, we present QHTML, a library for building Web-based applications in Oz. QHTML provides the Oz programmer with a basic set of abstractions through which creating Web-based interfaces becomes similar to traditional graphical toolkits. In the mean time, QHTML is an experiment investigating whether a single language can replace the numerous ad-hoc combined languages/technologies currently used for building Web-based interfaces. QHTML is realized thanks to the multi-paradigm features of the Oz programming language, which supports symbolic data structures, a functional programming style, an object-oriented style and concurrency via dataflow and lightweight threads.	concurrency (computer science);data structure;dataflow;functional programming;graphical user interface;hoc (programming language);list of toolkits;programmer;programming language;programming paradigm;programming style	Sameh El-Ansary;Donatien Grolaux;Peter Van Roy;Mahmoud Rafea	2004		10.1007/978-3-540-31845-3_10	fourth-generation programming language;first-generation programming language;natural language programming;web application;web modeling;very high-level programming language;language primitive;html;data structure;programming domain;functional reactive programming;computer science;artificial intelligence;theoretical computer science;operating system;third-generation programming language;functional logic programming;database;distributed computing;scripting language;programming paradigm;symbolic programming;fifth-generation programming language;programming language theory;programming language;object-oriented programming;functional programming;language technology;second-generation programming language;high-level programming language;algorithm	PL	-28.41804891574313	25.769705316082227	48225
a88915ab94d868d6661a297c5fe0359d0df58fea	on programming and supporting multimedia object synchronization	lenguaje programacion;distributed system;sistema operativo;systeme reparti;programmation;multimedia;programming language;programming environment;implementation;real time;programacion;synchronisation;medio ambiente programacion;ejecucion;sistema repartido;operating system;synchronization;temps reel;langage programmation;tiempo real;systeme exploitation;sincronizacion;programming;environnement programmation	This paper addresses the programming of synchronization in multimedia applications. It briefly introduces multimedia objects and describes a synchronous language model, which yields a rigorous framework for both expressing multimedia object behaviour and supporting the implementation of the related synchronization. A complete programming example is presented and analyzed. Implementation issues are then considered and a simple synchronous execution machine running on top of the Chorus distributed operating system is described		François Horn;Jean-Bernard Stefani	1993	Comput. J.	10.1093/comjnl/36.1.4	embedded system;synchronization;real-time computing;computer science;operating system;distributed computing;programming language	DB	-25.345167577801	32.01683941373823	48306
3d8afbfca0238b1e243c61e13a447dc5b78e7835	a lightweight and low-power activity recognition system for mini-wearable devices	wearable computers mobile computing pattern recognition power aware computing storage management;pervasive computing;handheld computers pervasive computing decision support systems;battery life activity recognition system mini wearable devices wristbands wristwatches armbands ar models model generation processing power storage capability;ligthtweight time complexity space complexity battery consumption energy efficient learning;decision support systems;handheld computers	Recent years, miscellaneous mini-wearable devices (e.g. wristbands, wristwatches, armbands) have emerged in our lives to recognize daily activities for the users. Owning to the limitations of hardware, Activity Recognition (AR) models running inside the device are bound to certain challenges, such as processing power, storage capability and battery life. This paper proposes an activity recognition system by considering three limitations above, and a model generation framework to construct AR models which are lightweight in different phases in model generation.	activity recognition;autoregressive model;low-power broadcasting;wearable technology	Lisha Hu	2014	2014 IEEE International Conference on Pervasive Computing and Communication Workshops (PERCOM WORKSHOPS)	10.1109/PerComW.2014.6815189	embedded system;simulation;decision support system;human–computer interaction;computer hardware;computer science;operating system;computer security;ubiquitous computing	Robotics	-32.240308505568166	21.366578082366573	48379
7c5756323ad24e229219863f02eead7479b5c831	certifying assembly programs with trails	partial correctness;wei wang 证明 创新 汇编程序 程序逻辑 认证过程 tcap 模块处理 控制流 certifying assembly programs with trails;certifying assembly code control flow partial correctness;control flow;certifying assembly code	In this paper, we introduce a new way of certifying assembly programs. Unlike previous program logics, we extract the control-flow information from the code and generate an intermediate trail between the specification and the real code. Trails are auxiliary specifications and treated as modules in the certification process. We define a simple modular program logic called trail-based certified assembly programming (TCAP) to certify and link different parts of a program using the corresponding trails. Because the control flow information in trails is explicit, the rules are easier to design. We show that our logic is powerful enough to prove partial correctness of assembly programs with features including stack-based abstractions and self-modifying code.We also provide a semantics for TCAP and prove that the logic is sound with respect to the semantics.	assembly language;control flow;correctness (computer science);self-modifying code;stack-oriented programming language	Wei Wang	2011	Frontiers of Computer Science in China	10.1007/s11704-011-0166-z	computer science;operating system;database;distributed computing;programming language;control flow;computer security;algorithm	PL	-22.03627196020423	27.72800874055277	48475
1bade4ca4d7429dcd802391c15ca90a3ac477c4f	sometimes, rainfall accumulates: talk-alouds with novice functional programmers		When functional programming is used in studies of the Rainfall problem in CS1, most students seem to perform fairly well. A handful of students, however, still struggle, though with different surface-level errors than those reported for students programming imperatively. Prior research suggests that novice programmers tackle problems by refining a high-level program schema that they have seen for a similar problem. Functional-programming students, however, have often seen multiple schemas that would apply to Rainfall. How do novices navigate these choices? This paper presents results from a talk-aloud study in which novice functional programmers worked on Rainfall. We describe the criteria that drove students to select, and sometimes switch, their high-level program schema, as well as points where students realized that their chosen schema was not working. Our main contribution lies in our observations of how novice programmers approach a multi-task planning problem in the face of multiple viable schemas.	accumulator (computing);computer multitasking;functional programming;high- and low-level;higher-order function;network switch;programmer;subgoal labeling;talk box;theory	Kathi Fisler;Francisco Enrique Vicente Castro	2017		10.1145/3105726.3106183	natural language processing;knowledge management;computer science;functional programming;schema (psychology);artificial intelligence	HCI	-28.428371934206268	23.272971108292737	48511
103cacae97da4517318d4970181cc3afd9c84632	transformations for communication fairness in csp	proceso secuencial comunicante;distributed system;systeme reparti;protocole transmission;communicating sequential process;transformacion;equivalence;protocolo transmision;sistema repartido;transformation;processus sequentiel communiquant;equivalencia;transmission protocol	Abstract   Transformations have shown to be a valuable tool for the incorporation of distributed control mechanisms into CSP-programs. This tool will be applied to solve the I/O-guard problem in CSP. The proposed transformation COMFAIR maps a given CSP-program to a CSP-program with I-guards only and introduces the necessary protocol communication. Moreover, for the transformed program we investigate the implications of selection fairness on communication fairness.	fairness measure	Dieter Zöbel	1987	Inf. Process. Lett.	10.1016/0020-0190(87)90132-3	transformation;equivalence;artificial intelligence;mathematics;algorithm	DB	-25.031228442177486	31.7895516008716	48539
2df71c8b3a26699a68afa38027ae2757c8949979	exploiting points-to maps for de-/serialization code generation	pointer analysis;code generation;serialization;aspect oriented development	Serialization code generators for C++ have restrictions on the implementation of dynamic arrays and void/function pointers. If the target program is not implemented with these restrictions, developers have to manually change the source code to facilitate serialization code generation. Unfortunately, such changes hamper the benefits of code generation, and they are not localized. This paper presents the de-/serialization code generator Ser++ that does not restrict the implementation of these pointer types and, hence, eliminates the need to adapt the source code for serialization code generation.  Ser++ can be considered an aspect weaver that i) traces the pointers, ii) identifies the statements in which properties regarding the serialization of pointer attributes can be extracted and, finally, iii) weaves the code to store these properties at runtime. It generates the de-/serialization functions in such a way that they serialize the pointer attributes according to the stored values of the properties. We have successfully used Ser++ to generate de-/serialization methods for a computer architecture and a power-flow simulator, without any modifications to the existing source code.	aspect weaver;c++;code generation (compiler);computer architecture;dynamic array;function pointer;pointer (computer programming);run time (program lifecycle phase);serialization;tracing (software)	Selim Ciraci;Oreste Villa	2013		10.1145/2480362.2480686	code injection;dead code;systematic code;code bloat;parallel computing;serialization;computer science;operating system;redundant code;pointer swizzling;programming language;pointer analysis;code generation;unreachable code	PL	-22.387022856220735	28.461156278692123	48617
d63838e33acba25a2ce3a9c40b95ab403914ac42	toward software synthesis for distributed applications		"""This paper describes Sage, a software environment supporting software development, synthesis, and testing for distributed computing applications. While the principal domain of interest is applications that must be fault-tolerant (i.e., be able to withstand the failure of some of the participants) Sage is not limited to this; it can be extended to distributed applications with no criticality requirements and to those with security requirements. Sage mechanically applies specialized knowledge-theoretic analyses to a distributed application's high-level specification to automatically derive the necessary communication between the participants in the computation. In particular, Sage implements the results of Chandy and Misra [2] and mimics the analyses of others [9, 10, 12, 15] which have previously only been performed theoretically. Sage applies these results to strategies commonly used by programmers of distributed applications, and commonly provided by packaged subsystems for distributed computing (also called """"middleware""""), to derive and synthesize correct, efficient solutions. Using the modal logic of knowledge to describe and reason about coordinating distributed entities is well accepted [2, 3, 5, 6, 9, 10, 12, 15], if highly specialized, and the semantic model has been adapted to suit different distributed environments [6, 7, 10, 11]. In particular, epistemic logic has been used to optimize solutions, to prove impossibility, and to prove possibility [2, 5, 7, 9, 10, 12, 13]. Despite the range of applications in the literature, no software development environment exists that applies and implements these techniques to facilitate writing distributed applications."""	computation;criticality matrix;distributed computing;entity;epistemic modal logic;fault tolerance;high- and low-level;integrated development environment;misra c;middleware;programmer;requirement;software development;theory	Aleta Ricciardi;Paul Grisham	1998				SE	-30.121114571152773	31.885146601076354	48687
7654657ce59704a0df9307546ce421b8f8b46928	macros that work		This paper describes a modified form of Kohlbecker’s algorithm for reliably hygienic (capture-free) macro expansion in block-structured languages, where macros are source-tosource transformations specified using a high-level pattern language. Unlike previous algorithms, the modified algorithm runs in linear instead of quadratic time, copies few constants, does not assume that syntactic keywords (e.g. if) are reserved words, and allows local (scoped) macros to refer to lexical variables in a referentially transparent manner. Syntactic closures have been advanced as an alternative to hygienic macro expansion. The problem with syntactic closures is that they are inherently low-level and therefore difficult to use correctly, especially when syntactic keywords are not reserved. It is impossible to construct a patternbased, automatically hygienic macro system on top of syntactic closures because the pattern interpreter must be able to determine the syntactic role of an identifier (in order to close it in the correct syntactic environment) before macro expansion has made that role apparent. Kohlbecker’s algorithm may be viewed as a book-keeping technique for deferring such decisions until macro expansion is locally complete. Building on that insight, this paper unifies and extends the competing paradigms of hygienic macro expansion and syntactic closures to obtain an algorithm that combines the benefits of both. Several prototypes of a complete macro system for Scheme have been based on the algorithm presented here.	algorithm;high- and low-level;hygienic macro;identifier;pattern language;referential transparency;reserved word;scheme;syntactic closure;time complexity	William D. Clinger;Jonathan Rees	1991		10.1145/99583.99607	programming language;time complexity;syntax;theoretical computer science;computer science;reserved word;macro;identifier;pattern language;hygienic macro;interpreter	PL	-23.910344041343023	24.992608174462575	48815
bf373e549bf28740b877f8955e1d85a226bf3734	automated detection of serializability violations under weak consistency		While a number of weak consistency mechanisms have been developed in recent years to improve performance and ensure availability in distributed, replicated systems, ensuring the correctness of transactional applications running on top of such systems remains a difficult and important problem. Serializability is a well-understood correctness criterion for transactional programs; understanding whether applications are serializable when executed in a weakly-consistent environment, however remains a challenging exercise. In this work, we combine a dependency graph-based characterization of serializability and leverage the framework of abstract executions to develop a fully-automated approach for statically finding bounded serializability violations under any weak consistency model. We reduce the problem of serializability to satisfiability of a formula in First-Order Logic (FOL), which allows us to harness the power of existing SMT solvers. We provide rules to automatically construct the FOL encoding from programs written in SQL (allowing loops and conditionals) and express consistency specifications as FOL formula. In addition to detecting bounded serializability violations, we also provide two orthogonal schemes to reason about unbounded executions by providing sufficient conditions (again, in the form of FOL formulae) whose satisfiability implies the absence of anomalies in any arbitrary execution. We have applied the proposed technique on TPC-C, a real-world database program with complex application logic, and were able to discover anomalies under Parallel Snapshot Isolation (PSI), and verify serializability for unbounded executions under Snapshot Isolation (SI), two consistency mechanisms substantially weaker than serializability. 2012 ACM Subject Classification Theory of computation → Automated reasoning	automated reasoning;business logic;consistency model;correctness (computer science);first-order logic;first-order predicate;ibm tivoli storage productivity center;sql;sensor;serializability;snapshot isolation;theory of computation;weak consistency	Kartik Nagar;Suresh Jagannathan	2018		10.4230/LIPIcs.CONCUR.2018.41	serialization;theoretical computer science;computer science;dependency graph;correctness;satisfiability;bounded function;weak consistency;distributed computing;snapshot isolation;serializability	PL	-20.724950792247462	31.367366096615154	48870
c7d4a370b6d23f17e65d1158bfbc6c65be8d33b9	recognizing a program's design: a graph-parsing approach	plan calculus;program diagnostics;common lisp graph parsing programming structures prototype recognizer cliches hierarchical description representation shift graphical representation plan calculus programmer s apprentice intelligent programming system;graph parsing;programming structures;hierarchical description;representation shift;documentation programming profession humans algorithm design and analysis data structures binary trees psychology prototypes debugging automatic programming;intelligent programming system;cliches;graphical representation;software tools program diagnostics;recognizer;software tools;source code;prototype;programmer s apprentice;common lisp	Psychological experiments have shown that programmers tend to use the same structure over and over. The authors call these commonly used programming structures 'cliches'. They describe a prototype, the Recognizer, that automatically finds all occurrences of a given set of cliches in a program and builds a hierarchical description of the program in terms of the cliches it finds. The key to the Recognizer's approach is a representation shift. Instead of looking for cliches directly in the source code, the Recognizer first translates the program into a language-independent, graphical representation called the Plan Calculus. The Plan Calculus is a program representation shared by all components of the Programmer's Apprentice, an intelligent programming system. Thus, although the authors have demonstrated the Recognizer only on small Common Lisp programs, the underlying technology is language-independent.<<ETX>>	admissible numbering;common lisp;experiment;finite-state machine;language-independent specification;parsing;plankalkül;programmer;prototype	Charles Rich;Linda M. Wills	1990	IEEE Software	10.1109/52.43053	computer science;theoretical computer science;software engineering;prototype;programming language;algorithm;source code	PL	-27.250537661623497	23.44479249747881	48904
af379d9cea2488a57e352d41eb0e755799076d4c	specifying robot reactivity in procedural languages	event recognition;procedural languages;programming language semantics;high level languages;robot programming control engineering computing high level languages intelligent robots programming language semantics;intelligent robots;mobile robot;event response;robotic system programming;python;conference item;semantics design;mobile robot programming;hybrid architecture;robot programming mobile robots intelligent robots human robot interaction embedded system programming profession visual basic application software computer architecture robustness;robot reactivity;control engineering computing;event response robot reactivity procedural languages robotic system programming mobile robot programming semantics design python event recognition;robot programming	A key part of programming a robotic system is specifying the responses to events that the robot may encounter. Existing methods of programming responses include event loops, reactive languages and hybrid architectures, none of which meet the specific needs of mobile robot programming. This work presents a design for new semantics for specifying reactivity in mobile robot programs, one that allows for effective specification of reactive behaviour within procedural robot programs. An initial evaluation version is implemented in Python. Events and responses are supported as program objects, and are connected together by new statements. Programmers specify connections between events and responses anywhere within the program code, so connections can easily be changed in response to changes in program and robot state	admissible numbering;event loop;mobile robot;preprocessor;programmer;programming complexity;programming language;python	Geoffrey Biggs;Bruce A. MacDonald	2006	2006 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2006.281755	mobile robot;simulation;python;computer science;theoretical computer science;programming language	Robotics	-30.43822517577008	30.36235332305493	48921
32db18ca6e3ff6f273a8f8a52b341683901eed8a	reflection-based language support for the heterogeneous capture and restoration of running computations	programming language;heterogeneous environment;internal structure	This work is devoted to the study of the problem of user-level capture and restoration of running computations in heterogeneous environments. Support for those operations has traditionally been offered through ready-made solutions for specific applications, which are difficult to tailor or adapt to different needs. We believe that a more promising approach would be to build specific solutions as needed, over a more general framework for capture and restoration. In this work, in order to explore the basic mechanisms a language should provide to support the implementation of different policies, we extend the Lua programming language with an API that allows the programmer to reify the internal structures of execution into fine-grained language values.	application programming interface;circuit restoration;computation;lua;programmer;programming language;user space	Anolan Milanés;Noemi de La Rocque Rodriguez;Roberto Ierusalimschy	2010	CoRR		simulation;computer science;theoretical computer science;programming language	PL	-28.057295038963897	29.059622277130774	48955
7ceebc28decb778950699e4c3a9df2872e022247	embedding a tcl web server into apl	apl backbone;available tcl extension;tcl web server;tool command language;string handling;new apl system command;tcl source code;apl2c system;powerful array handling capacity;tcl interpreter;source code;scripting language	Tcl stands for Tool Command Language. It is a script language, but unlike the well-known perl, it is also an extensible interpreter that is designed to be easily embedded into other applications. The Tcl source code is available from the Internet and the package can be used freely even in commercial applications. In addition to its condensed notation and simple syntax, APL's strength is its powerful array handling capacity. Tcl is based on string handling and very strong on gluing different pieces of software together. It is especially the many available Tcl extensions for GUI, network, database, HTML and XML support, which make the combination of APL and Tcl very attractive.With the implementation of a few new APL system commands into the APL2C system, it is possible to create one or several Tcl interpreters from within APL. As each of the Tcl interpreters runs on its own thread, a synchronization mechanism is provided for the communication between APL and Tcl. As an example, a Tcl web server with an APL backbone has been realized to demonstrate the power of the combination between APL and Tcl.	apl;command language;embedded system;graphical user interface;html;internet backbone;interpreter (computing);perl;scripting language;server (computing);string (computer science);tcl;web server;whole earth 'lectronic link;xml	Tilman P. Otto	2000		10.1145/570407.570415	computer science;operating system;database;scripting language;programming language;source code	OS	-30.826180014734526	28.19517151756441	49046
d012ed49c091754b335179686880c5e8c44cfb9e	very fast lr parsing	strongly connected component;error recovery;directed graph	LR parsers can be made to run 6 to 10 times as fast as the best table-interpretive LR parsers. The resulting parse time is negligible compared to the time required by the remainder of a typical compiler containing the parser. A parsing speed of 1/2 million lines per minute on a computer similar to a VAX 11/780 was achieved, up from an interpretive speed of 40,000 lines per minute. A speed of 240,000 lines per minute on an Intel 80286 was achieved, up from an interpretive speed of 37,000 lines per minute. The improvement is obtained by translating the parser's finite state control into assembly language. States become code memory addresses. The current input symbol resides in a register and a quick sequence of register-constant comparisons determines the next state, which is merely jumped to. The parser's push-down stack is implemented directly on a hardware stack. The stack contains code memory addresses rather than the traditional state numbers. The strongly-connected components of the directed graph induced by the parser's terminal and nonterminal transitions are examined to determine a typically small subset of the states that require parse-time stack-overflow-check code when hardware does not provide the check automatically. The increase in speed is at the expense of space: a factor of 2 to 4 increase in total table size can be expected, depending upon whether syntactic error recovery is required.	alphabet (formal languages);assembly language;code generation (compiler);compiler;context-free language;directed graph;lr parser;memory address;parsing;speedup;stack overflow;stack machine;strongly connected component;terminal and nonterminal symbols;vax	Thomas J. Pennello	1986		10.1145/12276.13326	directed graph;computer science;theoretical computer science;programming language;strongly connected component;algorithm	Arch	-24.41708867556565	24.946230612607405	49453
e4cab21f25d67de4e422072df12bb172d697640e	an object model for dynamic mixins		Dynamic mixins allow objects to be modified at runtime with modular extensions. In applications where method calls must traverse through multiple extensions, a performance penalty relative to static inheritance is realized as receivers of super-calls must be determined at run-time. This work describes an object model which significantly reduces this penalty. The approach is described in terms of a statically typed dynamic mixin-based language called mix.	mixin;run time (program lifecycle phase);traverse;type system	Eden Burton;Emil Sekerinski	2018	Computer Languages, Systems & Structures	10.1016/j.cl.2017.07.001	programming language;computer science;theoretical computer science;traverse;mixin;modular design;object model	PL	-25.908353476696423	28.587573562520866	49512
07e794b59e9ca4f6e6c12e886e6de0c89e65f657	an application of abstract interpretation to floating point arithmetic	floating point arithmetic;abstract interpretation		abstract interpretation	Yamine Aït Ameur;Patrice Cros;J.-J. Falcon;A. Gomez	1992			arithmetic	Logic	-19.934287409833896	19.422789185819635	49531
c039bd33dc23dae33e14f413389981d8954604e2	the distributed termination problem: formal solution and correctness based on petri nets	petri net	A network of processes exchanging information by message passing is called a distributed system. Each process has computing and storage facilities and is connected to some other processes via bidirectional communication channels. Termination detection in such a distributed system is one of the classical theoretical problems dealing with local protocols to achieve global knowledge about the system state. In the literature various solutions are proposed. They differ in their algorithmic idea and in their level of formal representation. Our work is based on [9] which generalizes the restrictive but interesting concept of [3] to consider a distributed computation as a Diffusing Computation towards Multi Diffusing Computations. The solution proposed in [9] was the first that fulfills the quality criteria of genericity, symmetry and indulgence. This was done by computing monotonous local states, thereby achieving consistent global system states without the need of synchronization mechanisms like interruption of the message exchange during a global state check. (i.e. [5]). To verify a solution correctly a formal model for the description of nonsequential behaviour must be chosen for both the problem specification and the modelling of the algorithm. The solution presented in this paper is based on Petri nets with structured tokens intrOduced in [8]. Besides the powerful modelling facilities adequate for dealing with distributed systems and the clarity of their graphical representations, Petri nets supply a lot of analysis techniques for formal verification. In this paper we will use particularly place invariants, and we will integrate graphtheoretical concepts into the calculus by regarding some predicates in the set of reachable markings. Chapter 2 gives some basic definitions. In chapter 3 the problem is specified as a property of a Petri net for an arbitrary distributed computation. A solution is given by two net extensions in chapter 4. We begin with a protocol for computing a stable meaningful condition. Then a diffusion protocol is specified to visit all processes. Thereby the stable condition is used as a delay. Both protocols together are verified to detect distributed termination always and correctly.	algorithm;computation;correctness (computer science);distributed computing;formal verification;generic programming;graphical user interface;interrupt;knowledge representation and reasoning;mathematical model;message passing;petri net	Dominik Gomm;Rolf Walter	1990		10.1007/3-540-53414-8_38	discrete mathematics;stochastic petri net;theoretical computer science;distributed computing;process architecture;petri net	Logic	-33.12646763699086	30.715802632589185	49619
a740eadbde86292a406ab98fd545d9d4ca15a391	an equivalence algorithm to point out errors for basic lotos in a distributed system environment and its prototype	distributed system;protocols;errors;educational support system;formal specification;distributed processing;specification;equivalence algorithm;educational support system equivalence algorithm error detection lotos distributed system prototype formal description technique specification osi protocol layers mathematical model equivalence relation verification error correction;formal description technique;support system;computer science education;equivalence relation;specification languages;error correction;knowledge acquisition;mathematical model;error correction protocols computer errors prototypes electronic mail open systems mathematical model computer science education educational programs communication standards;osi protocol layers;equivalence relation verification;error detection;open systems;prototype;computer science education specification languages formal specification errors distributed processing open systems protocols;lotos	L O T O S which is one o f the Formal Description Techniques (FDTs) is applied fo r the formal description of other distributed systems and protocol specification of OSI protocol layers. But LOTOS is dificult t o understand and to learn because it is based on the mathematical model. In this paper, for supporting the knowledge acqirisition to learners studying basic LOTOS, we suggest a new algorithm that can verify an equivalence relation, find an error location, and correct an error when the error is occurred at another process. Finally, we give an example of a prototype program for an educational support system of Basic LOTOS.	algorithm;distributed computing;environment variable;formal methods;mathematical model;osi model;prototype;turing completeness	Byung-Ho Park;Shigetomo Kimura;Eun-Seok Lee;Norio Shiratori	1997		10.1109/ICPADS.1997.652555	error detection and correction;computer science;theoretical computer science;programming language;statistics	Robotics	-32.13754024052601	32.31181935568232	49631
20125d504c591a21eeb9e706886f64cd0fcdce42	automatic transformation of bit-level c code to support multiple equivalent data layouts	data format;declarative languages;data layout	Portable low-level C programs must often support multiple equivalent in-memory layouts of data, due to the byte or bit order of the compiler, architecture, or external data formats. Code that makes assumptions about data layout often consists of multiple highly similar pieces of code, each designed to handle a different layout. Writing and maintaining this code is difficult and bug-prone: Because the differences among data layouts are subtle, implicit, and inherently low-level, it is difficult to understand or change the highly similar pieces of code consistently. We have developed a small extension for C that lets programmers write concise declarative descriptions of how different layouts of the same data relate to each other. Programmers then write code assuming only one layout and rely on our translation to generate code for the others. In this work, we describe our declarative language for specifying data layouts, how we perform the automatic translation of C code to equivalent code assuming a different layout, and our success in applying our approach to simplify the code base of some widely available software.	32-bit;64-bit computing;algorithm;bit numbering;bit-level parallelism;byte;code segment;compiler;declarative programming;endianness;high- and low-level;in-memory database;legacy code;machine translation;programmer;semiconductor industry;source transformation	Marius Nita;Dan Grossman	2008		10.1007/978-3-540-78791-4_6	code word;dead code;code bloat;computer science;theoretical computer science;redundant code;database;programming language;algorithm;code generation;unreachable code;source code	PL	-23.855294658951337	25.727435742110508	49636
9ff808e7caa736422c6f242d98e483431a65e52a	from inheritance to feature interaction or composing monads	object oriented programming;feature interaction	We show that techniques for monad composition can be used nicely for modeling object oriented programming concepts In this functional setting we develop a new model for composing objects from individual features in a modular way Features are similar to abstract subclasses but separate the core functionality of a subclass from overwriting meth ods We view method overwriting more generally as resolving interactions between two features The interaction handling is speci ed separately and added when features are composed This generalizes inheritance as found in object oriented languages and leads to a new view of objects in a func tional setting Our concepts are implemented in Gofer and generalize some monadic programming techniques where objects correspond to monads features to monad transformers and feature interactions are resolved by lifting functions through monad transformers	feature interaction problem;lambda lifting;monad (functional programming);monad transformer;overwriting (computer science);transformers	Christian Prehofer	1997		10.1007/978-3-642-60831-5_76	natural language processing;pattern recognition;mathematics;programming language	PL	-25.656766087319216	27.033017424794323	49679
052287e3e22853a2c5e92458c979e0123a9afd84	converting an imperative program to a declarative one	imperative language;transaltion;denotational semantic;declarative languages;denotational semantics;declarative language	"""The main issue of this paper is to study the problem of formal translation of programs from an imperative language to a declarative one. To that end, we define a simple imperative language, denoted by L""""1, that supports the basic types of statements that can be found in imperative paradigm such as assignment, looping, and selection or conditional branching. We also define a declarative language, denoted by L""""2, where a program is a set of independent variable definitions that may involve some special arithmetic expressions (conditional expression or recursive expression). For instance, x=5+if(a<3,2*a,a-1) could be a definition in L""""2 that affects to x the value 5+2*a if a<3 and otherwise it affects the value 5+(a-1). The semantics attached to each language is denotational, where the meaning of a program in a given environment (memory state) is an environment. Finally, we introduce a formal translation function that migrates any program in L""""1 to an equivalent (with respect to the semantics) one in L""""2 and we prove its correctness (the semantics of the original version of any program is equal to the semantics of its translated version)."""	declarative programming;imperative programming	Mohamed Mbarki;Mohamed Mejri;Béchir Ktari	2006	Knowl.-Based Syst.	10.1016/j.knosys.2005.10.003	natural language processing;imperative programming;declarative programming;action semantics;computer science;programming language;denotational semantics of the actor model;operational semantics;denotational semantics;algorithm	SE	-22.015016534016187	23.824400052927228	49745
1d84022e5a4c38287ebad69b8709cef5dae7cb73	a schema language for coordinating construction and composition of partial behavior descriptions	model based testing;partial behaviors;model composition;symbolic representation	We report on a schema language for coordinating the construction and composition of partial behavior descriptions. The language is a frontend to the semantical and implementation framework of action machines, which allows to encode behaviors of software artifacts in a language-agnostic manner, supporting both state-based and interaction-based description styles, as well as partial descriptions by means of symbolic representations. Our approach is currently being incorporated into an advanced model-based specification and testing environment at Microsoft Research.	encode;language-independent specification;microsoft research;model-based specification	Wolfgang Grieskamp;Nicolas Kicillof	2006		10.1145/1138953.1138966	natural language processing;computer science;communication;algorithm	SE	-32.89726436061074	23.80463961707578	49756
7c345ad5dd6cdb25dc8f342ea8b22f4f0b195434	an interpolation-based compiler and optimizer for relational queries (system design report)		We outline the implementation of a query compiler for relational queries that generates query plans with respect to a database schema, that is, a set of arbitrary first-order constraints, and a distinguished subset of predicate symbols from the underlying signature that correspond to access paths. The compiler is based on a variant of the Craig interpolation theorem, with reasoning realized via a modified analytic tableau proof procedure. This procedure decouples the generation of candidate plans that are interpolants from the tableau proof procedure, and applies A*-based search with respect to an external cost model to arbitrate among the alternative candidate plans. The tableau procedure itself is implemented as a virtual machine that operates on a compiled and optimized byte-code that faithfully implements reasoning with respect to the database schema constraints and a user query.	analysis of algorithms;byte;compiler;database schema;first-order predicate;interpolation;long division;mathematical optimization;method of analytic tableaux;query language;relational database;virtual machine	David Toman;Grant E. Weddell	2017			compiler;theoretical computer science;programming language;interpolation;systems design;compiler correctness;computer science	DB	-20.55225607076645	24.08259002452669	49874
7ceaffd3bfd58ee19a8634a1cd200c98c97921e4	profit: prolog with features, inheritance and templates	prolog programs usable;standard prolog;prolog program;prolog term representation;prolog term;built-in prolog term unification;profit program;feature term;feature structure;sorted feature term	ProFIT is an extension of Standard Prolog with Features, Inheritance and Templates. P roFIT Mlows the programmer or grammar developer to declare an inheritance hierarchy, features and templates. Sorted feature terms can be used in ProFIT programs together with Prolog terms to provide a clearer description language for linguistic structures. P roFIT compiles all sorted feature terms into a Prolog term representation, so that the built-in Prolog term unification can be used for the unification of sorted feature structures, and no special unification algorithm is needed. ProFIT programs are compiled into Prolog programs, so that no meta-interpreter is needed for their execution. P roFIT thus provides a direct step from grammars developed with sorted feature terms to Prolog programs usable for practical NLP systems.	algorithm;compiler;natural language processing;programmer;prolog;unification (computer science)	Gregor Erbach	1995			natural language processing;computer science;artificial intelligence;theoretical computer science;definite clause grammar;programming language;algorithm;profitability index	PL	-25.455881987477866	21.975362249438714	49942
37780aa64af3356b766439f451b6a69ca7530956	why it's nice to be quoted: quasiquoting for haskell	quasiquoting;complex data;domain specific language;meta programming;domain specificity	Quasiquoting allows programmers to use domain specific syntax to construct program fragments. By providing concrete syntax for complex data types, programs become easier to read, easier to write, and easier to reason about and maintain. Haskell is an excellent host language for embedded domain specific languages, and quasiquoting ideally complements the language features that make Haskell perform so well in this area. Unfortunately, until now no Haskell compiler has provided support for quasiquoting. We present an implementation in GHC and demonstrate that by leveraging existing compiler capabilities, building a full quasiquoter requires little more work than writing a parser. Furthermore, we provide a compile-time guarantee that all quasiquoted data is type-correct.	compile time;compiler;domain-specific language;embedded system;parse tree;programmer;the glorious glasgow haskell compilation system	Geoffrey Mainland	2007		10.1145/1291201.1291211	metaprogramming;computer science;domain-specific language;programming language;algorithm;complex data type	PL	-23.70046593422021	26.17812125538285	50000
ceb83182d4eca28ef5aae465e310e91998a77319	design of an image analysis system	image processing;graphical interface;software systems;public domain software;programming environment for pattern analysis image processing image analysis free software systems puma;image analysis;public domain software image processing;image analysis image processing image color analysis image edge detection application software software engineering software systems internet image segmentation libraries;free software	In this contribution we present a software system for image processing and image analysis (PUMA) and compare it to other free software systems for this purpose. The system has been developed over a decade and several applications are reported. The system can be used in various modes: over the Internet, via GIMP plugins, via a simple graphical interface, using commandline or scripting tools, and as an application programmer's library.	algorithm;class hierarchy;command-line interface;computer vision;external data representation;gimp;global variable;graphical user interface;image analysis;image processing;internet;java class library;plug-in (computing);programmer;software architecture;software system;xml	Dietrich Paulus;T. Dickscheid;K.-D. Berg	2005	Seventh International Workshop on Computer Architecture for Machine Perception (CAMP'05)	10.1109/CAMP.2005.20	domain analysis;software visualization;computer vision;software sizing;image processing;computer science;theoretical computer science;software framework;component-based software engineering;software development;digital image processing;software construction;automatic image annotation;software system;computer graphics (images)	Arch	-33.364625852865	24.27491873202571	50003
f7b340e1b2a09de03e2e54f3ab2fb4afe6df68d5	inap protocol test suite verification method using the iut simulator for ain system conformance testing	verification;configuracion;systeme intelligent;methode essai;protocole transmission;sistema inteligente;reseau ordinateur;simulation;simulacion;formal description technique;computer network;protocolo transmision;conformance testing;natural language;intelligent network;implementation under test;intelligent system;red ordenador;protocol specification;test method;verificacion;configuration;metodo ensayo;transmission protocol	The INAP(Intelligent Network Application Protocol) protocol test suite is the test procedures to conform that the implemented INAP protocol is equivalent to the standard specification of the INAP. The INAP test suite is generated automatically by using formal description technique. Before the generated test suite is applied to the real target system, it should be verified that this test suite has been correctly built according to the INAP specification. In this paper, we implemented the IUT(Implementation Under Test) simulator on the K1197 tester to verify the generated INAP test suite. This method enabled us to correct the syntactic errors in the test suite and unexpected test sequence errors. It guarantees the reliability for verified results and reduces time and costs to verify the test suite than the manual method based on the protocol specification represented by natural language.	conformance testing;test suite	Hyunsook Do;Seongyong Bae;Sangki Kim	1998		10.1007/BFb0053500	embedded system;intelligent network;model-based testing;verification;simulation;computer science;operating system;conformance testing;test suite;natural language;test method;configuration;statistics	Logic	-31.725481441019568	31.160453271519017	50050
50ca439de127b90f1eff32b6f441dd8b904da111	a polymorphic type system with progress for binary sessions	session types;progress;subtyping;pi calculus;communication centered programming	A static bounded polymorphic type system is presented in this paper, which ensures the progress property, i.e., the property that once a communication has been established, well-formed programs will never starve at communication points. The introduction of subtyping for session types and the relaxed duality relation increases the flexibility of the type system, and allows the participants in a conversation to follow different protocols that are nevertheless compatible in a sense defined by the subtyping relation. In addition, to keep progress in sessions, the type compliance is defined to associate with the relaxed duality relation, where the environment is balanced. Finally, the soundness and communication safety of the type system are proved, and some related work and possible future work in this area are discussed.	type system	Zhenguo Yang;Farong Zhong;Jinfang Zhang	2012		10.1007/978-3-642-33469-6_57	π-calculus;subtyping;computer science;distributed computing;algorithm	Crypto	-30.85203330980537	31.718551466342994	50072
2c3c01162f02202f7eef4b18c3ec13a4750e6243	multimethods and separate static typechecking in a language with c++-like object model	object oriented language;programming language;object oriented;polymorphism;it adoption;multiple inheritance;separate compilation;source code;object model	The goal of this paper is the description and analysis of multimethod implementation in a new objectoriented, class-based programming language called OOLANG. The implementation of the multimethod typecheck and selection, deeply analyzed in the paper, is performed in two phases in order to allow static typechecking and separate compilation of modules. The first phase is performed at compile time, while the second is executed at link time and does not require the modules’ source code. OOLANG has syntax similar to C++; the main differences are the absence of pointers and the realization of polymorphism through subsumption. It adopts the C++ object model and supports multiple inheritance as well as virtual base classes. For this reason, it has been necessary to define techniques for realigning argument and return value addresses when performing multimethod invocations.	algorithm;c++;compile time;compiler;computation;dispatch table;dynamic dispatch;link time;mathematical optimization;multiple dispatch;multiple inheritance;programming language;return statement;subsumption architecture;type system	Emanuele Panizzi;Bernardo Pastorelli	2000	CoRR		c++;computer science;object;theoretical computer science;database;new;programming language;object-oriented programming	PL	-25.239784936643076	27.6047727044764	50236
82309b9cb01dd373f2b2fcb49a18977f8f876e80	pattern matching with abstract data types	abstract data type;pattern matching		abstract data type;pattern matching	F. Warren Burton;Robert D. Cameron	1993	J. Funct. Program.	10.1017/S095679680000068X	computer science;pattern matching;programming language;abstract data type	PL	-23.47096301936818	20.838067438247847	50315
1078169697e5e70d656f983817f8dd1fb538322e	edicates - a specification of calling sequences	interface builder;clim;graphical user interface;rapid prototyping;lisp	• : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : • : : : : : • : : : : : : : : : : : : : : : : : • : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : • : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : . : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :		Mark Cashman	1993	SIGPLAN Notices	10.1145/163114.163124	computer architecture;computer science;operating system;lisp;graphical user interface;programming language;graphical user interface testing	PL	-30.738232772642412	27.310056679162702	50499
f6140fdddc9c4c4e9d2e11bbec13e88182487a2a	definition of programming language semantics using grammars for hierarchical graphs	programming language semantics	machine states and state transitions. An H-graph that represents the state of an abstract machine during execution of a program in a high-level programming language may contain thousands of nodes and hundreds of graphs. For example, HAL/S is a complex high-level language similar to PL/I with extensions for real-time programming. The HAL/S abstract machine contains 19 top-level nodes representing the various system data structures required for control of program execution, such as queues of active and waiting processes, stacks of activation records, lists of inputoutput channels, etc. Thus most of these top-level nodes contain graphs whose nodes themselves represent complex structures, such as processes, each modeled by many further levels of graph structure. All of the modeling power of H-graphs must be used to provide an intelligible, analyzable model of such a complex machine state. The situation is made more difficult by the fact that each step in program execution results in a transformation of the state H-graph, and program execution may involve thousands of execution steps. Most of the analysis that one wishes to do of the abstract machine involves	semantics (computer science)	Terrence W. Pratt	1978		10.1007/BFb0025735	natural language processing;first-generation programming language;very high-level programming language;language primitive;formal semantics;action semantics;programming domain;computer science;functional logic programming;linguistics;rank;programming paradigm;low-level programming language;fifth-generation programming language;programming language theory;programming language;well-founded semantics;operational semantics;programming language specification;high-level programming language;denotational semantics;semantics;computational semantics	PL	-26.74563584267064	31.641293332346663	50729
d24c1939b88cf16217ea7d7a3a103876abceefc8	towards a full formal specification of the javacard api	virtual machine;formal specification;java card;reponse transitoire;machine virtuelle;specification language;specification formelle;especificacion formal;transient response;respuesta transitoria;lenguaje especificacion;maquina virtual;langage specification	This paper reports on ongoing work to develop a formal specification of the JavaCard API using the specification language JML. It discusses the specification of the JCSystem class, which deals with the JavaCard firewall, (atomic) transactions and transient objects. The JCSystem class seems to be the hardest class in the API to specify, and it is closely connected with some of the peculiarities of JavaCard as opposed to Java.	application programming interface;firewall (computing);formal specification;java card;java modeling language;specification language	Hans Meijer;Erik Poll	2001		10.1007/3-540-45418-7_14	embedded system;java card;specification language;computer science;virtual machine;operating system;formal specification;database;programming language;transient response;language of temporal ordering specification	SE	-25.834273621288865	30.896632806216243	50807
0eb80ec80b809e40789e05b73687d0c77b0d06db	using the generative aspect of attribute grammars in a knowledge based way	attribute grammar;program generation;generic programming;knowledge base	The principles of the program generation environment FLR, which has been implemented at our department, are described. The system supports the reuse of already developed specifications of various kinds and was motivated by the idea to use the source text of already running programs again. This knowledge of parts of programs or documents, which were specified already, is stored as attributed grammar rules in a database. New documents can be composed in an interactive way using this database. It is demonstrated to use FLR to generate programs and Attribute Grammars.		Peter Forbrig	1991		10.1007/3-540-54572-7_17	natural language processing;l-attributed grammar;computer science;pattern recognition;context-free grammar;programming language;attribute grammar;adaptive grammar	DB	-27.39200004924995	22.96182551987396	50841
e09683e7d48f0d07574dd8cc2e5837fe27569387	actor grammars	computational mathematic;basic property;actor system;parallel system;message passing	Actor systems are a model of massively parallel systems based on asynchronous message passing. This paper presents a formalism for a restricted version of actor systems in the framework of graph grammars. To this aim actor grammars are introduced, motivated, and illustrated by examples. Some of the basic properties pertinent to graph transformations in actor grammars are discussed.	actor model;graph rewriting;message passing;relevance;semantics (computer science)	Dirk Janssens;Grzegorz Rozenberg	1989	Mathematical systems theory	10.1007/BF02088293		PL	-21.343140906613105	21.968896470656052	51002
54e748c2c157a733ddfe6f9d1a1b704ed59afa9d	reverse engineering through formal transformation: knuth's ‘polynomial addition’ algorithm	compilacion;generation code;generacion codigo;code generation;langage evolue;semantics;program transformation;specification programme;transformation programme;semantica;semantique;analisis programa;algorithme;algorithm;transformacion programa;compilation;lenguaje evolucionado;program analysis;analyse programme;program specification;high level language;especificacion programa;reverse engineering;algoritmo	"""In this paper we will take a detailed look at a larger example of program analysis by transformation. We will be considering Algorithm 2.3.3.A from Knuth's \Fundamental Algorithms"""" Knuth (1968) (P.357) which is an algorithm for the addition of polynomials represented using four-directional links. Knuth (1974) describes this as having \a complicated structure with excessively unrestrained goto statements"""" and goes on to say \I hope someday to see the algorithm cleaned up without loss of its eeciency"""". Our aim is to manipulate the program, using semantics-preserving operations, into an equivalent high-level speciication. The transformations are carried out in the WSL language, a \wide spectrum language"""" which includes both low-level program operations and high level speciications, and which has been speciically designed to be easy to transform."""	algorithm;goto;high- and low-level;high-level programming language;polynomial;program analysis;reverse engineering;wide-spectrum language	Martin P. Ward	1994	Comput. J.	10.1093/comjnl/37.9.795	program analysis;exact cover;dancing links;computer science;knuth's algorithm x;semantics;uniform binary search;programming language;high-level programming language;algorithm;code generation;reverse engineering	PL	-20.266920257875235	23.93196681525222	51035
0b31802845aaece8d41fbbdbbfebc136d4d514f3	user assisted data structure debugging and verication		OF THE DISSERTATION User Assisted Data Structure Debugging and Verification	data structure;debugging	Vineet Singh	2016				Logic	-30.01995270428835	27.675489419875067	51078
9f427b83fe9524190b240f158cbbc3b1049647c2	extending aspectj for separating regions	feature modeling;aspect oriented programming;synchronization;feature oriented programming;region;language design;open source software	Synchronization is a good candidate for an aspect in aspect-oriented programming (AOP) since programmers have to choose the best granularity of synchronization for the underlying hardware to obtain the best execution performance. If synchronization is an aspect, programmers can change the synchronization code independently of the rest of the program when the program runs on different hardware. However, existing AOP languages such as AspectJ have problems. They cannot select an arbitrary code region as a join point. Moreover, they cannot enforce weaving of a synchronization aspect. Since it is an alternative feature in feature modeling, at least one of available synchronization aspects must be woven. Otherwise, the program would be thread-unsafe. Since an aspect in AspectJ is inherently optional, programmers must be responsible for weaving it. To solve these problems, this paper proposes two new constructs for AspectJ, regioncut and assertions for advice. Regioncut selects arbitrary code region as a join point and assertion for advice enforces weaving a mandatory advice. We implemented these constructs by extending the AspectBench compiler. We evaluated the design of our constructs by applying them to two open-source software products, Javassist and Hadoop.	apache hadoop;arbitrary code execution;aspect-oriented programming;aspectj;assertion (software development);compiler;feature model;javassist;join point;open-source software;programmer;thread safety	Shumpei Akai;Shigeru Chiba	2009		10.1145/1621607.1621616	synchronization;real-time computing;region;aspect-oriented programming;computer science;distributed computing;programming language	PL	-26.324929086249462	29.676625629516487	51084
5fc00b8ebd7262cfe0f83aad62910602468919aa	computational logic in multi-agent systems	time temporal logic;dynamic logic	In open multi-agent systems agent interaction is usually ruled by public protocols defining the rules the agents should respect in message exchanging. The respect of such rules guarantees interoperability. Given two agents that agree on using a certain protocol for their interaction, a crucial issue (known as “a priori conformance test”) is verifying if their interaction policies, i.e. the programs that encode their communicative behavior, will actually produce interactions which are conformant to the agreed protocol. An issue that is not always made clear in the existing proposals for conformance tests is whether the test preserves agents’ capability of interacting, besides certifying the legality of their possible conversations. This work proposes an approach to the verification of a priori conformance, of an agent’s conversation policy to a protocol, which is based on the theory of formal languages. The conformance test is based on the acceptance of both the policy and the protocol by a special finite state automaton and it guarantees the interoperability of agents that are individually proved conformant. Many protocols used in multi-agent systems can be expressed as finite state automata, so this approach can be applied to a wide variety of cases with the proviso that both the protocol specification and the protocol implementation can be translated into finite state automata. In this sense the approach is general. Easy applicability to the case when a logic-based language is used to implement the policies is shown by means of a concrete example, in which the language DyLOG, based on computational logic, is used.	automata theory;automaton;characterization test;computational logic;conformance testing;encode;finite-state machine;formal language;interaction;interoperability;multi-agent system;software agent;verification and validation	João Leite;Paolo Torroni	2013		10.1007/978-3-642-40624-9	dynamic logic;linear temporal logic;description logic;logic optimization;logic family;interval temporal logic;computational logic;sequential logic;fair computational tree logic;substructural logic;multimodal logic;temporal logic of actions	AI	-33.121277170861454	30.793079101135486	51220
ddcec9b0ae37705018c9d8f855bf2e0821ea831f	reasoning support for caslwith automated theorem proving systems	algebraic specification;first order;automated theorem proving;first order logic	We connect the algebraic specification language Casl with a variety of automated first-order provers. The heart of this connection is an institution comorphism from Casl to SoftFOL (softly typed firstorder logic); the latter is then translated to the provers’ input syntaxes. We also describe a GUI integrating the translations and the provers into the Heterogeneous Tool Set. We report on experiences with provers, which led to fine-tuning of the translations. This framework can also be used for checking consistency of specifications.	algebraic specification;automated theorem proving;axiomatic system;change management (engineering);first-order predicate;for loop;graphical user interface;human-readable medium;infinite loop;isabelle;jack lutz;mathematical optimization;region connection calculus;spass;sensor;specification language;undecidable problem	Klaus Lüttich;Till Mossakowski	2006		10.1007/978-3-540-71998-4_5	computer science;theoretical computer science;first-order logic;programming language;algorithm	Logic	-19.77455178952168	20.890494840075487	51460
ee0ff449dad4dcb68af5b10ffdb543164773f9ac	semantics and type checking of dependently-typed functional programs			dependent type;type system	Yorck Hunke	2004				PL	-22.735454942609177	21.429164073369805	52038
01e5a01ff140b615f18d1590a171b2c868666d25	sharing analysis of lazy first-order functional programs	sharing analysis;functional programming;first order	In this paper, we describe two static analyses for detecting the sharing of data structures in lazy first order functional programs. The first analysis is used to determine sharing due to multiple references from program code which has yet to be executed. The second analysis is used to determine sharing due to multiple references from the heap after a program has been executed. Both these analyses are defined on a domain of sharing patterns. This domain contains infinite chains, so the usual iterative method for finding fixpoints will not terminate in general. We show how this domain can be mapped onto domains containing finite chains to allow the compile-time analysis of sharing.	first-order predicate;lazy evaluation	Geoff W. Hamilton	1992			reactive programming;theoretical computer science;lazy evaluation;database;distributed computing	Logic	-20.556953761252498	28.390921002426648	52050
3786dbd9f6dc0627d2c89834a0e5364a44e5b933	coverage of ocl operation specifications and invariants		We consider operation coverage of OCL operation specifications and invariants in class diagrams with respect to sequence diagrams. The coverage criteria are based on the operations that are executed from the sequence diagrams and their asserted OCL subexpressions. We propose an algorithm that automatically generates a set of sequence diagrams in order to maximise these coverage criteria. A model finder is leveraged for this purpose. As a result, also operations and constraints can be determined that can never be executed and asserted, respectively. Our algorithm has been implemented in the UML specification tool USE.	algorithm;class diagram;invariant (computer science);object constraint language;scalability;sequence diagram;unified modeling language;usability	Mathias Soeken;Julia Seiter;Rolf Drechsler	2015		10.1007/978-3-319-21215-9_12	theoretical computer science;object constraint language;sequence diagram;class diagram;unified modeling language;invariant (mathematics);computer science	SE	-20.257617736369532	26.292866625047278	52579
6b5049820e0ed61f252916b36cb7403bdcd2bd8e	compiler implementation of adts using profile data	software systems	There are many possible implementations of some very useful programming abstractions (sets, lists, and maps, to name a few), and selecting from among them is currently one of the early tasks in the design of a software system. While programming discipline and/or language features may allow the user to change implementations of an abstraction relatively easily, there remains the inherent problem of selecting a consistent and efficient set of implementations for a particular program. A small set of extensions to existing languages allows the specification of the necessary profile data within that of the implementation of the abstraction. The TypeSetter system selects a consistent and efficient set of implementations for a program's abstractions based on the collected profile data.	advanced audio coding;compiler	A. Dain Samples	1992		10.1007/3-540-55984-1_9	computer architecture;parallel computing;compiler correctness;computer science;programming language;software system	Robotics	-27.745278186866212	28.84580520393869	52879
e43449f54b927c2055e1caee9863b804328728cd	generation of incremental indirect threaded code for language-based programming environments	programming language;programming environment;code generation;attribute grammar	We present an approach to generating incremental threaded code for language-based programming environments from the specification of the runtime semantics of the programming language. Language-based environments (LBEs) that support incremental code generation have usually done so using ad hoc techniques for incremental recompilation. Our aim is to provide one uniform operational model based on attribute grammars that allows the specification of the runtime semantics, and thus code generation, to be incorporated with the specification of the syntax and static semantics of the language.	threaded code	Khalid Azim Mughal	1988		10.1007/3-540-51364-7_18	natural language processing;intentional programming;first-generation programming language;computer architecture;declarative programming;very high-level programming language;programming domain;reactive programming;functional reactive programming;computer science;programming language implementation;extensible programming;third-generation programming language;functional logic programming;computer programming;programming paradigm;procedural programming;symbolic programming;inductive programming;fifth-generation programming language;programming language;attribute grammar;second-generation programming language;high-level programming language;code generation	HPC	-26.844845928460764	27.022350028783755	53017
53b5f1a272539e3ceb78b087f66fb2606067f0df	personal visualization system: applications in research and engineering	dec computers;ibm computers;add-on boards;computer graphic equipment;computer graphics;engineering workstations;geophysics computing;microcomputer applications;personal computing;physics computing;satellite computers;space vehicles;superconductivity;100 mflops;pc-based software;pip plug in cards;quen array processor;graphic workstations;high-resolution displays;personal visualization system;program development time;supercomputer computation rates	This paper describes an innovative personal visualization system and its application to several research and engineering problems. The system bridges both hardware and software components to permit a user to graphically describe a visualization problem to the computer; thereby reducing program development time to a few hours. Low-cost visualization is achieved using PC-based software that can either be executed on the PC or drive graphic workstations for high resolution displays. In either case, supercomputer computation rates are made available to the visualization process. On PC's this is done with one or more PiP plug in cards, each of which is capable of 100 million floating point operations per second. On workstations this is done with the QUEN#8482; (trademark of Interstate Electronics Corporation, Anaheim, California) array processor.	array processing;component-based software engineering;computation;flops;supercomputer;vector processor;workstation;pip	Quentin E. Dolecek;K. Moorjani;B. F. Kim;D. G. Tilley;Thomas S. Denney	1990			software visualization;parallel coordinates;visualization;image resolution;interactive visualization;visual system;statistical graphics;computer hardware;computer science;floating point;component-based software engineering;operating system;computer graphics;superconductivity;statistics;computer graphics (images)	HPC	-32.14862535941221	28.37485505273113	53031
698f11f9a01162c4b75fe14de8fab6d315b53e63	macros for interaction nets: a conservative extension of interaction nets	programming language;macro systems;pattern matching;interaction nets;programming languages	We propose a conservative extension of interaction nets which offers enriched patternmatching facilities. The extension is conservative in the sense that it can be implemented inside standard interaction nets, and thus can be seen as a system of macros. Consequently, we are guaranteed to keep all the good properties of interaction nets, in particular strong confluence. We see this extension as a crucial step towards using interaction nets as a programming language, which remains a relatively unexplored area. One significant feature of the extension presented here is that, in contrast to other extensions presented previously, we essentially follow the syntax and spirit of interaction nets, and moreover the extension lives at the same level.	apl;confluence (abstract rewriting);deadlock;graph rewriting;interaction technique;lambda calculus;pattern matching;petri net;programmer;programming language;type system	François-Régis Sinot;Ian Mackie	2005	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2005.02.016	computer science;theoretical computer science;pattern matching;programming language;algorithm	PL	-26.670682826860485	26.07290472875119	53086
9145aef000dbdfdd511464f92d47f5af4fff43ea	asap - a simple assertion pre-processor	programmation technique;lenguaje programacion;software engineering tools;formal specification;tecnica programacion;langage c;programming language;implementation;aplicacion cliente servidor;software systems;ingenieria logiciel;software engineering;assertion;ejecucion;c language;pre processor;programming techniques;technique programmation;design and implementation;assertions;application client serveur;genie logiciel;langage programmation;software contract;lenguaje c;client server application	Assertions are widely known as a powerful tool to detect software faults during the debugging of software systems. Despite the maturity of software engineering tools, assertions are seldom used in practice. ASAP is a pre-processor for C programs which implements several concepts defmed in the theory of formal specification, such as preconditions, postconditions, assertions related to intermediate states, loop invariants and variants, existential and universal quantifiers. In this paper, the notion of software contract is introduced, and shown how it is applicable with assertions. Finally, a collection of useful examples is given, and ASAP design and implementation is described.	asynchronous array of simple processors;capability maturity model;debugging;design by contract;formal specification;loop invariant;postcondition;precondition;software engineering;software system;universal quantification	Igor D. D. Curcio	1998	SIGPLAN Notices	10.1145/307824.307859	postcondition;assertion;computer science;design by contract;operating system;formal specification;database;programming language;implementation;client–server model;software system	SE	-23.936117129828048	30.357512156739368	53141
f6ffd779011076151d09fa6606334d2c42882f28	optimizing the distributed evaluation of stratified datalog programs via structural analysis			datalog;optimizing compiler;structural analysis	Rosamaria Barilaro;Nicola Leone;Francesco Ricca;Giorgio Terracina	2011			programming language;datalog;database;computer science	DB	-23.246868848329964	20.591821565330825	53327
581d12f4885eaa8c47661970a2f433e692ca2262	simulation category languages-a ddp example	distributed data;front end;technical report;high level language	General simulation languages can be used to express any simulation situation. Often, however, the expressions are bulky and cumbersome because of the host language's need for generality. High-level languages for specific situations or problem areas, called “Category Languages”, can greatly improve users' access to simulation computing power and can quite easily and justifiably be implemented as “front-end” pre-processors to ordinary general simulation languages.  In this paper, definitions and motivations for Category Language Methodology are presented, with a discussion of applications areas, implementation methods, and an example of the Category Language Methodology drawn from Distributed Data Processing.	central processing unit;challenge-handshake authentication protocol;computer simulation;data-directed programming;david gries;distributed computing;esp game;experiment;john d. wiley;simula;simulation	P. Nick Lawrence	1977			natural language processing;simulation;computer science;technical report;theoretical computer science;front and back ends;world wide web;second-generation programming language;high-level programming language;simulation language	DB	-28.338528752459446	24.736200068729282	53398
1dc2a6fc93de72bfc6ecc1bcf584c0082a0c4410	applying low-level query optimization techniques by rewriting	optimisation;rewrite rule;optimizacion;query processing;interrogation base donnee;interrogacion base datos;query optimization;object oriented;reecriture;estructura datos;oriente objet;optimization;structure donnee;rewriting;static analysis;orientado objeto;data structure;database query;reescritura	In the paper we focus on those query optimization techniques that concern low-level mechanisms and data structures used in query processing. In this setting we discuss how such techniques can be applied at a textual level, in other words, how they can be used as rewriting rules. A number of such methods are considered, among others, so-called direct navigation and a widely known technique - indices.#R##N##R##N#Our rewriting rules are defined in an algorithmic manner, that is, they are associated with code. An important part of such algorithms is a special phase - static analysis - which gathers all the information needed to decide whether a given method can be applied and how.	query optimization;rewriting	Jacek Plodzien;Kazimierz Subieta	2001		10.1007/3-540-44759-8_84	query optimization;data structure;rewriting;computer science;theoretical computer science;database;programming language;object-oriented programming;static analysis;algorithm	DB	-19.16940083936261	24.52125100947105	53502
9f4ed7fdff41a9c8320fc815adaec155e0782146	an sgml-based programming environment for literate programming.	programming environment;literate programming	"""8 it to compile it afterwards. However, programs are text, so they can beneet from recent results in document research. We propose a new underlying structure for literate programs, based on SGML. As a result , we will use SGML tools to view and modify literate programs. These tools will isolate programmers from the low level structure of literate programs. Moreover, literate programming can be easily extended to use other types of information such as Z formal speciications, and in fact, literate programs could include any type of digital information.terloo. His current interests are in the area of software engineering, in particular software development environments and the application of text databases research to software repositories. 7 Z fragments do not include other fragments. Therefore, it is only necessary to extract all Z fragments and reorder them according to their numeric tag. During both \tangling"""" processes all macro usages are expanded and all SGML tags are removed. The nal result is a text le that can be processed by a compiler or checker. 3.4 Editing SGML literate programs ASCII text with embedded SGML is hard to read, and thus editing will be a cumbersome and unpleasant task. Remembering the speciic tag set for segments of text or code, and placing the tags to ensure a syntactically correct document will also create problems. In addition , if the tagged document is edited with a text editor, then all the visualization information that can be generated from the tags will not be present. Syntax-directed editors such as Rita 2], the SoftQuad Author/Editor 12] and editors produced by the Cornell Synthesizer 9] could be used to address some of these issues. For example, Rita uses a form of DTD and style sheets to support editing of SGML tagged text. At any point in the document this editor will prompt the user with a list of syntactically acceptable tags. The tags are also separated from the text so the program looks quite \natu-ral"""". In addition, the style-sheet can be used to maintain much of the visualization information which is critical to the understanding of the literate program. Another approach to editing is used by the Waterloo Database Browser 16]. It exports a section of an SGML document to a WordPerfect 2 le, and then formats the output text according to a WordPerfect style-sheet for the document. Then, the user uses the word processor as an …"""	compiler;database;embedded system;integrated development environment;level structure;literate programming;programmer;software development;software engineering;software repository;standard generalized markup language;structure editor;style sheet (web development);text editor;wordperfect	Daniel M. Germán	1994		10.1145/782232	natural language processing;intentional programming;first-generation programming language;declarative programming;very high-level programming language;programming domain;computer science;extensible programming;functional logic programming;multimedia;programming paradigm;symbolic programming;inductive programming;fifth-generation programming language;programming language	SE	-28.809030129380663	25.95622411924685	53996
7a53e0a227f31200719a9afccbcdc2e9f2ce85be	specifying and automatically generating ada tasks in prolog	formal specification;executable specification;automatic generation;definite clause grammar;concurrent programs;formal grammar;source code	Many concurrent programs may be specified by formal grammars that define the sequences of events that may occur in the program. We outline the use of Prolog definite clause grammars to specify and model the behavior of Ada server tasks in particular. We describe Prolog programs that translate Ada tasks into the grammars that specify them and that translate from specifying grammars into good-quality Ada code. These programs may be used to generate Ada tasking code automatically from specifications and to compare Ada code with its formal specification. The grammars themselves may be executed as Prolog programs to simulate the concurrent program they specify.	ada;concurrent computing;definite clause grammar;formal grammar;formal specification;prolog;server (computing);simulation	John Van Tassel;David Hemmendinger	1990		10.1145/100348.100367	grammar systems theory;natural language processing;operator-precedence grammar;regular grammar;computer science;affix grammar;formal specification;formal grammar;definite clause grammar;programming language;attribute grammar;unrestricted grammar;adaptive grammar;algorithm;source code	PL	-23.271182125651872	26.70464822663473	54013
4857ff6dd0aba0b2ff33575978d9f7438376fe5b	an introduction to context-oriented programming with contexts	programming environment;articulo;context oriented programming;smalltalk;an introduction to context oriented programming with contexts	Context-oriented Programming, or COP, provides programmers with dedicated abstractions and mechanisms to concisely represent behavioral variations that depend on execution context. By treating context explicitly, and by directly supporting dynamic composition, COP allows programmers to better express software entities that adapt their behavior late-bound at run-time. Our paper illustrates COP constructs, their application, and their implementation by developing a sample scenario, usingContextS in theSqueak/Smalltalkprogrammingenvironment.	binary expression tree;c object processor;connascence (computer programming);cross-cutting concern;dave thomas (programmer);entity;jan hoogervorst;microsoft outlook for mac;name binding;printing;programmer;smalltalk;squeak	Robert Hirschfeld;Pascal Costanza;Michael Haupt	2007		10.1007/978-3-540-88643-3_9	real-time computing;simulation;programming domain;reactive programming;functional reactive programming;computer science;extensible programming;functional logic programming;programming paradigm;procedural programming;symbolic programming;inductive programming;programming language	HCI	-27.63156306696137	29.261752099477405	54084
28766f84cd9731d165f3ea322614e5082f77fa7d	uas cloud surveillance system	databases;google;internet access;surveillance;sql;ce 71 uav operation uas cloud surveillance system 3g mobile communication uav flight data internet android smart phone data acquisition google earth 3d map ground computer mysql database management downlink data attitude display mode altitude display mode uav dynamic performance flight awareness;earth;smart phones;surveillance 3g mobile communication aerospace computing autonomous aerial vehicles cloud computing data acquisition smart phones sql;android smart phone;mobile 3g;3g mobile communication;aerospace computing;uav uas;surveillance cloud computing smart phones databases google earth;ground station;cloud system;internet access cloud system android smart phone uav uas mobile 3g ground station;data acquisition;autonomous aerial vehicles;cloud computing	On UAV missions, flight information should be prompt for use to all participating team members. Supporting from 3G mobile communications, UAV flight data can be uplink onto Internet and share with all users at different locations. Using an Android smart phone for data acquisition on airborne and cope with Google Earth® 3D map, the proposed UAS surveillance system is constructed on the cloud. The ground computer offers My SQL database management for all downlink data and converts into user friendly format for easy access. With special attitude and altitude display modes to match with UAV dynamic performance, it offers very good flight awareness to operator and observers throughout mission. This paper presents a technical realization on the UAS cloud surveillance system for Ce-71 UAV operations with excellent sense in surveillance.	accessibility;airborne ranger;android;computer display standard;data acquisition;database;google earth;mysql;smartphone;telecommunications link;usb attached scsi;unmanned aerial vehicle;usability	Chin E. Lin;Cheng-Ru Li;Ya-Hsien Lai	2012	2012 41st International Conference on Parallel Processing Workshops	10.1109/ICPPW.2012.25	embedded system;sql;simulation;internet access;cloud computing;computer science;operating system;earth;data acquisition;computer security	Robotics	-33.51517512709038	22.40872730192393	54114
498ff90afe4299c3a368e12f67e434ebf6642903	the programming languages rec and convert	declarative programming;programming language;procedure programming;prolog;control structure;translation;pattern matching;c;regular expression	Two symbol manipulation languages are presented. REC (an acronym for Regular Expression Compiler) is a very compact language possessing a simple control structure. CONVERT is a pattern matching and substitution language well suited to problems whose solution may conveniently be expressed in terms of transformation rules. REC is useful when conciseness is required. such as for microcomputers with limited memory, interactive programming via the keyboard, and so on. Its present form was carefully chosen to facilitate the compilation of CONVERT programs while still preserving a universal appearance. In turn, CONVERT is a natural alternative to consider for an application which is already expressed in terms of transformation rules, as are many compilers, assemblers and symbol manipulation systems. This article primarily describes the appearance of these two languages, but some of the applications which they have been given are mentioned.	ampersand;assembly language;compiler;control flow;interactive programming;microcomputer;pattern matching;programming language;regular expression	Harold V. McIntosh;Gerardo Cisneros	1990	SIGPLAN Notices	10.1145/382076.382648	translation;fourth-generation programming language;first-generation programming language;declarative programming;very high-level programming language;symbol;programming domain;computer science;programming language generations;theoretical computer science;third-generation programming language;functional logic programming;pattern matching;programming paradigm;procedural programming;inductive programming;fifth-generation programming language;programming language;control flow;prolog;second-generation programming language;comparison of multi-paradigm programming languages;regular expression;algorithm	PL	-24.828471754896366	23.682485331417507	54301
ee46bfb267eb7213a1231442c15dfd686bf7db44	object-oriented description of graph data structures	object oriented;data structure	A solution method for problems of processing graph data structures is presented. This method is based on the use of the specialized language TreeDL and on its extendable compiler. The capabilities of the language and of the compiler exceed the existing solutions, which makes the proposed method more efficient than its analogs.	compiler;data structure;extensibility;graph (abstract data type);treedl	Alexey V. Demakov	2007	Programming and Computer Software	10.1134/S0361768807050027	compiler;data structure;compiler correctness;computer science;theoretical computer science;database;programming language;object-oriented programming;functional compiler	PL	-23.986660134195002	25.06143882550338	55585
21efb21bac66081c8c83417415498189643d48a5	towards a higher-order synchronous data-flow language	stream functions;programming language;dynamic reconfiguration;formal semantics;synchronous data flow;functional programming;higher order;first order;synchronous data flow programming language;polymorphism;data flow;communication channels;stream function;kahn processes;type system	The paper introduces a higher-order synchronous data-flow language in which communication channels may themselves transport programs. This provides a mean to dynamically reconfigure data-flow processes. The language comes as a natural and strict extension of both lustre and lucy. This extension is conservative, in the sense that a first-order restriction of the language can receive the same semantics.We illustrate the expressivity of the language with some examples, before giving the formal semantics of the underlying calculus. The language is equipped with a polymorphic type system allowing types to be automatically inferred and a clock calculus rejecting programs for which synchronous execution cannot be statically guaranteed. To our knowledge, this is the first higher-order synchronous data-flow language where stream functions are first class citizens.	curry–howard correspondence;dataflow;expressive power (computer science);first-class function;first-order predicate;list of to heart series characters;lustre (programming language);semantics (computer science);synchronous data flow;type system	Jean-Louis Colaço;Alain Girault;Grégoire Hamon;Marc Pouzet	2004		10.1145/1017753.1017792	data flow diagram;polymorphism;real-time computing;higher-order logic;language primitive;type system;object language;specification language;data control language;type safety;computer science;theoretical computer science;formal semantics;first-order logic;channel;low-level programming language;programming language;functional programming;stream function;programming language specification;high-level programming language;algorithm;channel	Embedded	-26.874638585989597	31.723109926712382	55654
273a5d6cf01ea32f9b9580f123fd9e969e29914e	hospital rfid-based patient u-healthcare design over wireless medical sensor network		Ubiquitous sensor network is drawing a lot of attention as a method for realizing a ubiquitous society. It collects environmental information to realize a variety of functions, through a countless number of compact wireless nodes that are located everywhere to form an ad hoc arrangement, which does not require a communication infrastructure. However, there isn’t any flexible and robust communication infrastructure to integrate these devices into an emergency care setting. An efficient wireless communication substrate for medical devices that addresses ad hoc or fixed network for information, naming and discovery, transmission efficiency of the data, data security and authentication, as well as filtration and aggregation of vital sign data need to be studied. We propose a Hospital RFID-based Patient u-Healthcare Management System architecture that possesses the essential elements of each of the future medical applications are integration with existing medical practices and technology, real-time, long-term, remote monitoring, miniature, wearable sensors to assist the elderly and chronic patients, and to amplify the transmission of data by installing routers in the hospital.		Randy S. Tolentino;Sungwon Park	2010		10.1007/978-3-642-17622-7_31	emergency medicine;medical emergency;computer network	Mobile	-32.6056429230672	22.316914267901364	55710
1de01c9c38cacb5587bbde1c8f8eed3207c90939	incremental encoding of multiple inheritance hierarchies supporting lattice operations.	multiple inheritance		incremental encoding;multiple inheritance	Martin F. van Bommel;T. J. Beck	2000	Electron. Trans. Artif. Intell.		multiple inheritance;computer science;theoretical computer science;database;algorithm	HCI	-22.979702401809252	20.314237299919945	55718
e678b1a1899b65b91a825ecb708f5fa98188cf06	a rule-based and object-oriented ai programming language	lenguaje programacion;sistema experto;syntax;architecture systeme;langage c;programming language;implementation;regle production;rule based;semantics;automaton;intelligence artificielle;ingenieria logiciel;object oriented programming;syntaxe;semantica;semantique;software engineering;artificial intelligent;ejecucion;automata;c language;object oriented;object oriented programming languages;automate;inferencia;genie logiciel;langage programmation;oriente objet;artificial intelligence;arquitectura sistema;inteligencia artificial;systeme expert;logic programs;sintaxis;system architecture;orientado objeto;inference;production rule;lenguaje c;regla produccion;expert system	This paper presents a new artificial intelligence programming language called ROOP which is machine independent and has the ability to deal with logical programming in general. ROOP is a multiple paradigm language which combines rule-based, procedure, logic and object-oriented programming techniques.	algorithm;artificial intelligence;artificial neural network;computer science;declaration (computer programming);definition;experiment;expert system;heuristic;inference engine;logic programming;mycin;neural networks;ops5;programmer;programming language;programming paradigm;roop (programming language);reference implementation;rule-based system;visual prolog	Tao Li	1995	SIGPLAN Notices	10.1145/219726.219736	greenspun's tenth rule;first-generation programming language;declarative programming;very high-level programming language;language primitive;programming domain;reactive programming;functional reactive programming;computer science;programming language implementation;artificial intelligence;extensible programming;functional logic programming;semantics;automaton;programming paradigm;procedural programming;symbolic programming;low-level programming language;inductive programming;fifth-generation programming language;programming language;object-oriented programming;logic programming;programming language specification;high-level programming language;expert system;algorithm	AI	-25.133983908274633	21.176272342146984	55894
1dcd1964f8971df94a8ea38d1ff1424ea9056c08	pei: a simple unifying model to design parallel programs	transformations;unified model;parallel programmming;parallel programs;multisets	Abstract   Many ways have been explored nowadays about parallel programming. Derivation by refinement or transformations according to some computation models are main examples. This justifies to introduce a  unifying theory  which supposes an abstraction of the different concepts. Our theory is founded on the simple concept of  multiset , represented as a  data field  [7–10].		Eric Violard;Guy-René Perrin	1994	Future Generation Comp. Syst.	10.1016/0167-739X(94)90027-2	transformation;theoretical computer science;unified model;algorithm	Arch	-21.734751967506828	21.872363062762805	55979
d99c91864662c64ebe489d27bd692560bb6ef449	java consistency: nonoperational characterizations for java memory behavior	verification;computacion informatica;etude theorique;computer storage;java language specification;design aids;storage structure;sistema informatico;grupo de excelencia;java memory model;computer system;specification programme;abstract machine;consistency model;ciencias basicas y experimentales;analyse performance;compiler optimization;performance analysis;estudio teorico;nonoperational specification;systeme informatique;estructura memoria;structure memoire;aide conception;memoria ordinador;verificacion;theoretical study;program specification;especificacion programa;java memory models;multithreading;analisis eficacia;memoire ordinateur	The Java Language Specification (JLS) [Gosling et al. 1996] provides an operational definition for the consistency of shared variables. The definition remains unchanged in the JLS 2nd edition, currently under peer review, which relies on a specific abstract machine as its underlying model, is very complicated. Several subsequent works have tried to simplify and formalize it. However, these revised definitions are also operational, and thus have failed to highlight the intuition behind the original specification. In this work we provide a complete nonoperational specification for Java and for the JVM, excluding synchronized operations. We provide a simpler definition, in which we clearly distinguish the consistency model that is promised to the programmer from that which should be implemented in the JVM. This distinction, which was implicit in the original definition, is crucial for building the JVM. We find that the programmer model is strictly weaker than that of the JVM, and precisely define their discrepancy. Moreover, our definition is independent of any specific (or even abstract) machine, and can thus be used to verify JVM implementations and compiler optimizations on any platform. Finally, we show the precise range of consistency relaxations obtainable for the Java memory model when a certain compiler optimization— called prescient stores in JLS—is applicable.	abstract machine;algorithm characterizations;bendix g-15;computer data storage;constraint logic programming;discrepancy function;embedded system;formal verification;gosling emacs;java memory model;lock (computer science);mathematical optimization;memory model (programming);operational definition;optimizing compiler;programmer;sim lock;shared variables;thread (computing);transistor;verification and validation;virtual machine	Alex Gontmakher;Assaf Schuster	2000	ACM Trans. Comput. Syst.	10.1145/362670.362673	real-time computing;verification;multithreading;computer science;consistency model;operating system;java modeling language;computer data storage;optimizing compiler;abstract machine;programming language;java memory model;algorithm	PL	-23.57874464052369	29.88445731061488	56408
d8298e43f56a93208e0891153798188c3f7c33b9	transpiling programmable computable functions to answer set programs		Programming Computable Functions (PCF) is a simplified programming language which provides the theoretical basis of modern functional programming languages. Answer set programming (ASP) is a programming paradigm focused on solving search problems. In this paper we provide a translation from PCF to ASP. Using this translation it becomes possible to specify search problems using PCF.	answer set programming;computable function;functional programming;modeling language;programming computable functions;programming language;programming paradigm;search problem;source-to-source compiler;stable model semantics	Ingmar Dasseville;Marc Denecker	2018	CoRR		programming language;programming computable functions;theoretical computer science;computer science;functional programming;programming paradigm;answer set programming	AI	-22.457141766135653	22.462447983428852	56658
1d90e7fadaf8530bc63eec8a9d010413c5339a12	tabling for higher-order logic programming	teoria demonstracion;theorie preuve;automatic proving;proof theory;redundancia;intelligence artificielle;demostracion automatica;logical programming;higher order;theorem proving;demonstration automatique;refinement method;demonstration theoreme;redundancy;type checking;interpreteur;programmation logique;indexing;design and implementation;indexation;indizacion;artificial intelligence;inteligencia artificial;interpreter;logic programs;methode raffinement;demostracion teorema;programacion logica;metodo afinamiento;interprete;higher order logic;deduccion;redondance;deduction	We describe the design and implementation of a higher-order tabled logic programming interpreter where some redundant and infinite computation is eliminated by memoizing sub-computation and re-using its result later. In particular, we focus on the table design and table access in the higher-order setting where many common operations are undecidable in general. To achieve a space and time efficient implementation, we rely on substitution factoring and higher-order substitution tree indexing. Experimental results from a wide range of examples (propositional theorem proving, parsing, refinement type checking, small-step evaluator) demonstrate that higher-order tabled logic programming yields a more robust and more efficient proof procedure.	admissible numbering;automated theorem proving;computation;first-order predicate;garbage collection (computer science);integer factorization;interpreter (computing);logic programming;memoization;parsing;propositional calculus;refinement (computing);scheduling (computing);software testing;strongly connected component;test case;twelf;type system;undecidable problem;warren abstract machine	Brigitte Pientka	2005		10.1007/11532231_5	higher-order logic;computer science;artificial intelligence;theoretical computer science;mathematics;programming language;algorithm	PL	-19.316661962596392	22.703849503572155	56714
1017a836ed1acff3e716b922df9637a5e50f1ce7	automated protocol validation in argos: assertion proving and scatter searching	virtual memory;command language;systeme unix;scatter search;protocole transmission;langage c;unix system;protocol design;langage argos;data communication;quarded command;protocolo transmision;c language;symbolic execution;protocol validation;control flow;memoire virtuelle;commande gardee;analizador sintaxico;validation;guarded command languages;parser;symbolic execution assertion proving guarded command languages protocol design protocol validation scatter searching;machine etat fini;scattering performance analysis logic access protocols data communication automatic control communication system control automata command languages humans;scatter searching;analyseur syntaxique;communicating finite state machine;finite state machine;assertion proving;memoria virtual;lenguaje c;transmission protocol	Argos is a validation language for data communication protocols. To validate a protocol, a model in Argos is constructed consisting of a control flow specification and a formal description of the correctness requirements. This model can be compiled into a minimized lower level description that is based on a formal model of communicating finite state machines. An automated protocol validator trace uses these minimized descriptions to perform a partial symbolic execution of the protocol to establish its correctness for the given requirements.	communicating finite-state machine;compiler;control flow;correctness (computer science);mathematical model;requirement;symbolic execution;validator	Gerard J. Holzmann	1987	IEEE Transactions on Software Engineering	10.1109/TSE.1987.233206	computer science;virtual memory;theoretical computer science;operating system;finite-state machine;programming language;control flow;algorithm	SE	-31.577381146309243	32.073359225455825	56896
34e46707c44de1c37741a1ab30621247b8078313	the design of air and its application to ada separate compilation	separate compilation	AIR, the Ada Intermediate Representat ion in the York Ada Workbench compi le r [ t . 2 . 3 . 4 ] . is the form of an Aria p rogram between the front and back ends of the Ada compi le r , and is also used by other tools which work on Ada programs. AIR was des igned pr imar i ly by Colin Runciman and Chris Johnson in the spr ing of 1981 and rep laced the ear l ie r in termediate representat ion cal led WIT [Walker Intermediate Tree) des igned by lan Walker. The d i f ference between the two was that AIR was des igned to represent p rograms written in revised (1980) Ada [5]; WIT was based on pre l iminary (1979) Aria [6]. Since the init ial des ign, parts of the AIR have been modi f ied to cor respond with the more recent Ada standard [ 7 ] ,	ada;amiga walker;aria;front and back ends;init;workbench	Jim S. Briggs	1983		10.1007/3-540-13878-1_6	computer science;programming language	ML	-25.327903380201295	20.300787945572072	57090
5f8fc3b810cde4366e5750fb9a99e355843b3dbd	parallel devs: a parallel, hierarchical, modular, modeling formalism	parallel devs;discrete event simulation;stochastic approximation;parallel programming;formal specification;optimization	We present a revision of the hierarchical, modular Discrete Event System Specification (DEVS) modeling formalism. The revision distinguishes between transition collisions and ordinary external events in the external transition function of DEVS models. Such separation enables us to extend the modeling capability of the collisions. The revision also does away with the necessity for tie-breaking of simultaneously scheduled events, as embodied in the select function. The latter is replaced by a well-defined and consistent formal construct that allows all transitions to be simultaneously activated. The revision provides a modeler wit h both conceptual and parallel-execution benefits.	devs;formal system;modular programming;semantics (computer science)	Alex Chung Hen Chow;Bernard P. Zeigler	1994			stochastic approximation;parallel computing;real-time computing;discrete event dynamic system;computer science;technical report;theoretical computer science;discrete event simulation;devs;formal specification;sp-devs	Graphics	-27.398775408473472	32.20029960034027	57340
0873c26b9819900e677ab021ba27e1cb9b02c623	using decision procedures to accelerate domain-specific deductive synthesis systems	structure programme;heuristic programming;program design;satisfiabilite;program transformation;conception programme;logical programming;transformation programme;satisfiability;theorem prover;transformacion programa;estructura programa;decision procedure;programmation logique;programmation heuristique;programacion logica;program structure;domain specificity;concepcion programa	This paper describes a class of decision procedures that we have found useful for efficient, domain-specific deductive synthesis, and a method for integrating this type of procedure into a general-purpose refutation-based theorem prover. We suggest that this is a large and interesting class of procedures and show how to integrate these procedures to accelerate a general-purpose theorem prover doing deductive synthesis. While much existing research on decision procedures has been either in isolation or in the context of interfacing procedures to non-refutation-based theorem provers, this appears to be the first reported work on decision procedures in the context of refutationbased deductive synthesis where witnesses must be found.	automated theorem proving;computation;computational fluid dynamics;correctness (computer science);decision problem;deductive database;domain theory;general-purpose markup language;general-purpose modeling;literal (mathematical logic);norm (social);software design;theory (mathematical logic);tom;universal instantiation	Jeffrey Van Baalen;Steve Roach	1998		10.1007/3-540-48958-4_4	computer science;artificial intelligence;program design language;mathematics;automated theorem proving;programming language;algorithm;satisfiability	AI	-19.131877747114512	20.825691733626474	57404
339694fd7356a4aaf46c05bafb275edd3ce496a4	over-approximated control flow graph construction on pure esterel	data flow;control flow graph;graph theory;reactive system	Esterel is an imperative synchronous language for controldominant reactive systems. Regardless of imperative features of Esterel, combination of parallel execution and preemption makes it difficult to build control flow graphs (CFGs) of Esterel programs. Simple and convenient CFGs can help to analyze Esterel programs. However, previous researches are not suitable for flow analyses of imperative languages. In this work, we present a method to construct over-approximated CFGs for Pure Esterel. Generated CFGs expose invisible interferences among threads and show program structures explicitly so that they are useful for program analyses based on graph theory or control-/dataflows. key words: esterel, control flow graph, synchronous language	approximation algorithm;context-free grammar;control flow graph;esterel;graph theory;imperative programming;preemption (computing)	Chul-Joo Kim;Jeong-Han Yun;Seonggun Kim;Kwang-Moo Choe;Taisook Han	2010	IEICE Transactions		data flow diagram;parallel computing;real-time computing;reactive system;computer science;graph theory;programming language;control flow graph	PL	-20.953217463516097	31.223027102419255	57568
4fdcb6232ea735ef9bddbe76c96cf47857f4b313	experiences with combining formalisms in vvsl	formal specification;temporal logic;formal specification language;specification language	This paper primarily reports on semantic aspects of how a formal specification of the PCTE interfaces has been achieved in a situation where only a combination of existing formalisms could meet the needs. The motivations for combining a VDM specification language with a language of temporal logic, for translating the resulting language, called VVSL, to an extended COLD-K and for translating it also (partially) to the language of the logic MPLω are briefly outlined. The main experiences from this work on combination and transformation of formalisms are presented. Some important experiences with the application of VVSL to the formal specification of the PCTE interfaces and otherwise are also mentioned.	formal specification;specification language;temporal logic;vienna development method	Kees Middelburg	1989		10.1007/3-540-53912-3_19	natural language processing;formal system;formal methods;object language;specification language;formal verification;z notation;formal specification;refinement;programming language;programming language specification;algorithm;language of temporal ordering specification	HCI	-25.91244266939087	19.992118484741237	57774
07a2c29a76bbc706fc2c415a73acaaa0eed25b89	parameterized models for on-line and off-line use	science general;constraint modelling;gecode;embedded dsl;monads;staged compilation;constraint programming;model;monadic constraint programming;haskell	The Monadic Constraint Programming framework leverages Haskell’s rich static type system and powerful abstraction mechanisms to implement an embedded domain specific language (EDSL) for constraint programming. In this paper we show how the same constraint model expressed in the EDSL can be processed in various modes by external constraint solvers. We distinguish between on-line and off-line use of solvers. In off-line mode, the model is not solved; instead it is compiled to lower-level code that will search for solutions when compiled and run. For on-line use, the search can be handled by either the framework or in the external solver. Off-line mode requires recompilation after each change to the model. To avoid repeated recompilation, we separate model from data by means of parameters that need not be known at compile time. Parametrization poses several challenges, which we resolve by embedding the EDSL more deeply.	benchmark (computing);c++;compile time;compiler;constraint programming;domain-specific language;embedded system;gecode;haskell;line code;lisp;online and offline;solver;type system	Pieter Wuille;Tom Schrijvers	2010		10.1007/978-3-642-20775-4_6	constraint logic programming;concurrent constraint logic programming;constraint programming;constraint satisfaction;computer science;theoretical computer science;programming language;monad;algorithm	PL	-19.514157992843984	30.17758164227461	57791
36be34362aa1e95f546549336821bad4c5da0a0d	\lambda λ to ski, semantically - declarative pearl		We present a technique for compiling lambda-calculus expressions into SKI combinators. Unlike the well-known bracket abstraction based on (syntactic) term re-writing, our algorithm relies on a specially chosen, compositional semantic model of generally open lambda terms. The meaning of a closed lambda term is the corresponding SKI combination. For simply-typed as well as unityped terms, the meaning derivation mirrors the typing derivation. One may also view the algorithm as an algebra, or a non-standard evaluator for lambda-terms (i.e., denotational semantics).	declarative programming	Oleg Kiselyov	2018		10.1007/978-3-319-90686-7_3	computer science;algorithm;semantic data model;bracket;combinatory logic;denotational semantics;derivation;syntax;expression (mathematics);abstraction	NLP	-23.02000944205864	22.172265124731727	57821
494110f72ac432c12d716be98b08c268f15f0cc2	interactive functional objects in clean	part of book or chapter of book;functional programming language	The functional programming language Clean has a high level I/O system (version 0.8) in which complex yet eecient interactive programs can be created. In this paper we present its successor (version 1.0), the object I/O system. We consider some of the design considerations that have innuenced the design of the new I/O system greatly. Key issues are compositionality, orthogonality, and extensibility. Apart from design, the object I/O system improves on its predecessor by two major contributions: programmers can introduce polymorphic local state at every (collection of) user interface component(s) and programmers can create interactive processes in a exible way. All interface components can communicate with each other by sharing state but also using powerful message passing primitives in both synchronous, asynchronous, and uni-or bi-directional way. Using the message passing mechanism remote procedure calling can be added easily. The result is an object oriented I/O system. As in the previous system the uniqueness type system of Clean, ooering the possibility to do destructive updates in a pure functional framework, plays a crucial role. Furthermore, the object I/O system makes extensive use of new type system facilities, namely type constructor classes and existential types.	clean;extensibility;functional programming;high-level programming language;input/output;local variable;message passing;programmer;remote procedure call;type constructor;type system;uniqueness type;user interface	Peter Achten;Marinus J. Plasmeijer	1997		10.1007/BFb0055438	natural language processing;computer science;programming language;functional programming;algorithm	PL	-26.323900336325337	26.7842636592219	57922
91a1ad2fbf4d652d243ef0c74248b3b2269786a7	incremental design of statechart specifications	design process;statecharts;refinement;satisfiability;μ charts;reactive system;reactive systems	We present a Statecharts dialect with only three syntactic constructs and a semantics that is not restricted to describe reactive systems on an implementation level but allows to model them on an abstract, more speci cation oriented stage, where design alternatives are still left open. We give a re nement calculus with rules that tell the designer how to come from the abstract speci cation to the implementation such that the system under development only becomes more concrete but not more abstract; under-speci cation is eliminated by adding more information. The result of a design process that follows these rules is an implementation that satis es its speci cation by construction. c © 2001 Elsevier Science B.V. All rights reserved.	bernhard schölkopf;complex system;continuous design;correctness (computer science);critical section;denotational semantics;embedded system;hoc (programming language);model checking;semiconductor industry;smoothing;state diagram;systems design;systems engineering	Peter Scholz	2001	Sci. Comput. Program.	10.1016/S0167-6423(00)00026-5	reactive system;computer science;programming language;algorithm	Logic	-24.41270731228998	19.099788380102325	58000
0a01a8ba8d0c9ea1cb6ed084f8523bf80b24946d	verifying safety properties of concurrent heap-manipulating programs	verification;mise a jour;computacion informatica;analyse statique;algorithm analysis;securite;shape analysis;tipo dato;processus leger logiciel;program transformation;simultaneidad informatica;langage java;concurrent program;interpretacion abstracta;data type;transformation programme;safety properties;analisis estatica;actualizacion;proceso ligero logicial;transformacion programa;concurrency;analisis morfologico;ciencias basicas y experimentales;theory;safety;programa competidor;morphological analysis;thread software;property a;concurrent programs;analyse morphologique;deadlock;interbloqueo;invariante;lenguaje java;analyse algorithme;interblocage;interpretation abstraite;static analysis;abstract interpretation;type donnee;grupo a;seguridad;simultaneite informatique;languages;invariant;analisis algoritmo;updating;programme concurrent;java language;java	We provide a parametric framework for verifying safety properties of concurrent heap-manipulating programs. The framework combines thread-scheduling information with information about the shape of the heap. This leads to verification algorithms that are more precise than existing techniques. The framework also provides a precise shape-analysis algorithm for concurrent programs. In contrast to most existing verification techniques, we do not put a bound on the number of allocated objects. The framework produces interesting results even when analyzing programs with an unbounded number of threads. The framework is applied to successfully verify the following properties of a concurrent program:  —Concurrent manipulation of linked-list based ADT preserves the ADT datatype invariant.  —The program does not perform inconsistent updates due to interference.  —The program does not reach a deadlock.  —The program does not produce runtime errors due to illegal thread interactions.  We also found bugs in erroneous programs violating such properties. A prototype of our framework has been implemented and applied to small, but interesting, example programs.	algorithm;concurrent computing;deadlock;interaction;interference (communication);linked list;prototype;run time (program lifecycle phase);scheduling (computing);shape analysis (digital geometry);software bug;thread (computing);verification and validation	Eran Yahav;Shmuel Sagiv	2008	ACM Trans. Program. Lang. Syst.	10.1145/1745312.1745315	real-time computing;verification;concurrency;data type;morphological analysis;computer science;deadlock;invariant;shape analysis;programming language;java;static analysis;theory;algorithm	PL	-22.266258974521953	30.774280554770375	58135
150a641453f9579b5573258e5ee5c0d17448017b	a browser for incremental programming	incremental programming;computer software development;programming environment;bepress selected works;computer software development browsers computer programs smalltalk computer program language;intentional programming;smalltalk computer program language;method reachability;smalltalk;requires set;browsers computer programs;smalltalk browser	Much of the elegance and power of Smalltalk comes from its programming environment and tools. First introduced more than 20 years ago, the Smalltalk browser enables programmers to “home in” on particular methods using a hierarchy of manually-defined classifications. By its nature, this classification scheme says a lot about the desiredstate of the code, but little about the actualstate of the code as it is being developed. We have extended the Smalltalk browser with dynamically computed virtual categoriesthat dramatically improve the browser’s support for incremental programming. We illustrate these improvements by example, and describe the algorithms used to compute the virtual categories efficiently.	algorithm;business logic;cache (computing);categorization;communications protocol;computation;entity;integrated development environment;mathematical optimization;method overriding;object composition;point of view (computer hardware company);printing;programmer;real-time clock;real-time computing;requirement;smalltalk	Nathanael Schärli;Andrew P. Black	2004	Computer Languages, Systems & Structures	10.1016/j.cl.2003.09.004	self;intentional programming;higher-order programming;computer science;programming language implementation;database;programming language	PL	-28.579075236776678	26.723891313505447	58436
e31c06903342044606287a39cedeb5fab0f23962	a brief description of the protos-l system	abstract machine;logic programs;expert system	PROTOS-L ([2], [3]) is a logic programming language based on typed Horn clause logic. The type concept which has been derived from TEL ([9]) allows for subtypes as well as for parametric polymorphism. The types lead to better structured programs since they allow to make the data structure of a program explicit. They are exploited at compile time for static consistency checks, so that many programming errors can be detected early in the program development. The type information is also present at runtime through typed unification: Free variables can be constrained to subtypes without binding them to a particular value, thus offering a tool for saving time by reducing the amount of backtracking. Moreover, terms can be tested for subtype membership and types can be compared to each other w.r.t, the type hierarchy. In addition PROTOS-L offers a module concept with a threefold purpose: First, it provides a means for the structured development of large programs by supporting separate compilation of module interfaces and bodies. Furthermore, it provides a powerful means for the definition of abstract data types: At the level of interfaces the realization of types and operations on this type can be hidden. In this way, modules also provide both a structured and transparent database access. Through a special kind of module bodies called database bodies an external relational database can be accessed, where this access is transparent at the level of the interface. Moreover, the inference rules in database bodies are interpreted by a deductive database component, combining advantages of relational databases, like efficient set-oriented evaluation, with advantages of the logic programming paradigm, like high-level programming and recursion. Another highlight of PROTOS-L is the integration of an object-oriented interface to OSF/Motif. Based on this object oriented interface, high-level end-user interfaces can be developed within PROTOS-L. All described features together with an advanced set of built-ins like file handling, array manipulation, string operations, etc. are embedded into PROTOS-L in a completely type safe way.	abstract data type;backtracking;class hierarchy;compile time;compiler;data structure;deductive database;embedded system;graphical user interface;high- and low-level;high-level programming language;horn clause;logic programming;motif;parametric polymorphism;programming paradigm;recursion;relational database;run time (program lifecycle phase);string operations;taito l system;the european library;type safety;unification (computer science)	Christoph Beierle;Gregor Meyer;Heiner Semle	1991		10.1007/BFb0013547	legal expert system;computer science;theoretical computer science;programming language;algorithm	DB	-25.983426945438488	25.57827532431421	58457
3603e06de070e106dfd85613e2c6101b8ca84ca4	three patterns of data type composition in programming languages		Data type composition is used in programming languages to build complex data types (known as compound types) from simpler ones. Various programming languages of ten use common approaches to type composition, but their particular implementation and provided opportunities may differ between languages. The paper uses the patterns methodology to describe three recurring themes in data type composition: traversing a set of values (the Traversable Once pattern), providing a slot to assign a value (the Assignable Once pattern) and variables that do not have to contain a value (the Optional Value pattern). These patterns are general cases of a number of constructs used for building compound data types in various programming languages. The described patterns may be used by designers of programming languages, libraries and frameworks to support the corresponding data type composition mechanisms directly in the language or their replacement at a higher level. Users of languages, libraries and frameworks may use the description of the patterns to understand underlying mechanisms and related benefits and liabilities.		Ruslan Batdalov;Oksana Nikiforova	2018		10.1145/3282308.3282341	programming language;data type;software design pattern;computer science;complex data type	PL	-26.153023886161588	26.623179683257824	58544
b9eedc415dc19387315be09bdbb5687723e29911	direct implementation of abstract data types from abstract specifications	libraries;developpement logiciel;software;outil logiciel;software tool;specifications;implementation models;executable program abstract data types abstract specifications software development specification language synthesis system;synthesis system;abstract data types;abstract model;language translation;semantics;testing;abstract data type;software engineering;executable program;specification language;synthesis;prototipo;traduction;prototyping;syntactics;specification languages;translation;abstracts libraries software specification languages semantics testing syntactics;herramienta controlada por logicial;data structures;abstracts;desarrollo logicial;type abstrait;software development;specification languages data structures software engineering;transformation rules;traduccion;tipo abstracto;lenguaje especificacion;abstract specifications;langage specification;prototype;transformation rules abstract data types abstract model implementation models language translation prototyping specifications specification testing synthesis;specification testing	The development of correct specifications is a critical task in the software development process. An alternative approach for the development of specifications is described. The approach relies on a specification language for abstract data types and synthesis system. The system is capable of translating in abstract data type specification into an executable program. This process defines an alternative methodology that provides the necessary tools for the early testing of the specifications and for the development of prototypes and implementation models.	abstract data type;executable;software development process;software prototyping;specification language	Boumediene Belkhouche;Joseph E. Urban	1986	IEEE Transactions on Software Engineering	10.1109/TSE.1986.6312960	data structure;computer science;software engineering;formal specification;database;semantics;prototype;programming language;abstract data type	SE	-26.612483216199	24.069167925401448	58758
c5c0677461299900adc77573d5076edd8ffe44a6	functionality decomposition by compositional correstness preserving transformation	design principle;top down;concurrent systems;functional decomposition;protocol specification;process algebra	We present an algorithm for the decomposition of processes in a process algebraic framework. Decomposition, or the refinement of process substructure, is an important design principle in the top-down development of concurrent systems. In the approach that we follow the decomposition is based on a given partition of the actions of a system specification, such that for each partition class a subprocess must be created that realizes the actions in that class. In addition a suitable synchronization structure between the subprocesses must be present to ensure that the composite behaviour of the subprocesses is properly related to the behaviour of the original specification. We present our results for the process-algebraic specification language LOTOS and give a compositional algorithm for the transformation of the original specification into the required subprocesses. The resulting specification is observation congruent with the original, and, interestingly enough, the subprocesses inherit much of the structure of the original specification. The correctness preserving transformation has been implemented in a tool and has been used for the derivation of protocol specifications from formal descriptions of the desired service. This is possible as it can be shown that the required synchronization mechanisms between the subprocesses can be readily implemented over (reliable) asynchronous media.	algebraic specification;algorithm;child process;classful network;concurrency (computer science);correctness (computer science);language of temporal ordering specification;linear algebra;process calculus;program transformation;refinement (computing);specification language;top-down and bottom-up design	Ed Brinksma;Rom Langerak;Peter Broekroelofs	1993		10.1007/3-540-56922-7_31	functional decomposition;process calculus;computer science;theoretical computer science;top-down and bottom-up design;programming language;algorithm	PL	-32.33640016941157	30.34342410548567	58854
da5efa5bf7f22ce96ed3f9f4a397660cddc19041	order reduction for multi-core interruptible operating systems		If one wishes to verify a program in high-level semantics, one has to deal with the fact that the compiled code is run on an architecture very different from the one the program was verified in. For example, one unit of execution in the high-level language can be compiled to a block consisting of multiple units of execution in the target architecture. Order reduction is then the property that this block can indeed be considered to be executed in a single step, i.e., that the behavior of the program remains unchanged. Order reduction is dependent on certain properties of the compiled code, e.g., that there is at most one linearization point in each block. Conditions under which order reduction is possible have been studied in depth for user programs, but not for operating systems. Interruptible operating systems are particularly exciting because inter processor interrupts can interrupt an operating system thread while it has not yet completed a block of execution. In this paper, we show an order reduction theorem for interruptible operating systems. Unlike most order reduction theorems, all properties of the compiled code necessary for order reduction can be verified on the order-reduced program. Thus, one can verify high-level programs completely in the high-level semantics, including the property that the behavior of the program is unchanged when executed on a low-level machine. Furthermore, we make no assumptions about user code. We use a simple ownership annotation which can be deduced mechanically and thus be used to find data races in programs. The order reduction theorem presented here is strong in the sense that multiple memory accesses can be part of a single block, as long as at most one of them is racing.	multi-core processor	Jonas Oberhauser	2016		10.1007/978-3-319-48869-1_3	interrupt;computer science;operating system;multi-core processor;architecture;linearization;semantics;thread (computing);compiled language;annotation	Logic	-20.73638488658371	31.376705852826532	58876
ab3b2c62c59fd5b096a48896884bd16705c20629	symbolic evaluation graphs and term rewriting - a general methodology for analyzing logic programs			logic programming;rewriting;symbolic execution	Jürgen Giesl;Thomas Ströder;Peter Schneider-Kamp;Fabian Emmes;Carsten Fuhs	2012		10.1007/978-3-642-38197-3_1		AI	-19.500375808830075	20.061470762852856	59099
238b5d53d394d0124b06b07b0c2136d58d8f6b44	a lambda calculus of incomplete objects	operational semantics;lambda calculus;type system	This paper extends the Lambda Calculus of Objects as proposed in [5] with a new support for incomplete objects. Incomplete objects behave operationally as “standard” objects; their typing, instead, is different, as they may be typed even though they contain references to methods that are yet to be added. As a byproduct, incomplete objects may be typed independently of the order of their methods and, consequently, the operational semantics of the untyped calculus may be soundly defined relying on a permutation rule that treats objects as sets of methods. The new type system is a conservative extension of the system of [5] that retains the mytype specialization property for inherited methods peculiar to [5], as well as the ability to statically detect run-time errors such as message not understood.	lambda calculus;operational semantics;partial template specialization;smart objects;type system	Viviana Bono;Michele Bugliesi;Luigi Liquori	1996		10.1007/3-540-61550-4_150	lambda lifting;system f;deductive lambda calculus;fixed-point combinator;calculus of constructions;process calculus;typed lambda calculus;discrete mathematics;dependent type;binary lambda calculus;normalisation by evaluation;type system;pure type system;computer science;lambda calculus;simply typed lambda calculus;curry–howard correspondence;hindley–milner type system;programming language;church encoding;lambda cube;natural deduction;operational semantics;type inhabitation;algorithm	PL	-23.09947375915856	25.729729473145692	59126
c2793d4e3b5b1b7639032424637a3a31c1cfa4f4	settable and non-interfering signal functions for frp: how a first-order switch is more than enough	arrows;switch;arrowchoice;functional reactive programming	"""Functional Reactive Programming (FRP) provides a method for programming continuous, reactive systems by utilizing signal functions that, abstractly, transform continuous input signals into continuous output signals. These signals may also be streams of events, and indeed, by allowing signal functions themselves to be the values carried by these events (in essence, signals of signal functions), one can conveniently make discrete changes in program behavior by """"switching"""" into and out of these signal functions. This higher-order notion of switching is common among many FRP systems, in particular those based on arrows, such as Yampa.  Although convenient, the power of switching is often an overkill and can pose problems for certain types of program optimization (such as causal commutative arrows [14]), as it causes the structure of the program to change dynamically at run-time. Without a notion of just-in-time compilation or related idea, which itself is beset with problems, such optimizations are not possible at compile time.  This paper introduces two new ideas that obviate, in a predominance of cases, the need for switching. The first is a non-interference law for arrows with choice that allows an arrowized FRP program to dynamically alter its own structure (within statically limited bounds) as well as abandon unused streams. The other idea is a notion of a settable signal function that allows a signal function to capture its present state and later be restarted from some previous state. With these two features, canonical uses of higher-order switchers can be replaced with a suitable first-order design, thus enabling a broader range of static optimizations."""	causal filter;compile time;compiler;first-order predicate;functional reactive programming;interference (communication);just-in-time compilation;mathematical optimization;non-interference (security);program optimization	Daniel Winograd-Cort;Paul Hudak	2014		10.1145/2628136.2628140	real-time computing;switch;functional reactive programming;computer science;distributed computing;programming language;algorithm	PL	-19.12409367767822	30.759085944613986	59203
eee5691a7fde5fdd6bcfb6128dcc549de9b190e6	towards a calculus for dynamic architectures		The architecture of a system describes the system’s overall organization into components and connections between those components. With the emergence of mobile computing, dynamic architectures have become increasingly important. In such architectures, components may appear or disappear, and connections may change over time. The dynamic nature of such architectures makes reasoning about their behavior difficult. Since components can be activated and deactivated over time, their behavioral specifications depend on their state of activation. To address this problem, we introduce a calculus for dynamic architectures in a natural deduction style. Therefore, we provide introduction and elimination rules for several operators traditionally employed to specify component behavior. Finally, we show soundness and relative completeness of these rules. The calculus can be used to reason about component behavior in a dynamic environment. This is demonstrated by applying it to verify a property of dynamic blackboard architectures.	blackboard system;emergence;hol (proof assistant);isabelle;logic for computable functions;mobile computing;natural deduction;semantics (computer science);soundness (interactive proof);tracing (software);unified framework	Diego Marmsoler	2017		10.1007/978-3-319-67729-3_6	completeness (statistics);operator (computer programming);architecture;theoretical computer science;calculus;computer science;natural deduction;mobile computing;soundness	AI	-20.646956955163166	21.63031427235919	59246
d5ee55c3087cb5400c2a8dfc3c90338123d854a0	transformational semantics for concurrent programs	concurrent programs			Manfred Broy	1980	Inf. Process. Lett.	10.1016/0020-0190(80)90009-5	computer science;mathematics	DB	-22.58548981803485	21.136191033300836	59316
37c06e10ce7a2a6d9f1224f8e1c923af30d82aee	the elfe system - verifying mathematical proofs of undergraduate students		ELFE is an interactive system for teaching basic proof methods in discrete mathematics. The user inputs a mathematical text written in fair English which is converted to a special data-structure of first-order formulas. Certain proof obligations implied by this intermediate representation are checked by automated theorem provers which try to either prove the obligations or find countermodels if an obligation is wrong. The result of the verification process is then returned to the user. ELFE is implemented in HASKELL and can be accessed via a reactive web interface or from the command line. Background libraries for sets, relations and functions have been developed. It has been tested by students in the beginning of their mathematical studies.	automated theorem proving;command-line interface;data structure;discrete mathematics;first-order predicate;haskell;in the beginning... was the command line;interactivity;intermediate representation;library (computing);user interface	Maximilian Doré;Krysia Broda	2018		10.5220/0006681000150026	algorithm;computer science;obligation;intermediate language;mathematical proof;haskell;user interface	PL	-20.07492476525788	20.627570032228657	59523
f103caea968240308b456098f3dcd4ad217016e0	convenient use of legacy software in java with janet package	legacy software;high performance java;java native interface;ease of use;out of core computing;high performance;java language	This paper describes Janet package — highly expressive Java language extension that enables convenient creation of powerful native methods and efficient Java-to-native code interfaces. Java native interface (JNI) is a low-level API that is rather inconvenient if used directly. Therefore Janet, as the higher-level tool, combines flexibility of JNI with Java’s ease-of-use. Performance results of Janet-generated interface to the lip library are shown. Java code, which uses lip, is compared with native C implementation. © 2001 Elsevier Science B.V. All rights reserved.	application programming interface;computer science;graphical user interface;high- and low-level;java;legacy system;library (computing);machine code;metacomputing;refinement (computing);test harness	Marian Bubak;Dawid Kurzyniec;Piotr Luszczek	2001	Future Generation Comp. Syst.	10.1016/S0167-739X(01)00041-3	java data objects;java card;java api for xml-based rpc;parallel computing;jsr 94;java concurrency;usability;application programming interface;jar;computer science;operating system;java modeling language;interface;strictfp;embedded java;real time java;programming language;java;legacy system;generics in java;scala;java applet;java annotation;non-blocking i/o	PL	-28.656186224179017	28.586279313063265	59908
0cad65ae93474c96170f6935db3f820fc127255d	traceability system for agricultural productsbased on rfid and mobile technology	agricultural production;food chain;distributed processing;risk management;mobile computer;rfid tag;food safety;process control;navigation system;wireless lan;mobile technology	In agriculture, it is required to establish and integrate food traceability systems and risk management systems in order to improve food safety in the entire food chain. The integrated traceability system for agricultural products was developed, based on innovative technology of RFID and mobile computing. In order to identify individual products on the distribution process efficiently, small RFID tags with unique ID and handy RFID readers were applied. On the distribution process, the RFID tags are checked by using the readers, and transit records of the products are stored to the database via wireless LAN. Regarding agricultural production, the recent issues of pesticides misuse affect consumer confidence in food safety. The Navigation System for Appropriate Pesticide Use (Nouyaku-navi) was developed, which is available in the fields by Internet cell-phones. Based on it, agricultural risk management systems have been developed. These systems collaborate with traceability systems and they can be applied for process control and risk management in agriculture.	database;food chain;handy board;internet;mobile computing;radio-frequency identification;risk management;traceability	Koji Sugahara	2008		10.1007/978-1-4419-0213-9_82	embedded system;engineering;mobile computing;computer security;computer network	Mobile	-30.617558502747798	18.951835688347263	60069
238b91bf511acb1e1a03b4053d8c3d621b54be3d	optimization of chr propagation rules	propagation rules;programming language;constraint handling rules;optimization	Constraint Handling Rules (CHR) is an elegant, high-level programming language based on multi-headed, forward chaining rules. To ensure CHR propagation rules are applied at most once with the same combination of constraints, CHR implementations maintain a socalled propagation history. The performance impact of this history can be significant. We introduce several optimizations that, for the majority of CHR rules, eliminate this overhead. We formally prove their correctness, and evaluate their implementation in two state-of-the-art CHR systems.	compiler;constraint handling rules;correctness (computer science);discrepancy function;forward chaining;general-purpose modeling;high- and low-level;high-level programming language;idempotence;identifier;interrupt handler;interval propagation;iteration;mathematical optimization;operational semantics;overhead (computing);scientific literature;search algorithm;software propagation;solver;unit propagation;while	Peter Van Weert	2008		10.1007/978-3-540-89982-2_42	computer science;theoretical computer science;programming language;algorithm	AI	-21.145958102257357	23.697216392122435	60224
c7366a59a20afc8cba1602aeeb7f37d476e010c9	vdm: axiomatising its propositional logic	preuve programme;program proof;programmation;etude theorique;logique mathematique;specification;logica matematica;logique propositionnelle;ingenieria logiciel;mathematical logic;software engineering;programacion;especificacion;propositional logic;estudio teorico;prueba programa;genie logiciel;logica proposicional;theoretical study;programming		propositional calculus;vienna development method	P. F. Gibbins	1988	Comput. J.	10.1093/comjnl/31.6.510	zeroth-order logic;programming;mathematical logic;computer science;artificial intelligence;propositional variable;propositional calculus;specification;algorithm;autoepistemic logic	Logic	-19.368354325128735	21.425852254158595	60381
beb7761f36c965312acdee152adb5d45a5e39352	clocks in dataflow languages	informatica;lenguaje programacion;sequential machine;programming language;real time;flot donnee;machine sequentielle;flujo datos;maquina secuencial;continuous running;temps reel;langage programmation;tiempo real;informatique;computer science;data flow;memoire bornee;funcionamiento continuo;fonctionnement continu	This paper addresses the problem of using a dataflow language in “real-time” continuously operating systems. It shows that this raises a problem of bounded memory which can be characterized in terms of multiple input-output sequential machines, and proposes a generalization of the Ginsburg-Rose theorem in this case. Finally, it shows how these concepts have been applied in the clock calculus of the synchronous dataflow language Lustre.	dataflow programming	Paul Caspi	1992	Theor. Comput. Sci.	10.1016/0304-3975(92)90326-B	dataflow architecture;data flow diagram;lustre;computer science;artificial intelligence;dataflow;signal programming;programming language;algorithm	Logic	-24.083911627727748	32.14093030901475	60679
e5218e9b3c570d75b2fedb2dec223bb924fefe66	generic description of a software document environment	page description languages;document handling;language use;280301 programming techniques;document description languages;700102 application tools and system utilities;software documents;computer languages mathematics computer science application software software engineering programming environments program processors costs navigation technological innovation;diagrams;generic implementation;software tools;uq environment software document environment description generic language based environment structured document handling syntactic structure relational structure user interaction textual view diagrammatic view environment description language;280302 software engineering;280104 computer human interaction;structured documents;user interaction;tool description languages;user interfaces;page description languages document handling user interfaces diagrams	UQ is an evolving generic language-based environment for manipulation of structured documents. The environment is intended to capture both syntactic and relational structure within and between documents and to support user interaction via both textual and diagrammatic views. This paper illustrates the innovative features of the environment description language used to instantiate a UQ environment.	diagram;jones calculus;semantic analysis (compilers);the australian;traceability	Mark A. Toleman;David A. Carrington;Phil Cook;Andrew Coyle;Anthony MacDonald;Jim Welsh;Tim Jones	2001		10.1109/HICSS.2001.927258	natural language processing;computer science;diagram;operating system;database;programming language;user interface	Web+IR	-31.58996572466387	23.543776894620553	60771
76e6a0a2abe0db8f7b3277929fd4c06b1baad67c	a simulation study of the argonne model for or-parallel execution of prolog			prolog;simulation	Kish Shen;David H. D. Warren	1987			computer architecture;programming language;computer science;prolog	DB	-24.551121924021363	22.87552011663227	60927
b65c1b0d4802c4c55033f28675fb666b58bd8a9c	editorial: proof theory corner	proof theory			Arnon Avron	2009	J. Log. Comput.	10.1093/logcom/exn107	computer science;analytic proof;proof theory;mathematics;constructive proof;programming language	Logic	-19.91946053533237	18.33846043660377	60997
0d2d4ea2bec35ce771257195c7c2b15361decd8d	compiling real time functional reactive programming	performance guarantee;real time;discrete time;partial evaluation;reactive system;synchronous dataflow;functional reactive programming;tupling	Most of the past languages for reactive systems are based on synchronous dataflow. Recently, a new reactive language, called Real-Time Functional Reactive Programming (RT-FRP) [18] , has been proposed based on the functional paradigm. The unique feature of this language is the high-level abstraction provided in the form of behaviors for conti-nuous-time signals, and events for discrete-time signals. RT-FRP also features some performance guarantees in the form of bounded runtime and space usage for each reactive computation step.In this paper, we propose a new compilation scheme for RT-FRP. Our compilation scheme is based on two key stages. In the first stage, we translate RT-FRP program to an intermediate functional code. This code is deliberately kept at high level for two reasons. First, it is easier for us to validate its correctness. Second, it allows us to apply high-level source-to-source transformation to achieve further optimization. The second stage attempts to compile the intermediate code to a corresponding automata code. Our main novelty is the use of two high-level transformation techniques for this compilation. The first technique, partial evaluation, attempts to propagate constant values (wherever feasible) in order to perform more aggressive specialization. The second technique, tupling, combines mutually dependent automata together into a composite automaton whenever possible. Both techniques are needed for generating fast target code for RT-FRP.	automaton;compiler;computation;correctness (computer science);dataflow;functional programming;functional reactive programming;high- and low-level;high-level programming language;mathematical optimization;partial evaluation;partial template specialization;programming paradigm;real-time transcription;source transformation;space–time tradeoff;windows rt	Dana N. Xu;Siau-Cheng Khoo	2002		10.1145/568173.568183	discrete time and continuous time;real-time computing;reactive programming;reactive system;functional reactive programming;computer science;theoretical computer science;programming language;partial evaluation;algorithm	PL	-19.18217720524308	30.76165017553139	61132
7552048220b986150e2e7e70571144656ba625d9	theorem proving in higher order logics		ions for Fault-Tolerant Distributed System Verification . . . . . . . . . . 257 Lee Pike, Jeffrey Maddalon, Paul Miner, and Alfons Geser Formalizing Integration Theory with an Application to Probabilistic Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 271 Stefan Richter Formalizing Java Dynamic Loading in HOL . . . . . . . . . . . . . . . . . . . . . . . . . . . 287 Tian-jun Zuo, Jun-gang Han, and Ping Chen Certifying Machine Code Safety: Shallow Versus Deep Embedding . . . . . . . . 305 Martin Wildmoser and Tobias Nipkow Term Algebras with Length Function and Bounded Quantifier Alternation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321 Ting Zhang, Henny B. Sipma, and Zohar Manna Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337 Error Analysis of Digital Filters Using Theorem Proving Behzad Akbarpour and Sofiène Tahar Dept. of Electrical & Computer Engineering, Concordia University 1455 de Maisonneuve W., Montreal, Quebec, H3G 1M8, Canada {behzad,tahar}@ece.concordia.ca Abstract. When a digital filter is realized with floating-point or fixedWhen a digital filter is realized with floating-point or fixedpoint arithmetics, errors and constraints due to finite word length are unavoidable. In this paper, we show how these errors can be mechanically analysed using the HOL theorem prover. We first model the ideal real filter specification and the corresponding floating-point and fixed-point implementations as predicates in higher-order logic. We use valuation functions to find the real values of the floating-point and fixed-point filter outputs and define the error as the difference between these values and the corresponding output of the ideal real specification. Fundamental analysis lemmas have been established to derive expressions for the accumulation of roundoff error in parametric Lth-order digital filters, for each of the three canonical forms of realization: direct, parallel, and cascade. The HOL formalization and proofs are found to be in a good agreement with existing theoretical paper-and-pencil counterparts.	algorithm;automated theorem proving;bounded quantifier;computer engineering;digital filter;distributed computing;dynamic loading;entity–relationship model;error analysis (mathematics);fault-tolerant computer system;fixed point (mathematics);fixed-point arithmetic;hol (proof assistant);han unification;higher-order function;interactive theorem proving (conference);java;machine code;pike;quantifier (logic);round-off error;tree accumulation;value (ethics)	Takeo Kanade;Josef Kittler;John C. Mitchell;Moni Naor	2004		10.1007/b100400	discrete mathematics;fundamental theorem;mathematics;algebra	PL	-20.106695862571947	18.55417579889184	61245
343f64ae9b626d7bb1a54d9034994552bf10381a	extended ssa numbering: introducing ssa properties to languages with multi-level pointers		"""Static Single Assignment (SSA) intermediate representations have become quite popular in compiler development. One advantage of the SSA form is that each variable corresponds to exactly one definition, and thus two references of the same SSA variable must denote the same value. To date, most SSA forms concentrate on scalar variables, and it is difficult to extend these intermediate representations to languages with multi-level pointers, such as C.We introduce Extended  analysis, which concentrates on the """"same name, same definition"""" properties of SSA form while compromising on other aspects. What is not provided is well compensated for by new properties introduced for pointer references such as *p. Two references of *p with the same Extended SSA numbers denote the same value."""		Christopher Lapkowski;Laurie J. Hendren	1996		10.1145/782052.782075	program analysis;computer science;parsing;optimizing compiler;programming language;intermediate language;static single assignment form;algorithm	Logic	-21.9119138319199	24.63449190728107	61285
ccb0023182c6c4078e74be4378a341fa7d10c8be	a functional logic programming approach to graphical user interfaces	programming language;graphic user interface;concurrent programs;functional logic programming	We show how the features of modern integrated functional logic programming languages can be exploited to implement graphical user interfaces (GUIs) in a high-level declarative style. For this purpose, we have developed a GUI library in Curry, a multi-paradigm language amalgamating functional, logic, and concurrent programming principles. The functional features of Curry are exploited to deene the graphical structure of an interface and to implement new graphical abstractions, and the logic features of Curry are used to specify the logical dependencies of an interface. Moreover, the concurrent and distributed features of Curry support the easy implementation of GUIs to distributed systems.	concurrent computing;curry;declarative programming;distributed computing;functional logic programming;functional programming;graphical user interface;high- and low-level;programming language;programming paradigm	Michael Hanus	2000		10.1007/3-540-46584-7_4	concurrent constraint logic programming;declarative programming;functional reactive programming;computer science;theoretical computer science;functional logic programming;graphical user interface;programming paradigm;event-driven programming;inductive programming;fifth-generation programming language;programming language;prolog;logic programming;algorithm	PL	-26.855892009241934	26.829782909328994	61333
92c969f7f94f7672ec06e6320dcdab849c57a7c7	towards a formal operational semantics of uml statechart diagrams	unied modeling language;operational semantics;formal semantics;object oriented systems;model checking;intermediate format;formal language	Statechart Diagrams are a notation for describing behaviours in the framework of UML, the Unified Modeling Language of object-oriented systems. UML is a semi-formal language, with a precisely defined syntax and static semantics but with an only informally specified dynamic semantics. UML Statechart Diagrams differ from classical statecharts, as defined by Harel, for which formalizations and results are available in the literature. This paper sets the basis for the development of a formal semantics for UML Statechart Diagrams based on Kripke structures. This forms the first step towards model checking of UML Statechart Diagrams. We follow the approach proposed by Mikk and others: we first map Statechart Diagrams to the intermediate format of extended hierarchical automata and then we define an operational semantics for these automata. We prove a number of properties of such semantics which reflect the design choices of UML Statechart Diagrams. The work described in this paper has been performed in the context of the ESPRIT Project n. 27439 HIDE. C.N.R., Ist. CNUCE, Pisa, Italy, d.latella@cnuce.cnr.it Technical University of Budapest, Dept. of Measurement and Information Systems, Budapest, Hungary, majzik@mit.bme.hu. Partially supported by the CNR-NATO Guest Fellowship Programme. University of York, Dept. of Computing, York, United Kingdom, mieke@cs.york.ac.uk. Supported by the TACIT network under the European Union TMR Programme, Contract ERB FMRX CT97 0133.	automata theory;automaton;formal language;information system;kripke semantics;model checking;operational semantics;programming language;semantics (computer science);semiconductor industry;state diagram;triple modular redundancy;unified modeling language	Diego Latella;István Majzik;Mieke Massink	1999		10.1007/978-0-387-35562-7_25	model checking;state diagram;formal language;uml state machine;uml tool;computer science;theoretical computer science;applications of uml;formal semantics;database;programming language;operational semantics	PL	-25.54917020074796	20.500866481673846	61600
b18648c6b6ff5a0c650538c93ccd5fb1c6e749fb	compiler prototyping with vdm and standard ml	standard ml	Without Abstract	compiler;standard ml;vienna development method	Rob Arthan	1988		10.1007/3-540-50214-9_10	computer architecture;parallel computing;computer science;programming language	PL	-24.31730959736462	22.61078342227783	61697
e8be7b1947115566341f8394e2aba5a37a774a11	data structures, data abstraction - a contemporary introduction to data structures using c++	data abstraction;data structure		abstraction (software engineering);c++;data structure	Mitchell L. Model	1994			theoretical computer science;data structure;abstraction;computer science	Logic	-23.435231136949618	20.173735914663375	61699
1bf7b80b7084fc1500489658e1f58c028f322578	apl2os: design considerations for a nested array file system	nested array file system;design consideration;apl2 syntax;system file;apl2 application;external function;maximum advantage;design goal;apl2 system;system design;operating system	APL2OS is an External Function for the APL2 system, designed to enable APL2 applications to access operating system files (and information about these files) in a straightforward and efficient way, using the power of APL2 syntax to maximum advantage. The design goals and approaches for APL2OS are discussed, in the context of a summary of its features.	apl;operating system	David M. Weintraub	1990		10.1145/97808.97876	self-certifying file system;real-time computing;device file;computer file;computer science;operating system;unix file types;database;data file;systems design	OS	-32.78678431397385	25.77433272103675	61750
4b3ee121dad22228ed06e60ee4755a7a5229400e	an estelle-based incremental protocol design system	verification;concepcion asistida;interfase usuario;computer aided design;design system;modele client serveur;pascal language;syntax;architecture systeme;formal specification;multimedia;protocole transmission;langage c;user interface;etude experimentale;protocol verification;reseau ordinateur;teleinformatica;protocol design;semantics;syntaxe;semantica;semantique;specification language;formal description technique;computer network;specification formelle;especificacion formal;teleinformatique;protocolo transmision;c language;interpreteur;estructura datos;red ordenador;conception assistee;protocol specification;arquitectura sistema;interface utilisateur;structure donnee;lenguaje especificacion;pascal;interpreter;verificacion;sintaxis;system architecture;interprete;remote data processing;langage specification;data structure;estudio experimental;incremental protocol;lenguaje c;transmission protocol	Abstract   Formal description techniques (FDTs) provide formal and abstract ways to specify what protocols have to do and what features protocols need. Estelle is an FDT defined by the International Organization for Standardization for protocol specifications. We present an incremental protocol design system that contains an incremental protocol verification technique and an Estelle translator. Our incremental protocol design system allows on-line reverification after respecification. That is, instead of verifying respecified (modified) protocols from scratch, the reverification procedure is executed continuously and incrementally at the modification point. Using the translator, Estelle protocol specifications can be translated and interpreted for protocol verification. To meet the requirement of modifying protocol specifications written in Estelle at run time, the Estelle translator allows incremental translation and interpretation of the modified Estelle specification part for incremental verification. To further reduce the number of global states to be explored, the concept of dead and live variables is incorporated into our incremental verification technique. Based on the incremental verification technique and the Estelle translator, an incremental protocol design system (IPDS) is developed on SUN SPARC OPENLOOK work-stations. Using IPDS, protocol designers can analyze the verification results, interactively modify the protocols, and then continue the verification incrementally.		Chung-Ming Huang;Jenq-Muh Hsu;Huei-Yang Lai;Duen-Tay Huang;Jao-Chiang Pong	1997	Journal of Systems and Software	10.1016/0164-1212(95)00065-8	verification;pascal;interpreter;syntax;data structure;specification language;computer science;formal specification;database;semantics;programming language;user interface;algorithm;systems architecture;pascal	Embedded	-24.530665510726774	29.997689302044602	61794
9925e836ada88bb83d28da321b2a3906e6a34f21	a hypermedia lab manual for operating systems: using the network to teach	programming language;operating system;algorithm animation;concurrent programs	A hypermedia laboratory manual for operating systems and concurrent programming, based on the SR programming language, has been written and is available for use over the Internet. It is written in HTML and is accessible using Web browsers such as Mosaic and Netscape. Algorithm animations can be run by adding a new external viewer to the browser. Tbe Web helps solves the handout problem faced by many instructors.	algorithm;concurrent computing;html;hypermedia;internet;ncsa mosaic;operating system;programming language;sr (programming language)	Stephen J. Hartley	1996		10.1145/237466.237480	real-time computing;computer science;programming language	OS	-31.135381022180322	25.685945979745004	62050
f4c609e7905f25b8449198cf32bc6e08d00ae63b	verification of year 2000 conversion rules using the acl2 theorem prover	cobol;program transformation;automated reasoning;theorem prover;formal verification	The well-publicized Year 2000 problem provides interesting challenges for the remediation of noncompliant code. This paper describes some work done at EDS CIO Services, using the ACL2 theorem prover to formally verify correctness of remediation rules. The rules take into account the possibility of “flag” (non-date) values of date variables. Many of them have been implemented in an in-house tool, COGEN 2000TM, that corrects for noncompliant date-related logic in COBOL programs.	acl2;automated theorem proving;cobol;chief information officer;correctness (computer science);extended data services;whole earth 'lectronic link;year 2000 problem	Matt Kaufmann	2000	International Journal on Software Tools for Technology Transfer	10.1007/PL00010807	formal verification;computer science;theoretical computer science;software engineering;automated theorem proving;cobol;automated reasoning;programming language;algorithm	Logic	-20.454616595869627	20.631426992317433	62264
0abe8f9764c3253c032cf2851a7341384ff633bc	a formal model of asynchronous communication and its use in mechanically verifying a biphase mark protocol	verification;boyer moore logic;tolerancia falta;distributed system;transformations mathematics;hardware verification;systeme reparti;automatic proving;fault tolerant;synchronisation horloge;logic design;clocks;protocol verification;verification materiel;program verification computers;demostracion automatica;communication asynchrone;theorem proving;demonstration automatique;demonstration theoreme;modelisation performance;protocol computers;synchronism;sistema repartido;cycles;iso protocol level 1;asynchronous communication;fault tolerance;automatic theorem proving;performance model;logique boyer moore;manchester format;communication delay;clock synchronization;performance bounds;format manchester;messages;verificacion;demostracion teorema;performance modeling;tolerance faute;level 1;verification protocole	We present a formal model of asynchronous communication between two digital hardware devices. The model takes the form of a function in the Boyer-Moore logic. The function transforms the signal stream generated by one processor into that consumed by an independently clocked processor, given the phases and rates of the two clocks and the communications delay. The model can be used quantitatively to derive concrete performance bounds on communications at ISO protocol level 1 (physical level). We use the model to show that an 18-bit/cell biphase mark protocol reliably sends messages of arbitrary length between two processors provided the ratio of the clock rates is within 5% of unity.	cpu cache;central processing unit;clock rate;digital electronics;formal language;mathematical model;verification and validation	J. Strother Moore	1994	Formal Aspects of Computing	10.1007/BF01211081	fault tolerance;real-time computing;computer science;theoretical computer science;asynchronous communication;mathematics;programming language;algorithm	Theory	-24.083676945831915	32.17300710284408	62327
19d76f8bbeadb7f77ab91927a1843131c53b7546	verifying probabilistic correctness in isabelle with pgcl		This paper presents a formalisation of pGCL in Isabelle/HOL. Using a shallow embedding, we demonstrate close integration with existing automation support. We demonstrate the facility with which the model can be extended to incorporate existing results, including those of the L4.verified project. We motivate the applicability of the formalism to the mechanical verification of probabilistic security properties, including the effectiveness of side-channel countermeasures in real systems.	correctness (computer science);hol (proof assistant);isabelle;semantics (computer science);side-channel attack	David Cock	2012		10.4204/EPTCS.102.15	computer science;database;programming language;algorithm	AI	-20.84537348632329	27.71949266442513	62371
a1836592b97afa833101bc6014b58811c41fdfa2	the impact of synchronisation on secure information flow in concurrent programs	semantica operacional;confidencialidad;shared memory;proceso ligero;memoria compartida;operational semantics;simultaneidad informatica;concurrent program;secure information flow;flujo informacion;confidentiality;flux information;synchronisation;confidentialite;information flow;concurrency;semantique operationnelle;processus leger;synchronization;programa competidor;concurrent programs;sincronizacion;thread;simultaneite informatique;memoire partagee;programme concurrent;timing	Synchronisation is fundamental to concurrent programs. This paper investigates the security of information ow in multi-threaded programs in the presence of synchronisation. We give a small-step operational semantics for a simple shared-memory multi-threaded language with synchronisation, and present a compositional timing-sensitive bi-simulation-based conndentiality speciication. We propose a type-based analysis improving on previous approaches to reject potentially insecure programs.	concurrent computing;information flow;operational semantics;shared memory;simulation;thread (computing)	Andrei Sabelfeld	2001		10.1007/3-540-45575-2_22	synchronization;real-time computing;computer science;operating system;database;distributed computing;programming language;algorithm	PL	-22.754417752664743	31.27257462820972	62518
cb66627772a8d6623ca9ed9f3d3af4703bdf2aec	let a single flwor bloom	rewrite rule;search space;004 informatik;global optimization	To globally optimize execution plans for XQuery expression , a plan generator must generate and compare plan alternatives. In proven compiler architec tur s, the unit of plan generation is the query block. Fewer query blocks mean a larger search space fo r the plan generator and lead to a generally higher quality of the execution plans. The goal of this paper is to provide a toolkit for developers of XQuery evaluators to transform XQuery expres sion into expressions with as few query blocks as possible. Our toolkit takes the form of rewrite rules merging the inner and outer FLWOR expressions into single FLWORs. We focus on previously unpublished rewr ite rules and on inner FLWORs occurring in thefor, let, andreturn clauses in the outer FLWOR.	compiler;flwor;rewriting;sion's minimax theorem;xquery	Matthias Brantner;Carl-Christian Kanne;Guido Moerkotte	2007		10.1007/978-3-540-75288-2_5	searching the conformational space for docking;computer science;theoretical computer science;database;programming language;global optimization	AI	-21.585578461364626	25.073988394279976	62642
f86384e18223e55fd902efd7414388cc0c9fc539	answer set application programming: a case study on tetris		Answer-Set Programming (ASP) is a successful branch of the logic programming paradigm with many applications in modelling and solving of NP-hard problems. Combinatorial problems are the main application domain of ASP and it seems unsuitable for serving as a programming language for interactive applications. However, we conjecture that there is no theoretical obstacle for using ASP to that end. As witnessed by functional programming, it can be useful to use a declarative paradigm for creating applications. In this work we explore possibilities, benefits, and drawbacks, of programming an interactive application in ASP. We find that this is hard mainly for the following reasons: managing change over time, interaction with the user, generating output that is ordered (i.e., not a set), handling persistence of certain data, and ensuring efficiency. ASP and related fields provide powerful techniques for representing actions and change, executing programs with respect to external environments, and processing external events. Even if the full power of these techniques is not required to build an interactive application, combining them is necessary, and putting together these concepts in a practical framework is challenging. We realize such an integration in a framework we call Answer Set Application Programming framework which is based on the HEX language and features syntactic shortcuts to make application programming more intuitive. We describe design decisions and discuss alternative possibilities. Our sample application is a playable version of Tetris which demonstrates that ASP can be used as a general-purpose programming-language.	apl;answer set programming;application domain;asynchronous array of simple processors;declarative programming;emoticon;exception handling;fluent (artificial intelligence);functional programming;general-purpose markup language;hex;imperative programming;interactivity;logic programming;np-hardness;persistence (computer science);problem solving;programming language;programming paradigm;python;reinventing the wheel;relevance;solver;stable model semantics;tetris;whole earth 'lectronic link	Peter Schüller;Antonius Weinzierl	2015			programming domain;reactive programming;functional reactive programming;computer science;artificial intelligence;theoretical computer science;answer set programming;programming paradigm;event-driven programming;procedural programming;symbolic programming;inductive programming;algorithm	AI	-22.469197980103814	19.40955887067479	62717
3c1a4230c7e7ce7f9d237ccc7d5a08473703f01e	the specification of assemblers	degradation;formal specification;helium;data processing;program transformation;testing;automatic programming;program verification;assembly;abstract syntax;assemblers;computer science;transduction grammars;abstract syntax assemblers program verification formal specification transduction grammars program transformation;assembly concrete command and control systems data processing degradation automatic programming computer science knowledge based systems testing;program specification;command and control systems;knowledge based systems;concrete	The problem of applying formal techniques of program specification and verification to large complex programs is considered. It is argued that a practical solution requires a variety of techniques, including both procedural and nonprocedural specifications, hierarchical program organization, and the use of program transformations. In particular, a case is made for flexible problem-oriented choice of specification techniques and languages. These ideas are expanded by specifying a load-and-go assembler in three parts: a transduction grammar describing the correspondence between concrete and abstract syntax for assembly language programs; a set of transformations of the abstract form; and a nonconstructive axiomatic specification of the result of core assembly and loading of transformed abstract programs.	abstract syntax;assembly language;formal methods;formal specification;procedural programming;program transformation;transduction (machine learning)	Jay M. Spitzen	1976	IEEE Transactions on Software Engineering	10.1109/TSE.1976.233799	abstract syntax;degradation;concrete;data processing;computer science;theoretical computer science;software engineering;formal specification;assembly;software testing;helium;programming language;algorithm;language of temporal ordering specification	SE	-27.14053767567586	21.478967108462964	62818
e14f7991d04b74020f04aea8ac6ef3f728894220	mettel2: towards a tableau prover generation platform		This paper introduces MetTeL2, a tableau prover generator producing Java code from the specification of a logical syntax and a tableau calculus. It is intended to provide an easy to use system for non-technical users and allow technical users to extend the generated implementations.	blocking (computing);download;experiment;heuristic (computer science);java;method of analytic tableaux;rewriting;syntax (logic);user interface	Dmitry Tishkovsky;Renate A. Schmidt;Mohammad Khodadadi	2012				SE	-20.145340765486893	20.71250118189683	62957
049f176dc6cac7bd25f90ddb8ddc769d3492f163	marshalgen: marshaling objects in the presence of polymorphism.	object oriented programming;polymorphism;parallel computer;middleware;automatic parallelization	Marshaling or serialization of objects is an important component of both distributed and parallel computing. Current systems impose a significant burden on the programmer for describing the marshaling of complex, recursive data structures. Marshalgen provides support for retrofitting legacy and complex software with marshaling features. The original version of Marshalgen provided a semi-automatic process for marshaling in C and C++, based on annotations of the existing include files. The new version reported on here provides direct support for class inheritance, templates and other important features of an object-oriented programming style.	c++;data structure;distributed computing;parallel computing;programmer;programming style;recursion;semiconductor industry;serialization	Gene Cooperman;Viet Ha Nguyen	2004			parallel computing;computer science;distributed computing;programming language	PL	-24.883156388768587	28.06178168213819	63142
fd1d87c648622ebbc7f086b26c23a947889bb90d	simple imperative polymorphism	state;polymorphism;functional programming;continuations	This paper describes a simple extension of the Hindley-Milner polymorphic type discipline to call-by-value languages that incorporate imperative features like references, exceptions, and continuations. This extension sacrifices the ability to type every purely functional expression that is typable in the Hindley-Milner system. In return, it assigns the same type to functional and imperative implementations of the same abstraction. Hence with a module system that separates specifications from implementations, imperative features can be freely used to implement polymorphic specifications. A study of a number of ML programs shows that the inability to type all Hindley-Milner typable expressions seldom impacts realistic programs. Furthermore, most programs that are rendered untypable by the new system can be easily repaired.	concurrent ml;continuation;exception handling;functional programming;hindley–milner type system;imperative programming;modular programming;smoothing;standard ml of new jersey;yacc	Andrew K. Wright	1995	Lisp and Symbolic Computation		module;polymorphism;state;type system;data structure;computer science;continuation;programming language;functional programming;algorithm;modulo;evaluation strategy	PL	-23.07078491883173	26.116476267830485	63371
33581f92435fca6e74aa488cbc587449b47ee131	a new environment for interactive neural network experiments	personal computers;interactive simulation;languages;neural network	DESIRE/NEUNET is a new environment for interactive experiments with neural networks. Simulations on 80386/80387-based personal computers execute faster than in Microsoft FORTRAN. Screen-edited programs, moreover, execute directly without the usual annoying wait for program translation. A true (not incremental) compiler, small and extra fast, is built into a powerful interpreter and compiles all time-critical operations directly into memory. A reasonable matrix language makes it easy to split large neuron interconnection matrices into smaller, intuitively meaningful pieces. DESIRE/NEUNET includes two built-in screen editors, a DOS shell, a debugger, and a journal-file generator, plots graphs, and lets users write their own help screens and menus. The new program admits up to 16,383 neuron interconnections under MS-DOS and will be ported to AT clones.	artificial neural network;experiment	Granino A. Korn	1989	Neural Networks	10.1016/0893-6080(89)90006-3	computer hardware;computer science;theoretical computer science;machine learning;distributed computing;language;artificial neural network	ML	-29.16743707110072	21.702491859727864	63549
2b9538e1ce8dd41256f1dbe1124209d0fdd5b1cd	will mobile computing's future be location, location, location?	mobile user locations mobile computing location based services;location based services;location based service;mobile computing global positioning system satellite broadcasting poles and towers computer vision wireless networks marketing and sales signal analysis timing interference;cell phones;weather forecasting;global position system;mobile computer;navigation;gps;global positioning system;lbs;triangulation;mobile computing;wi fi;cell phones location based services lbs global positioning system gps navigation triangulation wi fi	As mobile computing has become more popular over the past decade, experts have touted the future of location-based services. LBS systems could determine mobile users' locations. Proponents say they could thus enable applications that would not only tell people where they are and how best to get to their destinations, but also whether friends are nearby, what the local weather forecast is, and where businesses of interest in the area are located. The technology could also help companies track packages or vehicles.	location-based service;mobile computing	Steven J. Vaughan-Nichols	2009	Computer	10.1109/MC.2009.65	simulation;global positioning system;telecommunications;computer science;operating system;location-based service;mobile computing;computer security	HCI	-29.694793612776582	19.73875357075688	63558
8f518154acb1c13ccf4cfeafef64bac4ec06b9a8	recompile: a decompilation framework for static analysis of binaries	malware algorithm design and analysis semantics software transforms standards control systems;high level control structure recompile decompilation framework static analysis reverse engineering binary code malware analysis machine code static single assignment form ssa data flow analysis machine specific detail type analysis control flow analysis;data flow analysis;invasive software;reverse engineering data flow analysis invasive software;reverse engineering	Reverse engineering of binary code is an essential step for malware analysis. However, it is a tedious and time-consuming task. Decompilation facilitates this process by transforming machine code into a high-level representation that is more concise and easier to understand. This paper describes REcompile, an efficient and extensible decompilation framework. REcompile uses the static single assignment form (SSA) as its intermediate representation and performs three main classes of analysis. Data flow analysis removes machine-specific details from code and transforms it into a concise high-level form. Type analysis finds variable types based on how those variables are used in code. Control flow analysis identifies high-level control structures such as conditionals, loops, and switch statements. These steps enable REcompile to produce well-readable decompiled code. The overall evaluation, using real programs and malware samples, shows that REcompile achieves a comparable and in many cases better performance than state-of-the-art decompilers.	assignment (computer science);binary code;binary file;compiler;conditional (computer programming);control flow analysis;data-flow analysis;dataflow architecture;decompiler;high- and low-level;intermediate representation;machine code;malware analysis;reverse engineering;static program analysis;static single assignment form;switch statement	Khaled Yakdan;Sebastian Eschweiler;Elmar Gerhards-Padilla	2013	"""2013 8th International Conference on Malicious and Unwanted Software: """"The Americas"""" (MALWARE)"""	10.1109/MALWARE.2013.6703690	real-time computing;computer science;theoretical computer science;database	SE	-22.18728253570343	27.856863858421367	63571
636204456d28c6e64af7b2bb75cdc51877f40ba2	designing a development environment for logic and multi-paradigm programming		The Eclipse platform has been extended to provide integrated development environments for many different languages and systems. Declarative programming, however, and in particular logic languages, has still to benefit from the state-of-the-art Eclipse infrastructure supporting a huge number of development activities. We set out to design an environment for logic programming built around tuProlog, a Java-based light-weight Prolog engine that provides seamless integration into the platform and promotes an innovative form of interaction with the interpreter, allowing a multiplicity of independently configurable instances to be exploited within the same project. Moreover, we use the Java/Prolog multi-paradigm capabilities of tuProlog as a case study for discussing the integration of two different programming languages in a single environment. Finally, we compare our environment to some related projects in the logic programming context. 1 Motivation and Background Eclipse is well-known as an open and extensible integrated development environment (IDE) [1] supporting many languages and systems with varying degree of quality and completeness—from Java and C/C++ with official Eclipse packaging and download, to scripting languages such as Python and Web programming languages such as PHP, up to niche languages such as Fortran, directly incubated within the Eclipse Foundation. However, languages inspired to the declarative programming paradigm, and in particular logic programming languages such as Prolog, have still to benefit from the extensive support given by the Eclipse platform to build full-fledged open source development environments. Indeed, among the major Prolog implementations [2], the interaction with the interpreter is typically reduced to a command line prompt in a text-based console, with no support for writing simple programs or more complex software; when an editor is available, it often suffers from the many limitations and shortcomings of in-house applications, each being rebuilt from scratch and following its own conventions; useful and innovative tools are provided only in few rare cases. Instead, building an IDE on top of Eclipse would mean not only to exploit a state-of-the-art infrastructure for the support of a huge number of programming activities, but also to follow a series of user interface standards and usage workflow guidelines that should ease the impact of the tool on developers and make them feel more comfortable working with it. The purpose of our research is to build a development environment for both logic and multi-paradigm logic/object-oriented programming on top of the Eclipse platform. To this aim, we mean to exploit tuProlog, an open source light-weight Prolog engine, as a case study to analyse and evaluate the many peculiarities of the integration of a logic-based language and support within Eclipse. We chose tuProlog both because it is written in Java, thus representing the easiest choice for platform integration, and for its engineering properties, which seemed ideal to explore innovative forms of interaction with the interpreter: in particular, minimality and configurability [3] let developers spawn a multiplicity of tuProlog engines, each with its own set of libraries and theories, within the same Prolog project, with the aim of exploring, tracing, or comparing different programming solutions. Moreover, multi-paradigm Java/Prolog support on the tuProlog side [4] calls for an adequate counterpart on the development environment side, introducing further requirements that a high quality instrument such as the JDT (Java Development Tools) plug-in helped satisfy. In this paper, we discuss the features, the objectives, and the design of such a logic-based multi-paradigm IDE: we first present the design of the whole environment supporting multi-paradigm Java/Prolog programming (Sect. 2), then illustrate the different parts composing the current prototype for the logic side of the IDE, emphasising the innovative way of interacting with multiple tuProlog engines embedded in Eclipse (Sect. 3); finally, we briefly discuss some related projects (Sect. 4), and draw some conclusions for future work (Sect. 5). 2 An Environment for Multi-Paradigm Programming A programming paradigm is a style to represent and organize computations. For example, logic programming [5] realises computations as deductions over a knowledge base formed by facts (also known as extensional knowledge) and rules (intensional knowledge) representing relations on which queries can be issued; object-oriented programming [6] organises computational activities into hierarchies of entities (objects) which encapsulate state and methods to manipulate it. Multi-paradigm programming is the integration of two or more paradigms in a unique model [7] that may be realised within a single language by following a multi-style approach, or within a system by following a multi-language approach. tuProlog supports multi-paradigm programming by integrating the logic and object-oriented paradigms of Prolog and Java at two different levels [4]: at the system level, where Java and Prolog can be combined to build tuProlog libraries, which can therefore be designed taking the best of both worlds; and at the application level, where bidirectional Java/Prolog integration can enable both 3 http://tuprolog.alice.unibo.it Java items to be effectively accessed from a Prolog program and Prolog engines to be dynamically exploited from a Java program. Multi-paradigm programming achieved by integrating two or more languages needs to be directly supported in its own right by a development environment, next to tools provided for the combined languages on their own. In the case of tuProlog, the integration at the application level is managed by the environments dedicated to each language, granting access to Java resources on the Prolog side and to Prolog engines on the Java side. Instead, the combination of Java and Prolog at the system level needs to be explicitly supported as a multi-paradigm programming feature in an IDE, supporting the creation of libraries and making them available within the environment. Thus, an IDE designed to enable multiparadigm Java/Prolog programming based on tuProlog has to supply tools for: (i) Java development; (ii) Prolog development; (iii) supporting the integration of the two languages by exploiting the tuProlog library mechanism. The Java Development Tools (JDT) available within the Eclipse platform already deliver a set of high quality instruments that help developers deal with the many activities of Java programming. On the Prolog side, the objective is to provide a comparable set of tools for tasks related to logic programming. Such a set would include wizards for the creation of the basic building blocks of Prolog software: new projects, as containers of all the resources needed for a certain application; new logic theories/modules, and new predicates that would also possibly need a visibility indicator in order to be imported/exported. Modules and theories need to be navigated with ease independently from the amount of code they contain: for this purpose, an outline view should provide a list of defined predicates (further subdivided in facts and rules) next to another view integrating cross-reference functionalities, so as to allow developers to reach the clauses of a predicate used in a theory but possibly defined in a different module. Editing Prolog programs should be supported by syntax highlighting, and by a code completion feature triggered to automatically enter the invocation of a predicate and step through the insertion of parameters by means of a template. General useful mechanisms for code editing include checking for predicates defined in some module but not used anywhere in the application, and for invoked predicates that have never been defined, alongside automatic importing management for any given theory. Syntax errors and possible suggestions for corrections should be indicated on the fly, while the developer is typing code in the editor. Among syntax warnings, specific logic programming issues should be considered, such as the use of singleton variables in the definition of a predicate clause. More advanced features include refactoring and debugging. Refactoring is a technique for improving code design without altering its external behaviour; such technique is a preliminary step to take before performing changes required to add new functionalities or improve efficiency [8]. From the object-oriented field, this practice recently came to prominence for Prolog programming [9]. Similarly to the JDT, Prolog development tools should include a facility to automate code refactoring, from simple renaming to predicate extraction, up to more complex transformations involving cut, if-then-else, and unification. Debugging should Fig. 1. The design of Java/Prolog multi-paradigm environment based on the Eclipse platform. Components of the final tuProlog plug-in are underlined by white background. exploit the instruments supplied by the Eclipse platform to improve the current state of inspection and manipulation of the demonstration process, also allowing dynamic configuration of the tool, possibly by means of a special logic theory. The Java/Prolog multi-paradigm programming support has to be placed on top of the JDT and the above-cited set of Prolog development tools. The integration between the Java side and the Prolog side of the environment needs to be first performed at the nature level. A nature is used to configure the capabilities of a project, typically to associate behaviour and functionalities in the form of builders that process modified files at specific times [10]. The Eclipse concept of natures is the straightforward counterpart of the multiple traits of a programming system involving more than one paradigm. Accordingly, next to the Java and Prolog nature in a multi-paradigm project, we need to p	admissible numbering;c++;code refactoring;command-line interface;computation;conditional (computer programming);cross-reference;debugging;declarative programming;display resolution;download;eclipse;embedded system;entity;fortran;integrated development environment;intensional logic;interaction;java;knowledge base;library (computing);logic programming;niche blogging;on the fly;open-source software;php;plug-in (computing);programming language;programming paradigm;programming tool;prolog;prototype;python;requirement;scripting language;seamless3d;software build;spawn (computing);syntax highlighting;text-based (computing);theory (mathematical logic);tracing (software);unification (computer science);user interface;tuprolog	Giulio Piancastelli;Enrico Denti	2008			systems engineering;development environment;computer science	SE	-27.842246134448693	27.70722156170641	63763
9a71e2bc1a80481b2222b8f565079f0484322366	garbage collection and run-time typing as a c++ library	garbage collection	"""provide garbage collection. This paper proposes the use of \smart pointer"""" template classes as an interface for the use of garbage collection in C ++ . Template classes and operator overloading are techniques allowing language extension at the level of user code; I claim that using these techniques to create smart pointer classes provides a syntax for manipulating garbage-collected storage safely and conveniently. Further, the use of a smart-pointer template class o ers the possibility of implementing the collector at the user-level, without requiring support from the compiler. If such a compiler-independent implementation is possible with adequate performance, then programmers can start to write code using garbage collection without waiting for language and compiler modi cations. If the use of such a garbage-collection interface becomes widespread, then C ++ compilation systems can be built to specially support the garbage collection interface, thereby allowing the use of collection algorithms with enhanced performance. This paper presents such a garbage collection interface and implementation. In particular, the collection scheme has the following properties:"""	algorithm;c++;compiler;garbage collection (computer science);generic programming;operator overloading;pointer (computer programming);programmer;reference counting;typing;user space	David Detlefs	1992			syntax;programming language;compiler;smart pointer;typing;bitwise operation;garbage collection;computer science	PL	-24.758234096599377	27.844797010769575	63835
bd78898d5ff148fe86875cb521534831eae24089	from msc and uml to sdl	automatic code generation;class diagram;distributed system;formal specification;software architecture specification languages formal specification object oriented programming distributed processing diagrams;distributed processing;diagrams;object oriented programming;formal semantics;software architecture;specification languages;software architecture msc uml sdl itu standard telecommunications software specification specification language formal semantics automatic code generation software tools distributed systems message sequence charts unified modeling language class diagrams object diagrams;unified modeling language computer architecture concrete software standards formal languages system testing telecommunication computing research and development telecommunication standards computational modeling;software specification	UML and MSC are widely used by software practitioners. SDL is an ITU standard language for telecommunications software specification. It has a formal semantics, and is supported by several commercial tools, which allow for simulation and validation of SDL design specifications as well as automatic code generation from these specifications. In order to take advantages of the SDL available tools and still use the very popular notations such as UML and MSC, we have developed an approach for specifying distributed systems in UML and MSC. MSCs are used to specify the behavior (scenarios) of the distributed system under consideration. UML, more precisely Class Diagrams and Object Diagrams, are used to specify the architecture of the system. By architecture, we mean the different components of the system and their interconnections. The MSC and UML specifications are translated automatically into a full SDL specification. In this paper, we introduce our distributed system specification style in UML and its automatic translation into an SDL architecture.	automatic programming;code generation (compiler);deadlock;diagram;distributed computing;formal specification;machine translation;semantics (computer science);simple directmedia layer;simulation;source-to-source compiler;syntax-directed translation;systems architecture;unified modeling language	Stephan Bourduas;Ferhat Khendek;Daniel Vincent	2002		10.1109/CMPSAC.2002.1044546	software architecture;model-driven architecture;software requirements specification;rm-odp;formal methods;specification language;uml tool;computer science;systems engineering;diagram;software engineering;applications of uml;class diagram;formal semantics;formal specification;diagramming software;programming language;object-oriented programming;language of temporal ordering specification;object constraint language	SE	-33.51779939078673	32.03113760030711	63905
943994988daf76fcd270c95e473a63c57546b365	verchor: a framework for the design and verification of choreographies	analytical models;protocols;semantics;contracts;skeleton;protocols unified modeling language logic gates contracts skeleton analytical models semantics;logic gates;choreography intermediate format verchor framework choreography design choreography verification contracts legal interactions concurrent distributed system top down approach bottom up approach generic modular extensible framework eclipse bpmn 2 0 designer cadp verification toolbox formal verification formal properties choreography description languages cif;unified modeling language;formal verification business data processing contracts formal specification	Choreographies are contracts specifying from a global point of view the legal interactions that must take place among a set of services. Such a contract may serve as a reference in the development of concurrent distributed system, whether it is achieved following a top-down or a bottom-up approach. In this article, we present VerChor, a generic, modular, and extensible framework for supporting the development based on choreographies. It relies on a choreography intermediate format (CIF) into which several existing choreography description languages can be transformed. VerChor builds around a set of formal properties whose verification is central to choreography-based development. To support this development process, we propose a connection between CIF and the CADP verification toolbox, which enables the full automation of the aforementioned properties. Finally, we illustrate a practical use of the VerChor framework through its integration with the Eclipse BPMN 2.0 designer.	bottom-up proteomics;construction and analysis of distributed processes;distributed computing;eclipse;interaction;server message block;top-down and bottom-up design	Matthias Güdemann;Pascal Poizat;Gwen Salaün;Lina Ye	2016	IEEE Transactions on Services Computing	10.1109/TSC.2015.2413401	unified modeling language;communications protocol;real-time computing;logic gate;computer science;database;semantics;programming language;skeleton	Visualization	-33.60179290482156	31.679549415278593	64085
280702dfd9939ee3fc236779253871a8ad430eaa	verifying haskell programs using constructive type theory	verification;proof assistant;publikationer;konferensbidrag;ghc core;functional programming;dependent type theory;polymorphism;type theory;monadic translation;artiklar;rapporter;partiality;functional programming language;haskell;type system	Proof assistants based on dependent type theory are closely related to functional programming languages, and so it is tempting to use them to prove the correctness of functional programs. In this paper, we show how Agda, such a proof assistant, can be used to prove theorems about Haskell programs. Haskell programs are translated into an Agda model of their semantics, by translating via GHC's Core language into a monadic form specially adapted to represent Haskell's polymorphism in Agda's predicative type system. The translation can support reasoning about either total values only, or total and partial values, by instantiating the monad appropriately. We claim that, although these Agda models are generated by a relatively complex translation process, proofs about them are simple and natural, and we offer a number of examples to support this claim.	agda;automated theorem proving;clutter;coinduction;correctness (computer science);dependent type;functional programming;impredicativity;intuitionistic type theory;monad (functional programming);norm (social);plug-in (computing);programming language;proof assistant;recursion;the glorious glasgow haskell compilation system;type system	Andreas Abel;Marcin Benke;Ana Bove;John Hughes;Ulf Norell	2005		10.1145/1088348.1088355	polymorphism;verification;type system;computer science;termination analysis;proof assistant;programming language;functional programming;type theory;algorithm	PL	-21.388520793106114	25.488711354274052	64484
9857e35b4297c1f7dad06797a5c66563053de207	computational pricing in internet era		Pricing plays a central rule to a company’s profitability, and therefore has been extensively studied in the literature of economics. When designing a pricing mechanism/ model, an important principle to consider is “price discrimination”, which refers to selling the same resources with different prices according to different values of buyers. To meet the “price discrimination” principle, especially when the number of buyers is large, computational methods, which act in a more accurate and principled way, are usually needed to determine the optimal allocation of sellers’ resources (whom to sell to) and the optimal payment of buyers (what to charge). Nowadays, in the Internet era in which quite a lot of buy and sell processes are conducted through Internet, the design of computational pricing models faces both new challenges and opportunities, considering that (i) nearly realtime interactions between people enable the buyers to reveal their needs and enable the sellers to expose their information in a more expressive manner, (ii) the large-scale interaction data require powerful methods for more efficient processing and enable the sellers to model different buyers in a more precise manner. In this paper, we review recent advances on the analysis and design of computational pricing models for representative Internet industries, e.g., online advertising and cloud computing. In particular, we introduce how computational approaches can be used to analyze buyer’s behaviors (i.e., equilibrium analysis), improve resource utilization (i.e., social welfare analysis), and boost seller’s profit (i.e., revenue analysis). We also discuss how machine learning techniques can be used to better understand buyer’s behaviors and design more effective pricing mechanisms, given the availability of large scale data. Moreover, we make discussions on future research directions on computational pricing, which hopefully can inspire more researchers to contribute to this important domain.	algorithm;cloud computing;computation;deep learning;game theory;interaction;internet;machine learning;mathematical optimization;nist hash function competition;online advertising;real-time computing;real-time transcription;sharing economy	Fei Tian;Tao Qin;Tie-Yan Liu	2017	Frontiers of Computer Science	10.1007/s11704-017-6005-0	the internet;online advertising;marketing;machine learning;artificial intelligence;computer science;cloud computing;mechanism design;price discrimination;payment;revenue;computer security;profitability index	Metrics	-33.2294274698578	20.438386175191383	64636
0c4f0793e186fb68ac01581f48f4b5ecf4bdd11a	declarative debugging of lazy functional programs	lazy evaluation;functional programming;functional programming language	We show how declarative (or algorithmic) debugging can be applied to lazy functional programming and describe a prototype implementation. We rst present a declarative debugger for logic programs which relies on three primitives that determine if an atom is valid in the intended interpretation, return the successful clause instance used by a call and return single atoms from a conjunction of atoms. By simply using diierent interpretations and versions of these three primitives the debugger can be used to locate errors in strict and non-strict functional programs. Debugging strict code is essentially the same as debugging Horn clause programs. Debugging non-strict code can result in questions containing unevaluated expressions. These questions can be simpliied by using quantiied variables. The prototype system is based on NUE-Prolog, which supports a functional language on top of NU-Prolog and is implemented using the attening transformation. The implementation has been modiied so functional code returns a representation of the computation which is then passed to the debugger. The basic algorithm is applicable to other functional programming languages also.	algorithm;algorithmic program debugging;computation;conditional (computer programming);correctness (computer science);debugger;declarative programming;functional programming;higher-order function;horn clause;interpretation (logic);lazy evaluation;logic programming;partial evaluation;pattern matching;programming language;programming paradigm;prolog;prototype;strict function;top-down and bottom-up design	Lee Naish	1992			strict programming language;very high-level programming language;algorithmic program debugging;computer architecture;lazy initialization;programming language;declarative programming;programming paradigm;fifth-generation programming language;lazy evaluation;computer science	PL	-20.287941195812788	23.98957076935769	64708
2836e0d79f07b2a56f2422be615d89edafbb1c79	automatic program annotation	automatic programming	Techniques were developed by which an Algol-like program, given together with its specifications, may be documented automatically. This documentation expresses invariant relationships that hold between program variables at intermediate points in the program, and explains the actual workings of the program regardless of whether the program is correct. These techniques, formulated as deduction rules for both guaranteed invariants and candidate invariants, represent a unification of existing approaches, and sometimes improve upon them.	algol;documentation;invariant (computer science);natural deduction;unification (computer science)	Nachum Dershowitz	1977			program analysis;computer science;theoretical computer science;database;programming language	SE	-21.17963947039918	26.286802082512864	64863
af3e4aeaa18cc77b86b210aea83af86854473606	the semantic foundations and a landscape of cache-persistence analyses		We clarify the notion of cache persistence and contribute to the understanding of persistence analysis for caches with least-recently-used replacement. To this end, we provide the first formal definition of persistence as a property of a trace semantics. Based on this trace semantics we introduce a semantics-based, i.e., abstract-interpretation-based persistence analysis framework. We identify four basic persistence analyses and prove their correctness as instances of this analysis framework. Combining these basic persistence analyses via two generic cooperation mechanisms yields a lattice of ten persistence analyses. Notably, this lattice contains all persistence analyses previously described in the literature. As a consequence, we obtain uniform correctness proofs for all prior analyses and a precise understanding of how and why these analyses work, as well as how they relate to each other in terms of precision. 2012 ACM Subject Classification Computer systems organization → Real-time system architecture, Theory of computation → Caching and paging algorithms, Hardware → Safety critical systems	abstract interpretation;algorithm;cpu cache;correctness (computer science);denotational semantics;operational semantics;paging;persistence (computer science);real-time operating system;real-time transcription;systems architecture;theory of computation	Jan Reineke	2018	LITES	10.4230/LITES-v005-i001-a003	cache;mathematical proof;correctness;theoretical computer science;computer science;distributed computing;semantics;persistence (computer science)	PL	-21.863587475971666	30.319343580760748	65055
798372a795d542d4a5fc4df4b1df38babc62b9c6	functional interfaces vs. function types in java with lambdas		In several steps the Java type system is extended by features, which we know from functional programming languages. In Java 5.0 [GJSB05] generic types as well as a limited form of existential types (bounded wildcards) are introduced. In Java 8 the language will be expanded by lambda expressions, functional interfaces as target types, method and constructor references and default methods. In an earlier approach real function types were introduced as types of lambdas. The function types have been replaced by functional interfaces (interfaces with one method).	function type;functional programming;java version history;lambda calculus;programming language;type system;wildcard character	Martin Plümicke	2014			programming language;lambda;wildcard character;bounded function;functional programming;real-valued function;expression (mathematics);java;computer science	PL	-22.815773660932543	24.846023038723843	65081
8005096bffc0a3146ba65b44b2724da98138961d	function definition language fdl and its implementation	formal specification;programming language;context free language;data type;parsing;recursive function;pattern matching;interpreter;functional programming language	A Function Definition Language (FDL) is presented. Though designed for describing specifications, FDL is also a general-purpose functional programming language. It uses context-free language as data type, supports pattern matching definition of functions, offers several function definition forms, and is executable. It is shown that FDL has strong expressiveness, is easy to use and describes algorithms concisely and naturally. An interpreter of FDL is introduced. Experiments and discussion are included.	algorithm;context-free language;executable;experiment;functional programming;gnu free documentation license;general-purpose modeling;pattern matching;programming language	Haiming Chen	1999	Journal of Computer Science and Technology	10.1007/BF02948745	natural language processing;first-generation programming language;very high-level programming language;interpreter;language primitive;specification language;data type;computer science;parsing;pattern matching;formal specification;context-free language;programming language;functional programming;programming language specification;algorithm	PL	-24.449929342031073	23.02476903494284	65168
137ed53c53793f1b3ffebbb925f103a4d7012b4a	embedding a tcl web server into apl	command language;source code;scripting language	Tcl  stands for  Tool Command Language.  It is a script language, but unlike the well-known  perl,  it is also an extensible interpreter that is designed to be easily embedded into other applications. The Tcl source code is available from the Internet and the package can be used freely even in commercial applications. In addition to its condensed notation and simple syntax, APL's strength is its powerful array handling capacity. Tcl is based on string handling and very strong on gluing different pieces of software together. It is especially the many available Tcl extensions for GUI, network, database, HTML and XML support, which make the combination of APL and Tcl very attractive.With the implementation of a few new APL system commands into the APL2C system, it is possible to create one or several Tcl interpreters from within APL. As each of the Tcl interpreters runs on its own thread, a synchronization mechanism is provided for the communication between APL and Tcl. As an example, a Tcl web server with an APL backbone has been realized to demonstrate the power of the combination between APL and Tcl.	apl;server (computing);tcl;web server	Tilman P. Otto	2000	ACM SIGAPL APL Quote Quad	10.1145/570406.570415	computer science;operating system;database;scripting language;programming language;source code	Theory	-30.81698049027548	28.16720582423626	65180
2be8b94e1317a922df15e2bb90d039f0fe3534af	generating transformers for deforestation and supercompilation	program specialization;first order;partial evaluation;generic programming	Our aim is to study how the interpretive approach — inserting an interpreter between a source program and a program specializer — can be used to improve the transformation of programs and to automatically generate program transformers by self-application of a program specializer. We show that a few semantics-preserving transformations applied to a straightforward interpretive definition of a first-order, call-by-name language are sufficient to generate Wadler’s deforestation algorithm and a version of Turchin’s supercompiler using a partial evaluator. The transformation is guided by the need to binding-time improve the interpreters.	algorithm;andrei broder;bootstrapping (compilers);cdr coding;constructor (object-oriented programming);divergence (computer science);first-order predicate;interpreter (computing);jones calculus;latex;metacompilation;name binding;partial evaluation;partial template specialization;refal;tame;transformer;transformers;ucph department of computer science	Robert Glück;Jesper Jørgensen	1994		10.1007/3-540-58485-4_57	computer science;theoretical computer science;first-order logic;programming language;generic programming;partial evaluation;algorithm	PL	-21.091239109977256	24.938651312850233	65225
874f46d4c55ad157f34757c374239464a1ca21b0	ferry: database-supported program execution	ferry;relational database;sql 1999;execution environment;pathfinder;linq;type system	We demonstrate the language Ferry and its editing, compilation, and execution environment FerryDeck. Ferry's type system and operations match those of scripting or programming languages; its compiler has been designed to emit (bundles of) compliant and efficient SQL:1999 statements. Ferry acts as glue that permits a programming style in which developers access database tables using their programming language's own syntax and idioms -- the Ferry-expressible fragments of such programs may be executed by a relational database back-end, i.e., close to the data. The demonstrator FerryDeck implements compile-and-execute-as-you-type interactivity for Ferry and offers a variety of (graphical) hooks to explore and inspect this approach to database-supported program execution.	compiler;graphical user interface;interactivity;programming language;programming style;relational database;sql:1999;table (database);type system	Torsten Grust;Manuel Mayr;Jan Rittinger;Tom Schreiber	2009		10.1145/1559845.1559982	type system;relational database;computer science;database;language integrated query;programming language;world wide web	PL	-28.039457455106305	26.460882399940633	65404
3c92252496d2e61b7934a96f4cecaae9439d642d	static validation of dynamically generated html documents based on abstract parsing and semantic processing		Abstract parsing is a static-analysis technique for a program that, given a reference LR(k) context-free grammar, statically checks whether or not every dynamically generated string output by the program conforms to the grammar. The technique operates by applying an LR(k) parser for the reference language to data-flow equations extracted from the program, immediately parsing all the possible string outputs to validate their syntactic well-formedness. In this paper, we extend abstract parsing to do semantic-attribute processing and apply this extension to statically verify that HTML documents generated by JSP or PHP are always valid according to the HTML DTD. This application is necessary because the HTML DTD cannot be fully described as an LR(k) grammar. We completely define the HTML 4.01 Transitional DTD in an attributed LALR(1) grammar, carry out experiments for selected real-world JSP and PHP applications, and expose numerous HTML validation errors in the applications. In the process, we experimentally show that semantic properties defined by attribute grammars can also be verified using our technique.	application domain;attribute grammar;context-free grammar;context-free language;data-flow analysis;dataflow;dynamic web page;experiment;html;javaserver pages;lalr parser;lr parser;php;parsing;static program analysis	Hyunha Kim;Kyung-Goo Doh;David A. Schmidt	2013		10.1007/978-3-642-38856-9_12	natural language processing;parser combinator;computer science;parsing;s-attributed grammar;database;programming language;top-down parsing	PL	-27.073712368700622	27.340052767002206	65425
f68512a55f3edc998ce1723f8b82399ee7bf9f8d	extending the concepts of object role modeling to capture natural language semantics for database access	object role modeling		natural language;object-role modeling	Frank Shou-Cheng Tseng;Teng-Kai Fan	2005			object definition language;modeling language;natural language programming;computational semantics;object constraint language;natural language processing;object language;database;artificial intelligence;formal semantics (linguistics);operational semantics;computer science	DB	-26.051506498110435	19.34411028466569	65698
94602337ce36c5d864912ebccf432d1d220913ad	a monadic interpretation of execution levels and exceptions for aop	monads;conceptual understanding;aop;large scale;category theory;aspect oriented programming;theoretical foundation;parc;execution levels	Aspect-Oriented Programming (AOP) started fifteen years ago with the remark that modularization of so-called crosscutting functionalities is a fundamental problem for the engineering of large-scale applications. Originating at Xerox PARC, this observation has sparked the development of a new style of programming features that is gradually gaining traction. However, theoretical foundations of AOP have been much less studied than its applicability. This paper proposes to put a bridge between AOP and the notion of 2-category to enhance the conceptual understanding of AOP. Starting from the connection between the λ-calculus and the theory of categories, we provide an internal language for 2-categories and show how it can be used to define the first categorical semantics for a realistic functional AOP language, called MinAML. We then take advantage of this new categorical framework to introduce the notion of computational 2-monads for AOP. We illustrate their conceptual power by defining a 2-monad for Éric Tanter's execution levels---which constitutes the first algebraic semantics for execution levels---and then introducing the first exception monad transformer specific to AOP that gives rise to a non-flat semantics for exceptions by taking levels into account.	a-normal form;algebraic semantics (computer science);algorithm;aspect-oriented programming;cartesian closed category;categorical logic;category theory;computation;exception handling;keystone effect;lambda calculus;linear algebra;monad (functional programming);monad transformer;program transformation;rewriting;software engineering;traction teampage	Nicolas Tabareau	2012		10.1145/2162049.2162059	aspect-oriented programming;computer science;programming language;algorithm	PL	-32.95360391301706	28.682241999967065	65805
45ea7ff126c9a73da155314041de4b87509750b6	guiliner: a configurable and extensible graphical user interface for scientific analysis and simulation software	scientific application;front end;computer program;human computer interaction;software engineering;graphical user inter face;simulation software;data analysis;command line interface;graphic user interface;computer application;user interaction	The computer programs most users interact with daily are driven by a graphical user interface (GUI). However, many scientific applications are used with a command line interface (CLI) for the ease of development and increased flexibility this mode provides. Scientific application developers would benefit from being able to provide a GUI easily for their CLI programs, thus retaining the advantages of both modes of interaction. GuiLiner is a generic, extensible and flexible front-end designed to “host” a wide variety of data analysis or simulation programs. Scientific application developers who produce a correctly formatted XML file describing their program’s options and some of its documentation can immediately use GuiLiner to produce a carefully implemented GUI for their analysis or simulation programs. Key-Words : Graphical user interfaces, XML, Computer applications, Software interfaces	command-line interface;computer program;documentation;graphical user interface;simulation software	Nicholas Manoukis;E. C. Anderson	2008	CoRR		user;command-line interface;10-foot user interface;interface metaphor;human–computer interaction;simulation software;computer science;front and back ends;operating system;console application;graphical user interface;natural user interface;data analysis;user interface;graphical user interface testing	SE	-31.740279259382966	27.737314689237234	65818
832b7e83ec53a2ede6827fafaa4596e88d5c8c29	verification of code generators via higher-order model checking	verification;higher order model checking;code generation	Dynamic code generation is useful for optimizing code with respect to information available only at run-time. Writing a code generator is, however, difficult and error prone. We consider a simple language for writing code generators and propose an automated method for verifying code generators. Our method is based on higher-order model checking, and can check that a given code generator can generate only closed, well-typed programs. Compared with typed multi-stage programming languages, our approach is less conservative on the typability of generated programs (i.e., can accept valid code generators that would be rejected by typical multi-stage languages) and can check a wider range of properties of code generators. We have implemented the proposed method and confirmed its effectiveness through experiments.	code generation (compiler);cognitive dimensions of notations;experiment;just-in-time compilation;model checking;multi-stage programming;programmer;programming language;prototype;self-modifying code;type inference;verification and validation	Takashi Suwa;Takeshi Tsukada;Naoki Kobayashi;Atsushi Igarashi	2017		10.1145/3018882.3018886	code word;dead code;systematic code;code bloat;verification;object code;computer science;theoretical computer science;dead code elimination;redundant code;code coverage;locally testable code;programming language;algorithm;code generation;unreachable code;source code	SE	-21.64632748398328	28.421396374491923	65933
48e5372a4ba7e2f4ffd2b8c065aa21ad2f908930	embedding the script language into optical simulation software		Integration of the Python script language into computer-aided design and simulation applications is considered. By the example of the physically correct optical simulation application, an approach based on the unified entity interface for domain objects is proposed. In particular, a scheme is proposed to extend the application with new types of parametric objects by writing extension classes that are simple in structure. Special attention is paid to making API user-friendly and ensuring compliance with the object-oriented programming principles.	application programming interface;archicad library part;business object;computer-aided design;daylight;domain-driven design;front panel;geographic coordinate system;human factors and ergonomics;liquid-crystal display;microsoft windows;multi-compartment model;python;scene graph;scripting language;simulation software;usability	N. B. Deryabin;D. D. Zhdanov;V. G. Sokolov	2017	Programming and Computer Software	10.1134/S0361768817010029	computer science;theoretical computer science;database;programming language;algorithm	EDA	-30.132450667461196	28.869553238930166	65942
8b53bb0dab36fd59de8a3c6a53c1f2e722ebe3bb	database programming with jdbc and java - java 1.1			jdbc;java version history	George Reese	1997			java data objects;java api for xml-based rpc;jsr 94;jar;java modeling language;strictfp;real time java;programming language;java;generics in java;scala;java applet;java annotation;non-blocking i/o	PL	-29.622753719012806	28.21455277625556	66044
ef4a860ceee5f0e8cb6e27c74c4d18ed42fa589a	parsing algorithms with backtrack	finite element methods;complexity theory;magnetic heads;top down;context free language;context modeling writing magnetic heads;automata;random access machine;artificial neural networks;registers;linear time;writing;artificial intelligence;context modeling;integrated circuits	Packrat parsing is a novel and practical method for implementing linear-time parsers for grammars defined in Top-Down Parsing Language (TDPL). While TDPL was originally created as a formal model for top-down parsers with backtracking capability, this thesis extends TDPL into a powerful general-purpose notation for describing language syntax, providing a compelling alternative to traditional context-free grammars (CFGs). Common syntactic idioms that cannot be represented concisely in a CFG are easily expressed in TDPL, such as longest-match disambiguation and “syntactic predicates,” making it possible to describe the complete lexical and grammatical syntax of a practical programming language in a single TDPL grammar. Packrat parsing is an adaptation of a 30-year-old tabular parsing algorithm that was never put into practice until now. A packrat parser can recognize any string defined by a TDPL grammar in linear time, providing the power and flexibility of a backtracking recursive descent parser without the attendant risk of exponential parse time. A packrat parser can recognize any LL(k) or LR(k) language, as well as many languages requiring unlimited lookahead that cannot be parsed by shift/reduce parsers. Packrat parsing also provides better composition properties than LL/LR parsing, making it more suitable for dynamic or extensible languages. The primary disadvantage of packrat parsing is its storage cost, which is a constant multiple of the total input size rather than being proportional to the nesting depth of the syntactic constructs appearing in the input. Monadic combinators and lazy evaluation enable elegant and direct implementations of packrat parsers in recent functional programming languages such as Haskell. Three different packrat parsers for the Java language are presented here, demonstrating the construction of packrat parsers in Haskell using primitive pattern matching, using monadic combinators, and by automatic generation from a declarative parser specification. The prototype packrat parser generator developed for the third case itself uses a packrat parser to read its parser specifications, and supports full TDPL notation extended with “semantic predicates,” allowing parsing decisions to depend on the semantic values of other syntactic entities. Experimental results show that all of these packrat parsers run reliably in linear time, efficiently support “scannerless” parsing with integrated lexical analysis, and provide the user-friendly error-handling facilities necessary in practical applications. Thesis Supervisor: M. Frans Kaashoek Title: Professor of Computer Science and Engineering	algorithm;backtrack;backtracking;combinatory logic;compiler-compiler;computer engineering;context-free grammar;context-free language;entity;extensible programming;functional programming;general-purpose markup language;haskell;information;java;ll parser;lr parser;lazy evaluation;lexical analysis;mathematical model;parsing expression grammar;pattern matching;programming language;prototype;recursion;recursive descent parser;scannerless parsing;syntactic predicate;table (information);time complexity;top-down and bottom-up design;top-down parsing language;usability;word-sense disambiguation	Alexander Birman;Jeffrey D. Ullman	1970		10.1109/SWAT.1970.18	time complexity;combinatorics;computer science;theoretical computer science;machine learning;finite element method;top-down and bottom-up design;context-free language;automaton;context model;processor register;programming language;writing;artificial neural network;algorithm	PL	-27.149590162675263	20.07460443383886	66095
17a2e5550d7039d9ea8e641ad80fc47110f0899a	modeling and reasoning with multirelations, and their encoding in alloy		Multisets and multirelations arise naturally in modeling. In this paper, we present a sound and practical mathematical framework, which encodes multisets and multirelations using only ordinary sets and total functions. We implement the encoding as a mutliconcepts library in Alloy, which is declarative, compatible with ordinary sets and relations, and can be incorporated into existing models seamlessly.	alloy (specification language);blueprint;graphical user interface;modeling language;satisfiability modulo theories	Peiyuan Sun;Zinovy Diskin;Michal Antkiewicz;Krzysztof Czarnecki	2016			alloy;theoretical computer science;encoding (memory);mathematics	SE	-21.958122936607374	18.793430726019512	66118
08fdccf21c3ccadbfa467138cf9359a1da95b31a	belief/goal sharing bdi modules	agent programming languages;modularity;bdi	This paper proposes a modularisation framework for BDI based agent programming languages developed from a software engineering perspective. Like other proposals, BDI modules are seen as encapsulations of cognitive components. However, unlike other approaches, modules are here instantiated and manipulated in a similar fashion as objects in object orientation. In particular, an agent’s mental state is formed dynamically by instantiating and activating BDI modules. The agent deliberates on its active module instances, which interact by sharing their beliefs and goals. The formal semantics of the framework are provided and some desirable properties of the framework are shown.	2apl;algorithm;algorithmic efficiency;black box;categorization;encapsulation (networking);genetic programming;killer application;mental state;programmer;programming language;public interface;rule-based system;run time (program lifecycle phase);self-modifying code;semantics (computer science);separation of concerns;software engineering;stream processing;web ontology language	Michal Cáp;Mehdi Dastani;Maaike Harbers	2011			computer science;knowledge management;artificial intelligence;modularity	SE	-28.703945002988174	29.81926429799819	66124
6e625484dddd6ac0a8a20ce7c7340e70376cf373	a discrete geometric model of concurrent program execution		A trace of the execution of a concurrent object-oriented program can be displayed in two-dimensions as a diagram of a non-metric finite geometry. The actions of a programs are represented by points, its objects and threads by vertical lines, its transactions by horizontal lines, its communications and resource sharing by sloping arrows, and its partial traces by rectangular figures.	concurrent computing;geometric modeling	Bernhard Möller;C. A. R. Hoare;Martin Eric Müller;Georg Struth	2016		10.1007/978-3-319-52228-9_1	computer architecture;parallel computing;real-time computing	Logic	-29.657428425395683	25.923907682184456	66210
df33f8def56940660b5b7c5901bfbda5eea14e6f	temporal logic and applications-a tutorial	verification;distributed system;logica formal;syntax;systeme reparti;logica temporal;temporal logic;systeme simultane;sistema informatico;specification;semantics;simultaneidad informatica;computer system;syntaxe;semantica;semantique;concurrency;sistema repartido;especificacion;formal logic;systeme informatique;verificacion;sintaxis;logique formelle;simultaneite informatique;logique temporelle	Gotzhein, R., Temporal logic and applications--a tutorial, Computer Networks and ISDN Systems 24 (1992) 203-218. Temporal logic is a branch of formal logic which is of particular interest for the specification and verification of concurrent systems including communication services and protocols. This tutorial presents the principles of temporal logic and explains how it can be applied.	computer networks (journal);concurrency (computer science);integrated services digital network;temporal logic	Reinhard Gotzhein	1992	Computer Networks and ISDN Systems	10.1016/0169-7552(92)90109-4	description logic;verification;syntax;concurrency;temporal logic;interval temporal logic;computation tree logic;computer science;artificial intelligence;bunched logic;semantics;signature;logic;specification;multimodal logic;algorithm;temporal logic of actions	Logic	-25.165092186498015	31.88311063305288	66369
332a45ad1d119302315bdf69085a3d8f94486576	on the semantics of natural language sentences in logic programming (panel session)	natural language		logic programming;natural language	Robin Cohen;Miguel Filgueiras;Martha Palmer;Patrick Saint-Dizier;Stan Szpakowicz	1987			first-generation programming language;very high-level programming language;computational semantics;natural language programming;programming language;natural language processing;functional logic programming;fifth-generation programming language;artificial intelligence;logic programming;well-founded semantics;computer science	AI	-25.181617307902386	19.512518889492828	66583
94a82f18fe3ce9168d36e71675bd76f2a46c2ec7	exception based modeling and forecasting.	modeling and forecasting	Introduction: Questions o How often does the need arise for modeling and forecasting? o Should it be done manually by ad-hoc by project requests or o Should it be done manually by ad-hoc, by project requests or automatically? o What tools and techniques are best for that? Answers o The capacity management system should automatically provide a small list of resources that needs to be modeled or forecasted; small list of resources that needs to be modeled or forecasted; then a simple spreadsheet tool can be used for forecasting. o This method is already implemented on the author's environment with thousands of servers.	hoc (programming language);server (computing);spreadsheet	Igor A. Trubin	2008			probabilistic forecasting;technology forecasting	Robotics	-32.711639078337	27.178358408980817	66692
7e134e5244a0fc0afab1ca8fef4ab7d9f9fc1e37	from words to wires by computer-aided design	computer aided design	Abstract   The paper describeds a laboratory procedure and a program which together simplify and accelerate the work of producing ‘one-off’ digital instruments and computer satellites.  The program checks the validity of a logic designer's scheme from a verbal description of it; detailed drawings need not be prepared since the program will produce all the necessary illustrations, component lists and wiring instructions. The computer will also control a semi-automatic wiring machine to produce the finished product.	computer-aided design	A. H. Evans;Peter Edmonds	1973	Computer-Aided Design	10.1016/0010-4485(73)90238-8	computer science;engineering;computer aided design;mathematics;engineering drawing;computer engineering;mechanical engineering	EDA	-30.868484230764672	22.538095398287684	66857
4ad6d010cdfffef1b4d4780667f1809c17d8edb2	formal definition of a general ontology pattern language using a graph grammar		In recent years, there has been a growing interest in the use of ontologica! theories in the philosophical sense (Foundational Ontologies) to analyze and (re)design conceptual modeling languages. This paper is about an ontologically well-founded conceptual modeling language in this tradition, termed OntoUML. This language embeds a number of ontological patterns that reflect the micro-theories comprising a particular foundational ontology named UFO. We here (re)define OntoUML as a formal graph grammar and demonstrate how the models of this language can be constructed by the combined application of ontological patterns following a number of graph transformation rules. As a result, we obtain a version of this language fully defined as a formal Ontology Pattern Grammar. In other words, this paper presents a formal definition of OntoUML that is both explicit in terms of the ontological patterns that it incorporates and is completely independent of the UML meta-model.	abstract syntax;design pattern;embedded system;experiment;formal ontology;general-purpose modeling;glossary of computer graphics;graph rewriting;mathematical model;metamodeling;mixin;ontouml;ontology (information science);pattern grammar;pattern language;semantics (computer science);theory;turing completeness;unified modeling language	Eduardo Zambon;Giancarlo Guizzardi	2017	2017 Federated Conference on Computer Science and Information Systems (FedCSIS)	10.15439/2017F001	pattern grammar;computer science;unrestricted grammar;attribute grammar;formal ontology;ontology (information science);grammar systems theory;natural language processing;regular grammar;pattern language;artificial intelligence	AI	-27.27270766728928	18.64689912563575	67037
999b6024a07030869225bd805725f7e1e9a24b21	interest-based mining and modeling of big mobile networks	mobile;wireless networks;ks test interest based mining mobile wireless internet mobile networks mobile internet characteristics network modeling generic traffic model campus wide wireless mobile network graph based approach;telecommunication computing big data data mining graph theory radio networks;internet;big data;mobile communication;traffic;ip networks;mobile communication mobile computing buildings internet ip networks wireless networks;interest;mobile computing;buildings	Usage of mobile wireless Internet has grown very fast in recent years. This radical change in availability of Internet has led to communication of big amount of data over mobile networks raising new challenges and opportunities for modeling of mobile Internet characteristics. While the traditional approach toward network modeling suggests finding a generic traffic model for the whole network, in this paper, we show that this approach does not provide enough accuracy for big mobile networks. We show that user interest acquired based on accessed domains and visited locations has a significant effect on traffic characteristics of big mobile networks. Our case study based on a big dataset including billions of netflow record collected from a campus-wide wireless mobile network reveals the fact that domains and locations showing similar point of interests (e.g. domains of news agencies or locations of fraternities) mostly follow similar types of traffic distributions. For this purpose, we utilize a novel graph-based approach based on KS-test. We also show that interest-based modeling of big mobile networks based on visited domains and locations can significantly improve the accuracy and reduce the KS distances by factor of 5 comparing to the generic approach.	internet;mobile phone;windows legacy audio components	Saeed Moghaddam;Ahmed Helmy	2015	2015 IEEE First International Conference on Big Data Computing Service and Applications	10.1109/BigDataService.2015.69	radio access network;mobile search;mobile web;imt advanced;internet access;public land mobile network;wireless application protocol;mobile database;engineering;mobile technology;internet privacy;mobile computing;world wide web;mobile communications over ip;computer network	Metrics	-29.6744573273077	19.7955423163695	67047
5987e93ebc1ce0803f9192f30bf4e3c2e2222637	proof general in eclipse: system and architecture overview	verification;eclipse plug in;timed model checking;theorem proving;obsslice;power generation;timed automata;vts;lapsus;zeus;program development;interactive theorem proving	Interactive theorem proving is the art of constructing electronic proofs. Proof development, based around a proof script, has much in common with program development, based around a program text. Proof developers use rather primitive tools for developing and manipulating proof scripts at present. The Proof General project aims at to change this, by providing powerful generic tools and interfaces. The flagship tool is our Eclipse plugin, which brings the features of a industrial-strength IDE to theorem proving for the first time. In this paper we give an overview of the Eclipse plugin and its underlying architecture.	automated theorem proving;eclipse;plug-in (computing);proof assistant	David Aspinall;Daniel Winterstein;Christoph Lüth;Ahsan Fayyaz	2006		10.1145/1188835.1188845	electricity generation;computer-assisted proof;verification;computer science;theoretical computer science;automated proof checking;automated theorem proving;proof assistant;programming language;algorithm;zeus	PL	-20.660298236204536	20.59793973918733	67170
fb9c69493e132e54e6a78d236bdb5cf0e18c18b8	verifiable semantic difference languages		Program differences are usually represented as textual differences on source code with no regard to its syntax or its semantics. In this paper, we introduce semantic-aware difference languages. A difference denotes a relation between program reduction traces. A difference language for the toy imperative programming language Imp is given as an illustration.  To certify software evolutions, we want to mechanically verify that a difference correctly relates two given programs. Product programs and correlating programs are effective proof techniques for relational reasoning. A product program simulates, in the same programming language as the compared programs, a well-chosen interleaving of their executions to highlight a specific relation between their reduction traces. While this approach enables the use of readily-available static analysis tools on the product program, it also has limitations: a product program will crash whenever one of the two programs under consideration crashes, thus making it unsuitable to characterize a patch fixing a safety issue.  We replace product programs by correlating oracles which need not be expressed in the same programming language as the compared programs. This allows designing correlating oracle languages specific to certain classes of program changes and capable of relating crashing programs with non-crashing ones. Thanks to oracles, the primitive differences of our difference language on Imp can be assigned a verifiable semantics. Besides, each class of oracles comes with a specific proof scheme which simplifies relational reasoning for a well-specified class of relations. We also prove that our framework is at least as expressive as several Relational Hoare Logic variants by encoding them as correlating oracles, (re)proving soundness of those variants in the process. The entirety of the framework as well as its instantiations have been defined and proved correct using the Coq proof assistant.	coq (software);formal verification;forward error correction;hoare logic;imperative programming;interface message processor;oracle database;oracle machine;patch (computing);programming language;proof assistant;static program analysis;tracing (software)	Thibaut Girka;David Mentré;Yann Régis-Gianas	2017		10.1145/3131851.3131870	programming language;oracle;theoretical computer science;static program analysis;imperative programming;computer science;source code;object-oriented programming;functional programming;hoare logic;proof assistant	PL	-20.863853730734256	27.624737715999263	67208
4a9f2b2c94efabf8adc1d0bb4677e8b0169f9212	jastadd - a java-based system for implementing front ends	algebraic specification;front end;object oriented language;code generation;abstract syntax tree;attribute grammar;type checking;object oriented;aspect oriented programming;design pattern;datavetenskap datalogi	We describe JastAdd, a Java-based system for specifying and implementing the frontend parts of a compiler that follow parsing. The system is built on top of a traditional Java parser generator which is used for parsing and treebuilding. JastAdd adds facilities for specifying and generating object-oriented abstract syntax trees with both declarative behavior (using Reference Attributedsyntax trees with both declarative behavior (using Reference Attributed Grammars (RAGs)) and imperative behavior (using ordinary Java code). The behavior can be modularized into different aspects, e.g. name analysis, type checking, etc., that are woven together into classes. This combination of objectoriented ASTs and aspect-modularized behavior results in a system which is easier and safer to use than solutions based on, e.g., the Visitor pattern. We also describe the implementation of the RAG evaluator (optimal recursive evaluation) which is implemented very conveniently using Java classes, interfaces, and virtual methods.	abstract syntax tree;booting;compiler;compiler-compiler;imperative programming;interpreter (computing);java;parsing;recursion;type system;visitor pattern	Görel Hedin;Eva Magnusson	2001	Electr. Notes Theor. Comput. Sci.	10.1016/S1571-0661(04)80920-4	computer science;theoretical computer science;java modeling language;real time java;programming language;object-oriented programming;java;algorithm;java annotation	PL	-27.506223621786198	27.108002923958733	67277
f679aa43cefb27c6c8a927d4a2e16a840f3cd48f	the design of cloud-based 4g/lte for mobile augmented reality with smart mobile devices	smart mobile device;broadband networks;smart phones;long term evolution;mobile communication computer architecture navigation mobile computing collaboration augmented reality mobile handsets;computer centres;wimax 4g mobile communication augmented reality broadband networks cloud computing computer centres long term evolution mobile computing smart phones;mobile augmented reality cloud computing smart mobile device 4g lte;proceedings paper;4g lte;4g mobile communication;augmented reality;wimax;mobile computing;mobile augmented reality;cloud computing	The system characteristics of 4G and beyond 4G broadband mobile system (BMS) are high data rate (throughput), low latency (delay), high mobility (speed), and high capacity. The current recognized 4G BMS needs to meet the requirements specified by IMT-Advanced of ITU-T. Those BMSs include 3GPP-LTE/LTE-Advanced and IEEE 802.16e/m (WiMAX 1/WiMAX 2). In the meantime, the smart device (smart phone and tablet) with powerful CPU/GPU, HD digital camera, digital compass, GPS, and various sensors are becoming rapidly popular. In addition, the architecture and capability of cloud computing are getting adopted in various applications and services, a cloud-based 4G/LTE is one example of telecommunications services. With the combination of more deployments of cloud-based BMSs and increasing usages of smart mobile devices, there are many potential appealing applications and services with real-time and/or interactive features can be created. In this article, we explore the technology and applications of mobile augmented reality (MAR) on the cloud-based 4G BMS (TD-LTE) and smart devices environment. The developed smart device-based MAR system (SD-MAR) with the 4G/TD-LTE experimental network test bed is located at MIRC/BML in the campus of National Chiao Tung University. This test bed consists of several brandy dongles/tablets/smartphones (as UE), two NSN TD-LTE base stations (as eNodeB), one core network (as EPC), and cloud-based servers and data center. To study the technology and applications on SD-MAR system, we have integrated research teams/people specialized in the areas of cloud computing, smart device technology, 4G broadband mobile system, computer vision and image processing, gesture recognition, computer graphics and rendering, and system integration. The applications discussed in the article include real-time accurate navigation/tourism for indoor and outdoor, collaborative urban design, and multiuser interactive motion learning system in the mobile environment.	augmented reality;battery management system;battle management language;central processing unit;cloud computing;compaq lte;computer graphics;computer vision;data center;data rate units;digital camera;dongle;electronic product code;gesture recognition;global positioning system;graphics processing unit;image processing;interactive machine translation;interactivity;mobile device;multi-user;real-time locating system;real-time transcription;rendering (computer graphics);requirement;sensor;smart device;smartphone;system integration;tablet computer;testbed;throughput	Bao-Shuh Paul Lin;Wen-Hsiang Tsai;C. C. Wu;P. H. Hsu;J. Y. Huang;Tsai-Hwa Liu	2013	2013 IEEE Seventh International Symposium on Service-Oriented System Engineering	10.1109/SOSE.2013.57	wimax;embedded system;augmented reality;simulation;cloud computing;computer science;engineering;operating system;multimedia;mobile computing;broadband networks	Mobile	-33.46334112953137	22.40610876245358	67290
2c1478ef32d3eea3a8fcec80983a0a5304a7cc91	using temporal logic for modular specification of telephone services	first order;temporal logic	We outline a methodology for the modular speciication of telephone services within rst-order linear-time temporal logic. Typically, the services ooered by a telephone system consist of a basic service and several optional additional services, such as automatic callback, redirection, etc. We argue informally that temporal logic provides a exible formalism for the speciication of individual services, and for the composition of diierent services. We present a style of speciication, in which the expected behavior of each additional service can be speciied independently of other services. In this style, it is straightforward to compose noninteracting services. We outline, by means of examples, how certain interactions between services that prescribe connicting behavior can manifest themselves as inconsistencies when the services are composed. We then outline how the resolution of such interactions can be described in the formalism.	callback (computer programming);interaction;linear temporal logic;redirection (computing);semantics (computer science)	Johan Blom;Bengt Jonsson;Lars Kempe	1994			interval temporal logic;feature interaction problem;linear temporal logic;multimodal logic;sequential logic;theoretical computer science;real-time computing;temporal logic;temporal logic of actions;modular design;computer science	SE	-32.8920842641534	30.65957014736486	67320
e109c27629025fc13d87b78a0421c505af1efca8	deductive program verification (a practitioner's commentary)	computer program;program verification;proof of correctness;software reliability	A proof of ‘correctness’ for a mathematical algorithm cannot be relevant to executions of a program based on that algorithm because both the algorithm and the proof are based on assumptions that do not hold for computations carried out by real-world computers. Thus, proving the ‘correctness’ of an algorithm cannot establish the trustworthiness of programs based on that algorithm. Despite the (deceptive) sameness of the notations used to represent them, the transformation of an algorithm into an executable program is a wrenching metamorphosis that changes a mathematical abstraction into a prescription for concrete actions to be taken by real computers. Therefore, it is verification of program executions (processes) that is needed, not of program texts that are merely the scripts for those processes. In this view, verification is the empirical investigation of: (a) the behavior that programs invoke in a computer system and (b) the larger context in which that behavior occurs. Here, deduction can play no more, and no less, a role than it does in the empirical sciences.	algorithm;computation;computer;emoticon;executable;formal verification;natural deduction;trust (emotion)	David A. Nelson	1992	Minds and Machines	10.1007/BF02454224	program analysis;correctness;computer science;theoretical computer science;programming language;program derivation;algorithm;software quality	SE	-22.878264673397446	28.673611544738055	67459
14f093fa0a2e5cd901e55da2cd4123c22f576324	runtime verification of concurrent haskell programs	runtime verification;model checking;source code;haskell;ltl checking	In this article we use model checking techniques to debug Concurrent Haskell programs. LTL formulas specifying assertions or other properties are verified at runtime. If a run which falsifies a formula is detected, the debugger emits a warning and records the path leading to the violation. It is possible to dynamically add formulas at runtime, giving a degree of flexibility which is not available in static verification of source code. We give a comprehensive example of using the new techniques to detect lock-reversal in Concurrent Haskell programs and introduce a template mechanism to define LTL formulas ranging over an arbitrary set of threads or communication abstractions.	algorithm;assertion (software development);concurrent haskell;debugger;debugging;embedded system;functional programming;java pathfinder;java platform, enterprise edition;java virtual machine;lazy evaluation;logic programming;maude system;model checking;posix threads;pattern recognition;printing;programming language;requirement;rewriting;rover (the prisoner);run time (program lifecycle phase);runtime verification;source transformation;state transition table;temporal logic;tracing (software);universal instantiation	Volker Stolz;Frank Huch	2005	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2004.01.026	model checking;parallel computing;real-time computing;computer science;runtime verification;programming language;source code	PL	-21.80631482394017	30.689529689972268	67740
59d6a6af82efabb5d693a35ef51ee8b4c0d5699c	a denotational semantics for joining description logics and object-oriented programming	object oriented programming;software engineering;denotational semantic;description logic;knowledge representation	Despite their common roots, the paradigms of object-orient ed programming and object-centered knowledge representatio could not really be joined again. While the knowledge representation branch hig lights clear semantics and focuses on subsumption and classification-ba sed inference algorithms, object-oriented programming shines with flexibl e control structures and software-engineering issues. Bridging both worlds see m rewarding, as it would allow for the description of complex knowledge-based ystems with dynamic behavior in a unified framework. In this paper we extend object-oriented programming into the classification-based paradigm of term inological reasoning. As an application, we consider a natural language under standing task within such an integrated specification framework.	algorithm;bridging (networking);control flow;denotational semantics;description logic;knowledge representation and reasoning;natural language;programming paradigm;sed;software engineering;specification language;subsumption architecture;unified framework	Susanne Schacht;Udo Hahn	1997			natural language processing;knowledge representation and reasoning;description logic;normalisation by evaluation;action semantics;computer science;artificial intelligence;theoretical computer science;functional logic programming;domain theory;inductive programming;programming language;denotational semantics of the actor model;object-oriented programming;denotational semantics	AI	-24.861004340864273	21.04027360123396	68055
e5c8636bc3766d2b6943b3e69e0e519692e9c14d	graceful dialects		Programming languages are enormously diverse, both in their essential concepts and in their accidental aspects. This creates a problem when teaching programming. To let students experience the diversity of essential concepts, the students must also be exposed to an overwhelming variety of accidental and irrelevant detail: the accidental differences between the languages are likely to obscure the teaching point. The dialect system of the Grace programming language allows instructors to tailor and vary the language to suit their courses, while staying within the same stylistic, syntactic and semantic framework, as well as permitting authors to define advanced internal domain-specific languages. The dialect system achieves this power though a combination of well-known language features: lexical nesting, lambda expressions, multi-part method names, optional typing, and pluggable checkers. Grace’s approach to dialects is validated by a series of case studies, including both extensions and restrictions of the base language.	domain-specific language;error-tolerant design;lambda calculus;programming language;relevance;type system	Michael Homer;Timothy Jones;James W Noble;Kim B. Bruce;Andrew P. Black	2014		10.1007/978-3-662-44202-9_6	speech recognition;computer science;third-generation programming language;fifth-generation programming language;programming language;second-generation programming language;algorithm	PL	-25.312595535924203	26.85481036730785	68188
620c34968e8fc601b57ec5455437fa0aca9b99c8	computer graphics for java programmers	arbitrary axis;computer graphics;efficient algorithm;good place;ready-to-run java program;useful java class;b-spline curve fitting;elementary concept;perspective drawing;accompanying web site;java programmers;classic graphics algorithm;hidden-line elimination;graphical object	From the Publisher:#R##N#Computer Graphics for Java Programmers is a good place to start for those with a little experience of Java who wish to create and manipulate 2D and 3D graphical objects. Two-dimensional subjects discussed include logical coordinates, triangulation of polygons and both Bezier and B-spline curve fitting. There is also a chapter about transformations, culminating in a useful Java class for 3D rotations about an arbitrary axis. The perspective representation of 3D solid objects is discussed in detail, including efficient algorithms for hidden-face and hidden-line elimination. These and many other algorithms are accompanied by complete, ready-to-run Java programs which can be downloaded from the accompanying web site.	computer graphics;java;programmer	Leen Ammeraal;Kang Zhang	2017		10.1007/978-3-319-63357-2	java concurrency;computer science;theoretical computer science;java modeling language;strictfp;real time java;programming language;java;generics in java;scala;java applet;java annotation;computer graphics (images)	HCI	-29.61768865113121	25.794479811576934	68261
f509a1a32827789fa3130ca81a3f46fc44d4ffc3	a verification environment for i/o automata based on formalized meta-theory	i o automata	This thesis deals with the computer-assisted veri cation of embedded systems described as Input/Output automata. We achieve contributions in two elds: the theory of untimed I/O automata and its tool support. For the latter a combination of the theorem prover Isabelle with model checking is used. Concerning the theory of I/O automata, we present a new temporal logic which considerably facilitates the usual implementation proofs between live I/O automata and serves as a property speci cation language. Furthermore, a new theory of abstraction is developed which allows us to combine theorem proving and model checking. Theorem proving is used to justify the reduction of the original system to a smaller model which is then analyzed automatically by model checking. The theorem prover reasons about rst-order proof obligations only. For the remaining obligations translations to the model checkers cke and STeP are given. In contrast to existing abstraction theories, the proof of both temporal properties and implementation relations can be abstracted, a methodology for treating liveness abstractions is proposed, and a formal relation to the dual concept of re nement is established. Concerning tool support, we provide a veri cation environment that covers both standard I/O automata theory and the aforementioned extensions. The overall guideline for its construction is reliability. Therefore, meta-theoretic notions of I/O automata such as compositionality and re nement are mechanically veri ed in Isabelle. Nevertheless, a practicable environment for system veri cation is obtained. This is due to a new methodology for Isabelle's logics HOL and HOLCF, which allows the user of the framework to employ the simpler logic HOL, whereas meta-theoretic investigations gain from the more expressive HOLCF. Possibly in nite communication histories of I/O automata are formalized as lazy lists based on Scott's domain theory. This results in a generally applicable sequence model which turns out to be favorable in an elaborate comparison with alternative formalizations. In addition to the usual inductive proof principles for lazy lists we provide an infrastructure for coinduction. The practical relevance of the resulting veri cation environment is proven by several case studies, in particular by an industrial helicopter alarm system. Acknowledgments First of all, I would like to thank Tobias Nipkow for his continuous support. His supervision has been a perfect mixture of advice, inspiration, and encouragement. Furthermore, I am grateful to many friends and colleagues for valuable discussions and helpful comments on draft versions of the thesis. They include Frits Vaandrager, Stephan Merz, Markus Wenzel, Larry Paulson, David Gri oen, Marco Devillers, Ingolf Kr uger, David von Oheimb, Jan Philipps, and Oscar Slotosch. Moreover, I wish to thank Tobias Hamberger for his e orts on carrying out a case study in order to demonstrate the developed tool set. I am indebted to Manfred Broy and Tobias Nipkow for providing the stimulating environment that made this thesis possible. This includes many other people of the Munich research group as well, especially those who were involved with Isabelle or participated in the KorSys project. I merely mention Peter, Stephan, Wolfgang, David, Jan, Oscar, Max, Christian, Franz, and Conny. Special thanks are due to my roommates Konrad Slind and Markus Wenzel for the excellent atmosphere in our o ce. I really enjoyed the inspiring discussions at tea time. I am glad that this thesis is completed, because during its writing I had less time to spend with my daughter Susanna than I would have liked to have. And I would like to thank my wife Christiane for her love and support and for reminding me of the more important things in life. Finally, there are two people who laid the basis for my current life and work with regard to many aspects: Pa & ma: thank you. To Christiane and Susanna	3d xpoint;64-bit computing;abstract data type;abstract interpretation;advice (programming);agile software development;andrey ershov;anisotropic filtering;antivirus software;audio feedback;automata theory;automated proof checking;automated theorem proving;avionics;backward chaining;bartlett's bisection theorem;bernhard schölkopf;bibliothèque du cinéma françois-truffaut;blue (queue management algorithm);category theory;chan's algorithm;claire;coinduction;colon classification;complex systems;computable function;computer aided verification;concurrency (computer science);concurrent computing;control flow;corecursion;correctness (computer science);damm algorithm;dataflow architecture;denotational semantics;dexter (malware);dexter kozen;dia;diagram;distributed computing;domain theory;dominic giampaolo;duplex (telecommunications);eine and zwei;esa;edmund m. clarke;embedded system;emoticon;european association for theoretical computer science;evert willem beth;experiment;ext js javascript framework;fairness measure;fekete polynomial;finite-state machine;first-order logic;formal methods europe;fractional quantum hall effect;fragmentation (computing);franz lisp;fred (chatterbot);functional programming;gab;general frame;gerald weinberg;grams;graphical user interface;gri;hol (proof assistant);handbook;history of type theory;hybrid system;iso 10303;industry 4.0;information and computation;information science;input/output;institut für dokumentologie und editorik;interference (communication);international federation for information processing;isabelle;iteration;jan bergstra;jart armin;joyce currie little;ka band;kateryna yushchenko (scientist);kerberos;konrad zuse medal;krishan sabnani;lr parser;larch prover;lazy evaluation;lecture notes in computer science;leslie speaker;light-emitting electrochemical cell;limbo;liquidity at risk;liveness;logic for computable functions;logic in computer science;logic of computable functions;look and feel;mason	Olaf Müller	1998			automata theory	Logic	-21.40434085585305	20.611348903980502	68264
b98da41494da8e0216778fc0bf1f754659395448	extendable object visualisation for software reengineering	dynamic typing;object oriented programs software reengineering functionality dynamic type visualisation class data structures object visualisation;runtime data structures tree data structures data visualization watches documentation information processing microprocessors software tools binary trees;program visualisation systems re engineering object oriented programming;object oriented programming;source code;data structure;concurrent process;program visualisation;binary tree;systems re engineering	This paper describes an experimental reengineering tool which assists the process of understanding the functionality of unknown software, particularly object oriented programs. In contrast to many other tools which analyse the source code, the inspected program is analysed at runtime by a concurrent process running in parallel. Information on all objects allocated by the inspected program is collected, in particular the dynamic type of each inspected object is determined. In contrast to the static type, the dynamic type of an object can only be determined at runtime. Each object is visualised by a corresponding visualisation class. Visualisation classes for well known data structures like binary trees can be used from the beginning. New visualisation can be derived by class extensions or can be added simply. The inspected program can be halted at specific locations to update the visualisation. Also, updating the visualisation can be triggered by specifying watch points.	binary tree;code refactoring;data structure;extensibility;parallel computing;run time (program lifecycle phase);type system	Jörg R. Mühlbacher;Peter R. Dietmüller;Markus Jöbstl	1999		10.1109/EURMIC.1999.794785	method;real-time computing;computer science;database;programming language	SE	-30.11864874685657	26.25015946098418	68356
20eee294f9d8fa7c69dee76e2165f8c3dbb6b930	a framework of syntactic models for the implementation of visual languages	programming environments;syntax of visual languages;implementation of visual languages;software tools visual languages visual programming program compilers programming environments;automatic generation;visual programming;visual languages;visual programming environments;visual language;software tools;object oriented modeling vocabulary;program compilers;visual programming environments syntactic models visual languages visual language compiler compiler vlcc software tool automatic program generation	In this paper we present a framework of syntactic models for the definition and implementation of visual languages. We analyze a wide range of existing visual languages and, for each of them, we propose a characterization according to a syntactic model. The framework has been implemented in the Visual Language Compiler-Compiler (VLCC) system. VLCC is a practical, flexible and extensible tool for the automatic generation of visual programming environments which allows to implement visual languages once they are modeled according to a syntactic model.	chart;compiler;compiler-compiler;diagram;emoticon;entity–relationship model;flowchart;graphical user interface;information needs;prototype;sql;semiconductor industry;text editor;visual language;visual programming language	Gennaro Costagliola;Andrea De Lucia;Sergio Orefice;Genny Tortora	1997		10.1109/VL.1997.626559	natural language processing;fourth-generation programming language;computer science;theoretical computer science;third-generation programming language;syntax;computer programming;programming paradigm;fifth-generation programming language;programming language theory;visual programming language;programming language;second-generation programming language;comparison of multi-paradigm programming languages	PL	-27.121434873301645	22.631962634545392	68524
27c4645886a11fec89b21f81e1ca26ef9bbaeedc	component programming and interoperability in constraint solver design	object oriented language;programming language;object oriented;industrial application	Prolog was once the main host for implementing constraint solvers. It seems that it is no longer so. To be useful, constraint solvers have to be integrable into industrial applications written in imperative or object-oriented languages; to be efficient, they have to interact with other solvers. To meet these requirements, many solvers are now implemented in the form of extensible object-oriented libraries. Following Pfister and Szyperski, we argue that “objects are not enough,” and we propose to design solvers as component-oriented libraries. We illustrate our approach by the description of the architecture of a prototype, and we assess its strong points and weaknesses.	imperative programming;interoperability;library (computing);prolog;prototype;requirement;solver	Frédéric Goualard	2001	CoRR		constraint programming;computer science;theoretical computer science;programming language;object-oriented programming;algorithm	PL	-28.132297502603816	28.90631487982631	68612
98a2e9bda15be934e3a72ddeb9c3ac2b717d2a87	apl2 - a risc business	exciting programming language;workstation apl2 interpreter;apl2-a risc business;original apl discovery;winchester scientific centres;feasibility study;mainframe computer;joint research;programming language	APL2 is one of the most exciting programming languages to evolve from IBM's original APL discovery. APL2 has been restricted to IBM 370 mainframe computers, due to the complexity of the interpreter required to implement all facets of the language. This paper describes a joint research / feasibility study carried out by IBM's Madrid and Winchester Scientific Centres on generating a “Portable” Workstation APL2 interpreter.	apl;ibm system/370;mainframe computer;programming language;workstation	Manuel Alfonseca;David Selby	1987		10.1145/55626.55627		PL	-28.84970971543489	22.17321180105376	68631
4f92144bc8a67544b8652836feb3a4163576bb7b	verification of tree-based hierarchical read-copy update in the linux kernel		Read-Copy Update (RCU) is a scalable, high-performance Linux-kernel synchronization mechanism that runs low-overhead readers concurrently with updaters. Production-quality RCU implementations are decidedly non-trivial and their stringent validation is mandatory. This suggests use of formal verification. Previous formal verification efforts for RCU either focus on simple implementations or use modeling languages. In this paper, we construct a model directly from the source code of Tree RCU in the Linux kernel, and use the CBMC program analyzer to verify its safety and liveness properties. To the best of our knowledge, this is the first verification of a significant part of RCU's source code — an important step towards integration of formal verification into the Linux kernel's regression test suite.	callback (computer programming);concurrency (computer science);formal verification;linux;liveness;model checking;modeling language;overhead (computing);preemption (computing);program analysis;read-copy-update;regression testing;scalability;test harness;test suite	Lihao Liang;Paul E. McKenney;Daniel Kroening;Thomas F. Melham	2018	2018 Design, Automation & Test in Europe Conference & Exhibition (DATE)	10.23919/DATE.2018.8341980	parallel computing;computer science;modeling language;real-time computing;regression testing;source code;read-copy-update;scalability;linux kernel;formal verification;liveness	EDA	-21.150194152333825	31.67911972623908	68643
52ed2b1bc2189b8afa286924e0308bbcf470dc1c	quicktalk: a smalltalk-80 dialect for defining primitive methods	virtual machine	QUICKTALK is a dialect of Smalltalk-80 that can be compiled directly into native machine code, instead of virtual machine bytecodes. The dialect includes “hints” on the class of method arguments, instance variables, and class variables. We designed the dialect to describe primitive Smalltalk methods. QUICKTALK achieves improved performance over bytecodes by eliminating the interpreter loop on bytecode execution, by reducing the number of message send/returns via binding some target methods at compilation, and by eliminating redundant class checking. We identify changes to the Smalltalk-80 system and compiler to support the dialect, and give performance measurements.	compiler;machine code;smalltalk;virtual machine	Mark B. Ballard;David Maier;Allen Wirfs-Brock	1986		10.1145/28697.28711	parallel computing;computer science;virtual machine;programming language;algorithm	PL	-22.341196853286093	29.44680688292186	68677
0562bddede669f31d9467861617607efa354e089	handbook of constraint programming		Syntax We distinguish between two different kinds of constraints: built-in (pre-defined) constraints which are solved by a built-in constraint solver, and CHR (user-defined) constraintswhich are defined by the rules in a CHR program. Built-in const rain s include syntactic equality=, true, andfalse. This distinction allows to embed and utilize existing constraint solvers as well as side-effect-free host langua ge statements. Built-in constraint solvers are considered as black-boxes in whose behavior is t rusted and that do not need to be modified or inspected. The solvers for the built-in constr ain s can be written in CHR itself, giving rise to a hierarchy of solvers [87]. A CHR programis a finite set of rules. There are three kinds of rules: Simplification rule: Name@ H ⇔ C B Propagation rule: Name@ H ⇒ C B Simpagation rule: Name@ H \H ′ ⇔ C B Nameis an optional, unique identifier of a rule, theadH , H ′ is a non-empty conjunction of CHR constraints, the guardC is a conjunction of built-in constraints, and the bodyB is a goal. Agoal is a conjunction of built-in and CHR constraints. A trivial g uard expression “ true” can be omitted from a rule. Simpagation rules abbreviate simplification rules of the fo rm H ∧H ′ ⇔ C H ∧B, so there is no further need to discuss them separately. Operational Semantics At runtime, a CHR program is provided with an initial state an d will be executed until either no more rules are applicable or a contradiction occur s. The operational semantics of CHR is given by a transition sys tem (Fig. 1.14). LetP be a CHR program. We define the transition relation 7→ by two computation steps (transitions), one for each kind of CHR rule. Statesare goals, i.e., conjunctions of built-in and CHR constraints. States are also called (constraint) stores . In the figure, all upper case letters are meta-variables that stand for conjunctions of cons trai ts. The constraint theory CT defines the semantics of the built-in constraints. Gbi denotes the built-in constraints of G. 2Integrating deduction and abduction, bottom-up and top-do wn execution, forward and backward chaining, tabulation and integrity constraints. 26 1. Constraints in Procedural and Concurrent Languages		Pascal van Beek;toby. walsh	2006			computer science;artificial intelligence;algorithm	AI	-23.513660262964102	19.015780193849317	68720
fd431274e8fd81295a55681b9f7ea55e26707eda	a sound and complete proof system for partial program correctness	program correctness	Without Abstract	correctness (computer science);proof calculus	J. W. de Bakker	1979		10.1007/3-540-09526-8_1	program analysis;correctness;computer science;theoretical computer science;mathematics;program derivation;algorithm	PL	-19.303368648857774	20.215493501845856	68737
a3a64a22c08cf8455d7f561586d78f53daec23a8	algorithmic state machine design and automatic theorem proving: two dual approaches to the same activity	declarative programming;computers;digital computers;sequential machine;computer program;fonction booleenne;general and miscellaneous mathematics computing and information science;automatic proving;programmation;logic design;prolog;logic;performance;synchronous;machine sequentielle;boolean function;state machine;demostracion automatica;synchrone;program synthesis;mathematical logic;maquina secuencial;principe resolution;programacion;theorem proving;algorithme;demonstration automatique;algorithm;demonstration theoreme;algorritmo;p functions;implementation of algorithms;programming languages 990200 mathematics computers;sincronico;funcion booliana;logique ordre 1;declarative languages;automatic theorem proving;algorithmic state machine;switching theory;computer codes;algorithms;switching theory algorithmic state machine automatic theorem proving declarative languages implementation of algorithms logic p functions resolution principle;demostracion teorema;machine etat fini;resolution principle;programming;functions;supercomputers;one order logic;logica orden 1	This paper shows that synthesizing binary decision programs (formed by means of decision instructions of the type if then else and of execution instructions of the type do) and proving theorems can be carried out by using the same approach. It is proved that the same transformations acting on P-functions can be interpreted in terms of binary program synthesis and of theorem proving. Since binary program leads to algorithmic state machine design while theorem proving leads to declarative programming, this allows us to lay a bridge between logic design and declarative languages such as Prolog.	algorithmic state machine;automated theorem proving;declarative programming;program synthesis;prolog	Dominique Snyers;André Thayse	1986	IEEE Transactions on Computers	10.1109/TC.1986.1676676	algorithmic state machine;programming;mathematical logic;discrete mathematics;parallel computing;declarative programming;performance;computer science;theoretical computer science;mathematics;finite-state machine;boolean function;programming language;prolog;logic;algorithm;algebra	Logic	-19.230468509485007	21.939787718099083	68944
c67018bddce35db3cbb05eae7629bca0ca9754a5	preface for selected and extended papers from principles and practice of declarative programming (ppdp'15)		This special issue gathers together extended versions of selected papers from the 17th International Symposium on Principles and Practice of Declarative Programming PPDP 2015, held in Siena, Italy in July 2015. The aim of the symposium is to cross-fertilize the logic, constraint and functional programming paradigms, and stimulate research in the use of logical formalisms and methods for specifying, performing, and analyzing computations. The best papers from the symposium were selected by the program committee. The authors of these papers were invited to submit extended versions for the special issue, and required to make a significant contribution over and above the version in the PPDP proceedings. In addition to some of the expert reviewers involved in the symposium, new reviewers were employed to ensure that all papers were evaluated with fresh eyes. We thank all the reviewers for their constructive suggestions and insightful comments. The result of this selection and reviewing processes are the following five articles that compose the special issue. The article “Concolic testing for functional languages” introduces CutEr, a concolic testing tool for a subset of the functional sublanguage of Erlang. Concolic testing has gained much attention in the recent years because it scales much better than traditional symbolic execution-based testing, allowing a more extensive exploration of the program under test, and because it enables testing of programs with calls to libraries written in different languages or unavailable code in general, where the traditional approach cannot be applied. The article “Static analysis of cloud elasticity” presents a type system for resource analysis of concurrent programs, for the particular type of resource of virtual machine (VM) usage. Such VMs can be both created and released by the program, which means that resource usage may increase and decrease along the execution. This non-cumulative behavior is a challenge in the field of resource analysis. The proposed approach consisting of a behavioral type system for a simple language that allows allocation and release of VMs, and a translation step into cost equations that can be solved by an off-the-shelf solver. The paper “Modeling and solving planning problems in tabled logic programming: experience from the cave diving domain” describes the use of the declarative programming language Picat for modeling planning problems. It demonstrates that, by using tabled logic programming, the Picat planner is able to avoid re-discovering previously learned knowledge. The experience gained from applying the modeling capabilities of the Picat planner on a well known planning domain is reported. The paper “Semantics-based generation of verification conditions via program specialization” presents a verification framework for a fragment of the C language based on program specialization. The approach consists in defining the semantics of C by means of an interpreter written in CLP. Then, given such interpreter, the program to be verified, and the (un)safety property of interest, the verification conditions (VCs) are generated by specializing the interpreter w.r.t. the given program in the form of Horn clauses. Finally, an external (Horn-clause) solver checks the satisfiability of the VCs. The paper “Improvements in a call-by-need functional core language: common subexpression elimination and resource preserving translations” introduces a framework for reasoning about operational improvements in call-by-need lambda-calculus including letrec, case, data constructors and ‘seq’ operator, so that a substantial part of the core of real functional languages (like Haskell) is covered. An important result of this article is the technical proof of a simple (in its formulation) and intuitive result, namely that common subexpression elimination – i.e., the replacement of multiple occurrences of a given subexpression by multiple occurrences of a shared locally letrec-defined variable – is an improvement. We would like to thank all the authors, for accepting our invitation to submit their work here, and all the reviewers, for spending so much time and efforts during the reviewing process. Also, we would like to thank the Science of Computer	cuter;common subexpression elimination;computation;concolic testing;declarative programming;dynamic testing;elasticity (cloud computing);erlang (programming language);functional programming;haskell;horn clause;interpreter (computing);lambda calculus;lazy evaluation;library (computing);logic programming;partial template specialization;programming language;programming paradigm;solver;sublanguage;symbolic execution;test automation;type system;virtual machine	Elvira Albert	2017	Sci. Comput. Program.	10.1016/j.scico.2017.06.008	natural language processing;programming language;declarative programming;computer science;artificial intelligence	PL	-20.295484773080894	23.370682657178047	68945
75dc6581b01e7c2a87d3819e693a990e7ff7d49a	on regularity in software design	developpement logiciel;model specification;expresion regular;program design;exigence usager;exigencia usuario;abstraction;conception programme;ingenieria logiciel;abstraccion;software engineering;estudio caso;specification modele;especificacion modelo;desarrollo logicial;user requirement;software development;etude cas;genie logiciel;expression reguliere;software design;regular expression;concepcion programa	A regular relationR, is one for whichR = R R̂ R, where “ ” is relational composition and “^” is relational transpose. By examining realistic case studies, and other examples, it is shown that when expressed using a rigorous specification notation, the majority of specifications turn out to be regular relations. This is certainly so for deterministic problems, and when abstraction relations are functions, reification preserves regularity. Nondeterministic specifications can appear to exhibit non-regularity, but at least in the most commonly occuring cases, it is argued that this is caused as much by a failure to separate concerns, as by any intrinsic lack of regularity in the specification. Such specifications can be recast into a regular form, and the process is analogous to a “transformation to orthogonal coordinates” of the original problem. A design philosophy is proposed, that places the search for regularity at the heart of specification construction, with implications for requirements capture.	cliff shaw;eisenstein's criterion;eurographics;garbage collection (computer science);hayes microcomputer products;jones calculus;memory management;ndb cluster;nondeterministic finite automaton;paging;reification (knowledge representation);requirement;software design;symbol table;unix	Richard Banach	1995	Sci. Comput. Program.	10.1016/0167-6423(95)00004-C	computer science;software design;user requirements document;software development;program design language;abstraction;programming language;specification;regular expression;algorithm	PL	-26.441061116068003	22.838549004207454	68950
8b57b9c4dc93d3296336a0374feee1a2420ca0f9	ptl: a model transformation language based on logic programming	model transformation;software engineering;logic programming;model driven engineering;domain specific languages	In this paper we present a model transformation language based on logic programming. The language, called PTL (Prolog based Transformation Language), can be considered as a hybrid language in which ATL (Atlas Transformation Language)-style rules are combined with logic rules for defining transformations. ATL-style rules are used to define mappings from source models to target models while logic rules are used as helpers. The implementation of PTL is based on the encoding of the ATL-style rules by Prolog rules. Thus, PTL makes use of Prolog as a transformation engine. We have provided a declarative semantics to PTL and proved the semantics equivalent to the encoded program. We have studied an encoding of OCL (Object Constraint Language) with Prolog goals in order to map ATL to PTL. Thus a subset of PTL can be considered equivalent to a subset of ATL. The proposed language can be also used for model validation, that is, for checking constraints on models and transformations. We have equipped our language with debugging and tracing capabilities which help developers to detect programming errors in PTL rules. Additionally, we have developed an Eclipse plugin for editing PTL programs, as well as for debugging, tracing and validation. Finally, we have evaluated the language with several transformation examples as well as tested the performance with large models.	debugging;eclipse;logic programming;model transformation language;object constraint language;pass transistor logic;prolog;tracing (software)	Jesús Manuel Almendros-Jiménez;Luis Iribarne;Jesús J. López-Fernández;Ángel Mora Segura	2016	J. Log. Algebr. Meth. Program.	10.1016/j.jlamp.2015.06.006	natural language processing;model-driven architecture;computer science;domain-specific language;programming language;logic programming;algorithm	PL	-21.30961781617232	25.891431452862665	69090
10e37ebe1fd0e2c1b7e36c635afd324573b3b90e	symbolic reasoning for automatic signal placement		Explicit signaling between threads is a perennial cause of bugs in concurrent programs. While there are several run-time techniques to automatically notify threads upon the availability of some shared resource, such techniques are not widely-adopted due to their run-time overhead. This paper proposes a new solution based on static analysis for automatically generating a performant explicit-signal program from its corresponding implicit-signal implementation. The key idea is to generate verification conditions that allow us to minimize the number of required signals and unnecessary context switches, while guaranteeing semantic equivalence between the source and target programs. We have implemented our method in a tool called Expresso and evaluate it on challenging benchmarks from prior papers and open-source software. Expresso-generated code significantly outperforms past automatic signaling mechanisms (avg. 1.56x speedup) and closely matches the performance of hand-optimized explicit-signal code.	algorithm;benchmark (computing);context switch;donkey kong country;network switch;open-source software;overhead (computing);run time (program lifecycle phase);software bug;speedup;static program analysis;symbolic computation;turing completeness	Kostas Ferles;Jacob Van Geffen;Isil Dillig;Yannis Smaragdakis	2018		10.1145/3192366.3192395	context switch;real-time computing;computer science;theoretical computer science;abductive reasoning;semantic equivalence;concurrent computing;speedup;thread (computing);shared resource;software	PL	-20.037783958596467	31.105300446783826	69361
964e8ef575cb877f21fcddf67afcdb8b8463831e	quantifying performance degradation of hvac systems for proactive maintenance using a data-driven approach		Poorly maintained and degraded Heating, Ventilating, Air Conditioning (HVAC) systems waste significant amount of energy. Current Facilities Management (FM) practice is mostly based on reactive and scheduled maintenance of HVAC systems instead of proactive maintenance, which aims at detecting anticipated failures before they occur, so that lower life cycle costs can be accomplished. Therefore, current FM practice needs approaches to detect anticipated failures, so that proactive measures can be taken. Building Automation Systems (BASs) in smart buildings provide historical data on HVAC operations, which can be leveraged for detecting performance degradation of HVAC systems. This study provides a data-driven methodology to quantify and visualize performance changes of HVAC systems over the years using historical BAS data. Our results on a case building demonstrated that there are statistically significant differences between the dataset over the years due to behavioral changes in the HVAC system when other factors (e.g., weather) are controlled. The contribution of this work is a computational approach to identify behavioral changes in HVAC equipment over time using custom selected algorithms for the HVAC domain.	elegant degradation	Gokmen Dedemen;Semiha Ergan	2018		10.1007/978-3-319-91635-4_25	air conditioning;reliability engineering;building automation;proactive maintenance;planned maintenance;hvac;data-driven;computer science;facility management	EDA	-31.93082813706799	20.291271344182675	69441
3300821bc37c75c664be3ae4eb9f99ef726cc666	maptool - supporting modular syntax development	abstract syntax;tree structure	In building textual translators, implementors often distinguish between a concrete syntax and an abstract syntax. The concrete syntax describes the phrase structure of the input language and the abstract syntax describes a tree structure that can be used as the basis for performing semantic computations. Having two grammars imposes the requirement that there exist a mapping from the concrete syntax to the abstract syntax. The research presented in this paper led to a tool, called Maptool, that is designed to simplify the development of the two grammars. Maptool supports a modular approach to syntax development that mirrors the modularity found in semantic computations. This is done by allowing users to specify each of the syntaxes only partially as long as the sum of the fragments allows deduction of the complete syntaxes.	abstract syntax;computation;existential quantification;natural deduction;parse tree;phrase structure rules;tree structure	Basim M. Kadhim;William M. Waite	1996		10.1007/3-540-61053-7_67	abstract syntax;computer science;syntax;abstract semantic graph;tree structure;programming language;homoiconicity;abstract syntax tree	SE	-26.149663046026223	18.914752684850658	69655
e0e0473d046e144185da9cc16274d1f4ba9ed201	on synchronous and asynchronous mobile processes	full abstraction;type system	Honda and Tokoro provide a formal system for communicating systems developed from Milner's {calculus. Unlike other formalisms, their work is based on asynchronous communication primitives. This paper proposes some minor but practically signiicant extensions to a model based on asynchronous communication and shows how the resulting system may be mapped very directly onto a graph rewriting system. While the model based on asynchronous communication permits the most direct translation , a related model using synchronous communication may be implemented in a similar manner.	data dependency;formal system;graph rewriting;high- and low-level;mobile agent;overhead (computing);process calculus;programming language;recursion;requirement	Paola Quaglia;David Walker	2000		10.1007/3-540-46432-8_19	real-time computing;type system;computer science;distributed computing;programming language;algorithm	PL	-28.712218828387932	32.278534334825444	69680
ef55985c151950ebcc3ebf6d0a3a4d39e6a85d5a	type inference for recursively constrained types and its application to oop	programming language;object oriented programming languages;type inference	We de ne a powerful type inference mechanism with application to object oriented programming The types inferred are recursively constrained types types that come with a system of constraints These types may be viewed as generalizations of recur sive types and F bounded polymorphic types the forms of type that are necessary to properly encode object typings The base language we study I Soop incorpo rates state and records the two features critical to encode objects in a non object oriented language Soundness and completeness of the type inference algorithm are established by operational means Our method for establishing these properties is somewhat novel We illustrate how the algorithm may be fruitfully applied to infer types of object oriented programs	algorithm;encode;recursion;type class;type inference	Jonathan Eifrig;Scott F. Smith;Valery Trifonov	1995	Electr. Notes Theor. Comput. Sci.	10.1016/S1571-0661(04)80008-2	composite data type;type system;type family;data type;type safety;computer science;recursive data type;theoretical computer science;algebraic data type;type inference;type constructor;programming language;object-oriented programming;generalized algebraic data type;algorithm;product type	PL	-22.975557071688236	25.678408938886925	69812
85ef581d482f66795d875d0712ba0084928a6f82	fusing effectful comprehensions		List comprehensions provide a powerful abstraction mechanism for expressing computations over ordered collections of data declaratively without having to use explicit iteration constructs. This paper puts forth effectful comprehensions as an elegant way to describe list comprehensions that incorporate loop-carried state. This is motivated by operations such as compression/decompression and serialization/deserialization that are common in log/data processing pipelines and require loop-carried state when processing an input stream of data.  We build on the underlying theory of symbolic transducers to fuse pipelines of effectful comprehensions into a single representation, from which efficient code can be generated. Using background theory reasoning with an SMT solver, our fusion and subsequent reachability based branch elimination algorithms can significantly reduce the complexity of the fused pipelines. Our implementation shows significant speedups over reasonable hand-written code (3.4Ã, on average) and traditionally fused version of the pipeline (2.6Ã, on average) for a variety of examples, including scenarios for extracting fields with regular expressions, processing XML with XPath, and running queries over encoded data.	algorithm;automata theory;baseline (configuration management);compiler;computation;control flow;data compression;finite-state machine;iteration;language integrated query;list comprehension;mathematical optimization;monad (functional programming);pipeline (computing);programmer;reachability;regular expression;runtime system;serialization;solver;state (computer science);stream (computing);transducer;transduction (machine learning);unicode;xml;xpath	Olli Saarikivi;Margus Veanes;Todd Mytkowicz;Madan Musuvathi	2017		10.1145/3062341.3062362	programming language;xml;serialization;xpath;theoretical computer science;computer science;regular expression;computation;abstraction;comprehension;satisfiability modulo theories	PL	-21.267429110162265	24.332349236740782	69875
5d1392c8ef99a31b07a7544e1c8d180e799344a6	a trait based re-engineering technique for java hierarchies	software;trait;code reuse;smalltalk;re engineering;formal concept analysis;java	Traits are pure behavior components introduced in the Smalltalk community in order to integrate the traditional class inheritance with a composition mechanism: a class is composed by traits and inherits from superclasses. This offers the advantage of promoting code reuse. In this paper, we tackle the problem of re-engineering a Java hierarchy into traits, by adapting to a Java setting a methodology developed by Lienhard, Ducasse, and Arévalo for a Smalltalk setting, based on Formal Concept Analysis. We illustrate the approach by applying it to the Java input stream library. We also obtain two by-products: (i) we identify clearly some workarounds that programmers must exploit in order to overcome some of the limitations of Java single inheritance; (ii) we single out some features a Java with traits might include, as none of the proposals in the literature in this sense has taken the lead yet.	code reuse;formal concept analysis;java;multiple inheritance;programmer;smalltalk;stream (computing);workaround	Lorenzo Bettini;Viviana Bono;Marco Naddeo	2008		10.1145/1411732.1411753	real-time computing;trait;java concurrency;computer science;formal concept analysis;java modeling language;real time java;programming language;java;generics in java;scala;java annotation	PL	-26.83697740630464	29.55861006407527	70042
d195fcb07159c07944528f3bf2647ceb054828bb	tradeoffs in the intensional representation of lambda terms	machine abstraite;lenguaje programacion;machine language;symbolic computation;lenguaje maquina;preuve programme;program proof;representacion sistema;programming language;maquina abstracta;lambda calculus;specification programme;abstract machine;higher order;representation systeme;system representation;prueba programa;metaprogrammation;langage programmation;lambda calculo;codage machine en terme lambda;metaprogramming;lambda calcul;program specification;metaprogramacion;langage machine;especificacion programa	Higher-order representations of objects such as programs, specifications and proofs are important to many metaprogramming and symbolic computation tasks. Systems that support such representations often depend on the implementation of an intensional view of the terms of suitable typed lambda calculi. Refined lambda calculus notations have been proposed that can be used in realizing such implementations. There are, however, choices in the actual deployment of such notations whose practical consequences are not well understood. Towards addressing this lacuna, the impact of three specific ideas is examined:the de Bruijn representation of bound variables, the explicit encoding of substitutions in terms and the annotation of terms to indicate their independence on external abstractions. Qualitative assessments are complemented by experiments over actual computations. The empirical study is based on ?Prolog programs executed using suitable variants of a low level, abstract machine based implementation of this language.	intensional logic;lambda calculus	Chuck Liang;Gopalan Nadathur	2002		10.1007/3-540-45610-4_14	metaprogramming;symbolic computation;higher-order logic;machine code;computer science;theoretical computer science;lambda calculus;mathematics;abstract machine;programming language;algorithm	NLP	-19.672672518640958	21.909904452516795	70122
0a0f4068c710785097c9f1c66dcedcbab93a7b7a	a confluent semantic basis for the analysis of concurrent constraint logic programs	operational semantics;concurrent constraint;confluent semantics;abstract interpretations;concurrent constraint logic programs;parallelism;transition systems;program analysis;logic programs;abstract interpretation;process scheduling	The standard operational semantics of concurrent constraint logic languages is not connuent in the sense that diierent schedulings of processes may result in diierent program behaviors. While implementations are free to choose speciic scheduling policies, analyses should be correct for all implementations. Moreover, in the presence of parallelism it is usually not possible to determine how processes will actually be scheduled. EEcient program analysis is therefore diicult as all process schedulings must be considered. To overcome this problem we introduce a connuent semantics which closely approximates the standard (non-connuent) semantics. This semantics provides a basis for eecient and accurate program analysis for these languages. To illustrate the usefulness of this approach we sketch analyses based on abstract interpretations of the connuent semantics which determine if a program is suspension and local suspension free.	concurrent constraint logic programming;operational semantics;parallel computing;program analysis;scheduling (computing)	Michael Codish;Moreno Falaschi;Kim Marriott;William H. Winsborough	1997	J. Log. Program.	10.1016/S0743-1066(96)00013-1	program analysis;concurrent constraint logic programming;computer science;theoretical computer science;formal semantics;programming language;well-founded semantics;operational semantics;scheduling;denotational semantics;algorithm	PL	-19.67999659673806	30.353718530824306	70126
4143584bed22ec1e1e50e5a0e686e967701d3ef1	template metaprogramming techniques for concept-based specialization	template specialization;automatic mechanism;template specialization mechanism;generic component;static specialization mechanism;specialization mechanism;concept-based specialization;type pattern;new concept;template metaprogramming technique;specialization process;concepts;software component;generic programming;template metaprogramming	"""In generic programming, software components are parameter iz d on types. When available, a static specialization mechanism allows selecting, for a given set of p arameters, a more suitable version of a generic component than its primary version. The normal C++ template specialization mechanism is based on the type pattern of the parameters, which is not always the best w ay to guide the specialization process: type patterns are missing some information on types that could be relevant to define specializations. The notion of a """"concept"""", which represents a set of requirem ents (including syntactic and semantic aspects) for a type, is known to be an interesting approach to ontrol template specialization. For many reasons, concepts were dropped from C++11 standard, this ar icle therefore describes template metaprogramming techniques for declaring concepts, """"modeling"""" re lationships (meaning that a type fulfills the requirements of a concept), and """"refinement"""" relationships (meaning that a concept refines the requirements of another concept). From a taxonomy of concepts and template specializations ba sed on concepts, an automatic mechanism selects the most appropriate version of a generic compo nent for a given instantiation. Our purely library-based solution is also open for retroactive extens ion: new concepts, relationships, and template specializations can be defined at any time; such additions wi ll then be picked up by the specialization mechanism."""	c++;c++11;compile time;compiler;component-based software engineering;coupling (computer programming);demoscene compo;experiment;generic programming;overhead (computing);partial template specialization;profiling (computer programming);refinement (computing);requirement;sed;substitution (logic);taxonomy (general);template metaprogramming;universal instantiation	Bruno Bachelet;Antoine Mahul;Loïc Yon	2013	Scientific Programming	10.3233/SPR-130362	template metaprogramming;computer science;theoretical computer science;component-based software engineering;programming language;generic programming;concept;algorithm	PL	-26.86406520251199	28.66253111256009	70165
c88e90fe32b0e38cb61433b3ff43b30b53a93d0e	modular analysis of executables using on-demand heyting completion		A function-modular analysis is presented that computes precise function summaries in the presence of pointers and indirect calls. Our approach computes several summaries for a function, each specialized to a particular input property. A call site combines the effect of several summaries, based on what properties hold. The key novelty is that the properties are tailored to the function being analyzed. Moreover, they are represented in a domain-agnostic way by using Herbrand terms with variables. Callers instantiate these variables, based on their state. For each variable instantiation, a new summary is computed. Since the computed summaries are exact with respect to the property, our fixpoint computation resembles the process of Heyting completion where a domain is iteratively refined to be complete wrt. the intersection with a property. Our approach combines the advantages of a modular analysis, such as scalability and context-sensitivity, with the ability to compute meaningful summaries for functions that call other functions via pointers that were passed as arguments. We illustrate our framework in the context of inferring indirect callees in x86 executables.	executable	Julian J Kranz;Axel Simon	2018		10.1007/978-3-319-73721-8_14	novelty;theoretical computer science;call site;pointer (computer programming);computer science;computation;scalability;executable;modular design;fixed point	Logic	-19.741217430129627	27.872117011958363	70350
10d449f3abe2ccfe8bbae53fe7f072686944f17d	regular structures in programming languages	programming language			Jan Maluszynski	1975	Elektronische Informationsverarbeitung und Kybernetik		very high-level programming language;declarative programming;mathematics;third-generation programming language;programming language;comparison of multi-paradigm programming languages;fifth-generation programming language;second-generation programming language;fourth-generation programming language;programming language theory	PL	-24.236224235175616	22.362646624196937	70708
1499d3cef829f8283b71abee3a38a58a5d055f17	ensuring consistency between designs, documentation, formal specifications, and implementations	java programming;software engineering;specification language;formal method;specification and verification;system design;source code;point of view	Software engineering experts and textbooks insist that all of the artifacts related to a system, (e.g., its design, documentation, and implementation), must be kept in-sync. Unfortunately, in the real world, it is a very rare case that any two of these are kept consistent, let alone all three. In general, as an implementation changes, its source code documentation, like that of Javadoc, is only occasionally updated at some later date. Unsurprisingly, most design documents, like those written in UML, are created as a read-only medium—they reflect what the designers thought they were building at one point in the past, but have little to do with the actual running system. Even those using formal methods make this mistake, sometimes updating an implementation and forgetting to make some subtle change to a related specification. The critical problem inherent in this approach is that abstraction levels, while theoretically inter-dependent, are actually completely independent in semantics and from the point of view of the tools in pervasive use. Entities in different layers have no formal relationship; at best, informal relations are maintained by ad hoc approaches like code markers, or code is generated once and never touched again. This paper presents a new approach to system design, documentation, implementation, specification, and verification that imposes a formal refinement relationship between abstraction levels that is invisible to the programmer and automatically maintained by an integrated set of tools. The new concept that enables this approach is called a semantic property, and their use is discussed in detail with a set of examples using the high-level specification language EBON, the detailed design and specification language JML, and the Java programming language as the implementation language.	b-method;category theory;concept learning;digital library;distributed control system;eclipse;entity;formal concept analysis;formal methods;graphical modeling framework;graphical user interface;high- and low-level;hoc (programming language);hypertext;inter-domain;java modeling language;javadoc;lero (software engineering);literate programming;object language;open-source software;pervasive informatics;programmer;programming language;read-only memory;refinement (computing);resolution (logic);semantics (computer science);semi-supervised learning;semiconductor industry;software documentation;software engineer;software engineering;specification language;systems design;unified modeling language	Joseph Kiniry;Fintan Fairmichael	2009		10.1007/978-3-642-02414-6_15	formal methods;object language;specification language;computer science;theoretical computer science;software engineering;formal specification;database;programming language;programming language specification;source code;systems design	PL	-27.02697990184446	28.158243956989395	70727
ef07baaae451dbb6d56ba2a2c21843b089bae205	a java toolkit for the design and the automatic checking of server architectures	formal model;separation of concern;java toolkit;internet server;software engineering;automatic generation;formal verification;source code;state explosion;java language	This paper presents Saburo, a Java toolkit that generates, from a single Java specification, Java Internet server implementations, together with their formal model that can be automatically checked using the model checker SPIN. This approach ensures the coherence between the Internet server behavior and the static verifications applied on its formal model. Moreover, the use of the Java language as a unique input specification should help the dissemination of formal verification techniques.  To simplify the automatic abstraction of the formal model, the Saburo approach is based on the separation of concerns principle. The Java specification is decomposed into three parts: a concurrency model chosen among several predefined ones, a graph describing all the communication and synchronization, and the business code. This approach has two main advantages: the change of the concurrency model is straightforward, and the classical problem of state explosion induced by the automatic extraction of the formal model from the source code is minimized.  This work blends together a set of features coming from server design, compilers, software engineering and formal verification. Even though we do not claim novelty in each of the techniques adopted for Saburo, they have been unified into our Java toolkit, which supports all server design phases.	compiler;concurrency (computer science);concurrent computing;formal language;formal verification;java;mathematical model;model checking;spin;separation of concerns;server (computing);software engineering	Gautier Loyauté;Rémi Forax;Gilles Roussel	2007		10.1145/1294325.1294341	java api for xml-based rpc;real-time computing;formal methods;jsr 94;java concurrency;formal verification;separation of concerns;computer science;java modeling language;strictfp;embedded java;database;real time java;programming language;java;application server;generics in java;scala;java applet;java annotation;source code	SE	-26.64164963006237	30.99985668756031	70782
0eea932015bbd977192cc21c43e510e488a37e20	a sound polymorphic type system for a dialect of c	langage fonctionnel;formal specification;compilateur;variables;natural semantics;langage c;programming language;calcul formel;lenguaje funcional;semantics;program transformation;ingenieria logiciel;transformation programme;compiler;type systems;semantica;semantique;software engineering;functional programming;calculo formal;specification formelle;especificacion formal;c language;transformacion programa;tipificacion;typing;polymorphism;typage;genie logiciel;polymorphisme;polimorfismo;c;transistion semantics;pointers;functional language;computer algebra;type soundness;compilador;variable;lenguaje c;type system	ion that represents a function with formal parameters x xn and body e One might expect that addresses would just be natural numbers but that would not allow the semantics to detect invalid pointer arithmetic So instead an address is a pair of natural numbers i j where i is the segment number and j is the o set Intuitively we put each variable or array into its own segment Thus a simple variable has address i and an n element array has addresses i i i n Pointer arithmetic involves only the o set of an address and dereferencing nonexistent or dangling pointers is detected as a segmentation fault Next we identify the set of values v consisting of literals pointers and lambda abstractions v c j a j x xn e The result of a successful evaluation is always a value Finally we require the notion of a memory A memory is a nite function mapping addresses to values or to the special results dead and uninit These results indicate that the cell with that address has been deallocated or is unini tialized respectively We write a for the contents of address a dom and we write a v for the memory that assigns value v to address a and value a to any address a other than a Note that a v is an update of if a dom and an extension of if a dom We can now de ne the evaluation relation e v which asserts that evaluating closed expression e in memory results in value v and new memory The evaluation rules are given in Figures and We write e x e to denote the capture avoiding substitution of e for all free occurrences of x in e Note the use of substitution in rules bindvar bindarr bindfun and apply It allows us to avoid environments and closures in the semantics so that the result of evaluating a Polymorphic C expression is just another Polymorphic C expression This is made possible by the exible syntax of the language and the fact that only closed expressions are ever evaluated during the evaluation of a closed expression We remark that rule apply speci es that function arguments are evaluated left to right C leaves the evaluation order unspeci ed Also note that if there were no operator there would be no need to specify in rule bindvar that	dangling pointer;dereference operator;pointer (computer programming);segmentation fault;type system;variable (computer science)	Geoffrey Smith;Dennis M. Volpano	1998	Sci. Comput. Program.	10.1016/S0167-6423(97)00030-0	variables;polymorphism;compiler;pointer;type system;type erasure;computer science;variable;theoretical computer science;formal specification;programming language;functional programming;algorithm	Logic	-21.84256670984042	26.21472450825573	70980
a424b86662884045c5f64984897a03aa9aadbdae	a (truly) usable and portable compiler writing system				Olivier Lecarme;Gregor von Bochmann	1974			compiler;usable;programming language;writing system;computer science	NLP	-29.309232640135303	23.34850465536199	71169
541da3b1f077c73d5f11a7aca50d84b7f7d60a0b	ranking the performance of compiled and interpreted languages in genetic algorithms.		Despite the existence and popularity of many new and classical computer languages, the evolutionary algorithm community has mostly exploited a few popular ones, avoiding them, especially if they are not compiled, under the asumption that compiled languages are always faster than interpreted languages. Wide-ranging performance analyses of implementation of evolutionary algorithms are usually focused on algorithmic implementation details and data structures, but these are usually limited to specific languages. In this paper we measure the execution speed of three common operations in genetic algorithms in many popular and emerging computer languages using different data structures and implementation alternatives, with several objectives: create a ranking for these operations, compare relative speeds taking into account different chromosome sizes and data structures, and dispel or show evidence for several hypotheses that underlie most popular evolutionary algorithm libraries and applications. We find that there is indeed basis to consider compiled languages, such as Java, faster in a general sense, but there are other languages, including interpreted ones, that can hold its ground against them.	compiled language;compiler;computer language;data structure;evolutionary algorithm;genetic algorithm;interpreted language;java;library (computing)	Juan Julián Merelo Guervós;Israel Blancas-Alvarez;Pedro A. Castillo;Gabriel A. Romero;Pablo García-Sánchez;Víctor Manuel Rivas Santos;Mario García Valdez;Amaury Hernández-Águila;Mario Román	2016		10.5220/0006048101640170	natural language processing;computer science;data mining;algorithm	PL	-21.838223389230226	25.547568208827556	71211
0de92be164b8179e19cbb80d2bdb9b191c341890	specializing the java object serialization using partial evaluation for a faster rmi [remote method	object oriented methods;software performance evaluation;object oriented programming;java performance evaluation runtime design engineering computer languages sockets distributed computing computer interfaces pervasive computing electrical capacitance tomography;object oriented methods java object oriented programming partial evaluation compilers remote procedure calls software performance evaluation;remote procedure calls;java applications java object serialization serialization code specialization partial evaluation remote method invocation run time object properties interpretation partial evaluation techniques variables arguments statically bound values statically bound types context sensitive optimized code generation interprocedural binding time analysis performance measurements;partial evaluation compilers;java	The Java object serialization is designed generic method to handle all possible objects, and it performs a considerable amount of interpretation in determining the properties of objects before taking an appropriate action. In this paper, we present a mechanism by which the Java serialization is specialized using partial evaluation techniques to reduce the amount of interpretation that happens at run-time. Our approach specializes the serialization code using information about variables and arguments whose values or types are statically bound. To generate optimized code in a context-sensitive manner, it performs an interprocedural binding-time analysis. Performance measurements show an average 13% improvement in serialization, with some cases showing as high as an 18% improvement. Our approach is not restricted to serialization; rather we believe it gives a basis for a more general solution that is applicable to other aspects of Java applications.		Jung Gyu Park;Arthur H. Lee	2001		10.1109/ICPADS.2001.934853	method;parallel computing;real-time computing;jsr 94;java concurrency;serialization;computer science;operating system;database;distributed computing;real time java;programming language;object-oriented programming;java;remote procedure call;generics in java;scala;java annotation	HCI	-26.517713367868414	30.23971246582163	71296
7e76b35cbd991f6c47ca690db9b7a7704ebdf8cd	data quality challenges in cyber-physical systems	cyber physical systems;data quality;faculty data detection	Recent emerging technologies, which integrate sensing, computation, communication, and control, have changed the way we observe and control the physical world. Cyberphysical systems (CPSs) [Rajkumar et al. 2010] are systems that have been applied in a variety of applications to collect data such as temperature, heart rate, speed, and so on from the physical world and make decisions based on the analysis of the data, thereby controlling and optimizing the physical objects in the real world. We are not only witnessing a seamless consolidation of the physical world and the cyber world but also experiencing a significant change in our lifestyles brought by the CPS. In medical CPSs, medical sensors are deployed to monitor the long-term biomedical condition of patients [Lee and Sokolsky 2010]. In the smart grid system, electricity usage data and the healthy status information of the smart grid are monitored and collected. Traffic information and city environmental data are also collected using sensors deployed across the city and on the vehicles [Arbabi and Weigle 2009]. Intelligent routing and emergency response can be more efficient based on the city surveillance data. When these systems become pervasive and ubiquitously available, large amounts of data will be collected, which may include information that is trustworthy or faked, publishable or sensitive, marginal or critical. Data-driven decisions have become very common and important for us to optimize and control the physical objects in the real world. Those decisions are long term, short term, or real time, but most of them are critical and could have significant consequences. Therefore, in such a data-driven decisionmaking system, the quality of the data is crucial for the success of the applications. Without high-quality data, no high-quality service based on the right decision could be provided. In CPSs, data is collected mostly from the physical world. The quality of the	computation;cyber-physical system;data quality;marginal model;pervasive informatics;quality of service;routing;seamless3d;semiconductor consolidation;sensor;trust (emotion);usage data	Kewei Sha;Sherali Zeadally	2015	J. Data and Information Quality	10.1145/2740965	data quality;computer science;data mining;database;cyber-physical system;computer security	HCI	-32.69219024008985	18.78706991667104	71336
3812de46526230450f1d5dbd2bd416764957e55f	improving software security with a c pointer analysis	field-sensitive points-to analysis;common c usage;dynamic string-buffer overflow detector;c variable;restricted c semantics;c pointer analysis;pointer analysis;optimistic analysis;conservative analysis;c program;proposed pointer alias analysis;improving software security;operating systems;context sensitive;algorithm design and analysis;buffer overflows;programming language;computer languages;data structures;data flow analysis;buffer overflow;type theory;dynamic analysis;program analysis;object oriented programming;national security;programming languages;software security;vulnerabilities;error detection;type safety	This paper presents a context-sensitive, inclusion-based, field-sensitive points-to analysis for C and uses the analysis to detect and prevent security vulnerabilities in programs. In addition to a conservative analysis, we propose an optimistic analysis that assumes a more restricted C semantics that reflects common C usage to increase the precision of the analysis.This paper uses the proposed pointer alias analyses to infer the types of variables in C programs and shows that most C variables are used in a manner consistent with their declared types. We show that pointer analysis can be used to reduce the overhead of a dynamic string-buffer overflow detector by 30% to 100% among applications with significant overheads. Finally, using pointer analysis, we statically found six format string vulnerabilities in two of the 12 programs we analyzed.	application security;pointer (computer programming);pointer analysis	Dzintars Avots;Michael Dalton;Benjamin Livshits;Monica S. Lam	2005		10.1109/ICSE.2005.1553576	real-time computing;data structure;buffer overflow;computer science;operating system;software engineering;escape analysis;database;tagged pointer;programming language;pointer analysis;smart pointer	SE	-21.517221346398735	31.022915433821062	71560
4398d60e0583a0a1944adacf5fe938c3496c1e1b	igks (abstract only): an integrated image processing and graphics environment	ada;object oriented design;user interface;graphical kernel system;rule based system;real time embedded system;artificial intelligence;frame;inference engine;parallel inference engine;forward backward chaining;blackboard driven;knowledge base	This paper describes a part of a large project at the University of Lowell to develop an integrated environment for the unified treatment of image processing and graphics with interfaces to pictorial databases. The overall systems consists of a knowledge-based user interface containing three orthogonal kernels:an image processing and vision kernel system (IKS) and the graphical kernel system (GKS) several pictorial databases  One can think of a kernel as a set of procedures, tools and utilities that are coherent, orthogonal and fundamental. In our environment, all kernels are closely linked. There is a very strong link between the image/vision kernel and the graphics one. This is specifically referred to as the Image and Graphics Kernel System (IGKS) and this is the focus of this paper. This system is interfaced to the user through a knowledge-base and rule-based system (referred to symbolically as ).	coherence (physics);comparison of command shells;database;graphical kernel system;graphics;image processing;kernel (operating system);rule-based system;user interface	Georges G. Grinstein	1987		10.1145/322917.323062	frame;knowledge base;real-time computing;ada;computer science;artificial intelligence;theoretical computer science;object-oriented design;software engineering;database;tree kernel;programming language;user interface;inference engine;computer graphics (images)	Graphics	-32.59605834521426	24.761593540890736	71578
16cb0d785c1fc07e4849b1362b845b5aa90918f7	the correspondence between the logical algorithms language and chr	constraint handling rules	This paper investigates the relationship between the Logical Algorithms formalism (LA) of Ganzinger and McAllester and Constraint Handling Rules (CHR). We present a translation scheme from LA to CHR: CHR with rule priorities and show that the metacomplexity theorem for LA can be applied to a subset of CHR via inverse translation. This result is compared with previous work. Inspired by the high-level implementation proposal of Ganzinger and McAllester, we demonstrate how LA programs can be compiled into CHR rules that interact with a scheduler written in CHR. This forms the first actual implementation of LA. Our implementation achieves the required complexity for the meta-complexity theorem to hold and can execute a subset of CHR with strong complexity bounds.	algorithm;compiler;constraint handling rules;formal system;high- and low-level;scheduling (computing);syntax-directed translation	Leslie De Koninck;Tom Schrijvers;Bart Demoen	2007		10.1007/978-3-540-74610-2_15	discrete mathematics;computer science;theoretical computer science;database;mathematics;programming language;algorithm	Logic	-21.592767070281955	23.040041312572377	71765
f086795fbe65790a3337afa004b9191eb9ca80fd	aplgol - a structured programming language for apl	programming language;control structure;control flow	The Harwood G. Kolsky collection is comprised of documents related to the development of super computers, microprocessors and microcomputers, computer modeling, digital image processing, computer programming languages, as well as materials related to teaching computer architecture and computer history. The collection primarily consists of technical papers and reports, manuals, notes, promotional material, and correspondence related to Los Alamos National Laboratory and International Business Machines (IBM)	apl;computer architecture;computer programming;computer simulation;digital image processing;history of computing hardware;microcomputer;microprocessor;programming language;structured programming;supercomputer	Harwood G. Kolsky	1974		10.1007/3-540-07131-8_25	natural language processing;first-generation programming language;natural language programming;very high-level programming language;language primitive;programming domain;reactive programming;programming language implementation;functional logic programming;jackson structured programming;programming paradigm;procedural programming;symbolic programming;low-level programming language;inductive programming;fifth-generation programming language;programming language;programming language specification;high-level programming language;algorithm;control flow analysis	Arch	-28.04502630441762	22.627572923590925	71941
6a71ab870fc31d5631f740808363088c8bb322e7	multiple object representations	object representation;engineering design;tuple space;transputer;genetics;multiple objectives;object oriented programming languages;speculative processing;knowledge representation;parallel lisp;object oriented paradigm	In the object-oriented paradigm, objects are instances of the conceptual entities that describe their structure and behavior i.e, the classes. Recent applications of object-oriented programming languages and databases in areas like engineering design and genetics have demonstrated the limits of this assumption. Object instances cannot always conform to [heir class definitions. They may evolve during the application lifetime. Further, a class may be only one specific object representation among several others required by the users. This paper relaxes some constraints usually associated with instantiation mechanisms. An object instance is no longer considered as a perfect representative of a specific class. [t ctin bear distortions with its class definition. This is called here tlexible instantiation. An instance can also belong simultaneously to different classes. These classes need not be related by any direct or indirect specialization relationship, “l’his is called here multiple instantiation, II is shown how classifiuauon mechanisms complement adequatly multiple instantiation. The characteristics of such LI clitssitication mechanism are described. It is implemented for tin object-based knowledge representation model called	database;distortion;engineering design process;entity;knowledge representation and reasoning;object-based language;partial template specialization;programming language;programming paradigm;triangulated irregular network;universal instantiation	Gia Toan Nguyen;Dominique Rieu	1992		10.1145/131214.131239	knowledge representation and reasoning;method;object model;computer science;tuple space;object;theoretical computer science;common object request broker architecture;distributed computing;programming language;object-oriented programming;engineering design process;object definition language	DB	-26.072740513105053	27.338101073200143	72003
8ba3b23bec4c894965c4fdc81308fd266ec39332	synchronizable test sequences of finite state machines	distributed system;empirical study;systeme reparti;protocole transmission;maquina estado finito;synchronizable test sequences;synchronisation;protocolo transmision;finite state machines;network protocols;sistema repartido;synchronization;necessary and sufficient condition;communication protocol;technical report;protocole reseau;machine etat fini;test sequence generation;finite state machine;communication protocols;transmission protocol	The finite state machine (FSM) model is commonly used for specifying communication protocols and other types of distributed systems. With the use of multiple testers for an FSM, the synchronization between inputs from different testers becomes a problem. A synchronizable test sequence of an FSM is a test sequence for which the synchronization problem either does not exist or can be solved by communication between testers. In this paper, we consider two testing strategies for an FSM: port-based testing, which does not allow testers for the FSM to communicate with each other, and group-based testing, which divides the ports of the FSM into groups and allows the testers for ports in the same group to communicate with each other. For each type of testing, we define a necessary and sufficient condition under which a test sequence of an FSM is synchronizable and show how to generate a set of testers according to a given test sequence. Also, we discuss the issues of test sequence generation and fault detection, and present the results of some empirical studies.	distributed computing;fault detection and isolation;finite-state machine;synchronization (computer science)	Kuo-Chung Tai;Yu-Chiou Young	1998	Computer Networks	10.1016/S0169-7552(98)00013-0	communications protocol;synchronization;real-time computing;telecommunications;computer science;finite-state machine;algorithm;computer network	SE	-31.629587464984752	31.19233352334274	72009
234d3d651a35cc553932a66d5b01b92c0f035786	hybrid executive: user's approach	satisfiability;digital systems;hybrid system;hybrid simulation;computer simulation;program development;large classes	Hybrid executive programs have long been prevalent in the hybrid computer simulation industry, however, what should be the essential features of a hybrid executive is still a controversial subject. For the most part, the design of hybrid executives has been undertaken by the manufacturers of hybrid systems and in many designs the complexity in the operation of these programs has resulted in their usage only on large class digital systems. Consequently, hybrid facilities which employ a small to medium class digital computer system are faced with the task of developing an executive program compatible with the facility environment. However, in many of these small to medium hybrid facilities, the segregated program development effort for a hybrid executive is not undertaken until considerable time after the installation of the hybrid system. The normal reasons are inadequate programming funds or a higher priority assignment of available personnel to satisfy programming and development needs of existing hybrid simulations.	computer simulation;digital electronics;hybrid computer;hybrid system	W. L. Graves;R. A. MacDonald	1969		10.1145/1478559.1478591	simulation;systems engineering;engineering;operations management	ECom	-31.16294440850116	21.607770035016657	72309
4b2e1e563132d1da1a026265146258fed197cf58	on benchmarking constraint logic programming platforms. response to fernandez and hill's “a comparative study of eight constraint programming languages over the boolean and finite domains”	benchmarking;programmation logique avec contrainte;evaluation performance;constraint logic programs;performance evaluation;solver performance;unit testing;estudio comparativo;evaluacion prestacion;programacion logica con restriccion;clp;etude comparative;finite domain;comparative study;constraint programming;eclipse;constraint logic programming	The comparative study published in this journal by Fernandez and Hill benchmarked some constraint programming systems on a set of well-known puzzles. The current article examines the positive and negative aspects of this kind of benchmarking. The article analyses some pitfalls in benchmarking, recalling previous published results from benchmarking different kinds of software, and explores some issues in comparative benchmarking of CLP systems. A benchmarking exercise should cover a broad set of representative problems and a broad set of programming constructs. This can be achieved using two kinds of benchmarking: Applications Benchmarking and Unit Testing. The article reports the authors' experiences with these two kinds of benchmarking in the context of the CHIC2 Esprit project. The benchmarks were used to unit test different features of the CLP system ECLiPSe and to compare application development with different high-level constraint platforms. The conclusion is that, in deciding which system to use on a new application, it is less useful to compare standard features of CLP systems, than to compare their relevant functionalities.	constraint logic programming;constraint programming;eclipse;estimation of signal parameters via rotational invariance techniques;high- and low-level;unit testing	Mark Wallace;Joachim Schimpf;Kish Shen;Warwick Harvey	2004	Constraints	10.1023/B:CONS.0000006181.40558.37	constraint logic programming;eclipse;constraint programming;computer science;artificial intelligence;comparative research;unit testing;programming language;algorithm;benchmarking	SE	-25.769098851922717	24.280106147199014	72544
43ad597f11f98266d56570a224faf1d0f77ba05d	a case for object-oriented real-time systems (oorts)	object oriented;real time systems	Real-time systems have several distinguishing features and requirements that make them difficult or, at least challenging, examples for object orientation. These include the need to satisfy much stricter timing, concurrency, and fault tolerance constraints than exist in most other kinds of systems. Generally, real-time systems must have predictable behaviors—in time and in other resource usage. The question is whether object-oriented (OO) techniques offer or potentially offer a framework and discipline for designing and implementing predictableandefficientsystems. OO systems are those that use data abstraction ideas, abstract data types, encapsulation, classes and instances, inheritance, and polymorphism, in a central way. The main arguments againstOORTS are that that they lead to unpredictable and inefficient systems. (Similar arguments, of course, were made against the use of higher-level languages, such as Fortran, in the 1950s, against the use of multi-tasking in the 70s, against the use of window systems in the 80s, and against many other innovations.) More specifically, inheritance and polymorphism seem to be especially offending in this regard; for example, so-called inheritance anomalies have been particularly worrisome. It is interesting to note that the other OO features have been around and used for a longer period of time, and are thus more mature. At this point, there is an overwhelming amount of evidence for further supporting the study and development of OORTS. We present some of this evidence below. In each case, we note the existence and application of specific systems that use OO ideas, and point out the relevance to OORTS; the cited references provide more details. An appendix discusses real-time inheritance anomalies.	abstract data type;abstraction (software engineering);computer multitasking;concurrency (computer science);encapsulation (networking);fault tolerance;fortran;real-time cmix;real-time clock;real-time computing;real-time operating system;real-time transcription;relevance;requirement	Alan C. Shaw	2000	Real-Time Systems	10.1023/A:1008106711942	real-time computing;computer science;object-oriented programming	Embedded	-23.06247739368226	31.77134878009369	72546
a4352888eaff3bf95c4a7c26f49632363df3ccf2	specifying multithreaded java semantics for program verification	diversity;multi threading;computer languages;yarn;java yarn multithreading interleaved codes computer science computer languages permission hardware formal verification software design;formal specification;architectural description language;executable specification;java programming;java language specification;multithreaded java semantics;component based development;rule based;java memory model;software construction idioms;program verification;product population;interleaved codes;formal verification;permission;operations reordering;product family;formal executable specification;computer science;software design;guarded commands;configuration management;software construction idioms multithreaded java semantics program verification multithreading java memory model operations reordering formal executable specification guarded commands;hardware;multithreading;java;formal specification java multi threading program verification	"""The Java programming language supports multithreading where the threads interact among themselves via read/write of shared data. Most current work on multithreaded Java program verification assumes a model of execution that is based on interleaving of the operations of the individual threads. However, the Java language specification (which any implementations of Java multithreading must follow) supports a weaker model of execution, called the Java Memory Model (JMM). The JMM allows certain reordering of operations within a thread and thus permits more behaviors than the interleaving based execution model. Therefore, programs verified by assuming interleaved thread execution may not behave correctly for certain Java multithreading implementations.The main difficulty with the JMM is that it is informally described in an abstract rule-based declarative style, which is unsuitable for formal verification. In this paper, we develop an equivalent formal executable specification of the JMM. Our specification is operational and uses guarded commands. We then use this executable model to verify popular software construction idioms (commonly used program fragments/patterns) for multithreaded Java. Our prototype verifier tool detects a bug in the widely used """"Double-Checked Locking"""" idiom, which verifiers based on interleaving execution model cannot possibly detect."""	declarative programming;double-checked locking;executable;formal verification;forward error correction;guarded command language;java memory model;lock (computer science);logic programming;multithreading (computer architecture);programming language specification;prototype;software construction;thread (computing)	Abhik Roychoudhury;Tulika Mitra	2002		10.1145/581339.581399	rule-based system;parallel computing;real-time computing;multithreading;jsr 94;java concurrency;computer science;operating system;java modeling language;strictfp;real time java;programming language;java;generics in java;scala;java annotation	SE	-26.32458499261847	28.684155981259362	72608
5d9cd39f16e1d4bf6288150e9d44f8bc350760a9	incorporating transformations into jflap for enhanced understanding of automata	church turing thesis;java programming;jflap;turing machine;educational software;automata;automata theory;undecidability;finite state machine	This paper describes our experience extending the educational software program JFLAP. JFLAP enables students to design and simulate automata such as Finite State Machines, push-down automata, and Turing Machines. It is a valuable aid in any Automata Theory course. We modified JFLAP so that students could write Java programs that alter the actual automaton itself. This utility greatly increases the students' ability to grasp such fundamental topics as the Church-Turing thesis and the theory of undecidability.	automata theory;church–turing thesis;computer program;finite-state machine;jflap;java;pushdown automaton;simulation;turing machine;undecidable problem	Joan M. Lucas;Jonathan Jarvis	2008		10.1145/1352135.1352143	computer science;turing machine;theoretical computer science;automata theory;church–turing thesis;automaton;educational software;finite-state machine;programming language;algorithm	PL	-27.291402974601883	23.242225474106377	73072
b213dcd5904279eb626fe7135a7cb971c9793af5	preventing instantiation errors and loops for logic programs with multiple modes using block declarations	program design;program transformation;qa 76 software;conception programme;logical programming;program verification;transformation programme;analisis programa;computer programming;verificacion programa;transformacion programa;programmation logique;program analysis;logic programs;analyse programme;verification programme;programacion logica;concepcion programa	This paper presents several verification methods for logic programs with delay declarations. It is shown how type and instantiation errors related to built-ins can be prevented, and how termination can be ensured. Three features are distinctive of this work: it is assumed that predicates can be used in several modes; it is shown that block declarations, which are a very simple delay construct, are sufficient to ensure the desired properties; the selection rule is taken into account, assuming it to be the rule of most Prolog implementations. The methods can be used both to verify existing programs and to assist in writing new programs.	logic programming;universal instantiation	Jan-Georg Smaus;Patricia M. Hill;Andy King	1998		10.1007/3-540-48958-4_16	program analysis;real-time computing;computer science;computer programming;programming language;algorithm	AI	-22.748996679633205	29.55929916774949	73215
fc957724c0063f79bc5153d4c7cc680514bdca85	continuous medical monitoring using wireless microsensors	wireless sensor;wireless sensors;medical monitoring;wearable computer;wearable computing		wireless router	Eugene Shih;Vladimir Bychkovsky;Dorothy Curtis;John V. Guttag	2004		10.1145/1031495.1031553	embedded system;wearable computer;computer science	Mobile	-32.38128715639486	22.40036949585458	73278
245c3e157ab8df11edb76d6aac22c0fb2ac2adce	characterizing transactional memory consistency conditions using observational refinement		Transactional memory (TM) facilitates the development of concurrent applications by letting a programmer designate certain code blocks as atomic. The common approach to stating TM correctness is through a consistency condition that restricts the possible TM executions. Unfortunately, existing consistency conditions fall short of formalizing the intuitive semantics of atomic blocks through which programmers use a TM. To close this gap, we formalize programmer expectations as observational refinement between TM implementations. This states that properties of a program using a concrete TM implementation can be established by analyzing its behavior with an abstract TM, serving as a specification of the concrete one.  We show that a variant of Transactional Memory Specification (TMS), a TM consistency condition, is equivalent to observational refinement for a programming language where local variables are rolled back upon a transaction abort. We thereby establish that TMS is the weakest acceptable condition for this case. We then propose a new consistency condition, called Strong Transactional Memory Specification (STMS), and show that it is equivalent to observational refinement for a language where local variables are not rolled back upon aborts. Finally, we show that under certain natural assumptions on TM implementations, STMS is equivalent to a variant of a well-known condition of opacity.  Our results suggest a new approach to evaluating TM consistency conditions and enable TM implementors and language designers to make better-informed decisions.	apl;code::blocks;consistency model;correctness (computer science);distributed computing;dynamic data;journal of the acm;liveness;local variable;nested transaction;programmer;programming language;programming language theory;programming model;refinement (computing);requirement;transactional memory	Hagit Attiya;Alexey Gotsman;Sandeep Hans;Noam Rinetzky	2017	J. ACM	10.1145/3131360	abort;local variable;theoretical computer science;atomicity;discrete mathematics;implementation;correctness;computer science;programmer;transactional memory;database transaction	PL	-20.82600250653034	25.094476435938343	73337
61704a94e044394e92e83446c7d567553bcf458a	smart card programming and security		ions The idea of deriving abstract functions from a concrete function and an abstraction function, which traces back to Cousot and Cousot’s seminal paper [14], see also [12,13], has been exploited in a number of contexts, and in particular in the context of formal verification, see e.g. [4,8,17] for recent examples. Our work around JTK can be viewed as a simple application of abstract interpretation techniques to term-rewriting. Applications to JavaCard Jakarta is tailored to the design of certified bytecode verifiers. There have been a number of related efforts, both to prove the standard bytecode verifier correct [7,27,33], and to suggest new bytecode verifiers based on more complex type systems [25]. Most of this work is tailored towards a particular security policy, to the exception of [27], where it is shown how to use a data flow analyser to construct a bytecode verifier from an abstract virtual machine. In contrast, our work is focused on deriving abstract virtual machines from defensive ones, and proving them correct. We are not aware of any similar effort, despite tremendous activity in the field—see e.g. [19] for a recent survey of ongoing work.	abstract interpretation;card reader;database;dataflow;formal verification;java card;java virtual machine;money;public-key cryptography;rewriting;smart card;tracing (software);type system	Jan van Leeuwen;Isabelle Attali;Thomas Jensen	2001		10.1007/3-540-45418-7	multos;smart card;java card;logical security;smart card application protocol data unit;card reader;open smart card development platform;openpgp card;basiccard;contactless smart card	Security	-20.677580901831288	28.011080438099363	73419
138155310771016016d6f10d57b0eebf4d2e36a1	soundness proof of eventb2java	electronic mail;semantics;computational modeling;syntactics;safety;context;java	The EventB2Java tool generates JML-specified Java implementations for Event-B models. Code generation is based on the definition of some syntactic rules. This paper presents a soundness proof for the translation encoded by those rules. This proof is important as Event-B is typically used to model safety critical systems, and hence we want to increase our trust on that the code generated by EventB2Java is correct. We conduct our proof in Coq. The proof relies on the definition of a state transition semantics for Event-B and JML in Coq. The soundness proof condition states that any transition step in the semantics of JML in Coq is matched by a transition step of the semantic of the Event-B construct from which the JML construct was translated.	axiomatic system;b-method;compiler;complexity;coq (software);discharger;formal language;java modeling language;level of detail;one-way function;state transition table;transducer;transition system;type constructor	Néstor Cataño;Shigeo Nishi	2016	2016 Seventh Latin-American Symposium on Dependable Computing (LADC)	10.1109/LADC.2016.15	computer science;theoretical computer science;semantics;programming language;java;computational model;algorithm	PL	-20.888233705705233	26.405908332802298	73494
6ba9b386bb63f9ea9a5266cfc137e9ac902bce29	programmable rewriting strategies in haskell: -- white paper --	rewrite startegies;programming language;functional programming;language processing;polymorphism;generating function;programming languages;haskell;generic programming	Programmable rewriting strategies provide a valuable tool for implementing traversal functionality in grammar-driven (or schema-driven) tools. The working Haskell programmer has access to programmable rewriting strategies via two similar options: (i) the Strafunski bundle for generic functional programming and language processing, and (ii) the “Scrap Your Boilerplate” approach to generic functional programming. Basic rewrite steps are encoded as monomorphic functions on datatypes. Rewriting strategies are polymorphic functions composed from appropriate basic strategy combinators. We will briefly review programmable rewriting strategies in Haskell. We will address the following questions: • What are the merits of Haskellish strategies? • What is the relation between strategic programming and generic programming? • What are the challenges for future work on functional strategies?	combinatory logic;functional programming;generic programming;haskell;programmer;rewrite (programming);rewriting;strafunski	Ralf Lämmel	2005	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2004.11.021	polymorphism;first-generation programming language;higher-order programming;generating function;protocol;declarative programming;very high-level programming language;programming domain;reactive programming;functional reactive programming;computer science;theoretical computer science;functional logic programming;programming paradigm;procedural programming;inductive programming;fifth-generation programming language;programming language;functional programming;generic programming;algorithm	PL	-23.327174122526532	23.08568097193264	73578
a4d1265e3b7473e73ab168b8fa06d185733f853c	inferring cost equations for recursive, polymorphic and higher-order functional programs	higher order;polymorphism;higher order functions;reconstruction algorithm	This paper presents a type-based analysis for inferring sizeand cost-equations for recursive, higher-order and polymorphic functional programs without requiring user annotations or unusual syntax. Our type reconstruction algorithm is capable of inferring first-order cost equations for a non-trivial subset of higher-order, recursive and polymorphic functions. We illustrate the approach with reference to some standard examples of recursive programs.	algorithm;computer algebra system;display resolution;effect system;first-order predicate;functional programming;general-purpose modeling;haskell;hindley–milner type system;hume (programming language);intuitionistic type theory;maple;mupad;principal type;prototype;recurrence relation;recursion (computer science);run time (program lifecycle phase);type inference;wolfram mathematica	Pedro B. Vasconcelos;Kevin Hammond	2003		10.1007/978-3-540-27861-0_6	polymorphism;higher-order logic;computer science;theoretical computer science;programming language;higher-order function;μ operator;algorithm;recursive partitioning	PL	-22.108764474594494	24.655090120850748	73632
2040b93934e4f50ac90214425ce3b9a58d7cf1e5	on the analysis of cascading style sheets	debugging;cascading style sheets;css;style sheets;web development	Developing and maintaining cascading style sheets (CSS) is an important issue to web developers as they suffer from the lack of rigorous methods. Most existing means rely on validators that check syntactic rules, and on runtime debuggers that check the behavior of a CSS style sheet on a particular document instance. However, the aim of most style sheets is to be applied to an entire set of documents, usually defined by some schema. To this end, a CSS style sheet is usually written w.r.t. a given schema. While usual debugging tools help reducing the number of bugs, they do not ultimately allow to prove properties over the whole set of documents to which the style sheet is intended to be applied. We propose a novel approach to fill this lack. We introduce ideas borrowed from the fields of logic and compile-time verification for the analysis of CSS style sheets. We present an original tool based on recent advances in tree logics. The tool is capable of statically detecting a wide range of errors (such as empty CSS selectors and semantically equivalent selectors), as well as proving properties related to sets of documents (such as coverage of styling information), in the presence or absence of schema information. This new tool can be used in addition to existing runtime debuggers to ensure a higher level of quality of CSS style sheets.	cascading style sheets;compile time;compiler;database schema;debugger;debugging;sensor;software bug;style sheet (web development);validator;web developer	Pierre Genevès;Nabil Layaïda;Vincent Quint	2012		10.1145/2187836.2187946	style sheet language;computer science;artificial intelligence;database;style sheet;cascading style sheets;world wide web;algorithm	SE	-26.70513536729585	28.11074810619073	73686
4e99cc4f9a690c9705c37418cecefbe6a04d8112	object-oriented functional programming and type reconstruction	functional programming;code reuse;object oriented;type classes;reconstruction algorithm;type inference;functional programming language	This paper presents an object-oriented functional programming language core, its ML-style static type inference and a corresponding type reconstruction algorithm. The language allows object definitions as in Ei f fe l , algebraic value definitions as in Haskell and most code reuses in E i f f e l and Haskell. The type inference combines the usual structural subtyping relation and Haskell type classes. The type reconstruction algorithm computes static types for those programs, in which explicit type annotations are given at the positions of subtyping coercions. 1 I n t r o d u c t i o n Object-oriented (programming) languages are suitable for describing real-world objects by features, while functional (programming) languages naturally construct algebraic values and operations at an abstract level. Although algebraic values and operations are supported in almost all object-oriented languages, they can usually be defined only at a low level. We believe that the situation should be improved in many applications, and a way to do this would be to extend an object-oriented language so that Mgebraic values and operations can be defined in the functional programming style. Note that real-world objects can also be represented as functional values in many cases, but the coding is much less natural than in object-oriented languages, since functional structures do not enjoy many properties of objects characterized by features. Both object-oriented languages and functional languages provide excellent support for writing reusable code. But the forms of code reuse they provide are very different. Some main features supporting code reuse in object-oriented languages, e.g. E i 2 f e l [28], are inheritance of classes and late binding of redefined methods. Major features supporting code reuse in functional languages, e.g. ML [29] and Haske l l [23], are ML-style polymorphism, overloaded functions [42] and higher-order functions. Note that ML-style polymorphism usually means at least two things: (1) absence of run-time type error for typable expressions, where a type may contain variables all-quantified at the outermost position, (2) the existence of principal ~ypes (i.e. most general types) for typable expressions and their automatic reconstruction. In order to combine the object-oriented and functional paradigms, we propose an object-oriented funclional (programming) language TOFL, which unifies several notions in object-oriented languages such as object classes, inheritance and late binding, and several notions in functional languages such as algebraic data types, higherorder functions and ML-style polymorphism. This paper presents some aspects in * Research partially supported by ESPRIT Basic Research WG COMPASS 6112.	algebraic data type;algorithm;compass;code reuse;definition;field electron emission;functional programming;haskell;higher-order function;late binding;linear algebra;programming language;programming style;structural type system;type class;type inference;type safety	Zhenyu Qian;Bernd Krieg-Brückner	1995		10.1007/3-540-61629-2_58	type conversion;protocol;declarative programming;uniqueness type;programming domain;data type;reactive programming;functional reactive programming;computer science;theoretical computer science;functional logic programming;object type;programming paradigm;procedural programming;symbolic programming;inductive programming;fifth-generation programming language;programming language;object-oriented programming;algorithm	PL	-24.979639436615614	26.867004991715444	73727
3fb629cc4c85364d711a6007d5dc9eb9de988715	proof support for raise by a reuse approach based on institutions	developpement logiciel;teoria demonstracion;theorie preuve;proof theory;reutilizacion;specification;semantics;semantica;semantique;algebre;specification language;reuse;theorem proving;demonstration theoreme;theorem prover;especificacion;algebra;desarrollo logicial;software development;lenguaje especificacion;demostracion teorema;langage specification;higher order logic;reutilisation	"""(03/02/2018)  Proof support for RAISE by a Reuse Approach Based on Institutions Formal methods are mathematically based methods for developing software. Such methods usually involve that software and requirements are specified in a formal specification language, after which it is verified that the software meets the requirements. RAISE is a formal method with the associated specification language RSL and a proof system. Computerbased proof tools are available for RAISE, but a higher degree of automation is desired. Isabelle/HOL is a proof assistant for higher-order logic (HOL). It is an instantiation of the generic proof assistant Isabelle which offers a suitable degree of automation and flexibility. In order to use the Isabelle/HOL proof assistant for the RAISE method, translation from RSL to HOL is considered. The translation is based on institutions which formalize the informal notion of """"a logical system"""". Institutions and morphisms between institutions are presented together with specifications over institutions and modeltheoretic semantics of specifications. The concept of """"light institution comorphisms"""" is introduced as a modification of wellknown institution comorphisms, and it is proved that a light institution comorphism enables sound reuse of proof assistants when it has certain properties. Moreover, the concept of looser semantics of specifications is introduced as a modeltheoretic description of the semantics of RSL specifications, and an equivalence result is proved. An institution for an applicative, deterministic subset of RSL, referred to as """"mRSL"""", is defined. Then, a well-known institution for HOL is presented, and Isabelle/HOL is briefly described. An institution comorphism from the mRSL institution to the HOL institution is defined, providing a translation from mRSL to Isabelle/HOL, and it is proved that the light institution comorphism has the properties that enable sound reuse of the Isabelle/HOL proof assistant. The use of the translation is described in connection with three examples: logical circuits, a generalized railway crossing, and an encoding of Duration Calculus in RSL. In Danish: Formelle metoder er matematisk baserede metoder til udvikling af programmel. Sådanne metoder involverer som regel, at programmel og krav beskrives i et formelt specifikationssprog, hvorefter det verificeres, at programmellet opfylder de ønskede krav. RAISE er en formel metode med det tilhørende specifikationssprog RSL og et bevissystem. Der findes datamatbaserede bevisværktøjer til RAISE, men en højere grad af automation er ønskværdig. Isabelle/HOL er en bevisfører for højereordens logik (HOL). Bevisføreren er en instantiering af den generiske bevisfører Isabelle, der tilbyder en passende grad af automation og fleksibilitet. Med henblik på at benytte bevisføreren Isabelle/HOL i forbindelse med RAISE metoden, betragtes oversættelse fra RSL til HOL. Oversættelsen baseres på institutioner, der formaliserer det uformelle begreb """"logisk system """". Institutioner og morfier mellem institutioner gennemgås sammen med specifikationer over institutioner og modelteoretisk semantik af specifikationer. Begrebet """"let institutionscomorfi"""" introduceres som en modifikation af velkendte institutionscomorfier, og det bevises, at en let institutionscomorfi muliggør sund genbrug af bevisførere, når den har visse egenskaber. Derudover introduceres begrebet løsere semantik af specifikationer som en modelteoretisk beskrivelse af semantikken af RSLspecifikationer, og et ækvivalensresultat bevises. Der defineres en institution for en applikativ, deterministisk delmængde af RSL, der benævnes """"mRSL"""". Herefter præsenteres en velkendt institution for HOL, og Isabelle/HOL gennemgås kort. Der defineres en let institutionscomorfi fra mRSLinstitutionen til HOL-institutionen, hvorved der fås en oversættelse fra mRSL til Isabelle/HOL, og det bevises, at den definerede lette institutionscomorfi har de egenskaber, der muliggør sund genbrug af bevisføreren Isabelle/HOL. Brugen af den definerede oversættelse beskrives i forbindelse med tre eksempler: logiske kredsløb, en jernbaneoverskæring og en indkodning af varighedskalkylen i RSL."""	applicative programming language;duration calculus;formal methods;formal specification;formal system;gradient;hol (proof assistant);isabelle;list of concept- and mind-mapping software;loose coupling;proof assistant;proof calculus;raise;requirement;specification language;the grid analysis and display system (grads);turing completeness;universal instantiation	Morten P. Lindegaard;Anne Elisabeth Haxthausen	2004		10.1007/978-3-540-27815-3_26	discrete mathematics;computer science;mathematics;semantics;automated theorem proving;programming language;algorithm	PL	-20.189454670542247	21.55105441722217	73965
c03179638244859e2d67ad16ed5c26f6a930bd38	neverlang 2 - componentised language development for the jvm		Traditional compiler development is non-modular. Although syntax extension and DSL embedding is making its way back in modern language design and implementation, componentisation in compiler construction is still an over- looked matter. Neverlang is a language development framework that emphasises modularity and code reuse. Neverlang makes extension, restriction and feature sharing easier, by letting developers define language components in distinct, in- dependent units, that can be compiled independently and shared across different language implementations, even in their compiled form. The semantics of the im- plemented languages can be specified using any JVM-supported language. In this paper we will present the architecture and implementation of Neverlang 2, by the help of an example inspired by mobile devices and context-dependent behaviour. The Neverlang framework is already being employed successfully in real-world environments.		Walter Cazzola;Edoardo Vacchi	2013		10.1007/978-3-642-39614-4_2	compiler;object language;computer science;theoretical computer science;compiled language;low-level programming language;programming language;communication	NLP	-28.294746872531693	28.612254558268567	74061
bc8cd196898ab7fe317384a886358844b5a4e2c1	tutorial i: syntax-guided synthesis	software engineering computational linguistics firmware game theory program verification programming languages security of data;game theory;program verification;firmware;software engineering;educational institutions awards activities tutorials software computer languages computers;syntax guided synthesis frenetic network programming game based synthesis secure programs firmware validation;computational linguistics;security of data;programming languages	These tutorials discusses the following: Syntax-Guided Synthesis; Firmware Validation Challenges and Opportunities; Secure Programs via Game-based Synthesis; and Network Programming in Frenetic.	firmware;frenetic	Rajeev Alur	2013	2013 Eleventh ACM/IEEE International Conference on Formal Methods and Models for Codesign (MEMOCODE 2013)		game theory;firmware;computer science;computational linguistics;software engineering;programming language;game programming	EDA	-28.526070557084193	21.33916966058227	74331
2f38968530d0970f7392d9d8868ff83759ae21c2	safkasi: a security mechanism for language-based systems	java bytecode;applets;internet;security passing style;security architecture;stack inspection;access control;static analysis;www;java	In order to run untrusted code in the same process as trusted code, there must be a mechanism to allow dangerous calls to determine if their caller is authorized to exercise the privilege of using the dangerous routine. Java systems have adopted a technique called stack inspection to address this concern. But its original definition, in terms of searching stack frames, had an unclear relationship to the actual achievement of security, overconstrained the implementation of a Java system, limited many desirable optimizations such as method inlining and tail recursion, and generally interfered with interprocedural optimization. We present a new semantics for stack inspection based on a belief logic and its implementation using the calculus of security-passing style which addresses the concerns of traditional stack inspection. With security-passing style, we can efficiently represent the security context for any method activation, and we can build a new implementation strictly by rewriting the Java bytecodes before they are loaded by the system. No changes to the JVM or bytecode semantics are necessary. With a combination of static analysis and runtime optimizations, our prototype implementation showes reasonable performance (although traditional stack inspection is still faster), and is easier to consider for languages beyond Java. We call our system SAFKASI (the Security Architecture Formerly Known as Stack Inspection).	authorization;inline expansion;interprocedural optimization;java;mathematical optimization;prototype;recursion;rewriting;static program analysis;tail call	Dan S. Wallach;Andrew W. Appel;Edward W. Felten	2000	ACM Trans. Softw. Eng. Methodol.	10.1145/363516.363520	stack trace;real-time computing;the internet;call stack;computer science;access control;theoretical computer science;software engineering;java modeling language;real time java;programming language;java;static analysis;enterprise information security architecture;java applet;java annotation	Security	-22.47443286427312	30.212672289693863	74490
1cd28569931193acb2b0dee2363eb61327d57666	a technique for creating small fast compiler frontends	programming language;semantics;abstraction;pseudo random function;it value;type checking;hash table;polymorphism;algebras;hash function;augmented transition network;security;constancy;data types	A technique using minimal perfect hash functions to generate small fast table driven frontends is described. A parser for the PASCAL language generated by this method is then presented.In this paper a technique for creating small fast table driven compiler frontends is described. To demonstate the practicality of this method, we give the code and tables for a demonstration parser for the PASCAL language.Our technique is based on minimal perfect hash functions, MPHF. A MPHF is a bijection from a set of N objects to the first N consecutive non-negative integers. We use the technique for generating MPHFs described by the author in [2] to generate three different hash tables: the parser table, the lexical table and the reserved word table. In [2] the author described an algorithm called the mincycle algorithm whose input is W: a set of N objects, R1 and R2: two positive integers, and hO:W-&gt;I, hi:W-&gt;O., R1-1 and h2:W-&gt;R1., R-1: three pseudo-random functions. Here I is the set of integers and R=R1+R2. The mincycle algorithm looks for and can be expected to find a function g:0., R-1-&gt;I such that h(w) = (h0(w) + g(h1(w)) + g(h2(w))) mod N is a MPHF.We will not give the details of the mincycle algorithm as they are described in detail elsewhere [2]. We merely note that if we can guarantee that w &#949; W, w' e W, h1(w)=hl(w') and h2(w)=h2(w') implies that w=w' then h0 serves no useful purpose and may be omitted.The parser is created directly from syntax diagrams which are slightly modified from those given by Jensen and Wirth [1]. From these syntax diagrams we have created an augmented transition network (ATN) [3]. The only actions we use are push and pop for a syntax stack containing states of the ATN and install, lookup, newscope and oldscope which manipulate a rudimentary symbol table. We use information from the symbol table only at states from which more than one valid transition is possible on seeing an identifier. Thus, checking for valid semantics is extremly rudimentary, but more complete semantic checking could be added to the existing syntactic framework.The parser code is given in figure 1. The output of lex is syntactic tokens. Statement 1: is a MPHF. If we do not find the state-token pair in the transition table (transtab) we use the default table (deftab) to find the next state and a (possibly null) action. The transition table (Table 1) gives each valid non-default state-token pair of the ATN along with its position in the transition table and its associated next state and action.The state table (Table 2) gives for each state its value in the state indirect table (stindtab) and its associated default next state and action. The token table (Table 3) gives for each token its value in the token indirect table (tokindtab). The parser contains 126 states, 53 tokens and 196 valid non-default state-token pairs.The lexical analyzer was created in a manner similar to the parser. The reserved word table was created using the algorithm given in [2]. Unlike Jensen and Wirth [1] we consider read, readln, write and writeln to be reserved words. For reasons of space only the parser is shown here.Although PASCAL is small as programming languages go, because of the ease with which the method described above created a PASCAL frontend, the author strongly suspects that this method is applicable to larger languages such as ADA.	algorithm;augmented transition network;compiler;gnu readline;hash table;identifier;jensen's inequality;lex (software);lexical analysis;lookup table;modulo operation;parsing;pascal;perfect hash function;programming language;pseudorandomness;r language;reserved word;stack (abstract data type);state transition table;symbol table;syntactic predicate;syntax diagram	Thomas J. Sager	1985	SIGPLAN Notices	10.1145/382286.382384	state transition table;polymorphism;hash table;hash function;augmented transition network;data type;computer science;branch table;theoretical computer science;pseudorandom function family;semantics;abstraction;programming language;symbol table;algorithm	AI	-24.323717981088723	25.038998959376855	74528
e1071db75746452b7d3ab866df62cb87bbabd545	silver: an extensible attribute grammar system	attribute grammar;specification language;pattern matching;domain specific language;data flow analysis;language extension;domain specificity	Attribute grammar specification languages, like many domain specific languages, offer significant advantages to their users, such as high-level declarative constructs and domain-specific analyses. Despite these advantages, attribute grammars are often not adopted to the degree that their proponents envision. One practical obstacle to their adoption is a perceived lack of both domain-specific and general purpose language features needed to address the many different aspects of a problem. Here we describe Silver, an extensible attribute grammar specification language, and show how it can be extended with general purpose features such as pattern matching and domain specific features such as collection attributes and constructs for supporting data-flow analysis of imperative programs. The result is an attribute grammar specification language with a rich set of language features. Silver is implemented in itself by a Silver attribute grammar and utilizes forwarding to implement the extensions in a cost-effective manner.	attribute grammar	Eric Van Wyk;Derek Bodin;Jimin Gao;Lijesh Krishnan	2008	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2008.03.047	grammar systems theory;natural language processing;l-attributed grammar;link grammar;specification language;operator-precedence grammar;computer science;domain-specific language;affix grammar;theoretical computer science;data-flow analysis;pattern matching;regular tree grammar;extended affix grammar;emergent grammar;programming language;attribute grammar;unrestricted grammar;adaptive grammar;mildly context-sensitive grammar formalism	Logic	-26.4404866724259	22.38883528894884	74660
5d01e349202322c6c536e48611b1e78be9adeff0	translating refined logic programs to mercury	target implementation language;mercury code;standard prolog;correct program;specification language;refinement calculus;prototype tool;refined logic program;logic program	A refinement calculus provides a method for transforming specifications to executable code, maintaining the correctness of the code with respect to its specification. In this paper we investigate the use of Mercury as the target implementation language for a refinement calculus for logic programs. We describe a prototype tool for translating programs in our specification language to Mercury code. More generally, we investigate the advantages that Mercury has over standard Prolog, with respect to developing correct programs from specifications.	correctness (computer science);executable;logic programming;mercury;object language;prolog;prototype;refinement (computing);refinement calculus;specification language	Robert Colvin;Ian J. Hayes;David Hemer;Paul A. Strooper	2002			refinement calculus;specification language;computer science;theoretical computer science;refinement;proof calculus;programming language;logic programming;algorithm	PL	-21.08272964452291	26.4028657048714	74768
5ac835f66338ce25a963ec27c70b216837b3eff6	my life with an automatic theorem prover		Sledgehammer integrates third-party automatic theorem provers in the proof assistant Isabelle/HOL. In the seven years since its first release in 2007, it has grown to become an essential part of most Isabelle users’ workflow. Although a lot of effort has gone into tuning the system, the main reason for Sledgehammer’s success is the impressive power of the external provers, especially E, SPASS, Vampire, and Z3. In this paper, I review Vampire’s strengths and weaknesses in this context and propose a few directions for future work. Isabelle/HOL [27] is a proof assistant based on polymorphic higher-order logic extended with axiomatic type classes. Its tool Sledgehammer [5,28] integrates third-party automatic theorem provers to increase the level of automation. It consists of three main components: • The relevance filter [23, 26] heuristically selects a few hundred facts (lemmas, definitions, axioms) from the thousands available to Isabelle users. • The translation module [6, 25] encodes polymorphic higher-order propositions in the target prover’s logic (e.g., untyped or monomorphic first-order logic). • The reconstruction module [29, 33] produces a textual proof that can be inserted in an Isabelle development to replay the proof. Sledgehammer was originally designed by Lawrence Paulson and his team from 2003 to 2009. I took over its development in 2010 at Tobias Nipkow’s suggestion. In the last seven years or so, it has grown to become an essential part of most Isabelle users’ workflow. The following provers are supported as backends: • Unit-equality prover: Waldmeister [17]; • Superposition-based first-order provers: E [32], SPASS [38], and Vampire [31]; • Instantiation-based first-order provers: iProver [19] and iProver-Eq [20]; • SMT (satisfiability modulo theories) solvers: Alt-Ergo [10], CVC3 [3], Yices [14], and Z3 [13]; • Higher-order provers: AgsyHOL [24], LEO-II [4], and Satallax [12]. It is straightforward to extend this list with provers that can communicate in the TPTP/ TSTP [35] or SMT-LIB [30] formats. For historical reasons, the SMT solver translation is distinct from the TPTP/TSTP translation. Originally, most provers developed in the community around CADE and TPTP had no support for types or arithmetic, but this is now changing [37]. Systems such as Isabelle adhere to the tradition initiated in the 1970s by the LCF system [16]: All inferences are derived by a small trusted kernel; types and functions are defined	alt-ergo;automated theorem proving;conference on automated deduction;ergo proxy;first-order logic;first-order predicate;hol (proof assistant);heuristic;isabelle;modulo operation;proof assistant;relevance;spass;satisfiability modulo theories;solver;statistical machine translation;type class;universal instantiation;z3 (computer)	Jasmin Christian Blanchette	2014			gas meter prover;automated theorem proving;axiom;workflow;algorithm;hol;vampire;satisfiability modulo theories;proof assistant;mathematics	Logic	-20.36768102212598	20.79550038421125	74888
111052541582e3512254f405a042341c66237b38	parametric trace slicing and monitoring	online monitoring;trace analysis;text;monitoring system;program analysis;computer science	Analysis of execution traces plays a fundamental role in many program analysis approaches. Execution traces are frequently parametric, i.e., they contain events with parameter bindings. Each parametric trace usually consists of many trace slices merged together, each slice corresponding to a parameter binding. Several techniques have been proposed to analyze parametric traces, but they have limitations: some in the specification formalism, others in the type of traces they support; moreover, they share common notions, intuitions, even techniques and algorithms, suggesting that a fundamental understanding of parametric trace analysis is needed. This foundational paper gives the first solution to parametric trace analysis that is unrestricted by the type of parametric properties or traces that can be analyzed. First, a general purpose parametric trace slicing technique is discussed, which takes each event in the parametric trace and distributes it to its corresponding trace slices. This parametric trace slicing technique can be used in combination with any conventional, non-parametric trace analysis, by applying the latter on each trace slice. An online monitoring technique is then presented based on the slicing technique, providing a logic-independent solution to runtime verification of parametric properties. The presented monitoring technique has been implemented and extensively evaluated. The results confirm that the generality of the discussed techniques does not come at a performance expense when compared with existing monitoring systems.	algorithm;formal system;language binding;name binding;numerical analysis;program analysis;runtime verification;tracing (software)	Feng Chen;Grigore Rosu	2009		10.1007/978-3-642-00768-2_23	program analysis;real-time computing;computer science;theoretical computer science;programming language;algorithm	PL	-19.703334317197903	28.869706313007065	74989
c038deddc3b11b316c544e12ec10376110d6405b	the precise handling of qualitative relationships	information retrieval;strategy;automatic programming language;translation;qualitative information;relationships;tactics	The Transtaxor is a computer program which correlates a given problem definition or overall strategy, expressed as a general input syntax, with a given set of circumstances, expressed as a specific input ''situation string'' to produce an output formula or course of action. The transtaxor is written once for a given computer and thereafter becomes applicable to any problem which can be expressed in terms of a syntax. In order to extend the methodology to a wide variety of problems which can be expressed qualitatively, the usual syntax definitions have been extended to provide for the inclusion of relations, the division of the syntax into levels, and the paralleling of these levels through ''developable agendums''. A simple but powerful ''transcribing language'' incorporates the ''action-directing language'' into the processing. The concept of ''delineation'' facilitates the generation of the basic parts, which are developed by the transtaxor, with the aid of a list-processing language, into progressively higher levels of the syntax until the final goal is produced. Examples are given of the application of the methodology to automatic-programming-language translation and information retrieval.		Robert S. Ledley;James Bruce Wilson	1975	Comput. Lang.	10.1016/0096-0551(75)90010-7	natural language processing;translation;abstract syntax;strategy;computer science;programming language;algorithm	AI	-24.691208413314342	18.297166727964953	75083
85c822e88f5929624fa53a9799715884e9bb7738	design and implementation of abstract graphical data types	tellurium graphics writing programming profession;tellurium;data type;design and implementation;programming profession;writing;graphics	Abstract graphical data types have been de signed and implemented as an extension of the PASCAL language. This extension gives the user a way of defining and using specific graphical types. It is shown how these types can be used as other PASCAL types. The concept of image transformation is also presented. This extension has been realized by a preprocessor and major features of the implementation are discussed.	graphical user interface	Daniel Thalmann;Nadia Magnenat-Thalmann	1979		10.1109/CMPSAC.1979.762551	data type;computer science;graphics;theoretical computer science;software engineering;database;tellurium;programming language;writing	Logic	-30.14115162789289	24.16716154297221	75094
2ca7f20442b44b0e986be22fc242d4f53f6ef734	antenna array design using spreadsheets	spreadsheet programs antenna arrays electrical engineering education computer aided instruction;computer program;antenna arrays;spreadsheet programs;computer aided instruction;linear array;computer aided instruction spreadsheet programs antenna arrays electrical engineering education electrical engineering computing;electrical engineering computing;half power beamwidth antenna array design spreadsheets linear arrays educational purposes uniform binomial chebyshev arrays polar array factor plots linear array factor plots;electrical engineering education;antenna array	This paper presents a spreadsheet method for the design and analysis of antenna arrays. The method exploits the familiarity of modern spreadsheets programs to a wide variety of computer users and offers them a quick and user friendly means to design and study the characteristics and performance of several classes of linear arrays. It is useful for educational purposes since it does not require writing computer programs nor access to advanced antenna design packages. The procedure is briefly described, and examples illustrating its use with uniform, binomial, and Chebyshev arrays are presented. Linear and polar array factor plots are generated, and estimates of the directivity and the half-power beamwidth are also obtained.	spreadsheet	Ali El-Hajj;Karim Y. Kabalan;Mohammed Al-Husseini	2003	IEEE Trans. Education	10.1109/TE.2003.813518	electronic engineering;computer science;electrical engineering;antenna array;computer engineering	EDA	-33.57085828139333	23.155812753429796	75292
288badc85b3e89a7addfc5c4347f28932670e344	functional logic design patterns	langage fonctionnel;lenguaje programacion;programming language;logic design;lenguaje funcional;data type;design and implementation;langage programmation;software design pattern;functional language	We introduce a handful of software design patterns for functional logic languages. Following usual approaches, for each pattern we propose a name and we describe its intent, applicability, structure, consequences, etc. Our patterns deal with data type construction, identifier declarations, mappings, search, nondeterminism and other fundamental aspects of the design and implementation of programs. We present some problems and we show fragments of programs that solve these problems using our patterns. The programming language of our examples is Curry. The complete programs are available on-line.	curry;functional logic programming;identifier;information needs;logic synthesis;online and offline;programming language;software design pattern	Sergio Antoy;Michael Hanus	2002		10.1007/3-540-45788-7_4	software design pattern;logic synthesis;data type;computer science;artificial intelligence;functional logic programming;mathematics;programming language;functional programming;algorithm	PL	-20.316554377267373	23.20246242673342	75364
9ec98724a1af12e951a7db594e6fd18010e5250f	visual tools for natural language processing	relational data;component based development;data flow program graphs;qa 76 software;module system;software engineering;visual programming;computer programming;large scale;development environment;research and development;visual modeling;visual development;language processing;visual representation;data dependence;general architecture for text engineering;visual programming systems;data flow;natural language processing	We describe GATE, the General Architecture for Text Engineering, an integrated visual development environment to support the visual assembly, execution and analysis of modular natural language processing systems. The visual model is an executable data flow program graph, automatically synthesised from data dependency declarations of language processing modules. The graph is then directly executable: modules are run interactively in the graph, and results are accessible via generic text visualisation tools linked to the modules. These tools lighten the ‘cognitive load’ of viewing and comparing module results by relating data produced by modules back to the underlying text, by reducing the amount of search in examining results, and by displaying results in context. Overall, the GATE integrated visual development environment leads to rapid understanding of system behaviour and hence to rapid system refinement, therefore demonstrating the utility of visual programming and visualisation techniques for the development of natural language processing systems. ( 2001 Academic Press	data dependency;dataflow;executable;gate;interactivity;natural language processing;refinement (computing);visual modeling;visual programming language	Robert J. Gaizauskas;Peter J. Rodgers;Kevin Humphreys	2001	J. Vis. Lang. Comput.	10.1006/jvlc.2000.0203	natural language processing;computer science;theoretical computer science;computer programming;database;development environment;programming language	SE	-32.480230727018125	24.168037982572788	75379
e52bbcc2a3f36cb16bf937a5efd6b6e515b7e2f6	research on warehouses management based on rfid and wsn technology	inventory management;human computer interaction;management system;rfid technology;storage management;information technology;temperature sensors;logistics data processing;cargo delivery;logistics information technology;wireless sensor network;wireless sensor networks human computer interaction inventory management logistics data processing radiofrequency identification supply chain management warehouse automation warehousing;servers;logistics;inventory pre warning;wsn technology;human computer interaction warehouse management wsn technology rfid technology supply chain management storage management logistics information technology radio frequency identification wireless sensor network inventory pre warning cargo delivery;warehousing;error rate;radio frequency identification;middleware;wireless sensor networks;radiofrequency identification;supply chain management;radiofrequency identification wireless sensor networks servers middleware real time systems temperature sensors logistics;warehouse automation;warehouse management;real time systems	in the supply chain management,the warehouse management is an important component.It is necessary to improve storage management efficiency and reduce operational costs and error rates.Therefore,it needs a new logistics information technology to replace the traditional mode of storage operation. RFID (radio frequency identification) and WSN (Wireless Sensor Network) technology can precisely solve this problem in logistics warehouse application.In this paper,a visualize warehouse management system based on RFID and WSN is designed.The model can manage entering storage,delivery of cargo and inventory pre-warning,and can push services actively.This model greatly improves the level of human-computer interaction and enhances the level and efficiency of warehouse management.	human–computer interaction;logistics;radio frequency;radio-frequency identification;real-time transcription;warehouse management system	Sai Shen	2010	2010 2nd International Workshop on Database Technology and Applications	10.1109/DBTA.2010.5658974	embedded system;supply chain management;wireless sensor network;computer science;information technology;computer security	DB	-30.560942649377324	18.859842727207266	75569
84f6ffdac412b93cee48309e601fb995322de445	uml activities at runtime - experiences of using interpreters and running generated code			interpreter (computing);unified modeling language	Dominik Gessenharter	2010		10.1007/978-3-642-16385-2_34		HCI	-30.070715625876232	27.50820436789804	75889
99b6ec9eabac65da106dc937f36cf8f5cd61b83e	a language for high-level programming of mathematical applications	mathematical programming libraries automatic programming mathematical model vocabulary arithmetic computer languages algorithms protocols boilers;high level languages;mathematics computing;automatic programming;system design;user interfaces automatic programming high level languages mathematics computing;synonymous syntactic structures minimal lexical restrictions language system automated programmer engineering mathematical application programming conventional textbook mathematical representation mathematical expressions conventional solution specifications flexible vocabulary technical english self documentation programming error maintainability verifiability user oriented two dimensional screen editor keyword synonyms;user interfaces	"""Classroom programme or live distance learning """" This course is a practical introduction to programming focused on financial applications…designed to build up libraries of code used in many applications…teaching is conducted by an expert who can pass on valuable advice…a """"must"""" for Quants to develop an understanding of the C++ language. """""""	c++;high- and low-level;high-level programming language;library (computing)	Fred Grossman;Robert J. Klerer;Melvin Klerer	1988		10.1109/ICCL.1988.13040	natural language processing;fourth-generation programming language;first-generation programming language;declarative programming;very high-level programming language;programming domain;reactive programming;computer science;third-generation programming language;functional logic programming;programming paradigm;event-driven programming;procedural programming;symbolic programming;inductive programming;fifth-generation programming language;programming language theory;programming language;user interface;second-generation programming language;high-level programming language;systems design	PL	-27.84651579855558	23.08416868577174	75918
4a2bf46612b2d123a11d82574c6b310568ca6522	resource usage analysis for the p-calculus	static checking;concurrent language;safety properties;concurrent programs;type inference;type system	We propose a type-based resource usage analysis for the π-calculus extended with resource creation/access primitives. The goal of the resource usage analysis is to statically check that a program accesses resources such as files and memory in a valid manner. Our type system is an extension of previous behavioral type systems for the πcalculus. It can guarantee the safety property that no invalid access is performed, as well as the property that necessary accesses (such as the close operation for a file) are eventually performed unless the program diverges. A sound type inference algorithm for the type system is also developed to free the programmer from the burden of writing complex type annotations. Based on our algorithm, we have implemented a prototype resource usage analyzer for the π-calculus. To the authors’ knowledge, this is the first type-based resource usage analysis that deals with an expressive concurrent language like the π-calculus.	algorithm;computer file;parallel computing;programmer;prototype;type inference;type system;usage analysis;π-calculus	Naoki Kobayashi;Kohei Suenaga;Lucian Wischik	2006	Logical Methods in Computer Science	10.2168/LMCS-2(3:4)2006	real-time computing;type system;computer science;type inference;database;programming language	PL	-20.755012404504182	30.590445748154824	75939
84810077d4ac0a2cd9f07e8920a570816639cd7b	advanced object-oriented features and programming in ada 95	object oriented		ada	Stéphane Barbey;Magnus Kempe;A. Strohmeir	1995		10.1145/216591.216607	programming language	PL	-25.571099909644307	22.6720884297722	75986
dcbff21da7608d6480d34a4a5a1b2ec8a32e33df	detection of version features in distributed systems	distributed system;context free grammar	In this paper we demonstrate how version features can be defined, detected, and used to determine language versions. Using the BNF notation, distinct features can be conveniently specified in the context-free grammar global to all of the versions. A feature matrix specifying the relationship maintained by terminal features is proposed to implement the global grammar, on which determining language versions is based.	beta normal form;context-free grammar;context-free language;distributed computing	K. C. Wong	1991	SIGPLAN Notices	10.1145/122598.122611	grammar systems theory;natural language processing;speech recognition;computer science;context-free grammar;programming language;attribute grammar;grammar-based code	NLP	-30.404123851354566	29.406623621298024	76195
6078fbe66d1d3e0a06985411063356c1ee85833a	vip: a visual interface for promela	unied modeling language;front end;interfaz grafica;graphical language;graphical interface;visual inteface to promela vip;state machine;langage java;specification language;real time object oriented modeling;operating system;object oriented;lenguaje especificacion;protocol meta language promela;visual interfaces;langage specification;interface graphique;hierarchical model;lenguaje grafico;langage graphique;java language	The Visual Interface to Promela (VIP) tool is a Java based graphical front end to the Promela specification language and the SPIN model checker [2]. VIP supports a visual formalism called v-Promela [3] which extends the Promela language with a graphical notation to describe structural and behavioral aspects of a system. v-Promela also introduces hierarchical modeling and object-oriented concepts. The formalism is largely consistent with the UML-RT proposal [5] which evolved from the Real-Time Object-Oriented Modeling (ROOM) language [4] and the Unified Modeling Language (UML) [1]. The structural part of a vPromela model consists of structural elements called capsules and describes their interconnection and hierarchical nesting using a variant of UML collaboration diagrams. The behavioral aspects of a v-Promela model are described by hierarchical communicating extended finite state machines and support such features as group transitions and optional return to history from group transitions. The VIP tool provides a graphical v-Promela editor supporting point and click editing of v-Promela structure diagrams and hierarchically nested state machines. The editor incorporates syntax checking to warn the user about incorrect use of v-Promela graphical syntax. Storage and retrieval of models is made possible using Java serialization. The tool also has a fully integrated vPromela compiler which generates Promela code. The resulting Promela models can be analyzed using existing SPIN technology. VIP requires the Java 1.2 Runtime Environment which is available for a variety of operating systems. VIP is not currently publicly available, but expected to be released in the near future.	compiler;diagram;finite-state machine;graphical user interface;interconnection;java version history;model checking;operating system;point and click;promela;real-time object-oriented modeling;real-time transcription;spin model checker;semantics (computer science);serialization;specification language;unified modeling language	Moataz Kamel;Stefan Leue	1999		10.1007/3-540-48234-2_12	natural language processing;specification language;computer science;theoretical computer science;front and back ends;operating system;graphical user interface;finite-state machine;programming language;object-oriented programming;hierarchical database model	SE	-30.595058685634783	28.8114008930669	76258
bd2e841becec51034f95e6852af0590d66a72bc7	debugging eli-generated compilers with noosa	formal specification;optimizacion compiladora;specification formelle;especificacion formal;analyse syntaxique;programa puesta a punto;analisis sintaxico;syntactic analysis;compiler optimization;analisis semantico;analyse semantique;optimisation compilateur;programme debogage;debugging program;semantic analysis	Source-level tools are not adequate for debugging generated compilers because they operate at the level of the generated implementation. It is inappropriate to expect compiler writers to be familiar with the implementation techniques used by the generation system. A higher-level approach presents debugging in terms of an abstract model of the implementation. For example, finite-state machines might be shown while debugging a scanner. This approach is inappropriate for developers who are not compiler experts and even for experts may present more information than is desirable. An even higher-level approach is used by the Noosa graphical debugger for the Eli compiler generation system. The compiler writer is required to understand a simple execution model that involves concepts that they already have to understand to write Eli specifications. Noosa allows high-level data examination in terms of the input to the compiler and the trees upon which attribution is performed. An event system allows fine-tuned control of program execution. The result is a debugging system that enables developers to diagnose bugs without having to have any knowledge of the underlying mechanisms used by their compiler.	compiler;debugging;eli	Anthony M. Sloane	1999		10.1007/978-3-540-49051-7_2	compiler;parallel computing;real-time computing;compiler correctness;computer science;compiler construction;parsing;formal specification;optimizing compiler;algorithmic program debugging;compilation error;programming language;functional compiler;algorithm	PL	-23.84729785517468	29.125878904695462	76355
73bbb0af3fbf615e6b15f2d62bc6aef2165c1918	visual patterns associated to abstract trees	compiler construction;model system;complex structure;graphical user interface;visual programming;trees;visual languages;visual representation;graphical representation;visual language;domain specific language;graphic user interface;dependent data;user interface design;user interaction;domain specificity;domain specific languages	Visual languages have an important role in modeling systems, specification of software, and in specific application domains. By using visual properties like spatial placement or line connections complex structures can be presented, so that humans can understand them quickly. Visual languages can be based on domain-specific metaphors, so that domain specialists can use their conventional way of description and abstraction. For working with a visual language, a specialized graphical frontend is needed. In contrast to textual languages, general purpose editors are insufficient for visual languages, because each visual language has its particular graphical requirements. The frontend should provide methods to aid efficient drawing and restructuring of visual expressions. Often, language-specific structure editors are used as frontends for visual languages. The visual program is stored in a language-dependent data structure. The user interacts with one or more visual representations. Edit operations are directly applied to the underlying structure and after a change the graphical representation is recomputed. The implementation of visual languages requires a wide range of conceptual and technical knowledge from issues of user interface design and graphical implementation to aspects of analysis and transformation for languages in general. We present a powerful toolset that incorporates such knowledge [16]. It generates editors from high level specifications: A language is specified by identifying certain patterns in the language structure, selecting a visual representation from a set of precoined solutions, and associating the pattern to constructs of the abstract grammar. A complete visual structure editor is generated from such a specification. It represents visual programs by attributed abstract trees. Therefore, further phases of processing visual programs can be generated by state-of-the-art tools for compiler construction. Even challenging visual languages can be implemented with reasonable small effort and with rather limited technical knowledge. The approach is suitable for a large variety of visual language styles.	abstract syntax tree;application domain;compiler;data structure;domain-specific language;eli;fits;front and back ends;graphical user interface;high-level programming language;language construct;layered system;requirement;structure editor;substitution (logic);tcl;user interface design;visual language	Uwe Kastens;Carsten Schmidt	2006	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2005.12.010	natural language processing;interactive visual analysis;computer science;domain-specific language;theoretical computer science;graphical user interface;visual programming language;human visual system model;programming language	Graphics	-32.41097179150963	23.781069529654616	76717
8259100eae4e956caa77e8c8b6d0db66b16da6f8	chaperones and impersonators: run-time support for reasonable interposition	contracts;interposition;intercession;proxies	Chaperones and impersonators provide run-time support for interposing on primitive operations such as function calls, array access and update, and structure field access and update. Unlike most interposition support, chaperones and impersonators are restricted so that they constrain the behavior of the interposing code to reasonable interposition, which in practice preserves the abstraction mechanisms and reasoning that programmers and compiler analyses rely on.  Chaperones and impersonators are particularly useful for implementing contracts, and our implementation in Racket allows us to improve both the expressiveness and the performance of Racket's contract system. Specifically, contracts on mutable data can be enforced without changing the API to that data; contracts on large data structures can be checked lazily on only the accessed parts of the structure; contracts on objects and classes can be implemented with lower overhead; and contract wrappers can preserve object equality where appropriate. With this extension, gradual typing systems, such as Typed Racket, that rely on contracts for interoperation with untyped code can now pass mutable values safely between typed and untyped modules.	aimall;access control;ada;ambienttalk;amiga walker;anurag kumar;application programming interface;aspect-oriented programming;aspect-oriented software development;bertrand (programming language);brian;compiler;computer vision;d programming language;daniel g. bobrow;data structure;denotational semantics;eiffel;euclid;functional programming;gary t. leavens;general-purpose markup language;gradual typing;ieee software;immutable object;interoperation;invariant (computer science);jean;java;javascript;keykos;lazy evaluation;lecture notes in computer science;logic programming;lookup table;mera 300;matthew flatt;mitchell corporation;overhead (computing);pc bruno;parametric polymorphism;programmer;proxy server;racket;ralf brown's interrupt list;record sealing;regular expression;robert bruce findler;run time (program lifecycle phase);smart contract;software engineering 2004;symposium on principles of programming languages;systems design;the art of the metaobject protocol;toby turner;tom gruber;type inference;vertex cover	T. Stephen Strickland;Sam Tobin-Hochstadt;Robert Bruce Findler;Matthew Flatt	2012		10.1145/2384616.2384685	computer science;database;programming language;computer security;algorithm	PL	-22.679439356852956	24.299558805080462	76872
35e3f20674d4408f303fbb4805bb8b52c5480bc9	web cc: a wbt system of programming for beginners	web based c compiler;programming environment;computer graphics;program compilers c language computer aided instruction computer graphics computer science education;computer aided instruction;computer graphic;wbt system;computer science education;c language;cg program;program compilers;web cc;programming profession computer languages educational programs java program processors graphical user interfaces electronic learning educational institutions upper bound natural languages;cg program wbt system web cc web based c compiler computer graphics	This paper introduces Web CC, a Web based C compiler. Web CC is simple but provides a ubiquitous programming environment for beginners. Web CC has several other features. Many beginners are interested in computer graphics (CG). But, CG programming is difficult for beginners because CG programs are implemented using pointers, structures, and such difficult techniques. Web CC provides several functions of CG as standard	compiler;computer graphics;integrated development environment;pointer (computer programming);server (computing);user (computing)	Minoru Uehara	2005	16th International Workshop on Database and Expert Systems Applications (DEXA'05)	10.1109/DEXA.2005.199	computer architecture;computer science;programming language;computer graphics	PL	-31.011834670773645	25.31874301013176	76909
20bee56992f7e7043d36092503fa4747cbe3ad54	visual conditional attributed rewriting systems in visual language specification	attribute grammars;visual sentence;algebraic specification;visual gestures;formal specification;textual context free grammars;concurrent computing;remuneration;conditional set rewrite systems;visual interaction;text analysis;vcarw;visual programming;algebraic specification rewriting systems attribute grammars visual languages visual programming formal specification;computational modeling;handicapped aids;picture layout grammars;visual languages;humans image generation character generation text analysis remuneration handicapped aids concurrent computing computational modeling documentation character recognition;image generation;rewriting systems;context free grammar;rewrite systems;character generation;visual language;visual gestures visual conditional attributed rewriting systems visual language specification visual interaction problems visual sentence vcarw conditional set rewrite systems textual context free grammars picture layout grammars constraint multiset grammars;humans;visual language specification;constraint multiset grammars;visual conditional attributed rewriting systems;visual interaction problems;character recognition;documentation	In order to manage visual interaction problems, we deepen a formalisation of visual languages, based on the notion of visual sentence, introduced in a previous paper. Visual Conditional attributed rewriting systems (vCARWs) are proposed for the specification of visual languages. We argue that vCARWs can be viewed as a generalisation of Conditional Set Rewrite Systems, which in turn were shown to be as expressive as textual context-free grammars, Picture Layout Grammars, and Constraint Multiset Grammars. Last, we outline a procedure to translate visual gestures into visual sentences according to vCARW rules.	context-free grammar;context-free language;rewriting;visual language	Paolo Bottoni;Maria Francesca Costabile;Stefano Levialdi;Piero Mussio	1996		10.1109/VL.1996.545282	natural language processing;tree-adjoining grammar;computer science;programming language;algorithm	AI	-26.82100883827015	19.12580256091473	76926
6bd81de7dbab812a8b309b9087b3aa93ef510422	on the way to intelligent query for functions in visual languages	containers user interfaces computer languages tree graphs database languages computer science prototypes automatic programming functional programming programming profession;query language;computer languages;programming environments;cone trees;user interface;database management systems;substitution;user interface intelligent query visual languages visual programming language software environment visavis cone trees cone graphs user interaction substitution;prototypes;software environment;intelligent query;automatic programming;functional programming;2 dimensional;query languages;visual programming language;cone graphs;tree graphs;visual languages;graphical user interfaces;programming profession;visual language;visavis;3 dimensional;computer science;user interaction;user interfaces;database languages;visual languages database management systems graphical user interfaces programming environments query languages;containers	The aim of developing a visual programming language is not achieved when the implementation of programs is possible, but still, as in the textual world, a complete software environment is needed. This paper describes an improvement of the function container of VisaVis [SI. The flat2-dimensional way of accessing functions is replaced by a hierarchical and 3-dimensional one. In the presentation, Cone Trees [7] are extended by a query language into Cone Graphs. The basic concept of user interaction, substitution, was taken from VisaVis. For the optimization of user interaction, a user interface prototype has been developed.	cone;mathematical optimization;prototype;query language;user interface;visual programming language	Bert Haberland;Jörg Poswig;Claudio Moraga	1993		10.1109/VL.1993.269563	computer science;theoretical computer science;database;programming language	Robotics	-30.587791302524895	23.931588301459758	77029
16bd92ba22b3b334f68fe53898441dbcfdba0c2c	on the complexity of finding low-level solutions			high- and low-level	Björn Grohmann	2014	IACR Cryptology ePrint Archive			Crypto	-28.61749299380547	19.259131903007646	77050
795361c17549587d36ac8249fd5baab406e6ca67	testing methods for sdl systems	verification;no determinismo;entrada salida;methode essai;protocole transmission;tool support;specifications;specification;automated test generation;nondeterminism;input output;protocolo transmision;non determinism;conformance testing;computer programming languages;non determinisme;test methods;finite automata;test generation;test purpose;automate fini;input output finite state machine;test method;verificacion;sdl;test sequence;finite state machine;entree sortie;langage programmation ordinateur;metodo ensayo;transmission protocol	The aim of this tutorial is to present an overview of conformance testing methods for SDL systems. These methods can be classified into two main groups: those whose intent is the totally automated test generation from the SDL system specification and those that provide interactive test generation methods. We present some of the more representative methods illustrating their application by a common example. This example, the Inres protocol and service, will facilitate the comparison of the methods with respect to SDL and test notations, restrictions on the language, results of the application to the protocol and the tools supporting the methods.		Ana R. Cavalli;Byoung-Moon Chin;Kilnam Chon	1996	Computer Networks and ISDN Systems	10.1016/0169-7552(95)00125-5	real-time computing;simulation;computer science;finite-state machine;test method;programming language;algorithm;statistics	Logic	-31.770484576706792	31.127536699960864	77522
17ee0ec93e342fd52c99aa97b926afa8b2125c6a	cool: a portable project for teaching compiler construction	compiler construction;programming language;software systems;software engineering;computer science education;language design	The compiler course is a fixture of undergraduate computer science education. Most CS programs offer a course on compilers that includes a substantial project where students write a compiler for a small programming language. The project often serves two distinct purposes: it teaches something about language design and compiler implementation, and it gives students the experience of building a substantial software system. A compiler project is the most complex software engineering task many students complete in an undergraduate program.	compiler;computer science;programming language;small;software engineering;software system;test fixture	Alexander Aiken	1996	SIGPLAN Notices	10.1145/381841.381847	compile time;compiler;dynamic compilation;compiler-compiler;compiler correctness;lc-3;computer science;software development;compiler construction;bootstrapping;programming language;software system	PL	-28.162559575838923	23.70003417315313	77583
3bc46cacff362d0861546ca34bfbb0b02d899fa0	enhancing performance and reliability of rule management platforms	parallelism;concurrency;rule driven application;expert system	RulE Management Platforms (REMPs) enable software engineers to represent programming logic as conditional sentences that relate statements of facts. A key benefit of REMPs is that they make software adaptable by burying the complexity of rule invocation in their engines, so that programmers can concentrate on business aspects of highly modular rules. Naturally, rule-driven applications are expected to have excellent performance, since REMP engines should be able to invoke highly modular rules in parallel in response to asserting different facts. In reality, it is very difficult to parallelize rule executions, since it leads to the loss of reliability and adaptability of rule-driven applications.  We created a novel solution that is based on obtaining a rule execution model that is used at different layers of REMPs to enhance the performance of rule-driven applications while maintaining their reliability and adaptability. First, using this model, possible races are detected statically among rules, and we evaluate an implementation of our abstraction of algorithms for automatically preventing races among rules. Next, we use the sensitivity analysis to find better schedules among simultaneously executing rules to improve the overall performance of the application. We implemented our solution for JBoss Drools and we evaluated it on three applications. The results suggest that our solution is effective, since we achieved over 225% speedup on average.	algorithm;drools;make;programmer;software engineer;speedup;wildfly	Mark Grechanik;B. M. Mainul Hossain	2015		10.1145/2668930.2688035	real-time computing;concurrency;computer science;theoretical computer science;distributed computing;expert system	SE	-29.03055690057805	30.104304049767528	77697
909362f0a030733c89e092d4d9664a6e8c2eab72	analysis of two flawed versions of a mutual exclusion protocol with maude and smga		We have analyzed two flawed versions of Qlock, a mutual exclusion (mutex) protocol with Maude and SMGA. Maud is a rewriting logic-based computer language equipped with model checking facilities and SMGA is a state machine graphical animation tool. We demonstrate that SMGA could make it better to comprehend counterexamples generated by Maude. Two properties called the mutex property and the lockout freedom property are taken into account for the two flawed versions of Qlock.	computer language;finite-state machine;graphical user interface;lockout chip;maude system;model checking;mutual exclusion;rewriting	May Thu Aung;Tam Thi Thanh Nguyen;Kazuhiro Ogata	2018		10.1145/3185089.3185110	algorithm;counterexample;animation;model checking;semaphore;finite-state machine;mutual exclusion;computer science;rewriting	Logic	-20.09741751811205	25.910377089685703	77747
ca8413953d3b09a93889942a51f2719a345dccbe	automatic hardware/software interface generation for embedded system	article	A large portion of the embedded system development process involves the integration of hardware and software. Unfortunately, communication across the hardware/software boundary is tedious and error-prone to create. This paper presents an automatic hardware/software interface generation system. As the front-end of hardware/software co-design frameworks, a system designer defines XML specifications for hardware functions. Our system generates hardware/software interfaces including Device Driver, Driver API, and Device Controller from these specifications. Embedded software designers can easily use hardware just like system libraries. Our system reduces the mistakes and errors that can be occurred when a software programmer directly connects software to hardware, and supports balancing labors between hardware developers and software programmers. Moreover, this system can be used as the back-end for a hardware/software co-design framework.	application programming interface;cognitive dimensions of notations;computer hardware;device driver;embedded software;embedded system;library (computing);programmer;systems design;xml	Choonho Son;Jeong-Han Yun;Hyun-Goo Kang;Taisook Han	2006	JIPS		hardware compatibility list;embedded system;computer architecture;embedded software;computer science;hardware architecture;computer engineering	Arch	-31.52736113767748	27.88646225040383	77822
b364fec36c611546be9d76bc49a51bb501c0b0a0	virtual reality - an approach to improve the generation of fault-free software for programmable logic controllers (plc)	virtual reality control engineering programmable control process planning mechanical engineering computer errors error correction assembly software tools machine tools;programmable controllers;virtual reality;boolean algebra;ladder diagram virtual reality fault free software programmable logic controllers low level languages instruction list;programmable logic controllers;software development;ladder diagram;low level languages;programmable logic controller;fault free software;free software;instruction list	Sofrware development for programmable logical controllers is usually based on low-level languages such as the instruction list or the ladder diagram. At the same time, the programmer look at a machine or an assembly system in a bit-oriented way; he translates the operational sequences into logical and/or time based combinations of binary signals described by the means of Boolean algebra. This classical method causes a lot of problems in reality so it should be improved. It is the aim of the report to show a way developing PLC-sofhuare graphically and interactively within a Virtual Reality (VR) based system (VPLC).	boolean algebra;diagram;high- and low-level;instruction list;interactivity;ladder logic;programmable logic device;programmer;virtual reality	Dieter Spath;Ulf Osmers	1996		10.1109/ICECCS.1996.558331	computer architecture;real-time computing;programmable logic array;computer science;control system;operating system;software engineering;programmable logic controller;virtual reality;function block diagram;simple programmable logic device;programming language;computer engineering	PL	-31.068153401310003	23.268458535996487	78032
e512637571c8f819b7549adc411f27e23e524ee5	the real-time refinement calculus: a foundation for machine-independent real-time programming	lenguaje programacion;sistema temporizado;programming language;real time;timed system;program verification;refinement method;verificacion programa;formal verification;calcul raffinement;temps reel;refinement calculus;systeme temporise;langage programmation;tiempo real;verification formelle;280302 software engineering;methode raffinement;verification programme;700199 computer software and services not elsewhere classified;metodo afinamiento;non real time	The real-time refinement calculus is an extension of the standard refinement calculus in which programs are developed from a precondition plus post-condition style of specification. In addition to adapting standard refinement rules to be valid in the real-time context, specific rules are required for the timing constructs such as delays and deadlines. Because many real-time programs may be nonterminating, a further extension is to allow nonterminating repetitions.A real-time specification constrains not only what values should be output, but when they should be output. Hence for a program to implement such a specification, it must guarantee to output values by the specified times. With standard programming languages such guarantees cannot be made without taking into account the timing characteristics of the implementation of the program on a particular machine. To avoid having to consider such details during the refinement process, we have extended our real-time programming language with a deadline command. The deadline command takes no time to execute and always guarantees to meet the specified time; if the deadline has already passed the deadline command is infeasible (miraculous in Dijkstra's terminology). When such a real-time program is compiled for a particular machine, one needs to ensure that all execution paths leading to a deadline are guaranteed to reach it by the specified time. We consider this checking as part of an extended compilation phase. The addition of the deadline command restores for the real-time language the advantage of machine independence enjoyed by non-real-time programming languages.	real-time transcription;refinement calculus	Ian J. Hayes	2002		10.1007/3-540-48068-4_3	refinement calculus;real-time computing;formal verification;computer science;artificial intelligence;operating system;database;mathematics;distributed computing;programming language;algorithm	Embedded	-24.026948206133337	31.949796781566036	78049
8ed0e84bd54879c3e7cf0725bf5176de026a47c6	lr(1) parser generation system: lr(1) error recovery, oracles, and generic tokens	error recovery;oracles;programming language;lr 1;generic tokens;parsing;parser construction;automata theory;pager;formal language	The LR(1) Parser Generation System generates full LR(1) parsers that are comparable in speed and size to those generated by LALR(1) parser generators, such as yacc [5]. In addition to the inherent advantages of full LR(1) parsing, it contains a number of novel features. This paper discusses three of them in detail: an LR(1) grammar specified automatic error recovery algorithm, oracles, and generic tokens.  The error recovery algorithm depends on the fact that full LR(1) parse tables preserve context. Oracles are pieces of code that are defined in a grammar and that are executed between the scanner and parser. They are used to resolve token ambiguities, including semantic ones. Generic tokens are used to replace syntactically identical tokens with a single token, which is, in effect, a variable representing a set of tokens.	algorithm;canonical lr parser;lalr parser;oracle machine;yacc	Arthur Sorkin;Peter Donovan	2011	ACM SIGSOFT Software Engineering Notes	10.1145/1943371.1943391	parser combinator;formal language;lalr parser;canonical lr parser;computer science;theoretical computer science;pager;parsing;glr parser;automata theory;programming language;recursive descent parser;ll parser;top-down parsing;algorithm;lr parser;simple lr parser	PL	-24.479439965652713	24.735309580816974	78160
c6f8b6077cbf1e67f92b614a77ff7a3303c8afbe	transactions for software model checking	software model checking;model checking	This paper presents a software model checking algorithm that combats state explosion by decomposing each thread’s execution into a sequence of transactions that execute atomically. Our algorithm infers transactions using the theory of reduction, and supports both left and right movers, thus yielding larger transactions and fewer context switches than previous methods. Our approach uses access predicates to support a wide variety of synchronization mechanisms. In addition, we automatically infer these predicates for programs that use lock-based synchronization.	algorithm;context switch;lock (computer science);model checking;network switch	Cormac Flanagan;Shaz Qadeer	2003	Electr. Notes Theor. Comput. Sci.	10.1016/S1571-0661(05)82560-5	model checking;real-time computing;computer science;database;distributed computing;programming language;abstraction model checking	PL	-20.276079798011715	30.88614394260196	78223
11877af2e6f2e4cf4684075fa242a3bc56538551	eptl - a temporal logic for weakly consistent systems		The high availability and scalability of weakly-consistent systems attracts system designers. Yet, writing correct application code for this type of systems is difficult; even how to specify the intended behavior of such systems is still an open question. There has not been established any standard method to specify the intended dynamic behavior of a weakly consistent system. There exist specifications of various consistency models for distributed and concurrent systems [13, 14]; and the semantics of replicated datatypes like CRDTs[14] have been specified in axiomatic and operational models based on visibility relations. In this paper, we present a temporal logic, EPTL, that is tailored to specify properties of weakly consistent systems. In contrast to LTL and CTL, EPTL takes into account that operations of weakly consistent systems are in many cases not serializable and have to be treated respectively to capture the behavior. We embed our temporal logic in Isabelle/HOL and can thereby leverage strong semi-automatic proving capabilities.	automated theorem proving;concurrency (computer science);consistency model;hol (proof assistant);high availability;isabelle;scalability;semiconductor industry;serializability;specification language;temporal logic;user error	Mathias Weber;Annette Bieniusa;Arnd Poetzsch-Heffter	2017		10.1007/978-3-319-60225-7_17	theoretical computer science;computer science;scalability;temporal logic;high availability;distributed computing	Logic	-19.9199179908616	29.07162923210845	78462
02072b4bf0731aa5bc004762352d643f48a4278b	banzai: a java framework for the implementation of high-performance servers	non blocking i o;server;protocol parsing and server architecture;parsing;formal grammar;high performance;protocol	This paper presents Banzai a Java framework that uses the Tatoo parser generator to simplify the implementation of high-performance servers based on plain-text protocols. This approach conciliates the use of formally defined grammars for protocol parsing and the efficiency of the implementation. We argue that the use of the formal grammars simplifies the implementation of the protocol and we show that an HTTP server built using the Banzai framework is as efficient as several existing ad-hoc high-performance HTTP servers. The Banzai framework relies on the ability of Tatoo to produce push non-blocking parsers with a fixed memory footprint during parsing and on a generic and efficient server architecture.	blocking (computing);compiler-compiler;formal grammar;hoc (programming language);hypertext transfer protocol;java;memory footprint;non-blocking algorithm;parsing;server (computing)	Julien Cervelle;Rémi Forax;Gautier Loyauté;Gilles Roussel	2009		10.1145/1529282.1529707	protocol;computer science;operating system;parsing;database;formal grammar;programming language;world wide web;server	PL	-28.062037379208117	30.105920634672703	78514
7f7dd8826a0407331b7f05f97c5cc5cc62668c6d	the billion-dollar fix - safe modular circular initialisation with placeholders and placeholder types	safe modular circular initialisation;placeholder type;initialising variable;modular circular initialisation;incorporating placeholders;object-oriented language;present placeholders;tony hoare;circular structure;billion dollar mistake;billion-dollar fix	Programmers often need to initialise circular structures of objects. Initialisation should be safe (so that programs can never suffer null pointer exceptions or otherwise observe uninitialised values) and modular (so that each part of the circular structure can be written and compiled separately). Unfortunately, existing languages do not support modular circular initialisation: programmers in practical languages resort to Tony Hoare’s “Billion Dollar Mistake”: initialising variables with nulls, and then hoping to fix them up afterward. While recent research languages have offered some solutions, none fully support safe modular circular initialisation. We present placeholders, a straightforward extension to object-oriented languages that describes circular structures simply, directly, and modularly. In typed languages, placeholders can be described by placeholder types that ensure placeholders are used safely. We define an operational semantics for placeholders, a type system for placeholder types, and prove soundness. Incorporating placeholders into object-oriented languages should make programs simultaneously simpler to write, and easier to write correctly.	assignment (computer science);compiler;hoare logic;operational semantics;pointer (computer programming);programmer;the six million dollar man;type system	Marco Servetto;Julian Mackay;Alex Potanin;James W Noble	2013		10.1007/978-3-642-39038-8_9	simulation;computer science;programming language;algorithm	PL	-24.249903631987692	27.879686504938288	78520
07b66b7707a3ff03eabf5cdf4b1dee4442b63a8e	c-clr: a tool for navigating highly configurable system software	system configuration;preprocessor directives;spectrum;modularization;aspect oriented programming;aspect mining;structured programming;tools;conditional compilation	In order to accommodate the spectrum of configuration options currently required for competitive system infrastructures, many systems leverage heavy usage of C preprocessor controlled conditional compilation. In herent costs associated with this heavy preprocessor usage include both the impaired readability of the base system, and the reduced reusability of the configuration code.  Our proposed solution, C-CLR, allows developers to sift through views of a system based on configuration options. Configuration-specific views improve readability of the system as a whole by including only relevant code. They also support reusability by aiding aspect mining through easy navigation to relevant configuration options, and automated identification of equivalent blocks of code within conditionally compiled segments.	c preprocessor;c++/cli;compiler;conditional compilation	Nieraj Singh;Celina Berg;Yvonne Coady	2007		10.1145/1233901.1233910	spectrum;aspect-oriented programming;computer science;modular programming;data mining;programming language;structured programming	SE	-28.17978696151716	28.14039428137783	78577
d9f94b75675f256ed3e3d5c510c9179e68905469	lap: a little language for os emulation		LAP, the Linux Application Platform, is a Linux emulation package for BSD/OS which uses a ‘‘little language’’ [Salu98] to describe transformations from Linux data types and values to BSD/OS data types and values, and vice versa. The little language simplifies and regularizes the specification of transformations, making the emulation easier to maintain. This paper describes the language and its place in the framework of LAP.		Donn Seeley	2000			embedded system;operating system;programming language	PL	-28.579407978977827	28.412001071421535	78610
03be275e1c0dc33673e9a328ff32dd674906fe58	compilation of concurrent declarative languages		"""The plethora of concurrent declarative language families, each with subtly diierent semantics , makes the design and implementation of static analyses for these languages a demanding task. However, many of the languages share underlying structure, and if this structure can be exploited, static analysis techniques can be shared across language families. These techniques can thus provide a common kernel for the implementation of quality compilers for this entire language class. The purpose of this paper is to exploit the similarities of non-strict functional and concurrent logic languages in the design of a common intermediate language (CIL). The CIL is introduced incrementally, giving at each step the rationale for its extension. As an application, we present, in CIL form, some state-of-the-art static partitioning algorithms from the literature. This allows us to \uncover"""" the relative advantages and disadvantages of the analyses, and determine promising directions for improving static partitioning."""	algorithm;common intermediate language;compiler;declarative programming;design rationale;static program analysis;strict function	Zena M. Ariola;Barton C. Massey;M. Sami;Evan Tick	1994				PL	-20.579124152710932	24.606989426418103	78781
8e1c487773fb2205e49f8d1b9bde92d12027a3e3	rc++: a rule-based language for game ai		"""""""Game AI"""" is the high-level control code for computer entertainment applications. Games are diverse, and the nature of game AI reflects this diversity. However, game AI, in contrast to rendering code and other game code, is distinguished by employing a high density of predicates on rapidly changing game state, and a high density of operations that change game state. Traditional programming languages, such as C and C++, are not designed to facilitate the creation of such code, unlike high-level AI rule-based languages. RC++, a full-featured, rule-based extension to C++, designed for Sony Computer Entertainment’s PlayStation2 hardware, is described. RC++ is based on OPS5 (Forgy, 81) and Poprulebase, a rule-based extension to the Pop-11 programming language. It combines optimised execution associated with OPS5 with the syntactical style and some of the useful features of Poprulebase. It is designed to facilitate the rapid creation of game AI by automating many of the common programming tasks associated with creating game rules, while maintaining the flexibility of C and C++ when required. The aim of the paper is to demonstrate the applicability of rule-based programming to game AI in general, and the usefulness of RC++ for game AI in particular. THE RULES OF THE GAME Currently, most game AI code consists of the implementation of behavioural rules that encapsulate knowledge about a particular game domain. For example, rules to control racing cars, footballers, enemies, and so forth. There are many different types of ‘rule’. The equations of motion that control a bouncing ball are ‘behavioural rules’ in a general sense. Such rules normally involve analytical or 1 Poprulebase is part of the Poplog programming system, which contains the languages Pop-11, Prolog, Lisp, ML, and the SIM_AGENT toolkit. It is freely available from http: // www.cs.bham.ac.uk/ research/ poplog/ freepoplog.html. numerical solutions to equations involving real numbers. In contrast, a rule to decide whether a footballer should pass or shoot involves conditions on world state, which may be represented not just by real numbers, but by symbolic representations of states-of-affairs, or combinations of such quantitative and qualitative information. These two types of behavioural rule require very different implementation strategies. Physical equations, in particular, require techniques from numerical mathematics, whereas ‘if-then’ rules require a different approach. This paper presents a language, called RC++, which is designed to facilitate the coding of ‘if-then’, as opposed to physics-based, rules. There are also many different types of game ‘agent’. A bouncing ball does not need to remember facts about its world, whereas a virtual footballer needs to remember where his team-mates are, what the current team strategy is, and so forth. Many games implement both kinds of agent: passive objects governed by approximations to physical laws and active agents constrained by those laws, but able to store information, make decisions, and perform actions that change the virtual environment. Examples are balls (passive, controlled by physical equations) and footballers (active, controlled by rules) in a football game, or cars and drivers in a racing game. The distinction between passive objects and active agents is reflected in the game industry’s distinction between physics modelling and game AI. Implementing behavioural rules for active agents is required by all games and is an everyday task for the game AI programmer. Rules are to AI as polygons are to graphics. The aim of RC++ is to provide methods and tools that allow easy scripting of high volumes of predetermined or logicbased events for game characters to perform, and ease the implementation difficulties of more sophisticated techniques required for correspondingly more intelligent behaviour. PROCEDURAL LANGUAGES C and C++ are fundamentally procedural languages. They are not ideally suited to writing rule-based AI code. Game AI code, in contrast to other game code, tends to be condition-rich, requiring many tests on state and conditional branching based on the results of those tests. A procedural implementation, such as a C implementation, codes condition checking on state as a succession of ordered ‘if’ or ‘switch’ statements. There is nothing intrinsically wrong with this approach. However, it does require the programmer to not only specify the condition and the actions to be executed if the condition holds, but also under what circumstances the test should be checked, where and how the predicated state should be stored, how the predicated state should be accessed, and when and how the state should be created and deleted. In addition, if the condition needs to be checked against multiple instances then the programmer must write looping code, including determining under what conditions the loop should terminate. Finally, the procedural programmer must incrementally abstract parts of conditions as separate function calls for reuse in other conditions: this takes time, and, if not performed, leads to a proliferation of 'same-but-slightly-different' conditional statements and code bloat. In summary, development times are increased by the need to perform a large number of peripheral programming tasks: it is never simply a matter of writing ‘rules’. Hence, scripting languages for game AI based on procedural languages, such as Java or subsets of C, are misguided: they are too similar to the main game language. The extra complexity introduced to the development process is not offset by any new language advantages. And any benefits obtained from the extra level of indirection between AI code and game engine code introduced by using a procedural scripting language can be also be obtained by simply implementing a good AI/game engine interface. In contrast, RC++ is specialised for writing game AI. It partially automates many of the peripheral tasks associated with creating game AI code. Programmers, and eventually designers, are able to concentrate on writing the rules themselves. RULE-BASED LANGUAGES RC++ is a rule-based extension to C++. Rule-based languages are based on a simple model of computation that is particularly suited to the direct encoding of large collections of highly conditional knowledge that constitutes intelligence within a particular domain, and hence are ideally suited to controlling intelligent agent behaviour. For example, they form the basis of all expert systems (e.g., the CLIPS language (Giarratano, 93), and JESS (Friedman-Hill, 00), a rule language written in JAVA), a technology widely used, and also agent toolkits (not yet widely used), such as SIM_AGENT (Sloman & Poli, 95; Baxter et al., 97). Rule-based languages are executed by a production system, which consists of a working memory (WM) that stores information or ‘assertions’, a ruleset that defines under what circumstances assertions in WM should change, and a scheduler that repeatedly applies rules to assertions in WM. Every rule has a condition part, a boolean expression that can be checked against assertions in WM, and an action part that alters assertions in WM. For example, a production system might have the following assertions in WM: [Person ^Name Fred ^is 10] [Person ^Name Mary ^is 11] [Person ^Name John ^is 17] // A ‘Person’ data type has two attributes: ‘^Name’ // and ‘^is’. The first attribute contains labels, // the second attribute contains numbers. and a ruleset consisting of a single rule:"""	aaron sloman;admissible numbering;approximation;artificial intelligence (video games);baxter (robot);boolean expression;branch (computer science);c++;clips;code bloat;expert system;fred (chatterbot);game engine;graphics;high- and low-level;hypertext transfer protocol;indirection;intelligent agent;java;jess;lisp;list of toolkits;logic programming;model of computation;numerical analysis;ops5;pop-11;peripheral;poplog;procedural programming;production system (computer science);programmer;programming language;prolog;rule 90;scheduling (computing);scripting language;succession;terminate (software);virtual reality	Ian Wright;James A. R. Marshall	2000			code bloat;rule-based system;data type;clips;programming language;procedural programming;expert system;scripting language;poplog;computer science	AI	-28.06528452036262	24.63832314703117	78861
8e79685417108c1300dd153542ce39f101f1b3b4	wireless sensor network for environmental monitoring: application in a coffee factory		Wireless sensor networks have been a big promise during the last few years, but a lack of real applications makes difficult the establishment of this technology. In this paper a real monitoring application in an instant coffee factory is presented. This application belongs to the group of environmental solutions based on wireless sensor networks, and it is focused on the impact of the instant coffee production processes in one of the largest instant coffee factories in Europe. The paper includes the entire application scenario, from the hardware of the WSN nodes to the software that will evaluate the impact and will close the loop.	closed-loop transfer function;http cookie;line level;requirement	Juan Valverde;Victor Rosello;Gabriel Mujica;Jorge Portilla;Amaia Uriarte;Teresa Riesgo	2012	IJDSN	10.1155/2012/638067	embedded system;real-time computing	Mobile	-31.829895703295588	20.12584980047081	78885
01c44e0d14d5f29de08e4f654a6d0e8e2e3ab0cc	down with emacs lisp: dynamic scope analysis	lenguaje programacion;representacion conocimientos;syntax;programming language;semantics;syntaxe;semantica;semantique;dynamic binding;codificacion;lisp language;target language;coding;langage programmation;lisp;knowledge representation;sintaxis;representation connaissances;flow analysis;codage	It is possible to translate code written in Emacs Lisp or another Lisp dialect which uses dynamic scoping to a more modern programming language with lexical scoping while largely preserving structure and readability of the code. The biggest obstacle to such an idiomatic translation from Emacs Lisp is the translation of dynamic binding into suitable instances of lexical binding: Many binding constructs in real programs in fact exhibit identical behavior under both dynamic and lexical binding. An idiomatic translation needs to detect as many of these binding constructs as possible and convert them into lexical binding constructs in the target language to achieve readability and efficiency of the target code. The basic prerequisite for such an idiomatic translation is thus a dynamic scope analysis which associates variable occurrences with binding constructs. We present such an analysis. It is an application of the Nielson/Nielson framework for flow analysis to a semantics for dynamic binding akin to Moreau's. Its implementation handles a substantial portion of Emacs Lisp, has been applied to realistic Emacs Lisp code, and is highly accurate and reasonably efficient in practice.	compiler;data-flow analysis;emacs lisp;late binding;programming language;scope (computer science)	Matthias Neubauer;Michael Sperber	2001		10.1145/507635.507642	natural language processing;computer science;fexpr;lisp;semantics;programming language;late binding;preprocessor;algorithm	PL	-23.939573691764963	25.060256582382177	78931
94107a434b2c9847ace029c9cb2239a07e8c658f	ruby on rails	dynamic typing;programming language;object oriented programming;web services java object oriented programming;object oriented;web services;entry barrier;rails testing productivity relational databases java spatial databases production application software programming profession auditory system;object oriented ruby programming language open source framework web 2 0 framework java architecture ruby on rail;programming languages;java;open source	Ruby on Rails is an open source framework developed to increase programmer productivity and reduce entry barriers to programming Web applications. Ruby on Rails is a novel Web 2.0 framework that attempts to combine PHP's simple immediacy with Java's architecture, purity, and quality. RoR is based on the dynamically typed, object-oriented Ruby programming language.	java;open-source software;php;programmer;programming language;programming productivity;pure function;ruby on rails;type system;web 2.0;web application	Michael Bächle;Jochen Ritscher	2006	IEEE Software	10.1109/MS.2007.176	fourth-generation programming language;first-generation programming language;higher-order programming;real-time computing;active record pattern;java concurrency;application programming interface;programming domain;reactive programming;computer science;strictfp;database;real time java;programming paradigm;programming language;object-oriented programming;java;generics in java;scala;java annotation	SE	-29.702716693782374	27.932827474998923	78967
6418d07c9cb1fa4b586831e378a5f2337bf46bcd	category theory for modeling oop		Category theory has been used extensively and successfully in modeling functional programming languages (see, e.g., [22, 23, 17, 20, 25, 15]). However, it has been used to a lesser extent in understanding and modeling object-oriented programming (OOP) languages, mainly focusing on OO languages extant during the early days of OOP research [13, 16, 18, 21]. Recently, we presented a detailed outline for using operads, from category theory, to model the iterative construction of the infinite subtyping relation in Java and other generic nominally-typed OO programming languages such as C#, C++ and Scala. Besides using operads to model the construction of the subtyping relation, we believe that there are plenty of other new uses of category-theoretic tools that can help in having better models and a better understanding of mainstream OOP languages. In this extended abstract we present outlines for four potential applications of category theory in OOP research. Namely, we first present (1) a summary of our use of operads to construct the Java subtyping relation, then we present (2) the possible use of representable functors (and Yoneda’s Lemma) in modeling and understanding generic types of generic nominally-typed OOP, followed by (3) the possible use of the equivalence of category presentations to relational database schema and of cartesian-closed categories as models of functional programming to model a structural view of OOP, and, finally, we present (4) the possible use of adjoint functors to model a particularly complex feature of Java generics, namely Java erasure. Operads and Generic OO Subtyping. Earlier this year, in [10, 11], we outlined how an operad, called JSO (for Java Subtyping Operad), can be defined to model the iterative construction of the generic subtyping relation in Java and other similar generic nominally-typed OO languages such as C# and Scala. Our model makes use of two facts: the fact that the generic subtyping relation in Java exhibits intricate self-similarity, due to the existence of wildcard types (and, accordingly, the existence of three subtyping rules for generic types),	c++;cartesian closed category;category theory;database schema;functional programming;generics in java;iterative method;mike lesser;programming language;relational database;scala;self-similarity;turing completeness	Moez A. AbdelGawad	2017	CoRR		discrete mathematics;cartesian closed category;category theory;lemma (mathematics);pure mathematics;functor;subtyping;mathematics;adjoint functors;object-oriented programming;java	PL	-24.881240993079356	25.560052811532742	79189
713f725a896be4805c46d69e75fa882e63036ca7	short paper: on high-assurance information-flow-secure programming languages	tierless;iot;sdn	We argue that high-assurance systems require high-assurance information-flow-secure programming languages. As a step towards such languages, we present the, to our knowledge, first concurrent theory of information flow security that supports (1) compositional reasoning under dynamic assumptions, and (2) value-dependent classification, to handle the dynamism inherent in modern high-assurance systems. We sketch out our vision and a roadmap for building self-certifying information-flow-secure programming languages.	programming language	Toby C. Murray	2015		10.1145/2786558.2786561	fourth-generation programming language;declarative programming;reactive programming;computer science;theoretical computer science;third-generation programming language;functional logic programming;computer programming;programming paradigm;ontology language;inductive programming;fifth-generation programming language;programming language theory;second-generation programming language;comparison of multi-paradigm programming languages;algorithm;control flow analysis;concurrent object-oriented programming	PL	-24.857905367397706	21.92142661728698	79212
594e966e1aff7be71e369dcb67a5e1db66f029a4	scj: memory-safety checking without annotations		The development of Safety-Critical Java (SCJ) has introduced a novel programming paradigm designed specifically to make Java applicable to safety-critical systems. Unlike in a Java program, memory management is an important concern under the control of the programmer in SCJ. It is, therefore, not possible to apply tools and techniques for Java programs to SCJ. We describe a new technique that uses an abstract language and inference rules to guarantee memory safety. Our approach does not require user-added annotations and automatically checks programs at the source-code level, although it can give false negatives.	java annotation;memory management;memory safety;programmer;programming paradigm	Chris Marriott;Ana Cavalcanti	2014		10.1007/978-3-319-06410-9_32	real-time computing;java concurrency;computer science;java modeling language;database;real time java;programming language;java annotation	PL	-21.249631600592462	31.67699559652167	79223
727e8faaf979d7519295c874249433d30f565b66	relational programming in libra		Libra is a general-purpose programming language based on the algebra of binary relations. It attempts to unify functional and logic programming, retaining the advantages of both, and avoiding some of the problems. It has all the features needed of a programming language, and a straightforward semantic interpretation. Since program speciications are easily expressed as relations, it ooers a simple path from a speciication to a program and from the program to its proof of correctness. The algebra of binary relations has several operators whose eeects are like those of familiar procedural language constructs, for example, relational composition is analogous to sequential execution. The Libra language is illustrated by its application to a simple programming exercise. Some conclusions are drawn.	apl;correctness (computer science);general-purpose programming language;logic programming;microsoft academic search;path (graph theory);procedural programming;semantic interpretation	Barry Dwyer	1997			inductive programming;programming language;sql;computer science	PL	-22.358589662736502	23.58688804527195	79470
12f20549964a0ee1d6863e363783d214bfc238e8	envision: the inside story	directed acyclic graph;high level languages;programming environments;optimization spreadsheet model directed graph programming environment envision test languages visual test programming language flow control static background dynamic background executable statements run time efficiency flexibility specification sheet timing equations directed acyclic graph;programming language;visual programming automatic testing computer graphics high level languages programming environments user interfaces;computer graphics;automatic testing;visual programming;information flow;flow control;user interfaces;timing force measurement software testing runtime flowcharts mice libraries books current measurement computer architecture	Two long standing concerns of device test engineers have been run-time efficiency and ease-of-modification. Traditionally, test languages optimizing one have sacrificed the other. This: paper examines the implementation of the envision test language from this perspective.	test engineer	Don Organ	1990		10.1109/TEST.1990.114064	fourth-generation programming language;embedded system;first-generation programming language;real-time computing;declarative programming;information flow;programming domain;reactive programming;computer science;theoretical computer science;extensible programming;operating system;third-generation programming language;software engineering;functional logic programming;flow control;computer programming;programming paradigm;event-driven programming;inductive programming;fifth-generation programming language;programming language theory;visual programming language;programming language;computer graphics;user interface;second-generation programming language;high-level programming language;directed acyclic graph;algorithm;control flow analysis	SE	-30.704469775727134	24.624492247717424	79574
9a79441a11bef5448aa7aae20bd5d175bab9a7bb	functional derivation of a virtual machine for delimited continuations	virtual machine;cps transformation;delimited continuation;program transformation;functional derivation;defunctionalization	This paper connects the definitional interpreter for the λ-calculus extended with delimited continuation constructs, shift and reset, with a compiler and a low-level virtual machine that copies a part of a data stack to implement delimited continuations. Following the functional derivation approach proposed and popularized by Danvy, we interrelate the two implementations via a series of meaning-preserving program transformations whose validity is independently known. As a result, this work formally establishes the correctness of a compiler and a low-level stack-copying implementation of delimited continuations. In particular, the resulting virtual machine properly models when to store return addresses into a data stack and which part of a data stack to copy. To our knowledge, this work is the first to prove correctness of such low-level features of delimited continuations. It also shows that the functional derivation approach is equally applicable to establish correctness of low-level implementations.	compiler;correctness (computer science);definition;delimited continuation;delimiter;high- and low-level;llvm;lambda calculus;program transformation;stack (abstract data type);virtual machine	Kenichi Asai;Arisa Kitani	2010		10.1145/1836089.1836101	delimited continuation;defunctionalization;computer science;virtual machine;theoretical computer science;programming language;algorithm	PL	-24.193521413678933	27.70976141202891	79580
97df15f0bbf1b7158fe7066827185bd47fcb8fde	push-button verification of file systems via crash refinement		The file system is an essential operating system component for persisting data on storage devices. Writing bug-free file systems is non-trivial, as they must correctly implement and maintain complex on-disk data structures even in the presence of system crashes and reorderings of disk operations. This paper presents Yggdrasil, a toolkit for writing file systems with push-button verification: Yggdrasil requires no manual annotations or proofs about the implementation code, and it produces a counterexample if there is a bug. Yggdrasil achieves this automation through a novel definition of file system correctness called crash refinement, which requires the set of possible disk states produced by an implementation (including states produced by crashes) to be a subset of those allowed by the specification. Crash refinement is amenable to fully automated satisfiability modulo theories (SMT) reasoning, and enables developers to implement file systems in a modular way for verification. With Yggdrasil, we have implemented and verified the Yxv6 journaling file system, the Ycp file copy utility, and the Ylog persistent log. Our experience shows that the ease of proof and counterexample-based debugging support make Yggdrasil practical for building reliable storage applications.	correctness (computer science);crash (computing);data structure;debugging;modulo operation;operating system;push-button;refinement (computing);satisfiability modulo theories;software bug;verification and validation	Helgi Sigurbjarnarson;James Bornholt;Emina Torlak;Xi Wang	2016			self-certifying file system;real-time computing;device file;computer file;computer science;class implementation file;versioning file system;operating system;unix file types;journaling file system;database;open;programming language	OS	-20.814505745343176	32.232894289783545	79983
4a771aa662e29c1c89e6cc911e46400555f732f0	an easy implementation of pil (prolog in lisp)	programming language;artificial intelligent	Much attention has been drawn to Prolog, the implementation of logic as a programming language. The decision of the Japanese to use Prolog as the core language of the Fifth Generation Computer [Warren, 1982] has given rise to some concern that Prolog may emerge as the chief rival to Lisp in artificial intelligence programming. I [Wallace, 1983] and others [Robinson and Sibert, 1982; Komorowski, 1982] have argued the benefits of extending Lisp to do what Prolog does. Here I present a simple procedure for implementing Prolog in Lisp (PiL), in order to demonstrate that it is easy to extend Lisp to do what Prolog does.	apl;artificial intelligence;fifth generation computer;lisp;programming language;prolog;wallace tree;warren abstract machine	Richard S. Wallace	1983	SIGART Newsletter	10.1145/1056635.1056638	exception handling;interpreter;symbol;computer science;fexpr;artificial intelligence;lisp;*lisp;symbolic programming;fifth-generation programming language;programming language;prolog;logic programming;preprocessor;s-expression;algorithm	PL	-23.43357726270817	23.561972897990877	80031
3c903ae770794205a0ad4f7d65b761267ff58991	java modular extension for operator overloading		The paper introduces a modular extension (plugin) for Java language compilers and Integrated Development Environments (IDE) which adds operator overloading feature to Java language while preserving backward compatibility. The extension use the idea of library-based language extensibility similar to SugarJ[1]. But unlike most language extensions, it works directly inside the compiler and does not have any external preprocessors. This gives much faster compilation, better language compatibility and support of native developer tools (IDE, build tools). The extension plugs into javac and Eclipse Java compilers as well as in all tools whose use the compilers such as IDEs (Netbeans, Eclipse, IntelliJ IDEA), build tools (ant, maven, gradle), etc. No compiler, IDE, build tools modification needed. Just add a jar library to classpath and/or install a plugin to your IDE. The paper also discuss on how to build such Java compiler extensions. The extension source code is open on http://amelentev.github.io/java-oo/	apache ant (another neat tool);apache maven;backward compatibility;eclipse;extensibility;gradle;integrated development environment;intellij idea;java compiler;netbeans ide;operator overloading;plug-in (computing);preprocessor;javac	Artem Melentyev	2014	CoRR	10.5121/ijpla.2014.4201	java api for xml-based rpc;parallel computing;real-time computing;jsr 94;java concurrency;computer science;java modeling language;strictfp;real time java;programming language;java;generics in java;scala;java applet;java annotation	PL	-28.66742954866765	28.298524585354663	80096
638761e38ae1b9f9b18118a7f9f338628d88474d	flexible language interoperability	datavetenskap datalogi	Virtual machines raise the abstraction level of the execution environment at the cost of restricting the set of supported languages. Moreover, the ability of a language implementation to integrate with other languages hosted on the same virtual machine typically constrains the features of the language. In this paper, we present a highly flexible yet efficient approach to hosting multiple programming languages on an objectoriented virtual machine. Our approach is based on extending the interface of each class with language-specific wrapper methods, offering each language a tailored view of a given class. This approach can be deployed both on a statically typed virtual machine, such as the JVM, and on a dynamic virtual machine, such as a Smalltalk virtual machine. We have implemented our approach to language interoperability on top of a prototype virtual machine for embedded systems based on the Smalltalk object model, which provides interoperability for embedded versions of the Smalltalk, Java, and BETA programming languages.	abstraction layer;beta;embedded system;java;language interoperability;programming language;prototype;smalltalk;type system;virtual machine	Torbjörn Ekman;Peter Mechlenborg;Ulrik Pagh Schultz	2007	Journal of Object Technology	10.5381/jot.2007.6.8.a2	self;language interoperability;interpreter;language primitive;computer science;programming language implementation;virtual machine;operating system;third-generation programming language;database;secd machine;compiled language;low-level programming language;fifth-generation programming language;programming language;second-generation programming language;virtual finite-state machine	PL	-27.057255192883357	28.949563207690158	80198
0e49097a9d9b487516dc4739f5b8221626e1009b	using parameterized signatures to express modular structure	theoretical framework;software systems;module system;polymorphism;type theory;separate compilation;theoretical foundation	Module systems are a powerful, practical tool for managing the complexity of large software systems. Previous attempts to formulate a type-theoretic foundation for modular programming have been based on existential, dependent, or manifest types. These approaches can be distinguished by their use of different quantifiers to package the operations that a module exports together with appropriate implementation types. In each case, the underlying type theory is simple and elegant, but significant and sometimes complex extensions are needed to account for features that are important in practical systems, such as separate compilation and propagation of type information between modules.This paper presents a simple type-theoretic framework for modular programming using parameterized signatures. The use of quantifiers is treated as a necessary, but independent concern. Using familiar concepts of polymorphism, the resulting module system is easy to understand and admits true separate compilation. It is also very powerful, supporting high-order, polymorphic, and first-class modules without further extension.	compiler;electronic signature;modular programming;quantifier (logic);software propagation;software system;type theory	Mark P. Jones	1996		10.1145/237721.237731	polymorphism;computer science;theoretical computer science;programming language;type theory;algorithm;software system	PL	-23.360626667228065	27.191696107012064	80286
4708aadadb0aba7933d04bed1fc5f6468eebc740	normal form approach to compiler design	lenguaje programacion;command language;compilateur;programming language;language theory;forma normal;teoria lenguaje;compiler;control protegido;commande gardee;langage programmation;normal form;guarded command;forme normale;high level language;theorie langage;compilador	This paper demonstrates how reduction to normal form can help in the design of a correct compiler for Dijkstra's guarded command language. The compilation strategy is to transform a source program, by a series of algebraic manipulations, into a normal form that describes the behaviour of a stored-program computer. Each transformation eliminates high-level language constructs in favour of lower-level constructs. The correctness of the compiler follows from the correctness of each of the algebraic transformations.	a-normal form;compiler;correctness (computer science);guarded command language;high- and low-level;high-level programming language;stored-program computer	C. A. R. Hoare;Jifeng He;Augusto Sampaio	1993	Acta Informatica	10.1007/BF01191809	compiler;compiler correctness;computer science;philosophy of language;compiler construction;compilation error;programming language;high-level programming language;functional compiler;algorithm	PL	-20.350276430774077	24.060869729477105	80388
fc58106b94269e5bed78080eec94e204437605c9	machine checked formal proof of a scheduling protocol for smartcard personalization	verification;pvs;simulation;theorem proving;theorem prover;scheduling algorithm;smartcard;model checking;cyclic scheduling;prototype verification system;state explosion;article in monograph or in proceedings;industrial design	Using PVS (Prototype Verification System), we prove that an industry designed scheduler for a smartcard personalization machine is safe and optimal. This scheduler has previously been the subject of research in model checked scheduling synthesis and verification. These verification and synthesis efforts had only been done for a limited number of personalization stations. We have created an executable model and have proven the scheduling algorithm to be optimal and safe for any number of personalization stations. This result shows that theorem provers can be successfully used for industrial problems in cases where model checkers suffer from state explosion.	algorithm;automated theorem proving;belt machine;bureaucracy;call of duty: black ops;care-of address;comment (computer programming);communications satellite;decision problem;dependent type;discharger;executable;expanded memory;expect;formal language;formal proof;graphical user interface;inductive reasoning;lambda lifting;lifting scheme;model checking;model-based testing;object-relational database;personalization;rejection sampling;relative intensity noise;scheduling (computing);smart card;software engineering;specification language;static timing analysis;subscriber identity module;surround sound;trac;type system;upload;xam	Leonard Lensink;Sjaak Smetsers;Marko C. J. D. van Eekelen	2007		10.1007/978-3-540-79707-4_10	industrial design;computer science;automated theorem proving;programming language	Logic	-21.439875843364973	20.570508797112527	80969
27345e7eff9fe2a9eb8d96ef12e6945cdbd0aedc	invokedynamic support in soot	java bytecode;java programming;invokedynamic;jsr292;java virtual machine;static analysis;dynamic analysis;intermediate representation	Java Specification Request (JSR) 292, which was realized with Java 7, defines a new java bytecode called invokedynamic, which can be used to call methods by name, without determining statically where the implementation of the called method is to be found. This mechanism eases the implementation of highly dynamic languages for the Java Virtual Machine.  In this work we explain how we extended the Soot framework for static analysis and transformation of Java programs to properly handle invokedynamic bytecodes. Our implementation required changes on all levels of Soot, as all intermediate representations needed to be adapted appropriately. We comment on the design decisions taken and how users can use our implementation to statically process or generate invokedynamic instructions.  Our support has been integrated into Soot release 2.5.0 and is thus already available for everyone to use.	assembly language;byte;documentation;intermediate representation;jasmin;java community process;java bytecode;java version history;java virtual machine;lam/mpi;norm (social);parsing;static program analysis;undocumented feature	Eric Bodden	2012		10.1145/2259051.2259059	java api for xml-based rpc;real-time computing;jsr 94;java concurrency;computer science;operating system;java modeling language;strictfp;embedded java;real time java;programming language;java;generics in java;scala;java applet;java annotation;non-blocking i/o	PL	-25.59578740158117	29.385041428271506	81132
6a98a6e70a1d3c21ad583ef6a83a83224d18c524	clean translation of an imperative reversible programming language	programming language;code generation;target language;source language	We describe the translation techniques used for the code generation in a compiler from the high-level reversible imperative programming language Janus to the low-level reversible assembly language PISA. Our translation is both semantics preserving (correct), in that target programs compute exactly the same functions as their source programs (cleanly, with no extraneous garbage output), and efficient, in that target programs conserve the complexities of source programs. In particular, target programs only require a constant amount of temporary garbage space. The given translation methods are generic, and should be applicable to any (imperative) reversible source language described with reversible flowcharts and reversible updates. To our knowledge, this is the first compiler between reversible languages where the source and target languages were independently developed; the first exhibiting both correctness and efficiency; and just the second compiler for reversible languages overall.	assembly language;code generation (compiler);common subexpression elimination;compiler;control flow;correctness (computer science);duplicate code;flowchart;garbage (computer science);high- and low-level;imperative programming;janus;machine code;programming language;register allocation;uncomputation	Holger Bock Axelsen	2011		10.1007/978-3-642-19861-8_9	first-generation programming language;compiler;computer science;programming language implementation;theoretical computer science;low-level programming language;programming language;high-level programming language;algorithm;code generation	PL	-22.148353797584118	28.90376063916467	81226
a4a90acd35c59f8e6d6b9362c128023fd756ab8f	and-or parallelism in full prolog with paged binding arrays	side effect;logic programs	Most models that have been proposed (or implemented) so far for exploiting both or-parallelism and independent and-parallelism have only considered pure logic programs (pure Prolog). We present an abstract model, called the Composition-Tree, for representing and-or parallelism in full Prolog. The Binding Array scheme is extended for And-Or parallel execution based on the Composition-tree. We also show how extra-logical predicates, such as cuts and side-effects, can be supported in this model.		Gopal Gupta;Vítor Santos Costa	1992		10.1007/3-540-55599-4_114	computer science;theoretical computer science;programming language;algorithm	PL	-21.552966274744122	23.59203813904774	81435
a697285cbf4be69e8f1735e5d1e1cfbfa31d16be	using dynamic pushdown networks to automate a modular information-flow analysis	concurrency;static analysis;information flow security	In this article, we propose a static information-flow analysis for multi-threaded programs with shared memory communication and synchronization via locks. In contrast to many prior analyses, our analysis does not only prevent information leaks due to synchronization, but can also benefit from synchronization for its precision. Our analysis is a novel combination of type systems and a reachability analysis based on dynamic pushdown networks. The security type system supports flowsensitive tracking of security levels for shared variables in the analysis of one thread by exploiting assumptions about variable accesses by other threads. The reachability analysis based on dynamic pushdown networks verifies that these assumptions are sound using the result of an automatic guarantee inference. The combined analysis is the first automatic static analysis that supports flow-sensitive tracking of security levels while being sound with respect to termination-sensitive noninterference.	data structure;data-flow analysis;lifting scheme;lock (computer science);non-interference (security);reachability;recursion;security type system;shared variables;shared memory;stack (abstract data type);static program analysis;synchronization (computer science);thread (computing)	Heiko Mantel;Markus Müller-Olm;Matthias Perner;Alexander Wenner	2015		10.1007/978-3-319-27436-2_12	real-time computing;concurrency;computer science;theoretical computer science;distributed computing;programming language;static analysis	PL	-20.428693697217398	30.916524328231464	81449
c9404ae68e35583fd0053ce247a557b7f5ebfc74	the procedures of the cade-13 atp system competition	competition;procedures;automated theorem proving	This article describes the practical procedures that were used to run the CADE-13 ATP System Competition. The article describes the hardware and software environments, the system installation, the soundness testing performed, the preparation of problems for the competition, the choice of the number of problems and the time limit, and the execution of the systems.	automated theorem proving	Geoff Sutcliffe;Christian B. Suttner	1997	Journal of Automated Reasoning	10.1023/A:1005858625038	procedure;simulation;competition;computer science;theoretical computer science;automated theorem proving;programming language;algorithm	PL	-21.01413626468429	19.440290485951603	81456
8aa386637b83b34006af77cfad68986d297f0003	compiling query constraints	earlier constraint query;actual constraint domain;full constraint;constraint selection;query constraint;constraint query;linear arithmetic constraint;database query evaluation system;equality constraint;parametrized constraint	We present a general technique to push query constraints (such as length≤1000) into database views and (constraint) logic programs. We introduce the notion of parametrized constraints, which help us push constraints with argument values that are known only at run time, and develop techniques for pushing parametrized constraints into predicate/view definitions. Our technique provides a way of compiling programs with constraint queries into programs with parametrized constraints compiled in, and which can be executed on systems, such as database query evaluation systems, that do not handle full constraint solving. Thereby our technique can push constraint selections that earlier constraint query rewriting techniques could not. Our technique is independent of the actual constraint domain, and we illustrate its use with equality constraints on structures (which are useful in object-oriented query languages) and linear arithmetic constraints.	assignment (computer science);compiler;constraint logic programming;constraint programming;constraint satisfaction problem;database;query language;rewriting;run time (program lifecycle phase);view (sql)	Peter J. Stuckey;S. Sudarshan	1994		10.1145/182591.182598	constraint logic programming;constraint programming;theoretical computer science;database;mathematics;constraint;programming language;algorithm	DB	-19.401641756542606	24.245224593486764	81477
b3d82bcd6c8d14b9f0e6c8b2b556aae1c9ea28b5	self-certification: bootstrapping certified typecheckers in f* with coq	refinement types;certification;general techniques;mobile code;dependent types	Well-established dependently-typed languages like Agda and Coq provide reliable ways to build and check formal proofs. Several other dependently-typed languages such as Aura, ATS, Cayenne, Epigram, F*, F7, Fine, Guru, PCML5, and Ur also explore reliable ways to develop and verify programs. All these languages shine in their own regard, but their implementations do not themselves enjoy the degree of safety provided by machine-checked verification. We propose a general technique called self-certification that allows a typechecker for a suitably expressive language to be certified for correctness. We have implemented this technique for F*, a dependently typed language on the .NET platform. Self-certification involves implementing a typechecker for F* in F*, while using all the conveniences F* provides for the compiler-writer (e.g., partiality, effects, implicit conversions, proof automation, libraries). This typechecker is given a specification (in~F*) strong enough to ensure that it computes valid typing derivations. We obtain a typing derivation for the core typechecker by running it on itself, and we export it to Coq as a type-derivation certificate. By typechecking this derivation (in Coq) and applying the F* metatheory (also mechanized in Coq), we conclude that our type checker is correct. Once certified in this manner, the F* typechecker is emancipated from Coq.  Self-certification leads to an efficient certification scheme---we no longer depend on verifying certificates in Coq---as well as a more broadly applicable one. For instance, the self-certified F* checker is suitable for use in adversarial settings where Coq is not intended for use, such as run-time certification of mobile code.	.net framework;ats;agda;authorization;bootstrapping (compilers);cayenne (programming language);code mobility;compiler;coq (software);correctness (computer science);dependent type;epigram;library (computing);programming language;simply typed lambda calculus;source lines of code;type system;verification and validation	Pierre-Yves Strub;Nikhil Swamy;Cédric Fournet;Juan Chen	2012		10.1145/2103656.2103723	dependent type;computer science;theoretical computer science;programming language;certification;algorithm	PL	-21.376589410005543	27.48908350572739	81494
8646ad39d63251feecd16def40a5206127b7d745	winhipe: an ide for functional programming based on rewriting and visualization	modelizacion;animacion por computador;lenguaje programacion;evaluation function;programming environments;red www;program visualization;visualizacion;programming language;maintenance;programming environment;generacion automatica;reseau web;customization;personnalisation;animation system;ejecucion programa;tracing;functional programming;expression evaluation;automatic generation;program execution;modelisation;medio ambiente programacion;large scale;visualization;generation automatique;internet;side effect;visualisation;rewriting systems;execution programme;reecriture;tracage;personalizacion;mantenimiento;langage programmation;world wide web;programmation fonctionnelle;computer animation;rewriting;evaluation expression;term rewriting;evaluacion expresion;program animation;modeling;programacion funcional;visual system;systeme reecriture;environnement programmation;reescritura;trazado;animation par ordinateur	The article describes an IDE for functional programming, called WinHIPE. It provides an interactive and flexible tracer, as well as a powerful visualization and animation system. The former tool is based on the rewriting model of evaluation, and the latter provides automatic generation of visualizations and animations, friendly support for customization, maintenance and exportation of animations to the Web, and facilities to cope with large scale. Its main advantage over other visualization systems is an effortless approach to animation creation and maintenance, based on generating visualizations and animations automatically, as a side effect of program execution. Finally, we briefly describe our experience using the system during several years in educational settings.	functional programming;image tracing;integrated development environment;rewriting;world wide web	Cristóbal Pareja-Flores;Jaime Urquiza-Fuentes;J. Ángel Velázquez-Iturbide	2007	SIGPLAN Notices	10.1145/1273039.1273042	visualization;computer science;programming language;functional programming;algorithm;computer graphics (images)	PL	-27.805796236043395	25.694147628010477	81576
2e364e4340a8f0635d69453b2e6ddcba8d324307	rbfeatures: feature-oriented programming with ruby	dynamic program;product line;feature oriented programming;domain specific language;software product line;dynamic programming languages;domain specific languages	Features are pieces of core functionality of a program that is relevant to particular stakeholders. Features pose dependencies and constraints among each other. These dependencies and constraints describe the possible number of variants of the program: A valid feature configuration generates a specific variant with unique behavior. Feature-Oriented Programming is used to implement features as program units. This paper introduces rbFeatures, a feature-oriented programming language implemented on top of the dynamic programming language Ruby. With rbFeatures, programmers use software product lines, variants, and features as first-class entities. This allows several runtime reflection and modification capabilities, including the extension of the product line with new features and the provision of multiple variants. The paper gives a broad overview to the implementation and application of rbFeatures. We explain how features as first-class entities are designed and implemented, and discuss how the semantics of features are carefully added to Ruby programs. We show two case studies: The expression product line, a common example in feature-oriented programming, and a web application.	feature-oriented programming;ruby	Sebastian Günther;Sagar Sunkle	2012	Sci. Comput. Program.	10.1016/j.scico.2010.12.007	fourth-generation programming language;first-generation programming language;real-time computing;declarative programming;programming domain;reactive programming;computer science;domain-specific language;theoretical computer science;extensible programming;third-generation programming language;functional logic programming;computer programming;programming paradigm;procedural programming;symbolic programming;inductive programming;fifth-generation programming language;programming language theory;visual programming language;programming language;second-generation programming language;high-level programming language;semantics	PL	-27.559290884986655	28.77511408744503	81656
6041068ed21b7539f7f3cf948a6e553f3c489715	efficient type inference using monads (summary)	efficient type inference;type inference	Efficient type inference algorithms are based on graph-rewriting techniques. Consequently, at first sight they seem unsuitable for functional language implementation. In fact, most compilers written in functional languages use substitution-based algorithms, at a considerable cost in performance. In this paper, we show how monads may be used to transform a substitution-based inference algorithm into one using a graph representation. The resulting algorithm is faster than the corresponding substitution-based algorithm.	type inference	Kevin Hammond	1991			graph (abstract data type);type inference;compiler;machine learning;functional programming;monad (functional programming);inference;artificial intelligence;mathematics	NLP	-19.893604660122502	22.61777370694809	81668
4801707f5497627c11a58407a1472e4e9b3b2cfc	the ekeko/x program transformation tool	libraries;code templates;refactoring;refactoring program transformation code templates;metaprogramming foundation ekeko x program transformation tool source code code templates syntactic characteristics data flow characteristics control flow characteristics structural characteristics rewriting directives;program transformation;receivers;syntactics;abstracts;source code software program compilers program debugging;syntactics receivers libraries reactive power concrete java abstracts;concrete;java;reactive power	Developers often need to perform repetitive changes to source code. For instance, to repair several instances of a bug or to update all clients of a library to a newer version. Manually performing such changes is laborious and error-prone. Program transformation tools enable automating changes, but specifying changes as a program transformation requires significant expertise. Code templates are often touted as a remedy, yet have never been endorsed wholeheartedly. Their use is mostly limited to expressing the syntactic characteristics of the intended change subjects. Less familiar means have to be resorted to for expressing their structural, control flow, and data flow characteristics. In this tool paper, we introduce a decidedly template-driven program transformation tool called Ekeko/X. Its specifications feature templates for specifying all of the aforementioned characteristics of its subjects. To this end, developers can associate different directives with individual components of a template. Each matching directive imposes particular constraints on the matches for the component it is associated with. Rewriting directives, on the other hand, determine how each match should be changed. We develop Ekeko/X from the ground up, starting from its applicative logic meta-programming foundation. We highlight the key choices in this implementation and demonstrate its use through two example program transformations.	applicative programming language;cognitive dimensions of notations;control flow;dataflow;directive (programming);metaprogramming;program transformation;rewriting;software bug	Coen De Roover;Katsuro Inoue	2014	2014 IEEE 14th International Working Conference on Source Code Analysis and Manipulation	10.1109/SCAM.2014.32	concrete;computer science;theoretical computer science;software engineering;ac power;programming language;java;code refactoring;algorithm	SE	-26.623365849033316	28.441651778779843	81789
9cdf3879fcb317aa85e4c739cfd4c4459993a5e7	miniaudicle and chuck shell: new interfaces for chuck development and performance		ChucK, a powerful audio synthesis programming language, currently supporting only a simple command line interface. Accompanying the ongoing development of the ChucK language is the production of two new interfaces for ChucK, the ChucK shell and the miniAudicle. The ChucK shell provides a lightweight method of access to ChucK in a shell-like console environment. The miniAudicle offers a powerful integrated solution to the ChucK development process and a framework for further enhancements to the ChucK programming environment. The miniAudicle also provides a set of generic user interface elements with which programmers can quickly construct graphical interfaces in ChucK programs. The miniAudicle streamlines and simplifies ChucK development, allowing ChucK programmers to focus on design and artistic issues while also exposing ChucK to computer musicians who are unfamiliar with or averse to the command line.	chuck;command-line interface;comparison of command shells;event (computing);graphical user interface;integrated development environment;linux;microsoft windows;operating system;programmer;programming language;software documentation;speech synthesis;system 7	Spencer Salazar;Ge Wang;Perry R. Cook	2006			command-line interface;computer hardware;computer science;user interface	HCI	-31.347054496415726	27.77961115890523	81884
5722932e2fbb4bf789056ac10aab0a8a200d2a1b	expressibility in type theory	type theory		type theory	H. Julian Wadleigh	1970	Notre Dame Journal of Formal Logic	10.1305/ndjfl/1093894000	mathematics;programming language;type theory	Logic	-22.020347298270956	20.58838275083754	81994
b085ec63f75111b2c558245ae18b33593639a1b4	liveness properties in cafeobj - a case study for meta-level specifications			liveness	Norbert Preining;Kazuhiro Ogata;Kokichi Futatsugi	2014		10.1007/978-3-319-17822-6_11		SE	-22.378020893070666	20.970067095532745	82005
080a5832eafd95a19d14f5f9723bd143f45198d0	the green language	exception handling system;object oriented language;type systems;object oriented;polymorphism;exception handling;object oriented languages;type system	Green is a statically-typed object-oriented language which supports parameterized classes, metaobjects, introspective reflection, and classes as first-class objects. Its exception system is completely object-oriented for it encapsulates in classes not only exceptions but also exception handling. The language definition of subtyping is more encompassing than subclassing, thus improving polymorphism. Classes are classless objects which have themselves types. This makes classes first-class objects without the problems associated to languages in which every class is an object of another class, its metaclass. Every basic value such as 7 or ’A’ is considered as an object whenever necessary which makes programming easy and increases polymorphism.	exception handling;metaclass;metaobject;type system	José de Oliveira Guimarães	2006	Computer Languages, Systems & Structures	10.1016/j.cl.2005.07.001	method;object-based language;computer science;object;distributed computing;exception handling syntax;programming language;object-oriented programming;algorithm	PL	-25.483256514733352	27.292825204873253	82178
6e73d8987c506f8f805422bed14a85abd104f07d	mesh: explicit and flexible generation of analog arrays		Analog layout design is a costly and error-prone task since analog synthesis is still far from applicable. It is expected that procedural bottom-up generators managing constructive tasks will be part of future synthesis flows. Generators contain expert knowledge implicitly within complex and hard-to-understand source code. Due to a lack of explicit layout definition and uncaptured design intent, generator layouts can hardly be adapted by constructive algorithms directly. Thus, synthesis flows need to adapt layout blocks by varying generator parameters which results in computation-expensive optimization. This paper introduces MESH — a software structure to define detailed and flexible layout generators explicitly. Using MESH, just a few lines of code describe complex layouts while all relations and design intents, such as element positions and routing styles, are captured through abstract commands. As a result, generators are created fast with less programming errors, and constructive algorithms can modify the generator structure directly.	algorithm;analog signal;bottom-up parsing;cognitive dimensions of notations;computation;mathematical optimization;routing;source lines of code	Benjamin Prautsch;Uwe Eichler;Torsten Reich;Jens Lienig	2017	2017 14th International Conference on Synthesis, Modeling, Analysis and Simulation Methods and Applications to Circuit Design (SMACD)	10.1109/SMACD.2017.7981572	constructive;software;engineering drawing;reuse;source code;page layout;as element;source lines of code;computer science	EDA	-32.734446014033296	23.399380900247788	82327
a3089cb1777b65d5f87316184ef1155126b72e00	optimizing compilation of constraint handling rules in hal	programming language;optimizing compiler;optimizing compilation;theory and practice of logic programming;functional dependency;constraint handling rules;language extension	In this paper we discuss the optimizing compilation of Constraint Handling Rules (CHRs). CHRs are a multi-headed committed choice constraint language, commonly applied for writing incremental constraint solvers. CHRs are usually implemented as a language extension that compiles to the underlying language. In this paper we show how we can use different kinds of information in the compilation of CHRs in order to obtain access efficiency, and a better translation of the CHR rules into the underlying language, which in this case is HAL. The kinds of information used include the types, modes, determinism, functional dependencies and symmetries of the CHR constraints. We also show how to analyze CHR programs to determine this information about functional dependencies, symmetries and other kinds of information supporting optimizations.	constraint handling rules;functional dependency;hal;indeterminacy in concurrent computation;optimizing compiler	Christian Holzbaur;Maria Garcia de la Banda;Peter J. Stuckey;Gregory J. Duck	2005	TPLP	10.1017/S1471068405002413	dynamic compilation;computer science;theoretical computer science;optimizing compiler;functional dependency;programming language;algorithm	DB	-20.7906135075809	23.64090779046169	82353
e9b7ad39c81bad69223a5654d0c1187e0d544605	isomorphic types for open coordination systems	coordinate system	Open coordination systems traditionally force programmers to use a very limited set of types when communicating. As the system is open nothing is known at creation time about which processes will connect, how many of them there will be and what types they will use. This paper describes the first steps in an architecture to allow open systems, such as Linda, to coordinate heterogeneous processes using general datatypes. By describing types in terms of products, sums and functions and providing functions that translate between this abstract representation and the concrete representation in the program, types can be transferred between programs which have no prior knowledge of each other’s type systems. Combining this with a type isomorphism algorithm allows types that are not identical but are equivalent to be used in communications between processes.	algorithm;c++;function type;graph isomorphism;library (computing);linda (coordination language);programmer;python;server (computing);stochastic process;type system	Andrew Wilkinson;Alan Wood	2005			computer science;coordinate system;distributed computing;algorithm	SE	-28.158211235819568	32.15536038689518	82677
0887a10e9a7a2b426135c8a89475871f3504fd01	jnuke: efficient dynamic analysis for java	verificacion modelo;java programming;programa control;verification modele;langage java;program verification;verificacion programa;model checking;checking program;programme controle;lenguaje java;verification programme;dynamic analysis;java language	JNuke is a framework for verification and model checking of Java programs. It is a novel combination of run-time verification, explicit-state model checking, and counter-example exploration. Efficiency is crucial in dynamic verification. Therefore JNuke has been written from scratch in C, improving performance and memory usage by an order of magnitude compared to competing approaches and tools.	formal verification;java;model checking;software verification	Cyrille Artho;Viktor Schuppan;Armin Biere;Pascal Eugster;Marcel Baur;Boris Zweimüller	2004		10.1007/978-3-540-27813-9_37	model checking;real-time computing;jsr 94;computer science;operating system;java modeling language;strictfp;real time java;dynamic program analysis;programming language;java;scala	SE	-22.232570444143274	31.248956031378604	82708
babf2bea2581e5ab3165acf9ca85169708345466	cspcons: a communicating sequential prolog with constraints	finite domain constraints;lenguaje programacion;constraint logic programs;protocole transmission;programming language;prolog;communicating process;ejecucion programa;metodo secuencial;logical programming;sequential method;constraint satisfaction;program execution;proceso comunicante;satisfaction contrainte;protocolo transmision;programmation logique;execution programme;processus communicant;langage programmation;methode sequentielle;constraint solving;satisfaccion restriccion;programacion logica;transmission protocol	Cspcons is a programming language that supports program execution over multiple Prolog processes with constraints. The language is an extended version of Csp-ii, a version of Prolog that supports, among other features, channel-based communicating processes and TCP/IP communication and is based on the CSP model introduced by Hoare. Cspcons inherits all the advanced features of Csp-ii and extends it by introducing constraint solving capabilities to the processes. In Cspcons each Prolog process has one or more solvers attached and each solver is independent from the others, following the original Csp-ii model, thus resulting to a communicating sequential constraint logic programming system. Such a model can facilitate greatly the implementation of distributed CLP applications. Currently Cspcons offers a finite domain constraint solver, but the addition of new solvers is supported as they can be integrated in the system in the form of linkable C libraries. This paper briefly describes the original Csp-ii system along with the extensions that resulted to the Cspcons system.	apl;admissible numbering;constraint logic programming;constraint satisfaction problem;hoare logic;internet protocol suite;library (computing);programming language;prolog;solver	Ioannis P. Vlahavas;Ilias Sakellariou;Ivan Futó;Zoltán Pásztor;János Szeredi	2002		10.1007/3-540-46014-4_8	constraint logic programming;constraint programming;constraint satisfaction;computer science;artificial intelligence;theoretical computer science;database;distributed computing;programming language;prolog;algorithm	PL	-24.3785816896298	30.817250882477776	82812
e66d07714824cea1360d93cfb174683db61e77d7	class hierarchy specialization	lenguaje programacion;compilateur;langage c;programming language;optimization method;compiler;metodo optimizacion;algorithme;algorithm;c language;methode optimisation;langage programmation;compilador;lenguaje c;algoritmo	Many class libraries are designed with an emphasis on generality and extensibility. Applications often exercise only part of a library's functionality. As a result, the objects created by an application may contain unused (user-specified or compiler-generated) members. Redundant members in objects are undesirable because they increase an application's memory usage. We present an algorithm for specializing a class hierarchy with respect to its usage in a program $\cal P$ . That is, the algorithm analyzes the member access patterns for $\cal P$ 's variables, and creates distinct classes for variables that access different members. The algorithm addresses the inheritance mechanisms of C++ in their full generality, including multiple inheritance and virtual (shared) inheritance. Class hierarchy specialization reduces object size, and can be viewed as a space optimization. However, execution time may also be reduced through reduced object creation or destruction time, and caching and paging effects. Class hierarchy specialization may also create new opportunities for existing optimizations such as call devirtualization and inlining. In addition, specialization may be useful in tools for software maintenance and program understanding.	access time;algorithm;benchmark (computing);beta normal form;c++;cache (computing);class hierarchy;compiler;constructor (object-oriented programming);declaration (computer programming);direct method in the calculus of variations;downcasting;download;dynamic loading;exception handling;extensibility;field (computer science);first-class function;information needs;inline expansion;interprocedural optimization;java;level of detail;library (computing);literal (computer programming);lookup table;mac os x 10.3 panther;mac os x 10.4 tiger;mathematical optimization;memory management;method (computer programming);multiple inheritance;name mangling;object code;object lifetime;operator overloading;paging;parametric polymorphism;partial template specialization;pointer (computer programming);program analysis;program comprehension;reduced cost;redundancy (engineering);requirement;rewriting;run time (program lifecycle phase);semantics (computer science);software maintenance;text simplification;type conversion;typeof;virtual inheritance	Frank Tip;Peter F. Sweeney	2000	Acta Informatica	10.1007/PL00013298	compiler;computer science;theoretical computer science;mathematics;programming language;algorithm	PL	-24.08761248658606	28.64890970848054	82852
52380052facc0c3cd22ebcf9f85031b43886ba87	reusable c++ code for an electron-phonon simulation	computational physics;reusability;standard template library;c;standard template library stl	This poster describes the code developed for simulations of disordered electron-phonon systems. This code uses the C++ Standard Template Library (STL) and is designed to be easily changed when the physicists want to change aspects of the simulation.	c++;electron;phonon;simulation;standard template library	Sandria N. Kerr;Guanghui Lei;William C. Kerr	2002		10.1145/985072.985110	reusability;standard template library;computer science;programming language	Embedded	-33.63181276578432	26.062691264919053	82879
4d73adafe1c34c7b09fd69659555c2be08ed5440	mechanical translation of i/o automaton specifications into first-order logic	teoria demonstracion;distributed system;entrada salida;systeme reparti;theorie preuve;proof theory;data management;safety properties;theorem proving;input output;demonstration theoreme;theorem prover;formal verification;sistema repartido;levels of abstraction;logique ordre 1;algorithme reparti;interactive proofs;verification formelle;algoritmo repartido;demostracion teorema;distributed algorithm;first order logic;entree sortie;logica orden 1	We describe a tool that improves the process of verifying relations between descriptions of a distributed algorithm at different levels of abstraction using interactive proof assistants. The tool automatically translates algorithms, written in the IOA language, into first-order logic, expressed in the Larch Shared Language, in a style that facilitates reasoning with a theorem prover. The translation uses a unified strategy to handle the various forms of nondeterminism that appear in abstract system descriptions. Applications of the tool to verify safety properties of three data management algorithms, including a substantial example based on Lamport’s logical time algorithm, suggest that the tool can be used to validate complicated, practical designs.	automated theorem proving;distributed algorithm;first-order logic;first-order predicate;input/output automaton;larch family;machine translation;nondeterministic algorithm;principle of abstraction;proof assistant;verification and validation	Andrej Bogdanov;Stephen J. Garland;Nancy A. Lynch	2002		10.1007/3-540-36135-9_24	distributed algorithm;discrete mathematics;data management;computer science;artificial intelligence;database;mathematics;automated theorem proving;programming language;algorithm	Logic	-19.9449526150963	24.07959876642723	83037
0ab616e86be501e2c4f0110ed864dfcced68d4a5	mechanisms for compile-time enforcement of security	combinators;programming language design;loops;type checking;secure system;language design	This paper discusses features of a secure systems programming language designed and implemented at IBM's Watson Research Lab. Two features of the language design were instrumental in permitting security to be enforced with minimum run-time cost: (1) Language constructs (e.g. pointer variables) which could result in aliasing were removed from the programmer's direct control and replaced by higher level primitive types; and (2) traditional strong type checking was enhanced with typestate checking, a new mechanism in which the compiler guarantees that for all execution paths, the sequence of operations on each variable obeys a finite state grammar associated with that variable's type. Examples are given to illustrate the application of these mechanisms.	aliasing;compiler;finite-state machine;pointer (computer programming);programmer;strong and weak typing;system programming language;thomas j. watson research center;type system	Robert E. Strom	1983		10.1145/567067.567093	language primitive;combinatory logic;computer science;theoretical computer science;programming language;algorithm	PL	-22.384021771350998	28.745685990491012	83201
1e55ec49521dd30c5e0b138ae49c0f3c22e86b55	simulation and formal verification of x86 machine-code programs that make system calls	theorem proving instruction sets machine code listings operating systems computers program compilers program verification;gcc compiler formal verification system calls program verification instruction set architecture isa acl2 theorem proving system formal analysis x86 machine code programs operating system x86 processor;computational modeling cognition registers standards operating systems analytical models semantics	We present an approach to modeling and verifying machine-code programs that exhibit non-determinism. Specifically, we add support for system calls to our formal, executable model of the user-level x86 instruction-set architecture (ISA). The resulting model, implemented in the ACL2 theorem-proving system, allows both formal analysis and efficient simulation of x86 machine-code programs; the logical mode characterizes an external environment to support reasoning about programs that interact with an operating system, and the execution mode directly queries the underlying operating system to support simulation. The execution mode of our x86 model is validated against both its logical mode and the real machine, providing test-based assurance that our model faithfully represents the semantics of an actual x86 processor. Our framework is the first that enables mechanical proofs of functional correctness of user-level x86 machine-code programs that make system calls. We demonstrate the capabilities of our model with the mechanical verification of a machine-code program, produced by the GCC compiler, that computes the number of characters, lines, and words in an input stream. Such reasoning is facilitated by our libraries of ACL2 lemmas that allow automated proofs of a program's memory-related properties.	acl2;automated theorem proving;correctness (computer science);executable;formal verification;gnu compiler collection;library (computing);machine code;nondeterministic algorithm;operating system;simulation;stream (computing);system call;theorem proving system;user space;verification and validation;x86	Shilpi Goel;Warren A. Hunt;Matt Kaufmann;Soumava Ghosh	2014	2014 Formal Methods in Computer-Aided Design (FMCAD)	10.1109/FMCAD.2014.6987600	parallel computing;real-time computing;concepts;formal verification;computer science;programming language;algorithm	PL	-21.468612190586214	31.978888930149388	83244
8580f9193c1fb62db0e2ed0d50056866b3f0ab77	an integrated program development tool for teaching and learning how to program	testing and debugging;introduction to programming;teaching and learning;integrated development environment;integrated program development tool;instructional environment;program development	Teaching and learning how to program requires environments designed to support these activities rather than commercially available integrated development environments. This paper presents an instructional environment which embraces the entire process of design, algorithm development, testing and debugging while minimizing the syntactic details with which students must cope. Students using this environment develop a view of programming in which design and testing are integral parts of program development.	algorithm;debugging;integrated development environment	Uta Ziegler;Thad Crews	1999		10.1145/299649.299786	simulation;computer science	HCI	-28.706978324246702	23.417748423565314	83544
578a6624343777891279094161a0c92da7a05f1b	yakyak: parsing with logical side constraints	programming language;first order logic;constraint programming;context free grammar	Programming language syntax is often described by means of a context-free grammar, which is restricted by constraints programmed into the action code associated with productions. Without such code, the grammar would explode in size if it were to describe the same language. We present the tool YakYak, which extends Yacc with rst-order logic for specifying constraints that are regular tree languages. Concise formulas about the parse tree replace explicit programming, and they are turned into canonical attribute grammars through tree automata calculations. YakYak is implemented as a preprocessor for Yacc, in which the transitions of the calculated tree automata are merged into the action code. We provide both practical experience and theoretical evidence that the YakYak approach results in fast and concisely speci ed parsers.	attribute grammar;automata theory;context-free grammar;context-free language;parse tree;parsing;preprocessor;regular tree grammar;tree automaton;yacc	Nils Klarlund;Niels Damgaard;Michael I. Schwartzbach	1999			parser combinator;bottom-up parsing;s-attributed grammar;top-down parsing	PL	-24.218533239326227	24.232119782326023	83822
7439a57734fc40310155f781676f679e10337c23	an analysis of generic expressions in situation semantics	conference paper	0. Introduction. This paper aims to provide a proper analysis of generic expressions within the framework of Situation Semantics. The analysis suggests a computational model for the appropriate semantic processing of the sentences with generic expressions. The theory in this paper is motivated on the basis of the data from Korean and English, and it is assumed to be generalized to other languages. For this purpose the generic expressions are divided into two categories: namely, Noun Phrase (NP) generics and Verb Phrase (VP) generics.	computational model;emoticon;generic programming;generics in java;np-completeness;vp/css	Ik-Hwan Lee	1995			operational semantics	NLP	-25.013713184225168	19.14257259845697	83853
4486f4a3c5cf7f292ef9030843211913284d2da8	attribute annotations and their use in c program deductive verification	deductive verification;attribute normalization;attribute annotations;axiomatic semantics;c-light;c-kernel	In this paper, a new kind of annotations called attribute annotations and the methodology for their application in deductive program verification are proposed. A collection of annotating attributes for the C-kernel subset of the C language is described, and, on their basis, two versions of axiomatic semantics of C-kernel—forward semantics and mixed forward semantics—are presented.	axiomatic semantics;formal verification;kernel (operating system)	M. M. Atuchin;Igor S. Anureev	2012	Automatic Control and Computer Sciences	10.3103/S0146411612070036	computer science;data mining;database;programming language	PL	-23.111076658890784	23.220195484631475	83887
ee884e582c85ccc5d123ec34064d93d616e71167	generation method of concurrency control program by using genetic programming	phase locking;genetic program;genetic programming;two phase locking protocol concurrency control program genetic programming database operation fitness measure function;genetic algorithms concurrency control;program generation;concurrency control;genetic algorithms;genetic programming concurrency control program generation;concurrency control schedules protocols probability genetics database systems	This paper proposes a generation system of concurrency control program by using genetic programming (GP). This system generates concurrency control program according to the features of transactions, which are collections of database operations. Functions and terminals of trees representing program in GP, and the fitness measure function used in GP are proposed. The functions and the terminals include those changing and testing variables attached to data items and transactions as well as those checking the kind of operation etc. These will bring us general concurrency control program, which is beyond the combination of the parts of traditional concurrency control program. As the granularity of the functions and the terminals is small, the sub-trees, which are used for the popular concurrency control protocol, and are prepared in advance, are used. The fitness measure function considers the goodness of concurrency control program. The experiments show that a concurrency control program using locks could be generated under the concurrent environment, while a concurrency control program better than the two-phase locking protocol could be generated under the not-so-concurrent environment.	concurrency control;experiment;genetic programming;lock (computer science);two-phase locking	Shinji Tamura;Teruhisa Hochin;Hiroki Nomiya	2011	2011 12th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing	10.1109/SNPD.2011.16	genetic programming;timestamp-based concurrency control;optimistic concurrency control;real-time computing;genetic algorithm;isolation;concurrent computing;concurrency;computer science;artificial intelligence;concurrency control;machine learning;database;distributed computing;multiversion concurrency control;non-lock concurrency control;programming language;serializability;concurrent object-oriented programming;distributed concurrency control	SE	-26.15886475527898	27.46583609632046	84127
a7f2702767e16b03a860d50f19fefd709c695f80	none, one, many - what's the difference, anyhow?	objects collections relationships pointers multiplicity null;004	We observe that compared to natural and modelling languages, the differences in expression required to deal with no, one, or many objects in programming languages are particularly pronounced. We identify some problems inherent in type-based unifications of different numbers, and advocate a solution that builds on the introduction of multiplicity as a new grammatical category of programming languages. 1998 ACM Subject Classification D.3.3 Language Constructs and Features	modeling language;programming language	Friedrich Steimann	2015		10.4230/LIPIcs.SNAPL.2015.294	arithmetic;mathematics;algorithm	PL	-23.295687469465005	21.836207800114597	84190
17cdc711ca8c9c850b7270cc7ef27b45ce15b789	towards typing for small-step direct reflection	symbolic computation;meta programming;reflection;type system;small step semantics	Direct reflection is a form of meta-programming in which program terms can intensionally analyze other program terms. Previous work defined a big-step semantics for a directly reflective language called Archon, with a conservative approach to variable scoping based on operations for opening a lambda-abstraction and swapping the order of nested lambda-abstractions. In this short paper, we give a small-step semantics for a revised version of Archon, based on operations for opening and closing lambda abstractions. We then discuss challenges for designing a static type system for this language, which is our ultimate goal.	archon;closing (morphology);lambda calculus;metaprogramming;opening (morphology);operational semantics;paging;reflection (computer programming);scope (computer science);type system	Jacques Carette;Aaron Stump	2012		10.1145/2103746.2103765	metaprogramming;symbolic computation;reflection;type system;computer science;theoretical computer science;programming language;algorithm	PL	-19.98736989105306	23.94876805493921	84197
2c618842c108c6169428a3075bcdfb5093738e2f	a python-based post-processing toolset for seismic analyses		This paper discusses the design and implementation of a Pythonbased toolset to aid in assessing the response of the UK’s Advanced Gas Reactor nuclear power stations to earthquakes. The seismic analyses themselves are carried out with a commercial Finite Element solver, but understanding the raw model output this produces requires customised post-processing and visualisation tools. Extending the existing tools had become increasingly difficult and a decision was made to develop a new, Python-based toolset. This comprises of a post-processing framework (aftershock) which includes an embedded Python interpreter, and a plotting package (afterplot) based on numpy and	embedded system;finite element method;numpy;programming tool;python;reactor (software);solver;video post-processing	Steve Brasier;Fred Pollard	2014	CoRR		real-time computing;simulation;computer science;operating system;programming language	EDA	-32.24744247517026	28.062728258875087	84226
7c0c9b93193dfe83099e19074949421fdd331583	the design and implementation of a program development system based on rewriting method	prototipificacion rapida;langage fonctionnel;developpement logiciel;lenguaje programacion;algebraic specification;concepcion asistida;computer aided design;rewrite rule;sistema experto;optimal rules;program development system;programming language;implementation;regle production;program design;lenguaje funcional;conception programme;ingenieria logiciel;software engineering;functional programming;specification language;term rewrite system;constrained type;ejecucion;rapid prototyping;type checking;design and implementation;algebraic specification language;rewrite systems;desarrollo logicial;reecriture;software development;genie logiciel;conception assistee;langage programmation;programmation fonctionnelle;lenguaje especificacion;systeme parallele;parallel system;systeme expert;rewriting;functional language;programacion funcional;langage specification;functional programming language;mixed language;program development;concepcion programa;sistema paralelo;reescritura;production rule;prototypage rapide;regla produccion;expert system	In this paper, we present a program development system based on rewriting method with the goal of providing an powerful tool for automatic software development and rapid prototyping. New mechanisms for defining constrained types and optimal rules are introduced to functional programming languages to achieve strong expressiveness. The enhanced functional programming language is combined with an algebraic specification language. Thus, the design from specification to program can be supported, and the efficiency and flexibility of programming can also be improved. In this system, both static and dynamic techniques are used to deal with constrained type check. All function definitions, computation constraints and optimal rules are regarded as rewriting rules. In order to provide a strong support for a large group of TRSs which may be neither terminating nor orthogonal, we propose a method using structure measure to decide TRSs' confluences. Based on this method, we present a partial completion algorithm to generate a rewriting model from a specification. Depending on term rewriting system, the mixed language system computes with parallel outermost and needed reduction. We explain the principles and implementation techniques in detail, some examples are provided.	algebraic specification;algorithm;computation;functional programming;newman's lemma;programming language;rapid prototyping;rewriting;software development;specification language;type class	Yongqiang Sun;Kai Lin;Li Shen	1997	SIGPLAN Notices	10.1145/251621.251627	specification language;rewriting;computer science;theoretical computer science;software development;program design language;programming language;functional programming;implementation;confluence;algorithm	PL	-23.561109683102256	28.392309689733768	84445
70cdab2bf4281ebebc163b41f4ab2139ffe7c6a8	composing transformations of compiled java programs with jabyce	java programming	This article introduces Jabyce, a software framework for the implementation and composition of transformations of compiled Java programs. Most distinguishing features of Jabyce are 1) its interaction orientation, i.e. it represents elements of transformed programs as interactions (method calls), which generally consumes less memory and CPU time than representing programs as graphs of objects; and 2) its component orientation, i.e. it allows for the design and composition of transformers as software components based on the Fractal component model. This latter point is strongly connected to infra-structural and architectural issues, and software engineering aspects such as composing, scaling, maintaining and evolving transformers. Jabyce is compared with other existing compiled Java programs transformation systems, using an extension of a previous well-known categorization of program transformation systems.	categorization;central processing unit;compiler;component-based software engineering;fractal component model;image scaling;interaction;java;list of program transformation systems;software framework;strongly connected component;transformers	Romain Lenglet;Thierry Coupaye;Eric Bruneton	2004	Comput. Sci. Inf. Syst.	10.2298/CSIS0402083L	real-time computing;java concurrency;computer science;theoretical computer science;operating system;java modeling language;database;real time java;programming language;java;generics in java;java annotation	PL	-27.826544626253646	29.598330806501963	84583
b57c99368ce04e039dbd84a3569b483bdc7abe40	on the problem of coupling java algorithms and xml parsers (invited paper)	xml parsine;application development;estensione di java;ingegneria industriale e dell informazione;api;java algorithms;java programming;dom;sax;loosely coupling techniques;application program interfaces;xml application program interfaces java program compilers;xml;api java algorithms xml parsers;xml parsers;program compilers;java xml computer languages tree data structures databases programming profession navigation data mining data structures expert systems;java	The joint use of Java and XML is a matter of fact for new developments, even in hard contexts. A recent research area is trying to address how to improve techniques for coupling Java programs and XML parsers and API. This paper briefly show the current state of the art of this young research area. Two perspectives are considered: efficiency (i.e. improvement of parsing performance) and effectiveness (development of techniques to obtain faster application development processes)	algorithm;application programming interface;computation;java;parsing;xml	Giuseppe Psaila	2006	17th International Workshop on Database and Expert Systems Applications (DEXA'06)	10.1109/DEXA.2006.102	document object model;simple api for xml;xml;application programming interface;computer science;operating system;xml schema;database;real time java;programming language;java;rapid application development;java annotation	DB	-29.780761971756107	24.898198590552216	84628
c149866b42050c6594a333a9e3ee85325d948b99	an operational semantics for zccs	sequences;language use;formal specification;boolean functions;boolean constants;formal specification language;operational semantics;zccs;agent language;z;data language;specification languages;tuples;refinement calculus;sets;value passing ccs;carbon capture and storage calculus data engineering systems engineering and theory computer science mathematics concrete specification languages vocabulary input variables;boolean functions formal specification refinement calculus specification languages;z operational semantics zccs value passing ccs agent language data language sets tuples sequences boolean constants formal specification language	G. Bruns (1995) has proposed a version of value-passing CCS in which an agent language, based on that proposed by Milner, is augmented with a rich data language. The data language can be used to describe sets, tuples and sequences etc. constructed from integer, Boolean and string constants. Z is a widely used formal specification language in which sets, tuples and sequences can be described, but also additional constructs such as free types and bindings. In addition, Z has a rich structuring mechanism-its schema calculus. Z is frequently used to specify the operations of a system on its state, and has a refinement calculus and formal semantics. This article introduces ZCCS, a version of value-passing CCS in which the data language used to describe the action/agent parameters and conditions is Z. We introduce the style and syntax of ZCCS and illuminate this with a small example. In addition, we present an operational semantics for ZCCS.	abstraction (software engineering);b-method;bisimulation;data structure;formal proof;formal specification;free variables and bound variables;gnu scientific library;input/output;language binding;logic gate;modal μ-calculus;model checking;nondeterministic algorithm;operation cyber condition zebra;operational semantics;ostensive definition;process calculus;prospective search;refinement (computing);refinement calculus;requirement;semantics (computer science);set theory;specification language;state space;symbolic computation;test template framework;workbench;z notation	Andy Galloway;Bill Stoddart	1997	First IEEE International Conference on Formal Engineering Methods	10.1109/ICFEM.1997.630434	natural language processing;refinement calculus;tuple;specification language;computer science;formal specification;sequence;boolean function;programming language;operational semantics;algorithm	DB	-22.141363463321895	21.863910277957704	84843
906fc0a6f7a269398df46e4997a0af5cd1b26a40	parsing languages by pattern matching	lenguaje programacion;bnf rule;high level languages;specification mechanism;programming language;right hand side;implementation;pattern search;specification;program interpreters;translator pattern matching universal algebras programming language specification bnf backus naur form signature language algebras program parsing code generation;code generation;context free;complexity;compiler;program evaluation;algorithm;ejecucion;grammars;pattern matching algebra programming profession program processors computer languages text recognition computer science cities and towns computer aided instruction;program parsing;program interpreters grammars high level languages program compilers;especificacion;pattern matching;source language;langage programmation;algebraic model;language;program compilers;universal algebra;free algebra	Any conventional programming language can be specified by a finite set of BNF rules and its algebra of symbols is generated by a finite set of generator classes. Thus any function defined on the finite set of generators offers an algebraic mechanism for a universal algorithm for source language program parsing. The right-hand side of the BNF rules are the patterns searched by the algorithm in the source text of the program. The essential feature of this algorithm is that it can be used a s a driver for code generation and optimization in a translator. This driver recognizes source language constructs in the source text. The code generator evaluates them into the target language regenerating the source language program inductively a s a target language program. Therefore this parser is also called a program evaluator.	algorithm;beta normal form;code generation (compiler);compiler;interpreter (computing);mathematical optimization;parsing;pattern matching;programming language	Teodor Rus	1988	IEEE Trans. Software Eng.	10.1109/32.4672	pattern search;free algebra;universal algebra;compiler;complexity;program evaluation;computer science;theoretical computer science;pattern matching;language;programming language;implementation;high-level programming language;specification;algorithm;code generation	PL	-25.59198595412982	23.90161005282316	84925
b40639cd4b3e8d46e913e69f1b333876271cfa61	deriving verification conditions and program assertions to support software inspection	verification;reading technique;loop termination conditions verification conditions program assertions software inspection reusable software component correctness formal code reading techniques syntax semantic information hoare style formal proofs human reasoning based code reading first order predicate calculus specifications input output diagrams constructive reports;verification conditions;inspection software engineering;syntax;reusable software component;first order predicate calculus specifications;stepwise abstraction;reading techniques;program control structures;loop termination conditions;general techniques;formal semantics;program verification;inspection;software engineering;program assertions;input output;pre2009 software engineering;correctness;semantic information;first order;human reasoning based code reading;faculty of engineering and information technology;software reusability;practical formal methods;software component;trusted components;constructive reports;proof of correctness;hoare style formal proofs;program verification program control structures software reusability;280302;input output diagrams;formal code reading techniques;software inspection;clean room software development	In order to trust a reusable software component, the correctness of its implementation with respect to its specification must be assured. Formal proof of correctness, while offering this assurance, is an often difficult, if not impractical, goal to achieve. Formal code reading techniques employed in software inspection have proved useful as a human reasoning process to verify correctness with a high degree of assuredness. Such techniques rely on the reader being able to abstract the semantics of a given component in order to reason about its correctness. The paper presents a method and supporting tools which yield formal semantic properties directly from the syntax of the component's code. The method includes an extension of existing algorithmic and heuristic invariant generation techniques. Although, in many cases, the semantic information derived is strong enough to be useful as program assertions in Hoare-style formal proofs, we focus on the generation of information to assist human reasoning based code reading processes. To this end, we use examples to illustrate the application of the method and prototype tools to yield semantic information directly from program code, such as first-order predicate calculus specifications, input-output diagrams, and constructive reports of loop termination conditions.	assertion (software development);software inspection	Daniel Powell	2002		10.1109/APSEC.2002.1183016	input/output;correctness;verification;syntax;inspection;formal verification;computer science;theoretical computer science;component-based software engineering;software engineering;formal semantics;first-order logic;formal specification;software inspection;programming language;algorithm	SE	-19.24508597156918	26.448249407189863	84929
218829a6ef265fc2b61a0d99c129d67f00ac4b2b	scripting: higher-level programming for the 21st century	computer languages assembly systems programming profession application software computer aided instruction registers program processors automatic generation control algorithm design and analysis buildings;computer languages;programming language;application software;computer aided instruction;scripting languages;authoring languages;system programming languages;object oriented programming;higher level programming;software components;perl;rapid application development;software components scripting higher level programming 21st century system programming languages c c scripting languages perl tcl data structures;registers;component framework;data structures;programming profession;automatic generation control;programming authoring languages;assembly systems;21st century;tcl;c;programming;program processors;scripting;algorithm design and analysis;buildings;scripting language;type system	"""Scripting languages such as Perl and Tcl represent a very different style of programming than system programming languages such as C or JavaTM. Scripting languages are designed for """"gluing"""" applications; they use typeless approaches to achieve a higher level of programming and more rapid application development than system programming languages. Increases in computer speed and changes in the application mix are making scripting languages more and more important for applications of the future."""	perl;programming language;rapid application development;scripting language;system programming;tcl	John K. Ousterhout	1998	IEEE Computer	10.1109/2.660187	fourth-generation programming language;read–eval–print loop;computer architecture;protocol;this;declarative programming;data structure;computer science;programming language generations;domain-specific language;theoretical computer science;third-generation programming language;software engineering;computer programming;scripting language;programming paradigm;fifth-generation programming language;programming language theory;programming language;second-generation programming language;comparison of multi-paradigm programming languages;programming in the large and programming in the small	Visualization	-30.09412881250461	26.63195351050196	84938
0a7b289d635a5617c4439780495bb0c6d4e3d456	the simple essence of automatic differentiation		Automatic differentiation (AD) in reverse mode (RAD) is a central component of deep learning and other uses of large-scale optimization. Commonly used RAD algorithms such as backpropagation, however, are complex and stateful, hindering deep understanding, improvement, and parallel execution. This paper develops a simple, generalized AD algorithm calculated from a simple, natural specification. The general algorithm is then specialized by varying the representation of derivatives. In particular, applying well-known constructions to a naive representation yields two RAD algorithms that are far simpler than previously known. In contrast to commonly used RAD implementations, the algorithms defined here involve no graphs, tapes, variables, partial derivatives, or mutation. They are inherently parallel-friendly, correct by construction, and usable directly from an existing programming language with no need for new data types or programming style, thanks to use of an AD-agnostic compiler plugin.	algorithm;automatic differentiation;backpropagation;compiler;deep learning;mathematical optimization;programming language;programming style;rapid application development;state (computer science)	Conal Elliott	2018	PACMPL	10.1145/3236765	compiler;data type;implementation;automatic differentiation;deep learning;programming style;theoretical computer science;stateful firewall;backpropagation;artificial intelligence;computer science	PL	-21.870411522096738	25.395582552660837	84939
e70777147ddf2acbddbe1ccb6e9c0b23a21166af	verification of cache coherence protocols wrt. trace filters		We address the problem of parameterized verification of cache coherence protocols for hardware accelerated transactional memories. In this setting, transactional memories leverage on the versioning capabilities of the underlying cache coherence protocol. The length of the transactions, their number, and the number of manipulated variables (i.e., cache lines) are parameters of the verification problem. Caches in such systems are finite-state automata communicating via broadcasts and shared variables. We augment our system with filters that restrict the set of possible executable traces according to existing conflict resolution policies. We show that the verification of coherence for parameterized cache protocols with filters can be reduced to systems with only a finite number of cache lines. For verification, we show how to account for the effect of the adopted filters in a symbolic backward reachability algorithm based on the framework of constrained monotonic abstraction. We have implemented our method and used it to verify transactional memory coherence protocols with respect to different conflict resolution policies.	algorithm;automata theory;cache (computing);cache coherence;executable;finite-state machine;formal language;memory coherence;non-monotonic logic;particle filter;prototype;reachability;shared variables;tracing (software);transactional memory	Parosh Aziz Abdulla;Mohamed Faouzi Atig;Zeinab Ganjei;Ahmed Rezine;Yunyun Zhu	2015	2015 Formal Methods in Computer-Aided Design (FMCAD)		acceleration;model checking;communications protocol;parallel computing;coherence;cache;computer science;write-once;theoretical computer science;cache invalidation;pattern matching;distributed computing;automaton;programming language;cache algorithms;algorithm;mesif protocol	Logic	-20.215007487603874	30.908248858360945	85275
9f5b515d6ad4d53ec2597becec89fcd9483d8144	approaches to work cell programming language design	programming language design;sbcl cell programming languages;bepress selected works	User-friendly work ce ll programming languages are needed for many flexible manufacturing system applications. Th is pape r surve ys re presen tative language s and code generators and identifies design crite ria for improved work ce ll program m ing languages. Two obje ct-like language s deve loped to support programming of a small work ce ll are presented. The first, Cell Programming Language ( CPL), is a traditional statement-oriented language that provides a simple and user-friendly model for programming a small work ce ll using object-like constructs. The second language , State-based Control Language (SBCL), is a state-based language using the same objects as CPL, but incorporating support for concurrency. Both languages focus on ce ll-leve l control. The languages are implemented in an interpreted (or virtual mach ine ) environment that distinguishes them from most othe r con trol language s, which are base d on machine language .	cpl;concurrency (computer science);machine code;naruto shippuden: clash of ninja revolution 3;programming language;sbcl;usability;wilhelm pape	Douglas Troy;Yasser Dessouky;George Hellstern;H. Mark;Zhuming Wang	2000	Int. J. Computer Integrated Manufacturing	10.1080/095119200129948	natural language processing;fourth-generation programming language;first-generation programming language;declarative programming;very high-level programming language;language primitive;programming domain;computer science;programming language generations;domain-specific language;third-generation programming language;functional logic programming;database;compiled language;programming paradigm;low-level programming language;fifth-generation programming language;programming language theory;programming language;second-generation programming language;high-level programming language;comparison of multi-paradigm programming languages;algorithm;control flow analysis	PL	-25.05698689487515	23.63260538338739	85287
aad8e509da9d9f9c632613b5465aee5811ea0ab3	fine-grained annotations for pointcuts with a finer granularity	aspectj;pointcut languages;annotations	A number of authors have suggested that AspectJ-like pointcut languages are too limited, and that they cannot select every possible join point in a program. Many enhanced pointcut languages have been proposed; they require virtually no change to the original code, but their improved expressive power comes often at the cost of making the pointcut expression too tightly connected with the structure of the programs that are being advised. Other solutions consist in simple extensions to the base language; they require only small changes to the original code, but they frequently serve no other immediate purpose than exposing pieces of code to the weaver. Annotations are a form of metadata that has been introduced in Java 5. Annotations have a number of uses: they may provide hints to the compiler, information to code processing tools and they can be retained at runtime. At the moment of writing, runtime-accessible annotations in the Java programming language can only be applied to classes, fields and methods. The support to annotate expressions and blocks feels like a natural extension to Java's annotation model, that can be also exploited to expose join points at a finer-grained level. In this paper we present an extension to the AspectJ language to select block and expression annotations in the @Java language extension.	aspectj;compiler;java;join point;pointcut;programming language;run time (program lifecycle phase)	Walter Cazzola;Edoardo Vacchi	2013		10.1145/2480362.2480685	aspect-oriented programming;computer science;theoretical computer science;operating system;database;programming language;world wide web;java annotation	PL	-25.213312358123773	28.381911308198774	85412
2664886d8ed6a214c89f4b6d3fc7edd95466337b	lowering the learning curve for declarative programming: a python api for the idp system		Programmers may be hesitant to use declarative systems, because of the associated learning curve. In this paper, we present an API that integrates the IDP Knowledge Base system into the Python programming language. IDP is a state-of-the-art logical system, which uses SAT, SMT, Logic Programming and Answer Set Programming technology. Python is currently one of the most widely used (teaching) languages for programming. The first goal of our API is to allow a Python programmer to use the declarative power of IDP, without needing to learn any new syntax or semantics. The second goal is allow IDP to be added to/removed from an existing code base with minimal changes.	algorithm;answer set programming;application programming interface;boolean satisfiability problem;correctness (computer science);debugging;declarative programming;formal system;knowledge base;logic programming;lua;programmer;programming language;python;satisfiability modulo theories;source lines of code;stable model semantics;standard streams	Joost Vennekens	2017		10.1007/978-3-319-51676-9_6	parallel computing;computer science;theoretical computer science;programming language	PL	-21.535575782304843	18.99281416355892	85460
12eb0c2302423de1877de3e0be62161a8e70e2ac	automated and scheduled maintenance of digital library collections	digital library;digital libraries;linux application program interfaces digital libraries;digital library collections;automated maintenance;application program interface;software libraries automatic programming vehicles processor scheduling linux windows modular construction mathematics computer science application software;linux digital library collections scheduled maintenance automated maintenance application programming interfaces;application program interfaces;application programming interfaces;linux;task scheduling;scheduled maintenance	In this paper, we propose a strategy for the automated and scheduled maintenance of a digital library collection. Existing systems require the user either to add new data manually to a collection, or to have programming knowledge in order to use existing application programming interfaces (APIs) in order to automate scheduled collection updates. We incorporate a scheduling module into the Greenstone digital library software, which allows the user to set up scheduled and automated building of a collection at periodic intervals. This module interacts with the task scheduler on the host platform, such as Linux. Windows and Mac OS X, thereby making it a simple yet powerful tool for scheduled collection maintenance.	application programming interface;cron;digital library;experiment;librarian;linux;mac os x public beta;microsoft windows;operating system;scheduling (computing);windows task scheduler	Wendy Osborn;Steve Fox	2007	2007 2nd International Conference on Digital Information Management	10.1109/ICDIM.2007.4444199	embedded system;digital library;application programming interface;computer science;operating system;database;programming language;world wide web	Embedded	-31.459159615454965	26.48886002525826	85504
b12fa9313d33b8c81d8726c8b73641ef16c7708a	language interoperability in control network programming		Control Network Programming (CNP) is a programming paradigm which is being described with the maxim “Primitives + Control Network = Control Network program”. It is a type of graphic programming. The Control Network is a recursive system of graphs; it can be a purely descriptive specification of the problem being solved. Clearly, ‘drawing’ the control network does not include any programming. The Primitives are elementary, easily understandable and clearly specified actions. Ultimately, they have to be programmed. Historically, they are usually coded in Free Pascal. The actual code of the primitives has never been considered important. The essence of an “algorithm” is represented by its control network. CNP was always meant to be an easy and fast approach for software application development that actually involves very little real programming. Language interoperability (using different languages in the same software project) is a distinguished current trend in software development. It is even more important and natural in the case of CNP than for other programming paradigms. Here, interoperability practically means the possibility to use primitives written in various programming languages. The current report describes our first steps in creating applications using a multi-language set of primitives. Most popular and interesting programming languages have been addressed: Python, Java, and C. We show how to create applications with primitives written in those ‘non-native’ languages. We consider examples where the primitives in all those four programming languages are simultaneously used (multiple-language CNP). We also discuss CNP programming without programming (language-free CNP).	algorithm;c++;cloud computing;contract net protocol;data (computing);elementary;free pascal;graph (discrete mathematics);integrated development environment;interpreted language;java;kotlin;language interoperability;language primitive;programming language;programming paradigm;programming tool;python;recursion;software development;software project management;virtual machine;visual basic;visual basic[.net]	Kostadin D. Kratchanov;Efe Ergün	2018	CoRR		theoretical computer science;programming language;interoperability;recursion;programming paradigm;software;software development;language interoperability;python (programming language);computer science;java	PL	-28.551636559615357	27.73843197112215	85563
2c045e0c3cea270792fd0d31a3a40d84dd6a6a6e	nesting in an object-oriented language is not for the birds	programming in the large;programming language design;object oriented language;programming language;programming environment;object oriented programming;multi user;nesting;block structure;object oriented;object oriented programming languages	The notion of nested blocks has come into disfavour or has been ignored in recent program language design. Many of the current object oriented programming languages use subclassing as the sole mechanism to establish relationships between classes and have no general notion of nesting. We argue that nesting (and, more generally, hierarchical organization) is a powerful mechanism that provides facilities that are not otherwise possible in a class based programming language. We agree that traditional block structure and its associated nesting have severe problems, and we suggest several extensions to the notion of blocks and block structure that indirectly make nesting a useful and powerful mechanism, particularly in an object oriented programming system. The main extension is to allow references to definitions from outside of the containing block, thereby making the contained definitions available in a larger scope. References are made using either the name of the containing entity or an instance of the containing entity. The extensions suggest a way to organize the programming environment for a large, multi-user system. These facilities are not available with subclassing, and subclassing provides facilities not available by nesting; hence, an object oriented language can benefit by providing nesting as well.	admissible numbering;integrated development environment;multi-user;programming language	Peter A. Buhr;C. Robert Zarnke	1988		10.1007/3-540-45910-3_8	fourth-generation programming language;first-generation programming language;protocol;method;very high-level programming language;language primitive;programming domain;reactive programming;computer science;object;theoretical computer science;extensible programming;third-generation programming language;programming paradigm;symbolic programming;low-level programming language;inductive programming;fifth-generation programming language;programming language;object-oriented programming;programming language specification;second-generation programming language;high-level programming language;algorithm	PL	-25.365806406992363	27.16739326009384	85604
6a234370c7d08861077ae9bc32567f8bd9c03a30	type assignment in programming languages	thesis or dissertation;programming language;kb thesis scanning project 2015;computer software programming computer software	The purpose of this work is to present and study a family of polymorphic type disciplines for programming languages similar to the type discipline of ML, the metalanguage of the LCF system, which are based on the use of type inference systems to define the notion of well typed expressions and programs and on the use of type assignment algorithms to compute the type or types that can be inferred for those same expressions or programs. Previous work on the theoretical foundations of the ML type discipline is reexamined and completed here. It is also extended in two directions, namely to handle overloading of identifiers and also to cope with a semantics involving references to a store as first class objects. For each of the theories studied here we present proofs of the semantic soundness of type inference, i.e. that well typed expressions evaluate to objects of the correct type and that in particular they do not lead to run-time errors like trying to add an integer to a list. Algorithms for computing the type or types which can be inferred for expressions are also presented together with proofs of the soundness and completeness of the algorithms, i.e. that the algorithms compute exactly the types which can be actually inferred for the expressions.	algorithm;applicative programming language;compiler;first-class function;function overloading;identifier;inference engine;principal type;program transformation;programming language;theory;type inference;type system	Luís Damas	1984			fourth-generation programming language;first-generation programming language;computing;declarative programming;very high-level programming language;programming domain;computer science;software development;extensible programming;third-generation programming language;software engineering;functional logic programming;computer programming;programming paradigm;symbolic programming;inductive programming;fifth-generation programming language;programming language theory;programming language;system programming;second-generation programming language;comparison of multi-paradigm programming languages	PL	-22.13994481942003	23.643114250964256	85623
4267293888fd3e5be3ff1de7a7830d0e58dd2fe6	how optimized environmental sensing helps address information overload on the web	optimal solution;mobile robot;information overload;reliable communication;sensor placement;sensor nodes;environmental monitoring;activity recognition	In this talk, we tackle a fundamental problem that arises when using sensors to monitor the ecological condition of rivers and lakes, the network of pipes that bring water to our taps, or the activities of an elderly individual when sitting on a chair: Where should we place the sensors in order to make effective and robust predictions?  Such sensing problems are typically NP-hard, and in the past, heuristics without theoretical guarantees about the solution quality have often been used. In this talk, we present algorithms which efficiently find provably near-optimal solutions to large, complex sensing problems. Our algorithms are based on the key insight that many important sensing problems exhibit submodularity, an intuitive diminishing returns property: Adding a sensor helps more the fewer sensors we have placed so far. In addition to identifying most informative locations for placing sensors, our algorithms can handle settings, where sensor nodes need to be able to reliably communicate over lossy links, where mobile robots are used for collecting data or where solutions need to be robust against adversaries and sensor failures.  We present results applying our algorithms to several real-world sensing tasks, including environmental monitoring using robotic sensors, activity recognition using a built sensing chair, and a sensor placement competition. We conclude with drawing an interesting connection between sensor placement for water monitoring and addressing the challenges of information overload on the web. As examples of this connection, we address the problem of selecting blogs to read in order to learn about the biggest stories discussed on the web, and personalizing content to turn down the noise in the blogosphere.		Carlos Guestrin	2009		10.1145/1601966.1601970	mobile robot;simulation;computer science;machine learning;information overload;data mining;environmental monitoring;computer security;activity recognition	HCI	-32.045197092901404	19.3370527665821	85763
d76dcb3fa025803f9c2fe42d1b21bc571e98fd19	sensors meet the cloud: planetary-scale distributed sensing and decision making	energy conservation;energy constraints;internet distributed decision making distributed sensors energy conservation;sensors;energy efficient;networked actuator;environmental pollutants;data collection;planetary scale distributed sensing;higher level knowledge;holistic approach;higher level information;cyber physical systems;networked sensor;sensor network;distributed sensors;wireless communication;media;internet of things;internet;distributed decision making;machine learning;monitoring;autonomous controllers;sensor networks;sensors asia computer science monitoring cloud computing media;decision theory;inventory tracking;cloud computing planetary scale distributed sensing decision making networked sensor networked actuator inventory tracking environmental pollutants data collection higher level information higher level knowledge autonomous controllers cyber physical systems internet sensor networks energy efficient sensor data aggregation affordable sensor data streaming energy constraints network constraints network uncertainty;sensors and actuators;energy efficient sensor data aggregation;affordable sensor data streaming;network uncertainty;network constraints;computer science;environmental pollutant;distributed sensing;asia;cloud computing	Networked sensors and actuators are increasingly pervasive in our daily lives, from tracking inventory in warehouses to monitoring movement of environmental pollutants and helping elderly living a more independent life. The data collected by the sensors needs to be distilled into higher level information and knowledge that must be promptly acted upon by autonomous controllers or humans. The scale of these systems — sometimes called Internet of Things (IOT), Cyber-Physical Systems (CPS), or Sensor Networks (Sensornet) — dwarfs the Internet as we know today. In addition to the challenges of making the sensors small, affordable, and energy efficient, and networking them into a reliable ensemble, the more arduous task is the timely processing of voluminous and streaming sensor data subject to energy and network constraints and uncertainties. To address these challenges, we must leverage the advances in disciplines such as devices, wireless communication, control and decision theory, machine learning, and database, to name a few. Cloud computing provides new opportunities in aggregating sensor data and exploiting the aggregates for greater coverage and relevancy, and yet at the same time exacerbates the issue of privacy and security. A multi-disciplinary, holistic approach is needed.	as-interface;autonomous robot;autonomous system (internet);cloud computing;cyber-physical system;database;decision theory;holism;internet of things;machine learning;pervasive informatics;planetary scanner;relevance;sensor	Feng C. Zhao	2010	9th IEEE International Conference on Cognitive Informatics (ICCI'10)	10.1109/COGINF.2010.5599715	real-time computing;simulation;engineering;computer security	Robotics	-32.67468039026811	18.923871271506446	85858
15cc1f94241e699603700c1bb70feae8949503d8	unifying theories of interrupts	unifying theory;algebraic law;denotational semantics;complex interaction;model failure;complex scheduling pattern;csp model;complete theory;interrupt operator;unifying theories;different area	The concept of an interrupt is one that appears across many paradigms, and used in many different areas. It may be used as a device to assist specifications to model failure, or to describe complex interactions between non co-operating components. It is frequently used in hardware to allow complex scheduling patterns. Although interrupts are ubiquitous in usage, the precise behaviour of a system incorporating interrupts can be difficult to reason about and predict. In this paper, a complete theory of the interrupt operator presented by Hoare in his original treatment of CSP is proposed. The semantics are given in the CSP model in Unifying Theories of Programming. New and existing algebraic laws are proposed and justified. The contribution of the paper is therefore a denotational semantics of an interrupt operator, and a collection of algebraic laws that assist in reasoning about systems incorporating interrupts.	denotational semantics;hoare logic;interaction;interrupt;linear algebra;scheduling (computing);unifying theories of programming	Alistair A. McEwan;Jim Woodcock	2008		10.1007/978-3-642-14521-6_8	real-time computing;computer science;theoretical computer science;programming language;algorithm	AI	-20.83786945016214	21.588101919644615	85862
8846830d114826b97cf3e532f1d621a6ae562178	cocovila - compiler-compiler for visual languages	compiler compiler;attribute grammar;extended attribute grammars;visual programming;visual languages;visual language	A compiler-compiler for visual languages is presented. It has been designed as a framework for building visual programming environments that translate schemas into textual representation as well as into programs representing the deep meaning of schemas. The deep semantics is implemented by applying attribute grammars to schema languages; attribute dependencies are implemented as methods of Java classes. Unlike compiler-compilers of textual languages, a large part of the framework is needed for support of interactive usage of a visual language.	attribute grammar;compiler;compiler-compiler;java;visual language;visual programming language	Pavel Grigorenko;Ando Saabas;Enn Tyugu	2005	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2005.05.009	natural language processing;l-attributed grammar;compiler-compiler;computer science;third-generation programming language;syntax;database;fifth-generation programming language;visual programming language;programming language;attribute grammar;second-generation programming language;comparison of multi-paradigm programming languages	PL	-26.956403843214186	22.373972271445393	85929
f2a9173d93016af42da4fa1e9671233bb338a73c	specifying the semantics of machine instructions	machine independent issues;read only memory instruments computer science process design specification languages australia council binary codes runtime operating systems programming profession;software portability;instruments;formal specification;australia council;retargetable;instruction set;binary codes;machine code manipulation tools;retargetable binary translator;runtime;instruction sets systems re engineering software portability specification languages;cisc;process design;computer architecture;machine instructions semantics;instrumentors;instruction set processor descriptions;specification languages;binary translation;programming profession;binary debuggers machine instructions semantics instruction set instruction set processor descriptions retargetable binary translator machine independent issues intermediate representation semantic specification language retargetable binary translation framework machine code manipulation tools optimizing compilers binary profilers instrumentors;risc;computer science;retargetable binary translation framework;binary debuggers;2200 engineering;optimizing compilers;binary profilers;read only memory;semantic specification language;operating systems;instruction sets;intermediate representation;systems re engineering	Computer architecture manuals describe the instruction set of the machine and the semantics of those instructions by a combination of natural language and ISP (Instruction Set Processor) descriptions. The syntax of the instructions in assembly is well deened in the form of tables in the manual. However, the semantics is not so well speciied and descriptions vary widely from one manual to another. When developing a retargetable binary translator, as much as possible needs to be speciied in order to automatically generate code from speciications, hence separating machine-independent issues from the manual coding stage. The speciication of the semantics of machine instructions is one such task, with the aim of generating suitable code for an intermediate representation that is to be used during the analysis stage. We describe the design process used to develop a semantic speciica-tion language, SSL, to integrate into a retargetable binary translation framework. The techniques described herein are suitable not just to binary translators but also to machine-code manipulation tools such as optimizing compilers, binary proolers, instrumentors, and binary debuggers.	binary code;binary translation;c++;computer architecture;debugger;email;intermediate representation;machine code;machine-dependent software;natural language;object-z;optimizing compiler;ramsey's theorem;table (database)	Cristina Cifuentes;Shane Sendall	1998		10.1109/WPC.1998.693332	computer architecture;parallel computing;computer science;operating system;instruction set;programming language	PL	-26.563974531233008	24.06806845471303	86344
08a69eb42f087ce76876aef224d32f2109367307	modular multiset rewriting		Data Types interface QUEUE out enq: nat→ o out deq_req: o in deq: nat→ o end module queue provide QUEUE local head : ι→ o tail : ι→ o data: nat× ι× ι→ o · ( ∃d . head(d), tail(d) !∀e.∀d . enq(e), head(d) ( ∃d ′. data(e, d ′, d), head(d ′) !∀e.∀d .∀d ′. deq_req, tail(d ′), data(e, d , d ′)  ( deq(e), tail(d) end Motivations Core Language Modularity Conclusions Sharing Private Names module cell (v : nat) provide out get : o in got : nat→ o out set : nat→ o local content : nat→ o · ( content(v) !∀v . get, content(v) ( got(v), content(v) !∀v .∀v ′. set(v ′), content(v) ( content(v ′) end · ( C as cell(s(z)). [ p(C .set), q(C .get,C .got) ] ∀write. p(write) ( write(z) [ ∀read_req. ∀read . ] q(read_req, read) ( [ read_req, ∀r . read(r) ( s(r) ] Motivations Core Language Modularity Conclusions	abstract data type;blueprint;c++;compiler;constraint handling rules;forward chaining;horn clause;lecture notes in computer science;logic programming;mobile app;modular programming;network address translation;operational semantics;process calculus;programming language;recursion;rewriting;rule-based system;springer (tank);λprolog	Iliano Cervesato;Edmund Soon Lee Lam	2015		10.1007/978-3-662-48899-7_36	programming language;theoretical computer science;algorithm;multiset;abstract data type;computer science;process calculus;standard ml;logic programming;modular design;concurrent computing;rewriting	Robotics	-24.20293543107563	22.757201707802672	86488
dfaaf3032daf84eb6f419ce38620084196781ba0	scalable local-recoding anonymization using locality sensitive hashing for big data privacy preservation	cloud;privacy preservation;lsh;big data;mapreduce	While cloud computing has become an attractive platform for supporting data intensive applications, a major obstacle to the adoption of cloud computing in sectors such as health and defense is the privacy risk associated with releasing datasets to third-parties in the cloud for analysis. A widely-adopted technique for data privacy preservation is to anonymize data via local recoding. However, most existing local-recoding techniques are either serial or distributed without directly optimizing scalability, thus rendering them unsuitable for big data applications. In this paper, we propose a highly scalable approach to local-recoding anonymization in cloud computing, based on Locality Sensitive Hashing (LSH). Specifically, a novel semantic distance metric is presented for use with LSH to measure the similarity between two data records. Then, LSH with the MinHash function family can be employed to divide datasets into multiple partitions for use with MapReduce to parallelize computation while preserving similarity. By using our efficient LSH-based scheme, we can anonymize each partition through the use of a recursive agglomerative $k$-member clustering algorithm. Extensive experiments on real-life datasets show that our approach significantly improves the scalability and time-efficiency of local-recoding anonymization by orders of magnitude over existing approaches.	algorithm;big data;cloud computing;cluster analysis;computation;data anonymization;data-intensive computing;experiment;information privacy;locality of reference;locality-sensitive hashing;mapreduce;minhash;real life;recursion;scalability;lsh	Xuyun Zhang;Christopher Leckie;Wan-Chun Dou;Jinjun Chen;Kotagiri Ramamohanarao;Zoran A. Salcic	2016		10.1145/2983323.2983841	big data;cloud computing;computer science;data mining;database;internet privacy;world wide web	DB	-30.691502451200932	20.271586485718792	86519
9453e539819048a7a72740bb9bfa438bd3b78184	checking array bounds by abstract interpretation and symbolic expressions		Array access out of bounds is a typical programming error. From the ’70s, static analysis has been used to identify where such errors actually occur at runtime, through abstract interpretation into linear constraints. However, feasibility and scalability to modern object-oriented code has not been established yet. This article builds on previous work on linear constraints and shows that the result does not scale, when polyhedra implement the linear constraints, while the more abstract zones scale to the analysis of medium-size applications. Moreover, this article formalises the inclusion of symbolic expressions in the constraints and shows that this improves its precision. Expressions are automatically selected on-demand. The resulting analysis applies to code with dynamic memory allocation and arrays held in expressions. It is sound, also in the presence of arbitrary side-effects. It is fully defined in the abstract interpretation framework and does not use any code instrumentation. Its proof of correctness, its implementation inside the commercial Julia analyzer and experiments on third-party code complete the work.	abstract interpretation;regular expression	Étienne Payet;Fausto Spoto	2018		10.1007/978-3-319-94205-6_46	computer science;discrete mathematics;correctness;scalability;abstract interpretation;expression (mathematics);polyhedron;static analysis;c dynamic memory allocation;instrumentation (computer programming)	Logic	-19.832362997026582	29.736560491629373	86555
238ebf746c9296fdb5df7db8498d7ea0d22800b2	a tractable scheme implementation	programming language;virtual machine;partial evaluation;scheme;virtual machines;modularity	Scheme 48 is an implementation of the Scheme programming language constructed with tractability and reliability as its primary design goals. It has the structural properties of large, compiler-based Lisp implementations: it is written entirely in Scheme, is bootstrapped via its compiler, and provides numerous language extensions. It controls the complexity that ordinarily attends such large Lisp implementations through clear articulation of internal modularity and by the exclusion of features, optimizations, and generalizations that are of only marginal value.	biconnected component;compiler;lisp;marginal model;programming language;scheme	Richard Kelsey;Jonathan Rees	1994	Lisp and Symbolic Computation		generalization;parallel computing;image processing;computer science;virtual machine;fexpr;theoretical computer science;programming language	PL	-22.54852364194064	23.722135821716105	86589
9da595666a9c388c54ee1bc6ae728e99fcf758dd	making exhaustive search programs deterministic	static checking;parallelisme;guarded horn clause;compilacion;multiple binding environments;clause horn gardee;algoritmo busqueda;modelo determinista;algorithme recherche;prolog;horn clause;search algorithm;program transformation;modele deterministe;transformation programme;parallelism;transformacion programa;paralelismo;mode analysis;continuation;compilation;guarded horn clauses;clause horn;deterministic model;exhaustive search	This paper presents a technique for compiling a Horn-clause program intended for exhaustive search into a GHC (Guarded Horn Clauses) program. The technique can be viewed also as a transformation technique for Prolog programs which compiles away the ‘bagof’ primitive and non-determinate bindings. The class of programs to which our technique is applicable is shown with a static checking algorithm; it is nontrivial and could be extended. An experiment on a compiler-based Prolog system showed that our technique improved the efficiency of exhaustive search by 6 times for a permutation generator program. This compilation technique is important also in that it exploits the AND-parallelism of GHC for parallel search.	algorithm;brute-force search;compiler;horn clause;parallel computing;prolog;property (philosophy);the glorious glasgow haskell compilation system	Kazunori Ueda	1987	New Generation Computing	10.1007/BF03037456	horn clause;computer science;theoretical computer science;deterministic system;continuation;brute-force search;programming language;prolog;algorithm;search algorithm	PL	-20.302527029677385	23.566935436959056	86730
08b7cff47b3e1c17cf856f641d6cbe207490f03d	a system of constructor classes: overloading and implicit higher-order polymorphism	language use;functional programming;higher order;polymorphism;type classes;functional language;type system	This paper deaeribes a flexible type system which combines overloading and higher-order polymorphism in an implicitly typed language using a system of constructor classes – a natural generalization of type classes in Haskell. We present a wide range of examples which demonstrate the usefulness of such a system. In particular, we show how constructor classes can be used to support the use of monads in a functional language. The underlying type system permits higher-order polymorphism but retains many of many of the attractive features that have made the use of HincUey/Milner type systems so popular. In particular, there is an effective algorithm which can be used to calculate principal types without the need for explicit type or kind annotations. A prototype implementation has been developed providing, amongst other things, the first concrete implementation of monad comprehensions known to us at the time of writing. 1 An overloaded map function Many functional programs use the map function to apply a function to each of the elements in a given list. The type and definition of this function as given in the Haskell standard prelude [6] are as follows: map :: (a 4 b) -) [a] + [b] map j [] = [1 mapj(s:n) = ~z:mapjzs It is well known that the map function satisfies the famihm laws: map id = id map f .mapg = map (j . g) A category theorist will recognize these observations as indicating that there is a functor from types to types whose object part maps any given type a to the list type [a] and whose arrow part maps each function f :: a + b to the function map f :: [a] + [b]. A functional programmer will recognize that similar const~ctions are also used with a wide Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, or to rapublish, raquires a fee and/or specific permission. ACM-FPCA’93-6/93 /Copenhagen, DK Q 1993 ACM O-89791 -595-X193 /0006 /0052 . ..$5rIrI range of other data types, ~ illustrated by the following examples: data Tree a = Leaj a I Tree a:”: Zkee a map Tree :: (a+ b)+(Tree a+ Tree b) mapZ’ree j (Leaf z) = Leaj (f z) map Tree j (1 :’: r) = map Tree j 1 :“: map Trwe j r data Opt a = Just a I Nothing map Opt :: (a -+ b) + (Opt a + Opt b) map Opt f (lust x) = Just (j z) mapOpt f Nothing = Nothing Each of these functions haa a similar type to that of the original m up and also satisfies the functor laws given above. With this in mind, it seems a shame that we have to use different names for eaeh of these variants. A more attractive solution would allow the use of a single name map, relying on the types of the objects involved to determine which particular vemion of the map function is required in a gjven situation. For example, it is clear that map (1+) [1, 2, t?] should be a list, calculated using the original map function on lists, while map (1+) (Just 1 ) should evaluate to Just 2 using map Opt. Unfortunately, in a language using standard Hindley/Milner type inference, there is no way to assign a type to the map function that would allow it to be used in this way. Furthermore, even if typing were not an issue, use of the map function would be rather limited unless some additional mechanism was provided to allow the definition to be extended to include new datatypes perhaps distributed across a number of distinct program modules. 1.1 An attempt to define map using type classes The ability to use a single function symbol with an interpretation that depends on the type of its arguments is commonly known as overloading. While some authom dismiss overloading as a purely syntactic convenience, this is certainly not the case in Haekell which has a flexible type system that supports both (parametric) polymorphism and overloading based on a system of t~peclasses [13]. One of the most attractive features of this system is that, although each primitive overloaded operator will require a separate definition for each different argument type, there is no need for these to be in the same module. Type classes in Haskell can be thought of as sets of types. The standard example is the class Eg which includes pre-	algorithm;b+ tree;const (computer programming);emoticon;function overloading;functional programming;haskell;map (higher-order function);monad (functional programming);operator overloading;parametric polymorphism;precondition;principal type;programmer;prototype;theory;type class;type inference;type system	Mark P. Jones	1993		10.1145/165180.165190	polymorphism;parametric polymorphism;higher-order logic;type system;computer science;ad hoc polymorphism;programming language;functional programming;algorithm	PL	-23.763476972828798	26.457730418156974	86846
4214fa9138de0940a2346939495e4a8ef2ba0037	understanding object oriented programming concepts in an advanced programming course	encapsulation;object oriented programming;open university;polymorphism;inheritance	"""Teaching Object Oriented Programming (OOP) is a difficult task, both for teachers who have to find the best way to illustrate the concepts and for students who have to understand them. Although the OOP paradigm and its concepts reflect the """"real world"""", it has been shown that students find hard to understand and internalize the OOP concepts such as encapsulation, inheritance and polymorphism. This paper describes difficulties in understanding OOP in an Advanced Java course given at the Computer Science Department of the Open University of Israel. We present a typical question which focuses on several aspects of OOP. We discuss the students' answers and point out typical hardships in grasping the topic."""	computer science;java;programming paradigm	Tamar Benaya;Ela Zur	2008		10.1007/978-3-540-69924-8_15	simulation;computer science;artificial intelligence;algorithm	DB	-29.167981862831116	24.218682800585178	86860
fc0ec71a26cf958aacd42edb0a9d4c3353c82e79	z specifications: syntactic sugar for prolog			prolog;z notation	Leon Sterling	1994			programming language;syntactic sugar;natural language processing;prolog;computer science;artificial intelligence	PL	-23.954144611755602	21.137726529539528	86895
28fd7c1a03cbaa917c59b5930c95defd7025b1bb	proposal for gks output level 3 segment hierarchy and editing		This proposal deals only with output functions and their side effects on the existing input functions. These functions are collectively referred to as CKS output level 3. This is a poor choice of names as most of the CKS output level 3 functions could exist without CKS output level 2. Fill area sets, and circles and arcs are not included here, although their standardization would be valuable. Functions such as these may be accesscd via the current escopc and generalized &mingprimirive functions. An attempt has been made to keep this proposal focused by not including unrelated issues in with the segment model.	cpu cache;graphical kernel system;side effect (computer science)	Jonathan E. Steinhart	1984	Comput. Graph. Forum	10.1111/j.1467-8659.1984.tb00163.x	computer science;algorithm	Crypto	-26.510451045880814	26.32881924180919	87084
62bf0c46add29077c6eb66c5e5dd67bcfae7811a	from monomorphic to polymorphic well-typings and beyond	strongly connected component;type analysis;optimizing compiler;prolog;polymorhpic types;termination analysis;type definition;type checking;logic programming;well typing;polymorphism;error detection;logic programs;type inference	Type information has many applications; it can e.g. be used in optimized compilation, termination analysis and error detection. However, logic programs are typically untyped. A well-typed program has the property that it behaves identically on well-typed goals with or without type checking. Hence the automatic inference of a well-typing is worthwhile. Existing inferences are either cheap and inaccurate, or accurate and expensive. By giving up the requirement that all calls to a predicate have types that are instances of a unique polymorphic type but instead allowing multiple polymorphic typings for the same predicate, we obtain a novel strongly-connected-component-based analysis that provides a good compromise between accuracy and computational cost.	algebraic data type;algorithm;algorithmic efficiency;artificial intelligence;compiler;component-based software engineering;computation;conway's game of life;declarative programming;depth-first search;electronic signature;error detection and correction;graph theory;international conference on services computing;journal of logical and algebraic methods in programming;lecture notes in computer science;linear algebra;logic programming;mercury;principal type;programming language;prolog;r language;recursion;sas;springer (tank);strongly connected component;termination analysis;type inference;type rule;type signature;type system	Tom Schrijvers;Maurice Bruynooghe;John P. Gallagher	2008		10.1007/978-3-642-00515-2_11	type class;polymorphism;option type;error detection and correction;unit type;computer science;theoretical computer science;type inference;termination analysis;optimizing compiler;programming language;kind;prolog;logic programming;strongly connected component;algorithm	PL	-19.842982780269033	22.473116667725392	87098
18739734d4f69a4a399b7d48a2d242d60206c7aa	jclec: a java framework for evolutionary computation	software tool;object oriented design;evolutionary computation software tools;software systems;knapsack problem;design pattern;graphic user interface;evolutionary algorithm;abstract types;framework;evolutionary computing;java	In this paper we describe JCLEC, a Java software system for the development of evolutionary computation applications. This system has been designed as a framework, applying design patterns to maximize its reusability and adaptability to new paradigms with a minimum of programming effort. JCLEC architecture comprises three main modules: the core contains all abstract type definitions and their implementation; experiments runner is a scripting environment to run algorithms in batch mode; finally, GenLab is a graphical user interface that allows users to configure an algorithm, to execute it interactively and to visualize the results obtained. The use of JCLEC system is illustrated though the analysis of one case study: the resolution of the 0/1 knapsack problem by means of evolutionary algorithms.	abstract type;application programming interface;batch processing;beagle;berg connector;c++;design pattern;differential evolution;dynamic energy budget;eo personal communicator;evolutionary algorithm;evolutionary computation;experiment;gnu compiler for java;graphical user interface;interactivity;knapsack problem;mathematical optimization;modular programming;parallel computing;point of view (computer hardware company);software system;speedup;xml	Sebastián Ventura;Cristóbal Romero;Amelia Zafra;Jose Antonio Delgado;César Hervás-Martínez	2008	Soft Comput.	10.1007/s00500-007-0172-0	evolutionary programming;evolutionary music;interactive evolutionary computation;java evolutionary computation toolkit;computer science;theoretical computer science;software framework;object-oriented design;evolutionary algorithm;graphical user interface;distributed computing;design pattern;programming language;java;knapsack problem;memetic algorithm;software system	SE	-30.387928031211583	28.540364510160167	87195
38fbf3cd23135c8d294ea8879c64436a0255b7c0	writing a compilers compiler in apl	compilers compiler;special capability;semantic function;source code;normal form;attribute grammar;complete description;extended backus;ternary tree;search algorithms;symbol table;data structure;search tree	This paper describes the special capabilities of APL2 for the construction of compilers and translator writing systems (TWS). To that purpose, a TWS has been written in APL2, which takes as input a complete description of a language including an attribute grammar, in extended Backus normal form, and the semantic function associated to each rule of the grammar, written in APL2. The output of the TWS is a compiler, generated directly inside the workspace and ready to compile source code written in the provided language.	apl;attribute grammar;compiler;ibm tivoli workload scheduler;workspace	Enrique Alfonseca	1998		10.1145/327559.327620	data structure;computer science;theoretical computer science;compiler construction;search tree;ternary tree;programming language;attribute grammar;symbol table;algorithm;search algorithm;source code	PL	-26.09563799029978	25.017303658761673	87278
5431f1c6ca9a462245ca77a4e064ee1fa9f7e5b2	sound rules for parallel evaluation of a functional language with callcc	functional language	Observationally equivalent programs are programs which are indistinguishable in aJl contexts, aa far aa their termination property ie concerned. In this paper, we present rules preserving observational equivrdence, for the parallel evaluation of programs using call/cc. These rules idlow the capture of continuations in any applicative context and they prevent from aborting the whole computation when a continuation is applied in the extent of the call/cc by which it was reified. As a consequence, these results prove that one can design a functional language with first-class continuations which haa transparent constructs for parallelism.	applicative programming language;call-with-current-continuation;computation;continuation;functional programming;parallel computing	Luc Moreau;Daniel Ribbens	1993		10.1145/165180.165197	computer science;theoretical computer science;programming language;functional programming;algorithm	PL	-20.9964955559939	24.598480119457204	87367
5ae3302ba9b4919451127973348c712f07eafa1f	a spreadsheet interface for logic programming	programming by example;programming environment;graphic user interface;visual feedback;logic programs;database query;end user programming	We present PERPLEX, a programming environment intended for the end-user. In its design, the concepts of logic programming and spreadsheets are combined. Thus, on the one hand, logic programming becomes an interactive, incremental task where the user gets direct visual feedback, on the other hand, functionality and scope of a conventional spreadsheet program are considerably extended. In order to perform calculations and queries, constraints are imposed on the contents of the spreadsheet cells. New predicates can be defined using a programming-by-example technique: Rules are extracted from the user's solutions for example problems. Thus, concrete intermediate results take over the role of abstract logic variables in the programming process. PERPLEX has been successfully implemented on a Symbolics Lisp Machine.	integrated development environment;lisp machine;logic programming;programming by example;spreadsheet	Michael Spenke;Christian Beilken	1989		10.1145/67449.67466	concurrent constraint logic programming;read–eval–print loop;constraint programming;protocol;declarative programming;programming domain;reactive programming;functional reactive programming;computer science;theoretical computer science;extensible programming;operating system;functional logic programming;graphical user interface;database;programming paradigm;event-driven programming;procedural programming;symbolic programming;inductive programming;fifth-generation programming language;visual programming language;programming language;prolog;logic programming	HCI	-27.236053949653428	24.51563447533884	87470
a7636d3b2a7d7524224b89f6441618026d01b62d	a framework for physiological parameters monitoring	wireless;sensors;real time;circuit design;blood pressure;monitoring;healthcare system;zigbee;hospital care;graphic user interface;cost effectiveness;patient monitoring;remote monitoring;communication technology;power consumption;wireless data;commercial building;product innovation;home automation	Monitoring of health and cost effective management is the only way to ensure economic viability of healthcare systems. Based on data rate, power consumption, security, scalability, price and the benefits of standardization, ZigBee offers interoperability and vendor independence as well. The impact of ZigBee could result in increased product innovation as a result of the industry standardization. It is likely that Zigbee will increasingly play an important role in the future of computer and communication technology [6]. This technology is best designed to suit certain applications like home automation, commercial building automation, smart energy including patient monitoring and care under Patient Health and Hospital Care (PHHC) [7]. Thus, there has been increased interest among research groups in developing wireless recording and monitoring for real-time physiological parameters (e.g. Heart Beat, Temperature, ECG, Blood Pressure etc.) from a patient body in medical environments [7],[4],[5]. Existing wireless data collection systems use standards such as ZigBee (IEEE 802.15.4) or Bluetooth (IEEE 802.15.1). This paper describes a system to remotely monitor a patient's parameters. The data is transferred to a central monitoring station using a wireless ZigBee module for display and storage. A user-friendly graphical user interface is designed for monitoring current and past measurements for all patients being monitored. The major part of the project is dedicated towards processing of sensor data and circuit design. Main task include working with the wireless module and creating a software application for sensor data display[10].	bluetooth;circuit design;data rate units;graphical user interface;home automation;interoperability;real-time clock;scalability;usability	N. Mahajan;K. T. V. Reddy	2011		10.1145/1980022.1980310	embedded system;neurfon;real-time computing;engineering;computer security	Mobile	-32.59734332432894	22.12784298041541	87565
8c308f6a0d8ae355478df9530acbea6a6a076c1a	the paradigm of open c++	lenguaje programacion;language class;langage c;programming language;object oriented programming;c language;object oriented programming languages;natural language;classe langage;langage programmation;language;c;programmation orientee objet;lenguaje formal;formal language;lenguaje c;clase lenguaje;langage formel	Open languages are a new class of formal languages initially defined in [Sunik]. A language of this class combines the grammar of an object-oriented programming language with the universality of a natural language. This work details Open C++ based on the syntax and the conceptual system of C++.	c++;conceptual system;formal language;natural language;programming language;programming paradigm;universality probability	Boris Sunik	2003	SIGPLAN Notices	10.1145/885638.885648	natural language processing;object language;computer science;third-generation programming language;syntax;context-free language;fifth-generation programming language;policy-based design;programming language;object-oriented programming;second-generation programming language;java syntax	PL	-24.8483692092816	22.462102926301384	87602
3a25b318343e064eadd102c5baac50a9f8791bbe	"""tichy's response to r. w. schwanke and g. e. kaiser's """"smarter recompilation"""""""	program transformation;data structure;programming tool;type system	Schwanke and Kaiser's extension of smart recompilation is an intriguing idea. Their mechanism aims at delaying recompilation work by permitting “harmless” compilation inconsistencies to remain after changes. Full consistency can be reestablished at a later time, after the change has been tested in a subpart of the system. If the change was inadequate, then no needless compilation work was performed. This strategy is used frequently in practice, by exploiting loopholes in system generation tools. Schwanke and Kaiser's mechanism is novel in that it makes this practice safe. The compiler is aware of the inconsistencies, and will not overlook dangerous ones. Furthermore, it can help reestablish full consistency once a change is deemed acceptable. Smarter recompilation defines a harmless inconsistency as follows. If a declaration is changed, this action is treated as introducing a new version of the declaration. The coexistence of both the old and new versions in the same configuration is a harmless inconsistency as long as the uses of the two versions do not conflict. The system must be separable into two partitions, one that uses the old version and the other the new one, such that the interface between the two depends on neither. Since the inconsistent declarations do not cross the interface, the two partitions may even communicate with each other. Of course, inconsistencies not captured by the type system cannot be treated in this way. Schwanke and Kaiser's note leaves a few minor questions unanswered. For instance, a new or changed declaration might cause a redeclaration or overloading error that can only be detected by recompilation. Is this potential problem left undetected until full consistency is desired, or is it checked immediately? If a declaration is deleted that is still in use, is the deletion treated as an error or as a delayed deletion that will take effect after the last use disappears? If the old and new versions of a procedure operate on the same data structure, is it always desirable to let both versions coexist, or can the programmer indicate that the old version should be eliminated before the next program execution? Perhaps a future paper about an implementation will clarify these points. Smarter recompilation also opens the door for more powerful programming tools. For example, since the mechanism maintains cross-reference information, a tool like Masterscope [1] could be built relatively easily. The tool would have the advantage that cross-reference information is immediately available once a module has been compiled. Only little additional data would be needed to classify the uses of symbols. The information could also be exploited by a Maintainer's Assistant. This program helps with reestablishing consistency after changes by suggesting corrections of the affected program parts. For example, it could attempt to make call sites of changed procedures consistent with their headers, update operations on changed record fields, or perform some simple program transformations in response to data structure changes.	a new kind of science;coexist (image);compiler;cross-reference;data structure;declaration (computer programming);disk partitioning;function overloading;oldversion.com;program transformation;programmer;programming tool;system generation;type system	Walter F. Tichy	1988	ACM Trans. Program. Lang. Syst.	10.1145/48022.214507	type system;data structure;computer science;artificial intelligence;data mining;programming language;algorithm	PL	-24.590196690070776	28.997875421790198	87631
cb414fcb40332db9aa75a860d94b266b7fb68186	optimal divide and query (extended version)	optimal solution;programming language;software engineering;source code	Algorithmic debugging is a semi-automatic debugging technique that allows the programmer to precisely identify the location of bugs without the need to inspect the source code. The technique has been successfully adapted to all paradigms and mature implementations have been released for languages such as Haskell, Prolog or Java. During three decades, the algorithm introduced by Shapiro and later improved by Hirunkitti has been thought optimal. In this paper we first show that this algorithm is not optimal, and moreover, in some situations it is unable to find all possible solutions, thus it is incomplete. Then, we present a new version of the algorithm that is proven optimal, and we introduce some equations that allow the algorithm to identify all optimal solutions.	algorithm;debugging;haskell;java;programmer;prolog;semiconductor industry;software bug	David Insa;Josep Silva	2011	CoRR		computer science;engineering;theoretical computer science;software engineering;algorithmic program debugging;programming language;algorithm;source code	PL	-20.135338858323333	22.567678167959027	87809
db0f6ba50ef2059c6fcb87800bc42d2604ce320b	abstract syntax notation x (asn.x) representation of encoding instructions for the generic string encoding rules (gser)	extensible markup language;abstract syntax	Abstract Syntax Notation X (ASN.X) is an Extensible Markup Language#N#(XML) representation for Abstract Syntax Notation One (ASN.1)#N#specifications. This document specifies the ASN.X representation of#N#encoding instructions for the Generic String Encoding Rules (GSER).#N#This memo defines an Experimental Protocol for the Internet community.		Steven Legg	2007	RFC	10.17487/RFC4913	natural language processing;abstract syntax;computer science;theoretical computer science;programming language;abstract syntax tree	NLP	-27.625660144336145	19.96184988638082	87835
dadedbdfda9a2786d9cef20be99f98d12a83c58b	managing the production and evolution of e-learning tools with attribute grammars	e learning tools;grammar;attribute grammars;programming language semantics;electronic learning;dsl;programming language;tool support;xml based dsl;xml processing;semantics;programming language community semantic;courseware system;attribute grammar;educational domain;production management;educational aids;tutorials;syntactics;xml processing production management e learning tools attribute grammars domain specific languages educational domain programming language community semantic declarative specification method courseware system xml based dsl xlop language oriented processing;declarative specification method;intelligent tutoring systems;xml;domain specific language;production;xml attribute grammars courseware educational aids intelligent tutoring systems programming language semantics;tutoring systems e learning tools attribute grammars domain specific languages xml processing;grammar semantics dsl electronic learning production syntactics tutorials;language oriented processing;courseware;xlop;tutoring system;domain specific languages;tutoring systems;semantics of programming languages	Many e-learning tools are based on domain-specific languages (DSLs) targeted to the educational domain. Thus, methods and techniques from the programming language community can help in developing these tools. In this paper, we show how attribute grammars, a well-known declarative specification method for the syntax and semantics of programming languages, can facilitate the production and subsequent evolution of e-learning tools. We also describe how we produced and extended, a courseware system supporting an XML-based DSL, by using XLOP (XML Language-Oriented Processing), a meta-tool supporting attribute grammars for the development of XML processing applications.	attribute grammar;central processing unit;digital subscriber line;domain-specific language;evolution;graphical user interface;information management system (ims);markup language;programming language;qti;rational clearcase ucm;semantics (computer science);xml	Bryan Temprado-Battad;Antonio Sarasa Cabezuelo;José Luis Sierra	2010	2010 10th IEEE International Conference on Advanced Learning Technologies	10.1109/ICALT.2010.124	natural language processing;l-attributed grammar;computer science;domain-specific language;syntax;database;semantics;xml signature;programming language	DB	-27.503886716582585	22.060503744414092	87897
eba6376498a51473befbe9e2261dde50721c9094	an address mapping approach for test data generation of dynamic linked structures	software;software testing;linear array;test data generation;linked structure;dynamic linking	Software testing is an important technique to assure the correctness of the software. One of the essential prerequisite tasks of software testing is test data generation. This paper proposes an approach to generate test data specifically for dynamic pointer structures. In our context, a pointer is considered and handled as a location in memory, represented by a dynamic linear array that expands and shrinks during execution. As such, pointer test data can be directly generated from this linear array. The proposed technique can also support any dynamic structures, as well as homogeneous and heterogeneous recursive structures. q 2004 Elsevier B.V. All rights reserved.	address space;algorithm;charge-coupled device;correctness (computer science);critical section;declarative programming;doubly linked list;function pointer;memory leak;memory management unit;nonlinear system;overhead (computing);pointer (computer programming);programming paradigm;real-time cmix;recursion;rendering (computer graphics);software testing;stored-program computer;test data generation;tree traversal	Sittisak Sai-ngern;Chidchanok Lursinsap;Peraphon Sophatsathit	2005	Information & Software Technology	10.1016/j.infsof.2004.08.004	test data generation;real-time computing;computer science;engineering;software engineering;database;dynamic testing;software testing;programming language	SE	-20.916175033425375	31.974757992990455	87952
78c32811815cf6835fc3d090a0151d36335b189e	a middleware: python plugin transform on different gis platforms	middleware geographic information systems;transforms;geographic information system gis middleware python plugin gis platforms universal property;mapgis middleware python plugin application transform gis platform kernel gis plugin application qgis	GIS plugins expand existing GIS function with obvious pertinence. However, they are disappointing and difficult to be used for cross-platform. Based on the plugin application depending on specific GIS platforms, this paper proposes a middleware to achieve the transform of Python plugin applications on different GIS platforms. The middleware separates out the GIS platforms kernel and GIS plugin application. The kernel layers of different GIS platforms provide necessary external interface to the middleware. With docking to middleware, the plugin application can be transplanted to another GIS platform. As a GIS overpass, the middleware can help the plugin application ignore the details of the GIS platform kernel. The middleware is designed to build Python plugin application which avoids many code alterations and can be used in different GIS platforms. Lastly, a project that uses a Python plugin application transform from QGIS to MAPGIS is demonstrated. Experiments show that the transform method is feasible and promote universal property of plugin application.	docking (molecular);geographic information system;kernel (operating system);middleware;plug-in (computing);python;qgis;relevance	Zhuoran Pan;Xiaohong Yang;Zhong Xie	2015	2015 23rd International Conference on Geoinformatics	10.1109/GEOINFORMATICS.2015.7378577	computer science;operating system;database;world wide web	SE	-29.319192368490544	28.59076309149663	88009
6b7d1f67f9b3c600e530e00f0e255229d93dd881	an introduction to logic programming through prolog	logic programs	Introduction. Programming with Relations. Recursive Structures. The Meaning of Logic Programs. Inference Rules. Unification and Resolution, SLD -Resolution. SLD-Resolution and Answer Substitutions. Negation as Failure. Searching Problems. Parsing Evaluating and Simplifying Expressions. Hardware simulations. Program Transformation. About PicoProlog. Implementing Depth-First Search, Representing Terms and Substitutions. Implementation Notes. Interpreter Optimizations. In Conclusion. Bibliography, Index.	blu-ray;deluxe paint;end-of-file;human body weight;limewire;logic programming;operating system;printing;prolog;symbolic computation;waits	J. Michael Spivey	1996			horn clause;stable model semantics;computer science;negation as failure;sld resolution;theoretical computer science;programming language;prolog;logic programming;algorithm	PL	-21.355837195090746	19.361752447527223	88450
3cb9bba2dd901c9920ba44d66c5a5b92fa4f2bbe	a pure meta-interpreter for flat ghc, a concurrent constraint language	virtual machine;concurrent constraint programming;concurrent language;operational semantics;concurrent constraint;partial evaluation;code mobility;logic programs	This paper discusses the construction of a meta-interpreter of Flat GHC, one of the simplest and earliest concurrent constraint languages. Meta-interpretation has a long history in logic programming, and has been applied extensively to building programming systems, adding functionalities, modifying operational semantics and evaluation strategies, and so on. Our objective, in contrast, is to design the pair of (i) a representation of programs suitable for code mobility and (ii) a pure interpreter (or virtual machine) of the represented code, bearing networked applications of concurrent constraint programming in mind. This is more challenging than it might seem; indeed, meta-interpreters of many programming languages achieved their objectives by adding small primitives into the languages and exploiting their functionalities. A metainterpreter in a pure, simple concurrent language is useful because it is fully amenable to theoretical support including partial evaluation. After a number of trials and errors, we have arrived at treecode, a groundterm representation of Flat GHC programs that can be easily interpreted, transmitted over the network, and converted back to the original syntax. The paper describes how the interpreter works, where the subtleties lie, and what its design implies. It also describes how the interpreter, given the treecode of a program, is partially evaluated to the original program by the unfold/fold transformation system for Flat GHC.	canonical account;code mobility;computation;concurrency (computer science);concurrent computing;concurrent constraint logic programming;constraint programming;control flow;decision tree;disjunctive normal form;distributed computing;heap (data structure);high- and low-level;interpreter (computing);language primitive;nondeterministic algorithm;operational semantics;optimizing compiler;parallel computing;partial evaluation;program analysis;programming language;reduction (complexity);reification (computer science);run-time type information;semantics (computer science);source transformation;synchronization (computer science);test stub;the glorious glasgow haskell compilation system;virtual machine;warren abstract machine	Kazunori Ueda	2002		10.1007/3-540-45628-7_7	constraint logic programming;concurrent constraint logic programming;constraint programming;constraint satisfaction;computer science;theoretical computer science;programming language;algorithm	PL	-20.723085756316728	23.677148196635816	88456
1ff41ca461f9d9c7d93d808cae55979a4918d1ae	constraint logic programming	declarative programming;constraint logic programs;logic programs;open source	Constraint Logic Programming (CLP) is one of the most successful branches of Logic Programming; it attracts the interest of theoreticians and practitioners, and it is currently used in many commercial applications. Since the original proposal, it has developed enormously: many languages and systems are now available either as open source programs or as commercial systems. Also, CLP has been one of the technologies able to recruit researchers from other communities to the declarative programming cause. Current CLP engines include technologies and results developed in other communities, which themselves discovered logic as an invaluable tool to model and solve real-life problems.	constraint logic programming;declarative programming;open-source software;real life	Marco Gavanelli;Francesca Rossi	2010		10.1007/978-3-642-14309-0_4	concurrent constraint logic programming;constraint programming;declarative programming;constraint satisfaction;computer science;theoretical computer science;functional logic programming;programming paradigm;procedural programming;inductive programming;fifth-generation programming language;programming language;prolog;logic programming;algorithm	PL	-22.222268449060774	19.36526612677452	88533
ed0404f3dcc8306495b7ec2428d32db050761db4	map: a pascal macro preprocessor for large program development	software tool;preprocessor;macro;pascal;program development	The programming language Pascal waH originally designed for tcaching introductory programming, currently, however, production systems use it as the primary implementation language. This paper describes extensions of Pascal intended to aid the large program developer. The extensions are implemented in a macro preprocessor MAP, which supports constant expression evaluation, source file inclusion, conditional compilation, and macro substitution. While each of these features can be used independently, they are all implemented with a simple, uniform syntax. Furthermore, in keeping wi th the spirit of Pascal, an attempt has been made to make the facilities straightforward and simple. ~he design and implementation details arc discussed.	compiler;conditional compilation;map;object language;pascal;preprocessor;programming language	Douglas Comer	1979	Softw., Pract. Exper.	10.1002/spe.4380090305	pascal;computer science;translation unit;macro;modular programming;programming language;preprocessor;algorithm	PL	-27.441965721919185	26.014609786202147	88605
1c6e9f70425569181af1e813ea3049df74f8630f	using x with the ada mind-set	ada mind-set	L INTRODUCTION As the X Window Systemm has emerged as an industry standard for implementing graphical user interfaces (GUIS)on workstadondisplays,softwareengineershave been challenged by the inherent complexity of X when accessing its rich set of capabilities. Much effort has been invested encapsulating the functionality of X for simplifkd GUI development. What has evolved from such efforts is the concept of the graphical object or widget along with associated widget toolkits, utility pdcagea, and GUI builders to create and handle these widgets. Many of these “widget systems” have proven to be extremely useful tools for easily accessing the X Window System functionality and for m.ducing the work of developing GUIS.	ada;concurrency (computer science);exception handling;extensibility;graphical user interface builder;list of toolkits;requirement;scalability;scott continuity;software portability;software widget;technical standard;widget toolkit;x window system	Mike Downs;Judy Duffy;Karen Mackey;Luke Teyssier;Chris Tonas	1993		10.1145/170657.170662	programming language;computer science	AI	-30.900309239106623	28.34400852198431	88677
250b6e9a4f167e7e9e7430d9a22ee70c36e41f91	aplgol, an experimental structured programming language	programming language;program design	An experimental programming language called APLGOL adds structured programming facilities to the existing framework of APL. The conventional semantics of APL is unaltered and only minor changes are incorporated in the syntax. The advantages of the proposed interstatement structuring and control are outlined. Programs designed and written using “structured” programming techniques have been demonstrated to be more readily produced, more reliable, and more easily maintained than unstructured programs [ 1 1. These techniques essentially involve arranging the application into principal components, which in turn, are further organized to produce a set of highly structured procedures. For this purpose, key programming language constructs have been used in conjunction with many languages to highlight interstatement structuring and control. Since structured programming techniques have been successful when applied to programming in other languages, they should be equally advantageous for APL programming efforts. In APL the compact and concise operators extend to vector or array operands, and single expressions often can subsume the equivalent of several statements in a language such as ALGOL or PLh. However, even in spite of the famous APL “one-liners”, many APL programs do require quite a few statements and frequently utilize rather complicated control flow. If this were highlighted by structured programming language constructs, the resulting programs should prove easier to write and debug, and more importantly, easier for others to read and understand. Consequently, the APLGOL language [ 2 ] , based on a notation of Abrams [3], is an experiment to add structured programming language constructs to the basic APL framework. In adding the structured statements, a deliberate effort has been made to augment APL in the areas where these constructs are absent, and to avoid establishing other constructs, e.g., a new rule for name scope, that would tend to compete with existing APL facilities. Close attention has been paid to the relation between the new APLGOL facilities and existing APL operations. In this sense, APL was considered the target machine language to which APLGOL source programs could be compiled readily without materially affecting the speed or size of the object program or the compiler, or otherwise distorting the APL system. APLGOL semantics Before any new features were incorporated in APLGOL, it was decided first that all the semantic functions of APL should appear in APLGOL without change, since removing or altering any of them would be a step in the wrong direction. To achieve this (and still attain the goal of furnishing structured programming functions), the APL procedure format with its procedural header and numbered statements was abandoned in favor of a free form in which statements could span lines in an arbitrary manner. Thus, a new PROCEDURE statement having a different syntax but identical semantics, and a new EiVD PROCEDURE statement were incorporated to contain the procedure body. Also, a slight change was introduced in the original APL statement syntax to permit a semicolon not enclosed in subscript brackets to terminate a statement. The semicolon formerly used to catenate items for printing was replaced with the union symbol, “u”. The syntax for comments was changed to require the comment delimiter to follow as well as to precede the comment. In 69	algol;apl;comment (computer programming);compiler;control flow;debugging;delimiter;distortion;executable;machine code;non-structured programming;operand;printing;programming language;structured programming;terminate (software)	Robert A. Kelley	1973	IBM Journal of Research and Development	10.1147/rd.171.0069	natural language processing;structured english;first-generation programming language;natural language programming;very high-level programming language;language primitive;programming domain;reactive programming;data control language;computer science;extensible programming;functional logic programming;jackson structured programming;program design language;programming paradigm;procedural programming;symbolic programming;inductive programming;fifth-generation programming language;programming language theory;programming language;programming language specification;high-level programming language;algorithm	PL	-26.639314089101	25.53827537885242	88746
926cafb776d769890c0d4685d49831c322e11bbd	poor man's genericity for java	poor man	"""A number of proposals have been made as to how Java can be changed to support parameterized types. We present a new proposal that does not try to provide more powerful constructs or cleaner semantics, but instead minimizes the changes that need to be made to existing Java compilers. In particular, we found that changing only one method in Sun's Java compiler already results in a reasonable implementation of parameterized types, which we call """"Poor Man's Genericity"""" (PMG). We have implemented our solution based on simple byte{code transformations both at compile{time and at load{time. The paper explains how our solution works, and compares it to other proposals. We also describe how the drawbacks of our approach can be overcome by making additional, but minimal changes to an existing Java compiler."""	generic programming;java compiler;parametric polymorphism	Boris Bokowski;Markus Dahm	1998		10.1007/3-540-49255-0_182	real-time computing;programming language;algorithm	PL	-25.182814994165344	28.886653314839815	88824
37f0bbd084b7ad6e496bd6e15fdbcc8babccc9c7	a visual system for compositional relational programming	information systems;computer science	Combilog is a compositional relational programming language that allows writing relational logic programs by functionally composing relational predicates. Higraphs, a diagram formalism is consulted to simplify some of the textual complexity of compositional relational programming to achieve a visual system that can represent these declarative meta-programs, with the final intention to design an intuitive and visually assisted complete development practice. As a proof of concept, an implementation of a two-way parser/visualizer is presented.	diagram;logic programming;programming language;relational algebra;semantics (computer science)	Görkem Paçaci;Andreas Hamfelt	2013		10.3233/978-1-61499-361-2-221	programming language;computer science;sql;discrete mathematics;diagram;domain relational calculus;proof of concept;theoretical computer science;parsing;statistical relational learning;relational calculus;relational database	PL	-26.847188107549684	18.49352762696274	89136
c5857e4a5dab1fcbe64b1223d116abc557993c8c	the art of the meta-aspect protocol	debugging;open implementation;aspect interactions;aspect oriented programming;meta object protocols	Alternative semantics for aspect-oriented abstractions can be defined by language designers using extensible aspect compiler frameworks. However, application developers are prevented from tailoring the language semantics in an application-specific manner. To address this problem, we propose an architecture for aspect-oriented languages with an explicit meta-interface to language semantics. We demonstrate the benefits of such an architecture by presenting several scenarios in which aspect-oriented programs use the meta-interface of the language to tailor its semantics to a particular application execution context.	aspect-oriented software development;compiler	Tom Dinkelaker;Mira Mezini;Christoph Bockisch	2009		10.1145/1509239.1509248	aspect-oriented programming;computer science;theoretical computer science;formal semantics;database;low-level programming language;programming language;well-founded semantics;debugging;operational semantics;denotational semantics;semantics;computational semantics	PL	-27.215693819803565	28.923659034596998	89146
0ecb53a688a4be673469bfe8e4d1296d094ff268	practical compiling with pascal-s by michael rees and dave robson, addison-wesley, wokingham, uk, 1988, 307 pages (incl. index (£16.95);programming language translation: a practical approach by patrick d. terry, addison-wesley, wokingham, uk, 1986, 443 pages (incl. index) (£15.95);syntax analysis an	software tool;programming language;indexation		gareth rees (software developer);intel turbo memory;michael rees;parsing;pascal	Iain D. Craig	1989	Robotica	10.1017/S026357470000566X	computer science;artificial intelligence	NLP	-27.80011708776549	22.679136534100586	89326
1cb3a5ab80477e3f14e29a23b655a6acfb94957d	a comparative study of language support for generic programming	programming language;generic programming;generics;c;power generation;eiffel;polymorphism;java	Many modern programming languages support basic generic programming, sufficient to implement type-safe polymorphic containers. Some languages have moved beyond this basic support to a broader, more powerful interpretation of generic programming, and their extensions have proven valuable in practice. This paper reports on a comprehensive comparison of generics in six programming languages: C++, Standard ML, Haskell, Eiffel, Java (with its proposed generics extension), and Generic C. By implementing a substantial example in each of these languages, we identify eight language features that support this broader view of generic programming. We find these features are necessary to avoid awkward designs, poor maintainability, unnecessary run-time checks, and painfully verbose code. As languages increasingly support generics, it is important that language designers understand the features necessary to provide powerful generics and that their absence causes serious difficulties for programmers.	c++;eiffel;generic programming;haskell;java;programmer;programming language;standard ml;type safety	Ronald Garcia;Jaakko Järvi;Andrew Lumsdaine;Jeremy G. Siek;Jeremiah Willcock	2003		10.1145/949305.949317	fourth-generation programming language;first-generation programming language;protocol;declarative programming;very high-level programming language;programming domain;type safety;computer science;programming language generations;third-generation programming language;functional logic programming;computer programming;programming paradigm;fifth-generation programming language;programming language theory;programming language;generic programming;second-generation programming language;high-level programming language;comparison of multi-paradigm programming languages;algorithm;generics in java	PL	-25.30476257781014	27.466185173727446	89342
0d6ae6129b1348a62302f32bc145ae520d3618ed	introduction to type theory	programming language;proof theory;functional programming;theorem proving;type theory;article in monograph or in proceedings;type system	These notes comprise the lecture “Introduction to Type Theory” that I gave at the Alpha Lernet Summer School in Piriapolis, Uruguay in February 2008. The lecture was meant as an introduction to typed λ-calculus for PhD. students that have some (but possibly not much) familiarity with logic or functional programming. The lecture consisted of 5 hours of lecturing, using a beamer presentation, the slides of which can be found at my homepage. I also handed out exercises, which are now integrated into these lecture notes. In the lecture, I attempted to give an introductory overview of type theory. The problem is: there are so many type systems and so many ways of defining them. Type systems are used in programming (languages) for various purposes: to be able to find simple mistakes (e.g. caused by typing mismatches) at compile time; to generate information about data to be used at runtime, . . . . But type systems are also used in theorem proving, in studying the the foundations of mathematics, in proof theory and in language theory. In the lecture I have focussed on the use of type theory for compile-time checking of functional programs and on the use of types in proof assistants (theorem provers). The latter combines the use of types in the foundations of mathematics and proof theory. These topics may seem remote, but as a matter of fact they are not, because they join in the central theme of these lectures:	automated theorem proving;compile time;compiler;functional programming;lambda calculus;proof assistant;run time (program lifecycle phase);type system;type theory	Herman Geuvers	2008		10.1007/978-3-642-03153-3_1	computer science;mathematics;type theory;algorithm	PL	-20.746356612855017	19.967703543961907	89551
2eeb94cd9f9352aac69b84a40b3789e7f3510242	vmtl-a modular termination laboratory	term rewrite system;termination analysis;scientific communication	The automated analysis of termination of term rewriting systems (TRSs) has drawn a lot of attention in the scientific community during the last decades and many different methods and approaches have been developed for this purpose. We present VMTL (Vienna Modular Termination Laboratory), a tool implementing some of the most recent and powerful algorithms for termination analysis of TRSs, while providing an open interface that allows users to easily plug in new algorithms in a modular fashion according to the widely adopted dependency pair framework. Apart from modular extensibility, VMTL focuses on analyzing the termination behaviour of conditional term rewriting systems (CTRSs). Using one of the latest transformational techniques, the resulting restricted termination problems (for unconditional context-sensitive TRSs) are processed with dedicated algorithms.	abstract rewriting system;algorithm;context-sensitive grammar;extensibility;open interface;termination analysis	Felix Schernhammer;Bernhard Gramlich	2009		10.1007/978-3-642-02348-4_20	computer science;theoretical computer science;termination analysis;programming language;algorithm	PL	-32.616161308477366	29.230210324174013	89712
bfcfc428f37e36a07df8c55fc3b8451d7e47bbb0	javalog: a framework-based integration of java and prolog for agent-oriented programming	object oriented language;multi agent system;multi paradigm languages;temporal logic;object oriented framework;object oriented programming;object oriented frameworks;expressive power;intelligent agents;logic programming;agent oriented programming;intelligent agent;logic programs	Intelligent agent development has imposed new challenges on the necessary language support. Object-oriented languages have been proposed as an appropriate tool, although logic-oriented languages are more adequate for managing mental attitudes. Multi-paradigm languages supporting encapsulation of actions, hiding of private knowledge and 9exible manipulation of knowledge are, certainly, a good alternative for programming agents. However, a unique language to support 9exible and e<cient development of multi-agent systems confronts with the tradeo=s imposed by expressive power, e<ciency and support technology. An alternative to conciliate these tradeo=s is not to think about a single language but an incrementally compatible family of agent-oriented multi-paradigm languages. In this work we present an approach based on object-oriented framework technology for integrating object and logic paradigms in such a way that new language features can be incrementally added to the core language. This core language is based on logic modules integrated as object abstractions in the object paradigm. JavaLog is a materialization of this framework integrating Java and Prolog. This core was extended to provide multi-threading support, mobility and temporal-logic operators to Prolog. MoviLog, the mobile part of the family provides a novel mobility mechanism, reactive mobility by failure, which enables virtual Prolog databases distributed across Web sites. c © 2004 Elsevier Ltd. All rights reserved.	agent-oriented programming;algorithm;automated planning and scheduling;causal filter;database;emoticon;encapsulation (networking);expressive power (computer science);intelligent agent;java;logic programming;multi-agent system;point of view (computer hardware company);programming paradigm;prolog;requirement;software requirements;temporal logic;thread (computing)	Analía Amandi;Marcelo R. Campo;Alejandro Zunino	2005	Computer Languages, Systems & Structures	10.1016/j.cl.2004.03.001	fourth-generation programming language;first-generation programming language;method;declarative programming;object-based language;object language;computer science;object;theoretical computer science;third-generation programming language;functional logic programming;programming paradigm;ontology language;low-level programming language;fifth-generation programming language;programming language;object-oriented programming;prolog;logic programming;second-generation programming language;intelligent agent;algorithm	PL	-26.12981618895192	25.620042042656998	89946
7b2d66632333d64b3382402891e551bdcb0b719e	a smart precision-agriculture platform for linear irrigation systems		A smart platform is presented that manages the components of an automatic precision irrigation system. The platform has a distributed architecture that includes a decision support system(Irriframe), a server node, a mobile application for user interaction, and embedded IoT devices that operate linear irrigation machines. The decision support system is queried by the server and it computes an irrigation map, i.e., the amount of water to be supplied in each cell of the field by integrating geographic, meteorological and soil data, as well as the vegetation map obtained from an aerial survey and the technical specifications of the irrigation machine. The mobile application is used by the farmer to register user data on the decision support system, to request an irrigation plan from the server and to control the irrigation process with real-time monitoring. Preliminary experiments were conducted in tomato fields to test the main components of the system.		Jacopo Aleotti;Michele Amoretti;Alessandro Nicoli;Stefano Caselli	2018	2018 26th International Conference on Software, Telecommunications and Computer Networks (SoftCOM)	10.23919/SOFTCOM.2018.8555841	real-time computing;computer science;computer network;aerial survey;decision support system;irrigation;node (networking);precision agriculture;internet of things	Embedded	-30.883774340171303	18.47052189319124	90363
0b781122718428d27df3c1cad6945293dc3564ec	gdsl: a generic decoder specification language for interpreting machine language	program semantics;instruction decoder;binary analysis;executable analysis	The analysis of executable code requires the reconstruction of instructions from a sequence of bytes (or words) and a specification of their semantics. Most front-ends addressing this problem only support a single architecture, are bound to a specific programming language, or are hard to maintain. In this work, we present a domain specific language (DSL) called GDSL (Generic Decoder Specification Language) for specifying maintainable instruction decoders and the translation of instructions to a semantics. We motivate its design by illustrating its use for the Intel x86 platform. A compiler is presented that generates C code that rivals hand-crafted decoder implementations.	byte;compiler;digital subscriber line;domain-specific language;executable;machine code;programming language;specification language;x86	Alexander Sepp;Julian J Kranz;Axel Simon	2012	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2012.11.006	real-time computing;specification language;computer science;theoretical computer science;low-level programming language;programming language;executable	PL	-22.52373947387376	27.725045260896625	90402
c7513ebd5a282d6b062e379f2997a1e4bd98b0df	smalltalk as a programming language for robotics	debugging;software testing;computer languages;programming environments;programming language;graphical interface;application software;object oriented programming;computer languages programming environments robot kinematics object oriented modeling debugging application software program processors proportional control object oriented programming software testing;language evolution;object oriented;smalltalk;proportional control;program processors;object oriented modeling;robot kinematics	Programming languages for robotics applications are continually being developed and extended as the applications become more sophisticated. Language evolution is proceeding along two directions: (1) providing more and better facilities for task-level as opposed to robot-level programming and (2) providing better facilities for simulation, graphics and symbolic manipulation. The trend makes it clear that the full capabilities of a general purpose programming language are needed. Instead of developing a new language from the ground up, it is easier and more productive to take an existing language with all the requisite general purpose facilities and specialize it for robotics. Because of its symbolic processing facilities, its object-oriented nature, its usefulness as a simulation language, and its sophisticated graphical interface, Smalltalk is an ideal candidate for specialization. We discuss in more detail why this is the case and we show how a programming language that approaches the power of AL can be imbedded in Smalltalk within 2-4 person-months of effort.	apl;programming language;robotics;smalltalk	Wilf R. LaLonde;Dave A. Thomas;Kent Johnson	1987		10.1109/ROBOT.1987.1087894	fourth-generation programming language;natural language programming;real-time computing;very high-level programming language;first-class citizen;language primitive;programming domain;computer science;programming language implementation;theoretical computer science;third-generation programming language;programming paradigm;symbolic programming;low-level programming language;fifth-generation programming language;programming language;object-oriented programming;programming language specification;high-level programming language	Robotics	-27.742376733026827	24.26145084401369	90415
f89d680f6e6db2f73277f102858f780d5ee2f334	instruction set simulator generation using harmless, a new hardware architecture description language	hardware architecture	Instruction set simulators are commonly used in embedded system development processes for early functional validation of code and exploration of new instruction set design. Such a simulator can be either hand-written or generated automatically, based on a Hardware Architecture Description Language. Automatically generated simulators are more maintainable and are faster to develop, but they also generally suffer from low performances in simulation speed and a lack of expressivity in the description. This paper introduces HARMLESS, a new language to automatically generate instruction set simulators. It differs from other languages in many ways: it resolves most expressivity issues and naturally offers a flexible description by explicitly splitting the syntax (mnemonic), format (binary code) and behavior descriptions. Thus, it allows an incremental description, starting for example by the disassembler (requiring format and syntax descriptions). When the first two descriptions are validated, the behavior description is added to obtain the simulator. Some results are also presented on the simulator build process, especially on the decoder generation. An instruction cache is also introduced to speed up simulation in the same order of magnitude as hand-written simulators. Some experimental results are eventually presented.		Rola Kassem;Mikaël Briday;Jean-Luc Béchennec;Yvon Trinquet;Guillaume Savaton	2009		10.1145/1537614.1537646	computer architecture;computer architecture simulator;simulation;computer science;theoretical computer science;operating system;instruction set;hardware architecture;programming language	EDA	-22.722359025060495	27.58940692183543	90617
01be952906f0dd4319027860a2345d2b0ab291ca	invited talk: program generators and the tools to make them	safety guarantees;software libraries;java programming;program generators;application server;program generation;partial evaluation;domain specific language;data flow analysis;meta programming;type inference;domain specificity;generic programming	"""Program generation is among the most promising techniques in the effort to increase the automation of programming tasks. In this talk, we discuss the potential impact and research value of program generation, we give examples of our research in the area, and we outline a future work direction that we consider most interesting.Specifically, we first discuss why program generators have significant applied potential. We believe that program generators can be made easy-to-implement so that they are competitive with traditional software libraries in many software domains. Compared to a common library, a generator implementing a domain-specific language can offer more concise syntax, better static error checking, and better performance through cross-operation optimizations.Despite the significant applied value of generators, however, we argue that meta-programming tools (i.e., language tools for writing program generators) may be of greater value as a research topic. The reason has to do with the domain-specificity of generators. The value of a program generator is often tied so closely to a software domain that there is little general and reusable knowledge to transmit to other generator researchers. We discuss meta-programming tools as an area with both interesting conceptual problems and great value. A good meta-programming infrastructure can simplify the creation of generators to make them an effective solution for many more domains.We illustrate our views on generators and meta-programming tools with two artifacts from our latest work: the Meta-AspectJ meta-programming language [6] and the GOTECH generator [5]. Meta-AspectJ enables generating Java and AspectJ programs using code templates, i.e., quote and unquote operators. Meta-AspectJ has two interesting elements. First, we believe that using the AspectJ language as a back-end simplies the task of writing a generator. The GOTECH generator uses this technique to adapt a Java program for server side execution in a J2EE application server. Second, Meta-AspectJ is a technically mature meta-programming tool|in many respects the most advanced meta-programming tool for Java. For instance, Meta-AspectJ reduces the need to deal with low level syntactic types for quoted entities (e.g., """"expression"""", """"statement"""", """"identifier"""", etc.) through type inference and a context-sensitive parsing algorithm.Finally, we examine the problem of statically determining the safety of a generator and present its intricacies. We limit our focus to one particular kind of guarantee for generated code: ensuring that the generated program is free of compile-time errors, such as type errors, references to undened variables, etc. We argue that it is the responsibility of a good meta-programming tool to ensure that the generators written in it will always produce legal programs. Nevertheless, if we do not severely limit the generator, the problem becomes one of arbitrary control- and data flow analysis. We discuss why the limitations of current meta-programming tools [1, 4] that offer safety guarantees are too strict and present possible avenues for future research.For further reading, a full paper accompanying the current talk can be found in the PEPM'04 proceedings. The reader may also want to consult one of the good surveys on program generation, examining the topic either from an applied perspective [3] or from a partial evaluation perspective [2]."""	algorithm;application server;aspectj;compile time;compiler;context-sensitive grammar;data-flow analysis;dataflow;domain-specific language;entity;identifier;java platform, enterprise edition;library (computing);metaprogramming;parsing;partial evaluation;programming language;programming tool;sensitivity and specificity;server (computing);server-side;tree-meta;type inference	Yannis Smaragdakis	2004		10.1145/1013963.1013968	metaprogramming;computer science;domain-specific language;theoretical computer science;type inference;data-flow analysis;programming language;generic programming;partial evaluation;algorithm;application server	PL	-26.535212703277935	28.03008633289979	90712
aa7327d276734e5df62021342583049a0b9279ae	the elaboration order problem of ada	developpement logiciel;lenguaje programacion;context clauses;compilateur;programming language;order;ada;structure programme;sistema informatico;computer system;ingenieria logiciel;ada 9x;compiler;software engineering;modulo programa;elaboration;estructura programa;desarrollo logicial;software development;genie logiciel;ordre;langage programmation;systeme informatique;fiabilite logiciel;elaboracion;fiabilidad logicial;static analysis;ada language;module programme;program structure;software reliability;compilador;orden;program module	A serious problem in huge Ada programs is finding a satisfactory order of elaboration for all of the program’s compilation units. Some elaboration sequence must he selected but access to a program entity before its elaboration will rake the exception (PROGRAM_ERROR) shortly after execution of the program begins. The rules of the 1983 Ada standard 1 fail to ensure that validated compilers produce a satisfactory elaboration order, or report an error if static analysis shows that such an order may not exist. The problem can make programs non-portable and sensitive to modifications. This paper presents an algorithm ada_elab which determines an order of elaboration, free of PROGRAM_ERROR, if link-time, static analysis can determine that there is one. Examples are used to illustrate the occurrence of elaboration errors and the inadequacy of pragma ELABORATE to resolve the problem. The paper closes with descriptions of elaboration and elaboration-related problems that cannot be handled by ada_elab, access before initialization and coding practices that should avoid elaboration and initialization errors. While Ada is the subject of this paper, the problem being studied could occur in any language that permits a complex initialization of individual program modules prior to the execution of the main program module.	ada;algorithm;compiler;directive (programming);entry point;linker (computing);rake;robert;software engineering;static program analysis	Leslie C. Lander;Sandeep Mitra;Nitin Singhvi;Thomas F. Piatkowski	1992	Softw., Pract. Exper.	10.1002/spe.4380220504	compiler;ada;order;computer science;artificial intelligence;software development;programming language;static analysis;algorithm;software quality	SE	-23.571530715775292	29.229743047847176	90733
a28d20d66274e8265d1a39287fb75511b224f3c1	design aspects of the redwood programming environment	index terms — development environment;snippet;visual programming.;redwood;development environment;indexing terms;visual programming	Redwood is a development environment that supports drag-and-drop manipulation of programming constructs and visual representation of program structure. Redwood’s architecture and functionality are based on the concept of snippet, defined loosely as a program component that encapsulates both a coding solution and its visual presentation. In addition, snippets support creation of unrestricted code libraries, thus fostering open-source development. This paper presents the motivation for Redwood, briefly overviews its functionality and mode of operation, and then focuses on the concepts underlying its design. Two essential parts of this design are the Snippet Display syntax (SDS) and the Snipplet Language (SL), created by the authors and presented in the paper. Implementation details, examples of use, and several directions of future work are also included in the paper. Index Terms — Development environment, Redwood, snippet, visual programming.	block cipher mode of operation;drag and drop;library (computing);open-source software;sl (complexity);structured programming;visual programming language	Brian T. Westphal;Frederick C. Harris;Sergiu M. Dascalu	2005			computer science;inductive programming;syntax;coding (social sciences);snippet;reactive programming;theoretical computer science;architecture;visual programming language;drag and drop	HCI	-30.08963335293829	23.90764630483013	90767
28fd36a24a38de08532efe7594b2d29f4035fd14	the implications of method placement on api learnability	libraries;frameworks;user study;user studies;application program interface;multiple objectives;object oriented;apis;usability;documentation	"""To better understand what makes Application Programming Interfaces (APIs) hard to use and how to improve them, recent research has begun studying programmers' strategies and use of APIs. It was found that method placement --- on which class or classes a method is placed --- can have large usability impact in object-oriented APIs. This was because programmers often start their exploration of an API from one """"main"""" object, and were slower finding other objects that were not referenced in the methods of the main object. For example, while mailServer.send(mailMessage) might make sense, if programmers often begin their API explorations from the MailMessage class, then this makes it harder to find the MailServer class than the alternative mailMessage.send(mailServer). This is interesting because many real APIs place methods essential to common objects on other, helper objects. Alternate versions of three different APIs were compared, and it was found that programmers gravitated toward the same starting classes and were dramatically faster --- between 2 to 11 times --- combining multiple objects when a method on the starting class referred to the other class."""	application programming interface;learnability;programmer;usability	Jeffrey Stylos;Brad A. Myers	2008		10.1145/1453101.1453117	real-time computing;application programming interface;computer science;programming language;world wide web	SE	-29.990474163306526	25.058014847976843	90884
31bee7dde90cffc66cb31343e55ec326b94da897	free-ordered cug on chemical abstract machine	functional type;concurrent natural language generation;drastic rearrangement;japanese causative auxiliary verb;case domination;free-ordered cug;lambda calculus;categorial unification grammar;grammar rules distributively;concurrent calculus;chemical abstract machine;typed lambda calculus;abstract machine	We propose a paradigm for concurrent natural language generation. In order to represent grammar rules distributively, we adopt categorial unification grammar (CUG) where each category owns its functional type. We augment typed lambda calculus with several new combinators, to make the order of λ-conversions free for partial / local processing. The concurrent calculus is modeled with Chemical Abstract Machine. We show an example of a Japanese causative auxiliary verb that requires a drastic rearrangement of case domination.	abstract machine;algorithm;categorial grammar;combinatory logic;concurrency (computer science);concurrent computing;distributed computing;dominating set;model of computation;natural language generation;parse tree;parsing;programming paradigm;self-similarity;typed lambda calculus;unification (computer science);viz: the computer game	Satoshi Tojo	1994			natural language processing;system f;typed lambda calculus;categorial grammar;binary lambda calculus;pure type system;computer science;simply typed lambda calculus;abstract machine;programming language;lambda cube;type inhabitation;algorithm	NLP	-23.636144812669176	20.566764180945086	90949
5b5d2fe170b1d37273d5a8761a092dbb59d31b06	record manipulation in prolog	lenguaje programacion;records;programming language;prolog;logical programming;programmation logique;estructura datos;structured programming;langage programmation;structure donnee;programmation structuree;programacion logica;data structure;programacion estructurada	Abstract#R##N##R##N#Prolog is a relatively new programming language that has proved excellent for symbolic computation. However, Prolog was not specifically designed for industrial scale work and it lacks some standard features that are useful for reading, maintaining and debugging large programs. In particular, Prolog has no record mechanism, and programs often require major changes when data structures are modified.#R##N##R##N##R##N##R##N#The record is a standard data abstraction concept that improves the robustness of programs. The main advantage in using records is that data structures can be modified and extended with minimal repercussion to program code. Furthermore, the use of significant names to access data fields means that the intent of code is generally clearer.#R##N##R##N##R##N##R##N#We present a set of primitive operators that support a readable and robust programming style for the manipulation of record data structures in standard Prolog. The proposal covers both simple and imbricated record types and handles selective modification of records cleanly. We also treat property lists and records in a uniform way. These benefits are achieved with minimal overhead while retaining the traditional Prolog non-deterministic style.	prolog	Jean G. Vaucher	1989	Softw., Pract. Exper.	10.1002/spe.4380190808	data structure;computer science;theoretical computer science;operating system;symbolic programming;programming language;structured programming;prolog;algorithm	SE	-20.272030175212855	23.26811208362506	91451
5b6a05bccdef9123b805238cf878b71991388244	a formal verification tool for ethereum vm bytecode		In this paper, we present a formal verification tool for the Ethereum Virtual Machine (EVM) bytecode. To precisely reason about all possible behaviors of the EVM bytecode, we adopted KEVM, a complete formal semantics of the EVM, and instantiated the K-framework's reachability logic theorem prover to generate a correct-by-construction deductive verifier for the EVM. We further optimized the verifier by introducing EVM-specific abstractions and lemmas to improve its scalability. Our EVM verifier has been used to verify various high-profile smart contracts including the ERC20 token, Ethereum Casper, and DappHub MakerDAO contracts.	automated theorem proving;ethereum;formal verification;reachability;scalability;semantics (computer science);smart contract;virtual machine	Daejun Park;Yi Zhang;Manasvi Saxena;Philip Daian;Grigore Rosu	2018		10.1145/3236024.3264591	computer science;bytecode;theoretical computer science;automated theorem proving;scalability;security token;virtual machine;formal verification;abstraction;reachability	Logic	-20.48046159768691	27.678944606399142	91510
b619ca6abd4e96323401610a5597c92b8697c0c2	michael john caldwell gordon (frs 1994), 28 february 1948 - 22 august 2017		Michael Gordon was a pioneer in the field of interactive theorem proving and hardware verification. In the 1970s, he had the vision of formally verifying system designs, proving their correctness using mathematics and logic. He demonstrated his ideas on real-world computer designs. His students extended the work to such diverse areas as the verification of floating-point algorithms, the verification of probabilistic algorithms and the verified translation of source code to correct machine code. He was elected to the Royal Society in 1994, and he continued to produce outstanding research until retirement. His achievements include his work at Edinburgh University helping to create Edinburgh LCF, the first interactive theorem prover of its kind, and the ML family of functional programming languages. He adopted higher-order logic as a general formalism for verification, showing that it could specify hardware designs from the gate level right up to the processor level. It turned out to be an ideal formalism for many problems in computer science and mathematics. His tools and techniques have exerted a huge influence across the field of formal verification.	automated theorem proving;caldwell catalogue;computer science;correctness (computer science);formal verification;functional programming;logic for computable functions;machine code;programming language;proof assistant;randomized algorithm;semantics (computer science);verification and validation	Lawrence C. Paulson	2018	CoRR	10.1098/rsbm.2018.0019	probabilistic analysis of algorithms;computer science;theoretical computer science;machine code;functional programming;correctness;source code;formal verification;formalism (philosophy);proof assistant	Logic	-19.399326482403282	18.85866457895901	91651
38cee8c346fc410badae8feb0db6507ac28c887c	enhanced localization solution	location service;location tracking;performance evaluation;sensors;hidden markov model;sensors hidden markov models batteries mobile handsets global positioning system predictive models performance evaluation;auxiliary information;smart phones;global position system;automated reasoning;mobile phone;device resources usage enhanced localization solution smartphones resources capability embedded sensor availability radio interface availability mobile phones location data location service els localization strategy location tracking techniques built in technologies human mobility modelling machine learning techniques continuous service ubiquitous service;hidden markov models;machine learning;global positioning system;cognitive modelling;batteries;mobile handsets;ubiquitous computing;computer science not elsewhere classified;predictive models;prediction model;ubiquitous computing intelligent sensors learning artificial intelligence smart phones;learning artificial intelligence;intelligent sensors	The high potential of new generation smartphones in terms of resources capability as well as availability of embedded sensors and radio interfaces has opened new perspectives to developers, which came out with a massive number of applications for mobile phones. Most of them are location based, where location data is used either as main or as auxiliary information. However, the location service is power hungry. Thus, a major challenge is the optimization of the trade-off between resources usage of the location service and its accuracy. We introduce the architecture of the Enhanced Localization Solution (ELS), an efficient localization strategy for smartphones which smartly combines the standard location tracking techniques (e.g., GPS, GSM and WiFi localization), the newly built-in technologies, as well as Human Mobility Modelling and Machine Learning techniques. This solution aims to provide a continuous and ubiquitous service while reducing the impact on the device's resources usage.	built-in self-test;cluster analysis;embedded system;extreme loading for structures;global positioning system;internationalization and localization;location-based service;machine learning;mathematical optimization;mobile device;mobile phone;propagation of uncertainty;sensor;smartphone;tracing (software)	Michela Papandrea;Silvia Giordano	2012	2012 IEEE International Conference on Pervasive Computing and Communications Workshops	10.1109/PerComW.2012.6197487	embedded system;real-time computing;simulation;computer science;predictive modelling;hidden markov model	Mobile	-29.124117201096094	18.761675190880126	92152
4b78c90469e9e3c7a638db69d14cd0dd1ed44b12	the spoofax name binding language	declarative;name resolution;spoofax;name binding;meta language	In textual software languages, names are used to identify program elements such as variables, methods, and classes. Name analysis algorithms resolve names in order to establish references between definitions and uses of names. In this poster, we present the Spoofax Name Binding Language (NBL), a declarative meta-language for the specification of name binding and scope rules, which departs from the programmatic encodings of name binding provided by regular approaches. NBL aspires to become the universal language for name binding, which can be used next to BNF definitions in reference manuals, as well as serve the generation of implementations.	algorithm;beta normal form;language binding;name binding;scope (computer science);stratego/xt;variable (computer science)	Gabriël D. P. Konat;Vlad A. Vergu;Lennart C. L. Kats;Guido Wachsmuth;Eelco Visser	2012		10.1145/2384716.2384748	natural language processing;name binding;fully qualified name;name resolution;metalanguage;computer science;programming language;algorithm	PL	-26.460578744412434	20.243501530506105	92355
bd688cedb9a2a9861cd4c65f7cb8d738e5d64f55	symbolic programming		Automatic calculators can be programmed to interpret programs which have been written with symbolic instead of actual addresses. This method allows the calculator to assume much of the clerical burden which must otherwise be borne by the programmer.	symbolic programming	Nathaniel Rochester	1953	Trans. I.R.E. Prof. Group on Electronic Computers	10.1109/IREPGELC.1953.6499409	computer science;programming language;algorithm	HCI	-26.502406439040822	24.372678742171722	92507
5b91924f17f8f64b4f7ef4f425ffc8383240af3a	automatic incrementalization of prolog based static analyses	lenguaje programacion;mise a jour;analyse statique;programming language;prolog;langage declaratif;logical programming;program verification;analisis automatico;analisis estatica;actualizacion;verificacion programa;automatic analysis;development environment;programmation logique;declarative language;analyse automatique;langage programmation;logic programs;static analysis;verification programme;programacion logica;lenguaje declarativo;updating	Modern development environments integrate various static analyses into the build process. Analyses that analyze the whole project whenever the project changes are impractical in this context. We present an approach to automatic incrementalization of analyses that are specified as tabled logic programs and evaluated using incremental tabled evaluation, a technique for efficiently updating memo tables in response to changes in facts and rules. The approach has been implemented and integrated into the Eclipse IDE. Our measurements show that this technique is effective for automatically incrementalizing a broad range of static analyses.	algorithm;dataflow;dynamic problem (algorithms);eclipse;prolog;static program analysis;static single assignment form	Michael Eichberg;Matthias Kahl;Diptikalyan Saha;Mira Mezini;Klaus Ostermann	2007		10.1007/978-3-540-69611-7_7	declarative programming;computer science;database;development environment;programming language;prolog;static analysis;algorithm	PL	-22.946445254493284	31.08013866023374	92540
72afe1dcf261899ab386ddea453278ff7c2171e1	a class hierarchy for building stream-oriented file systems	object oriented design	This paper describes the object-oriented design and implementation of a family of stream-oriented le systems under UNIX and under an object-oriented operating system called Choices . A class hierarchy provides an object-oriented taxonomy of the algorithms and data structures used in the design of this family. The family includes the System V le system, the 4.2 BSD le system, and the MS-DOS le system. The class hierarchy has been developed by a series of experiments that are designed to lead to a framework for object-oriented le systems. The class hierarchy for stream-oriented le systems is the product of the second experiment in this series in which we revised a class hierarchy for UNIX-like le systems[MLRC88] to include the MS-DOS le system. We describe the hierarchy, how it evolved from the rst experiment to the second, and review the lessons that we have learned from the two experiments.	algorithm;bsd;c++;class hierarchy;dos;data structure;debugging;experiment;ms-dos;operating system;software portability;unix;unix-like	Peter Madany;Roy H. Campbell;Vincent F. Russo;Douglas E. Leyens	1989			everything is a file;zap file;class implementation file;design rule for camera file system;class hierarchy;unix file types;database;self-certifying file system;torrent file;computer science	PL	-29.242022939140565	27.370570638784766	92556
db57353d7c32534eb2aa7efa51b5ac3e5d294f7b	some facile chemistry	distribution;machine abstraite;functional concurrent programming;distributed system;chemical abstract machines;structural operational semantics;systeme reparti;cml;programming language;programacion paralela;poly ml;implementation;maquina abstracta;semantics;lambda calculus;simultaneidad informatica;parallel programming;concurrent program;facile;semantica;semantique;functional programming;abstract machine;ejecucion;concurrency;sistema repartido;computer programming languages;structured operational semantics;distribution channel;programa competidor;concurrent programs;lambda calculo;chemical abstract machine;channel management;programmation fonctionnelle;systeme parallele;parallel system;lambda calcul;programacion funcional;simultaneite informatique;sistema paralelo;programme concurrent;programmation parallele;langage programmation ordinateur	In this paper we use the chemical abstract machine (CHAM) framework [BeB90, BeB92, Bou94] for discussing various semantics for the Facile programming language [GMP89, GMP90, FAR93] and for formalising (parts of) its implementations. We use these formal descriptions to argue (informally) about implementability and cost of implementation in terms of low level machinery needed to implement the given semantics. We take the Facile language as source for discussion, but the results also apply to several other new languages such as CML [Rep91, BMT92] and Poly/ML [Mat91]. Characteristic for all these languages is that they combine ideas from the λ-calculus and process algebra, such as CCS [Mil80, Mil89], to support high level constructs for programming concurrent, parallel and/or distributed systems. The full version of this extended summary can be found in [LeT94].	abstract machine;calculus of communicating systems;centralisation;distributed computing;experiment;fault tolerance;high-level programming language;lambda calculus;medicinal chemistry;overhead (computing);process calculus;programmer	Lone Leth Thomsen;Bent Thomsen	1995	Formal Aspects of Computing	10.1007/BF01211076	distribution;concurrency;computer science;theoretical computer science;lambda calculus;semantics;abstract machine;programming language;functional programming;implementation;algorithm	PL	-25.377549007117832	31.219304095653026	92714
050ac807bc1badd466ffde7cec646dc7ae373d17	linear continuation-passing	lenguaje programacion;symbolic computation;exceptions;continuations;programming language;tt call cc;exception;continuation passing style;tt goto;linear functionals;linear continuation passing style;lcps;calculo simbolico;coroutines;programming theory;target language;backtracking;cps;langage programmation;theorie programmation;linear typing;langage cible;calcul symbolique	Continuations can be used to explain a wide variety of control behaviours, including calling/returning (procedures), raising/handling (exceptions), labelled jumping (goto statements), process switching (coroutines), and backtracking. However, continuations are often manipulated in a highly stylised way, and we show that all of these, bar backtracking, in fact use their continuations linearly ; this is formalised by taking a target language for cps transforms that has both intuitionistic and linear function types.	backtracking;compiler;continuation;coroutine;exception handling;function type;goto;linear function	Josh Berdine;Peter W. O'Hearn;Uday S. Reddy;Hayo Thielecke	2002	Higher-Order and Symbolic Computation	10.1023/A:1020891112409	symbolic computation;computer science;artificial intelligence;continuation-passing style;continuation;coroutine;programming language;algorithm;backtracking	PL	-19.737226817357904	23.798643868094132	92840
03d1d2fc04bef5c9b1f8b522205c7c13ad738bf6	a core calculus of classes and mixins	semantica operacional;object oriented language;formal model;operational semantics;lambda calculus;object oriented programming;functional programming;analyse syntaxique;semantique operationnelle;analisis sintaxico;object oriented;informatique theorique;syntactic analysis;lambda calculo;programmation fonctionnelle;programmation orientee objet;lambda calcul;programacion funcional;type system;computer theory;informatica teorica	We develop an imperative calculus that provides a formal model for both single and mixin inheritance. By introducing classes and mixins as the basic object-oriented constructs in a-calculus with records and references, we obtain a system with an intuitive operational semantics. New classes are produced by applying mixins to superclasses. Objects are represented by records and produced by instantiating classes. The type system for objects uses only functional, record, and reference types, and there is a clean separation between subtyping and inheritance.	formal language;imperative programming;instance (computer science);mixin;operational semantics;reference type;type system	Viviana Bono;Amit Patel;Vitaly Shmatikov	1999		10.1007/3-540-48743-3_3	computer science;artificial intelligence;lambda calculus;mathematics;programming language;object-oriented programming;functional programming;algorithm	PL	-25.319224629084484	26.729379681654674	92971
1213aa91a7710b45eb2f980135b98b51d10787bb	model transformation languages under a magnifying glass: a controlled experiment with xtend, atl, and qvt		In Model-Driven Software Development, models are automatically processed to support the creation, build, and execution of systems. A large variety of dedicated model-transformation languages exists, promising to efficiently realize the automated processing of models. To investigate the actual benefit of using such specialized languages, we performed a large-scale controlled experiment in which over 78 subjects solve 231 individual tasks using three languages. The experiment sheds light on commonalities and differences between model transformation languages (ATL, QVT-O) and on benefits of using them in common development tasks (comprehension, change, and creation) against a modern general-purpose language (Xtend). Our results show no statistically significant benefit of using a dedicated transformation language over a modern general-purpose language. However, we were able to identify several aspects of transformation programming where domain-specific transformation languages do appear to help, including copying objects, context identification, and conditioning the computation on types.	atlas transformation language;computation;declarative programming;domain-specific language;general-purpose language;general-purpose modeling;imperative programming;model transformation language;model-driven architecture;model-driven engineering;programmer;qvt;recursion;software development	Regina Hebig;Christoph Seidl;Thorsten Berger;Jens Joachim K. Pedersen;Andrzej Wasowski	2018		10.1145/3236024.3236046	computer science;programming language;theoretical computer science;model transformation;software development;transformation language;computation;xtend;magnifying glass;copying;comprehension	SE	-28.053420569365223	25.138037409453553	92995
6437eb71794c907e88a15b94f59ea2ad593eaae7	gpmx: a portable general purpose macro processor adapted for preprocessing fortran	dynamic change;control structure;fortran	GPMX is an extension of GPM, a simple, elegant yet powerful language independent macro processor described by Strachey. Unextended, GPM is not suited for preprocessing languages which use column position and end of record to delimit statements. Examples are FORTRAN and many assembly languages. Many programmers are constrained to work in such limited languages and GPMX is a simple yet powerful tool for extending and modifying these languages. Others have developed preprocessors dedicated to a particular language. This has advantages for the implementor, but requires the user to learn a different preprocessor for each language he uses. GPMX is designed to work on any language so that the (non-trivial) effort of learning to use it need not be repeated later. Extensions in GPMX include macro control over: files, record input and output, spacing, conditional macro processing and compilation, access to input and output buffers, and dynamic changing of the macro flag characters. Most of the extensions are accomplished simply by putting the control information on the macro stack where the processor has access to it (ala von Neumann). GPMX has been implemented in ANS FORTRAN for portability. Several applications are shown, including GO-TO free control structures for FORTRAN. Source is available.	assembly language;compiler;control flow;data buffer;fortran;general-purpose macro processor;input/output;preprocessor;programmer;statement (computer science)	Robert C. Gammill	1976		10.1145/1499799.1499925	parallel computing;computer science;theoretical computer science;macro;programming language	PL	-24.18410892450409	25.935152540042036	93083
fdc76c2cfaead732fd87de6eccad689df19a7d32	higher-order causal stream functions in sig from first principles		The Sig programming language is a total functional, clocked synchronous data-flow language. Its core has been designed to admit concise coalgebraic semantics. Universal coalgebra is an expressive theoretical framework for behavioral semantics, but traditionally phrased in abstract categorical language, and generally considered inaccessible. In the present paper, we rephrase the coalgebraic concepts relevant for the Sig language semantics in basic mathematical notation. We demonstrate how the language features characteristic of its paradigms, namely sequential and parallel composition for applicative style, delay for data flow, and apply for higher-order functional programming, are shaped naturally by the semantic structure. Thus the present paper serves two purposes, as a gentle, self-contained and applied introduction to coalgebraic semantics, and as an explication of the Sig core language denotational and operational design.	abstract data type;accessibility;agda;applicative programming language;bisimulation;causal filter;clock rate;coinduction;compiler;computation;continuation;coq (software);curry;curry–howard correspondence;dataflow;dependent type;hands-on computing;high- and low-level;input/output;lucid;mathematical optimization;microsoft outlook for mac;operational semantics;program optimization;program synthesis;program transformation;recursion (computer science);sequent calculus;signature block;stream processing;synchronous data flow;total functional programming;turing completeness	Baltasar Trancón y Widemann;Markus Lepper	2016			natural language processing;functional programming;coalgebra;mathematical notation;categorical variable;semantics;artificial intelligence;data flow diagram;explication;core language;mathematics	PL	-22.680861435559862	22.805618086152162	93225
c9f5379d146b4f1f7e1fa76263a5e1bdd431d5a8	what is automated theorem proving?	theorem proving;automated theorem proving		automated theorem proving	W. W. Bledsoe;Lawrence J. Henschen	1985	J. Autom. Reasoning			Logic	-19.833838840669358	19.38560308156424	93332
31bda8e3263acaf398de36242beaeb344d08a216	visual syntax diagrams for programming language statements	programming language	According to J. Henno ( I987), progr‘amming language statements still remain one of the most widely used forms of interaction between the computer and the human user and Uey have hardly changed since the introduction of Fortran. He says that the presentation of syntax is equally as important [as the design of the progr,ammin’g language], since this is the base for a user to form her or his own model of the language. The presentation mirrors the structure of the language; therefore the language, whose grammar is difficult to understand, is usually also difficult to use.	beta normal form;fortran;graphics;ibm informix;infographic;john d. wiley;programming language;syntax diagram;tandem computers;user (computing);visual basic	Lisa M. Braz	1990		10.1145/97426.97987	natural language processing;first-generation programming language;natural language programming;very high-level programming language;language primitive;data manipulation language;programming domain;data control language;computer science;programming language implementation;functional logic programming;syntax;programming paradigm;symbolic programming;low-level programming language;fifth-generation programming language;visual programming language;programming language;homoiconicity;programming language specification;high-level programming language;syntax error	PL	-26.914622057504776	23.48770927392658	93355
1bcffc4f2c43f765ed3cbd17290dded487b223a1	"""review of """"types and programming languages by benjamin c. pierce"""", mit press, 2002"""	programming languages;mit press;benjamin c. pierce;type system;programming language	Type systems and type checking are playing an increasingly important role in the development of programming languages. Almost every programming language developed today are developed with type systems as an integral part of the language. The goals of this book are to cover all important core topics of and give useful examples of how to use and implement type systems for programming languages. The book is separated into several parts, each part with coverage of one important core topic. After the introduction of lambda-calculus and some basic formalism for describing languages, the book presents a simple type system for lambda-calculus. In the following parts, the type system is then extended to cover recursive types, subtypes, polymorphic types, and higher-order polymorphic types (of the form that the purely functional language Haskell uses). In the course of defining and extending the type systems, the author gives several detailed examples of how to use them. The examples are interesting and to-the-point. Each important topic contain at least one in-depth study of an either an application or an example type system. In the book, there are case studies of both functional and imperative versions for objects (as in object-oriented programming) and also an introduction to Featherweight Java, which is a minimal core system for modeling Java’s type system.	floor and ceiling functions;haskell;imperative programming;java;lambda calculus;parametric polymorphism;pierce oscillator;programming language;purely functional programming;recursion;semantics (computer science);type system;types and programming languages	Mats Kindahl	2006	SIGACT News	10.1145/1189056.1189062		PL	-24.377162648973535	22.041551722854347	93404
52ad33abeee1b17f5f7f93c1cfe5bc33effa6547	some operational tools in a osi protocols study environment	protocols;resolution;automatic proofs;specifications;temporal logic;queue;conformance testing;safety;liveness;protocol specification;finite state machine	This paper deals with a strategy for a development chain of protocols (specification, validation, conformity testing) based on knowledge of the RHIN project methodology and the tools which this project produced. This methodology is based on finite state machines with predicates and includes “conceptual” tools for specification (PDIL)and for validation (VADILOC). “Experimental” tools play a part in conformity testing (STQ, Cerbere, Genepi). The interaction between conceptual and experimental tools is provided by a test sequence generator GAST which, starting from an automaton whose communication has been validated, provides sequences to be used by the test tools.	automaton;conformity;finite-state machine;osi model	Jean-Pierre Ansart;Omar Rafiq;Richard Castanet;Pascal Guitton	1984	Computer Communication Review	10.1145/800056.802073	communications protocol;real-time computing;resolution;temporal logic;computer science;conformance testing;finite-state machine;queue;liveness	SE	-32.43852394970675	31.54046935735562	93491
1753a285f3d1d746e50d8175c09e223746f2a98e	software engineering and the sp theory of intelligence		This paper describes a novel approach to software engineering derived from the SP Theory of Intelligence and its realisation in the SP Computer Model. Despite superficial appearances, it is shown that many of the key ideas in software engineering have counterparts in the structure and workings of the SP system. Potential benefits of this new approach to software engineering include: the automation or semi-automation of software development, with support for programming of the SP system where necessary; allowing programmers to concentrate on ‘world-oriented’ parallelism, without worries about parallelism to speed up processing; support for the long-term goal of programming the SP system via written or spoken natural language; reducing or eliminating the distinction between ‘design’ and ‘implementation’; reducing or eliminating operations like compiling or interpretation; reducing or eliminating the need for verification of software; reducing the need for validation of software; no formal distinction between program and database; the potential for substantial reductions in the number of types of data file and the number of computer languages; benefits for version control; and reducing technical debt.	compiler;computer language;computer simulation;natural language;numerical weather prediction;parallel computing;programmer;semiconductor industry;software development;software engineering;technical debt;the superficial;version control	J. Gerard Wolff	2017	CoRR		computer science;software engineering;data type;software development;automation;systems engineering;realisation;natural language;software;technical debt;speedup	SE	-25.04906425842585	23.329564521958883	93703
53adfa46893bc0155cc81a35be8e81a9d830047f	iterative development of transformation models by using classifying terms		In this paper we propose an iterative process for the correct speci cation of model transformations, i.e., for developing correct transformation models. This permits checking the correctness of a model transformation speci cation before any implementation is available. The proposal is based on the use of classifying terms for partitioning the input space and for simplifying the testing process.	correctness (computer science);iterative and incremental development;iterative method;model transformation	Frank Hilken;Loli Burgueño;Martin Gogolla;Antonio Vallecillo	2015			iterative and incremental development;mathematical optimization;correctness;mathematical analysis;mathematics;model transformation	DB	-19.424938258610982	26.01157116126662	93915
144df74e714ec35d424ef3d0c202e5936690995f	finding user groups on the basis of gsm logs - a survey	mobile device;mobile handsets mobile communication data models computers prediction algorithms gsm context;cellular radio;wireless network;mobile communications networks gsm logs ubiquitous computing wireless networks mobility data mobile devices mobile phones geographic positioning system gps discovering user context mobility prediction fraud detection;mobile communication network;mobility prediction;mobile phone;affinity model;global positioning system;velocity trap;model formulation;mobile communication;telecommunication security;positioning system;ubiquitous computing;social groups;telecommunication security cellular radio global positioning system;global system for mobile communication;block crediting;fraud detection;velocity trap model formulation affinity model block crediting;mobile user	The technologies of mobile communications and ubiquitous computing pervade our society and wireless networks sense the movement of people and vehicles, generating large volumes of mobility data. Miniaturization, wearability, pervasiveness of mobile devices are producing traces of our mobile activity, with increasing positioning accuracy and semantic richness: location data from mobile phones (Global System for Mobile Communications: GSM cell positions), Geographic Positioning System (GPS) tracks from mobile devices receiving geo-positions from satellites, etc. The objective of this paper is to review the works carried out by different group of researchers using varied techniques in Discovering User context, Mobility Prediction of Mobile users, Discovering social groups, Fraud detection in mobile communications networks etc.	global positioning system;mobile device;mobile phone;telecommunications network;tracing (software);ubiquitous computing	S. S. Deshpande;Rajiv V. Dharaskar	2010	2010 International Conference on Computer Information Systems and Industrial Management Applications (CISIM)	10.1109/CISIM.2010.5643500	social group;embedded system;mobile identification number;mobile search;mobile web;mobile telephony;global positioning system;imt advanced;public land mobile network;gsm services;mobile database;computer science;operating system;wireless network;mobile technology;location-based service;mobile device;mobile station;mobility model;mobile computing;mobile communications over ip;computer security;ubiquitous computing;computer network	Mobile	-29.69063526587187	19.78707600825598	93945
19a787ee9b4f68df880c2395873e78209d0a3a56	exception handling and object-oriented programming: towards a synthesis	fault tolerant;object oriented programming;object oriented;smalltalk;exception handling;context dependent;knowledge representation	The paper presents a discussion and a specification of an exception handling system dedicated to object-oriented programming. We show how a full object-oriented representation of exceptions and of protocols to handle them, using meta-classes, makes the system powerful as well as extendible and solves many classical exception handling issues. We explain the interest for object-oriented programming of handlers attached to classes and to expressions. We propose an original algorithm for propagating exceptions along the invocation chain which takes into account, at each stack level, both kind of handlers. Any class can control which exceptions will be propagated out of its methods; any method can provide context-dependant answers to exceptional events. The whole specification and some keys of our Smalltalk implementation are presented in the paper.	algorithm;exception handling;extensibility;smalltalk;speech synthesis	Christophe Dony	1990		10.1145/97945.97984	knowledge representation and reasoning;real-time computing;computer science;programming language;object-oriented programming;algorithm	PL	-26.064193755635483	26.949311209103815	94053
2681d5fdcedb88be815546b610741878969e259f	an assessment of the xr project for compiler development and transportation	xr project;abstract machine;systems implementation language;hierarchical software;software transportation	Abstract#R##N##R##N#A software engineering project, the XR project, is described, and its results assessed. The goal of the XR project is to contribute to the mechanical development and transportation of compilers on widely used Chinese-made computers. The basic tools are the compiler writing language XHY and the intermediate language CJY.#R##N##R##N##R##N##R##N#All the compilers are written in XHY and produce code in CJY form. In this paper, experiences, statistics and lessons of practising this project are given, together with an overview and a conclusion.	compiler	Lu Ru Quian;Wei Zi Chu	1983	Softw., Pract. Exper.	10.1002/spe.4380131103	computer science;software engineering;database;abstract machine;programming language	SE	-28.776830021433074	23.407344558495005	94295
736768fe05e6d114f9d0d2b10ba4a04db6c5ba75	types and programming languages	programming language	ion Another important way in which type systems support the programming process is by enforcing disciplined programming. In particular, in the context of large-scale software composition, type systems form the backbone of the module languages used to package and tie together the components of large systems. Types show up in the interfaces of modules (and related structures such as classes); indeed, an interface itself can be viewed as “the type of a module,” providing a summary of the facilities provided by the module—a kind of partial contract between implementors and users. Structuring large systems in terms of modules with clear interfaces leads to a more abstract style of design, where interfaces are designed and discussed independently from their eventual implementations. More abstract thinking about interfaces generally leads to better design.	function composition (computer science);internet backbone;programming language;type system;types and programming languages	Benjamin C. Pierce	2002			fourth-generation programming language;polymorphism;type conversion;type system;subtyping;type erasure;data type;type safety;computer science;recursive data type;theoretical computer science;third-generation programming language;functional logic programming;void type;programming paradigm;inductive programming;fifth-generation programming language;programming language theory;programming language;second-generation programming language;comparison of multi-paradigm programming languages;algorithm;control flow analysis;semantics	PL	-25.202114564954186	26.728874781558606	94344
6e08a8499a10b6a8cda389c6ec8f2fd0c8230a68	apl as the doundation for a universal computer language	universal keyboard;computer system;universal character representation;universal computer language;operating system;universal dictionary;common use;universal communication;acceptable universal computer language;universal alphabet;chip	Some ways in which universal computer languages differ from those in common use today. Universal dictionary. Universal alphabet. Universal character representation. Universal keyboard. Universal communication among computer systems. Elimination of an operating system as a separate entity. Some advantages of universal computer languages over those in common use today. Easier to teach and to learn. Easier and faster creation and modification of software. Faster execution. Easier transfer of software from one system to another. Fewer application software packages required. Some of the changes and additions required to make APL an acceptable universal computer language. Implement the APL interpreter at the chip level. Add language features to meet operating system requirements. Eliminate the features now required to communicate with operating systems of the present kind. Improve communication among users of a system. Improve the handling of integers. Improve the handling of graphics. Improve alphabet, character representation, and keyboard. The second edition of my monograph on this subject will be available at the Copenhagen APL Conference.	apl;computer keyboard;computer language;dictionary;graphics;operating system;requirement;system requirements;universal turing machine	Stephen W. Dunwell	1990		10.1145/97808.97828		Arch	-30.867723099348588	22.541623354962365	94451
03cd88906741871603b68b554459b60dd96fd5a9	veriml: typed computation of logical terms inside a language with effects	verification;logical frameworks;estensibilidad;modelizacion;proof assistant;lenguaje programacion;ml language;proof assistants;high order logic;preuve programme;program proof;language class;automatic proving;logical framework;programming language;langage ml;prise de decision;demostracion automatica;logical programming;program verification;development process;programming model;logica orden superior;modelisation;demonstration automatique;large scale;verificacion programa;formal reasoning;interpreteur;decision procedure;programmation logique;classe langage;type theory;comportement utilisateur;prueba programa;langage programmation;extensibilite;scalability;user behavior;dependent types;logique ordre superieur;interpreter;toma decision;verification programme;programacion logica;modeling;interprete;higher order logic;languages;language design;domain specificity;comportamiento usuario;clase lenguaje	Modern proof assistants such as Coq and Isabelle provide high degrees of expressiveness and assurance because they support formal reasoning in higher-order logic and supply explicit machine-checkable proof objects. Unfortunately, large scale proof development in these proof assistants is still an extremely difficult and time-consuming task. One major weakness of these proof assistants is the lack of a single language where users can develop complex tactics and decision procedures using a rich programming model and in a typeful manner. This limits the scalability of the proof development process, as users avoid developing domain-specific tactics and decision procedures.  In this paper, we present VeriML - a novel language design that couples a type-safe effectful computational language with first-class support for manipulating logical terms such as propositions and proofs. The main idea behind our design is to integrate a rich logical framework - similar to the one supported by Coq - inside a computational language inspired by ML. The language design is such that the added features are orthogonal to the rest of the computational language, and also do not require significant additions to the logic language, so soundness is guaranteed. We have built a prototype implementation of VeriML including both its type-checker and an interpreter. We demonstrate the effectiveness of our design by showing a number of type-safe tactics and decision procedures written in VeriML.	computation;coq (software);decision problem;domain-specific language;isabelle;logical framework;programming model;proof assistant;prototype;scalability;type safety;type system	Antonis Stampoulis;Zhong Shao	2010		10.1145/1863543.1863591	dependent type;scalability;verification;systems modeling;higher-order logic;logical framework;interpreter;object language;computer science;artificial intelligence;programming paradigm;proof assistant;programming language;type theory;software development process;algorithm	PL	-19.22817795429728	22.07820535841735	94817
eff686bbaeb2af057d5afbd36b3b30161b6e8a14	annotations for prolog: a concept and runtime handling	logical programming;specification programme;program verification;analisis programa;verificacion programa;programmation logique;informatique theorique;program analysis;analyse programme;verification programme;program specification;programacion logica;especificacion programa;computer theory;informatica teorica	A conceptof annotationsfor renderingprocedural aspectsof Prologis presented, built aroundwellknown proceduralconceptsof StandardProlog.Annotationsdescribepropertiesof predicates. Suchproperties canbe pre or postconditions,which musthold true whena predicateis calledor exited, respecti vely. Our concepttranscendspre/postconditions:weintroducetwo morekindsof annotations, fail andredoannotations, henceincorporatinga wholemodelof Prologexecutioninto our language.This enablesnaturalrenderingof many proceduralpropertiesof Prologwhichcannotbeexpressedwith only pre/postconditions.Therearefour morenoveltiesin ourapproach. First,any annotationcanbe“narroweddown” to asubsetof calls,via templates andcontexts, giving muchmoreflexible assertions. Notablythenovel ideaof calling context addssignificant expressi ve power, asa bridgetowardsprogram-pointassertions. Theannotationsaredefinedsimply asProlog goals,makingthemfully parametricandthereforeverycomfortablefor debugging.Finally, theannotationsare appliedvia ageneralkind of matchinginsteadof unification,enablingtheuseof local variables.All examples presentedhereareactualrunsof oursystemNope,which is a Prologmodule.	debugging;local variable;postcondition;predicate (mathematical logic);prolog;unification (computer science)	Marija Kulas	1999		10.1007/10720327_14	program analysis;computer science;artificial intelligence;operating system;database;programming language;algorithm	PL	-19.17001472584107	22.954177099493684	95011
348cac79268461c4542501e4874e18bb86f8a4cc	a novel multimedia streaming system for urban rail environments using wi-fi peer-to-peer technology		The amount of streaming multimedia data delivered to mobile devices is growing at a high rate. Research shows that a large number of daily commuters stream audio and video to their mobile devices during their travels. This makes urban rail environments a suitable platform for delivering entertainment, information and advertisement multimedia using novel delivery techniques. In order to do so, the system presented in this paper utilizes the unused bandwidth of a Communications-Based Train Control link to transmit multimedia to urban trains. Once on the train, multimedia is distributed to passenger devices using Wi-Fi Peer-to-Peer (P2P) technology. Such a multimedia distribution system can be deployed incrementally, as it can function concurrently with Wi-Fi connections already available in a number of trains. This paper presents the results obtained by emulating multimedia streaming in an urban rail use-case. Namely, it evaluates the received streaming multimedia quality parameters when new users arrive or existing users are replaced during the train stops.		Justas Poderys;Jahanzeb Farooq;José Soler	2018	2018 IEEE 87th Vehicular Technology Conference (VTC Spring)	10.1109/VTCSpring.2018.8417799	computer network;multimedia;peer-to-peer;train;wi-fi direct;computer science;mobile device	Visualization	-31.343381099560283	19.443176535695383	95040
da4cb76b38fb95abf4174fcbd40f5aeb09d49114	a java simulator for membrane computing	software tool;object oriented language;natural computing;p system;declarative languages;membrane computing;parallel simulation	Membrane Computing is a recent area of Natural Computing, a topic where much work has been done but still much remains to be done. There are some applications which have been developed in imperative languages, like C++, or in declaratives languages, as Prolog, working in the framework of P systems. In this paper, a software tool (called SimCM, from Spanish Simulador de Computación con Membranas) for handling P systems is presented. The program can simulate basic transition P Systems where dissolution of membranes and priority rules are allowed. The software application is carried out in an imperative and object-oriented language – Java. We choose Java because it is a scalable and distributed language. Working with Java is the first step to cross the border between simulations and a distributed implementation able to capture the parallelism existing in the membrane computing area. This tool is a friendly application which allows us to follow the evolution of a P system easily and in a visual way. The program can be used to move the P system theory closer to the biologist and all the people who wants to learn and understand how this model works.	c++;imperative programming;java;membrane computing;naruto shippuden: clash of ninja revolution 3;natural computing;p (complexity);p system;parallel computing;programming tool;prolog;scalability;simulation;systems theory	Isabel A. Nepomuceno-Chamorro	2004	J. UCS	10.3217/jucs-010-05-0620	natural computing;computer science;membrane computing;artificial intelligence;theoretical computer science;operating system;software engineering;programming language;object-oriented programming;world wide web;algorithm;p system	PL	-24.978787279289076	22.974009916534012	95058
ea2633b109104e7ad6e38b92d9f824a08ea80f42	on efficiency and optimization of c++ programs		classes are useful in object modeling. A pure abstract class is an abstract class with no real implementation. The inheritance of a pure abstract class is not for implementation but for interfiuce. A pure abstract class may be mapped to an empty structure of C. Unfortunately, the size of a structure in C must be greater than zero, so an empty structure contains at least one field of char. Inheriting a pure abstract class may thus inherit a useless field and cause overhead in object copying. This space overhead can be easily removed by compilers, if pure abstract classes are handled directly by using specific code generation routines. Removing offset adjustment in dynamic dispatch of multiple inheritance Although dynamic dispatch sometimes causes inefficiency, dynamic dispatch is useful and flexible in programming. The type of an object may not be statically determined, qSOrt..c: I n function ‘qsortvvInt-FP5wIntiT2~: qsort..c:295: warning: address of register variable ‘-0k’ requested qsort..c:323: warning: address of register variable ’-2tmp‘ requested Figure 11. The messages in adding ‘register’ to the variables k and t m g for the class wInt 462 P.-C. WU AND F.-J. WANG so it is impossible to completely eliminate function calls of dynamic dispatch. Such a kind of inefficiency can be overcome with more optimization techniques. The designers of the C t t language carefully considered the implementation of virtual functions. The solution is a virtual function table (vtuble) (Reference 1, Section 10.8.1~). For single inheritance, support of dynamic dispatching requires an indirect procedure call. For multiple inheritance, in addition to an indirect call, two operations, a load of offset, and an add operation are needed in most implementations. Because the program qsort uses only single inheritance, we can simply remove the code for offset adjustment. Table I1 shows the speed-up by removing offset adjustment in dynamic dispatch of multiple inheritance in G t t and CC. The asterisked instructions in Figure 10 are removed, and the register ‘%ol’ is changed to ‘%oO’. The execution time reduction is nearly the same for G t t and CC. The speed-up is 6.9 per cent for G++ and 10.5 per cent for CC. The code generated by CC is more compact than that generated by G t t , so the execution time reduced with CC thus looks more significant. Note that BCC and MSC have already applied this optimization. There are three means of applying this optimization. Stroustrup (Reference 22, p. 265) presented an alternative implementation for dynamic dispatch of multiple inheritance. In his approach, a small piece of code is used to adjust the this pointer and jump to the corresponding member function. There is no need to store the offset in a virtual function table, no code duplication, and no execution overhead for dynamic dispatch of single inheritance. This implementation technique looks good, but is less portable. Borland C t t version 3.P3 (BCC3 in short, the older version of BCC) restricts the use of multiple inheritance and provides more efficient dynamic dispatching. BCC3 disallows an inheritance with interactions of sibling classes (e.g., the inheritance considered in Reference 1, p. 234). Some code rewriting is needed when the inheritance violates the restriction. Figure 12 shows an example of interactions between sibling classes. Class C defines the function g ( ) by calling the function f ( ) , which is defined in class B. The example works in G t t , CC, MSC, and BCC (the newer version), but not in BCC3. Another approach is to duplicate the code inherited from base classes when the interactions of sibling classes happen in an inheritance hierarchy. Considering the above example, Figure 13 shows an equivalent class hierarchy without interactions. The function f ( ) is defined once again in class D. There is an additional copy of f specific for objects of D, so the offset adjustment for converting an object of D to that of B is not needed when calling B : : f . The equivalent class hierarchy can be automatically generated by compilers. The code duplicated (e.g., function f ( ) ) may be small, if the inheritance hierarchy is very simple. Table 11. The speed-up in replacing multiple inheritance by single inheritance	abstract type;bricx command center;bundle adjustment;c++;class hierarchy;code generation (compiler);dispatch table;duplicate code;dynamic dispatch;gnu compiler collection;goto;interaction;mathematical optimization;method (computer programming);multiple inheritance;object copying;overhead (computing);pointer (computer programming);program optimization;qsort;rewriting;run time (program lifecycle phase);speedup;subroutine;virtual method table	Pei-Chi Wu;Feng-Jian Wang	1996	Softw., Pract. Exper.	10.1002/(SICI)1097-024X(199604)26:4%3C453::AID-SPE21%3E3.0.CO;2-9	real-time computing;computer science;operating system;programming language;object-oriented programming;algorithm	PL	-24.69756557954262	28.65099000829702	95263
e89056023e4c414bd16b83272d3522829ce5d285	types for active objects with static deadlock prevention	types;subtyping;concurrency;object calculus;active objects;deadlocks	Process types statically ensure the acceptability of all messages sent to an object even if the set of acceptable messages changes dynamically. As proposed so far, process types do not ensure the return of answers; deadlocks may prevent objects from behaving as expected. In this article we propose an object calculus with types distinguishing between obligatory messages (that must be sent, for example, as answer to a question) and optional messages (that can, but need not be sent). A type checker enforces the sending of obligatory messages and ensures that obligatory messages are not suppressed by deadlocks. The proposed type concept supports subtyping.	deadlock	Franz Puntigam;Christof Peter	2001	Fundam. Inform.		concurrency;subtyping;computer science;theoretical computer science;deadlock;distributed computing;programming language;algorithm	Robotics	-22.22363325922018	32.06818183415965	95643
b8b66dab6dcc55788384af552eeb441c7840719c	modeling and model checking mobile phone payment systems	developpement logiciel;distributed system;money;atomicidad;tratamiento transaccion;verificacion modelo;modeling technique;systeme reparti;labelled transition system;mobile radiocommunication;formal specification;informatique mobile;pago;securite;telephone portable;cellular radio;metodo formal;methode formelle;verification modele;monnaie;program verification;payment;radiocommunication service mobile;safety properties;automatic generation;systeme transition etiquete;formal method;mobile phone;specification formelle;moneda;especificacion formal;verificacion programa;paiement;modelo logico;telefono movil;sistema repartido;model checking;atomicity;atomicite;desarrollo logicial;software development;safety;defaillance;failures;logic model;radiotelephonie cellulaire;transaction processing;radiocomunicacion servicio movil;mobile computing;verification programme;seguridad;fallo;traitement transaction;sistema transicion marcada;payment system;modele logique	Recently a technique for transacting goods using GSM mobile phones has become very popular. We present a formal model of these novel transactions using a views based modeling technique. We show how to express two safety properties namely goods and money atomicity within this model using a sub-logic of CTL. By automatically generating a labelled transition system from our views model we can model check these properties. We show how to generalise this model to arbitrary numbers of processes. Goods atomicity fails under certain circumstances thus exposing some deficiencies that exist in existing implementations.	atomicity (database systems);mobile phone;model checking;transition system	Tim Kempster;Colin Stirling	2003		10.1007/978-3-540-39979-7_7	model checking;embedded system;simulation;formal methods;transaction processing;computer science;money;software development;formal specification;database;programming language;mobile computing;computer security;atomicity;payment	SE	-23.501458826229968	30.67462078947259	95930
7537f4d5361ca480ae3e086329948b5091aab065	efficient pattern matching in python		Pattern matching is a powerful tool for symbolic computations. Applications include term rewriting systems, as well as the manipulation of symbolic expressions, abstract syntax trees, and XML and JSON data. It also allows for an intuitive description of algorithms in the form of rewrite rules. We present the open source Python module MatchPy, which offers functionality and expressiveness similar to the pattern matching in Mathematica. In particular, it includes syntactic pattern matching, as well as matching for commutative and/or associative functions, sequence variables, and matching with constraints. MatchPy uses new and improved algorithms to efficiently find matches for large pattern sets by exploiting similarities between patterns. The performance of MatchPy is investigated on several real-world problems.	abstract syntax tree;algorithm;json;open-source software;pattern matching;python;rewriting;s-expression;symbolic computation;wolfram mathematica;xml	Manuel Krebber;Henrik Barthels;Paolo Bientinesi	2017	CoRR	10.1145/3149869.3149871	theoretical computer science;xml;computer science;python (programming language);pattern matching;expression (mathematics);abstract syntax;json;commutative property;rewriting	Logic	-27.278017965422634	22.085165422643495	96064
390edb56ebf7fed3bebca40ab0e44f0d3aa9f07e	exception analysis and points-to analysis: better together	precision;exception handling;program analysis;logic programs;context sensitive;points to analysis	"""Exception analysis and points-to analysis are typically done in complete separation. Past algorithms for precise exception analysis (e.g., pairing throw clauses with catch statements) use pre-computed points-to information. Past points-to analyses either unsoundly ignore exceptions, or conservatively compute a crude approximation of exception throwing (e.g., considering an exception throw as an assignment to a global variable, accessible from any catch clause). We show that this separation results in significant slowdowns or vast imprecision. The two kinds of analyses are interdependent: neither can be performed accurately without the other. The interdependency leads us to propose a joint handling for performance and precision. We show that our exception analysis is expressible highly elegantly in a declarative form, and can apply to points-to analyses of varying precision. In fact, our specification of exception analysis is """"fully precise"""", as it models closely the Java exception handling semantics. The necessary approximation is provided only through whichever abstractions are used for contexts and objects in the base points-to analysis. Our combined approach achieves similar precision relative to exceptions (exception-catch links) as the best past precise exception analysis, with a runtime of seconds instead of tens of minutes. At the same time, our analysis achieves much higher precision of points-to information (an average of half as many values for each reachable variable for most of the DaCapo benchmarks) than points-to analyses that treat exceptions conservatively, all at a fraction of the execution time."""	algorithm;approximation;computer performance;dacapo;exception handling;global variable;interdependence;java;pointer analysis;precomputation;run time (program lifecycle phase)	Martin Bravenboer;Yannis Smaragdakis	2009		10.1145/1572272.1572274	program analysis;exception handling;real-time computing;computer science;data mining;accuracy and precision;programming language;algorithm;context-sensitive language	SE	-19.767416836544	31.139498712485803	96356
94ccedae1c7724ec71af8a75d6e0e697b2ee82aa	predictive intelligence to the edge through approximate collaborative context reasoning	collaborative event inference;federated reasoning;edge predictive intelligence;optimal stopping theory;adaptive vector quantization;type-2 fuzzy logic inference	We focus on Internet of Things (IoT) environments where a network of sensing and computing devices are responsible to locally process contextual data, reason and collaboratively infer the appearance of a specific phenomenon (event). Pushing processing and knowledge inference to the edge of the IoT network allows the complexity of the event reasoning process to be distributed into many manageable pieces and to be physically located at the source of the contextual information. This enables a huge amount of rich data streams to be processed in real time that would be prohibitively complex and costly to deliver on a traditional centralized Cloud system. We propose a lightweight, energy-efficient, distributed, adaptive, multiple-context perspective event reasoning model under uncertainty on each IoT device (sensor/actuator). Each device senses and processes context data and infers events based on different local context perspectives: (i) expert knowledge on event representation, (ii) outliers inference, and (iii) deviation from locally predicted context. Such novel approximate reasoning paradigm is achieved through a contextualized, collaborative belief-driven clustering process, where clusters of devices are formed according to their belief on the presence of events. Our distributed and federated intelligence model efficiently identifies any localized abnormality on the contextual data in light of event reasoning through aggregating local degrees of belief, updates, and adjusts its knowledge to contextual data outliers and novelty detection. We provide comprehensive experimental and comparison assessment of our model over real contextual data with other localized and centralized event detection models and show the benefits stemmed from its adoption by achieving up to three orders of magnitude less energy consumption and high quality of inference.	approximation algorithm;centralized computing;cloud computing;cluster analysis;display resolution;internet of things;novelty detection;programming paradigm	Christos Anagnostopoulos;Kostas Kolomvatsos	2017	Applied Intelligence	10.1007/s10489-017-1032-y	artificial intelligence;contextual design;computer science;machine learning;cloud computing;optimal stopping;data stream mining;data mining;cluster analysis;inference;novelty detection;phenomenon	AI	-32.2146815803541	18.35050789811839	96433
231e2250aa10e23db7eb50f92b2ee9cd48b4fbff	engineering higher-order modules in sml/nj	lambda calculus;module system;standard ml;higher order;functional language	SML/NJ and other Standard ML variants extend the ML module system with higher-order functors, elevating the module language to a full functional language. In this paper, we describe the implementation of the higher-order module system in SML/NJ, which is unique in providing “true” higher-order behavior at the static level. This second generation implementation of higher-order modules in SML/NJ is based on three key techniques: unique internal variables (entity variables) for naming static entities, a factorization of the static information in both basic modules and functors into signatures and realizations, and a static lambda calculus we call the entity calculus with static “effects” to represent the type-level mapping performed by a functor. This system implements MacQueen-Tofte’s re-elaboration semantics without having to re-elaborate functor bodies or appeal to fragile stamp properties.	antivirus software;basic stamp;compiler;entity;functional programming;ibm notes;icfp programming contest;international conference on functional programming;interpreter (computing);lambda calculus;modular programming;ramsey's theorem;scalability;second generation multiplex plus;self-hosting;standard ml of new jersey;type safety	George Kuan;David MacQueen	2009		10.1007/978-3-642-16478-1_13	higher-order logic;computer science;lambda calculus;programming language;functional programming;algorithm	PL	-21.850922429531533	21.779029595094414	96443
6ce0394222f900a03141357ea2b514fac4cde651	towards object-oriented graphics standards	object oriented	"""1. INTRODUCrlON Object-oriented systems, with their communication through message passing and inheritance mechanisms for constructing rich and sophisticated hierarchies of object classes, were not basically designed for computer graphics applications--one source is, for example, the field of artificial intelligence [1 ]. In addition, as pointed out in characterizing Smalltalk-80 [2], object-oriented programming is very useful for creating user interfaces and graphical applications. The multilevel class concept, creation of class instances (objects), and definition of methods harmonize well with the demands of geometric modeling, and the construction, maintenance, and processing of an object hierarchy required in a graphics application system. This has been shown by object-oriented implementations, e.g., an objectoriented graphical kernel system on top of GKS, using an object-oriented compiler for the C language from Productivity Products International (Sandy Hook, CT) [3]. In this paper we illustrate by examples that a language binding of PHIGS to Smalltalk-80 is possible. But we recommend defining a new functional description of PHIGS which will better harmonize with the object-oriented philosophy. Although we focus on the PHIGS standard proposal which is in the review process, analogous considerations are valid for the GKS Output Level 3 Proposal of J. E. Steinhart [4]. 2. THE PHIGS STANDARD PROPOSAL """"The Programmer's Hierarchical Interactive Graphics Standard (PHIGS) is a functional specification of the interface between an application program and its graphics support system . . . . PHIGS is governed by a number of global philosophical positions. • . . These global positions are: multilevel/hierarchical structuring of graphics data, a high degree of interac~"""" This paper is one of the award winning papers from EUROGRAPHICS '85, the annual Conference of the European Association for Computer Graphics. The paper is published here in a revised form, with permission of North Holland Publishing Company (Amsterdam), the Publisher of EUROGRAPH1CS '85 Conference Proceedings. tivity, rapid modification of graphics data and the relationships among the data, applicability to a diverse application family, minimality of functionality while producing an effective standard."""" [5]. PHIGS graphics data are organized into units called structures. Structures can be related to each other hierarchically. The hierarchy is modeled by an acyclic directed graph. Structures contain structure elements such as output primitives, attribute selections, viewing selections and so-called application-specific data (nongraphical data used by an application program). A structure is organized as a one-dimensional array (linear list). The PHIGS structure editing operations are essentially deletions and insertions of structure elements at a position described by an (integer type) element pointer. 2.1 PHIGS language binding In [5], 9 rules are formulated for language binding. We will focus on the following rule L5: Rule L5. """"The language binding should specify, for each of the PHIGS data types, a corresponding data type acceptable to the language. Where convenient for the host language, additional data types may be specified in terms of the PHIGS data types. The data types used in the standard are merely tools for describing the semantics of the standard; they should be replaced by actual data types conforming to the restrictions of the host language."""" As pointed out in the PHIGS Baseline Document [5], the """"objective of a language binding is to provide the functions and data types of PHIGS in a natural and efficient manner using the facilities of the host language, without violating the style or design philosophy of the language."""" 3. ASPECTS OF A PHIGS BINDING TO SMALLTALK-80 3.1 Introductory remarks Besides [2] a good overview about programming in Smalltalk-80 is given in [6]. The paper of T. Rentsch compares SmaUtalk-80 with other languages and characterizes some important aspects of object-oriented programming as follows [7]:"""	array data structure;artificial intelligence;compiler;computer graphics;directed acyclic graph;directed graph;eurographics;functional specification;geometric modeling;graphical kernel system;graphical user interface;graphics software;image file formats;integer (computer science);language binding;message passing;phigs;pointer (computer programming);programmer;smalltalk;stepstone;structure editor;vector graphics	Peter Wißkirchen	1986	Computers & Graphics	10.1016/0097-8493(86)90045-2	computer science;artificial intelligence;theoretical computer science;operating system;programming language;object-oriented programming;algorithm;computer graphics (images)	PL	-32.07260030549103	24.828803855726665	96465
51df7612248ee738ba5f8ad5c103e834c2007100	automatic programming using abstract data types	abstract data type;automatic programming	In th i s paper we f i r s t t r y to character ize one meaning of automatic programming. We consider it to be one part of the Programming environment re la ted to A r t i f i c i a l I n te l l i gence techniques. We then i l l u s t r a t e an automatic programning process, on a simple example, using an Abstract Data Type theory to which we add the not ion of schemes which are p a r t i c u l a r l y useful in program de r i va t i on from Abs t rac t Type decomposition. We conclude that a l l the concepts t reated in th i s paper must be con ta i ned in one way or another in any automatic programming system. However th i s necessitates fu r the r study in such t heo re t i ca l f i e l d s as Abstract Data Type Theory, Spec i f i ca t i on languages, Theorem provers or proof cheekers and ru le r e w r i t i n g systems.	abstract data type;admissible numbering;automated theorem proving;automatic programming;execution unit;linear algebra;naruto shippuden: clash of ninja revolution 3;oxford spelling;spec#;type theory	Gérard D. Guiho	1983			type conversion;programming domain;data type;reactive programming;computer science;abstract type;theoretical computer science;programming paradigm;procedural programming;symbolic programming;inductive programming;fifth-generation programming language;programming language;abstract data type;algorithm;complex data type	AI	-23.964928821877173	21.950000585824682	96608
5a40390ee5bcdb71045e8608dfd3afda25cfb343	presenting gpss/h results with the graphical kernel system (gks)	common random numbers;computer program;bonferroni inequality;graphical kernel system;selection;graphics system;technical report;variance reduction	This paper discusses the interfacing of the latest version of GPSS, namely GPSS/H, and GKS, a graphics system which allows programs to support a wide variety of graphics devices. A computer program to convert a running GPSS/H program to a form compatible for graphical emulation has been developed. An animation program which takes as input the data generated by the modified GPSS/H program has also been developed. Thus, the GPSS programmer needs only to take care of correct modelling of the system under study and need not know anything at all about graphics and animation.	care-of address;emulator;gpss;graphical user interface;graphics;programmer	Raphael Parambi;Aseem S. Chandawarkar	1986		10.1145/318242.318486	selection;simulation;human–computer interaction;computer science;technical report;gpss;world wide web;variance reduction;computer graphics (images)	Graphics	-32.4685463929765	27.28847912361913	96617
99ca7548010d3dbe9ba19319f6c1d776ae618bec	featherweight wrap java	language extensions;wrappers;featherweight java;delegation;language extension	We present an extension for a Java like language with a mechanism for dynamically extending object behaviors. Our approach consists in moving the addition of new features from class (static) level to object (dynamic) level: the basic features of entities (representing their structure) are separated from the additional ones (wrapper classes whose instances represent run-time added behaviors). At run-time, these entities can be dynamically composed by instantiating wrapper objects which are attached to basic entities. We formalize our extension by adding the new constructs to Featherweight Java; the core language so extended (Featherweight Wrap Java) is type safe.	entity;java;type safety	Lorenzo Bettini;Sara Capecchi;Elena Giachino	2007		10.1145/1244002.1244242	delegation;real-time computing;computer science;operating system;java modeling language;database;programming language;management;world wide web;generics in java;java annotation	PL	-26.1872238131361	28.767426776020407	96634
a7bd1a52ff7ec9ee3a734a15a3125e37bdb9b181	distinctness and sharing domains for static analysis of java programs	lenguaje programacion;theorie type;type analysis;analyse statique;programming language;java programming;program transformation;langage java;optimizacion compiladora;object oriented programming;program verification;transformation programme;analisis programa;program specialization;verificacion programa;transformacion programa;specialisation;object oriented;type theory;compiler optimization;langage programmation;oriente objet;program analysis;interpretation abstraite;analyse programme;static analysis;abstract interpretation;verification programme;orientado objeto;optimisation compilateur;java language;etat partage;shared state	The application field of static analysis techniques for objectoriented programming is getting broader, ranging from compiler optimizations to security issues. This leads to the need of methodologies that support reusability not only at the code level but also at higher (semantic) levels, in order to minimize the effort of proving correctness of the analyses. Abstract interpretation may be the most appropriate approach in that respect. This paper is a contribution towards the design of a general framework for abstract interpretation of Java programs. We introduce two generic abstract domains that express type, structural, and sharing information about dynamically created objects. These generic domains can be instantiated to get specific analyses either for optimization or verification issues. The semantics of the domains are precisely defined by means of concretization functions based on mappings between concrete and abstract locations. The main abstract operations, i.e., upper bound and assignment, are discussed. An application of the domains to source-to-source program specialization is sketched to illustrate the effectiveness of the analysis.	abstract interpretation;assignment (computer science);correctness (computer science);formal verification;java;mathematical optimization;optimizing compiler;partial template specialization;static program analysis	Isabelle Pollet;Baudouin Le Charlier;Agostino Cortesi	2001		10.1007/3-540-45337-7_5	computer science;theoretical computer science;operating system;database;distributed computing;programming language;object-oriented programming;algorithm	PL	-23.048105436458943	29.30411129703917	96636
f8a53126016e32a12e1f60a4afbb93a8fa858ea7	software reusability through versions	software reusability	In this paper a new approach for modelling program variants (versions) is proposed, which is focused on increasing the level of software reuse, rather than on enriching data model. In this approach, a program is composed of a program body and a set of logically independent program contexts. The program body contains global functions and global data structures. Each program context contains exactly one variant of every class defined in the program and one variant of every non-global function, called a context function. Variants of the same cladfunction belonging to different contexts need not be different. During the program execution only one context is active. It may, however, be changed dynamically at program run-time. Thus, at a particular moment of time the program is viewed as a sum of the program body and exactly one program context.	code reuse;data model;data structure;run time (program lifecycle phase)	Waldemar Wieczerzycki	1996	Softw., Pract. Exper.	10.1002/(SICI)1097-024X(199608)26:8%3C911::AID-SPE39%3E3.0.CO;2-E	systems modeling;computer science;operating system;reuse;database;programming language;object-oriented programming	PL	-25.964752683096474	28.660138379347565	96812
d3b85eefaf3df5984952ecef1aefa326f669c007	a visual supporting method for sequence controller programming	sequences;programmable controllers;programming language;actions visual support method sequence controller programming user friendliness programming languages programmer productivity efficient program writing software components combination target system definition icons monitor screen conditions;graphical user interfaces;software component;control engineering computing;control engineering computing programmable controllers sequences subroutines program visualisation graphical user interfaces;program visualisation;programming profession productivity computer science microcomputers central processing unit switches valves writing software systems condition monitoring;subroutines	The objective of this research is to establish a visual supporting method for developing sequence controller programs. Current sequence controller programming methods have such problems as user-unfriendly programming languages and low programmer productivity. Our supporting method will provide solutions to the problems that will lead to a more eflcient way of writing a sequence controller program. The method lets a programmer combine software parts to define a target system. The software parts appear as icons on a monitor Then, he develops a sequence controller program by setting conditions as well as actions to the software parts. Our method will be illustrated in detail with examples.	computer monitor;programmer;programming language;programming productivity	Yongil Shin;Atsushi Ohnishi	2000		10.1109/CMPSAC.2000.884743	real-time computing;n-version programming;functional reactive programming;computer science;software framework;subroutine;component-based software engineering;software development;operating system;software engineering;programmable logic controller;sequence;computer programming;graphical user interface;database;programming paradigm;event-driven programming;procedural programming;fifth-generation programming language;programming language;system programming;computer engineering	SE	-30.914364716048695	24.650902516005107	97005
12cb32f090300e09e2cf4be8cafded972d7abe71	narcissus: deriving correct-by-construction decoders and encoders from binary formats		It is a neat result from functional programming that libraries of parser combinators can support rapid construction of decoders for quite a range of formats. With a little more work, the same combinator program can denote both a decoder and an encoder. Unfortunately, the real world is full of gnarly formats, as with the packet formats that make up the standard Internet protocol stack. Most past parser-combinator approaches cannot handle these formats, and the few exceptions require redundancy – one part of the natural grammar needs to be hand-translated into hints in multiple parts of a parser program. We show how to recover very natural and nonredundant format specifications, covering all popular network packet formats and generating both decoders and encoders automatically. The catch is that we use the Coq proof assistant to derive both kinds of artifacts using tactics, automatically, in a way that guarantees that they form inverses of each other. We used our approach to reimplement packet processing for a full Internet protocol stack, inserting our replacement into the OCaml-based MirageOS unikernel, resulting in minimal performance degradation.	combinatory logic;elegant degradation;encoder;functional programming;library (computing);narcissus (javascript engine);network packet;ocaml;parser combinator;proof assistant;protocol stack;unikernel	Sorawit Suriyakarn;Clément Pit-Claudel;Benjamin Delaware;Adam Chlipala	2016	CoRR		theoretical computer science;encoder;correctness;computer science;decoding methods;injective function;network packet;binary number;monad (functional programming);proof assistant	OS	-22.41581576209985	27.79413502746816	97277
7bef719324892d2d4eff8049d231404e213ff07d	handling context-sensitive syntactic issues in the design of a front-end for a matlab compiler	colon expressions;front end;assignments;control flow graph;abstract syntax tree;control constructs;syntax analysis for matlab;matrices;design and implementation;signal processing;command form function invocations;off the shelf;single quote character;intermediate representation	In recent times, the MATLAB language has emerged as a popular alternative for programming in diverse application domains such as signal processing and meteorology. The language has a powerful array syntax with a large set of pre-defined operators and functions that operate on arrays or array sections, making it an ideal candidate for applications involving substantial array-based processing.Yet, for all the programming convenience that the language offers, designing a parser and scanner capable of mimicking the language's syntax has proven to be an acutely difficult task. The language has many context-sensitive constructions, and though numerous front-end implementations of MATLAB and MATLAB-like languages exist, not much has been discussed regarding the efficient compile-time parsing of such languages or how its syntax impacts the parsing process.In this paper, we present the design and implementation of a compiler front-end for the MATLAB language. We discuss in detail both the indigenously designed grammar responsible for syntax analysis as well as the lexical specification that complements the grammar. In the course of our attempts to emulate MATLAB's syntax, we were able to unravel certain key issues relating to its syntax, such as the complications arising in parsing command-form function invocations within a compile-time environment, the context-sensitive interpretation of the single quote character, and the translation of white space within matrices into element separators.The front-end effects a conversion of the original source to an intermediate form in which statements are represented as abstract syntax trees and the flow of control between statements by a control-flow graph. All subsequent compiler passes work on this intermediate representation.The front-end was designed and implemented as part of the MATCH project, which addresses the translation of a MATLAB program by a compiler onto a heterogeneous target consisting of embedded and commerical-off-the-shelf processors.	compiler;context-sensitive help;matlab	Pramod G. Joisha;Abhay Kanhere;Prithviraj Banerjee;U. Nagaraj Shenoy;Alok N. Choudhary	2001	ACM SIGAPL APL Quote Quad	10.1145/969781.969784	abstract syntax;computer science;artificial intelligence;theoretical computer science;front and back ends;operating system;compiler construction;parsing;signal processing;syntax;abstract semantic graph;mathematics;linguistics;programming language;attribute grammar;intermediate language;homoiconicity;algorithm;matrix;abstract syntax tree;syntax error;control flow graph	PL	-24.034748890882316	26.242396116385482	97406
3d055cdcc543558da08f827028196f2b6ae58049	proving correctness of concurrent constraint programs			constraint logic programming;correctness (computer science)	Frank S. de Boer;Maurizio Gabbrielli;Elena Marchiori;Catuscia Palamidessi	1994			theoretical computer science;concurrent constraint logic programming;correctness;computer science	DB	-19.14951467812225	20.37013098783234	97417
10bd3325124379dcede5342ce5af3255b073877a	programming paradigms of the andorra kernel language	programming paradigm	The Andorra Kernel Language (AKL) is introduced. It is shown how AKL provides the programming paradigms of both Prolog and GHC. This is the original goal of the design. However, it has also been possible to provide capabilities beyond that of Prolog and GHC. There are means to structure search, more powerful than plain backtracking. It is possible to encapsulate search in concurrent reactive processes. It is also possible to write a multi-way merger with constant delay. In these respects AKL is quite original. Although AKL is an instance of our previously introduced Kernel Andorra Prolog framework, this exposition contains important extensions, and a considerable amount of unnecessary formal overhead has been stripped away.	backtracking;kl1;kernel (operating system);overhead (computing);programming paradigm;prolog;the glorious glasgow haskell compilation system	Sverker Janson;Seif Haridi	1991			computer science;theoretical computer science;programming language;algorithm	Arch	-21.11707258852432	23.75607286300256	97439
34dde23d9c80a9c93b78d2e38c8d8cad1bb510e4	fitting the pieces together: a machine-checked model of safe composition	product lines;feature modeling;product line;satisfiability;feature models;software product line;type safety;type system	Programs of a software product line can be synthesized by composing features which implement a unit of program functionality. In most product lines, only some combination of features are meaningful; feature models express the high-level domain constraints that govern feature compatibility. Product line developers also face the problem of safe composition - whether every product allowed by a feature model is type-safe when compiled and run. To study the problem of safe composition, we present Lightweight Feature Java (LFJ), an extension of Lightweight Java with support for features. We define a constraint-based type system for LFJ and prove its soundness using a full formalization of LFJ in Coq. In LFJ, soundness means that any composition of features that satisfies the typing constraints will generate a well-formed LJ program. If the constraints of a feature model imply these typing constraints then all programs allowed by the feature model are type-safe.	compiler;coq (software);curve fitting;feature model;high- and low-level;lightweight java;software product line;type safety;type system;well-formed formula	Benjamin Delaware;William R. Cook;Don S. Batory	2009		10.1145/1595696.1595733	real-time computing;type system;type safety;computer science;programming language;engineering drawing;algorithm;feature model;satisfiability	SE	-21.823903275823756	27.63482659252606	97505
5ab18a0ca3dc4801ebc3e5c4580a11454995cb76	vast: visualization of abstract syntax trees within language processors courses	construction process;abstract syntax trees;abstract syntax tree;visualization	In this poster we present VAST, a visualization tool to support teaching language processors. On the one hand, VAST provides an API that allows generating visualizations of abstract syntax trees (AST). This process is independent of the parser generator. On the other hand, VAST provides an advanced interface to animate the AST construction process and to cope with huge ones.	abstract syntax tree;application programming interface;central processing unit;compiler-compiler;video ad serving template	Francisco J. Almeida-Martínez;Jaime Urquiza-Fuentes;J. Ángel Velázquez-Iturbide	2008		10.1145/1409720.1409759	natural language processing;abstract syntax;visualization;computer science;programming language;homoiconicity;algorithm;abstract syntax tree;syntax error	PL	-29.134253967446572	25.395163201319757	97641
7cae22b9af27053d48d30e44b6dc9af556756639	ocaml for the masses	next language	Why the next language you learn should be functional	ocaml	Yaron Minsky	2011	ACM Queue	10.1145/2030256.2038036	world wide web;computer science	PL	-23.825852724855796	21.217273386506715	97665
77f9cb335b2bceacc92efd9bb095c7b1d83547f6	automatic elicitation system of network service specifications from a set of rules	rule based language state transition diagram automatic elicitation;enhanced state transition model;enhanced state transition model automatic elicitation system network service specifications rule based language state transition diagrams;probability density function;rule based;data mining;computer bugs telecommunication services proposals programming profession;state transition diagrams;system recovery;programming profession;automatic elicitation system;telecommunication services;rule based language;network services;state transition diagram;computer bugs;proposals;telecommunication services knowledge based systems telecommunication networks;automatic elicitation;knowledge based systems;state transition;telecommunication networks;telecommunications;network service specifications	Proposals have been made to describe services programs in a rule-based language, because it is easier to describe programs in a rule-based language than a procedural language. But, it is difficult to find bugs in programs described in a rule-based language. To solve the problem and to make it easier for service developers to find bugs in programs, service specifications can be elicited from programs described in a rule-based language. This paper proposes a system for automatically eliciting state transition diagrams based on an enhanced state transition model from a set of rules.	experimental system;logic programming;procedural programming;rule-based system;software bug;state diagram;state transition table	M. Ohba;T. Ohta	2009	2009 Fifth Advanced International Conference on Telecommunications	10.1109/AICT.2009.27	rule-based system;probability density function;state diagram;software bug;telecommunications;computer science;systems engineering;telecommunications service;theoretical computer science;knowledge-based systems;data mining	SE	-33.39872429522111	29.418803883261994	97700
6cee812c077e6f9fadec0530b8adc806a9f4275e	a formal basis for a program compilation proof tool	k;program assemblers;compilacion;preuve programme;program proof;systeme developpement;lenguaje ensamblador;280403 logics and meanings of programs;langage evolue;refinement method;280300 computer software;langage assembleur;280000 information computing and communication sciences;prueba programa;compilation;lenguaje evolucionado;development systems;assembler;280302 software engineering;methode raffinement;700199 computer software and services not elsewhere classified;high level language;metodo afinamiento;assembleur programme	This paper presents a case study in verified program compilation from high-level language programs to assembler code using the Cogito formal development system. A form of window-inference based on the Z schema is used to perform the compilation. Data-refinement is used to change the representation of integer variables to assembler word locations.		Luke Wildman	2002		10.1007/3-540-45614-7_28	single compilation unit;dynamic compilation;potassium;computer science;just-in-time compilation;compilation error;programming language;high-level programming language;algorithm	PL	-23.321803876720395	28.175542468157513	97819
06ced11dc05609bf022bca60073d999bda33ca49	program annotation in xml: a parse-tree based approach	hypermedia markup languages;xml information analysis computer science program processors independent component analysis reverse engineering java software tools application software visualization;program diagnostics;c programs;java programming;application software;compiler generators;source code annotation;object oriented programming;independent component analysis;program diagnostics compiler generators hypermedia markup languages object oriented programming c language java;objective c programs;program annotation;parse tree;visualization;arbitrary lalr grammar;portable gcc compiler modification;c language;xml;bison parser generator;java programs;software tools;source code;computer science;syntactic tags;information analysis;java programs source code annotation syntactic tags xml program annotation parse tree arbitrary lalr grammar bison parser generator portable gcc compiler modification objective c programs c programs c programs;program processors;reverse engineering;java	In this paper we describe a technique that can be used to annotate source code with syntactic tags in XML format. This is achieved by modifying the parser generator bison to emit these tags for an arbitrary LALR grammar. We also discuss an immediate application of this technique, a portable modification of the gcc compiler, that allows for XML output for C, Objective C, C++ and Java programs. While our approach is based on a representation of the parse-tree and does not have the same semantic richness as other approaches, it does have the advantage of being language independent, and thus re-usable in a number of different domains.	c++;compiler-compiler;gnu compiler collection;java;lalr parser;objective-c;parse tree;parsing;xml	James F. Power;Brian A. Malloy	2002		10.1109/WCRE.2002.1173077	xml validation;independent component analysis;xml encryption;application software;xml;visualization;streaming xml;computer science;theoretical computer science;operating system;xml framework;database;data analysis;programming language;object-oriented programming;java;efficient xml interchange;reverse engineering;source code	SE	-29.280623489486608	27.680171848141555	97872
1044946161a1ab743c59d45c71e0e31724541a95	dynamic aspectj	context awareness;aspectj;aspect scheduling;dynamic aop;aspect oriented programming	This paper considers the difficulties linked to the static scheduling strategy of AspectJ and shows how to overcome them by turning to a more dynamic strategy, making it possible to order, cancel, and deploy aspects at runtime. We show that this more dynamic strategy can be obtained by a minor update of the semantics of AspectJ introducing the notion of current aspect group, that is, the aspects scheduled for the current join point. We show how to reflect this change at the language level and present a prototype of the resulting AspectJ variant, Dynamic AspectJ. This prototype reuses AspectJ to perform a first step of static weaving, which we complement by a second step of dynamic weaving, implemented through a thin interpretation layer. This can be seen as an interesting example of reconciling interpreters and compilers, the dynamic and the static world.	aspectj;compiler;join point;prototype;run time (program lifecycle phase);scheduling (computing)	Ali Assaf;Jacques Noyé	2008		10.1145/1408681.1408689	real-time computing;aspect-oriented programming;computer science;distributed computing;programming language	PL	-26.62962862422524	30.26470905171845	97944
248c814f7f64a0ec36f3d5b0ef7c04dae10da629	interpreting specialization in type theory	type theory;partial evaluation;typed lambda calculus	We de ne the static semantics of o ine partial evaluation for the simply-typed lambda calculus using a translation into a Martin-Lof-style type theory with suitable extensions. Our approach clari es that the distinction between specialization-time and run-time computation in partial evaluation can model the phase distinction between compile-time and run-time computation in a module language. Working backwards from that connection, we de ne partial evaluation for a core language with modules.	compile time;compiler;computation;partial evaluation;partial template specialization;phase distinction;programming language;simply typed lambda calculus;type theory	Peter Thiemann	1999			theoretical computer science;pure type system;cognitive science;curry–howard correspondence;type inhabitation;lambda cube;system f;type constructor;algorithm;computer science;hindley–milner type system;normalisation by evaluation	PL	-22.265488344542224	22.697513819878775	98139
578665a2cb22d6f544eeeea6b567bd8748c9aa63	point-free program transformation	equational reasoning;program transformation;functional programming;point free programming;accumulation strategy;program calculation	Functional programs are particularly well suited to formal manipulation by equational reasoning. In particular, it is straightforward to use calculational methods for program transformation. Well-known transformation techniques, like tupling or the introduction of accumulating parameters, can be implemented using calculation through the use of the fusion (or promotion) strategy. In this paper we revisit this transformation method, but, unlike most of the previous work on this subject, we adhere to a pure point-free calculus that emphasizes the advantages of equational reasoning. We focus on the accumulation strategy initially proposed by Bird, where the transformed programs are seen as higher-order folds calculated systematically from a specification. The machinery of the calculus is expanded with higher-order point-free operators that simplify the calculations. A substantial number of examples (both classic and new) are fully developed, and we introduce several shortcut optimization rules that capture typical transformation patterns.	algorithm;combinatory logic;const (computer programming);currying;exptime;google+;keyboard shortcut;level of detail;mathematical optimization;network address translation;program transformation;programmer;quantum gate;recursion;rewriting;strictness analysis;tacit programming;tree accumulation	Alcino Cunha;Jorge Sousa Pinto	2005	Fundam. Inform.		discrete mathematics;computer science;artificial intelligence;mathematics;programming language;functional programming;algorithm;algebra	PL	-19.555477927921707	23.402988066831917	98297
739c68ce2578e28dca797bf3b1644864e9909cf2	an efficient computation of right context for lr-based error repair	lr parsing;compilateur;syntax error repair;language theory;teoria lenguaje;compiler;compilers;grammaire lr;gramatica lr;analizador sintaxico;parser;lr grammar;analyseur syntaxique;article;theorie langage;compilador;kernel item	The left context in LR-based parsing is the sequence of states in the parsing stack. The right context is the vocabulary strings to appear for a given left context. We propose an efficient method of computing right context for LR-based syntax error repair. The efficiency of our method is achieved from removing some redundancies in the method of previous work.	computation;lr parser;parsing;syntax error;vocabulary	Min-Soo Jung;Kwang-Moo Choe;Taisook Han	1994	Inf. Process. Lett.	10.1016/0020-0190(94)90029-9	natural language processing;compiler;computer science;programming language;algorithm	NLP	-24.66931998067465	24.455004478478514	98334
6b5f6fe36706d745c07df8ad9b19249e48c188cd	spec: a framework for the specification and reuse of uis and their models	lse pub	Implementing UIs is often a tedious task. To address this, UI Builders have been proposed to support the description of widgets, their location, and their logic. A missing aspect of UI Builders is however the ability to reuse and compose widget logic. In our experience, this leads to a significant amount of duplication in UI code. To address this issue, we built Spec: a UIBuilder for Pharo with a focus on reuse. With Spec, widget properties are defined declaratively and attached to specific classes known as composable classes. A composable class defines its own widget description as well as the model-widget bridge and widget interaction logic. This paper presents Spec, showing how it enables seamless reuse of widgets and how these can be customized. After presenting Spec and its implementation, we discuss how its use in Pharo 2.0 has cut in half the amount of lines of code of six of its tools, mostly through reuse. This shows that Spec meets its goals of allowing reuse and composition of widget logic.	composability;duplicate code;hypertext transfer protocol;pharo;seamless3d;software widget;source lines of code;spec#;user interface;wiring	Benjamin Van Ryseghem;Stéphane Ducasse;Johan Fabry	2012		10.1145/2448963.2448965	real-time computing;computer science;database;world wide web	HCI	-29.149463409863205	27.885179198995008	98405
efd24bb336563fce02a8a928980d8718d7d99da0	a toolset for the symbolic examination of finite state transition systems			state transition table	Jürgen Ruf	2000			theoretical computer science;the symbolic;computer science	Logic	-19.83808186839209	18.43114983727553	98478
8b5acf81c59f59c2e19f3cbcfb8f7d19fd2a8b24	executable hybriduml and its application to train control systems	systeme temps reel;formal specification;sistema hibrido;semantica formal;systeme discret;lenguaje uml;formal specification language;langage modelisation unifie;specification programme;formal semantics;specification language;specification formelle;semantique formelle;especificacion formal;control system;systeme commande train;unified modelling language;unified modeling language;hybrid system;real time system;lenguaje especificacion;sistema tiempo real;hybrid automata;sistema discreto;program specification;langage specification;especificacion programa;discrete system;hard real time;systeme hybride	In this paper, the authors introduce an extension of UML for the purpose of hybrid systems modeling. The construction uses the profile mechanism of UML 2.0 which is the standard procedure for extending the Unified Modeling Language. The “intuitive semantics” of the syntactic extension is based on the semantics for hierarchic Hybrid Automata, as suggested by Alur et. al. In contrast to Alur’s formalism, HybridUML allows to label transitions not only with conditions and assignments, but also with signals. Furthermore, our approach associates formal semantics by definition of a transformation from HybridUML specifications into programs of a “low-level” language which is both executable in hard real-time and semantically well-defined. When compared to approaches assigning semantics directly to the high-level constructs of a formal specification language, the transformation approach offers two main advantages: First, semantics can be more easily adapted to syntactic extensions by extending the transformation in an appropriate way. Second, all models are automatically executable, since the low-level language is.	abstract machine;best, worst and average case;central processing unit;christof ebert;code coverage;compiler;concurrency (computer science);control system;encode;embedded system;executable;formal specification;formal system;forward error correction;high- and low-level;hybrid automaton;hybrid system;kernel patch protection;linux;look and feel;low-level programming language;multiprocessing;network switch;parallel computing;real-time clock;real-time computing;real-time locating system;runtime system;schedule (computer science);scheduling (computing);semantics (computer science);shared variables;simulation;specification language;state diagram;system under test;systems modeling;test automation;theory;unified modeling language;windows nt processor scheduling	Kirsten Berkenkötter;Stefan Bisanz;Ulrich Hannemann;Jan Peleska	2004		10.1007/978-3-540-27863-4_10	unified modeling language;computer science;control system;artificial intelligence;programming language;algorithm	PL	-24.69713018432246	31.784363495604218	98548
122578229094a124c9454c922230bf73701e35fd	ezcontract: using marker library and bytecode instrumentation to support design by contract in java	program compilers contracts java;contracts;streamlined integration ezcontract marker library bytecode instrumentation java annotate contracts annotated programs java compilers contract evaluation instructions source compatibility;libraries instruments contracts java program processors runtime specification languages programming code standards software engineering;language extension;design by contract;program compilers;java	Several approaches have been proposed to support Design by Contract in Java. In this paper, through the use of markers which are predefined dummy methods and attributes, a new approach to annotate contracts is presented. The annotated programs can be directly compiled by standard Java compilers. A bytecode instrumentor is developed to manipulate the bytecode to inject contract evaluation instructions and make the contracts executable at runtime. The marker approach avoids two primary problems found in the existing practices: source compatibility that depends on language extension and symbolic barrier that leaves contracts and their targets unrelated. It also facilitates streamlined integration with IDEs and improves readability as well as writability of the contract- annotated programs.	compiler;computer performance;design by contract;dummy variable (statistics);eclipse;executable;integrated development environment;java;plug-in (computing);robustness (computer science);run time (program lifecycle phase)	Yu Chin Cheng;Chien-Tsun Chen;Chin-Yun Hsieh	2007	14th Asia-Pacific Software Engineering Conference (APSEC'07)	10.1109/APSEC.2007.55	java api for xml-based rpc;jsr 94;java concurrency;computer science;design by contract;operating system;software engineering;java modeling language;strictfp;database;real time java;programming language;java;generics in java;scala;java applet;java annotation	PL	-26.846967127390208	28.570836145594697	98826
83ee0174252663c273edcabc863ef22cce29e71f	generalized points-to graphs: a new abstraction of memory in the presence of pointers		ions Fig. 3. Inter-relationships between ideas and algorithms for defining and computing GPUs, GPBs, and GPGs. Each layer is defined in terms of the layers below it. Figure 8 fleshes out this picture by listing specific abstractions, operations, data flow analyses, and optimizations. Given variables x and y and i > 0, j ≥ 0, a generalized points-to update (GPU) x i |j −−→ s y represents a memory transformer in which all locations reached by i − 1 indirections from x in the abstract memory are defined by the pointer assignment labelled s , to hold the address of all locations reached by j indirections from y. The pair i |j represents indirection levels and is called the indlev of the GPU (i is the indlev of x , and j is the indlev of y). The letter γ is used to denote a GPU unless named otherwise. Definition 1. Generalized Points-to Update. 3.1 Defining a Generalized Points-to Graph (GPG) We model the effect of a pointer assignment on an abstract memory by defining the concept of generalized points-to update (GPU) in Definition 1. We use the statement label s to capture weak versus strong updates and for computing points-to information.4 Definition 1 gives the abstract semantics of a GPU. The concrete semantics of a GPU x i |j −−→ s y can be viewed as the following C-style pointer assignment with i − 1 dereferences of x5 and j dereferences of &y: ∗ ∗ . . . ∗ x = ∗ ∗ . . . ∗&y (i − 1) j A GPU γ : x i |j −−→ s y generalizes a points-to edge 6 from x to y with the following properties: • The direction indicates that the source x with indlev i identifies the locations being defined and the targety with indlev j identifies the locations whose addresses are read. • The GPU γ abstracts away i − 1 + j placeholders. • The GPU γ represents may information because different locations may be reached from x and y along different control flow paths reaching the statement s in the procedure. 4We omit the statement labels in GPUs at some places when they are not required. 5Alternatively, i dereferences of &x . We choose i − 1 dereference from x because the left-hand side cannot be &x . 6Although a GPU can be drawn as an arrow just like a points-to edge, we avoid the term ‘edge’ for a GPU because of the risk of confusion with a ‘control flow edge’ in a GPG. 10 Pritam M. Gharat, Uday P. Khedker, and Alan Mycroft Pointer GPU Relevant memory graph assignment after the assignment s : x = &y x 1|0 −−→ s y x y s : x = y x 1|1 −−→ s y x y s : x = ∗y x 1|2 −−→ s y x y s : ∗x = y x 2|1 −−→ s y x y Fig. 4. GPUs for basic pointer assignments in C. In the memory graphs, a double circle indicates the location whose address is being assigned, a thick arrow shows the generated edges. Unnamed nodes may represent multiple pointees (implicitly representing placeholders). A generalized points-to block (GPB), denoted δ , is a set of GPUs abstracting memory updates. A generalized points-to graph (GPG) of a procedure, denoted ∆, is a graph (N ,E) whose nodes in N are labelled with GPBs and edges in E abstract the control flow of the procedure. By common abuse of notation, we often conflate nodes and their GPB labellings. Definition 2. Generalized Points-to Blocks and Generalized Points-to Graphs. We refer to a GPU with i = 1 and j = 0 as a classical points-to edge as it encodes the same information as edges in classical points-to graphs. Example 4. The pointer assignment in statement 01 in Figure 2 is represented by a GPU r 1 |0 −−→ 01 a where the indirection levels (1|0) appear above the arrow and the statement number (01) appears below the arrow. The indirection level 1 in “1|0” indicates that r is defined by the assignment and the indirection level 0 in “1|0” indicates that the address of a is read. Similarly, statement 02 is represented by a GPU q 2 |0 −−→ 02 m. The indirection level 2 for q indicates that some pointee of q is being defined and the indirection level 0 indicates that the address ofm is read. Figure 4 presents the GPUs for basic pointer assignments in C. (To deal with C structs and unions, GPUs are augmented to encode lists of field names—for details see Figure 18). GPUs are useful rubrics of our abstractions because they can be composed to construct new GPUs with smaller indirection levels whenever possible thereby converting them progressively to classical points-to edges. The composition between GPUs eliminates the data dependence between them and thereby, the need for control flow ordering between them. Section 3.2 briefly describes the operations of GPU composition and GPU reduction which are used for the purpose; they are defined formally in later sections. A GPU can be seen as a atomic transformer which is used as a building block for the generalized points-to graph (GPG) as a memory transformer for a procedure (Definition 2). The GPG for a procedure differs from its control flow graph (CFG) in the following way: • The CFG could have procedure calls whereas the GPG does not.7 Besides, a GPG is acyclic in almost all cases, even if the procedure it represents has loops or recursive calls. 7In the presence of recursion and calls through function pointers (Sections 6.2 and 9), we need an intermediate form of GPG called an incomplete GPG containing unresolved calls that are resolved when more information becomes available. Generalized Points-to Graphs: A New Abstraction of Memory in the Presence of Pointers 11 • The GPBs which form the nodes in a GPG are analogous to the basic blocks of a CFG except that the basic blocks are sequences of statements but GPBs are (unordered) sets of GPUs. A concrete semantic reading of a GPB δ is defined in terms of the semantics of executing a GPU (Definition 1). Execution of δ implies that the GPUs in δ are executed non-deterministically in any order. This gives a correct abstract reading of a GPB as a may property. But a stronger concrete semantic reading also holds as a must property: Let δ contain GPUs corresponding to some statement s . Define Xs ⊆ δ by Xs = {x i |j −−→ s y ∈ δ }, Xs , ∅. Then, whenever statement s is reached in any execution, at least one GPU in Xs must be executed. This semantics corresponds to that of the points-to information generated for a statement in the classical points-to analysis. This gives GPBs their expressive power—multiple GPUs arising from a single statement, produced by GPU-reduction (see later), represent may-alternative updates, but one of these must be executed.8 Example 5. Consider a GPB {γ 1 :x 1 |0 −−→ 11 a,γ 2 :x 1 |0 −−→ 11 b,γ 3 :y 1 |0 −−→ 12 c,γ 4 :z 1 |0 −−→ 13 d,γ 5 :t 1 |0 −−→ 13 d, }. After executing this GPB (abstractly or concretely) we know that the points-to sets of x is overwritten to become {a,b} (i.e. x definitely points to one of a and b) because GPUs γ 1 and γ 2 both represent statement 11 and define a single location x . Similarly, the points-to set of y is overwritten to become {c} because γ 3 defines a single location c in statement 12. However, this GPB causes the points-to sets of z and t to include {d} (without removing the existing pointees) because γ 4 and γ 5 both represent statement 13 but define separate locations. Thus, x and y are strongly updated (their previous pointees are removed) but z and t are weakly updated (their previous pointees are augmented). The above example also illustrates how GPU statement labels capture the distinction between strong and weak updates. Themay property of the absence of control flow between the GPUs in a GPB allows us to model a WaR dependence as illustrated in the following example: Example 6. Consider the code snippet on the right. There is a WaR data dependence between 01 y = x; 02 x = &a; statements 01 and 02. If the control flow is not maintained, the statements could be executed in the reverse order and y could erroneously point to a. We construct a GPB {y 1 |1 −−→ 01 x,x 1 |0 −−→ 02 a} for the code snippet. Themay property of this GPB ensures that there is no data dependence between these GPUs. The execution of this GPB in the context of the memory represented by the GPU x 1 |0 −−→ 12 b, computes the points-to information {y− →b, x − →a}. It does not compute the erroneous points-to information y− →a thereby preserving the WaR dependence. Thus, WaR dependence can be handled without maintaining control flow. 3.2 An Overview of GPG Operations Figure 5 lists the GPG operations based on the concept of generalized points-to updates (GPUs). Each layer is defined in terms of the layers below it. For each operation, Figure 5 describes the types of its operands and result, and lists the section in which the operation is defined. 3.2.1 GPU Composition. In a compiler, the sequence p = &a; ∗p = x is usually simplified to p = &a;a = x to facilitate further optimizations. Similarly, the sequence p = &a;q = p is usually simplified to p = &a;q = &a. While both 8A subtlety is that a GPB δ may contain a spurious GPU that can never be executed because the flow functions of points-to analysis are nondistributive [15]. 12 Pritam M. Gharat, Uday P. Khedker, and Alan Mycroft A generalized points-to update (GPU) γ :x i |j −−→ s y Sec . 3. 1 GPU composition γ 1◦ τ γ 2 ◦τ : γ × γ → γ (partial function) Sec . 4. 2 GPU reduction γ ◦R ◦ : γ × R → 2γ Sec . 4. 3 Fig. 5. A hierarchy of core operations involving GPUs. Each operation is defined in terms of the layers below it. The set of GPUs reaching a GPU γ (computed using the reaching GPUs analyses of Sections 4.4 and 4.5) is denoted by R . By abuse of notation, we use γ , δ , and R also as types to indicate the signatures of the operations. The operator “◦ ” is overloaded and can be disambiguated using the types of the operands. simplifications are forms of constant propagation, they play rather different roles, and in the GPG framework, are instances of (respectively) SS and TS variants of GPU composition (Section 4.2). Suppose a GPU γ 1 precedes γ 2 on some control flow path. If there is a RaW dependence between γ 1 and γ 2 then, a GPU composition γ 2 ◦ τγ 1	blocking (computing);data-flow analysis;dataflow architecture;gnu privacy guard;graphics processing unit;inline expansion;integrated information theory;strength reduction	Pritam M. Gharat;Uday P. Khedker;Alan Mycroft	2016	CoRR		pointer (computer programming);parallel computing;program analysis;redundancy (engineering);scalability;control flow;indirection;strength reduction;data flow diagram;computer science	PL	-25.58294148913936	26.203670828091493	98934
9a060663a0856599b80c3b5040412b3ae7e51793	on implementing symmetry detection	graph automorphism;experimental evaluation;automatic symmetry detection	Automatic symmetry detection has received a significant amount of interest, which has resulted in a large number of proposed methods. This paper reports on our experiences while implementing the approach of Puget (CP2005, LNCS, vol. 3709, pp. 475–489. Springer, 2005). In particular, it proposes a modification to the approach to deal with general expressions, discusses the insights gained, and gives the results of an experimental evaluation of the accuracy and efficiency of the approach.	benchmark (computing);binary constraint;constraint (mathematics);constraint algorithm;correctness (computer science);eclipse;graph (abstract data type);graph (discrete mathematics);graph automorphism;graphical user interface;intensional logic;lecture notes in computer science;local consistency;pp (complexity);search tree;sensor;springer (tank);standard boolean model	Christopher Mears;Maria Garcia de la Banda;Mark Wallace	2008	Constraints	10.1007/s10601-008-9057-9	combinatorics;computer science;artificial intelligence;graph automorphism;pure mathematics;mathematics;algorithm	SE	-19.38068317377962	19.601215349065026	99006
11aab53770db4d46bbc0be5fd22b46874eaf9509	partial inversion of constructor term rewriting systems	article publisher;compilateur;inversion;problema inverso;compiler;term rewrite system;automatic generation;inverse problem;rewriting systems;reecriture;rewriting;term rewriting;probleme inverse;systeme reecriture;compilador;reescritura;generic programming	Partial-inversion compilers generate programs which compute some unknown inputs of given programs from a given output and the rest of inputs whose values are already given. In this paper, we propose a partial-inversion compiler of constructor term rewriting systems. The compiler automatically generates a conditional term rewriting system, and then unravels it to an unconditional system. To improve the efficiency of inverse computation, we show that innermost strategy is usable to obtain all solutions if the generated system is right-linear.	compiler;computation;rewriting	Naoki Nishida;Masahiko Sakai;Toshiki Sakabe	2005		10.1007/978-3-540-32033-3_20	inversion;compiler;rewriting;computer science;inverse problem;theoretical computer science;database;mathematics;distributed computing;programming language;generic programming;confluence;algorithm	Logic	-22.712694288830818	28.015204587315523	99200
7ad9f76adfb451308b6291a4924f58dc34a95860	smart composition of game objects using dependency injection	component based architecture;game engine;code reuse;dependency injection;design pattern;design patterns;system architecture	Most game engines are based on inheritance of game objects and/or componentization of behaviors. While this approach enables clear visualization of the system architecture, good code reuse, and fast prototyping, it brings some issues, mostly related to the high dependency between game objects/components instances. This dependency often leads to static casts and null pointer references that are difficult to debug. In this article we propose the use of the dependency injection design pattern to safely initialize game objects and lessen the programmer's role in handling these issues both during the prototyping and production phases. Since these dependencies are attributes of game objects and the injection occurs only at the initialization pass, there is no performance penalty at the game loop.	code reuse;dependency injection;game engine;pointer (computer programming);programmer;software design pattern;systems architecture;type conversion	Erick Baptista Passos;Jonhnny Weslley S. Sousa;Esteban Walter Gonzalez Clua;Anselmo Antunes Montenegro;Leonardo Gresta Paulino Murta	2009	Computers in Entertainment	10.1145/1658866.1658872	software design pattern;real-time computing;simulation;computer science;component-based software engineering;operating system;dependency injection;distributed computing;design pattern;algorithmic game theory;programming language;algorithm;systems architecture	HCI	-28.62929380913295	30.139373210928895	99425
03e4eeb5e3826e9c4225a20e71168ece48c0516d	coeffects: unified static analysis of context-dependence	logics;qa 9 formal systems	Monadic effect systems provide a unified way of tracking effects of computations, but there is no unified mechanism for tracking how computations rely on the environment in which they are executed. This is becoming an important problem for modern software – we need to track where distributed computations run, which resources a program uses and how they use other capabilities of the environment. We consider three examples of context-dependence analysis: liveness analysis, tracking the use of implicit parameters (similar to tracking of resource usage in distributed computation), and calculating caching requirements for dataflow programs. Informed by these cases, we present a unified calculus for tracking context dependence in functional languages together with a categorical semantics based on indexed comonads. We believe that indexed comonads are the right foundation for constructing context-aware languages and type systems and that following an approach akin to monads can lead to a widespread use of the concept. Modern applications run in diverse environments – such as mobile phones or the cloud – that provide additional resources and meta-data about provenance and security. For correct execution of such programs, it is often more important to understand how they depend on the environment than how they affect it. Understanding how programs affect their environment is a well studied area: effect systems [13] provide a static analysis of effects and monads [8] provide a unified semantics to different notions of effect. Wadler and Thiemann unify the two approaches [17], indexing a monad with effect information, and showing that the propagation of effects in an effect system matches the semantic propagation of effects in the monadic approach. No such unified mechanism exists for tracking the context requirements. We use the term coeffect for such contextual program properties. Notions of context have been previously captured using comonads [14] (the dual of monads) and by languages derived from modal logic [12,9], but these approaches do not capture many useful examples which motivate our work. We build mainly on the former comonadic direction (§3) and discuss the modal logic approach later (§5). We extend a simply-typed lambda calculus with a coeffect system based on comonads, replicating the successful approach of effect systems and monads. Examples of Coeffects. We present three examples that do not fit the traditional approach of effect systems and have not been considered using the modal F.V. Fomin et al. (Eds.): ICALP 2013, Part II, LNCS 7966, pp. 385–397, 2013. © Springer-Verlag Berlin Heidelberg 2013 386 T. Petricek, D. Orchard, and A. Mycroft logic perspective, but can be captured as coeffect systems (§1) – the tracking of implicit dynamically-scoped parameters (or resources), analysis of variable liveness, and tracking the number of required past values in dataflow computations. Coeffect Calculus. Informed by the examples, we identify a general algebraic structure for coeffects. From this, we define a general coeffect calculus that unifies the motivating examples (§2) and discuss its syntactic properties (§4). Indexed Comonads. Our categorical semantics (§3) extends the work of Uustalu and Vene [14]. By adding annotations, we generalize comonads to indexed comonads, which capture notions of computation not captured by ordinary comonads.	categorical logic;cloud computing;computation;data-flow analysis;dataflow programming;dependence analysis;distributed computing;effect system;hugo thiemann;icalp;indexed grammar;linear algebra;live variable analysis;liveness;mobile phone;modal logic;monad (functional programming);orchard;requirement;scope (computer science);simply typed lambda calculus;software propagation;static program analysis;type system	Tomas Petricek;Dominic A. Orchard;Alan Mycroft	2013		10.1007/978-3-642-39212-2_35	combinatorics;computer science;artificial intelligence;theoretical computer science;mathematics;programming language;algorithm	PL	-20.800091496503924	29.66114385043413	99657
e0de33bff8d967fcd1b74c347e431a05eaf4517f	generating function versions with rational strictness patterns	lenguaje programacion;compilacion;compilateur;langage ordre 1;programming language;first order language;tratamiento lenguaje;compiler;language processing;traitement langage;langage programmation;compilation;generating function;daisy;lenguaje orden 1;compilador	Abstract   Expression evaluation in lazy applicative languages is usually implemented by an expensive mechanism requiring time and space which may be wasted if the expression eventually needs the values anyway. Strictness analysis, which has been successfully applied to flat domains and higher order functions, is used here to annotate programs in a first order language containing lazy list constructors so that they retain their original behavior, but run more efficiently. In practice, the strictness in fields within these constructors often follows regular patterns that can be finitely represented, especially in programs that manipulate such useful structures as finite or infinite trees. The approach presented here typically generates efficient, mutually recursive function versions for these programs. Weak and strong safety are defined and discussed, and the compiler is shown to be weakly safe. Termination is guaranteed by several factors, including a finite resource which controls the increase in code size, and a regularity constraint placed upon the strictness patterns propagated during compilation.		Cordelia V. Hall;David S. Wise	1989	Sci. Comput. Program.	10.1016/0167-6423(89)90027-0	generating function;compiler;computer science;theoretical computer science;programming language;strictness analysis;algorithm	Logic	-22.0707164957297	25.935615434812668	99796
b76b1d11badefadca1ece402c798b8989b9d011c	step-wise validation of communication protocols and services	developpement logiciel;prueba;trajectoire;protocole transmission;specification;simulation;simulacion;trace;test;consistencia;protocolo transmision;trajectory;especificacion;desarrollo logicial;consistance;software development;communication protocol;validation;trayectoria;traza;consistency;transmission protocol	Abstract   A highly automated approach is proposed for testing the consistency of distinct representations of identical system functionality. This approach is based on dynamic comparison and analysis of observable behaviors presented by system functionality representations given at different levels of abstraction. These representations are encoded in a relatively mechanical way as procedures in sequential PROLOG and thus provide the capability of generating and checking the system functionality they capture. The approach is extremely flexible, straightforward to use, and particularly appropriate to systems whose externally observable behavior can be modeled by finite state automata. The use of this approach is illustrated in the context of OSI communication protocol design and validation.		Hasan Ural;Robert L. Probert	1986	Computer Networks	10.1016/0169-7552(86)90034-6	communications protocol;real-time computing;simulation;computer science;trajectory;software development;trace;software testing;consistency;specification;algorithm	Networks	-31.742668032193848	31.073241775214708	99857
52e322a1cb57ad62c937a36ebe0cfc5ae6936790	property-preserving subnet reductions for designing manufacturing systems with shared resources	verification;fabrication;preservation propriete;fabricacion;concepcion sistema;05bxx;red petri;reversibilite;reduction;transformacion;partage des ressources;design method;reversibility;system design;informatique theorique;resource sharing;manufacturing;particion recursos;reduccion;reversibilidad;transformation;verificacion;property preserving;petri net;robot;reduction method;manufacturing system;conception systeme;reseau petri;68q60;computer theory;informatica teorica	This paper handles two problems in manufacturing system design: resource sharing and system abstraction. In a manufacturing system, resources such as robots, machines, etc. are shared by several processes. When the resources are switched from one process to another, they may need some modifications such as cleaning oil, adding equipments and so on. Previous designing methods assume that the resources have no intermediate modifications. Hence, they need to be extended to handle such kinds of resource-sharing problems. As for abstraction, modeling operations with single places in manufacturing system design is very popular. From the viewpoint of verification, the objective is to verify whether the reduced model has the same desirable properties as the original one. This paper presents three kinds of property-preserving subnet reduction methods. For each reduction method, conditions are presented for ensuring that the properties liveness, boundedness and reversibility are preserved. Applications of these reduction methods to handling the above resource sharing and system abstraction problems are illustrated with an example from the manufacturing system.	subnetwork	H. J. Huang;Li Jiao;To-Yat Cheung	2005	Theor. Comput. Sci.	10.1016/j.tcs.2004.12.010	transformation;robot;shared resource;verification;simulation;reduction;design methods;computer science;mathematics;manufacturing;fabrication;petri net;algorithm;algebra;systems design	ECom	-24.895961193055417	31.63318300648216	99921
a42a78e0015618d566a95975d6fcaa6673c3511e	some challenges for constraint programming	constraint programming	We propose a number of challenges for future constraint programming systems, including improvements in implementation technology (using global analysis based optimization and parallelism), debugging facilities, and the extension of the application domain to distributed, global programming. We also briefly discuss how we are exploring techniques to meet these challenges in the context of the development of the CIAO constraint logic programming system.		Manuel V. Hermenegildo	1996	ACM Comput. Surv.	10.1145/242224.242305	concurrent constraint logic programming;mathematical optimization;constraint programming;declarative programming;constraint satisfaction;programming domain;reactive programming;functional reactive programming;computer science;theoretical computer science;extensible programming;programming paradigm;procedural programming;inductive programming;fifth-generation programming language;programming language;concurrent object-oriented programming	Logic	-23.014704256521078	20.020818998913008	100062
6930e3c8255438054240698e6c2c156987af743e	a modification of the lambda-calculus as a base for functional programming languages	lambda calculus;functional programming language	Church's λ-calculus is modified by introducing a new mechanism, the lambda-bar operator “#”, which neutralizes the effect of one preceeding λ-binding. This operator can be used in such a way that renaming of bound variables in any reduction sequence can be avoided, with the effect that efficient interpreters with comparatively simple machine organization can be designed.	functional programming;lambda calculus	Klaus J. Berkling;Elfriede Fehr	1982		10.1007/BFb0012755	natural language processing;system f;first-generation programming language;higher-order programming;typed lambda calculus;declarative programming;very high-level programming language;programming domain;reactive programming;functional reactive programming;computer science;third-generation programming language;functional logic programming;lambda calculus;mathematics;programming paradigm;inductive programming;fifth-generation programming language;programming language theory;programming language;functional programming;second-generation programming language;comparison of multi-paradigm programming languages;algorithm	PL	-21.982674650335536	23.626623231558014	100070
187ab953dab2f89bbd10c708182cb37f21248c79	control flow reversal for adjoint code generation	control flow;simple branch;adjoint code generation;adjoint code;arbitrary combination;structured program;general control flow graph;simple example;control flow reversal;algorithmic step;structured control flow graph;code reversal;control flow graph;code generation	We describe an approach to the reversal of the control flow of structured programs. It is used to automatically generate adjoint code for numerical programs by semantic source transformation. After a short introduction to applications and the implementation tool set, we describe the building blocks using a simple example. We then illustrate the code reversal within basic blocks. The main part of the paper covers the reversal of structured control flow graphs. We show the algorithmic steps for simple branches and loops and give a detailed algorithm for the reversal of arbitrary combinations of loops and branches in a general control flow graph	algorithm;basic block;code generation (compiler);control flow graph;numerical analysis;source transformation;structured programming	Uwe Naumann;Jean Utke;Andrew Lyons;Michael W. Fagan	2004	Source Code Analysis and Manipulation, Fourth IEEE International Workshop on	10.1109/SCAM.2004.9	computer science;theoretical computer science;basic block;programming language;control flow;algorithm;code generation;control flow graph	Arch	-22.814971290374817	24.840523429201816	100141
1c49abae1c1183d5bae45a797a1ecd78639bfa17	formal semantics and soundness of a translation from event-b actions to sql statements		The EventB2SQL tool translates Event-B models to persistent Java applications that store the state of the model in a relational database. Most Event-B assignments are translated directly to SQL database modification statements, which can then be executed against the database. In this work, we present a formal semantics for and prove the soundness of the translation of sets of assignment statements representing the actions of an Event-B event. This allows the generated code to be used with confidence in its correctness.	b-method;correctness (computer science);java;relational database;sql;semantics (computer science)	Tim Wahls	2016	CoRR		stored procedure;computer science;data mining;autocommit;database;programming language	PL	-21.734097666807383	26.486608774153645	100340
bcd9bb468d57f647b8584d64ef5f81e2b452e4fc	a look at the design of lua		Simplicity, small size, portability, and embeddability set Lua apart from other scripting languages.	lua;scripting language;software portability	Roberto Ierusalimschy;Luiz Henrique De Figueiredo;Waldemar Celes	2018	Commun. ACM	10.1145/3186277	programming language;computer science	Theory	-28.6385710656052	28.634733651927203	100407
6badb632fd0d900424348bc768704e1bffda93c9	graph creation, visualisation and transformation	programming language;user interface;computer model;software engineering;strategic interaction;graph rewriting;logic in computer science	We describe a tool to create, edit, visualise and compute with interaction nets — a form of graph rewriting systems. The editor, called GraphPaper, allows users to create and edit graphs and their transformation rules using an intuitive user interface. The editor uses the functionalities of the TULIP system, which gives us access to a wealth of visualisation algorithms. Interaction nets are not only a formalism for the specification of graphs, but also a rewrite-based computation model. We discuss graph rewriting strategies and a language to express them in order to perform strategic interaction net rewriting.	algorithm;graph rewriting;model of computation;pattern matching;rewrite (programming);semantics (computer science);tulip;usability;user interface;visual programming language	Maribel Fernández;Olivier Namet	2010		10.4204/EPTCS.21.1	computer simulation;computer science;theoretical computer science;abstract semantic graph;graph;programming language;user interface;algorithm;graph rewriting	HCI	-32.191536242711535	23.933812936773418	100619
e6da833c16d96f43d14df1a88b9b65ee4fe083e6	efficient edge storage management based on near real-time forecasts		Nowadays, data analytics is utilized on edge based systems to perform near real-time decisions in proximity of the user. When performing near real-time decisions on the Edge, we need historical data to perform accurate data analytics. Since storage capacities on the Edge are limited, we are faced with a challenge to balance the quantity of data stored with the quality of near real-time decisions. In this paper, we present a three-layer architecture model for data storage management on the Edge including an adaptive algorithm that dynamically finds a trade-off between providing high forecast accuracy necessary for efficient real-time decisions, and minimizing the amount of data stored in the space-limited storage. We focus on time series data, typical in the context of sensor-based monitoring in IoT environments. By using the proposed approach it is possible to reduce the amount of stored data by an average 80.27% without affecting specified threshold for prediction accuracy.	adaptive algorithm;computation;computer data storage;missing data;real-time clock;real-time computing;run time (program lifecycle phase);simulation;three-layer architecture;time series	Ivan Lujic;Vincenzo De Maio;Ivona Brandic	2017	2017 IEEE 1st International Conference on Fog and Edge Computing (ICFEC)	10.1109/ICFEC.2017.9	architecture;time series;adaptive algorithm;data mining;real-time computing;data analysis;internet of things;computer data storage;computer science	Embedded	-31.940631466498147	18.361653530270527	100681
afb7196258821a665241a044dc4f26a415eee899	formalizing and verifying a modern build language	qa mathematics inc computing science	CLOUDMAKE is a software utility that automatically builds executable programs and libraries from source code—a modern MAKE utility. Its design gives rise to a number of possible optimizations, like cached builds, and the executables to be built are described using a functional programming language. This paper formally and mechanically verifies the correctness of central CLOUDMAKE algorithms. The paper defines the CLOUDMAKE language using an operational semantics, but with a twist: the central operation exec is defined axiomatically, making it pluggable so that it can be replaced by calls to compilers, linkers, and other tools. The formalization and proofs of the central CLOUDMAKE algorithms are done entirely in DAFNY, the proof engine of which is an SMT-based program verifier.	algorithm;apevia;axiomatic system;bottom-up parsing;cache (computing);compiler;correctness (computer science);executable;functional programming;lazy evaluation;library (computing);linker (computing);make;mathematical optimization;operational semantics;programming language;recursion;simultaneous multithreading;traverse;top-down and bottom-up design;turing completeness;verification and validation	Maria Christakis;K. Rustan M. Leino;Wolfram Schulte	2014		10.1007/978-3-319-06410-9_43	computer science;theoretical computer science;software engineering;programming language;algorithm	PL	-20.520735891435468	26.414807741474043	100776
0c8b92cb23fe044d83520d483f965385ea8e3727	new families of combinators for efficient list manipulation	new family;efficient list manipulation	In most contemporary functional programming languages, the list data type is one of the primary compound data domains provided to the programmer. Lists are appropriate for a large class of problems; however, the lack of arraylike operators (e.g., index) makes lists an inefficient way to represent direct-access data structures. Although these types of operators can be added to a programming language without disturbing the desirable properties of the language (e.g., referential transparency), these features will most likely then be translated into machine-level constructs that still do not process lists in an efficient manner. In this article, we focus on improving the list-processing capabilities of a functional language that is implemented by a combinator-based graph reduction architecture. We introduce new families of combinators that express certain commonly occurring combinations of lists or elements of lists as functions of those lists or elements of lists. A reduction rule (defined for each family) states that when a combinator is applied to a list with a finite series of elements, the resulting combination reduces to a certain combination of the list elements. An efficient implementation of these combinators demands a list representation that is different from the conventional recursively defined list structure. For comparison purposes, however, the reduction semantics of the new combinators will be represented as reductions of equivalent strings of already known combinators.	apl;combinatory logic;compiler;data structure;functional programming;graph reduction;imperative programming;operational semantics;programmer;programming language;recursion;recursive definition;referential transparency;sequential access	S. Mansoor Sarwar;James A. Davis	1994	Journal of Systems and Software	10.1016/0164-1212(94)90027-2	difference list;list;computer science;theoretical computer science;programming language;algorithm	PL	-21.968081115088793	24.279307374489903	101042
007c183a5c34404d644254cb5dea414a2b205b9e	the semantics of spectrum	spectrum;formal semantics;information presentation	The Spectrum project concentrates on the process of developing well-structured, precise system speci cations. Spectrum is a speci cation language, with a deduction calculus and a development methodology. An informal presentation of the Spectrum language with many examples illustrating its properties is given in [2, 3]. The purpose of this article is to describe its formal semantics.	natural deduction;semantics (computer science)	Radu Grosu;Franz Regensburger	1993		10.1007/3-540-58233-9_7	natural language processing;formal semantics;formal semantics;linguistics;programming language;operational semantics;denotational semantics;computational semantics	PL	-25.41464574275586	20.02935578229805	101198
fdc842e409794e19cc2e4216a833b4fb1cfe85bb	case-based exploration of bidirectional transformations in qvt relations		QVT Relations (QVT-R), a standard issued by the Object Management Group, is a language for the declarative specification of model transformations. This paper focuses on a particularly interesting feature of QVT-R: the declarative specification of bidirectional transformations. Rather than writing two unidirectional transformations separately, a transformation developer may provide a single relational specification which may be executed in both directions. This approach saves specification effort and ensures the consistency of forward and backward transformations. This paper explores QVT-R’s support for bidirectional model transformations through a spectrum of transformation cases. The transformation cases vary with respect to several factors such as the size of the transformation definition or the relationships between the metamodels for source and target models. The cases are solved in QVT-R, but may be applied to other bidirectional transformation languages, as well; thus, they may be used as a benchmark for comparing bidirectional transformation languages. In our work, we focus on the following research questions: functionality of bidirectional transformations in terms of relations between source and target models, solvability (which problems may be solved by a single relational specification of a bidirectional transformation), variability (does a bidirectional transformation contain varying elements, i.e., elements being specific to one direction), comprehensibility (referring to the ease of understanding and constructing QVT-R transformations), and the semantic soundness of bidirectional transformations written in QVT-R.	benchmark (computing);bidirectional transformation;declarative programming;metamodeling;model transformation;qvt;spatial variability;transformation language	Bernhard Westfechtel	2016	Software & Systems Modeling	10.1007/s10270-016-0527-z	computer science;artificial intelligence;theoretical computer science;algorithm	PL	-28.064034677421112	20.83364808477696	101289
8cd942baa6b58f7030f12fd17e62744c3580aa43	polymorphic type analysis in logic programs by abstract intepretation	polymorphism;logic programs	interpretation frameworks. It is also fully automated in that its only inputs are the program to be analysed and type de nitions for the function symbols in the program. References [1] H. Azzoune. Type inference in Prolog. In E. Lusk and R. Overbeek, editors, Proceedings of the ninth International Conference on Automated Deduction, pages 258{277, Argonne, Illinois, USA, May 23-26 1988. Springer-Verlag. Springer-Verlag Lecture Notes in Computer Science 310. [2] R. Barbuti and R. Giacobazzi. A bottom-up polymorphic type inference in logic programming. Science of computer programming, 19(3):133{181, 1992. [3] A. Bossi, M. Gabbrielli, G. Levi, and M. Meo. A compositional semantics for logic programs. Journal of Theoretical Computer Science, 122(1-2):3{47, 1994. [4] M. Bruynooghe. Adding redundancy to obtain more reliable and more readable Prolog programs. In Proceedings of the rst International Logic Programming Conference, pages 129{133, Marseille, France, 1982. [5] M. Bruynooghe. A practical framework for the abstract interpretation of logic progams. Journal of Logic Programming, 10(2):91{124, 1991. [6] M. Bruynooghe, G. Janssens, A. Callebaut, and B. Demoen. Abstract interpretation: towards the global optimisation of Prolog programs. In Proceedings of the 1987 Symposium on Logic Programming, pages 192{204. The IEEE Society Press, 1987. [7] L. Cardelli and P. Wegner. On understanding types, data abstraction, and polymorphism. ACM computing surveys, 17(4):471{522, 1985. [8] W.F. Clocksin and C. Mellish. Programming in Prolog. Springer-Verlag, 1984. [9] M. Codish, D. Dams, and Yardeni E. Derivation and safety of an abstract uni cation algorithm for groundness and aliasing analysis. In K. Furukawa, editor, Proceedings of the Eighth International Conference on Logic Programming, pages 79{93, Paris, France, 1991. The MIT Press. [10] M. Codish and B. Demoen. Deriving polymorphic type dependencies for logic programs using multiple incarnations of Prop. In Proceedings of the First Symposium on Static Analysis, volume 864 of Lecture Notes in Computer Science, pages 281{297. SpringerVerlag, September 1994. [11] M.M. Corsini and K. Musumbu. Type inference: a new approach. Journal of Theoretical Computer Science, 119(1):23{38, 1993. 53 [12] A. Cortesi, B. Le Charlier and P. van Hentenryck. Combinations of Abstract Domains for Logic Programming. The Conference Record of the 21st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, pages 227{239, Portland, Oregon, January 17-21, 1994. [13] P. Cousot and R. Cousot. Abstract interpretation: a uni ed framework for static analysis of programs by construction or approximation of xpoints. In Proceedings of the fourth annual ACM symposium on Principles of programming languages, pages 238{252, Los Angeles, California, 1977. [14] P. Cousot and R. Cousot. Abstract interpretation and application to logic programs. Journal of Logic Programming, 13(1, 2, 3 and 4):103{179, 1992. [15] M. A. Covington, D. Nute, and A. Vellino. PROLOG PROGRAMMING IN DEPTH. Scott, Foresman & Co., 1988. [16] P.W. Dart and J. Zobel. A regular type language for logic programs. In Frank Pfenning, editor, Types in Logic Programming. MIT Press, Cambridge,Massachusetts, 1992. [17] M. Falaschi, G. Levi, and C. Palamidessi. Declarative modelling of the operational behavior of logic programs. Theoretical Computer Science, 69:289{318, 1989. [18] T.W. Fr uehwirth. Using meta-interpreters for polymorphic type checking. In M. Bruynooghe, editor, Proceedings of the Second Workshop on Meta-programming in Logic, pages 339{351, Leuven, Belgium, April 4-6 1990. [19] N. Heintze and J. Ja ar. A nite presentation theorem for approximating logic programs. In The seventh Annual ACM Symposium on Principles of Programming Languages, San Francisco, California, January 17-19 1990. The ACM Press. [20] N. Heintze and J. Ja ar. Semantic types for logic programs. In Frank Pfenning, editor, Types in Logic Programming. MIT Press, Cambridge,Massachusetts, 1992. [21] M. Hermenegildo, R. Warren, and S.K. Debray. Global ow analysis as a practical compilation tool. Journal of Logic Programming, 13(1, 2, 3 and 4):349{366, 1992. [22] K. Horiuchi and T. Kanamori. Polymorphic type inference in Prolog by abstract interpretation. In K. Furukawa, H. Tanaka, and T. Fujisaki, editors, Proceedings of the Sixth Conference on Logic Programming, pages 195{214, Tokyo, June 1987. [23] D. Jacobs and A. Langen. Static analysis of logic programs for independent and parallelism. Journal of Logic Programming, 13(1, 2, 3 and 4):291{314, 1992. [24] G. Janssens and M. Bruynooghe. Deriving descriptions of possible values of program variables by means of abstract interpretation. Journal of Logic Programming, 13(1, 2, 3 and 4):205{258, 1992. 54 [25] N.D. Jones and H. S ndergarrd. A semantics-based framework for abstract interpretation of Prolog. In S. Abramsky and C. Hankin, editors, Abstract interpretation of declarative languages, pages 123{142. Ellis Horwood Limited, 1987. [26] T. Kanamori. Abstract interpretation based on Alexander templates. Journal of Logic Programming, 15(1 & 2):31{54, January 1993. [27] T. Kanamori and K. Horiuchi. Type inference in Prolog and its application. In Proceedings of the ninth International Joint Conference on Arti cial Intelligence, pages 704{707, 1985. [28] T. Kanamori and T. Kawamura. Abstract interpretation based on OLDT resolution. Journal of Logic Programming, 15(1 & 2):1{30, January 1993. [29] J.W. Lloyd. Foundations of Logic Programming. Springer-Verlag, 1987. [30] L. Lu. Abstract interpretation, bug detection and bug diagnosis in normal logic programs. PhD thesis, University of Birmingham, 1994. [31] L. Lu. Type analysis of logic programs in the presence of type de nitions. In Proceedings of the ACM SIGPLAN Symposium on Partial Evaluation and Semantics-Based Program Manipulation (PEPM'95), pages 241{252, La Jolla, California, June 21-23 1995. The ACM Press. [32] K. Marriott and H. S ndergaard. Semantics-based data ow analysis of logic programs. In G. Ritter, editor, Information Processing'89. North-Holland, 1989. [33] K. Marriott and H. S ndergaard. Bottom-up data ow analysis of normal logic programs. Journal of Logic Programming, 13(1, 2, 3 and 4):181{204, 1992. [34] K. Marriott. Frameworks for abstract interpretation. Acta Informatica, 30(2):103{129, 1993. [35] K. Marriott, H. S ndergaard and N.D. Jones. Denotational Abstract Interpretation of Logic Programs. ACM Transactions on Programming Languages and Systems, 16(3):607{ 648, 1994. [36] C. Mellish. Abstract interpretation of Prolog programs. In S. Abramsky and C. Hankin, editors, Abstract interpretation of declarative languages, pages 181{198. Ellis Horwood Limited, 1987. [37] P. Mishra. Towards a theory of types in Prolog. In Proceedings of the IEEE international Symposium on Logic Programming, pages 289{298. IEEE, 1984. [38] K. Muthukumar and M. Hermenegildo. Compile-time derivation of variable dependency using abstract interpretation. Journal of Logic Programming, 13(1, 2, 3 and 4):315{347, 1992. 55 [39] A. Mycroft and R.A. O'Keefe. A polymorphic type system for Prolog. Arti cial Intelligence, 23:295{307, 1984. [40] U. Nilsson. Towards a framework for the abstract interpretation of loic programs. In P. Deransart, B. Lorho, and J. Ma luszynski, editors, Proceedings of the International Workshop on Programming Language Implementation and Logic Programming, pages 68{82, Orl eans, France, 1988. Springer-Verlag. [41] R. A. O'Keefe. Finite xed-point problems. In J.-L. Lassez, editor, Proceedings of the fourth International Conference on Logic programming, volume 2, pages 729{743. The MIT Press, 1987. [42] T. Sato and H. Tamaki. Enumeration of success patterns in logic programs. Journal of Theoretical Computer Science, 34(1):227{240, 1984. [43] H. S ndergaard. An application of abstract interpretation of logic programs: occur check problem. In Proceedings of the European symposium on programming, pages 324{338. Springer-Verlag, 1986. [44] L. Sterling and E. Shapiro. The Art of Prolog. The MIT Press, 1986. [45] J. Tiuryn. Type inference problems: A survey. In B. Roven, editor, Proceedings of the Fifteenth International Symposium on Mathematical Foundations of Computer Science, pages 105{120, Berlin,Heidelberg, 1990. Springer-Verlag. [46] M.H. van Emden and R.A. Kowalski. The semantics of predicate logic as a programming language. Arti cial Intelligence, 23(10):733{742, 1976. [47] P. van Hentenryck, A. Cortesi and B. Le Charlier. Type Analysis of Prolog Using Type Graphs. Journal of Logic Programming, 22(3):179{209, 1995. [48] J. Xu and D.S. Warren. A type inference system for Prolog. In R.A. Kowalski and K.A. Bowen, editors, Proceedings of the fth International Conference and Symposium on Logic Programming, pages 604{619. The MIT Press, 1988. [49] E. Yardeni and E. Shapiro. A type system for logic programs. Journal of Logic Programming, 10(2):125{153, 1991. [50] J. Zobel. Derivation of polymorphic types for Prolog programs. In J.-L. Lassez, editor, Logic Programming: Proceedings of the fourth international conference, pages 817{838, Australia, 1987. 56	acm computing surveys;acm sigact;acm transactions on programming languages and systems;apl;abstract interpretation;abstraction (software engineering);acta informatica;algorithm;aliasing;approximation;automated theorem proving;bottom-up parsing;bruce ellis;compiler;computer programming;conference on automated deduction;dart (programming language);declarative programming;frank pfenning;global optimization;inference engine;international conference on logic programming;jolla;jones calculus;journal of logical and algebraic methods in programming;lu decomposition;lecture notes in computer science;linear algebra;mathematical optimization;metaprogramming;natural deduction;occurs check;parallel computing;partial evaluation;proceedings of the ieee;programming language implementation;prolog;software bug;springer (tank);symposium on principles of programming languages;theoretical computer science;top-down and bottom-up design;type inference;type system;type theory;warren abstract machine;zobel network	Lunjin Lu	1998	J. Log. Program.	10.1016/S0743-1066(97)10010-3	polymorphism;discrete mathematics;computer science;parametricity;mathematics;programming language;algorithm	Logic	-19.486520067036395	20.222503139071893	101313
1e69cba2422364b5b6df00230f4f51d030f62e61	types and effects for non-interfering program monitors	policy enforcement;formal specication;security policy	A run-time monitor is a program that runs in parallel with an untrusted application and examines actions from the application’s instruction stream. If the sequence of program actions deviates from a specified security policy, the monitor transforms the sequence or terminates the program. We present the design and formal specification of a language for defining the policies enforced by program monitors. Our language provides a number of facilities for composing complex policies from simpler ones. We allow policies to be parameterized by values or other policies, and we define operators for forming the conjunction and disjunction of policies. Since the computations that implement these policies modify program behavior, naive composition of computations does not necessarily produce the conjunction (or disjunction) of the policies that the computations implement separately. We use a type and effect system to ensure that computations do not interfere with one another when they are composed.	computation;effect system;formal specification;run time (program lifecycle phase)	Lujo Bauer;Jay Ligatti;David Walker	2002		10.1007/3-540-36532-X_10	real-time computing;computer science;distributed computing;algorithm	PL	-21.652564671772424	30.508775864141146	101376
54015252063595929c407403c68b60a7b4ec1d77	re-engineering needs generic programming language technology	compiler construction;lenguaje programacion;representacion conocimientos;compilateur;programming language;specialized application languages;maintenance;programming environment generator;programming environment;language technology;generic languqge technology;software systems;generic language technology;ingenieria logiciel;compiler;software engineering;universiteitsbibliotheek;data representation;analisis programa;systeme conversationnel;medio ambiente programacion;interactive system;compiler construction techniques;system renovation;re engineering;sistema conversacional;genie logiciel;mantenimiento;langage programmation;intermediate data representation;program analysis;analyse programme;knowledge representation;representation connaissances;compilador;environnement programmation;reverse engineering;generic programming	Generic language technology and compiler construction techniques are a prerequisite to build analysis and conversion tools that are needed for the re-engineering of large software systems. We argue that generic language technology is a crucial means to do fundamental re-engineering. Furthermore, we address the issue that the application of compiler construction techniques in re-engineering generates new research questions in the field of compiler construction.	compiler;generic programming;language technology;programming language;software system	Mark van den Brand;Paul Klint;Chris Verhoef	1997	SIGPLAN Notices	10.1145/251621.251633	program analysis;compiler;dynamic compilation;compiler correctness;computer science;theoretical computer science;compiler construction;external data representation;programming language;generic programming;language technology;intrinsic function;functional compiler;algorithm;reverse engineering;software system	PL	-25.667677737287917	23.917610356817463	101458
75cf6e01e6f19197f34ef03b95cd3f40eb18965c	fether: an extensible definitional interpreter for smart-contract verifications in coq		Recently, blockchain technology, which adds records to a list using cryptographic links, has been widely applied in the financial field. Therefore, the security of blockchain smart contracts is among the most popular contemporary research topics. To improve the theorem-proving technology in this field, we are developing an extensible hybrid verification tool chain, denoted as FSPVM-E, for Ethereum smart contract verification. This hybrid system extends the Coq proof assistant, a formal proof-management system. Combining symbolic execution with higher-order theorem-proving, it solves consistency, automation, and reusability problems by standard theorem-proving approaches. This article completes the FSPVM-E by developing its proof engine. FSPVM-E is an extensible definitional interpreter based on our previous work FEther, which is totally developed in the Coq proof assistant. It supports almost all semantics of the Solidity programing language, and simultaneously executes multiple types of symbols. FEther also contains a set of automatic strategies that execute and verify the smart contracts in Coq with a high level of automation. The functional correctness of FEther was verified in Coq. The execution efficiency of FEther far exceeded that of the interpreters which are developed in Coq in accordance with the standard tutorial. To our knowledge, FEther is the first definitional interpreter of the Solidity language in Coq. Keyword: symbolic execution; formal verification; smart contract; Coq; Etheruem; definitional interpreter	bitcoin;coq (software);correctness (computer science);cryptography;definition;ethereum;formal proof;formal verification;high-level programming language;hybrid system;proof assistant;smart contract;solidity;symbolic execution;toolchain	Zheng Yang;Hang Lei	2018	CoRR		programming language;automation;distributed computing;correctness;smart contract;hybrid system;cryptography;computer science;proof assistant;interpreter;symbolic execution	PL	-20.7812424645662	26.98239465272587	101479
2b5eb50ae8b3258ed3f8985f2c084f99bda74b14	model checking machine code with the gnu debugger	modelizacion;atomicidad;verificacion modelo;calculateur embarque;verification modele;semantics;abstraction;simultaneidad informatica;program verification;abstraccion;semantica;semantique;modelisation;verificacion programa;concurrency;model checking;atomicity;atomicite;design and implementation;boarded computer;state explosion;verification programme;modeling;simultaneite informatique;calculador embarque;embedded software	Embedded software verification is an important verification problem that requires the ability to reason about the timed semantics of concurrent behaviors at a low level of atomicity. The level of atomicity is the smallest execution block (such as a machine instruction or a C instruction) that cannot be split by an interrupt. Combining a cycleaccurate debugger with model checking algorithms provides an accurate model of software execution at the machine-code level while supporting concurrency and allowing abstractions to manage state explosion. We report on the design and implementation of such a model checker using the GNU debugger (gdb) with different processor backends. A significant feature of the resulting tool is that we can adjust the level of atomicity during the model checking run to reduce state explosion while focusing on behaviors that are likely to generate an error.	algorithm;atomicity (database systems);concurrency (computer science);embedded software;gnu debugger;machine code;model checking;software verification	Eric Mercer;Michael D. Jones	2005		10.1007/11537328_20	model checking;real-time computing;systems modeling;concurrency;embedded software;computer science;operating system;database;semantics;abstraction;programming language;atomicity	Logic	-22.8834165060416	31.338083015921946	101505
2fc2c598d100a64fc4feef70018df8c3c656d6d0	figaro: yet another constraint programming library	constraint programming	Existing libraries and languages for nite domain constraint programming usually have depth-rst search (with branch and bound) built-in as the only search algorithm. Exceptions are the languages claire and Oz, which support the programming of diierent search algorithms through special purpose programming language constructs. The goal of this work is to make abstractions for programming search algorithms available in a language-independent setting. Figaro is an experimentation platform being designed to study non-standard search algorithms, diierent memory policies for search (trailing vs copying), consistency algorithms, failure handling and support for modeling. This paper focuses on the use and implementation of such abstractions for investigating programmable search algorithms and memory policies in a C++ constraint programming library.	branch and bound;c++;canonical account;claire;consistency model;constraint programming;language-independent specification;library (computing);programming language;search algorithm;yet another	Martin Henz;Tobias Müller;Ka Boon Ng	1999	Electr. Notes Theor. Comput. Sci.	10.1016/S1571-0661(05)80107-0	constraint logic programming;concurrent constraint logic programming;constraint programming;constraint satisfaction;programming domain;computer science;fifth-generation programming language;programming language	PL	-21.11324706933198	23.600555010204772	101548
1abc3169c9686112cb00069d52a897ca9a7ad0ec	the constrained-monad problem	monads;class constraints;deep embeddings;institutional repository research archive oaister;haskell	In Haskell, there are many data types that would form monads were it not for the presence of type-class constraints on the operations on that data type. This is a frustrating problem in practice, because there is a considerable amount of support and infrastructure for monads that these data types cannot use. Using several examples, we show that a monadic computation can be restructured into a normal form such that the standard monad class can be used. The technique is not specific to monads, and we show how it can also be applied to other structures, such as applicative functors. One significant use case for this technique is domain-specific languages, where it is often desirable to compile a deep embedding of a computation to some other language, which requires restricting the types that can appear in that computation.	a-normal form;applicative programming language;compiler;computation;domain-specific language;haskell;monad (functional programming)	Neil Sculthorpe;Jan Bracker;George Giorgidze;Andy Gill	2013		10.1145/2500365.2500602	computer science;theoretical computer science;programming language;monad;algorithm	PL	-23.324240215987576	25.79413527901997	101553
dcc314d51b57f7fe248ddf5ffc1132c33dbc456e	glossary of z notation	z notation	Names a,b identiﬁers d,e declarations (e.g., a:A; b,...:B ...) f,g functions m,n numbers p,q predicates s,t sequences x,y expressions A,B sets C,D bags Q,R relations S,T schemas X schema text (e.g., d, d|p or S)		Jonathan P. Bowen	1995	Information & Software Technology	10.1016/0950-5849(95)90001-2	x-ray notation;computer science;z notation;software engineering;steinhaus–moser notation;programming language	SE	-26.36758718469477	19.67595553759805	101631
d3ee1049568d1ec586a09fea8610f08481d17d4e	jade: a graphical tool for fast development of imaging applications	software tool;image processing;java programming;indexing terms;development environment;graphic user interface;image analysis;source code;program specification;user interaction	  This paper presents a novel graphic tool to develop imaging applications. Users interact with this tool by means of constructing  a DIP graph, which is a series of nodes and edges indicating the processing flow of the images to be analyzed. Solutions created  using our tool can run inside the developing environment and also we can get the equivalent Java source code; so that, we  can reused the code in other platforms. Another advantage of our software tool is the fact that users can easily propose and  construct new algorithms following the Java beans rules. Our proposal can be seen as a DIP compiler because our tool produces  fullfunctional Java programs that can solve an specific problem. The program specification is not a text based one, but a  graphic specification and that is one of the main contributions of this work.    	jade	Alberto Chávez-Aragón;Leticia Flores-Pulido;Edgar Alfredo Portilla-Flores;Oleg Starostenko;Gustavo Rodríguez Gómez	2008		10.1007/978-90-481-3660-5_69	image analysis;index term;human–computer interaction;image processing;computer science;operating system;graphical user interface;development environment;programming language;source code	EDA	-32.765490541771754	24.560288747968155	101810
3020e0d793fee5a036bdaf1f45c8ac19770b8f82	a technique for speeding up lr(k) parsers	parser optimization;single productions;parsing;lr k parsing;compiling	We present a new transformation that reduces the size and increases the speed of ${\text{LR}}(k)$ parsers. This transformation can be applied to all ${\text{LR}}(k)$ parsers including those produced by Knuth’s and DeRemer’s techniques. The transformation causes the parser to avoid reductions by productions of the form $A \to B$, where A and B are nonterminals.	parsing	Alfred V. Aho;Jeffrey D. Ullman	1973	SIAM J. Comput.	10.1137/0202010	natural language processing;compiler;speech recognition;computer science;parsing;programming language	Theory	-24.501483721789693	24.425470629733518	101933
5d6ac29c42cc66f08900c772b5a53ec7342bfa1a	towards an accurate mathematical model of generic nominally-typed oop		The construction of GNOOP as a domain-theoretic model of generic nominally-typed OOP is currently underway. This extended abstract presents the concepts of ‘nominal intervals’ and ‘full generification’ that are likely to help in building GNOOP as an accurate mathematical model of generic nominally-typed OOP. The abstract also presents few related category-theoretic suggestions. The presented concepts and suggestions are particularly geared towards enabling GNOOP to offer a precise and simple view of so-far-hard-to-analyze features of generic OOP such as variance annotations (e.g., Java wildcard types) and erased generics (e.g., Java type erasure).	category theory;domain theory;java applet;mathematical model;type erasure	Moez A. AbdelGawad	2016	CoRR		wildcard;computer science;theoretical computer science;programming language;algorithm	SE	-24.98168687282891	25.603399969292635	102146
be430fd02a2b8fc049fd18350b8dc051049f57f6	code: a unified approach to parallel programming	general and miscellaneous mathematics computing and information science;programming environments;dependency types;programming language;parallel programming concurrent computing parallel architectures parallel processing parallel languages prototypes manufacturing industries user interfaces memory architecture computer architecture;dependence graph;parallel programming;fill in templates;firing rules code computation oriented display environment modular parallel programs fill in templates expressive power dependency types generalized dependency graphs unified parallel computation model abstraction level;unified parallel computation model;expressive power;computer architecture;firing rules;programming environments parallel programming;abstraction level;parallel computer;computer codes;computation oriented display environment;programming 990200 mathematics computers;code;dependent types;parallel architecture;parallel programs;modular parallel programs;parallel processing;programming languages;generalized dependency graphs	The authors describe CODE (computation-oriented display environment), which can be used to develop modular parallel programs graphically in an environment built around fill-in templates. It also lets programs written in any sequential language be incorporated into parallel programs targeted for any parallel architecture. Broad expressive power was obtained in CODE by including abstractions of all the dependency types that occur in the widely used parallel-computation models and by keeping the form used to specify firing rules general. The CODE programming language is a version of generalized dependency graphs designed to encode the unified parallel-computation model. A simple example is used to illustrate the abstraction level in specifying dependencies and how they are separated from the computation-unit specification. The most important CODE concepts are described by developing a declarative, hierarchical program with complex firing rules and multiple dependency types.<<ETX>>	abstraction layer;code (programming language);declarative programming;encode;expressive power (computer science);model of computation;parallel computing;programming language	James C. Browne;Muhammad Azam;Stephen Sobeck	1989	IEEE Software	10.1109/52.31648	parallel processing;dependent type;computer science;theoretical computer science;distributed computing;programming language;code;expressive power;source code	PL	-31.204828290449505	24.092871165862146	102183
57c1f77db060c0f6c1ca1c713fc37a11d5d26888	safe clone-based refactoring through stereotype identification and iso-generation	clones;maintainability;refactoring;safety;cobol;design rules;spectrum;software maintenance;cloning;databases;programming;generators;skeleton	Most advanced existing tools for clone-based refactoring propose a limited number of pre-defined clone-removal transformations that can be applied automatically, typically under user control. This fixed set of refactorings usually guarantee that semantics is preserved, but is inherently limited to generally-applicable transformations (extract method, pull-up method, etc.). This tool design rules out many potential domain-specific or application-specific clone removals. Such cases are ordinarily recognized by humans as stereotypes derived from a higher-level concept and manually replaced with an appropriate abstraction. Thus, in current tools, generality is sacrificed for the safety of the transformation. This paper proposes an alternative approach, in which the spectrum of refactoring techniques is open, including manual interventions, while keeping strong safety guarantees based on the notion of iso-generation. Our method can operate on multiple languages and has been prototyped on a subset of a real-world legacy asset containing C and COBOL programs, with promising results.	cobol;code generation (compiler);code refactoring;preprocessor;programming language;semiconductor industry;sensor;stereotype (uml);user interface	Nic Volanschi	2012	2012 6th International Workshop on Software Clones (IWSC)		computer science;systems engineering;database;programming language	SE	-26.724481888495365	28.20439471020696	102369
844aa56d9eb5118d3884dfede8f8b03ca61d0891	a theorem about automatic programming	partial information;automatic programming	"""An automatic programming system is a system (usually a programmed computer) that generates a program from partial information about that program (Figure 1). Looked at in this general way, automatic programming systems are hardly new. Assemblers are automatic programming systems. So are compilers, debugging systems, and even loaders. In this paper, I propose to focus on systems that """"do more"""" than such systems in the sense that they require less complete information about the programs they are to generate."""	admissible numbering;automatic programming;compiler;computer;debugging	Peter Kugel	1975	SIGART Newsletter	10.1145/1045231.1045233	programming domain;reactive programming;computer science;theoretical computer science;programming paradigm;procedural programming;symbolic programming;inductive programming;programming language;system programming;algorithm;concurrent object-oriented programming	PL	-27.070115562405356	24.383546168951803	102437
1b2cfb8f0a6f8398a96dd5874b9156270081c4ff	a tool for the synthesis of cryptographic orchestrators	secure service composition;temporal logic;partial model checking;synthesis of functional and secure processes;process algebras	Security is one of the main challenges of service oriented computing. Services need to be loosely coupled, easily accessible and yet provide tight security guarantees enforced by cryptographic protocols. In this paper, we address how to automatically synthesize an orchestrator process able to guarantee the secure composition of electronic services, supporting different communication and cryptographic protocols. We present a theoretical model based on process algebra, partial model checking and logical satisfiability, plus an automated tool implementing the proposed theory.	business process execution language;cryptographic protocol;cryptography;deadlock;loose coupling;model checking;process calculus;programming language;service-oriented architecture;theory	Vincenzo Ciancia;Fabio Martinelli;Ilaria Matteucci;Marinella Petrocchi;José Antonio Martín;Ernesto Pimentel	2012		10.1145/2422498.2422508	cryptographic primitive;computer science;theoretical computer science;cryptographic protocol;database;distributed computing	Security	-33.41636918237035	31.432091523270284	102507
545828cfa0e31c872b75b437e10a8f565fc8af2f	functional un|unparsing	acm press;functional un;acm conference;new york;programming languages;annual acm symposium;heterogeneous sequence;j. acm;heterogeneous argument;mit press;functional programming	Danvy’s functional unparsing problem (Danvy 1998) is to implement a typesafe ‘printf’ function, which converts a sequence of heterogeneous arguments to a string according to a given format. The dual problem is to implement a type-safe ‘scanf’ function, which extracts a sequence of heterogeneous arguments from a string by interpreting (Friedman and Wand 1984, 2008) the same format as an equally heterogeneous sequence of patterns that binds zero or more variables. We derive multiple solutions to both problems (Wand 1980b) from their formal specifications (Wand 1982b). On one hand, our solutions show how the Hindley-Milner type system, unextended, permits accessing heterogeneous sequences with the static assurance of type safety. On the other hand, our solutions demonstrate the use of control operators (Felleisen et al. 1988; Meyer and Wand 1985; Wand 1985) to communicate with formats as coroutines (Haynes et al. 1984; Wand 1980a).	coroutine;duality (optimization);hindley–milner type system;mitchell wand;scanf format string;type safety;unparser	Kenichi Asai;Oleg Kiselyov;Chung-chieh Shan	2011	Higher-Order and Symbolic Computation	10.1007/s10990-012-9087-2	computer science;artificial intelligence;algorithm	PL	-23.125027396893607	23.66609478769811	102516
9ef3d4bbcb0caff6a0f7b4bd2b4451140981b8cf	think pad: a graphical system for program-ming by demonstration	dynamic programming;atherosclerosis dynamic programming programming profession programming environments data structures workstations transaction databases functional programming binary trees testing;programming environments;atherosclerosis;testing;functional programming;binary trees;graphics system;transaction databases;data structures;programming profession;workstations;data structure	Specifying complex programs may become easier if the programming system allows the programmer to manipulate data structures graphically.	admissible numbering;data structure;graphical user interface;programmer	Robert V. Rubin;Eric J. Golin;Steven P. Reiss	1985	IEEE Software	10.1109/MS.1985.230354	first-generation programming language;constraint programming;computer architecture;protocol;declarative programming;workstation;delegation;data structure;programming domain;binary tree;reactive programming;functional reactive programming;computer science;theoretical computer science;extensible programming;dynamic programming;functional logic programming;software testing;programming paradigm;event-driven programming;procedural programming;symbolic programming;inductive programming;fifth-generation programming language;programming language;system programming;concurrent object-oriented programming	Embedded	-30.567000924074087	24.423944553444993	102554
6783aa1fcd3e972a5e3c9a16c910ccf351c55724	on predictive parsing and extended context-free grammars	right-hand side;top-down predictive parser construction;context-free grammar;partial syntax tree;extended context-free grammar;novel view;predictive parsing;java toolkit;finite language;continuing investigation;regular language;context free grammar;top down	Extended context-free grammars are context-free grammars in which the right-hand sides of productions are allowed to be any regular language rather than being restricted to only finite languages. We present a novel view on top-down predictive parser construction for extended context-free grammars that is based on the rewriting of partial syntax trees. This work is motivated by our development of ECFG, a Java toolkit for the manipulation of extended context-free grammars, and by our continuing investigation of XML.	context-free grammar;parsing expression grammar;recursive descent parser	Anne Brüggemann-Klein;Derick Wood	2003		10.1007/3-540-36477-3_6	natural language processing;context-sensitive grammar;tree-adjoining grammar;indexed grammar;l-attributed grammar;deterministic context-free grammar;parsing expression grammar;phrase structure grammar;computer science;s-attributed grammar;extended affix grammar;database;automaton;definite clause grammar;context-free grammar;programming language;stochastic context-free grammar;embedded pushdown automaton;immediate constituent analysis;algorithm;c-command	NLP	-24.710196290052732	23.688563142498413	102637
a11f81fc27df87459f2533ad354184de9d04046c	the expression problem, trivially!	extensibility;object oriented programming;design patterns;modularity;expression problem	This paper presents a novel and simple solution to Wadler’s Expression Problem that works in conventional object-oriented languages. Unlike all existing solutions in Java-like languages, this new solution does not use any kind of generics: it relies only on subtyping. The key to the solution is the use of covariant type refinement of return types (or fields): a simple feature available in many object-oriented languages, but not as widely known or used as it should be. We believe that our results present valuable insights for researchers and programming language designers interested in extensibility. Furthermore our results have immediate applicability as practical design patterns for programmers interested in improving extensibility of their programs.	benchmark (computing);bitwise operation;design pattern;expectation propagation;expression problem;extensibility;functional programming;haskell;java;programmer;programming language;recursion;refinement (computing);type system	Yanlin Wang;Bruno C. d. S. Oliveira	2016		10.1145/2889443.2889448	computer science;theoretical computer science;programming language;algorithm	PL	-23.297687081408476	26.484786593450266	102679
853d07cd013698fec50dc896d26a0efef259aa91	a type-safe macro system for xml	time complexity;document processing	XML is originally designed for the use in document processing. Every classical document processing system supports a mechanism which is called macro. In general, a macro is a named collection of actions. These actions are executed whenever the name of the macro appears in a document instance. We describe a very small but powerful macro language for XML. The presented language supports macros with arguments. As a main result we will show that for a macro expansion H and a recognizable set R the set H-1(R) of trees t, whose macro expansion H(t) are in R, is recognizable. This implies that the typechecking problem for macro expansion is decidable. A major drawback of this approach is the time complexity. Therefore we designed a practical typechecking discipline. We equip every macro definition with a type expression describing the input forests. On the basis of this type information the macro processor computes an output type for the macro. Having inferred this output type, we aim at extending the given output DTD to a DTD for admissible input possibly containing unexpanded macros. A Type-safe Macro System for XML Table of	computer science;document processing;general-purpose macro processor;helmut schwarz;markup language;perst;programmer;recognizable set;time complexity;type system;world wide web;xml schema	Thomas Perst;Helmut Seidl	2002			time complexity;document processing;theoretical computer science;macro;mathematics;algorithm	PL	-23.354127573719413	23.795896065648858	102768
3235b27709b4c9aaad5d34b4f012ebe8581d9d86	verified peephole optimizations for compcert	verification;compcert;coq;formal methods;compilers;peek	Transformations over assembly code are common in many compilers. These transformations are also some of the most bug-dense compiler components. Such bugs could be elim- inated by formally verifying the compiler, but state-of-the- art formally verified compilers like CompCert do not sup- port assembly-level program transformations. This paper presents Peek, a framework for expressing, verifying, and running meaning-preserving assembly-level program trans- formations in CompCert. Peek contributes four new com- ponents: a lower level semantics for CompCert x86 syntax, a liveness analysis, a library for expressing and verifying peephole optimizations, and a verified peephole optimiza- tion pass built into CompCert. Each of these is accompanied by a correctness proof in Coq against realistic assumptions about the calling convention and the system memory alloca- tor. Verifying peephole optimizations in Peek requires prov- ing only a set of local properties, which we have proved are sufficient to ensure global transformation correctness. We have proven these local properties for 28 peephole transfor- mations from the literature. We discuss the development of our new assembly semantics, liveness analysis, representa- tion of program transformations, and execution engine; de- scribe the verification challenges of each component; and detail techniques we applied to mitigate the proof burden.	assembly language;calling convention;compcert;compiler;coq (software);correctness (computer science);formal verification;live variable analysis;peek;peephole optimization;program transformation;software bug;tor messenger;verification and validation;x86	Eric Mullen;Daryl Zuniga;Zachary Tatlock;Dan Grossman	2016		10.1145/2908080.2908109	compiler;verification;formal methods;peephole optimization;computer science;theoretical computer science;programming language;algorithm;peek	PL	-20.986756859741728	27.74270919231044	102827
e691fe71eab166e1e3d365c599ec9511a34696b9	object logic integration: a multiparadigm design methodology and a programming language	programming language;object oriented programming;logic programming;multiparadigm programming languages;logic programs;design methodology	In the past decade, there has been much research eeort dedicated to combine the object-oriented programming paradigm and the logic programming paradigm. Most of this eeort sheds light upon the philosophy of multiparadigm programming as a near ideal mental model for a wide class of problem domains. In this paper we propose a scheme for object and logic integration|the OLI scheme. This scheme contributes to the multiparadigm programming philosophy by putting forward a multiparadigm design methodology and describing a multi-paradigm programming language. Above all, the OLI scheme integrates the object-oriented and the logic programming paradigms at the design and language levels with a precise and well-balanced interface so that each paradigm shares an equal and cooperating partnership in problem analysis and problem solving. An important property of the OLI language is that programmers can program either in one of the paradigms alone or in a mixed paradigm without sacriicing expressiveness and eeciency. We give a formal deenition of the OLI language and study its semantics both from the logical perspective and the object-oriented perspective. By viewing objects as an enrichment of the Herbrand universe, we deene the declarative and operational semantics of OLI. We show that OLI's operational semantics, a generalized form of SLD-resolution, is sound and complete. From the object-oriented point of view, the logic part of OLI is simply an object with logic programs as states and methods for performing logical deduction.	apl;gene ontology term enrichment;logic programming;mental model;natural deduction;operational semantics;point of view (computer hardware company);problem domain;problem solving;programmer;programming language;programming paradigm;sld resolution;term algebra	Jimmy Ho-Man Lee;P. K. C. Pun	1997	Comput. Lang.	10.1016/S0096-0551(97)00004-0	first-generation programming language;declarative programming;very high-level programming language;horn clause;design methods;programming domain;reactive programming;computer science;functional logic programming;programming paradigm;procedural programming;symbolic programming;inductive programming;fifth-generation programming language;programming language theory;programming language;object-oriented programming;well-founded semantics;prolog;logic programming;algorithm	PL	-25.035214745565344	21.73721906647823	102897
406539c46b7ac0f8ac1d6295dfea64406467088e	logical composition of object-oriented interfaces	logical form;object oriented;interface design	This paper describes an approach to object-oriented interface design that goes beyond mere object decomposition. In our user interface management system we use logic and filters to declaratively specify and control a space of ways that objects may be composed to create interfaces. A filter is a package of constraints and associated typed objects that express the relationship of data and representation objects. Conceptually our system is completely based on constraints. Filters provide the high bandwidth constraints to maintain the components of the direct-manipulation interface while the logic forms the low bandwidth constraints to combine and provide communication between these components. The use of Horn-clause logic to compose separate interface objects facilitates both the distribution of computation onto multiple processors and the generation of multiple views of data. Intelligent backtracking implemented in the logic allows for user- and system-initiated undo operations to correct errors and/or try alternative approaches to a problem. We illustrate the power and flexibility of this approach by describing a floor layout and design system.	backtracking;central processing unit;computation;constraint (mathematics);declarative programming;direct manipulation interface;graphical user interface;horn clause;internet access;logic form;type system;undo;user interface management systems	Mark Grossman;Raimund K. Ege	1987		10.1145/38765.38834	real-time computing;logical form;computer science;theoretical computer science;interface design;distributed computing;programming language;object-oriented programming	PL	-27.072001082920124	26.985540607231933	102943
6f0980e0f2896c9ab8cf8211b5afc136e5280784	a logic programming approach for planning workflows evolutions			logic programming	Gianluigi Greco;Antonella Guzzo;Domenico Saccà	2003			inductive programming;programming language;theoretical computer science;functional logic programming;concurrent constraint logic programming;logic programming;workflow;computer science;prolog	AI	-23.427646945129897	20.75686104626564	103218
13808131fb9b47ee7249eda5086ec10cdef98aed	customizing the visualization and interaction for embedded domain-specific languages in a structured editor	libraries;programming environments;human computer interaction;visual programming embedded domain specific languages large software projects internal domain specific language ide convenient specialized notations structured code editor edsl code net code contracts api editor customization;dsl;prototypes;contracts;visual programming;embedded systems;visualization;human computer interaction programming environments embedded domain specific languages structured editors editor customization visual programming;syntactics;application program interfaces;visual programming application program interfaces embedded systems programming languages;visualization contracts dsl syntactics context libraries prototypes;editor customization;embedded domain specific languages;structured editors;context;programming languages	Large software projects are often based on libraries that provide abstractions for a particular domain such as writing database queries, staging, or constraint solving. The API provided by such a library can be considered a domain-specific language within the implementation language of the library, a so-called internal or embedded domain-specific language (eDSL). Embedding a DSL leverages the tool infrastructure of the host language, but also restricts the syntax and IDE support to that of the host language. This restriction prevents programmers from using convenient specialized notations and, thus, has a negative effect on their productivity. To address this problem, we outline concepts for a structured code editor that enable developers of eDSLs to customize how eDSL code is rendered and what interactions are available. We demonstrate the benefits of our approach by customizing a structured editor for the .NET Code Contracts API. Our prototype shows in particular that we can customize many aspects of visualization and interaction with little effort.	application programming interface;constraint satisfaction problem;database;design by contract;digital subscriber line;disk staging;domain-specific language;embedded system;integrated development environment;interaction;library (computing);object language;programmer;prototype;structure editor	Dimitar Asenov;Peter Müller	2013	2013 IEEE Symposium on Visual Languages and Human Centric Computing	10.1109/VLHCC.2013.6645255	computer science;theoretical computer science;programming language	SE	-28.835461215283054	28.430488167246715	103331
1473f26f851e25293cbdef172ae66ad7b39e22f8	formal techniques for java-like programs (ftfjp)	article in monograph or in proceedings	The dynamic frames approach has proven to be a powerful formalism for modular specification and verification of object-oriented programs. However, the approach requires writing and checking frame annotations. In this paper, we propose a variant of the dynamic frames approach that eliminates the need to explicitly write and check frame annotations. Reminiscent of separation logic’s frame rule, programmers write accessibility predicates inside preand postconditions instead of writing frame annotations. From the precondition one can then infer an upper bound on the set of locations writable or readable by the corresponding method. We implemented our approach in a tool, and used it to automatically verify several challenging examples, including the iterator and observer patterns.	accessibility;design pattern;framing (world wide web);iterator;java;postcondition;precondition;programmer;semantics (computer science);separation logic	Alessandro Coglio;Marieke Huisman;Joseph Kiniry;Peter Müller;Erik Poll	2004		10.1007/978-3-540-30554-5_8	computer science	SE	-22.466354309723076	26.44261048648976	103361
8aad461119c393ff531b70f1232084566d11ed95	forging a silver bullet from the essence of software	encapsulation;developpement logiciel;software;lenguaje programacion;iterative method;representation;instruccion condicional;iterative process;metodo matematico;conceptual content;mathematical method;conceptualization;programmation;programming language;logiciel;structure programme;program design;conception programme;encapsulacion;instruction conditionnelle;programacion;metodo iterativo;conceptualizacion;proceso iterativo;processus iteratif;estructura programa;object oriented;methode iterative;desarrollo logicial;software development;methode mathematique;langage programmation;oriente objet;logicial;conditional instruction;multiple subdomain;program structure;orientado objeto;programming;conceptual construct;conceptualisation;concepcion programa;representacion	Most improvements in software development technology have occurred by eliminating the accidental aspects of the technology. Further progress now depends on addressing the essence of software. Fred Brooks has characterized the essence of software as a complex construct of interlocking concepts. He concludes that no silver bullet will magically reduce the essential conceptual complexity of software. This paper expands on Brooks's definition to lay a foundation for forging a possible silver bullet. Discussed are the three essential attributes of software entities from which a number of consequences arise in software development: (1) conceptual content, (2) representation, and (3) multiple subdomains. Four basic approaches to develop technologies are proposed that directly address the essential attributes. Although some of these technologies require additional development or testing, they present the most promise for forging a silver bullet. Among them, design reabstraction addresses the most difficult attribute, multiple subdomains, and the most difficult consequence, enhancing existing code, making it the best prospect.	no silver bullet	Robert G. Mays	1994	IBM Systems Journal	10.1147/sj.331.0020	conceptualization;programming;encapsulation;computer science;artificial intelligence;software development;iterative and incremental development;program design language;iterative method;programming language;object-oriented programming;representation;algorithm	Logic	-30.73848131469512	22.0153906883553	103439
89a9fa6adb14ebb37420f609751ae88b8f443616	refinement of history-based policies	finite subset;large class;non history-based policy;monitoring rule;policy set;efficient method;non history-based form;history-based policy;logic program	We propose an efficient method to evaluate a large class of history-based policies written as logic programs. To achieve this, we dynamically compute, from a given policy set, a finite subset of the history required and sufficient to evaluate the policies. We maintain this history by monitoring rules and transform the policies into a non history-based form. We further formally prove that evaluating history-based policies can be reduced to an equivalent, but more efficient, evaluation of the non history-based policies together with the monitoring rules.	authorization;automated theorem proving;correctness (computer science);recursion (computer science);refinement (computing);system monitoring	Jorge Lobo;Jiefei Ma;Alessandra Russo;Emil C. Lupu;Seraphin B. Calo;Morris Sloman	2011		10.1007/978-3-642-20832-4_18	computer science;operations management;data mining;algorithm	DB	-21.447711909975894	30.308009311646654	103556
9850b6fda267aa281d4358329c87be2a7e1be63d	seal: a domain-specific language for novice wireless sensor network programmers	seals;computer languages;computer aided instruction;temperature sensors;novice wireless sensor network programmers seal general purpose programming languages scientific incentives financial incentives wireless sensor network applications qualified computer science professional sensor network programming language syntax readable code run time efficiency domain specific language;sensor networks;specification languages;professional aspects;programming profession;seals wireless sensor networks programming profession temperature sensors computer languages;computational linguistics;wireless sensor networks computational linguistics computer aided instruction professional aspects specification languages telecommunication engineering education;telecommunication engineering education;programming languages sensor networks novice programming;novice programming;wireless sensor networks;programming languages	A lot of the prospective wireless sensor network users are novice programmers. Their experience in general-purpose programming languages is either limited or completely nonexistent. There are both financial and scientific incentives to empower these users and allow them to write sensor network applications on their own, rather than having to rely on a qualified computer science professional. We present SEAL, a sensor network programming language designed for novice programmers. SEAL manages to avoid computer science concepts that are hard to grasp for novices, while remaining suitable for typical sensor network application scenarios. The language is extensible in application-specific way, has easy-to-learn syntax and allows to implement common sensor network tasks by writing compact, readable code. It is also shown to have high run-time efficiency.	binary code;compiler;computer network programming;computer science;domain-specific language;general-purpose programming language;overhead (computing);programmer;prospective search;random-access memory;seal (cipher);sensor web	Atis Elsts;Janis Judvaitis;Leo Selavo	2013	2013 39th Euromicro Conference on Software Engineering and Advanced Applications	10.1109/SEAA.2013.16	simulation;wireless sensor network;computer science;engineering;theoretical computer science;computational linguistics;operating system;software engineering;programming language;computer engineering	Mobile	-29.563540583030782	23.08010629506199	103723
38abb94a01fcd1ee848f6982068acaa95bbeed64	special issue on synchronization and concurrency in object-oriented languages	object oriented language		concurrency (computer science);synchronization (computer science)	Timothy L. Harris;Doug Lea	2006	Sci. Comput. Program.	10.1016/j.scico.2006.06.001	natural language processing;object-based language;object language;computer science;theoretical computer science;programming language;object-oriented programming;second-generation programming language	PL	-24.80611775851806	22.34947668148601	103845
50e4f17521c1f58cb3328765cbad2787c05b5674	static analysis of parallel logic programs	static analysis		logic programming;static program analysis	Saumya K. Debray	1988			programming language;logic synthesis;logic optimization;ladder logic;logic family;concurrent constraint logic programming;static analysis;computer science;prolog	AI	-19.805369969364097	20.514128519481293	103891
99c94562db01f101128a823b59f217adc30c29f8	aspect-oriented prolog in a language processing context	grammar;language processor;legibility;program compiler aspect oriented prolog language processing context logic grammar prolog programming techniques domain specific language;prolog;orientado aspecto;program compiler;tratamiento lenguaje;object oriented programming;langage dedie;prolog grammars object oriented programming program compilers;grammars;analyse syntaxique;language processing;prolog programming techniques;analisis sintaxico;ligne abonne numerique;digital subscriber line;grammaire;syntactic analysis;traitement langage;domain specific language;aspect oriented prolog;language processing context;aspect oriented;legibilidad;lisibilite;logic grammar;programa tratamiento lenguaje;program compilers;programme traitement langage;gramatica;oriente aspect;linea abonado digital;lenguaje dedicado	Language processors can be derived from logic grammars. That several concerns in the processor such as parsing, several kinds of analysis or transformations, can be specified as aspects of the logic grammar is demonstred. For that purpose, the authors bring the concepts of aspect-oriented programming to Prolog in a systematic way, based on established Prolog technology. The authors illustrate that typical Prolog programming techniques can be described as generic aspects and provided in a library to support reusable concerns. A domain-specific language (DSL) is developed to improve readability of aspect-oriented specifications.	aspect-oriented programming;central processing unit;digital subscriber line;domain-specific language;parsing;prolog	Wolfgang Lohmann;Günter Riedewald;Guido Wachsmuth	2008	IET Software	10.1049/iet-sen:20070064	natural language processing;constraint programming;digital subscriber line;aspect-oriented programming;horn clause;computer science;domain-specific language;parsing;grammar;definite clause grammar;programming language;object-oriented programming;prolog;logic programming;algorithm	PL	-25.323054224793122	23.924101441348856	103955
5bfe9e3df3ba192c2a92ba863bba955bb309ac60	spécialisation et sous-typage : thème et variations	contravariance.;subtyping;genericity;mots-clés : langage à objets;généricité;sous-typage;inheritance;multi- ple dispatch;héritage;contravariance. keywords:object-oriented languages;covariance;sélection mul- tiple;specialization;spécialisation;multiple dispatch;object oriented language;type system;type safety;object oriented	Class specialization is the most original feature of object orientation, but identifying it to subtyping leads to the well known covariance-contravariance controversy. Type safety requires contravariance while specialization needs covariance. This paper aims to precisely analyse this problem, to show how irreductible it is and the need for type errors. We show that many alternatives as multiple dispatch, genericity or matching cannot solve the problem. Thus, a realistic solution is to adopt covariant redefinition as the basis of a type system where type errors are explicited, as they should be at the analysis and design stages. MOTS-CLÉS :langage à objets, spécialisation, sous-typage, généricité, héritage, sélection multiple, covariance, contravariance.	dynamic dispatch;generic programming;multiple dispatch;partial template specialization;type safety;type system;whole earth 'lectronic link;word lists by frequency	Roland Ducournau	2002	Technique et Science Informatiques		computer science;database	PL	-26.25928522913879	21.203906128478696	104020
f14c206ab65c1b452d9a47dbcfa3bd38610936a6	state-transition machines, revisited	symbolic computation;krivine machine;lambda calculus;continuation passing style;calculo simbolico;denotational semantic;denotational semantics;calcul symbolique;state transition;defunctionalization	In the autumn of 1978, Neil Jones and Steve Muchnick, working at the University of Kansas, were studying compiler synthesis from Scott-Strachey denotational-semantics definitions; I was Neil’s student. Neil read intently John Reynolds’s 1972 paper, Definitional Interpreters for Higher-Order Programming Languages [14], and applied Reynolds’s continuation-passing and defunctionalization transformations to lambda-calculus-coded denotational-semantics definitions, using the transformed definitions as templates for syntax-directed translation. Neil dubbed the translated source programs, “State-Transition Machines” (STMs), because an object program was a set of equationally defined functions that looked like the transition rules of a finite-state machine. Our initial efforts were spent on transforming denotational definitions of block-structured, imperative languages into compiling schemes that generated STMs that looked like ordinary assembly code. In the summer of 1979, Steve moved to the University of California, Berkeley, and Neil and I left for the University of Aarhus, Denmark, where we continued the research project. A summary of the work was eventually published as the paper, Compiler generation from denotational semantics [10]. The continuation-passing and defunctionalization transforms were tedious, and I suggested to Neil that one could do better by writing a translator from lambda-calculus into STMs and then constructing a compiler by composing a denotational definition with the lambdacalculus translator. It was unclear whether this tactic would generate better target code than that generated by Neil’s smart transformations, but Neil agreed that it was worth a try. After several false starts, I formulated a translator from a call-byvalue lambda-calculus to STMs written in a variant of Landin’s SECDmachine, which later appeared in [16] as the “VEC-machine.” (Neil preferred a call-by-value lambda-calculus metalanguage.) At the same time, I was reading Chris Wadsworth’s paper, The relation between computational and denotational properties for Scott’s models of the lambda-calculus [18], and I was fascinated by Wadsworth’s use	assembly language;bsd;compiler;continuation;continuation-passing style;definition;defunctionalization;denotational semantics;executable;finite-state machine;google summer of code;higher-order programming;imperative programming;jones calculus;lambda calculus;production (computer science);syntax-directed translation	David A. Schmidt	2007	Higher-Order and Symbolic Computation	10.1007/s10990-007-9017-x	defunctionalization;symbolic computation;normalisation by evaluation;computer science;continuation-passing style;theoretical computer science;lambda calculus;programming language;denotational semantics of the actor model;denotational semantics;algorithm	PL	-21.7654159205214	25.759358235316437	104214
2a78c1849b32bb953397f21c3a47d84d0d4d8db8	operational and logical semantics for polling real-time systems	systeme temps reel;semantica operacional;controleur logique programmable;automata programable;controlador logica programable;real time;operational semantics;fault tolerant system;semantique operationnelle;duration calculus;programmable logical controller;system synthesis;synthese systeme;programmable automaton;sistema tolerando faltas;sintesis sistema;systeme tolerant les pannes;real time system;source code;timed automata;sistema tiempo real;external research report;automate programmable;real time systems	PLC Automata are a class of real time automata suitable to describe the behaviour of polling real time systems PLC Automata can be compiled to source code for PLCs a hardware widely used in industry to control processes Also PLC Automata have been equipped with a logical and operational semantics using Duration Calculus DC and Timed Automata TA respectively The three main results of this paper are A simpli ed operational semantics A minor extension of the logical semantics and a proof that this semantics is complete relative to our operational semantics This means that if an observable satis es all formulas of the DC seman tics then it can also be generated by the TA semantics A proof that the logical semantics is sound relative to our operational semantics This means that each observable that is accepted by the TA semantics constitutes a model for all formulas of the DC semantics	automata theory;compiler;duration calculus;observable;operational semantics;real-time operating system;real-time transcription;timed automaton	Henning Dierks;Ansgar Fehnker;Angelika Mader;Frits W. Vaandrager	1998		10.1007/BFb0055334	duration calculus;fault tolerance;real-time computing;real-time operating system;action semantics;type erasure;failure semantics;computer science;artificial intelligence;theoretical computer science;proof-theoretic semantics;formal semantics;database;programming language;well-founded semantics;operational semantics;denotational semantics;algorithm;source code	Logic	-24.271037273770656	32.017761966522464	104549
2586d8ecf99f8c9806277450fc0d5ef33d8ab74d	formal methods adoption: what's working, what's not!	verification;transferencia tecnologia;lenguaje programacion;programming language;eves system;methode formelle;specification language;formal method;technology transfer;research and development;model checking;langage programmation;lenguaje especificacion;verificacion;langage specification;transfert technologie	Drawing from the author’s twenty years of experience in formal methods research and development, and, particularly, with the EVES-based systems, this paper provides personal impressions on what is and what is not working with regards to the adoption and application of formal methods. As both the community’s understanding of technology transfer issues and formal methods technology improve, one is optimistic that formal methods will play an increasingly important role in industry. However, significant impediments continue to exist with, perhaps, the increasing complexity of systems being both a blessing and a curse.	formal methods	Dan Craigen	1999		10.1007/3-540-48234-2_6	model checking;verification;formal methods;specification language;computer science;artificial intelligence;programming language;algorithm	AI	-23.820477571134603	31.036319996486835	104796
6623ca1130f0781b5bce11a1ed80de86743d9f4f	hermes language experiences	lenguaje programacion;distributed system;communication process;interprocess communication;systeme reparti;programming language;programmation repartie;distributed programs;system programming;program verification;proceso comunicacion;processus communication;verificacion programa;sistema repartido;programmation systeme;programacion sistema;data aggregation;distributed programming;typestate checking;langage programmation;process model;verification programme;hermes	We recount and examine experiences with Hermes, an experimental language for programming distributed systems. Hermes has several unusual language features, including compile-time checking of data initialization, representation independent data aggregates, and an integrated process model. To facilitate compile-time initialization checking, Hermes stores data in tables and does not expose pointers. We study these features in light of the experiences of Hermes users around the world, analyzing the strengths and weaknesses of the language.	compile time;compiler;distributed computing;process modeling	Willard Korfhage;Arthur P. Goldberg	1995	Softw., Pract. Exper.	10.1002/spe.4380250404	data aggregator;real-time computing;computer science;operating system;process modeling;database;programming language;system programming;inter-process communication	PL	-23.754858484457706	31.512707418272846	104860
a1e70bd1873f596ab9ccfea3d1325260ca128405	kblab: an equational theorem prover for the macintosh	theorem prover	KBlab is a Completion based theorem prover for equational logic, written in the language C and developed on the Macintosh in the MPW (Macintosh Programmer Workshop) programming environment. The core of KBlab is the Knuth–Bendix Completion Procedure (KB) [9,7,1], extended to Unfailing Knuth– Bendix (UKB) [5], S–strategy [5] and inductive theorem proving (IKB). IKB implements the Huet–Hullot method for inductionless induction [8] and the Fribourg linear strategy [3]. The Knuth–Bendix ordering [9] and both the multiset extension and the lexicographic extension of the recursive path ordering are available.	automated theorem proving;integrated development environment;knuth–bendix completion algorithm;lexicographical order;macintosh programmer's workshop;path ordering (term rewriting);programmer	Maria Paola Bonacina;Giancarlo Sanna	1989		10.1007/3-540-51081-8_135	discrete mathematics;computer science;mathematics;automated theorem proving;programming language;algorithm	Logic	-23.50887580227861	22.056136760342618	104919
180a59ba03199e64c7b0bd46dac2bca45593406a	a per model of secure information flow in sequential programs	programa;security properties;probabilistic covert channels;confidencialidad;security analysis;formal specification;flow;program;relation equivalence;semantic security;publikationer;securite;semantica formal;sequential program;powerdomains;lenguaje;semantics;langage;oleada;konferensbidrag;analyse temporelle;secure information flow;formal semantics;flujo informacion;flow models;programme sequentiel;semantica;semantique;analisis temporal;equivalence;analisis programa;time analysis;confidentiality;specification formelle;semantique formelle;flux information;modele ecoulement;especificacion formal;modele semantique;confidentialite;covert channel;semantic model;non interference;information flow;equivalence relation;first order;safety;artiklar;rapporter;programme;binding time analysis;flot;program analysis;partial equivalence relations;higher order functions;language;analyse programme;security;seguridad;relacion equivalencia;correctness proof;equivalencia;noninterference	This paper proposes an extensional semantics-based formal specification of secure information-flow properties in sequential programs based on representing degrees of security by partial equivalence relations (pers). The specification clarifies and unifies a number of specific correctness arguments in the literature, and connections to other forms of program analysis. The approach is inspired by (and equivalent to) the use of partial equivalence relations in specifying binding-time analysis, and is thus able to specify security properties of higher-order functions and “partially confidential data”. We extend the approach to handle nondeterminism by using powerdomain semantics and show how probabilistic security properties can be formalised by using probabilistic powerdomain semantics.	confidentiality;correctness (computer science);data security;formal specification;higher-order function;information flow;name binding;operational semantics;power domains;program analysis;turing completeness	Andrei Sabelfeld;David Sands	2001	Higher-Order and Symbolic Computation	10.1023/A:1011553200337	semantic data model;program analysis;equivalence;semantic security;information flow;confidentiality;flow;covert channel;computer science;theoretical computer science;formal semantics;first-order logic;formal specification;semantics;language;equivalence relation;security analysis;programming language;higher-order function;algorithm	PL	-22.827966595911306	29.606869135038608	105037
d789923de710fe984835486a7350b438b4248393	a method for solving synchronization problems	programacion paralela;parallel programming;concurrent program;simultaneite;exclusion mutual;mutual exclusion;synchronisation;concurrency;simultaneidad;synchronization;programa competidor;sincronizacion;exclusion mutuelle;programme concurrent;programmation parallele	Abstract   This paper presents a systematic method for solving synchronization problems. The method is based on viewing processes as invariant maintainers. First, a problem is defined and the desired synchronization property is specified by an invariant predicate over program variables. Second, the variables are initialized to make the invariant true and processes are annotated with atomic assignments so the variables satisfy their definition. Then, atomic assignments are guarded as needed so they are not executed until the resulting state will satisfy the invariant. Finally, the resulting atomic actions are implemented using basic synchronization mechanisms. The method is illustrated by solving three problems using semaphores. The solutions also illustrate three general programming paradigms: changing variables, split binary semaphores, and passing the baton. Additional synchronization problems and synchronization mechanisms are also discussed.		Gregory R. Andrews	1989	Sci. Comput. Program.	10.1016/0167-6423(89)90013-0	synchronization;real-time computing;computer science;theoretical computer science;distributed computing;synchronization;programming language	Logic	-23.137607922804474	29.655215966921347	105171
7694e7abb69015eb14ff227ec74afb5742e577cd	the incorporation of early interface evaluation into command language grammar			command language	Brian Sharratt	1987			programming language;natural language processing;command language;grammar;artificial intelligence;computer science	NLP	-24.95712367275105	19.42252681815872	105174
af0d26147a9b62fccb95a3cf43e5feb7a896ae17	object-oriented theorem proving (ootp): first thoughts		Automatic (i.e., computer-assisted) theorem proving (ATP) can come in many flavors. This document presents early steps in our effort towards defining object-oriented theorem proving (OOTP) as a new style of ATP. Traditional theorem proving (TTP) is the only well-known flavor of ATP so far. OOTP is a generalization of TTP. While TTP is strongly based on functional programming (FP), OOTP is strongly based on object-oriented programming (OOP) instead. We believe OOTP is a style of theorem proving that is no less powerful and no less natural than TTP and thus likely will be no less practically useful than TTP. In the document we also discuss, very briefly, a related notion of OO software verification (OOSV) based on OOTP. To clarify the relation between OOTP and TTP, we also touch on the relation between OOP and FP.	automated theorem proving;functional programming;software verification;trusted third party	Moez A. AbdelGawad	2017	CoRR		theoretical computer science;computer science;object-oriented programming;automated theorem proving;functional programming;software verification	Logic	-27.19335550553212	24.943801785464213	105433
89bcc173de898173c2e8edd04cec4bb0a1f7e264	composable memory transactions for java using a monadic intermediate language		Transactional memory is a new programming abstraction that simplifies concurrent programming. This paper describes the parallel implementation of a Java extension for writing composable memory transactions in Java. Transactions are composable i.e., they can be combined to generate new transactions, and are first-class values, i.e., transactions can be passed as arguments to methods and can be returned as the result of a method call. We describe how composable memory transactions can be implemented in Java as a state passing monad, in which transactional blocks are compiled into an intermediate monadic language. We show that this intermediated language can support different transactional algorithms, such as TL2i¾?[9] and SWissTMi¾?[10]. The implementation described here also provides the high level construct retry, which allows possibly-blocking transactions to be composed in sequence. Although our prototype implementation is in Java using BGGA Closures, it could be implemented in any language that supports objects and closures in some way, e.g. C#, C++, and Python.	java;software transactional memory	Rafael Bandeira;André Rauber Du Bois;Maurício L. Pilla;Juliana Kaizer Vizzotto;Marcelo Machado	2015		10.1007/978-3-319-24012-1_10	transactional memory;parallel computing;real-time computing;java concurrency;computer science;real time java;programming language;java;generics in java;java annotation	PL	-25.1034360199797	28.4251449437189	105717
08e468e58f6ad2410ccd6f8bdd7acc5dae52dbeb	hoare-style reasoning with (algebraic) continuations	continuations;hoare logic;dependent types;callcc	"""Continuations are programming abstractions that allow for manipulating the """"future"""" of a computation. Amongst their many applications, they enable implementing unstructured program flow through higher-order control operators such as callcc. In this paper we develop a Hoare-style logic for the verification of programs with higher-order control, in the presence of dynamic state. This is done by designing a dependent type theory with first class callcc and abort operators, where pre- and postconditions of programs are tracked through types. Our operators are algebraic in the sense of Plotkin and Power, and Jaskelioff, to reduce the annotation burden and enable verification by symbolic evaluation. We illustrate working with the logic by verifying a number of characteristic examples."""	assertion (software development);call-with-current-continuation;computation;continuation;control flow;coq (software);dependent type;first-class function;higher-order function;hoare logic;immutable object;linear algebra;plotkin bound;postcondition;separation logic;symbolic execution;type theory;verification and validation	Germán Andrés Delbianco;Aleksandar Nanevski	2013		10.1145/2500365.2500593	dependent type;computer science;theoretical computer science;continuation;hoare logic;programming language;algorithm	PL	-19.126128056683076	26.672689648370792	105732
6e1d9a0e4d00012bab22d198befd9f59add17c9e	on jones-optimal specialization for strongly typed languages	program transformation;specification programme;transformation programme;functional programming;evaluation partielle compilateur;analisis programa;program optimization;program specialization;transformacion programa;complex data;programmation fonctionnelle;optimisation programme;program analysis;analyse programme;program specification;programacion funcional;especificacion programa;partial evaluation compilers;optimizacion programa	"""The phrase \optimal program specialization"""" was de ned by Jones et al. in 1993 to capture the idea of a specializer being strong enough to remove entire layers of interpretation. As it has become clear that it does not imply \optimality"""" in the everyday meaning of the word, we propose to rename the concept \Jones-optimality"""". We argue that the 1993 de nition of Jones-optimality is in principle impossible to ful l for strongly typed languages due to necessary encodings on the inputs and outputs of a well-typed self-interpreter. We propose a technical correction of the de nition which allows Jones-optimality to remain a meaningful concept for typed languages. We extend recent work by Hughes and by Taha and Makholm on the long-unsolved problem of Jones-optimal specialization for strongly typed languages. The methods of Taha and Makholm are enhanced to allow \almost optimal"""" results when a self-interpreter is specialized to a typeincorrect program; how to do this has been an open problem since 1987. Neither Hughes' nor Taha{Makholm's methods are by themselves su cient for Jones-optimal specialization when the language contains primitive operations that produce or consume complex data types. A simple postprocess is proposed to solve the problem. An implementation of the proposed techniques has been produced and used for the rst successful practical experiments with truly Jones-optimal specialization for strongly typed languages."""	bellman equation;experiment;interpreter (computing);jones calculus;jones polynomial;partial template specialization;rename (relational algebra);strong and weak typing;technical standard;type system	Henning Makholm	2000		10.1007/3-540-45350-4_11	program analysis;computer science;strong and weak typing;artificial intelligence;program optimization;database;mathematics;programming language;functional programming;algorithm;complex data type	PL	-20.064969890697434	23.33991831675606	105800
ca4c62443c8abc9992773f2cfe0dc7306f4d4db0	interactive theorem proving and computer algebra	computer algebra;interactive theorem proving	Without Abstract	proof assistant;symbolic computation	Johannes Ueberberg	1994		10.1007/3-540-60156-2_1	filtered algebra;discrete mathematics;symbolic computation;computer science;pure mathematics;fundamental theorem;mathematics;proof assistant;programming language;algebra	Logic	-19.399796426858074	18.781884081342003	105818
46875db73fffa3ffd9f88e9ed6ddeb609f45a7fc	event driven software quality	lenguaje programacion;programming language;gestion evenement;embedded system;sistema reactivo;operating system;event management;reactive system;sensor nodes;langage programmation;systeme reactif;gestion aconticimiento;high performance;qualite logiciel;software quality	Event-driven programming has found pervasive acceptance, from high-performance servers to embedded systems, as an efficient method for interacting with a complex world. The fastest research Web servers are eventdriven, as is the most common operating system for sensor nodes. An event-driven program handles concurrent logical tasks using a cooperative, application-level scheduler. The application developer separates each logical task into event handlers; the scheduler runs multiple handlers in an interleaved fashion. Unfortunately, the loose coupling of the event handlers obscures the program's control flow and makes dependencies hard to express and detect, leading to subtle bugs. As a result, event-driven programs can be difficult to understand, making them hard to debug, maintain, extend, and validate. This talk presents recent approaches to event-driven software quality based on static analysis and testing, along with some open problems. We will discuss progress on how to avoid buffer overflow in TCP servers, stack overflow and missed deadlines in microcontrollers, and rapid battery drain in sensor networks. Our work is part of the Event Driven Software Quality project at UCLA, which is aimed at building the next generation of language and tool support for event-driven programming.	buffer overflow;control flow;embedded system;event (computing);event-driven programming;fastest;interaction;loose coupling;microcontroller;operating system;pervasive informatics;scheduling (computing);software bug;software quality;stack overflow;static program analysis;web server	Jens Palsberg	2006		10.1007/11924661_10	embedded system;real-time computing;reactive system;computer science;operating system;software quality	PL	-23.35732781003609	32.28227153844899	105919
400959dbf2be521e3cc123628e0124afcf3a8caf	factor: a dynamic stack-based programming language	application development;dynamic programming;lenguaje programacion;dynamic languages;stack based languages;factor;programacion dinamica;programming language;optimizing compiler;lenguaje script;performance;optimizacion compiladora;desarrollo verbal;langage dedie;interactive development environment;object oriented;object oriented programming languages;compiler optimization;language development;programmation dynamique;domain specific language;metaprogrammation;langage programmation;developpement verbal;oriente objet;design;metaprogramming;orientado objeto;metaprogramacion;languages;optimisation compilateur;scripting language;langage script;lenguaje dedicado	Factor is a new dynamic object-oriented programming language. It began as an embedded scripting language and evolved to a mature application development language. The language has a simple execution model and is based on the manipulation of data on a stack. An advanced metaprogramming system provides means for easily extending the language. Thus, Factor allows programmers to use the right features for their problem domain. The Factor implementation is self-hosting, featuring an interactive development environment and an optimizing compiler. In this paper, the language and its implementation are presented.	actionscript;domain-specific language;embedded system;integrated development environment;metaprogramming;optimizing compiler;problem domain;programmer;scripting language;self-hosting;stack-oriented programming language	Sviatoslav Pestov;Daniel Ehrenberg;Joe Groff	2010		10.1145/1869631.1869637	natural language processing;first-generation programming language;natural language programming;compiler;dynamic compilation;very high-level programming language;language primitive;call stack;data manipulation language;object language;specification language;programming domain;data control language;computer science;programming language implementation;domain-specific language;common intermediate language;optimizing compiler;scripting language;low-level programming language;programming language;object-oriented programming;programming language specification;high-level programming language;algorithm	PL	-25.350851900957124	24.291221183649803	106005
e02933d39b740c0e37436e2284c22ed79a0907e8	plaggie: gnu-licensed source code plagiarism detection engine for java exercises	cheating;plagiarism;source code plagiarism detection engine;java programming;web service;computer assisted instruction;source code;plagiarism detection;java;open source	A source code plagiarism detection engine Plaggie is presented. It is a stand-alone Java application that can be used to check Java programming exercises. Plaggie's functionality is similar with previously published JPlag web service but unlike JPlag, Plaggie must be installed locally and its source code is open. Apparently, Plaggie is the only open-source plagiarism detection engine for Java exercises.	gnu;java;open-source software;web service	Aleksi Ahtiainen;Sami Surakka;Mikko Rahikainen	2006		10.1145/1315803.1315831	jsr 94;computer science;operating system;strictfp;programming language;java;world wide web;java applet;java annotation	PL	-31.15201259253265	25.810345930085703	106035
a00c76ff42357f28cd3f5dcfc03ceba80205357a	design and implementation of an economical instrument-computer interface based on the sdk-60 microcomputer	design and implementation	Interfacing NMR spectrometers through a Fabritek 1074 computer to a DECsystem-10 computer has been economically accomplished using an SDK-80 microcomputer. Features of the system include minimal hardware design; ease of operation; the use of LED program state indicators and the elimination of a conventional terminal; and the use of structured programming and prior programming specifications. The system has now been extensively used for several months with no problems.	microcomputer	Mark J. Abramson;J. H. Goldstein	1978	Computers & Chemistry	10.1016/0097-8485(78)85006-2	biology;real-time computing;chemistry;computer hardware;computer science	HCI	-31.319066409194072	21.394425959761342	106037
4d88947d0b4bedebf2320207638b1c451c2cbe4a	incremental mining for facility management	adaptive control;control system;air conditioning;change detection;facility management;stream processing;pattern analysis	Modern buildings are equipped with high-tech systems that take care of several fundamental aspects, e.g., air-conditioning, heating and water supply. The requirements posed on facility management by such buildings are challenging. Modern techniques implement adaptive control systems to achieve this, in which decisions are preferably based on the results of (multiple correlated) mining tasks on recently gathered sensor data. In this work, we discuss the general relationship between such control systems and the underlying mining tasks. We exemplary choose change detection in the context of pattern analysis as a representative, because this mining task involves general requirements known from stream processing like the need for incremental algorithms, but also poses specific challenges like in-time detection. We present three concrete approaches for this and an according evaluation.	algorithm;anomaly detection;care-of address;control system;data mining;dynamic problem (algorithms);floor and ceiling functions;pattern recognition;requirement;stream processing	Katja Hose;Marcel Karnstedt;Daniel Klan;Kai-Uwe Sattler;Jana Quasebarth	2007			stream processing;data mining;adaptive control;facility management;change detection;control system;engineering	ML	-32.602975743005864	18.827396585369605	106056
0d82411c355a915588162c89c95065d0655313f2	communication flow expressions in a notification and response system	commerce electronique;comercio electronico;data path;flot donnee;flujo datos;general techniques;notices;chemin donnee;notification;data flow;electronic trade	We introduce communication flow expressions (CFEs), a general technique for specifying the who, how, when and where of communication. CFEs use a three-value logic including some new logical primitives that are useful in supporting communication. CFEs integrate the communication requirements of applications with the communication preferences of users. We describe the first application of CFEs in the Avaya Xui Notification and Response System.	traffic flow (computer networking)	Joann J. Ordille;Thomas Petsche	2002		10.1007/3-540-46121-3_11	data flow diagram;real-time computing;computer science;operating system;database;distributed computing;computer security;algorithm	HCI	-25.373441621556648	32.02929764451847	106087
3f4510fcea0868161b601c16601785987a8e5005	ad-hoc polymorphism and dynamic typing in a statically typed functional language	dynamic typing;parametric polymorphism;ad hoc polymorphism;design space;network connectivity;polymorphism;it adoption;functional language;article in monograph or in proceedings;functional programming language;type system	Static typing in functional programming languages such as Clean, Haskell, and ML is highly beneficial: it prevents erroneous behaviour at run time and provides opportunities for optimisations. However, dynamic typing is just as important as sometimes types are not known until run time. Examples are exchanging values between applications by deserialisation from disk, input provided by a user, or obtaining values via a network connection. Ideally, a static typing system works in close harmony with an orthogonal dynamic typing system; not discriminating between statically and dynamically typed values. In contrast to Haskell's minimal support for dynamic typing, Clean has an extensive dynamic typing; it adopted ML's support for monomorphism and parametric polymorphism and added the notion of type dependencies. Unfortunately, ad-hoc polymorphism has been left out of the equation over the years. While both ad-hoc polymorphism and dynamic typing have been studied in-depth earlier, their interaction in a statically typed functional language has not been studied before. In this paper we explore the design space of their interactions.	ad hoc polymorphism;clean;functional programming;haskell;hoc (programming language);interaction;parametric polymorphism;programming language;run time (program lifecycle phase);type system	Thomas van Noort;Peter Achten;Marinus J. Plasmeijer	2010		10.1145/1863495.1863505	parametric polymorphism;type system;manifest typing;strong and weak typing;theoretical computer science;functional programming;duck typing;algorithm	PL	-25.1978203725604	29.764691513020974	106098
831db0286c3c085f99897cb887a1d5a992be7b26	rewriting logic and maude: a wide-spectrum semantic framework for object-based distributed systems	distributed system;spectrum;rewriting logic	Rewriting logic seems very well suited as a semantic framework for open object-based distributed systems. Both the distributed states and the local concurrent transitions of such systems can be naturally specified by rewrite theories in which such local concurrent transitions are described by rewrite rules. Maude is a high-performance rewriting logic language and system developed at SRI International that supports executable specification and programming, and a flexible variety of formal analyses.	distributed computing;maude system;object-based language;rewriting	José Meseguer	2000		10.1007/978-0-387-35520-7_5	spectrum;description logic;rewriting;computer science;theoretical computer science;programming language;algorithm	EDA	-28.88868595606969	31.40230131655288	106266
75423c208b9863323aca2611e9db65ea56f06333	ensuring efficiently the integrity of persistent object systems via abstract interpretation.		In this paper, we propose an eecient and reliable method to deal with integrity constraints in a persistent object system. First we provide the application programmer with the ability to express integrity constraints but we also give him the possibility to use high level language constructs to help him in writing safe transactions. The goal of our approach is to avoid the (run time) checking of constraints by proving formally that transactions preserve integrity constraints. We mainly use two abstract interpretation techniques to do that. Abstract interpretation is a semantics-based tool that yields some reliable information about the possible run-time behaviour of programs, with fully automatic algorithms. We present informally the methods that we use: a simple method, based on path reachability, and a more powerful and complex method that uses a predicate transformer. A predicate transformer is a function that, given a transaction and a formula describing its input data, yields a formula describing its output data. We nally describe the current prototype that applies those diierent techniques. It provides in fact the O 2 compiler with an integrity constraint manager.	abstract interpretation;algorithm;algorithmic trading;compiler;data integrity;database transaction;high-level programming language;programmer;prototype;reachability;run time (program lifecycle phase);transformer	Véronique Benzaken;Xavier Schaefer	1996			real-time computing;database;distributed computing	SE	-20.71212719568877	28.91415986948445	106462
850c6eae64c74c2746876d8fbcd7ca2ab6345e90	multi-prover verification of c programs	developpement logiciel;teoria demonstracion;anotacion;naming;theorie preuve;langage c;proof theory;specification;metodo formal;methode formelle;annotation;program verification;arithmetique;formal method;theorem proving;demonstration theoreme;verificacion programa;c language;formal verification;marcador;aritmetica;pointer;especificacion;arithmetics;desarrollo logicial;software development;denomination;denominacion;pointeur;invariante;verification formelle;demostracion teorema;verification programme;invariant;lenguaje c	Our goal is the verification of C programs at the source code level using formal proof tools. Programs are specified using annotations such as preand postconditions and global invariants. An original approach is presented which allows to formally prove that a function implementation satisfies its specification and is free of null pointer dereferencing and out-of-bounds array access. The method is not bound to a particular back-end theorem prover. A significant part of the ANSI C language is supported, including pointer arithmetic and possible pointer aliasing. We describe a prototype tool and give some experimental results.	ansi c;array data structure;automated theorem proving;avionics;c dynamic memory allocation;c string handling;dereference operator;embedded system;experiment;formal proof;image scaling;interactive proof system;pointer (computer programming);pointer aliasing;postcondition;precondition;predicate transformer semantics;proof assistant;prototype;separation logic;smart card;specification language;static program analysis;tracing (software);verification and validation	Jean-Christophe Filliâtre;Claude Marché	2004		10.1007/978-3-540-30482-1_10	opaque pointer;pointer;formal methods;formal verification;computer science;theoretical computer science;software development;invariant;escape analysis;proof theory;mathematics;function pointer;automated theorem proving;programming language;specification;algorithm	Logic	-22.556942475266542	28.58375378253671	106562
2e6bbe8903a65a6ef24cd0d01e6d95df7f5fac9f	a proposed categorical semantics for ml modules	proposed categorical semantics;ml modules	We present a simple categorical semantics for ML signatures, structures and functors. Our approach relies on realizablity semantics in the category of assemblies. Signatures and structures are modelled as objects in slices of the category of assemblies. Instantiation of signatures to structures and hence functor application is modelled by pullback.	categorical logic;electronic signature;function object;type signature;universal instantiation	Michael P. Fourman;Hayo Thielecke	1995		10.1007/3-540-60164-3_30	natural language processing;database;programming language	Logic	-23.841159906585588	19.983095746493387	106605
2b655b10852c7fd1e2103978adfc1b3dd81bf812	branch testing of concurrent programs using petri net models	concurrent programs;petri net			Hong-Fa Ho;Gen-Huey Chen;Te-Son Kuo	1990	Comput. Syst. Sci. Eng.		computer science;petri net	Logic	-21.933031498743222	20.93722075249637	106684
e8c0a59afc2fc8e49e78e6506d23c779cb7c62fd	on trace assertion method of module interface specification with concurrency	machine abstraite;algebraic specification;metodo formal;maquina abstracta;methode formelle;simultaneidad informatica;state machine;abstract machine;formal method;concurrency;specification algebrique;simultaneite informatique	The trace assertion method is a formal state machine based method for specifying module interfaces [1, 9]. It can be seen as an alternative to algebraic specification technique. We extend the sequential model presented in [9] by allowing simple concurrency.	denotational semantics	Ryszard Janicki;Yan Liu	2000		10.1007/3-540-45554-X_80	concurrency;computer science;theoretical computer science;formal specification;abstract machine;programming language;algorithm	SE	-24.320996824145336	30.73053821229127	106692
7ee58566b8aeaeadd74d66a2fb1389b1f4b970e4	validating assertion language rewrite rules and semantics with automated theorem provers	hardware design languages;computer assisted proof;rewrite rule;proofs;generators;assertion language semantics;automated theorem provers;semantics;psl syntax;theorem proving rewriting systems specification languages;assertion language semantics assertion language rewrite rules automated theorem provers property specification language systemverilog assertion computer assisted proof psl syntax prototype verification system theorem prover psl semantics;prototype verification system theorem prover;theorem proving;automata;theorem prover;assertion languages;language semantics;assertion language rewrite rules;systemverilog assertion;rewrite rules assertion languages automated theorem provers language semantics proofs;guidelines;syntactics;rewriting systems;specification languages;psl semantics;rewrite rules;prototype verification system;semantics syntactics guidelines generators hardware automata hardware design languages;property specification language;hardware	Modern assertion languages such as property specification language (PSL) and SystemVerilog assertions include many language constructs. By far, the most economical way to process the full languages in automated tools is to rewrite the majority of operators to a small set of base cases, which are then processed in an efficient way. Since recent rewrite attempts in the literature have shown that the rules could be quite involved, sometimes counterintuitive, and that they can make a significant difference in the complexity of interpreting assertions, ensuring that the rewrite rules are correct is a major contribution toward ensuring that the tools are correct, and even that the semantics of the assertion languages are well founded. This paper outlines the methodology for computer-assisted proofs of several publicly known rewrite rules for PSL properties. We first present the ways to express the PSL syntax and semantics in the prototype verification system (PVS) theorem prover, and then prove or disprove the correctness of over 50 rewrite rules published without proofs in various sources in the literature. In doing so, we also demonstrate how to circumvent known issues with PSL semantics regarding the never and eventually! operators, and offer our proposals on assertion language semantics.	assertion (software development);automated theorem proving;clock rate;correctness (computer science);experiment;local variable;property specification language;prototype verification system;provable security;rewrite (programming);rewriting;simulation;software verification;systemverilog;tracing (software)	Katell Morin-Allory;Marc Boule;Dominique Borrione;Zeljko Zilic	2010	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.2010.2049150	computer science;theoretical computer science;semantics;automated theorem proving;programming language;algorithm	Logic	-19.300737995182452	25.559872501180664	106889
78b9c6d0994bd9f539d93e1948c540a2d1570ead	an extensible formal framework for the specification and verification of an optimistic simulation protocol	parallel and distributed system;time warp;protocols;formal specification;design and development;pvs;software systems;protocols discrete event simulation application software software systems design optimization data structures rivers large scale systems software prototyping virtual prototyping;parallel programming;software engineering;general purpose higher order logic framework extensible formal framework optimistic simulation protocol specification optimistic simulation protocol verification distributed software systems parallel software systems large scale critical systems large scale complex systems nondeterministic systems prototype verification system time warp protocol parallel optimistic discrete event simulation algorithm;theorem proving;large scale;formal verification;specification and verification;complex system;parallel discrete event simulation;parallel programming time warp simulation formal specification formal verification protocols;prototype verification system;time warp simulation;higher order logic;formal specification and verification;discrete event simulation	Parallel and distributed software systems are representative of large scale critical and complex systems that require the application of normal methods. Parallel and distributed software systems are notoriously unreliable because implementors often design and develop such systems without a complete understanding of the problem domain; in addition, the nondeterministic nature of certain parallel and distributed systems make system validation difficult if not impossible. In this paper, the application of normal specification and verification to a class of parallel and distributed software systems is presented. Specifically, the prototype verification system (PVS) is applied to the specification and verification of the time warp protocol, a parallel optimistic discrete event simulation algorithm. The paper discusses how the specification of the time warp protocol can be mechanized within a general-purpose higher-order logic framework like PVS. In addition, the paper presents the extensibility of the specification to address and verify different aspects and optimizations of the basic time warp protocol.	simulation	Peter Frey;Radharamanan Radhakrishnan;Philip A. Wilsey;Perry Alexander;Harold W. Carter	1999		10.1109/HICSS.1999.772888	communications protocol;complex systems;real-time computing;higher-order logic;formal verification;computer science;discrete event simulation;operating system;formal specification;distributed computing;automated theorem proving;programming language;software system	Logic	-29.92767945275181	31.673171992525894	107001
01602b077c33b7f78aa3bf94348ff5d3a81b5b09	parametric polymorphism for xml	anotacion;protocolo acceso;compilacion;soustype;subtype;parametric polymorphism;protocole soap;theorie type;computacion informatica;automate arbre;xml language;subtyping;service web;semantics;annotation;web service;semantica;semantique;access protocol;teoria de tipos;tipificacion;protocolo soap;subtipo;typing;tree automaton;ciencias basicas y experimentales;polymorphism;automata arbol;theory;type theory;tree automata;xml;typage;compilation;algorithms;design;polymorphisme;polimorfismo;language;protocole acces;grupo a;simple object access protocol;langage xml;lenguaje xml;abstract types;servicio web;type system	Despite the extensiveness of recent investigations on static typing for XML, parametric polymorphism has rarely been treated. This well-established typing discipline can also be useful in XML processing in particular for programs involving “parametric schemas,” that is, schemas parameterized over other schemas (e.g., SOAP). The difficulty in treating polymorphism for XML lies in how to extend the “semantic” approach used in the mainstream (monomorphic) XML type systems. A naive extension would be “semantic” quantification over all substitutions for type variables. However, this approach reduces to an NEXPTIME-complete problem for which no practical algorithm is known and induces a subtyping relation that may not always match the programmer's intuition. In this article, we propose a different method that smoothly extends the semantic approach yet is algorithmically easier. The key idea here is to devise a novel and simple  marking  technique, where we interpret a polymorphic type as a set of values with annotations of which subparts are parameterized. We exploit this interpretation in every ingredient of our polymorphic type system such as subtyping, inference of type arguments, etc. As a result, we achieve a sensible system that directly represents a usual expected behavior of polymorphic type systems—“values of abstract types are never reconstructed”—in a reminiscence of Reynold's parametricity theory. Also, we obtain a set of practical algorithms for typechecking by local modifications to existing ones for a monomorphic system.	parametric polymorphism;xml	Haruo Hosoya;Alain Frisch;Giuseppe Castagna	2009	ACM Trans. Program. Lang. Syst.	10.1145/1596527.1596529	xml validation;xml;subtyping;computer science;theoretical computer science;database;semantics;programming language;algorithm	PL	-23.283816766568705	26.61814642722611	107301
531ad58f18bd3572ef94b9b950abf8b07803bf27	chapter 1. generic haskell: practice and theory		Generic Haskell is an extension of Haskell that supports the construction of generic programs. These lecture notes describe the basic constructs of Generic Haskell and highlight the underlying theory. Generic programming aims at making programming more effective by making it more general. Generic programs often embody non-traditional kinds of polymorphism. Generic Haskell is an extension of Haskell [38] that supports the construction of generic programs. Generic Haskell adds to Haskell the notion of structural polymorphism, the ability to define a function (or a type) by induction on the structure of types. Such a function is generic in the sense that it works not only for a specific type but for a whole class of types. Typical examples include equality, parsing and pretty printing, serialising, ordering, hashing, and so on. The lecture notes on Generic Haskell are organized into two parts. This first part motivates the need for genericity, describes the basic constructs of Generic Haskell, puts Generic Haskell into perspective, and highlights the underlying theory. The second part entitled “Generic Haskell: applications” delves deeper into the language discussing three non-trivial applications of Generic Haskell: generic dictionaries, compressing XML documents, and a generic version of the zipper data type. The first part is organized as follows. Section 1 provides some background discussing type systems in general and the type system of Haskell in particular. Furthermore, it motivates the basic constructs of Generic Haskell. Section 2 takes a closer look at generic definitions and shows how to define some popular generic functions. Section 3 highlights the theory underlying Generic Haskell and discusses its implementation. Section 4 concludes.	dictionary;generic function;haskell;mathematical induction;parsing;prettyprint;printing;social equality;structural induction;type system;xml	Ralf Hinze;Johan Jeuring	2003		10.1007/978-3-540-45191-4_1	computer science;programming language;algorithm	PL	-25.033951724856802	26.15828049253281	107499
65ce4e683041a7ef3c0c2cdabec8b3d3a0870008	verification of java card programs		syntax tree, 13 annotated, 13 addarray, 40, 41 addarrays, 41 addclass, 26, 55 addobj, 26 algebraic specifications, 23 allsupers, 32 antecedent, 63 array access proof rule, 78 semantics, 35 array assignment proof rule, 79 semantics, 37 array creation proof rule, 81 semantics, 40 array initializer proof rule, 82 semantics, 41 ArrayAccess, 15 ArrayAssign, 15 asgcomp, 37, 52, 55, 128 definition, 31 assignment compatible, 37 definition, 31 basic expression, 64, 65 big-step semantics, 27 binary operator proof rule, 77 semantics, 33 BinaryExpr, 14 block proof rule, 88 semantics, 47 box, 63 break statement proof rule, 90 semantics, 51 cast proof rule, 75 semantics, 31 catches statement, 16 proof rule, 91 semantics, 53 class instance creation proof rule, 82 semantics, 39 ClassCastException, 31 CompAssign, 15 compatible, 135 compound assignment proof rule, 81 semantics, 38 conclusion, 70 CondBinExpr, 14 conditional binary operator proof rule, 76 semantics, 33 conditional operator proof rule, 76 semantics, 33 ConstrCall, 15 ConstrDecl, 17 constructor invocation proof rule, 83 semantics, 41 decrement operator proof rule, 80 semantics, 37 derivation, 70 deterministic semantics, 61 diamond, 63 DL, 63 do statement proof rule, 89 semantics, 49 done, 24 dynamic logic, 63	augmented assignment;conditional operator;control flow;do while loop;increment and decrement operators;initialization (programming);instance (computer science);java card;operational semantics;parse tree;rule 90	Kurt Stenzel	2005			java applet;masterbatch;operating system;real time java;java card openplatform;basiccard;open smart card development platform;java card;java;computer science	PL	-24.025260917006992	26.32477253684828	107684
100e2b76196634231cca5e6a365a63a5bdbaffcc	syntactic heroin	feel-good fix;ugly function name;syntactic heroin;dangerous addiction;user-defined overloading	User-defined overloading is a drug. At first, it gives you a quick, feel-good fix. No sense in cluttering up code with verbose and ugly function names such as IntAbs, FloatAbs, DoubleAbs, or ComplexAbs; just name them all Abs. Even better, use algebraic notation such as A+B, instead of ComplexSum(A,B). It certainly makes coding more compact. But a dangerous addiction soon sets in. Languages and programs that were already complex enough to stretch everyone’sability suddenly get much more complicated.	function overloading	Rodney Bates	2005	ACM Queue	10.1145/1071713.1071738	computer science;algorithm	PL	-23.02300592570132	25.037632541561926	108035
02f19f847ccef4d3d12a31b5e3606cb3f1490abd	safe cross-language inheritance	dynamic typing;type classes	Inheritance is a standard means for reuse and for interfacing with external libraries. In a multi-language software product, extending a class written in a statically-typed language with a dynamicallytyped class can require a significant number of manual indirections and other error-prone complications. Building on our previous interoperability work, we introduce a technique that allows safe, easy inheritance across languages. We demonstrate our technique for cross-language inheritance with a statically-typed object calculus and a dynamically-typed object calculus, where a statically-typed class can extend a dynamicallytyped one and vice versa. We provide a proof sketch of soundness, as well as a guarantee that dynamic type errors do not arise due to staticallytyped expressions. 1 Crossing Language Boundaries Object-oriented libraries often require that clients extend a class. For a multilanguage product, extending the proper class may require developing a superfluous bridge between two languages with manual data marshaling, dispatching, and dynamic type-checking. Providing access to useful libraries can lead to language developers manually building these bridges for each relevant class (MrEd [1], Groovy [2], etc.), developing reflective APIs (MLj, Jython, JScheme, Bigloo, etc.), or using an external tool. All of these techniques add complexity to developing programs in multiple languages and allow conversion/checking omissions that violate language safety and cause obscure runtime errors. In our previous work, Java+dynamic [3], we demonstrated a compiler technique for automatically inserting dynamic checks and data conversions in programs that connect two languages, namely Java and PLT Scheme. Runtime type-errors report which value failed to match a required type specification and blame the origin of this value for the program fault. The programmer should expect a Scheme value, never a Java value, to be blamed for any dynamic type fault. However, inheritance of a Java class from Scheme performed no data conversions and caused misallocated blame, and inheritance of a Scheme class from Java carried the same problems while requiring an external type specification. We expand our support for interoperability by modifying the dynamic dispatch mechanism of a class so that cross-language inheritance automatically performs conversions and cannot result in dynamic type errors within typed ancestors or descendants. We present this in terms of a combined calculus, with class Wizard extends Character {	bigloo;cognitive dimensions of notations;compiler;computer programming;distributed computing;dynamic dispatch;ecoop;embedded system;executable;functional programming;gradual typing;groovy;immutable object;indirection;intel turbo memory;interoperability;jscheme;java;javascript;jython;leaf class (computer programming);lecture notes in computer science;library (computing);matthew flatt;operational semantics;over-the-top content;overhead (computing);plotkin bound;programmer;programming language;racket;reduction strategy (code optimization);requirement;ruby;run time (program lifecycle phase);scheme;smalltalk;software development;springer (tank);strongtalk;symposium on principles of programming languages;type system;universal instantiation;wizard (software)	Kathryn E. Gray	2008		10.1007/978-3-540-70592-5_4	type system;computer science;theoretical computer science;composition over inheritance;programming language;algorithm	PL	-25.05689091367026	28.741740270450588	108168
33047e42a8442b81fc656d2b094240e72d87dbed	protocol design and implementation using formal methods	distributed system;systeme reparti;metodologia;protocole transmission;concepcion sistema;implementation;sistema informatico;transformacion;protocol design;computer system;ingenieria logiciel;software engineering;methodologie;formal method;ejecucion;protocolo transmision;sistema repartido;design method;design and implementation;system design;genie logiciel;lotosphere;systeme informatique;transformation;methodology;lenguaje formal;conception systeme;formal language;design methodology;transmission protocol;langage formel	This paper reports on a number of formal methods that support correct protocol design and implementation. These methods are placed in the framework of a design methodology for distributed systems that was studied and developed within the ESPRIT II Lotosphere project (2304). The paper focuses on design methods for synthesizing protocols by successive application of correctness-preserving LOTOS transformations. This transformational approach is described in some detail and is illustrated with a protocol design example. The paper concludes with some suggestions for relating design methods to milestones in the protocol design and implementation processes.	formal methods	Marten van Sinderen;Luís Ferreira Pires;Chris A. Vissers	1992	Comput. J.	10.1093/comjnl/35.5.478	formal methods;design methods;computer science;programming language;algorithm	Logic	-32.328663275771255	31.20489456692008	108202
e43d6c5c600c1c8dddaba45066ea558ebdf65e80	weakly sensitive analysis for unbounded iteration over javascript objects		JavaScript framework libraries like jQuery are widely used, but complicate program analyses. Indeed, they encode clean high-level constructions such as class inheritance via dynamic object copies and transformations that are harder to reason about. One common pattern used in them consists of loops that copy or transform part or all of the fields of an object. Such loops are challenging to analyze precisely, due to weak updates and as unrolling techniques do not always apply. In this paper, we observe that precise field correspondence relations are required for client analyses (e.g., for call-graph construction), and propose abstractions of objects and program executions that allow to reason separately about the effect of distinct iterations without resorting to full unrolling. We formalize and implement an analysis based on this technique. We assess the performance and precision on the computation of call-graph information on examples from jQuery tutorials.	iteration;javascript;μ operator	Yoonseok Ko;Xavier Rival;Sukyoung Ryu	2017		10.1007/978-3-319-71237-6_8	computer science;programming language;theoretical computer science;computation;javascript;abstraction;distributed computing	Logic	-21.381106804361835	28.146983646150282	108349
7b8fd10eaf1175d1cedfeb9d435c73cbeabc5ad0	the construction of a common objective function for analytical infrastructures		The paper deals with the increasing growth of embedded systems and their role within structures similar to the Internet (Internet of Things) as those that provide calculating power and are more or less appropriate for analytical tasks. Faced with the example of a cyber-physical manufacturing system, a common objective function is developed with the intention to measure efficient task processing within analytical infrastructures. A first validation is realized on base of an expert panel.	analytical engine;big data;embedded system;internet of things;loss function;mathematical optimization;optimization problem	Marcus Grum;Benedict Bender;Attahiru Alfa	2017	2017 International Conference on Engineering, Technology and Innovation (ICE/ITMC)	10.1109/ICE.2017.8279892	task analysis;throughput;cloud computing;the internet;memory management;mathematical optimization;linear programming;internet of things;computer science	EDA	-33.48355991464258	20.56156532138455	108459
6e36fdb0028c8040195747548a0913c0ef604124	refactoring support for class library migration	aspect mechanism;lenguaje programacion;instruccion declaracion;aspectj;domain specific aspect language;theorie type;programming language;aop;collaboration;aspectwerkz;retroingenierie;aosd;langage java;aspectual effect;mixin;aop semantics;reuse;software components;cbse;cool;object oriented;type theory;declaration;langage programmation;oriente objet;lenguaje java;langage theorie conception;third party composition;instruction declaration;orientado objeto;ingeniera inversa;aspect extension;reverse engineering;java language	As object-oriented class libraries evolve, classes are occasionally deprecated in favor of others with roughly the same functionality. In Java's standard libraries, for example, class Hashtable has been superseded by HashMap, and Iterator is now preferred over Enumeration. Migrating client applications to use the new idioms is often desirable, but making the required changes to declarations and allocation sites can be quite labor-intensive. Moreover, migration becomes complicated---and sometimes impossible---if an application interacts with external components, if a legacy class is not completely equivalent to its replacement, or if multiple interdependent classes must be migrated simultaneously. We present an approach in which mappings between legacy classes and their replacements are specified by the programmer. Then, an analysis based on type constraints determines where declarations and allocation sites can be updated. The method was implemented in Eclipse, and evaluated on a number of Java applications. On average, our tool could migrate more than 90% of the references to legacy classes.	code refactoring;deprecation;eclipse;hash table;interdependence;iterator;java class library;library (computing);programmer;standard library	Ittai Balaban;Frank Tip;Robert M. Fuhrer	2005		10.1145/1094811.1094832	declaration;aspect-oriented programming;computer science;component-based software engineering;reuse;programming language;object-oriented programming;type theory;algorithm;reverse engineering;collaboration	PL	-26.416201192545525	29.347627129303977	108510
07a401e8a8a11bcb4c50478bc1f512b87cb32936	formal semantics of heterogeneous cuda-c: a modular approach with applications	paper;heterogeneous systems;cuda;package;nvidia;computer science;programming languages	We extend an off-the-shelf, executable formal semantics of C (Ellison and Ros ,u’s K Framework semantics) with the core features of CUDA-C. The hybrid CPU/GP U computation model of CUDA-C presents challenges not just for programmers, but also for p ractitioners of formal methods. Our formal semantics helps expose and clarify these issues. We demo nstrate the usefulness of our semantics by generating a tool from it capable of detecting some race co nditi ns and deadlocks in CUDA-C programs. We discuss limitations of our model and argue that i s extensibility can easily enable a wider range of verification tasks.	.net framework;cuda;central processing unit;deadlock;executable;extensibility;formal methods;formal verification;graphics processing unit;heterogeneous computing;interaction;model of computation;multi-core processor;parallel computing;programmer;rewriting;semantics (computer science);sensor;software design pattern	Chris Hathhorn;Michela Becchi;William L. Harrison;Adam M. Procter	2012		10.4204/EPTCS.102.11	parallel computing;formal methods;formal verification;computer science;theoretical computer science;formal semantics;programming language;package;well-founded semantics;operational semantics;denotational semantics;algorithm;computational semantics	PL	-27.2216665714242	29.182590225789202	108520
aee9c44100cb6aeb41327ee9e5a3e6ae92456e20	dynamic optimization of covered tail recursive functions in applicative languages	variable binding;optimization technique;recursion operator;first order predicate logic;prolog;logic data base;software engineering;artificial intelligent;data base view;efficient implementation;query by example;group;dynamic optimization	"""Within the last years, the interest in efficient implementations of high level interpreter-based applicative languages has increased considerably, both in the areas of software engineering and artificial intelligence. Further, a preference for static scope languages can be observed. This paper presents an optimization technique for interpreters of such languages, taking an implementation of LISP as an example. First the technique of shallow binding, which is used as a fast method of accessing variable bindings in many (dynamic scope !) LISP-systems, is adapted to static scoping . Then a dynamic optimization of simple and formal tail recursive functions is introduced. This technique is extended such that """"covered tail recursire"""" functions are also interpreted very efficiently. Functions of this kind are typical for recursive operations on list structures. Finally, it is shown that the combination of these three techniques keeps the extra expense for static scoping small."""	applicative programming language;artificial intelligence;dynamic programming;high-level programming language;lisp;mathematical optimization;recursion (computer science);scope (computer science);software engineering;tail call	Kay-Ulrich Felgentreu;Wolfram-Manfred Lippe	1986		10.1145/324634.325437	logic optimization;computer science;query by example;theoretical computer science;first-order logic;database;group;programming language;prolog;algorithm	PL	-21.200411306817966	23.530777191964322	108604
b280e6aab723d18d5176bfdd4a191f3815980d85	design and implementation of multiple type objects in a persistent programming language	programming language;type system multiple type objects persistent programming language classes types database design inada object oriented persistent programming language persistent objects enhanced c language object model;object oriented databases object oriented languages object oriented programming c language data structures;object oriented programming;c language;design and implementation;object oriented;data structures;computer languages object oriented modeling object oriented databases computer science design engineering libraries laboratories runtime prototypes;object oriented databases;database design;object oriented languages;type system;object model	In general it takes a lot of time to decide the forms of classes, or types, in database design. This is because the forms of objects stored in a database can hardly be changed. If the objects can get and lose types dynamically, this may be solved. We describe design of multiple type objects in INADA, an object oriented persistent programming language. Any persistent objects in INADA may get any types at any time the types are needed, and may lose any unnecessary types dynamically. INADA is an enhanced C++ language; it borrows the object model of C++ and extends it to provide facilities needed for processing on a large amount of persistent objects. We also show implementation of multiple type objects borrowing the type system of C++ just as it is.	persistent programming language	Masayoshi Aritsugi;Akifumi Makinouchi	1995		10.1109/CMPSAC.1995.524760	type conversion;protocol;method;run-time type information;object-based language;object model;data structure;object composition;object language;type safety;computer science;object;theoretical computer science;object-relational mapping;object-oriented design;common object request broker architecture;database;object type;distributed object;has-a;programming language;object-oriented programming;god object;object definition language	PL	-26.159293933908607	27.112455568708338	108792
20e00381468ecad04b1f7347eb2fb0316a661464	automatic dynamic generation of likely invariants for ws-bpel compositions	dynamic invariant generation;web service composition;ws bpel;white box testing	The wide adoption of Web Services has led to the development of languages to compose them, like the WS-BPEL standard. In order to check whether the composition works as expected, one common approach is to analyze it and infer functional properties describing its behavior. Traditional approaches for inferring properties in WS-BPEL have been static: compositions are transformed into specialized analysis models based on some formalization. However, this formalization could be inexact due to theoretical limitations or differing interpretations of the standard by implementers. Dynamic invariant generation solves these problems by extracting the properties from actual executions and has been successfully used in popular languages, but not to WS-BPEL yet. In this work, we apply dynamic invariant generation to WS-BPEL, providing innovative solutions for several features that require special consideration, like highly multidimensional values in variables, an advanced type system or unstructured code. We have implemented these solutions in Takuan and evaluated its performance with several compositions of varying complexity. We present the results obtained and a comparative analysis of the efficiency and effectiveness of our solutions. Results show that the solutions are successful in reducing the cost of applying dynamic invariant generation and the number of uninteresting invariants generated. © 2014 Elsevier Ltd. All rights reserved.	business process execution language	Manuel Palomo-Duarte;Antonio García-Domínguez;Inmaculada Medina-Bulo	2014	Expert Syst. Appl.	10.1016/j.eswa.2014.01.037	simulation;white-box testing;computer science;artificial intelligence;theoretical computer science;machine learning;mathematics;algorithm	Arch	-22.56137765006815	27.223146652825626	108941
4a186f18817ce1a9cfb6f29a73e6ea844eb69e09	implementing function block adapters	object oriented programming;object oriented language;programming language	Programmable Logic Controllers (PLCs) are widely used for controlling industrial manufacturing systems. The programming of PLCs is normally done in special languages defined in the IEC 61131-3 standard [2]. The increasing complexity of the controlling software for manufacturing systems leads to the need for more powerful specification languages. Latest developments in object oriented technology like UML-RealTime (successor of ROOM [4]) face this need [1]. But in most cases it is not possible to completely substitute PLCs in existing plants with object oriented systems. Therefore, our approach is to integrate object oriented technology (UML-RealTime) into an existing PLC-environment in the case of extending a manufacturing system with new components without throwing away the PLC. New components can be for example an Industrial Personal Computer (IPC) which is connected over a fieldbus system to the PLC. We assume, that the IPC program is then designed with UML-RealTime.	fieldbus;iec 61131;iec 61131-3;personal computer;power-line communication;programmable logic device;specification language;unified modeling language	Torsten Heverhagen;Rudolf Tracht	2001			method;computer architecture;first-generation programming language;object definition language;object (computer science);fifth-generation programming language;fourth-generation programming language;programming domain;programming language implementation;computer science	Robotics	-31.87063745060004	29.01727360949213	109068
1aacd7f379e51e0c42f5835e13cb746c84bd4098	introducing the graspin specification language segras,	developpement logiciel;distributed system;004 datenverarbeitung;systeme reparti;formalization;programming environment;assisted programming;concurrent program;specification language;graspin;medio ambiente programacion;sistema repartido;desarrollo logicial;informatik;software development;programa competidor;formalizacion;ayuda programacion;lenguaje especificacion;formalisation;langage specification;aide programmation;environnement programmation;programme concurrent	Abstract   SEGRAS is a novel language for writing formal specifications of concurrent and distributed systems. It integrates algebraic specifications of abstract data types, high-level Petri net specifications of concurrent behavior, and a sort of parametric polymorphism in a common syntactic and semantic framework. The language treats data abstractions with distributed states and state-dependent operations that can dynamically create new data objects and concurrently modify their properties according to a specified dynamic behavior. Interactive construction and formal analysis of SEGRAS specifications is supported by a collection of tools integrated in the GRASPIN environment. This article illustrates some salient features of the language through examples and discusses methodological issues concerned with constructing, analyzing, and executing SEGRAS specifications with the aid of the GRASPIN environment.	specification language	Bernd J. Krämer	1991	Journal of Systems and Software	10.1016/0164-1212(91)90074-G	specification language;computer science;software development;database;programming language;algorithm	SE	-25.663773881935274	31.685531155790695	109241
059d11043943dcfa72a6f439a088dab87674f5a3	submodules in racket: you want it when, again?	racket programming language;language model;run time;different time;language extensible;test time;module system;documentation time;extensible programming language;built-in distinction;macros;modules	"""In an extensible programming language, programmers write code that must run at different times - in particular, at compile time versus run time. The module system of the Racket programming language enables a programmer to reason about programs in the face of such extensibility, because the distinction between run-time and compile-time phases is built into the language model. Submodules extend Racket's module system to make the phase-separation facet of the language extensible. That is, submodules give programmers the capability to define new phases, such as """"test time"""" or """"documentation time,"""" with the same reasoning and code-management benefits as the built-in distinction between run time and compile time."""	algol;autodesk maya;automatic programming;closing (morphology);compile time;compiler;disk staging;documentation;eli;embedded system;extensibility;extensible programming;functional programming;haddock;haskell;higher-order and symbolic computation;hoc (programming language);hygienic macro;jason;java;jones calculus;klaus samelson;language model;lisp;matthew flatt;metalinguistic abstraction;modular programming;multi-stage programming;multiple dispatch;oscar;organizing (structure);programmer;programming language design and implementation;r. kent dybvig;racket;reduction strategy (code optimization);regret (decision theory);robert bruce findler;run time (program lifecycle phase);scheme;scope (computer science);self-modifying code;symposium on principles of programming languages;template metaprogramming;theoretical computer science;type system	Matthew Flatt	2013		10.1145/2517208.2517211	real-time computing;computer science;macro;modular programming;programming language;algorithm	PL	-23.07570656128762	21.93679759875711	109349
10e85273a596ee893d2b508eaddd57afd1ba8597	haskell#: parallel programming made simple and efficient	parallel programs;petri net;parallel languages	This paper presents the final result of the designing efforts for the development of a new specification for the Haskell # Language, including new features to increase its expressiveness, but without losing neither efficiency nor obedience to its original premisses.	comparison of command shells;haskell;java;machine translation;obedience (human behavior);parallel computing;petri net;semantics (computer science);simulation	Francisco Heron de Carvalho Junior;Rafael Dueire Lins	2003	J. UCS		parallel computing;computer science;programming language;petri net;algorithm	PL	-24.586056848526486	22.919594033760184	109362
ff95f7fa755f15e941d16d54d0f0426e957b5d9d	using javatm apis with native ada compilers	developpement logiciel;architecture logicielle;logiciel a securite critique;computer software maintenance;interface programme application;ingenieria logiciel;java native interface;object oriented programming;software engineering;maintenance logiciel;software architecture;critical system;desarrollo logicial;object oriented programming languages;safety critical software;application program interfaces;software development;genie logiciel;graphic user interface;cost effectiveness;programmation orientee objet	Ada is an ISO standard Object Oriented programming language specifically designed to support the cost effective development of robust, maintainable software. Because of this, Ada is widely used in the development of critical systems such as commercial aircraft. However, despite its advantages and general purpose nature, Ada is not often used for the development of main stream applications. This is partly because of Ada's poor integration with contemporary technologies such as Graphical User Interfaces. Described within this paper is a technique which uses the Java Native Interface to provide Ada programmers with immediate access to any software that has a Java API, thus substantially improving the suitability of Ada for the development of a wide range of applications.	ada	Shayne Flint;Brian Dobbing	2000		10.1007/10722060_7	ada;computer science;operating system;software engineering;programming language;object-oriented programming	DB	-28.72930078954719	29.32919910631746	109449
3c4d735a89ba43964712e1fab25a2c2d671ec70e	wireless sensor and actuator networks: charecterization and case study for confined spaces healthcare applications	wireless sensor;wireless sensor networks biomedical communication health care;wireless sensor networks actuators medical services;heart arrhythmias wireless sensor and actuator networks health sectors environmental sectors agricultural sectors industrial sectors;arrhythmia;wireless sensor networks;biomedical communication;health care	Nowadays developments in wireless sensor and actuators networks (WSAN) applications are determined by the fulfillment of constraints imposed by the application. For this reason, in this work a characterization of WSAN applications in health, environmental, agricultural and industrial sectors are presented. A case study for detecting heart arrhythmias in non-critical patients during rehabilitation sessions in confined spaces is presented, and finally an architecture for the network and nodes in these applications is proposed.	sensor	Diego Martínez;Francisco Blanes;José-Enrique Simó-Ten;Alfons Crespo	2008	2008 International Multiconference on Computer Science and Information Technology	10.1109/IMCSIT.2008.4747317	wireless sensor network;telecommunications;computer science;key distribution in wireless sensor networks;mobile wireless sensor network;health care;computer network	Embedded	-32.52395194625319	22.419627822776086	109611
a2337b81407a055e37f8a1047f6bee81ec9cdf5f	validating z specifications using the probanimator and model checker	data type;higher order;model checking;industrial application	We present the architecture and implementation of the proz tool to validate high-level Z specifications. The tool was integrated into prob, by providing a translation of Z into B and by extending the kernel of prob to accommodate some new syntax and data types. We describe the challenge of going from the tool friendly formalism B to the more specification-oriented formalism Z, and show how many Z specifications can be systematically translated into B. We describe the extensions, such as record types and free types, that had to be added to the kernel to support a large subset of Z. As a side-effect, we provide a way to animate and model check records in prob. By incorporating proz into prob, we have inherited many of the recent extensions developed for B, such as the integration with CSP or the animation of recursive functions. Finally, we present a successful industrial application, which makes use of this fact, and where proz was able to discover several errors in Z specifications containing higher-order recursive functions.	high- and low-level;industrial pc;kernel (operating system);model checking;recursion (computer science);scientific visualization;semantics (computer science);side effect (computer science);test template framework;z notation	Daniel Plagge;Michael Leuschel	2007		10.1007/978-3-540-73210-5_25	model checking;real-time computing;higher-order logic;data type;computer science;programming language;algorithm	SE	-23.227965952497723	27.21822490361747	109872
cb8f2cdca4e5d0787eb43c01e0160d61dbf328a1	the refactoring browser	smalltalk	Subject–oriented programming (SOP) is an extension of OO programming that permits non–invasive extension, customization and integration of OO components. Support for SOP in C++ and Java was demonstrated.	c++;class browser;code refactoring;java	John Brant;Don Roberts	1998		10.1007/3-540-49255-0_180	computer science;operating system;programming language	SE	-30.047468210595056	28.170052926379935	110049
fff330ed50055ab07e048230ad1c359b03d8b89e	program synthesis for program analysis		In this article, we propose a unified framework for designing static analysers based on program synthesis. For this purpose, we identify a fragment of second-order logic with restricted quantification that is expressive enough to model numerous static analysis problems (e.g., safety proving, bug finding, termination and non-termination proving, refactoring). As our focus is on programs that use bit-vectors, we build a decision procedure for this fragment over finite domains in the form of a program synthesiser. We provide instantiations of our framework for solving a diverse range of program verification tasks such as termination, non-termination, safety and bug finding, superoptimisation, and refactoring. Our experimental results show that our program synthesiser compares positively with specialised tools in each area as well as with general-purpose synthesisers.	algorithm;code refactoring;decision problem;divergence (computer science);formal verification;general-purpose markup language;program synthesis;safety engineering;static program analysis;unified framework	Cristina David;Pascal Kesseli;Daniel Kroening;Matt Lewis	2018	ACM Trans. Program. Lang. Syst.	10.1145/3174802	theoretical computer science;program synthesis;program analysis;code refactoring;static analysis;computer science	PL	-19.280161299771038	27.67075101065017	110165
1129419611d8538661cf47f94e3ae300b2889773	planner: a language for proving theorems in robots	various change;hierarchical control structure;general purpose;world change;deductive system	PLANNER is a language f o r p r o v i n g theorems and m a n i p u l a t i n g models in a r o b o t . The language i s b u i l t ou t o f a number o f p rob lem s o l v i n g p r i m i t i v e s t o g e t h e r w i t h a h i e r a r c h i c a l c o n t r o l s t r u c t u r e . S ta tements can b e a s s e r t e d and perhaps l a t e r w i t h d r a w n a s t h e s t a t e o f t he w o r l d changes. C o n c l u s i o n s can be drawn f r om t h e s e v a r i o u s changes i n s t a t e . Goals can be e s t a b l i s h e d and d i s m i s s e d when t h e y a r e s a t i s f i e d . The d e d u c t i v e system o f PLANNER is s u b o r d i n a t e t o t h e h i e r a r c h i c a l c o n t r o l s t r u c t u r e i n o r d e r t o make the language e f f i c i e n t . The use o f a g e n e r a l purpose m a t c h i n g language makes t h e d e d u c t i v e system more p o w e r f u l .	planner;re-order buffer	Carl Hewitt	1969			artificial intelligence;theoretical computer science;mathematics;programming language;algorithm	AI	-23.426092849417483	19.476893776319166	110230
1cfac480b841acf2a309502d05b7064e09420236	monitoring hyperproperties by combining static analysis and runtime verification		Hyperproperties are properties whose reasoning involve sets of traces. Examples of hyperproperties include information-flow security properties, properties of coding/decoding systems, linearizability and other consistency criteria, as well as privacy properties like data minimality. We study the problem of runtime verification of hyperproperties expressed as HyperLTL formulas that involve quantifier alternation. We first show that even for a simple class of temporal formulas, virtually no ∀∃ property can be monitored, independently of the observations performed. To manage this problem, we propose to use a combination of static analysis with runtime verification. By using static analysis/verification, one typically obtains a model of the system that allows to limit the source of “hypothetical” traces to a sound over-approximation of the traces of the system. This idea allows to extend the effective monitorability of hyperproperties to a larger class of systems and properties. We exhibit some examples where instances of this idea have been exploited, and discuss preliminary work towards a general method. A second contribution of this paper is the idea of departing from the convention that all traces come from executions of a single system. We show cases where traces are extracted from the observed traces of agents, from projections of a single global trace, or from executions of different (but related) programs.	approximation;black box;distributed computing;linearizability;mutual exclusion;predicate abstraction;quantifier (logic);runtime verification;static program analysis;symbolic execution;temporal logic;tracing (software);universal quantification	Borzoo Bonakdarpour;César Sánchez;Gerardo Schneider	2018		10.1007/978-3-030-03421-4_2		SE	-19.601325522404625	28.719770309344796	110336
0f7523ac81354f6be1959dd9ac6a8d5d5970a27a	work it, wrap it, fix it, fold it	institutional repository research archive oaister	The worker/wrapper transformation is a general-purpose technique for refactoring recursive programs to improve their performance, without compromising their correctness. The two previous approaches to formalising the technique were based upon different recursion operators, and different correctness conditions. In this article we show how these two approaches can be generalised in a uniform manner by combining and extending their correctness conditions, and explore the benefits that result.	code refactoring;correctness (computer science);foldit;general-purpose markup language;recursion	Neil Sculthorpe;Graham Hutton	2014	J. Funct. Program.	10.1017/S0956796814000045	computer science;data mining;database;programming language;algorithm	PL	-22.2459986052605	26.33207910503053	110469
5dc14bcb3abde09f281341d3fb527628e010cb56	an automatically generated, realistic compiler for an imperative programming language	formal specification;programming language;automatic generation	We describe the automatic generation of a complete, realistic compiler from formal specifications of the syntax and semantics of Sol/C, a nontrivial imperative language “sort of like C.” The compiler exhibits a three pass structure, is efficient, and produces object programs whose performance characteristics compare favorably with those produced by commercially available compilers. To our knowledge, this is the first time that this has been accomplished.	compiler;imperative programming;programming language;sol-feace	Uwe F. Pleban;Peter Lee	1988		10.1145/53990.54012	compiler;dynamic compilation;compiler-compiler;object language;compiler correctness;interprocedural optimization;computer science;theoretical computer science;compiler construction;formal specification;programming language;programming language specification;intrinsic function;functional compiler;algorithm	PL	-22.93190435183725	26.400177703686758	110583
b74e6aa45508a54bef0f65d450dea59e07cb9545	a retro/prospective on apl graphpak		This paper suggests two general directions that one could take to modernize APL2 Graphpak and revitalize its evolution. One direction springboards off lessons learned during Graphpak's first decade and from some thinking that evolved during early (c. 1980) experiments with general arrays. The second direction exploits general arrays and APL2 functionality at a user-level. Some experiments are reported relating to both areas	agi-plan;apl;array data structure;device independence;experiment;geometry instancing;graphics;high- and low-level;history of operating systems;operating system;prospective search;user space	Walter H. Niehoff	1998		10.1145/312627.312731	simulation;computer science	NLP	-29.70742120641172	25.267536258654626	111087
b8ac8bf1e629a548db846977a825940a98e2c1c2	mixer, supporting the model-view-controller design pattern in servlets	model view controller;design pattern	The problem of separating Java code and HTML code in Servlet programming is not yet completely solved since the existing tentative solutions either are too complex or not complete. We present Mixer which is a novel helper tool based on a template engine design philosophy to solve this problem. Mixer is easy to learn and to use. Mixer is also computationally very efficient. We have already tested and used Mixer successfully in large university courses dealing with HTTP server side programming.	algorithmic efficiency;gnu;html;hypertext transfer protocol;internet;java servlet;model–view–controller;open-source software;server-side;software design pattern;xml	Pierre Wijkman;Suru Dissanaike;Mitra Wijkman	2004			model–view–controller;real-time computing;design pattern;control engineering;computer science	Security	-30.760350785044153	27.3649983872438	111822
10f0b4107f1c70b7a41883aa6248244b6787788a	a conceptual and contextual object-oriented logic programming: the prolog++ language	object oriented programming;object oriented;conceptual graph;logic programs;knowledge base	Building upon J. Sowa's Conceptual Graph (CG) theory, this paper introduces basic elements of the new language, PROLOG++, subsuming Prolog with various objet oriented, conceptual and contextual extensions. A Prolog++ program is composed of a declarative knowledge base and a distributed strategic knowledge base; the latter forms a network of objects that communicate by sending messages. A message corresponds to a goal described by a term or a CG (simple or compound). Declarative knowledge base corresponds to a conceptual dictionary describing the semantic of concepts and relations used in CG. The declarative base is composed of two hierarchies, one for concepts and the other for relations, each element of the two hierarchies corresponds to an object made up of conceptual structures. This base thus endows Prolog++ with a second form of object oriented programming. Finally, Prolog++ provides, as predefined methods, a set of conceptual operations for editing and handling CG.	logic programming;prolog++	Adil Kabbaj;Claude Frasson;Marc Kaltenbach;Jean-Yves Djamen	1994		10.1007/3-540-58328-9_17	natural language processing;first-generation programming language;knowledge base;constraint programming;declarative programming;horn clause;programming domain;computer science;object;theoretical computer science;functional logic programming;programming paradigm;symbolic programming;inductive programming;fifth-generation programming language;programming language;object-oriented programming;prolog;logic programming	PL	-26.605406263863696	18.73513318079517	111842
f49bd69b6bac5e388f78f9a0356d644db9e9d01c	efficient formalism-only parsing of xml/html using the s-calculus	mismatching;virtual machine;representacion conocimientos;language interoperability;common type system;base donnee;metadata;xml language;database;base dato;semantics;semantica;semantique;analyse syntaxique;desadaptacion;intermediate language;analisis sintaxico;syntactic analysis;analizador sintaxico;xml document;exception handling;parser;desadaptation;analisis semantico;knowledge representation;analyse semantique;representation connaissances;analyseur syntaxique;langage html;langage xml;lenguaje xml;html language;lenguaje html;semantic analysis	Traditionally, correct parsing of XML and HTML has been littered with semantic hacks in the parsing code to deal with the oddities of these languages, since HTML accepts unbalanced tags and tags that do not match in case, but XML is less forgiving. The detection of well-formedness of XML documents has, to date, required semantic analysis outside of the grammar specification. We present a grammar-only (HT\X)ML parser which, upon detecting that it is parsing XML, modifies itself dynamically in order to insure that the document conforms to XML'S stricter rules. Our grammar detects unbalanced tags in XML, as well as mismatched case in otherwise balanced tags, while, at the same time, requiring XML document tag's attribute values to be in quotes, but accepting the looser attribute syntax when in an HTML document. On a 733 MHz Windows 2000 machine, our parser did a wellformedness detecting parse on XML documents such as the KJV Old Testament at a rate of 92 Kb/second, the Austin's Pride and Prejudice at a rate of 108 Kb/second, and Wolfgang May 's Mondial 3.0 database at a rate of 149 Kb/second.	html;loose coupling;microsoft windows;parsing;semantics (computer science);sensor;testament;unbalanced circuit;xml	Quinn Tyler Jackson	2003	SIGPLAN Notices	10.1145/772970.772974	well-formed document;xml catalog;xml validation;xml namespace;simple api for xml;xml;relax ng;computer science;document type definition;document structure description;parsing;xml framework;xml database;xml schema;database;semantics;xml signature;programming language;world wide web;xml schema editor;efficient xml interchange;sgml	Web+IR	-26.691568677652405	19.398106059725283	111883
5db4d7485db144001d54929e136384047eaf72f6	smallholder agriculture in the information age: limits and opportunities		Recent projections by the United Nations show that the food production needs to double by 2050 in order to meet the nutrition demand of the world's growing population. A key enabler of this growth are smallholder family farms, that form the backbone of agricultural (AG) production worldwide. To meet this increasing demand, smallholder farms need to implement critical advances in task management and coordination, crop and livestock monitoring and efficient farming practices. Information and Communication Technology (ICT) will play a critical role in these advances by providing integrated and affordable cyber-physical systems (CPS) that can longitudinally measure, analyze and control AG operations. In this paper we make headway towards the design and integration of such AG-CPS. We begin by characterizing the information and communication technology demand of smallholder agriculture based on traffic analysis of farm Internet use. Our findings inform the design and integration of an end-to-end AG-CPS called FarmNET that provides (i) robust control mechanisms for multi-sensor AG data collection and fusion, (ii) wide-area, heterogeneous wireless networks for ubiquitous farm connectivity, (iii) algorithms and models for farm data analytics that produce actionable information from the collected agricultural data, and (iv) control mechanisms for autonomous, proactive farming.	algorithm;autonomous robot;control system;cyber-physical system;end-to-end principle;gold farming;internet backbone;robust control;traffic analysis	Mariya Zhivkova Zheleva;Petko Bogdanov;Daphney-Stravoula Zois;Wei Xiong;Ranveer Chandra;Mark Kimball	2017		10.1145/3080556.3080563	the internet;environmental economics;task management;traffic analysis;information age;enabling;agriculture;environmental resource management;population;information and communications technology;engineering	Mobile	-32.7627633159585	19.012637812413494	112044
3e77d3f511b6c89967a8fede96a3f7836f2bbdbb	towards a verified reference implementation of a trusted platform module		We develop a reference implementation for a fragment of the API for a Trusted Platform Module. Our code is written in a functional language, suitable for verification with various tools, but is automatically translated to a subset of C, suitable for interoperability testing with production code, and for inclusion in a specification or standard for the API. One version of our code corresponds to the widely deployed TPM 1.2 specification, and is vulnerable to a recently discovered dictionary attack; verification of secrecy properties of this version fails producing an attack trace and highlights an ambiguity in the specification that has security implications. Another version of our code corresponds to a suggested amendment to the TPM 1.2 specification; verification of this version succeeds. From this case study we conclude that recent advances in tools for verifying implementation code for cryptographic APIs are reaching the point where it is viable to develop verified reference implementations. Moreover, the published code can be in a widely understood language like C, rather than one of the specialist formalisms aimed at modelling cryptographic protocols.	application programming interface;cryptographic protocol;dictionary attack;functional programming;interoperability;reference implementation;trusted platform module;verification and validation	Aybek Mukhamedov;Andrew D. Gordon;Mark Ryan	2009		10.1007/978-3-642-36213-2_11	trusted platform module	Security	-20.873556238386676	27.787368123093447	112058
fb8cc9cf2cd015275d9cfd6885a449f979c86b1b	a java desktop tool for mining moodle data			java desktop system	Rafael Pedraza Perez;Cristóbal Romero;Sebastián Ventura	2011			operating system;programming language;java;world wide web;java annotation	Logic	-30.141721667826626	27.82155506343456	112072
428ffe6eb382334cc7e058df6f708abdb2e68261	extending feathertrait java with interfaces	variable etat;indonesia;objet;types;langage oriente objet;indonesie;object oriented language;comportement;composition;composicion;05bxx;object oriented language design;68n19;state variable;object;conception;featherweight java;interfase;asie;conducta;object oriented;informatique theorique;interface;diseno;variable estado;oriente objet;design;behavior;inheritance;orientado objeto;objeto;object oriented languages;asia;computer theory;java;informatica teorica	In the context of Featherweight Java by Igarashi, Pierce, and Wadler, and its recent extension FeatherTrait Java (FTJ) by the authors, we investigate classes that can be extended with trait composition. A trait is a collection of methods, i.e., behaviors without state; it can be viewed as an “incomplete stateless class” i.e., an interface with some already written behavior. Traits can be composed in any order, but only make sense when “imported” by a class that provides state variables and additional methods to disambiguate conflicting names arising between the imported traits. We introduce FeatherTrait Java with Interfaces (iFTJ), where traits need to be typechecked only once, which is necessary for compiling them in isolation, and considering them as regular types, like Java-interfaces with a behavioral content.	compiler;java platform, enterprise edition;stateless protocol	Luigi Liquori;Arnaud Spiwack	2008	Theor. Comput. Sci.	10.1016/j.tcs.2008.01.051	computer science;artificial intelligence;programming language;object-oriented programming;algorithm;generics in java	PL	-25.67335906437337	27.035218080336445	112257
126a1a4534f7fbf6fd03217874b9d743fa5db4e5	a language based formalism for domain driven development	language-based formalism;high level language;4compiler;compiler framework;4th generation languages;application engineering;domain feature;domain engineering;driving force;two-level grammar;feature model;domain driven development paradigm;machine language;generation language;generative domain model;assembly language;programming language;domain model;software development	The evolution of programming languages (e.g. machine languages, assembly languages and high level languages) has been the driving force for the evolution of software development from the machine-centric to the application-centric. The 4th generation languages (4GLs), languages defined directly by the composition of domain features, serve as the language-based formalism for the emerging Domain Driven Development paradigm. The 4GLs are defined in Two-Level Grammar++ and can be compiled into 3GLs using the 4GL compiler framework.	assembly language;compiler;fourth-generation programming language;high-level programming language;history of programming languages;programming paradigm;semantics (computer science);software development	Wei Zhao	2003		10.1145/949344.949449	domain analysis;natural language processing;fourth-generation programming language;domain;programming domain;computer science;domain-specific language;software development;third-generation programming language;feature-oriented domain analysis;domain engineering;domain model;syntax;computer programming;programming paradigm;ontology language;fifth-generation programming language;programming language theory;programming language;second-generation programming language;high-level programming language;comparison of multi-paradigm programming languages;feature model	SE	-25.583616585891722	21.914468451536983	112382
8b7168abaad2669b446f78f842a12515f3459dac	completeness and nondeterminism in model checking transactional memories	content management;processor architecture;model checking;software transactional memory;concurrent programs;transactional memory	Software transactional memory (STM) offers a disciplined concurrent programming model for exploiting the parallelism of modern processor architectures. This paper presents the first deterministic specification automata for strict serializability and opacity in STMs. Using an antichain-based tool, we show our deterministic specifications to be equivalent to more intuitive, nondeterministic specification automata (which are too large to be determinized automatically). Using deterministic specification automata, we obtain a complete verification tool for STMs. We also show how to model and verify contention management within STMs. We automatically check the opacity of popular STM algorithms, such as TL2 and DSTM, with a universal contention manager. The universal contention manager is nondeterministic and establishes correctness for all possible contention management schemes.	algorithm;automata theory;automaton;concurrent computing;correctness (computer science);model checking;programming model;software transactional memory	Rachid Guerraoui;Thomas A. Henzinger;Vasu Singh	2008		10.1007/978-3-540-85361-9_6	model checking;transactional memory;parallel computing;real-time computing;microarchitecture;content management;computer science;software transactional memory;distributed computing;programming language	PL	-20.391890837739222	31.17972363466813	112887
0f7be62ac797ef6711328a3d746fbcbcd0c5fd01	modules for standard ml	standard ml;data type;polymorphism;separate compilation;functional programming language;type system	The functional programming language ML has been undergoing a thorough redesign during the past year, and the module facility described here has been proposed as part of the revised language, now called Standard ML. The design has three main goals: (1) to facilitate the structuring of large ML programs; (2) to support separate compilation and generic library units; and (3) to employ new ideas in the semantics of data types to extend the power of ML's polymorphic type system. It is based on concepts inherent in the structure of ML, primarily the notions of a declaration, its type signature, and the environment that it denotes.	approximation;compiler;curry;declaration (computer programming);functional programming;lambda calculus;programming in the large and programming in the small;programming language;standard ml;type signature;type system;type theory	David B. MacQueen	1984		10.1145/800055.802036	type class;polymorphism;parametric polymorphism;type system;data type;type safety;computer science;database;programming language;functional programming;algorithm	PL	-25.185608067595584	26.4286086989465	113178
8c7ada381a69c8db78f3543093910a7f728a8291	towards an intermediate language based on graph rewriting	term rewrite system;intermediate language;graph rewriting;declarative languages;part of book or chapter of book;support function	Lean is an experimental language for specifying computations in terms of graph rewriting. It is based on an alternative to Term Rewriting Systems (TRS) in which the terms are replaced by graphs. Such a Graph Rewriting System (GRS) consists of a set of graph rewrite rules which specify how a graph may be rewritten. Besides supporting functional programming, Lean also describes imperative constructs and allows the manipulation of cyclic graphs. Programs may exhibit non-determinism as well as parallelism. In particular, Lean can serve as an intermediate language between declarative languages and machine architectures, both sequential and parallel.	abstract rewriting system;compiler;computation;functional programming;graph (discrete mathematics);graph rewriting;imperative programming;lean integration;lean software development;model of computation;nondeterministic algorithm;parallel computing;rewrite (programming);unix	Hendrik Pieter Barendregt;Marko C. J. D. van Eekelen;John R. W. Glauert;Richard Kennaway;Marinus J. Plasmeijer;M. Ronan Sleep	1987		10.1007/3-540-17945-3_9	natural language processing;computer science;abstract semantic graph;programming language;confluence;algorithm;graph rewriting	PL	-23.70958596550633	22.42337269017557	113306
079ffd68f2a88a9aed2fe05bff48c91682eb6b75	exercises for teaching logic in a formal methods course: formalizing erds	formal specification;software engineering;formal method;computer science education;first order;entity relationship	In a first course in formal methods for software engineers, the emphasis on the topic of logic should be on using (first-order predicate) logic to specify and formally describe properties. In this paper, we suggest using ERDs (Entity-Relationship Diagrams) as support for formalization exercises. Starting from graphical descriptions (ERDs) and textual informal specification of various constraints, students have to produce an equivalent textual and formal specification. We present the notation we use in our course (Spec), some heuristics to obtain the formal concepts from ERDs and a small example.	diagram;entity–relationship model;expressive power (computer science);first draft of a report on the edvac;first-order predicate;formal methods;formal specification;graphical user interface;heuristic (computer science);philippe kruchten;software engineer;spec#;unified modeling language	Guy Tremblay	1999		10.1145/299649.299698	formal methods;entity–relationship model;formal verification;computer science;theoretical computer science;first-order logic;formal specification;programming language	SE	-25.92766915408112	20.058364447218267	113310
0336e17dbc6b67e26ee8aa273e04416c5681e223	robogardner: a low-cost system with automatic plant identification using markers		In this modern era, automation is inevitable. With the fast paced and busy lifestyles, there is no time even for day to day household activities. There is a need of automation in each and every small activity performed by humans. In this paper we present an autonomous system that caters to the need of automation in gardening by providing a low cost, portable and efficient system for watering indoor potted plants at home and offices. The system comprises of a mobile ROBO equipped with a camera for auto-location and identification of the plants and a sensing circuitry for analyzing the watering needs. The RoboGardner is designed to ease out human workload and performs all the functions without any human intervention. It even provides an automatic feedback mechanism to inform the user about its daily performance. The water level of the RoboGardner’s on-board reservoir is also scrutinized automatically by it and the user is alarmed to refill when required. The paper outlines the complete architecture, functional modules and detailed implementation supported with the design circuits. It concludes with the system performance of the RoboGardner along with graphical analysis.		Reema Aswani;N. Hema	2013		10.1007/978-3-642-37949-9_27	embedded system;simulation;engineering;operations management	Mobile	-31.607736754482794	20.769968626123458	113491
4c9853e18f605fdad43e550eaa2a8fdad0fcb423	binding time analysis for high order untyped functional languages	side effect;partial evaluation;time use;binding time analysis;higher order functions;static analysis;abstract interpretation;functional language;data structure	When some inputs of a program are known at compile-time, certain expressions can be processed statically; this is the basis of the notion of partial evaluation. Identifying these early computations can be determined independently of the actual values of the input by a static analysis called binding time analysis. Then, to process a program, one simply follows the binding time information: evaluate compile-time expressions and defer the others to run-time. Using abstract interpretation, we present a binding time analysis for an untyped functional language which provides an effective treatment of both higher order functions and data structures. To our knowledge it is the first such analysis. It has been implemented and is used in a partial evaluator for a side-effect free dialect of Scheme. The analysis is general enough, however, to be valid for non-strict typed functional languages such as Haskell. Our approach and the system we have developed solve and go beyond the open problem of partially evaluating higher order functions described in [3] since we also provide a method to handle data structures. Our analysis improves on previous work [5, 15, 4] in that: (1) it treats both higher order functions and data structures, (2) it does not impose syntactic restrictions on the program being processed, and (3) it does not require a preliminary phase to collect the set of possible functions that may occur at each site of application.	abstract interpretation;compile time function execution;compiler;computation;data structure;functional programming;haskell;higher-order function;interpreter (computing);name binding;partial evaluation;scheme;side effect (computer science);static program analysis;strict function	Charles Consel	1990		10.1145/91556.91668	data structure;computer science;theoretical computer science;programming language;functional programming;higher-order function;partial evaluation;static analysis;side effect;algorithm	PL	-20.416029129119693	29.90374320584668	113568
6bc2fe099c51540a0ddd1372a9ad63025a738312	reachability analysis of the html5 parser specification and its application to compatibility testing		A draft standard for HTML, HTML5, includes the detailed specification of the parsing algorithm for HTML5 documents, including error handling. In this paper, we develop a reachability analyzer for the parsing specification of HTML5 and automatically generate HTML documents to test compatibilities of Web browsers. The set of HTML documents are extracted using our reachability analysis of the statements in the specification. This analysis is based on a translation of the specification to a conditional pushdown system and on a new algorithm for the reachability analysis of conditional pushdown systems. In our preliminary experiments, we generated 353 HTML documents automatically from a subset of the specification and found several compatibility problems by supplying them to Web browsers.	algorithm;alloy analyzer;compatibility testing;exception handling;experiment;html;html5;library (computing);parser;reachability;stack (abstract data type)	Yasuhiko Minamide;Shunsuke Mori	2012		10.1007/978-3-642-32759-9_26	computer science;database;programming language;algorithm	SE	-27.50458600240166	27.1060397333127	113664
d67ff630749af002600f26b83ce54128763e1c04	lambda-calculi with decidable cappa-type checking	type checking	Teaching and research in computer science and (until 1982) mathematics. Initiated and led the design and implementation of major and new department in Computer Science. Chaired department for first three years of its existence. Involved in designing a typed intermediate language for a Java compiler. Part-time consultant on cryptography and computer security.	computer science;computer security;cryptography;java compiler;type system	Kevin E. Flannery	1993			discrete mathematics;model checking;lambda;decidability;mathematics	Theory	-21.097244933373357	20.053196532273045	113755
605d55066f16e6b4ae05c0ce446a9188c9e69deb	constraint-based specification of production rules	human computer interaction;formal specification;expert systems;user interfaces expert systems formal specification knowledge representation software tools specification languages;specification languages;production systems programming profession automatic testing filtering pipelines computer science specification languages graphical user interfaces software engineering expert systems;human computer interaction rule representation rule based systems production rules constraint languages specification rule pattern tests programmer network program;software tools;knowledge representation;network programming;user interfaces;production rule	Production system languages are designed primarily for specification of rule match conditions. Currently, such languages suffer three drawbacks: conceptual reliance on binary tests, explicit ordering of tests, and unintuitive syntax mirroring the underlying match implementation. We introduce Constraints as a specification language for match, thereby eliminating these problems. Besides their conceptual simplicity for specifying match conditions, constraints can be visually represented and manipulated with graphical user interfaces. Use of constraint languages ultimately improves the software engineering of production systems. The production system match-act cycle [15] underlies most efficient rule-based expert systems. The core of this efficiency is RETE [3], a hghly optimized algorithm for performing the computationally expensive matching of rule pattems against working memory (WM). RETE uses a compiled network of ordered tests to filter and join WM objects into the tuple instantiations satisfying those tests. The OPS family of languages [4] is the programmer's primary interface to RETE and production systems. An OPS rule specifies both the match conditions necessary for firing, and the rule's actions. Since OPS is specifically designed for RETE network specification and construction, its syntax very much resembles the resulting networks. This requires the programmer to 1. specify the test conditions, and 2. spec@ the ordering of tests, so that 3. OPS can construct the network. With advances in optimization of test ordering [8] and general program transformations for network construction [lo], the mixing of disparate tasks at the language level is becoming unnecessary. Constraints offer a new approach to pattem specification. Constraint languages are used for a variety of general [13] and specific [5] programming tasks. Constraints and their graphs are also readily manipulated via graphical interfaces [ 13. Further, constraints have been used for representing database join relations, which are similar to RETE'S joins [7]. In this paper, we propose the use of constraint languages for the specification of rule pattem tests. This wiIl help the programmer focus on his primary objective: specifying the match condtions. We show below how such constraints can then be mechanically operationalized into rule tests. Following the automated optimal ordering of these tests, the resulting ordered tests can be cast into an efficient network (or other [ 111) program. By restricting the programmer's task to the match specification, new (graphical and other) languages can be devised for highly effective human-computer interaction. The Se mantics of RETE Tests RETE is an algorithm for conjunctive match. If a rule has n conditions1, there are IWMP candidate tuples that can be formed from working memory. Conjunctive match determines which of these tuples satisfy all the match conditions. Operationally, a candidate tuple undergoes a sequence of tests; a single failure eliminates the candidate tuple as a possible instantiation. Since RETE is derivable via partial evaluation [6] and finite differencing [9] from a simpler program that tests a single candidate tuple against the tests [lo], it suffices to discuss the operational semantics of testing just a single tuple. RETE first tests each component WM object of a tuple with filtering predicates. These inexpensive unary and binary2 tests are performed on an object in pipeline fashion as a chain of 1-input alpha sift nodes. *This work was supported in part by grant R29 LM 04707 from the National Library of Medicine. Without loss of generality, we shall not discuss the blocking action of absence test conjuncts. * To remain consistent with the OPS-5 language, we restrict the k-ary tests, k r l , to binary tests (k=2). 332 1984/89/0000/0332$01.00	analysis of algorithms;autoregressive integrated moving average;blocking (computing);compiler;disk mirroring;expert system;graph (discrete mathematics);graphical user interface;human–computer interaction;join (sql);logic programming;mathematical optimization;ops5;operational semantics;partial evaluation;pipeline (computing);production (computer science);production system (computer science);program transformation;programmer;rete algorithm;rule 90;software engineering;specification language;unary operation;universal instantiation	Mark W. Perlin	1989		10.1109/TAI.1989.65338	knowledge representation and reasoning;specification language;computer science;artificial intelligence;theoretical computer science;formal specification;programming language;computer network programming;user interface;expert system;language of temporal ordering specification;specification pattern	PL	-28.314576659401943	19.895232537017364	113768
1bcb1f55bf16e3c002d49bafad166deb51105d3e	compact resettable counters through causal stability	haslab haslab uminho	Conflict-free Data Types (CRDTs) were designed to automatically resolve conflicts in eventually consistent systems. Different CRDTs were designed in both operation-based and state-based flavors such as Counters, Sets, Registers, Maps, etc. In a previous paper [2], Baquero et al. presented the problem with embedded CRDT counters and a solution, covering state-based counters that can be embedded in maps, but needing an ad-hoc extension to the standard counter API. Here, we present a resettable operation-based counter design, with the standard simple API and small state, through a causal-stability-based state compaction.	application programming interface;causal filter;conflict-free replicated data type;counter (digital);data compaction;embedded system;eventual consistency;hoc (programming language);map	Georges Younes;Paulo Sérgio Almeida;Carlos Baquero	2017		10.1145/3064889.3064892	distributed computing;data type;real-time computing;computer science;eventual consistency	Logic	-27.532430083386526	31.40535224846282	113828
0775660f6b029be1085573cf95b63546cd24b06a	dependent types in practical programming	types;errors;continuations;programming language;theses;constraint satisfaction;computer programming;accuracy;type checking;polymorphism;indexation;compiler optimization;optimization;bytecode;dependent types;higher order functions;type inference;language design;programming languages;type system;polymorphic recursion;subroutines;java	We present an approach to enriching the type system of ML with a restricted form of dependent types, where type index objects are drawn from a constraint domain C, leading to the DML(C) language schema. This allows specification and inference of significantly more precise type information, facilitating program error detection and compiler optimization. A major complication resulting from introducing dependent types is that pure type inference for the enriched system is no longer possible, but we show that type-checking a sufficiently annotated program in DML(C) can be reduced to constraint satisfaction in the constraint domain C. We exhibit the unobtrusiveness of our approach through practical examples and prove that DML(C) is conservative over ML. The main contribution of the paper lies in our language design, including the formulation of type-checking rules which makes the approach practical. To our knowledge, no previous type system for a general purpose programming language such as ML has combined dependent types with features including datatype declarations, higher-order functions, general recursions, let-polymorphism, mutable references, and exceptions. In addition, we have finished a prototype implementation of DML(C) for an integer constraint domain C, where constraints are linear inequalities (Xi and Pfenning 1998).	constraint satisfaction;dependent type;error detection and correction;exception handling;general-purpose programming language;higher-order function;hindley–milner type system;immutable object;integer programming;linear inequality;mathematical optimization;optimizing compiler;prototype;recursion;type inference	Hongwei Xi;Frank Pfenning	1999		10.1145/292540.292560	polymorphism;type system;constraint satisfaction;computer science;theoretical computer science;type inference;computer programming;programming language;java;algorithm	PL	-22.288669220608938	26.72939848750304	114037
f78991580c418caba3a987a69687c790ebf1acfb	synchronous sequence charts in action	distributed application;electronic mail;computacion informatica;reference model;grupo de excelencia;expressive power;ciencias basicas y experimentales;point of view;high level language	We identify a number of styles for using Interworkings (synchronous sequence charts), together with their roles in the context of the OS1 reference model. \Ve employ the well-known ABP (alternating bit protocol) to see how Interworkings can and cannot be used. This experiment shows that the charts are attractive from an intuitive point of view, but when used in their purest form, lack sufficient expressive power. Some of the distinctions in style can be interpreted as distinct approaches to adding expressive power.		Loe M. G. Feijs	1997	Information & Software Technology	10.1016/S0950-5849(97)00019-0	reference model;computer science;artificial intelligence;database;programming language;high-level programming language;expressive power;algorithm	SE	-32.44529466433531	30.45509834267616	114039
c65f406aaceddda0fa4c4ab6de15c13b1235915e	strongaspectj: flexible and safe pointcut/advice bindings	aspectj;interprocedural analysis;java programming;dataflow analysis;aspect oriented;language engineering;type safety;type system	AspectJ was designed as a seamless aspect-oriented extension of the Java programming language. However, unlike Java, AspectJ does not have a safe type system: an accepted binding between a pointcut and an advice can give rise to type errors at runtime. In addition, AspectJ's typing rules severely restrict the definition of certain generic advice behavior.  In this paper, we analyze the roots of these type errors, and describe measures to recover type safety for both generic and non-generic pointcut/advice declarations. Pointcuts quantify over heterogeneous sets of join points and are hence typed using type ranges in our approach, while type variables and a dual advice signature allow to express the generic and invasive nature of advices. Using these mechanisms, we can express advice that augments, narrows or replaces base functionality in possibly generic contexts. As a language engineering contribution, we integrate our proposal with the AspectJ language, and we provide a prototype implementation as a plugin for the AspectBench Compiler (abc). On a theoretical level, we present a formal definition of the proposed constructs and typing rules, and develop proofs for their type safety properties.	aspect-oriented software development;aspectj;compiler;java;pointcut;programming language;prototype;run time (program lifecycle phase);seamless3d;type rule;type safety;type system	Bruno De Fraine;Mario Südholt;Viviane Jonckers	2008		10.1145/1353482.1353491	real-time computing;type system;aspect-oriented programming;type safety;computer science;programming language;algorithm	PL	-26.06487593578094	29.521911621125668	114060
7e596f636ec143f0769fcb8933065f439998419f	semantics of programming languages: using asf+sdf	asf sdf;reuse;action semantics;modularity;component based semantics	A semantic specification of a programming language can be relevant for programmers to understand software written in the language, as well as for the implementers of a language to understand the intentions of its designers. In the early 1980s, Jan Heering and Paul Klint envisioned complete language specifications as libraries of reusable individual constructs, supported by a generic programming environment. This led to the development of the Asf+Sdf specification language and its Meta-Environment.This paper first recalls how programming languages can be specified in Asf+Sdf. It then analyses the apparent difficulty of reusing the specifications of individual constructs, and considers some alternative styles of semantics that have been supported using Asf+Sdf and its Meta-Environment. It is suggested that these alternative styles could facilitate reuse when specifying programming languages in Rascal, which has superseded Asf+Sdf. We recall how programming languages can be specified in ASF+SDF.We analyse the reusability of ASF+SDF specifications of individual constructs.Some alternative styles of semantics provide greater reusability than ASF+SDF.Component-based semantics is based on reusable fundamental constructs (funcons).Our analysis could influence the evolution of Rascal, which has superseded ASF+SDF.	programming language;semantics (computer science)	Peter D. Mosses	2015	Sci. Comput. Program.	10.1016/j.scico.2013.11.038	natural language processing;action semantics;computer science;reuse;modularity;programming language	PL	-25.033605004577552	21.996117813718463	114548
5e81c14c7b1e2044e1385c242db16afb44936543	a generic visualization platform for cp	multiple views;search trees;design and implementation;constraint programming;constraint system;open source	In this paper we describe the design and implementation of CP-VIZ, a generic visualization platform for constraint programming. It provides multiple views to show the search tree, and the state of constraints and variables for a postmortem analysis of a constraint program. Different to most previous visualization tools, it is system independent, using a light-weight, intermediate XML format to exchange information between solvers and the visualization tools. CP-VIZ is available under an open-source licence, and has already been interfaced to four different constraint systems.	constraint programming;open-source license;open-source software;search tree;xml	Helmut Simonis;Paul Davern;Jacob Feldman;Deepak Mehta;Luis Quesada;Mats Carlsson	2010		10.1007/978-3-642-15396-9_37	concurrent constraint logic programming;constraint programming;constraint satisfaction;computer science;theoretical computer science;database;distributed computing;programming language	Visualization	-28.403805255352754	26.578974646039562	114660
28b870339d243e7e69fedfa1bea53c1a8cd2c7f3	timelog-an intelligent spreadsheet for school timetabling	intelligent design;programming language;rule based	Timetabling consists of manipulating a large file of teaching allocations within a set of constraints. There are available large wall charts with coloured stick-on squares which make the scheduling task a little easier; but most timetables are designed on a large sheet of paper divided into cells, each cell corresponding to a particular teaching period and holding data for a single allocation of teacher, class, room and subject. The computing equivalent of such a grid of cells is a spreadsheet. The process of manipulating data within the constraints of a set of rules is a task for which the programming language PROLOG is ideally suited. This paper outlines an attempt to combine the rule based approach of PROLOG with the interactive power of the spreadsheet to produce an intelligent design aid for a school timetabler.	chart;programming language;prolog;schedule;scheduling (computing);spreadsheet;timeline	Charles J. V. Murphy	1987	SIGART Newsletter	10.1145/29264.29265	rule-based system;computer science;artificial intelligence;intelligent design;programming language;algorithm	AI	-30.17706620904178	23.126856298170942	114738
131e070266977513c3295b4c26fce96d1bba26ce	a tla solution to the rpc-memory specification problem	tla solution;rpc-memory specification problem	We present a complete solution to the Broy Lamport speci cation problem Our speci cations are written in TLA a formal lan guage based on TLA We give the high levels of structured proofs and sketch the lower levels which will appear in full elsewhere	remote procedure call;specification (regression)	Martín Abadi;Leslie Lamport;Stephan Merz	1994		10.1007/BFb0024426	programming language;mathematical proof;sketch;formal language;computer science	Theory	-21.112459877186858	21.162916963553663	114889
0e2fee68f739e1548b9dbb16063444c62cea078f	real-time data reduction at the network edge of internet-of-things systems	time series analysis logic gates delays real time systems monitoring intelligent sensors;ubiquitous computing data reduction internet of things power aware computing time series;logic gates;monitoring;time series analysis;robot sensors real time data reduction internet of things systems iot data sources network edge computing io bottlenecks energy costs storage reduction bandwidth reduction time series big datasets perceptually important points;intelligent sensors;delays;real time systems	The expected huge increase in the number of IoT data sources (sensors, embedded systems, personal devices etc.) will give rise to network-edge computing, i.e., data pre-processing, local storage, and filtering close to the data sources. Specifically, data reduction at the network edge (e.g., on an IoT gateway device or a mini-server deployed locally at an IoT area network) can prevent I/O bottlenecks, as well as dramatically reduce storage, bandwidth, and energy costs. However, current solutions face two main obstacles towards achieving this benefits of network-edge computing. Firstly, the most efficient algorithms for data reduction of time series (which is one of the prevailing kinds of data in IoT) are developed to work a posteriori upon big datasets and they cannot take decisions per incoming data item. Secondly, the state of the art lacks systems that can apply any of many different possible data reduction methods without adding significant delays or heavyweight re-configurations. This paper presents a solution that automates the switching between different data handling algorithms at the network edge, including an analysis of adjusted data reduction methods, as well as three flavors of a new algorithm that is capable of performing real-time reduction of incoming time series items based on the concept of Perceptually Important Points. The potential benefits are evaluated upon real datasets from street, household, and robot sensors, showing that our solution achieves accuracies between 76,1 % and 93,8 % despite forwarding only 1/3 of the data items, without adding significant forwarding delays.	algorithm;big data;clickstream;data item;data pre-processing;database;edge computing;embedded system;input/output;preprocessor;real-time clock;real-time data;robot;sonar (symantec);sensor;server (computing);smart meter;thread-local storage;time series;universal instantiation	Apostolos Papageorgiou;Bin Cheng;Ernö Kovacs	2015	2015 11th International Conference on Network and Service Management (CNSM)	10.1109/CNSM.2015.7367373	embedded system;real-time computing;logic gate;computer science;operating system;time series;data mining;computer security;statistics;computer network;intelligent sensor	DB	-31.98096945233656	18.43009262733688	115078
2aa61e33fe2af8bed45f74f71de3e422865db657	a unification of inheritance and automatic program specialization	automatic control;developpement logiciel;herencia;lenguaje programacion;compilacion;object oriented language;programming language;programacion automatica;componente logicial;object oriented design;heritage;commande automatique;program transformation;composant logiciel;langage java;automatic programming;transformation programme;evaluation partielle;program specialization;program generation;unification;transformacion programa;programa aplicacion;application program;object oriented;desarrollo logicial;programme application;partial evaluation;software development;software component;metaprogrammation;langage programmation;compilation;oriente objet;lenguaje java;control automatico;evaluacion parcial;inheritance;metaprogramming;orientado objeto;metaprogramacion;unificacion;programmation automatique;java language;generic programming	Inheritance allows a class to be specialized and its attributes refined, but implementation specialization can only take place by overriding with manually implemented methods. Automatic program specialization can generate a specialized, efficient implementation. However, specialization of programs and specialization of classes (inheritance) are considered different abstractions. We present a new programming language, Lapis, that unifies inheritance and program specialization at the conceptual, syntactic, and semantic levels. This paper presents the initial development of Lapis, which uses inheritance with covariant specialization to control the automatic application of program specialization to class members. Lapis integrates objectoriented concepts, block structure, and techniques from automatic program specialization to provide both a language where object-oriented designs can be efficiently implemented and a simple yet powerful partial evaluator for an object-oriented language.	han unification;interpreter (computing);lapis;pl/i;partial evaluation;partial template specialization;programming language	Ulrik Pagh Schultz	2004		10.1007/978-3-540-30175-2_13	computer science;theoretical computer science;automatic control;programming language;object-oriented programming;partial evaluation;algorithm	PL	-24.837452023607938	27.749830256410032	115215
b2861d355146a427247ac27b3f5dafcea99ff49b	formal semantics for the pacemaker system specification	aadl annex;bless;pacemaker;assertion;requirements/specifications	"""This paper formally expresses the timing behavior of a cardiac pacemaker as defined in the PACEMAKER System Specification as understood by its principal author.  The PACEMAKER System Specification was publicly released by Boston Scientific to provide a real-world subject for application of formal methods in response to Jim Woodcock's request at FM2006 for an industrial Grand Challenge problem. PACEMAKER's use for purposes other than formal methods has been surprising in its variety. Most ambitious is the Software Certification Consortium's mock regulatory submission, PACEMAKER Grand Challenge, to show that a product with safety-critical software is in fact safe. McMaster University is designing a second-generation hardware platform to execute formally-verified software during system feature test validation with an electrical heart simulator to show correct behavior.  This paper uses first-order predicates, extended with a simple temporal operator, to formally express what the principal author understands to be """"correct"""" behavior defined in PACEMAKER."""	artificial cardiac pacemaker;consortium;darpa grand challenge;first-order predicate;formal methods;formal verification;grand challenges;ibm notes;jim woodcock;mock object;natural language;requirement;simulation	Brian R. Larson	2014		10.1145/2663171.2663182	embedded system;real-time computing;assertion;computer science;programming language	Logic	-19.622915179078817	28.05725992551458	115261
9423f54ac35fd5e9b19993d4f13164dda93b1d93	a fusion of lotus 1-2-3 and apl2	programming language	A fusion of the most popular computer program to date with one of the most. powerful and innovative programming language to date presents a great opportunity for synergy and accomplishment. Part of this synergy has been explored in the construction of an APL2 based prototype system called FUSION. The FUSION prototype has encompassed the ability to communicate with Lotus Development Corporation's 1-2-3 by way of the binary .WK1 file format.The following salient features of the FUSION prototype are discussed:&bull; the modelling of a worksheet in APL2&bull; the translation of Lotus 1-2-3 formulas to executable APL2 expressions&bull; access to other host programs from the worksheet	amd accelerated processing unit;apl;computer program;executable;lotus 1-2-3;programming language;prototype;synergy	Erik S. Friis	1989		10.1145/75144.75164	simulation;computer science;artificial intelligence;programming language	OS	-29.257896591412507	22.682748791214642	115262
95c9dda3a03da1fd4d00eece019525d5b8da6ce7	prover 91 - a parallel theorem prover (extended abstract).	theorem prover		automated theorem proving	Aleksandar Krapez;Miodrag Kapetanovic;Zoran Ognjanovic;Tatjana M. Petrovic	1992			discrete mathematics;computer science;artificial intelligence;automated theorem proving;programming language	Theory	-19.598721954000478	19.512917758509627	115334
c559455875fb53e9d97a8dc130830c5891f1c883	escher's complex objects: a demonstration of simplicity	complex objects;data model;public domain	ESCHER is a visual database editor for the object-relational data model. Here, we describe the essential features of E SCHER and what is demonstrated at this conference with our prototype. Readers are invited to download the public domain software or to follow more detailed write-ups given in the references. 1 VISUAL INFORMATION SYSTEMS Most data documents, time tables, CAD drawings, etc.  have a natural, usually hierarchical structure. V isually browsing and editing these structured data has several advantages over working with “flat” relational tables; navigating on the outer levels permits travelling great distances with few key strokes, e.g. between departure cities in Figure 1, which shows parts of 260 inner -Australian flight connections from the 1995 Ansett T ravel Planner . Secondly , unzooming gives a bird‘s view of the global structure, yet by descending into substructures, details like particular flight times can be reviewed. When used as an editor , aggregate objects can be deleted, moved, pasted with the push of a button. Complex objects may also include multimedia types of which we currently support GIF , JPG and XBM. ESCHER may also serve as producer of HTML-code, respectively Mathematica® input by means of a scripting language, called TclDB. Scripts written in TclDB, acting as methods, can be stored as ESCHER attributes as described in this proceeding‘s companion paper (Thamm and Wegner , 1998). 2 FLYING OVER DATALAND ESCHER includes some unusual features which can best be demonstrated hands-on. At the core of ESCHER‘s interaction paradigm is the notion of a finger. A finger is like a cursor and points to atomic or complex objects. In Figure 1, a finger F1, Figure 1 Navigating inside the flights database with F1 on destination Cairns which is depicted as a shaded area, rests on the tuple for Cairns within the set of destinations reachable from Darwin by direct flight. There can be several fingers within one table, in a collaborative environment even fingers belonging to dif ferent users. Navigating with these fingers in and out, back and to the next item, setting them with a QBE-like query , is rather intuitive, in particular when initiated with the mouse. Figure 2 shows the control stick-like dimensional characteristics. Another interesting feature, which was present right from the start of the project (Wegner , 1989), is the self-referencing metaschema, which is the (infinite) schema of all schemas, including of itself, and which also has a visualization. Other research topics treated in E SCHER are the proper handling of null values for atomic and complex objects, empty sets and lists, sorting and duplicate elimination algorithms, nesting and unnesting, ef ficient storage techniques  including pointer swizzling (Wegner et al., 1996)  and the use of self-referential methods for GUI and index management.	aggregate data;algorithm;computer-aided design;copy-on-write;cursor (databases);darwin;data model;database schema;download;gif;graphical user interface;html;hands-on computing;jpeg;joystick;object-relational database;pointer (computer programming);pointer swizzling;programming paradigm;prototype;public-domain software;query by example;relational model;scripting language;self-reference;shading;sorting;wolfram mathematica;x bitmap	Burkhardt Fischer;Jens Thamm;Lutz M. Wegner;Stephan Wilke;Christian Zirkelbach	1998			computer vision;computer science;artificial intelligence;object-oriented design;communication	DB	-29.723760923974943	25.60898813081267	115341
2bea126e1fd283340d78412dc565ce227c244a32	extending ease with new asn.1 encoding rules	software tool;ease;computacion informatica;grupo de excelencia;research and development;lessons learned;ciencias basicas y experimentales;abstract syntax;distinguished encoding rules;canonical encoding rules;asn 1;estelle	EASE (R. Lai, A. Lo, EASE: a software environment for automatic implementation of application protocol, Software—Practice and Experience, 26(1) (1996) 83–103) is a software tool that integrates Estelle and ASN.1 (Abstract Syntax Notation One) specifications to produce an integrated specification for application layer protocol, from which an automatic implementation can be generated. EASE, though a new tool, is already outdated because of the recent changes, including new syntaxes, and additions of encoding rules, to the 1994 ASN.1 standard. Building a complete compiler that supports the full ASN.1:94 standard is a huge task; many researchers and developers only implement a subset of the standard that meets their requirements, while others use tools for building certain stages of such a compiler. For EASE to keep in line with the 1994 standard, it is a big task and requires a major rewrite; the only two encoding rules that could be added to EASE without implementing the full 1994 ASN.1 syntaxes are the canonical encoding rules (CER) and the distinguished encoding rules (DER). This paper describes how EASE has been extended to include these new rules, and aims to provide researchers and developers with the lessons learned from this work, and with some pointers to developing an ASN.1 compiler that complies with the 1994 standard.	abstract syntax notation one	Richard Lai;France Cheong	1999	Information & Software Technology	10.1016/S0950-5849(99)00040-3	abstract syntax;computer science;engineering;software engineering;database;programming language;algorithm	SE	-27.94902128363473	20.089465739360207	115437
20bfa1c1c260c8b4993bbc056fe4c7b1797b54e4	towards model checking security of real time java software		More and more software libraries and applications in high-performance computing and distributed systems are coded using the Java programming language. The correctness of such pieces of code w.r.t. a given set of security policies often depends on the correct handling of timing between concurrent or recurrent events. Model-checking has proven to be an effective tool for verifying the correctness of software. In spite of the growing importance of this application area of formal methods, though, no approach exists that targets the problem of verifying the correctness of real-time software w.r.t. timed specifications. The few existing works focus on very different problems, such as schedulability analysis of Java tasks. In this paper we present an approach combining rule-based static analysis together with symbolic execution of Java code to extract networks of timed automata from existing software and then use Uppaal to model-check them against timed specifications. We show through a real-world case study that this approach can be helpful in model-checking security policies of real-time Java software.	automata theory;best, worst and average case;cognitive dimensions of notations;control flow;correctness (computer science);distributed computing;formal methods;heuristic;library (computing);logic programming;model checking;programming language;prototype;real time java;real-time clock;real-time computing;real-time web;refinement (computing);requirement;run time (program lifecycle phase);scheduling analysis real-time systems;software engineering;state space;static program analysis;supercomputer;symbolic execution;temporal logic;timed automaton;uppaal;verification and validation;worst-case execution time	Luca Spalazzi;Francesco Spegni;Giovanni Liva;Martin Pinzger	2018	2018 International Conference on High Performance Computing & Simulation (HPCS)	10.1109/HPCS.2018.00106	model checking;programming language;correctness;formal methods;software;real time java;computer science;symbolic execution;security policy;java	SE	-20.649723542554863	32.06970006451819	115506
7620e6acd927ba1bbc949f915350603f924a9c63	integration of the standard template library and the microsoft foundation class	lenguaje programacion;langage c;programming language;microsoft foundation classe mfc;programmation generique;microsoft foundation classes;c language;application windows;standard template library;iterator;langage programmation;c;standard template library stl;stl;lenguaje c;generic programming	A short example of a Windows® application is modified to use the generic containers, iterators, and function adapters of the C++ Standard Template Library. Comparison is made with the containers and iteration mechanism provided by the Microsoft Foundation Classes.	iteration;iterator;microsoft foundation class library;microsoft windows;standard template library	Paul Wolfgang;Yang Song	1999	SIGPLAN Notices	10.1145/606666.606679	microsoft visual studio;standard template library;c++;computer science;operating system;programming language;generic programming;algorithm	PL	-28.83104894247714	28.075134147109086	115602
975f962d445dfe4d40426091488a5c8c3ab1b46e	program development by stepwise transformations - the project cip. appendix: programming languages under educational and under professional aspects	professional aspects;programming languages;stepwise transformations;project cip;program development;programming language	Without Abstract	stepwise regression	Friedrich L. Bauer	1978		10.1007/BFb0014671	first-generation programming language;computer science;software engineering;fifth-generation programming language;programming language theory;programming language;algorithm	PL	-27.02168795946874	22.982916136648104	115921
17fff3918ef5b585f3f9dc69a1688d23a4e40188	a special-purpose language for picture-drawing	functional language	Special purpose languages are typically characterized by a type of primitive data and domain-speci c operations on this data. One approach to special purpose language design is to embed the data and operations of the language within an existing functional language. The data can be de ned using the type constructions provided by the functional language, and the special purpose language then inherits all of the features of the more general language. In this paper we outline a domain-speci c language, FPIC, for the representation of two-dimensional pictures. The primitive data and operations are de ned in ML. We outline the operations provided by the language, illustrate the power of the language with examples, and discuss the design process.	functional programming	Samuel N. Kamin;David Hyatt	1997			natural language processing;language identification;cache language model;first-generation programming language;picture language;universal networking language;language primitive;data manipulation language;object language;specification language;data control language;computer science;linguistics;low-level programming language;modeling language;natural language;programming language;language technology;high-level programming language;context-sensitive language	PL	-25.449601697549973	22.82254950105778	116068
83e64ce37ad088002841ffcd2545dccf8f0c5e8f	building tailorable hypermedia systems: the embedded-interpreter approach	framework language;persistent object-store;embedded-interpreter approach;new drawing media-type;hypermedia application framework;embeddable interpreter;dynamically tailorable hypermedia system;specific hypermedia system;new media-types;open point;hypermedia framework;object oriented;new media;application development	This paper discusses an approach for developing dynamically tailorable hypermedia systems in an object-oriented environment. The approach is aimed at making applications developed in compiled languages like Beta and C++ tailorable at run-time. The approach is based on use of: 1) a hypermedia application framework (DEVISE Hyper-media), and 2) an embeddable interpreter for the framework language. A specific hypermedia system is instantiated from the framework with the interpreter embedded in the executable. The specific hypermedia system has a number of “open points” which can be filled via the interpreter at run-time. These open points and the interpreter provide sufficient support to allow tailoring at run-time as well as compile-time. Among the types of tailoring supported are: 1) adding new media-types, 2) alternating editors for supported media-types, and 3) removing a supported media-type. The paper describes the framework and illustrates how the interpreter is integrated. It describes steps involved in tailoring a specific hypermedia system with a new drawing media-type, where graphical objects can be endpoints for links. Since the hypermedia framework uses a persistent object-store, a solution for handling persistent interpreted objects is presented. Finally, the approach is compared with other environments supporting tailoring.	application framework;beta;c++;compile time;compiled language;compiler;embedded system;executable;graphical user interface;hypermedia;new media;persistent object store;via (electronics)	Kaj Grønbæk;Jawahar Malhotra	1994		10.1145/191080.191098	new media;computer science;theoretical computer science;database;distributed computing;programming language;object-oriented programming;rapid application development	PL	-29.123664615843314	27.832259168794824	116526
215eeef55161d841457584772fe8dc71e29d786f	static analysis via abstract interpretation of multithreaded programs. (analyse statique de logiciels multitâches par interprétation abstraite)		The goal of this thesis is to present a generic static analysis of Java multithreaded programs. Multithreaded programs execute many task, called threads, in parallel. Threads communicate through the shared memory implicitly, and they synchronize on monitors, wait-notify primitives, etc... Some years ago dual core architectures started being distributed on the broad market at low price. Today almost all the computers are at least dual core. Manycore, i.e. putting more and more cores on the same CPU, is now the current trend of CPU market. This multicore revolution yields to new challenges on the programming side too, asking the developers to implement multithreaded programs. Multithreading is supported natively by the most common programming languages, e.g. Java and C#. The goal of static analysis is to compute behavioral information about the executions of a program, in a safe and automatic way. An application of static analysis is the development of tools that help to debug programs. In the field of static analysis, many different approaches have been proposed. We will follow the framework of abstract interpretation, a mathematical theory that allows to define and soundly approximate semantics of programs. This methodology has been already applied to a wide set of programming languages. The basic idea of generic analyzers is to develop a tool that can be plugged with different numerical domains and properties. During the last years many works addressed this issue, and they were successfully applied to debug industrial software. The strength of these analyzers is that the most part of the analysis can be re-used in order to check several properties. The use of different numerical domains allows to develop faster and less precise or slower and more precise analyses. In this thesis, the design of a generic analyzer for multithreaded programs is presented. First of all, we define the happens-before memory model in fixpoint form and we abstract it with a computable semantics. Memory models define which behaviors are allowed during the execution of a multithreaded program. Starting from the (informal) definition of the happens-before memory model, we define a semantics that builds up all the finite executions following this memory model. An execution of a multithreaded program is represented as a function that relates threads to traces of states. We show how to design a computable abstract semantics, and we prove the correctness of the resulting analysis, in a formal way. Then we define and abstract a new property focused on the non-deterministic behaviors due to multithreading, e.g. the arbitrary interleaving during the execution of different threads. First of all, the non-determinism of a multithreaded program is defined as difference between executions. If two executions expose different behaviors because of values read from and written to the shared memory, then that program is not deterministic. We abstract it in two steps: in the first step we collect, for each thread, the (abstract) value that it may write into a given location of the shared memory. At the second level we summarize all the values written in parallel, while tracking the set of threads that may have written it. At the first level of abstraction, we introduce the new concept of weak determinism. We propose other ways in order to relax the deterministic property, namely by projecting traces and states, and we define a global hierarchy. We formally study how the presence of data races may afflict the determinism of the program. We apply this theoretical framework to Java. In particular, we define a concrete semantics of bytecode language following its specification. Then we abstract it in order to track the information required by the analysis of multithreaded programs. The core is an alias analysis that approximates references in order to identify threads, to check the accesses to the shared memory, and to detect when two threads own a common monitor thereby inferring which parts of the code cannot be executed in parallel. The generic analyzer described above has been fully implemented, leading to heckmate, the first generic analyzer of Java multithreaded programs. We report and deeply study some experimental results. In particular, we analyze the precision of the analysis when applied to some common pattern of concurrent programming and some case studies, and its performances when applied to an incremental application and to a set of well-known benchmarks. An additional contribution of the thesis is about the extension of an existing industrial generic analyzer, Clousot, to the checking of buffer overrun. It turns out that this analysis is scalable and precise. In summary, we present an application of an existing, industrial, and generic static analyzer to a property of practical interest, showing the strength of this approach in order to develop useful tools for developers.	abstract interpretation;alias analysis;approximation algorithm;benchmark (computing);buffer overflow;central processing unit;computable function;computer;concurrent computing;correctness (computer science);debugging;fixed point (mathematics);forward error correction;generic programming;java;manycore processor;monitor (synchronization);multi-core processor;multithreading (computer architecture);numerical analysis;performance;programming language;scalability;shared memory;simultaneous multithreading;static program analysis;thread (computing);tracing (software)	Pietro Ferrara	2009				PL	-19.292772174929908	29.397373448361336	116887
f8fbb7461541ef655e2a5852ea32c253772717b4	compiler construction using modern tools	compiler construction;code generation	In this paper we discuss our experiences using a translator writing system in the compiler construction course. We have found that such a system provides a great deal of flexibility to the instructor. Students can easily construct a complete compiler including code generation for a small language. We believe our tools and experiences are transferable to other translator writing systems.	code generation (compiler);compiler	Robert E. Noonan	1986		10.1145/5600.5697	computer architecture;compiler;compiler correctness;computer science;compiler construction;optimizing compiler;bootstrapping;compilation error;programming language;inline expansion;intrinsic function;functional compiler;code generation	OS	-28.128095499273616	23.699939483386668	117004
21b50b06c9cf64a474fa06cea5fa66848d56a4c3	towards a theory of natural language generation: the connection between syntax and semantics.	natural language generation			Charles Grant Brown;T. Pattabhiraman;Pierre Massicotte	1987			natural language processing;abstract syntax;natural language programming;universal networking language;language primitive;formal semantics;object language;natural language user interface;syntax;semantics;linguistics;lexical choice;language technology;abstract syntax tree;computational semantics	NLP	-25.248987520145498	19.55980158898016	117019
23b238ec023fa88a73cdb471df180f562964e831	computer aided verification		Formal techniques for guaranteeing software correctness have made tremendous progress in recent decades. However, applying these techniques to real-world safety-critical systems remains challenging in practice. Inspired by goals set out in prior work, we report on a largescale case study that applies modern verification techniques to check safety properties of a radiotherapy system in current clinical use. Because of the diversity and complexity of the system’s components (software, hardware, and physical), no single tool was suitable for both checking critical component properties and ensuring that their composition implies critical system properties. This paper describes how we used state-of-theart approaches to develop specialized tools for verifying safety properties of individual components, as well as an extensible tool for composing those properties to check the safety of the system as a whole. We describe the key design decisions that diverged from previous approaches and that enabled us to practically apply our approach to provide machine-checked guarantees. Our case study uncovered subtle safety-critical flaws in a prerelease of the latest version of the radiotherapy system’s control software.	computer aided verification;content-control software;correctness (computer science);critical system;formal language;source lines of code;verification and validation	Stuart Pernsteiner;Calvin Loncaric;Emina Torlak;Zachary Tatlock;Xi Wang;Michael D. Ernst;Jonathan Jacky	2016		10.1007/978-3-319-41540-6		SE	-20.652753181234125	27.950652209380124	117270
f3298103a69e720027bb57e0939cebce50b32be8	implementing a domain-specific language using stratego/xt: an experience paper	compiler implementation;computer vision;domain specific language;transformation;language definition;language design	We describe the experience of implementing a Domain-Specific Language using transformation to a General Purpose Language. The domain of application is image processing and low-level computer vision. The transformation is accomplished using the Stratego/XT language transformation toolset. The implementation presented here is contrasted with the original implementation carried out many years ago using standard compiler implementation tools of the day. We highlight some of the unexpected advantages afforded to us, as language designers and implementers, by the source-to-source transformation technique. We also present some of the practical challenges faced in the implementation and show how these issues were addressed.	compiler;computer vision;digital subscriber line;domain-specific language;high- and low-level;image processing;parse tree;retargeting;source transformation;stratego/xt	Leonard G. C. Hamey;Shirley Goldrei	2008	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2008.03.043	transformation;compiler;universal networking language;object language;computer science;programming language implementation;domain-specific language;low-level programming language;programming language	PL	-28.679257700544817	27.67216760373731	117618
de55d2ee2ccad440983d404c297c11f1d6de1d30	extended timing diagrams as a specification language	distributed system;communication system;temporal logic;formal semantics;specification language;process calculus;formal verification;hardware design	Hardware designs increasingly evolve to distributed systems composed of multiple interacting components working in parallel. Extended Timing Diagrams presented in this paper are an intelligible graphical speci cation language especially suited for the description of such communicating systems. The formal semantics of extended timing diagrams is de ned in terms of a process calculus. This formalization permits applying the language as an entrance to formal design systems.	digital timing diagram;distributed computing;graphical user interface;high- and low-level;high-level synthesis;interaction;process calculus;prototype;semantics (computer science);simulation;specification language;timing diagram (unified modeling language);vhdl;well-founded semantics	Stefan Lenk	1994			formal system;formal methods;formal semantics;object language;specification language;formal verification;syntax;computer science;theoretical computer science;z notation;formal semantics;formal specification;formal equivalence checking;refinement;proof calculus;hardware description language;programming language;programming language specification;intelligent verification;algorithm;language of temporal ordering specification	EDA	-33.38591456321236	31.869958553540343	117670
ef2cbb33b7aacc7f65fd0d5810ebbcf51b4ebf08	exploring a model-oriented and executable syntax for uml attributes	object oriented language;code generation;first principle;target language;open source	Omar Badreddin, Andrew Forward, Timothy C. Lethbridge School of Electrical Engineering and Computer Science, University of Ottawa, Canada K1N 6N5 e-mail: obadr024@eecs.uottawa.ca, aforward@eecs.uottawa.ca, tcl@eecs.uottawa.ca Abstract Implementing UML attributes directly in an object-oriented language may not appear to be complex, since such languages already support member variables. The distinction arises when considering the differences between modelling a class and implementing it. In addition to representing attributes, member variables can also represent association ends and internal data including counters, caching, or sharing of local data. Attributes in models also support additional characteristics such as being unique, immutable, or subject to lazy instantiation. In this paper we present modeling characteristics of attributes from first principles and investigate how attributes are handled in several open-source systems. We look code-generation of attributes by various UML tools. Finally, we present our own Umple language along with its code generation patterns for attributes, using Java as the target language.	code generation (compiler);compiler;computer science;email;executable;human error;immutable object;java;lazy evaluation;lazy initialization;list of unified modeling language tools;open-source software;umple;universal instantiation	Omar Bahy Badreddin;Andrew Forward;Timothy Lethbridge	2013		10.1007/978-3-319-00948-3_3	natural language processing;object code;object language;computer science;applications of uml;common intermediate language;database;programming language;executable;code generation;abstract syntax tree;object constraint language	PL	-25.60801685051019	29.098912378251594	117839
d01aef04b4585977ba3730dcad6af25f76c7e303	visitor-based attribute grammars with side effect	dynamic typing;abstract syntax tree;attribute grammar;side effect;design pattern;visitor;scientific	The visitor design pattern is often applied to program traversal algorithms over Abstract Syntax Trees (ASTs). It defines a visitor, an object with a visit method that is executed for each node in the AST. These visitors have the advantage that the order of traversal is explicitly under control of the programmer, which is essential to deal with side-effectful computations. Unfortunately, the exchange of results between traversals is error-prone. Attribute Grammars (AGs) are an alternative way to write multi-traversal algorithms. An attribute evaluator decorates the AST with attributes in one or more traversals. The attributes form a convenient mechanism to exchange results between traversals. Unfortunately, AGs discourage the use of side effect. In this paper, we present ruler-front, a language capturing the combination of the above approaches. A ruler-front grammar can be translated to traversal algorithms in multiple languages. In this paper, we translate to the imperative, dynamically-typed language JavaScript.	side effect (computer science)	Arie Middelkoop;Atze Dijkstra;S. Doaitse Swierstra	2011	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2011.06.004	l-attributed grammar;type system;computer science;theoretical computer science;design pattern;programming language;attribute grammar;side effect;algorithm;abstract syntax tree	ECom	-22.77345206216033	26.03841235307668	117860
0a47c6099409b48a358f7479f2869a01c7226d0d	a typed representation for html and xml documents in haskell	domain specific language;xml document;domain specic language;type classes;structured documents	We define a family of embedded domain specific languages for generating HTML and XML documents. Each language is implemented as a combinator library in Haskell. The generated HTML/XML documents are guaranteed to be well-formed. In addition, each library can guarantee that the generated documents are valid XML documents to a certain extent (for HTML only a weaker guarantee is possible). On top of the libraries, Haskell serves as a meta language to define parameterized documents, to map structured documents to HTML/XML, to define conditional content, or to define entire web sites. The combinator libraries support element-transforming style, a programming style that allows programs to have a visual appearance similar to HTML/XML documents, without modifying the syntax of Haskell.	combinator library;domain-specific language;embedded system;html;haskell;library (computing);programming style;well-formed element;xml	Peter Thiemann	2002	J. Funct. Program.	10.1017/S0956796802004392	style sheet language;well-formed document;xml catalog;xml validation;xml;streaming xml;computer science;domain-specific language;document type definition;document structure description;xml framework;xml database;xml schema;database;xml signature;programming language;html element;world wide web;xml schema editor;efficient xml interchange;sgml	PL	-25.321663864945464	25.82062818272138	117907
df0389410407dd7053c294528814cdf501e0e520	obtaining generic classes automatically through a parameterization operator: a focus on constrained genericity	constrained genericity;abstract data types;object oriented programming;reuse;mathematical operators object oriented programming abstract data types software reusability;mathematical operators;reorganizing hierarchies;eiffel;software reusability;reorganizing hierarchies generic classes parameterization operator constrained genericity type parameter instantiation class reuse class restructuring operator eiffel classes restriction class generic parameter constraints object oriented programming;restructuring classes;object oriented programming libraries fellows object oriented modeling performance loss safety encapsulation security	Generic classes allow one, through type parameter instantiation, to obtain new classes that are adapted to different contexts. Therefore, genericity is an important support for class reuse. When the possibility of reusing a class to deal with elements of different domains arises, it is desirable to have the appropriated generic class. Nevertheless, the construction of generic classes is a task that must be planned in advance. Hence, the possibility of reusing a class in other domain can appear, but maybe the class is not prepared for this. In this paper, we present the definition of a class restructuring operator which allows one to obtain generic classes from non-generic classes. The operator is named “parameterize”. In particular, we present the operator definition for an environment that supports the reuse of Eiffel classes. First, we present briefly the operator and then we focus on those aspects concerned with how to obtain a restriction class to constrain generic parameters. All these aspects are introduced by using examples.	generic programming	Yania Crespo;Juan José Rodríguez Diez;José Manuel Marqués Corral	1999		10.1109/TOOLS.1999.796480	computer science;theoretical computer science;programming language;concept;algorithm	Vision	-29.776037846429713	29.786184211644	118010
2ff3bf8132d4b0ac869c638af8ae130ea0ae49a4	a dual-engine for early analysis of critical systems		This paper presents a framework for modeling, simulating, and checking properties of critical systems based on the Alloy language – a declarative, first-order, relational logic with a built-in transitive closure operator. The paper introduces a new dual-analysis engine that is capable of providing both counterexamples and proofs. Counterexamples are found fully automatically using an SMT solver, which provides a better support for numerical expressions than the existing Alloy Analyzer. Proofs, however, cannot always be found automatically since the Alloy language is undecidable. Our engine offers an economical approach by first trying to prove properties using a fully-automatic, SMT-based analysis, and switches to an interactive theorem prover only if the first attempt fails. This paper also reports on applying our framework to Microsoft’s COM standard and the mark-and-sweep garbage collection algorithm.	algorithm;alloy (specification language);alloy analyzer;automated theorem proving;canonical account;correctness (computer science);declarative programming;first-order predicate;garbage collection (computer science);informatics;international standard book number;network switch;numerical analysis;proof assistant;relational algebra;satisfiability modulo theories;simulation;simultaneous multithreading;solver;tracing garbage collection;transitive closure;undecidable problem	Aboubakr Achraf El Ghazi;Ulrich Geilmann;Mattias Ulbrich;Mana Taghdiri	2011			computer science;artificial intelligence;theoretical computer science;mathematics;programming language;algorithm	PL	-19.430593202414887	20.485356296484593	118028
83b614cdc02447555ae1cf75dc9f79440d1bfa01	structured fortran&#8212;an evolution of standard fortran	scientific application;structured programing;data type;control structures;control structure;natural languages computer languages surgery computer science application software computer applications vehicles genetic programming data structures standards publication;data structures;procedure oriented languages;fortran;fortran extensions;data structure;structured programing control structures data structures data types fortran extensions procedure oriented languages;data types	Fortran is 21 years old; many would say that it has not reached adulthood but senility. Yet it remains the language of use in the overwhelming majority of scientific applications of computers and no end to this situation is in sight. Moreover, its evolution through the standards process is very slow, too slow to give any hope that Fortran will ever in this way become a language in which structured programs can be easily and effectively written.	computer;fortran	Anthony Ralston;Jerrold L. Wagener	1976	IEEE Transactions on Software Engineering	10.1109/TSE.1976.233813	data structure;data type;computer science;theoretical computer science;operating system;third-generation programming language;software engineering;programming language;high-level programming language	Visualization	-28.462438576457174	24.043685254075562	118047
6dae5282d694a2fd150897e6c2bd2fbd6819ab38	metamodeling languages and metaprogrammable tools		ion over the object models of various underlying back-end persistence technologies. UDM supports multiple such back-ends as shown in 1.3. Figure 1.3: Universal Data Model Framework With the GMeta back-end, the model object network physically resides in GME’s relational internal object database. Direct access to this database is accomplished through the GMeta domain-generic API. With the XML back-end, the model object network resides in an XML file which may be manipulated through the domain-generic DOM API. The generated UDM API for a DSML is a code-level realization of the UML metamodel used to specify the abstract syntax of the DSML. This domain-specific API is a wrapper facade for the domain-generic UML API, which in turn is a wrapper facade for the domain-generic API of each supported underlying model persistence mechanism. Consequently, different metatransformations are used to metaprogram UDM for each of the supported back-ends. For example, the metatransformation for the GMeta back-end may be formalized as: UMLTGMeta : UMLADSML → GMetaADSML (1.4) It is often useful to construct a domain-specific UDM API which supports a DSML originally specified 2 MODELING TOOL ARCHITECTURES AND METAPROGRAMMABILITY 11 using MetaGME rather than UDM’s implementation of UML. Consequently, the UDM set of tools includes the MetaGME2UML model-to-model transformation: MetaGMETUML : MetaGMEADSML → UMLADSML (1.5) Clearly, UDM was designed with an eye toward the manipulation of models irrespective of the platform on which they were developed or the mechanism used to persist them. Because of its relationship with UML, UDM even supports XMI [18] import and export of models and metamodels. 2.3 Metaprogrammable Design Space Exploration DESERT When large-scale systems are constructed, in the early design phases it is often unclear what implementation choices could be used in order to achieve the required performance. In embedded systems, frequently multiple implementations are available for components (e.g. software on a general purpose processor, software on a DSP, FPGA, or an ASIC), and it is not obvious how to make a choice, if the number of components is large. Another metaprogrammable MIC tool can assist in this process. This tool is called DESERT (for Design Space Exploration Tool). DESERT expects that the DSML used allows the expression of alternatives for components (the design space) in a complex model. Once a design space is modeled, one can attach applicability conditions to the design alternatives. These conditions are symbolic logical expressions that describe when a particular alternative is to be chosen. Conditions could also link alternatives in different components via implication. One example for this feature is: “if alternative A is chosen in component C1, then alternative X must be chosen in component C2”. During the design process, engineers want to evaluate alternative designs, which are constrained by high-level design parameters like latency, jitter, power consumption, etc. Note that these parameters can also be expressed as symbolic logic expressions. DESERT provides an environment in which the design space can be pruned and alternatives rapidly generated and evaluated. DESERT consumes two types of models: component models (which are abstract models of simple components) and design space models (which contain alternatives). Note that for design space exploration the internals of simple components are not interesting, only a ”skeleton” of these components is needed. The 2 MODELING TOOL ARCHITECTURES AND METAPROGRAMMABILITY 12 design space models reference these skeletons. DESERT uses a symbolic encoding technique to represent the design space that is based on Ordered Binary Decision Diagrams (OBDD-s) [6]. OBDD-s are also used to represent applicability constraints, as well as design parameters. The conversion to OBDD-s happens in an encoding module. Once the symbolic representation is constructed, the designer can select which design parameters to apply to the design space and thus “prune away” unsuitable choices and alternatives. This pruning is controlled by the designer, but it is done on the symbolic representation: the OBDD structures. Once the pruning is finished, the designer is left with 0, 1, or more than one designs. 0 means that no combination of choices could satisfy the design parameters, 1 means a single solution is available, and more than one means multiple alternatives are available that cannot be further pruned based on the available information. This latter case often means that other methods must be used to evaluate the alternatives, e.g. simulation. The result of the pruning is in symbolic form, but it can be easily decoded and one (or more) appropriate (hierarchical) model structure reconstructed from the result. DESERT is metaprogrammable as it can be configured to work with various DSML-s (however all of them should be capable of representing alternatives and constraints). The metaprogramming here happens in two stages: one for the model skeleton generation, and the other one for the model reconstruction. DESERT has been used to implement domain-specific design space exploration tools for embedded control systems and embedded signal processing systems. 2.4 Metaprogrammable Model Transformations GReAT The Graph Rewriting And Transformation language (GReAT) is the MIC model-to-model transformation language[1][11]. GReAT supports the development of graphical language semantic translators using graph transformations. These translators can convert models of one domain into models of another domain. GReAT transformations are actually graphically-expressed transformation algorithms consisting of partially-ordered sets of primitive transformation rules. To express these algorithms, GReAT has three sub-languages: one for model instance pattern specification, one for graph transformation, and one for flow control. The GReAT execution engine takes as input a source domain metamodel, a destination domain metamodel, a set of 2 MODELING TOOL ARCHITECTURES AND METAPROGRAMMABILITY 13 mapping rules, and an input domain model, and then executes the mapping rules on the input domain model to generate an output domain model. Each mapping rule is specified using model instance pattern graphs. These graphs are defined using associated instances of the modeling constructs defined in the source and destination metamodels. Each instance in a pattern graph can play one of the following three roles: • Bind: Match objects in the graph. • Delete: Match objects in the graph and then delete them from the graph. • New: Create new objects provided all of the objects marked Bind or Delete in the pattern graph match successfully. The execution of a primitive rule involves matching each of its constituent pattern objects having the roles Bind or Delete with objects in the input and output domain model. If the pattern matching is successful, then for each match the pattern objects marked Delete are deleted and then the objects marked New are created. The execution of a rule can also be constrained or augmented by Guards and AttributeMappings which are specified using a textual scripting language. GReAT’s third sub-language governs control flow. During execution, the flow of control can change from one potentially-executable rule to another based on the patterns matched (or not matched) in a rule. Flow control allows for conditional processing of input graphs. Furthermore, a graph transformation’s efficiency may be increased by passing bindings from one rule to another along input and output ports to lessen the search space on a graph. Ultimately, GReAT transformation models are used to generate C++ code which uses automaticallygenerated domain-specific UDM APIs for the source and destination metamodels to programmatically execute model-to-model transformations. Consequently, GReAT inherits its metaprogrammability from UDM. The source and destination metamodels accepted by GReAT are expressed using UMD-style UML class diagrams. Formally stated, let SRC and DST be respectably the source and destination metamodels of GReAT Transformation SRCTDST , and let IN and OUT be arbitrary input and output domain models expressed 2 MODELING TOOL ARCHITECTURES AND METAPROGRAMMABILITY 14 using the languages defined by SRC and DST . Then, assuming that the GMeta back-end is always used for persistent model storage, any execution of SRCTDST may be formalized as the following series of transformations: INTOUT : ((((GMetaAIN → UMLAIN ) → SRCAIN ) (1.6) → DST AOUT ) → UMLAOUT ) → GMetaAOUT To summarize, the MIC metaprogrammable tool suite allows users to: • Specify a domain-specific modeling language, include a concrete and abstract syntax and domain constraints using GME and MetaGME • Build system models using the metamodeled DSML and verify that the models do not violate the domain constraints • Construct model interpreters to parse and analyze the system models using UDM • Transform the system models of into models expressed using a different DSML using GReAT; a specialized case of this is generating code directly from models to begin implementing the system • Formally express the semantics of the DSML by mapping it onto another modeling language that expresses a formal model of comutation using GReAT • Perform design-space exploration on the modeled system using DESERT. • Serialize the models to or import models from XMI using UDM The MIC toolsuite supports full-featured, semantically rich model-based design. Furthermore, we note that it is a framework which already utilizes multiple metamodeling languages mediated by a model-to-model transformation: MetaGME and UDM UML mediated by the MetaGME2UML transformation. 3 A COMPARISON OF METAMODELING LANGUAGES 15 3 A Comparison of Metamodeling Languages This section provides a brief overview of three metamodeling languages: MetaGME, Ecore, and Microsoft’s Domain Model Designer (DMD) language. We 	abstract syntax;abstraction (software engineering);abstraction layer;algorithm;application programming interface;application-specific integrated circuit;binary decision diagram;bootstrapping (compilers);c++;class diagram;control flow;control system;data domain;data model;defense in depth (computing);delete (sql);design space exploration;design tool;digital signal processor;directory services markup language;document object model;domain model;domain-driven design;domain-specific language;domain-specific modeling;eclipse modeling framework;embedded system;executable;field-programmable gate array;gme of deutscher wetterdienst;great;generic modeling environment;global variable;graph rewriting;graphical user interface;high- and low-level;input/output;language binding;level design;mathematical model;meta-object facility;metamodeling;metaprogramming;microphone;microsoft access;model transformation language;model-driven architecture;model-driven engineering;model-driven integration;parse tree;parsing;pattern matching;persistence (computer science);race condition;sample rate conversion;scripting language;serialization;signal processing;simulation;unified modeling language;universal media disc;xml metadata interchange	Janos Sztipanovits;Sandeep Neema;Matthew J. Emerson	2007		10.1201/9781420011746.ch33	metamodeling;metadata modeling	PL	-28.32925870058278	27.332135574878833	118082
5a42199fce7a41acc63a8fea8110bc2536ec8c3d	typesetting apl dialects: a bitter legacy of the 20th century?	typesetting apl dialect;code segment;direct typesetting approach;apl snippet;apl font;alternative solution;bitter legacy;specific transliteration scheme;straightforward integration;latex package;encoding vector	Solutions to the problem of typesetting APL dialects were so far severely constrained, for they always relied on specific transliteration schemes or workspaces for automatic transliteration available for only a few interpreters. In this article I introduce an alternative solution to the problem of typesetting APL dialects which does not require transliteration of primitives. Instead, I implement a direct typesetting approach using a Type 1 (or outline) APL font, an encoding vector, and a LATEX package named listings. I finally present a series of examples in order to illustrate the typesetting of APL snippets, code segments, lists of standing files, and their straightforward integration into a document.	apl;chomsky hierarchy;code segment;latex;workspace	Pedro de Almeida	2004	ACM SIGAPL APL Quote Quad	10.1145/1088529.1088534	speech recognition;computer science;artificial intelligence;linguistics;programming language;algorithm	PL	-28.34561561015475	25.66580884362235	118396
4f1c176347838c21da32ab948a6d095be2f63286	denotational semantics of a synchronous vhdl subset	vhdl elaboration;programming language;denotational semantic;denotational semantics;clock synchronization;vhdl simulation cycle	A denotational definition for a single clock synchronous subset of VHDL is proposed. The different domains for variables and signals, the elaboration of static environments, and the formulation of a simulation algorithm for the sub-language characterize this definition, and distinguish it from more traditional denotational semantics of programming languages.	denotational semantics;vhdl	Dominique Borrione;Ashraf M. Salem	1995	Formal Methods in System Design	10.1007/BF01383873	clock synchronization;normalisation by evaluation;action semantics;computer science;theoretical computer science;programming language;denotational semantics of the actor model;operational semantics;denotational semantics;algorithm	Logic	-26.10687848411682	31.43400313800613	118423
5f8ccdfbff1e7f9c43a96370c09dc32b0c5a7b7d	program readability: procedures versus comments	developpement logiciel;readability procedures comments factorial experiment procedure format pl 1 program;maintenance;readability;programming environment;etude experimentale;programming pl 1;procedure format;testing programming profession software engineering proposals costs computer science;ingenieria logiciel;factorial experiment;software engineering;costo;medio ambiente programacion;desarrollo logicial;pl 1 program;software development;structured programming;genie logiciel;mantenimiento;comments;programmation structuree;procedures;programming;estudio experimental;environnement programmation;pl 1;programacion estructurada;cout	Abs&act-A 3 x 2 factorial experiment was performed to compare the effects of procedure format (none, internal, or external) with those of comments (absent or present) on the readability of a PL/I program. The readability of six editions of the program, each having a different combination of these factors, was inferred from the accuracy with which students could answer questions about the program after reading it. Both extremes in readability occurred in the program editions having no procedures: without comments the procedureless program was the least readable and with comments it was the most readable.	human-readable medium;pl/i	Ted Tenny	1988	IEEE Trans. Software Eng.	10.1109/32.6171	factorial experiment;procedure;programming;input/output;computer science;software development;software engineering;programming language;structured programming;management;engineering drawing;algorithm	SE	-27.26452306191996	25.550283036098683	118479
3b758c1337b12610db907a7112a2f839a5e27210	formal models of communication services: a case study	protocols;communication system;formal specification;formal model;data communication;formal method;input output;formal verification;correctness communication services data communications protocols standardization customization atomic multicast input output automata formal model service specification;telecommunication services formal specification formal verification protocols;telecommunication services;computer aided software engineering protocols standardization data communication natural languages telecommunication computing humans power system modeling power system management software systems;communication service	Formal methods can play an important role in exploring new communication systems services. The telecommunications and data communications communities have long accepted the need for formally describing protocols, but only recently have they considered formally describing a service by abstracting specifications from a particular protocol that provides that service. Specifying a service at an abstract level meets two important needs: standardization and customization. The author presents a simplified atomic multicast as an example service and input/output automata for the formal model. He shows how to represent the service specification, a protocol, and implementations of that protocol. He also sketches how to prove the correctness of the protocol and implementation, that is, how to show that the specified service is actually provided.<<ETX>>	automata theory;communications protocol;correctness (computer science);formal methods;input/output;mathematical model;multicast	Alan Fekete	1993	Computer	10.1109/2.223535	input/output;communications protocol;formal methods;formal verification;computer science;telecommunications service;theoretical computer science;software engineering;formal specification;database;distributed computing;programming language;communications system	Networks	-33.60948704129225	31.897657890775488	118504
e99f920cf7c7368ec781807c6870fb75a872b8a0	special issue on programming with dependent types editorial		There has been sustained interest in functional programming languages with dependent types in recent years. The foundations of dependently typed programming can be traced back to Martin–Löf’s work in the 1970s. In the past decades, this vision has given rise to the development of proof assistants and functional programming languages based on dependent types. The increased popularity of systems such as Agda, Coq, Idris, and many others, reflects the growing momentum in this research area. After sending out our first call for papers in October 2015, we are happy to accept six articles in this special issue covering a wide spectrum of topics. Despite their theoretical appeal, there are very few examples of programming languages based on dependent types which have been used to construct applications with a graphical user interface. Abel et al. show how to write such GUIs in Agda in their article Interactive Programming in Agda – Objects and Graphical User Interfaces. Programming languages with dependent types provide a rich design space for describing data types that capture invariants precisely. One drawback, however, is that there may be many subtle variations of the same data type, resulting in duplicated functions for each such variation. Ornaments provide a language for describing the relationship between data types. Ko and Gibbons’s article Programming with Ornaments shows illustrative examples of ornaments in action; Dagand’s article The Essence of Ornaments gives a novel description of ornaments in terms of many-sorted signatures. A higher order unification algorithm lies at the heart of many implementations of dependently typed programming languages. Ziliani and Sozeau present a new such algorithm for the Calculus of Inductive Constructions in their article A Comprehensible Guide to a New Unifier for CIC Including Universe Polymorphism and Overloading. This algorithm both provides useful heuristics and deals with several of the features of the calculus specific to the Coq system. Stump addresses another central concept of dependently typed programming languages in his article The Calculus of Dependent Lambda Eliminations: the notion of inductively defined data type. One of the motivations for the original Calculus of Constructions was the use of impredicative quantification for compactly and elegantly encoding data types. Such encodings, however, were soon discovered to have several drawbacks. To remedy these the calculus was extended with a system of primitive inductive data types in the Calculus of Inductive Constructions. With his Calculus of Dependent Lambda Eliminations, Stump proposes another alternative: a new strengthened system based on impredicative encodings, but with	admissible rule;agda;algorithm;calculus of constructions;character encoding;coq (software);decision stump;dependent type;functional programming;graphical user interface;heuristic (computer science);idris;impredicativity;interactive programming;operator overloading;programming language;proof assistant;type signature;unification (computer science)	Wouter Swierstra;Peter Dybjer	2017	J. Funct. Program.	10.1017/S0956796817000065		PL	-24.164860177388043	21.740045114227122	118557
47ca618399d9248b5b6e7c2f6861d06b34922d75	performance vs. productivity in the context of arcgis server 10	likelihood ratio;programming language;web performance;logistic regression;web mapping;gis;visual basic;mineral potential;software package;artificial neural network;open source	While many programming languages excel in their ability to execute commands quickly, others embody a greater focus on programmer productivity and clear syntax. In ESRI's GIS software package ArcGIS, Python is now the choice language for many GIS Analysts as an alternative to the more complex ArcObjects library. ArcObjects is written in C#, Visual Basic, Java, or C++, all more difficult languages to learn than Python, but also much faster. In modern web mapping, ArcGIS Python scripts are now making their way onto the server, sometimes at the expense of application performance and stability.  I have explored the idea of code performance vs. programmer productivity in the context of ArcGIS Server by writing several web-based geoprocessing services in both Python and C# ArcObjects. The goal was to identify the classes of tools which are best developed using one technology or the other, either based on performance or ease of development. From the outset, I made the assumption that under equal circumstances, it is easier to develop a service in Python, but that C# will always execute faster.  The different geoprocessing services were divided into three categories: raster-based, vector-based, and server utilities. The services had different inputs and outputs ranging from text to polygons to zip files. Multi-Mechanize web performance and load testing framework was used to automate requests and make testing repeatable. Multi-Mechanize is an open source testing framework written in python which assisted in replaying requests, logging responses, and compiling statistics. Using this framework, I was able to make an assessment of the exact types of geoprocessing services which should be built using python, and which should be avoided.	arcgis server;arcobjects;c++;compiler;geographic information system;geoprocessing;java;load testing;open-source software;programmer;programming language;programming productivity;python;server (computing);visual basic;web application;web mapping;web performance	Brendan Collins	2011		10.1145/1999320.1999377	python;computer science;database;programming language;world wide web;arcgis server	OS	-32.52491816922545	27.18795149078614	118593
1baf62357fb0b8c60f735c27f89444d1492e62c5	idris ---: systems programming meets full dependent types	application development;programming language;data description;burden of proof;program verification;operating system;decision procedure;levels of abstraction;dependent types;type system;program correctness	Dependent types have emerged in recent years as a promising approach to ensuring program correctness. However, existing dependently typed languages such as Agda and Coq work at a very high level of abstraction, making it difficult to map verified programs to suitably efficient executable code. This is particularly problematic for programs which work with bit level data, e.g. network packet processing, binary file formats or operating system services. Such programs, being fundamental to the operation of computers in general, may stand to benefit significantly from program verification techniques. This paper describes the use of a dependently typed programming language, Idris, for specifying and verifying properties of low-level systems programs, taking network packet processing as an extended example. We give an overview of the distinctive features of Idris which allow it to interact with external systems code, with precise types. Furthermore, we show how to integrate tactic scripts and plugin decision procedures to reduce the burden of proof on application developers. The ideas we present are readily adaptable to languages with related type systems.	agda;binary file;bit-level parallelism;computer;coq (software);correctness (computer science);daemon (computing);dependent type;executable;formal specification;formal verification;high- and low-level;high-level programming language;idris;network packet;operating system;plug-in (computing);system programming;type system;verification and validation	Edwin Brady	2011		10.1145/1929529.1929536	dependent type;type system;computer science;theoretical computer science;programming language;rapid application development;algorithm	PL	-21.643673573262742	28.28570777393852	118655
445ac23ec8bbc751af017f9e0baf5da51f552b80	attacking a complex distributed algorithm from different sides: an experience with complementary validation tools	distributed system;preuve programme;program proof;systeme reparti;protocole transmission;red petri;simulation;simulacion;protocolo transmision;sistema repartido;prueba programa;petri net;distributed algorithm;reseau petri;transmission protocol	We consider a complex distributed mutual exclusion algorithm which was a part of a distributed system, Galaxie, studied by the CNET (French PTT) at Lannion, France. This protocol deals with failures and an unreliable environment.#R##N##R##N#In order to validate the design of this protocol, we have experimented with three methods of validation. These methods brought complementary results: simulation (prototyping), Petri net analysis and logical proof.#R##N##R##N#After a short and informal description of the algorithm, we discuss the scope of the methods which pointed out errors in the original design and ascertained the validity range of the final version of the algorithm. For each method, we present the problems encountered during the modeling, the properties which can be verified and some results. The methods are compared with respect to their ability to find errors in the design and the interest of the results provided: we conclude to their complementarity.	distributed algorithm	Roland Groz;Claude Jard;Claire Lassudrie	1984	Computer Networks	10.1016/0169-7552(85)90068-6	distributed algorithm;simulation;telecommunications;computer science;artificial intelligence;petri net;algorithm	HCI	-32.353146386431945	31.17514170147298	118699
e64c56217433e7f242d318089e881930957f89e7	arbus, a tool for developing application grammars	elaborate interface;arbus system;frequent change;natural language processing;grammar rule;grammar editor;implementation detail;application grammar;parsing program;grammar interactively;natural language system;natural language	The development of a natural language system usually requires frequent changes to the grammar used. It is then ~ery useful to be able to define and modify the gra~ar rules easily, without having to tamper with the parsing program. The ARBUS system was designed to help develop grammars for natural language processing. With this system one can build, display, test, modify and file a grammar interactively in a very convenient way. This was achieved by packaging a parser and a graummr editor with an elaborate interface which isolates the user from implementation details and guides him as much as possible.	interactivity;natural language processing;parsing	Daniel Memmi;Joseph-Jean Mariani	1982			grammar systems theory;natural language processing;synchronous context-free grammar;l-attributed grammar;link grammar;parsing expression grammar;operator-precedence grammar;computer science;affix grammar;parsing;extended affix grammar;linguistics;natural language;programming language;attribute grammar;ambiguous grammar;adaptive grammar	NLP	-28.701800273796277	25.493134215685608	118843
0526b73561f2d4307467791095adfaa28c2dfffc	fastscript3d: a javascript companion to java3d	applets;object oriented programming;java3d;fastscript3d a javascript companion to java3d;javascript	FastScript3D is a web-friendly companion to Java3D that makes it easy to get started using Java3D via JavaScript and HTML. The FastScript3D web site shows how you can create Java3D web content without having to be an experienced Java3D programmer.	html;java 3d;javascript;programmer;web content	Patti Koenig Koehler	2003		10.1145/965333.965385	ajax;web application;rich internet application;content security policy;computer science;unobtrusive javascript;operating system;dynamic web page;dynamic html;javascript;programming language;object-oriented programming;world wide web	HCI	-31.0520507761583	25.878703981041294	118925
0a2457ad4253e1812fac76c11bd18c9bfaaf0c10	c-corn, the constructive coq repository at nijmegen	document structure;teoria demonstracion;proof assistant;mathematics;theorie preuve;automatic proving;estructura documental;proof theory;ingenierie connaissances;availability;disponibilidad;structure document;demostracion automatica;program verification;theorem proving;demonstration automatique;demonstration theoreme;theorem prover;verificacion programa;matematicas;demostracion teorema;verification programme;disponibilite;article in monograph or in proceedings;mathematiques;knowledge engineering	We present C-CoRN, the Constructive Coq Repository at Nijmegen. It consists of a mathematical library of constructive algebra and analysis formalized in the theorem prover Coq. We explain the structure and the contents of the library and we discuss the motivation and some (possible) applications of such a library. The development of C-CoRN is part of a larger goal to design a computer system where ‘a mathematician can do mathematics’, which covers the activities of defining, computing and proving. An important proviso for such a system to be useful and attractive is the availability of a large structured library of mathematical results that people can consult and build on. C-CoRN wants to provide such a library, but it can also be seen as a case study in developing such a library of formalized mathematics and deriving its requirements. As the actual development of a library is very much a technical activity, the work on C-CoRN is tightly bound to the proof assistant Coq.	automated theorem proving;computer;coq (software);library (computing);proof assistant;requirement	Luís Cruz-Filipe;Herman Geuvers;Freek Wiedijk	2004		10.1007/978-3-540-27818-4_7	computer science;artificial intelligence;knowledge engineering;database;mathematics;automated theorem proving;programming language;algorithm	PL	-19.22274066938597	21.38141730839352	119174
c68ec937eb2c0bd8bff85479cd8ad888d1e879b9	a discussion and implementation of brown's rex simplification algorithm	symbolic computation;normal form	We discuss a simplification algorithm written by W. S. Brown [Bro69], which gives a normal form algorithm for a class of rational expressions merged with (perhaps complex) exponential expressions. Here we describe the algorithm as revised for implementation in the Maple Symbolic Computation System at the University of Waterloo [Cha83]. We provide some tests and comparisons with alternative systems.	a-normal form;algorithm;maple;rex black;symbolic computation;time complexity	Patrick C. McGeer	1984	ACM SIGSAM Bulletin	10.1145/1089348.1089350	symbolic computation;computer science;theoretical computer science;mathematics;normal-form game;algorithm;algebra	PL	-19.265350722019264	18.82410831906803	119237
b1da8a2931ea03251da34a826bf1acd4f2a3e8d7	improving lazy non-deterministic computations by demand analysis	functional logic programming implementation program analysis;004	Functional logic languages combine lazy (demand-driven) evaluation strategies from functional programming with non-deterministic computations from logic programming. The lazy evaluation of non-deterministic subexpressions results in a demand-driven exploration of the search space: if the value of some subexpression is not required, the complete search space connected to it is not explored. On the other hand, this improvement could cause efficiency problems if unevaluated subexpressions are duplicated and later evaluated in different parts of a program. In order to improve the execution behavior in such situations, we propose a program analysis that guides a program transformation to avoid such inefficiencies. We demonstrate the positive effects of this program transformation with KiCS2, a recent highly efficient implementation of the functional logic programming language Curry. 1998 ACM Subject Classification D.1.6 Logic Programming	computation;curry;functional logic programming;functional programming;lazy evaluation;program analysis;program transformation;programming language	Michael Hanus	2012		10.4230/LIPIcs.ICLP.2012.130	reactive programming;computer science;theoretical computer science;lazy evaluation;programming paradigm;inductive programming;programming language;algorithm	PL	-19.700659745716646	23.12777400050934	119360
846e78b92ed7d7cd2bfa6561090e5b1ade85eeaa	the design of cost efficient health monitoring system based on internet of things and big data		Nowadays, penetration of the Internet of Things (IoT) technology was spread in various domains. One of the promising business domains of IoT adoption is healthcare. With the vast amount and variance of smart wearable device based on IoT principles on the market, a consumer could easily find equipment to track their health status (i.e., blood pressure, heart rate, body temperature, etc.). But, those devices have two drawbacks. First, the significant cost of investment was required for implementing those devices on a massive scale (i.e., patient monitoring in a hospital). The second drawback is the lack of platform which can efficiently manage multiple devices concerning storing and organizing the data generated by those wearable devices. In this paper, we introduced a cost-efficient design of healthcare monitoring system based on IoT technology. This design proposed holistic approach from providing less expensive hardware for sensing purposes and a sub-platform for managing acquired data from sensing activity based on big data technology.	big data;cost efficiency;holism;internet of things;organizing (structure);prototype;real-time clock;server (computing);single-instance storage;software as a service;software license;wearable technology	Muhammad Rifqi Mararif;Agung Priyanto;Chanief Budi Setiawan;Puji Winar Cahyo	2018	2018 International Conference on Information and Communication Technology Convergence (ICTC)	10.1109/ICTC.2018.8539374	drawback;computer engineering;big data;wearable computer;wearable technology;cost efficiency;remote patient monitoring;internet of things;computer science	EDA	-33.219310511397154	19.255290811668157	119405
c41fc48d8aae7334378dd03b12229de15b1a3812	b74-10 computer semantics, studies of algorithms, processors and languages	language development;computational semantics	This is a textbook in Computer Science based on the Vienna Definition Language developed by P. Lucas and his colleagues at the IBM laboratory in Vienna. Their original purpose was to produce a formal definition of PL/I, but the language and the methodology that they have developed has much wider application.	algorithm;computer science;pl/i;vienna development method	Saul Rosen	1974	IEEE Transactions on Computers	10.1109/T-C.1974.223899	natural language processing;vienna development method;computer science;programming language;algorithm;computational semantics	Visualization	-24.661566467228813	21.722567758255725	119543
85a687db9c09767c172700403372df59651c4505	simulation language for multiple mobile robots	graph structure;path searches;petrinet;interpreter		simulation language	Yoshinobu Adachi;Masayoshi Kakikura	1997	JRM	10.20965/jrm.1997.p0373	computer science;theoretical computer science;distributed computing;programming language	Robotics	-24.208523141963855	22.699781307354975	119591
abf9472ae3813c013f2667b8cc956dae4f818348	object-oriented programming	programming language;data acquisition system;system design;object oriented programming;elementary particles;high energy physics;particle acceleration	This paper is an introduction to object oriented programming. The object oriented approach is very powerful and not inherently difficult, but most programmers find a relatively high threshold in learning it. Thus, this paper will attempt to convey the concepts with examples rather than explain the formal theory.	elsevier biobase;encapsulation (networking);function overloading;graphical user interface;human-readable medium;java class library;library (computing);nextstep;programmer;software crisis;software engineering	Richard P. Ten Dyke;John C. Kunz	1989	IBM Systems Journal	10.1147/sj.283.0465		PL	-27.657559939192772	24.81132439557271	119702
3329f27327d093a849687256420816c15b0892ce	verifying robustness of event-driven asynchronous programs against concurrency		We define a correctness criterion, called robustness against concurrency, for a class of event-driven asynchronous programs that are at the basis of modern UI frameworks in Android, iOS, and Javascript. A program is robust when all possible behaviors admitted by the program under arbitrary procedure and event interleavings are admitted even if asynchronous procedures (respectively, events) are assumed to execute serially, one after the other, accessing shared memory in isolation. We characterize robustness as a conjunction of two correctness criteria: event-serializability (i.e., events can be seen as atomic) and event-determinism (executions within each event are insensitive to the interleavings between concurrent tasks dynamically spawned by the event). Then, we provide efficient algorithms for checking these two criteria based on polynomial reductions to reachability problems in sequential programs. This result is surprising because it allows to avoid explicit handling of all concurrent executions in the analysis, which leads to an important gain in complexity. We demonstrate via case studies on Android apps that the typical mistakes programmers make are captured as robustness violations, and that violations can be detected efficiently using our approach.	algorithm;android;complexity;concurrency (computer science);correctness (computer science);event-driven finite-state machine;event-driven programming;javascript;metro (design language);polynomial;polynomial-time reduction;programmer;reachability problem;serializability;shared memory;user interface;ios	Ahmed Bouajjani;Michael Emmi;Constantin Enea;Burcu Kulahcioglu Ozkan;Serdar Tasiran	2017		10.1007/978-3-662-54434-1_7	real-time computing;isolation;distributed computing;multiversion concurrency control;non-lock concurrency control;programming language	PL	-20.517066051039528	31.23764062301786	119716
e20316409f9c718666ba6e039d0ad36beccda67a	how to formally specify the java bytecode semantivs using the b method	java bytecode;b method		b-method;java bytecode	Ludovic Casset;Jean-Louis Lanet	1999			jsr 94;java concurrency;java modeling language;programming language;java;generics in java;scala;java applet;java annotation	EDA	-25.682210123752927	29.35465627093601	119737
36ae3ce45f06848d8ce1e81c7331932585e6db20	a type system for components	qa75 electronic computers computer science	In modern distributed systems, dynamic reconfiguration, i.e., changing at runtime the communication pattern of a program, is challenging. Generally, it is difficult to guarantee that such modifications will not disrupt ongoing computations. In a previous paper, a solution to this problem was proposed by extending the object-oriented language ABS with a component model allowing the programmer to: i) perform updates on objects by means of communication ports and their rebinding; and ii) precisely specify when such updates can safely occur in an object by means of critical sections. However, improper rebind operations could still occur and lead to runtime errors. The present paper introduces a type system for this component model that extends the ABS type system with the notion of ports and a precise analysis that statically enforces that no object will attempt illegal rebinding.	component-based software engineering;computation;critical section;distributed computing;programmer;run time (program lifecycle phase);type system	Ornela Dardha;Elena Giachino;Michael Lienhardt	2013		10.1007/978-3-642-40561-7_12	real-time computing;simulation;computer science;engineering;operating system;software engineering;programming language;algorithm	PL	-26.238508089708784	30.556111738102786	119819
a72eead71d4da987e20c5e6e51d9416efdcdd09b	constraint problem specification as compression		Constraint Programming is a powerful and expressive framework for modelling and solving combinatorial problems. It is nevertheless not always easy to use, which has led to the development of high-level specification languages. We show that Constraint Logic Programming can be used as a meta-language to describe itself more compactly at a higher level of abstraction. This can produce problem descriptions of comparable size to those in existing specification languages, via techniques similar to those used in data compression. An advantage over existing specification languages is that, for a problem whose specification requires the solution of an auxiliary problem, a single specification can unify the two problems. Moreover, using a symbolic representation of domain values leads to a natural way of modelling channelling constraints.	cp/m;constraint logic programming;constraint programming;data compression;declarative programming;high- and low-level;horn clause;negation as failure;programming language;regular expression;specification language;turing completeness	Steven David Prestwich;Armagan Tarim;Roberto Rossi	2016			mathematical optimization;compression (physics);computer science	SE	-21.776509224848795	18.85589607674157	119836
0a6ae3e51a82636291cb21f0065c3ed4a1ae96f7	design and implementation of an environment for component-based parallel programming	category theory;design and implementation;high performance computer;component model;parallel programs;petri net	Motivated by the inadequacy of current parallel programming artifacts, the # component model was proposed to meet the new complexity of high performance computing (HPC). It has solid formal foundations, layed on category theory and Petri nets. This paper presents some important design and implementation issues on the implementation of programming frameworks based on the # component model.	category theory;component-based software engineering;interoperability;message passing;parallel computing;petri net;problem solving;semantics (computer science);software development;supercomputer	Francisco Heron de Carvalho Junior;Rafael Dueire Lins;Ricardo C. Corrêa;Gisele Azevedo Araújo;Chanderlie Freire de Santiago	2006		10.1007/978-3-540-71351-7_15	parallel computing;computer science;theoretical computer science;component object model;distributed computing;petri net;category theory	HPC	-33.32288409523812	28.888472596173884	119880
2a4500a31b47427d0299ace515e799cabff91477	sre-a syntax recognizing editor	incremental parsing;syntax directed editor;program development	Abstract#R##N##R##N#This paper presents a syntax-directed editor dubbed SRE, for syntax recognizing editor. Unlike other syntax-directed editors, SRE enables the user to edit programs with nearly the same natural and unrestrictive ease as a conventional text editor. In addition, it helps identify syntax errors and many typing errors before leaving the editor and attempting to compile a program. Programs are also formatted during entry, thereby providing immediate visual feedback of the recognized structure. The editor replaces the scanner/parser pass of a conventional compiler and thus reduces compilation time substantially.		Frank J. Dudinsky;Richard C. Holt;Safwat G. Zaky	1985	Softw., Pract. Exper.	10.1002/spe.4380150507	natural language processing;speech recognition;computer science;programming language	NLP	-25.449120461848718	25.05738024047559	120204
6a56819b932a8eb4185a543d54e117c1b4788fde	javascript module system: exploring the design space	module system;source to source transformation;javascript	While JavaScript is one of the most widely used programming languages not only for web applications but also for large projects, it does not provide a language-level module system. JavaScript developers have used the module pattern to avoid name conflicts by themselves, but the prevalent uses of multiple libraries and even multiple versions of a single library in one application complicate maintenance of namespace. The next release of the JavaScript language specification will support a module system, but the module proposal in prose does not clearly describe its semantics. Several tools attempt to support the new features in the next release of JavaScript by translating them into the current JavaScript, but their module semantics do not faithfully implement the proposal.  In this paper, we identify some of the design issues in the JavaScript module system. We describe ambiguous or undefined semantics of the module system with concrete examples, show how the existing tools support them in a crude way, and discuss reasonable choices for the design issues. We specify the formal semantics of the module system, which provides unambiguous description of the design choices, and we provide its implementation as a source-to-source transformation from JavaScript with modules to the plain JavaScript that the current JavaScript engines can evaluate.	apache harmony;compiler;ecmascript;embedded system;foobar;hoc (programming language);javascript engine;library (computing);modular programming;module pattern;open-source software;programming language specification;python;return statement;ruby;scripting language;semantics (computer science);source transformation;typescript;undefined behavior;web application;web page	Junhee Cho;Sukyoung Ryu	2014		10.1145/2577080.2577088	javascript syntax;computer science;theoretical computer science;unobtrusive javascript;database;programming language	PL	-27.178477370991587	28.12916948543014	120209
5ad466046c1956f9ecfcbcfaa3e1f2230628b7d7	copying and swapping: influences on the design of reusable software components	passage;developpement logiciel;lenguaje programacion;swapping style;data movement primitive;computer languages;pasaje;mathematics;formal specification;programmation;programming language;information science;reutilizacion;parametre;program design;formal specifications;conception programme;ingenieria logiciel;abstract data type;object oriented programming;software engineering;reuse;programacion;generic reusable software components;parametro;parameter;object oriented;data structures;desarrollo logicial;indexation;software reusability;type abstrait;software development;software component;software design software reusability embedded software computer science computer languages formal specifications object oriented programming mathematics information science;genie logiciel;langage programmation;oriente objet;tipo abstracto;generic module designs;computer science;software design;swapping style copying data movement primitive generic reusable software components generic module designs;orientado objeto;programming;software reusability data structures;concepcion programa;copying;reutilisation;embedded software	The only built-in mechanism for data movement in most modern programming languages is the assignment statement for copying the value of one variable to another. In the context of reusable software components, this reliance on copying leads to two classes of difficulties: components whose implementations are inherently inefficient, and client programs that are hard to reason about. A powerful substitute for copying as the primary data movement primitive is a swapping mechanism that exchanges the values of two variables. Reusable components and client programs designed with a “swapping style” of programming have many advantages over designs based on the traditional “copying , style,” including improved execution efficiency, higher reliability, and enhanced reusability. Index Tems-Abstract data type, assignment statement, formal specification, object-oriented programming, parameter passing, pointer, reusable software component.	abstract data type;assignment (computer science);component-based software engineering;formal specification;hot swapping;paging;pointer (computer programming);programming language	Douglas E. Harms;Bruce W. Weide	1991	IEEE Trans. Software Eng.	10.1109/32.90445	data structure;information science;computer science;engineering;theoretical computer science;operating system;software engineering;formal specification;programming language;object-oriented programming;parameter	SE	-24.309289048317876	28.612821818643827	120269
eb00f5537dcc288fd65bc6cc5b3b895f055bf0af	action semantics of unified modeling language	thesis	The Unified Modeling Language or UML, as a visual and general purpose modeling language, has been around for more than a decade, gaining increasingly wide application and becoming the de-facto industrial standard for modeling software systems. However, the dynamic semantics of UML behaviours are only described in natural languages. Specification in natural languages inevitably involves vagueness, lacks reasonability and discourages mechanical language implementation. Such semi-formality of UML causes wide concern for researchers, including us. The formal semantics of UML demands more readability and extensibility due to its fast evolution and a wider range of users. Therefore we adopt Action Semantics (AS), mainly created by Peter Mosses, to formalize the dynamic semantics of UML, because AS can satisfy these needs advantageously compared to other frameworks. Instead of defining UML directly, we design an action language, called ALx, and use it as the intermediary between a typical executable UML and its action semantics. ALx is highly heterogeneous, combining the features of Object Oriented Programming Languages, Object Query Languages, Model Description Languages and more complex behaviours like state machines. Adopting AS to formalize such a heterogeneous language is in turn of significance in exploring the adequacy and applicability of AS. In order to give assurance of the validity of the action semantics of ALx, a prototype ALx-to-Java translator is implemented, underpinned by our formal semantic description of the action language and using the Model Driven Approach (MDA). We argue that MDA is a feasible way of implementing this source-to-source language translator because the cornerstone of MDA, UML, is adequate to specify the static aspect of programming languages, and MDA provides executable transformation languages to model mapping rules between languages. We also construct a translator using a commonly-used conventional approach, in	action language;action semantics;executable uml;extensibility;general-purpose modeling;java;natural language;programming language;prototype;semantics (computer science);semiconductor industry;software system;transformation language;unified modeling language;vagueness	Mikai Yang	2009			natural language processing;formal semantics;action semantics;computer science;linguistics;modeling language;programming language;operational semantics;computational semantics	PL	-26.211854923963635	21.574948007070834	120330
6adebdac2cf57e9b7db3002f857fbcf9fee64cd7	the common list object-oriented programming language standard			programming language	David A. Moon	1989				PL	-29.77734301491183	26.774665709294826	120339
fb1838077df3bd2bb17ad5a48574e74a1ed2d52f	programming with behaviors in an ml framework - the syntax and semantics of lcs	programming language;operational semantics;standard ml;higher order;parallel programming language;transition systems	LCS is an experimental high level asynchronous parallel programming language primarily aimed at exploring design, implementation and use of programming languages based upon the behavioral paradigms introduced by CSP and CCS. The language extends Standard ML with primitives for concurrency and communication based upon a higher order extension of the CCS formalism. Typechecking enforces consistency of communications. An abstract operational semantics of the language is given in terms of a transition system.	asynchronous i/o;automatic variable;calculus of communicating systems;concurrency (computer science);formal methods;general-purpose programming language;guard (computer science);high-level programming language;operational semantics;parallel computing;parallel programming model;semantics (computer science);standard ml;transition system;type system	Bernard Berthomieu;Thierry Le Sergent	1994		10.1007/3-540-57880-3_6	natural language processing;first-generation programming language;declarative programming;very high-level programming language;higher-order logic;language primitive;programming domain;reactive programming;computer science;extensible programming;functional logic programming;programming paradigm;procedural programming;symbolic programming;inductive programming;fifth-generation programming language;programming language theory;programming language;operational semantics;programming language specification;high-level programming language;denotational semantics;algorithm;semantics;parallel programming model	PL	-23.02770993480817	22.900216518781594	120350
ff8ab2ef819a4915cdeb68e4ae50c6343a1b1e2d	history of interactive theorem proving		syntax, 6, 54 ACL2, 18, 27, 29, 32, 34, 35, 40, 44, 48, 55, 60	acl2;proof assistant	John Harrison;Josef Urban;Freek Wiedijk	2014		10.1016/B978-0-444-51624-4.50004-6	discrete mathematics;fundamental theorem;mathematics;automated theorem proving;algorithm	Logic	-19.75783336114453	19.218298386935945	120481
599f3b766913703916751168e6a6fbba139431b3	semantics and analysis of instruction list programs	tool support;operational semantics;typed assembly language;formal semantics;formal method;programmable logic controllers;abstract simulation;abstract interpretation;programmable logic controller;instruction list	Instruction List (IL) is a simple typed assembly language commonly used in embedded control. There is little tool support for IL and, although defined in the IEC 61131-3 standard, there is no formal semantics. In this work we develop a formal operational semantics. Moreover, we present an abstract semantics, which allows approximative program simulation for a (possibly infinte) set of inputs in one simulation run. We also extended this framework to an abstract interpretation based analysis, which is implemented in our tool Homer. All these analyses can be carried out without knowledge of formal methods, which is typically not present in the IL community.	abstract interpretation;approximation algorithm;division by zero;embedded system;formal methods;iec 61131;iec 61131-3;instruction list;operational semantics;piaget's theory of cognitive development;polyhedron;programmer;semantics (computer science);simulation;static program analysis;typed assembly language	Ralf Huuck	2005	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2004.09.026	formal system;formal methods;formal semantics;action semantics;formal verification;syntax;computer science;theoretical computer science;programmable logic controller;formal semantics;programming language;well-founded semantics;operational semantics;denotational semantics;algorithm;computational semantics	PL	-22.831504073403483	23.090886046495413	120689
1ab71cdb84ead6864f9aff65d94034070ad0f306	architecture for logic programing with arrangements of finite-state machines	robot sensing systems;computer architecture;robotic middleware software models sequential schedule logic programming reactive systems logic labeled finite state machines;engines;cognition engines schedules computer architecture middleware robot sensing systems;cognition;pull approach logic programing prolog reactive systems logic labeled finite state machines llfsms deterministic scheduling middleware pull approach deterministic semantics time domain value domain interprocess communication formal verification push approach ros;schedules;middleware;scheduling finite state machines logic programming middleware program verification prolog	We incorporate logic programs (in particular Prolog) into reactive systems. We do this using Logic-labeled finitestate machines (LLFSMs), whose non-event-driven nature results in a deterministic schedule. We also advocate the use of a middleware under the Pull-approach, as opposed to the, currently very common, Push-approach, achieving deterministic semantics and the ability to ensure correctness in both the time and value domains. The deterministic schedule has other advantages, such as bounded resource use for inter-process communication as well as a smaller state space for formal verification. We demonstrate this architecture through a simple case study, contrasting the current prevalent Push approach in ROS [1] with the Pull approach recommended for LLFSMs.	correctness (computer science);event-driven programming;finite-state machine;formal verification;inter-process communication;middleware;prolog;robot operating system;state space	Vladimir Estivill-Castro;René Hexel;Alberto Ramirez Regalado	2016	2016 1st CPSWeek Workshop on Declarative Cyber-Physical Systems (DCPS)	10.1109/DCPS.2016.7588297	real-time computing;computer science;theoretical computer science;distributed computing	Logic	-30.47882811397653	30.578648050463094	120697
57137d12f8de0b315a81da500185dbe4298c249b	annotating user-defined abstractions for optimization	optimising compilers;general and miscellaneous mathematics computing and information science;reordering;information systems;optimization technique;relevant semantic information;memorization;performance;data layout transformation;data layout transformation user defined abstraction conventional compiler relevant semantic information compiler optimization memorization reordering;user defined abstraction;standardized terminology;semantic information;object oriented;optimizing compilers design optimization scientific computing laboratories profitability computer languages computer science costs us department of energy containers;compiler optimization;optimising compilers computational linguistics data flow analysis;conventional compiler;data flow analysis;functional data;profitability;computational linguistics;data layout;information system;optimizing compilers	Although conventional compilers implement a wide range of optimization techniques, they frequently miss opportunities to optimize the use of abstractions, largely because they are not designed to recognize and use the relevant semantic information about such abstractions. In this position paper, we propose a set of annotations to help communicate high-level semantic information about abstractions to the compiler, thereby enabling the large body of traditional compiler optimizations to be applied to the use of those abstractions. Our annotations explicitly describe properties of abstractions that are needed to guarantee the applicability and profitability of a broad variety of such optimizations, including memoization, reordering, data layout transformations, and inlining and specialization	high- and low-level;inline expansion;mathematical optimization;memoization;optimizing compiler;partial template specialization;turing completeness;verification and validation	Daniel J. Quinlan;Markus Schordan;Richard W. Vuduc;Qing Yi	2006	Proceedings 20th IEEE International Parallel & Distributed Processing Symposium	10.1109/IPDPS.2006.1639722	parallel computing;computer science;theoretical computer science;computational linguistics;operating system;database;distributed computing;programming language;information system;algorithm	PL	-26.30554654449965	22.76288638106293	120826
c04dbffb221167d71723fb729e112a4170d27a2a	executable c++ program generation from the structured object-oriented design diagrams	decision diagrams;program skeleton executable c program generation structured object oriented design diagrams event trace diagram decision table operational definition high level language data structure part cliches lola expression frame cliche;object oriented design;object oriented programming;application generators;c program generation;logic remuneration industrial engineering skeleton computer aided software engineering automatic generation control switches object oriented modeling design engineering high level languages;satisfiability;program generation;c language;object oriented;design diagram;event trace diagram;high level language;data structure;decision diagrams c language application generators object oriented programming;decision table	This paper describes a system for generating an executable C++ program from object-oriented design diagrams. Especially an event-trace diagram and a decision table are used to describe an operational definition. The decision table offers a high-level language LOLA for the brief expression of controls that are dependent on the data structure. This language allows the methods to be expressed easily, briefly, and visually. A C++ program was generated from these diagrams in such a way that the part cliches are edited to satisfy LOLA expression and embedded into the frame cliche representing the skeleton of the whole program.	c++;data structure;decision table;embedded system;executable;high- and low-level;high-level programming language;operational definition;sequence diagram	Minoru Harada;Takahiro Mizuno	1999		10.1109/APSEC.1999.809659	data structure;computer science;theoretical computer science;programming language;object-oriented programming;algorithm	PL	-31.3462783769872	23.927239330449016	120944
60a58e9afc4a7b1b8bc91c0216947f17c84f5894	computational music representation based on the generative theory of tonal music and the deductive object-oriented database	deductive object oriented database;generative theory of tonal music;computer music		computation	Keiji Hirata;Tatsuya Aoyagi	2003	Computer Music Journal	10.1162/014892603322482547	natural language processing;speech recognition;computer science;pop music automation;computer music;programming language	ML	-25.804957494920625	21.24108056409629	121095
9fccfba0251e4433306f38bc2c120365fa59c952	spatial computing with labels	robot sensing systems;atron robot reconfigurable robot robot programming programming language designer spatial programming language modular robots;programming language design;computer languages;programming language;shape robot programming computer languages concrete hardware connectors programming profession conferences councils production;robotics;software engineering;atron robot;shape;robots;reconfigurable robot;software engineering programming languages robot programming;spatial programming language;modular robots;programming language designer;spatial computing;programming;programming languages;robot programming;spatial computing programming languages robotics modular robots;robot kinematics;wheels	A reconfigurable robot is a robot that can change shape. Programming reconfigurable robots is complicated by the need to adapt the behavior of each of the individual module to the overall physical shape of the robot. In this position paper, we investigate a simple approach to allow the programmer to abstract over the concrete shape of a robot using the notion of a label as a simple means of addressing various parts of the structure of a robot. Labels provide the programming language designer with a means of stratifying two main components of a spatial programming language for modular robots, namely specifying the physical structure of a robot and specifying its behavior. Based on previous experience with the ATRON robot, we find that labels are a useful concept for programming modular robots.	programmer;programming language;self-reconfiguring modular robot	Ulrik Pagh Schultz;Mirko Bordignon;David Johan Christensen;Kasper Støy	2008	2008 Second IEEE International Conference on Self-Adaptive and Self-Organizing Systems Workshops	10.1109/SASOW.2008.42	robot;robot learning;programming;simulation;shape;computer science;artificial intelligence;theoretical computer science;social robot;arm solution;self-reconfiguring modular robot;robotics;programming language;personal robot;robot kinematics	Robotics	-30.413942673652432	30.256433179505482	121119
16ea4940056842b05fafd13458da47c6c1e3585b	an algol-based implementation of snobol4 patterns	string processing;pattern matching;algorithms in pascal;patterns;snobol4;pattern implementation	"""J. Nevil Brownlee University of Auckland which SNOBOL4 patterns are compiled into Algol functions, which can be combined into larger (more complicated) patterns, and which are directly executed. It was developed as part of the implementation of """"Snobal /67"""" [1], a Burroughs B6700 compiler for a SNOBOL4-1ike language. The algorithms presented below were developed in Burroughs B6700 Extended Algol, but are described here in Pascal [6, 7], since this is probably a more widely known language. The version of Pascal used has two extensions: record-valued functions, and the ability to declare varying length arrays. The algorithms are by no means complete; for example, they do not include any attempt to avoid futile searching for a match [3]."""	algol;algorithm;burroughs large systems;compiler;nevil maskelyne (magician);pascal;snobol	Nevil Brownlee	1977	Commun. ACM	10.1145/359636.359716	parallel computing;computer science;theoretical computer science;pattern matching;pattern;programming language	PL	-25.863640686957197	25.624205611499708	121274
bb7954a61bd8797d854356f040dffe60a1203316	matlabcompat.jl: helping julia understand your matlab/octave code		Scientific legacy code in MATLAB/Octave not compatible with modernization of research workflows is vastly abundant throughout academic community. Performance of non-vectorized code written in MATLAB/Octave represents a major burden. A new programming language for technical computing Julia, promises to address these issues. Although Julia syntax is similar to MATLAB/Octave, porting code to Julia may be cumbersome for researchers. Here we present MatlabCompat.jl a library aimed at simplifying the conversion of your MATLAB/Octave code to Julia. We show using a simplistic image analysis use case that MATLAB/Octave code can be easily ported to high performant Julia using MatlabCompat.jl.	gnu octave;image analysis;julia;legacy code;matlab;pl/i;programming language	Vardan Andriasyan;Yauhen Yakimovich;Artur Yakimovich	2017	CoRR		computer science;engineering;theoretical computer science;algorithm	HPC	-30.521697349611333	25.84851341271996	121687
971b4ecd277ee460edecbca77f7c45fc3fb84710	on checking correctness of some classes of control systems	control system	Without Abstract	control system;correctness (computer science)	N. P. Redkin	1987		10.1007/3-540-18740-5_80	computer science;automated proof checking;mathematics;distributed computing;programming language	Logic	-19.84599308843459	19.605181085443505	121986
d53e002b652de8c0dc95810c000bf868625a1659	evolving high-level imperative program trees with genetic programming	qa 76 software;computer programming	Genetic Programming (GP) is a technique which uses an evolutionary metaphor to automatically generate computer programs. Although GP proclaims to evolve computer programs, historically it has been used to produce code which more closely resembles mathematical formulae than the well structured programs that modern programmers aim to produce. The objective of this thesis is to explore the use of GP in generating high-level imperative programs and to present some novel techniques to progress this aim. A novel set of extensions to Montana’s Strongly Typed Genetic Programming system are presented that provide a mechanism for constraining the structure of program trees. It is demonstrated that these constraints are sufficient to evolve programs with a naturally imperative structure and to support the use of many common high-level imperative language constructs such as loops. Further simple algorithm modifications are made to support additional constructs, such as variable declarations that create new limited-scope variables. Six non-trivial problems, including sorting and the general even parity problem, are used to experimentally compare the performance of the systems and configurations proposed. Software metrics are widely used in the software engineering process for many purposes, but are largely unused in GP. A detailed analysis of evolved programs is presented using seven different metrics, including cyclomatic complexity and Halstead’s program effort. The relationship between these metrics and a program’s fitness and evaluation time is explored. It is discovered that these metrics are poorly suited for application to improve GP performance, but other potential uses are proposed.	algorithm;computer program;control flow;cyclomatic complexity;declaration (computer programming);experiment;genetic programming;high- and low-level;imperative programming;parity bit;programmer;software development process;software engineering;software metric;sorting;type system;variable (computer science);whole earth 'lectronic link	Tom Castle	2012			genetic programming;computer science;theoretical computer science;algorithm	PL	-21.5607751721036	25.692282672416503	122137
2e4b85707d8ee92d629a29d6ec98a90b5ed1bd2d	parallel object-oriented descriptions of graph reduction machines	parallelisme;distributed system;systeme reparti;metodologia;concepcion sistema;sistema informatico;computer system;methodologie;computer architecture;parallelism;sistema repartido;paralelismo;architecture ordinateur;object oriented;system design;oriente objet;systeme informatique;arquitectura ordenador;methodology;orientado objeto;conception systeme	Abstract machine descriptions of parallel computer architectures must capture communications and concurrency characteristcs at a high level. Current design techniques and notations are weak in this respect. We present a layered method for refinement of a requirements specification through to a detailed systems architecture design. This paper concentrates on the two highest layers, the logical model, which is a requirements statement, and the systems architecture, which specifies logical processes and explicit communications. While requirements are expressed in a language that matches the problem domain, we suggest that a parallel object-oriented notation is most appropriate for the systems architecture layer. Refinements within this layer reflect implementation details (eg. structure sharing and distribution of work among processing elements). We introduce a parallel object-oriented notation based on rewriting systems concepts and use it to refine the design of a parallel graph reduction machine to execute functional programs.machine descriptions of parallel computer architectures must capture communications and concurrency characteristcs at a high level. Current design techniques and notations are weak in this respect. We present a layered method for refinement of a requirements specification through to a detailed systems architecture design. This paper concentrates on the two highest layers, the logical model, which is a requirements statement, and the systems architecture, which specifies logical processes and explicit communications. While requirements are expressed in a language that matches the problem domain, we suggest that a parallel object-oriented notation is most appropriate for the systems architecture layer. Refinements within this layer reflect implementation details (eg. structure sharing and distribution of work among processing elements). We introduce a parallel object-oriented notation based on rewriting systems concepts and use it to refine the design of a parallel graph reduction machine to execute functional programs. The notation used is a natural extension of a graph rewriting language and the work forms the basis for a structured explication of parallel graph rewriting in which all communications are made explicit.		David Bolton;Chris Hankin;Paul H. J. Kelly	1990	Future Generation Comp. Syst.	10.1016/0167-739X(90)90021-5	parallel computing;computer science;artificial intelligence;theoretical computer science;operating system;methodology;database;distributed computing;programming language;object-oriented programming;algorithm;graph rewriting;systems design	SE	-29.83325999169085	30.84879904743887	122151
3f359357a3c790f0ed5a6d9685b0c8ea136b6467	public programming in a web world	distributed application;web;public programming;parallel programming;form based visual programming language;web environment;symbolic urls;formulate;spreadsheet language;distributed applications;visual programming language;visual programming;web browsers;visual languages;internet;internal evaluation algorithm;distributed environment;symbolic urls public programming web world web browsers platform independent distributed environment systems tools spreadsheet language public programmers visual programming language formulate distributed applications web environment internal evaluation algorithm non deterministic distributed environment;object oriented;web world;system design;systems tools;public programmers;non deterministic distributed environment;programming profession java application software computer languages object oriented programming buildings algorithm design and analysis uniform resource locators visual basic internet;language design;parallel programming visual programming visual languages internet;platform independent distributed environment	Web browsers have created a truly platform-independent, distributed environment. While the main focus for this environment has been pre-built applications, there is certainly an opportunity for systems designed to facilitate programming new applications. Systems tools, such as Java, are of course readily available, but not generally usable by public programmers, i.e., programmers without training in either object-oriented or imperative programming. What we have not seen is the equivalent of a spreadsheet language, designed for public programmers and making it possible for these people to build applications that collect and manipulate data, both from within the Web and from without. In this paper we discuss the application of the visual programming language Formulate to building distributed applications via the Web environment. Formulate has certain inherent advantages for application to this environment. Principle among these are: (1) it was designed for public programmers and (2) its internal evaluation algorithm is wellsuited for adaptation to a non-deterministic distributed environment using symbolic URLs.	algorithm;distributed computing;imperative programming;java;programmer;spreadsheet;visual programming language;world wide web	Allen L. Ambler;Jennifer L. Leopold	1998		10.1109/VL.1998.706152	pair programming;computer science;theoretical computer science;programming language;world wide web	SE	-30.98725710774869	25.594477064585515	122239
6e69ad3daf1d4dfe98b83a4ed448cfa0ff016102	delay-bounded scheduling	asynchronous programs;verification;no determinismo;analyse sequentielle;verificacion modelo;fiabilidad;reliability;tarea concurrente;analyse statique;etude experimentale;programa control;bounded delay;verification modele;simultaneidad informatica;sequential analysis;concurrent program;testing;program verification;buffer system;approche deterministe;analisis estatica;analisis programa;sistema amortiguador;deterministic approach;verificacion programa;concurrency;non determinism;model checking;non determinisme;scheduling;fiabilite;programa competidor;retardo limitado;checking program;enfoque determinista;programme controle;concurrent programs;algorithms;sequentialization;conmutador;program analysis;tâche concurrente;analyse programme;static analysis;systeme tampon;verification programme;retard borne;simultaneite informatique;analisis secuencial;concurrent task;estudio experimental;ordonnancement;reglamento;commutateur;selector switch;programme concurrent;delay bound	We provide a new characterization of scheduling nondeterminism by allowing deterministic schedulers to delay their next-scheduled task. In limiting the delays an otherwise-deterministic scheduler is allowed, we discover concurrency bugs efficiently---by exploring few schedules---and robustly---i.e., independent of the number of tasks, context switches, or buffered events. Our characterization elegantly applies to any systematic exploration (e.g., testing, model checking) of concurrent programs with dynamic task-creation. Additionally, we show that certain delaying schedulers admit efficient reductions from concurrent to sequential program analysis.	concurrency (computer science);model checking;network switch;program analysis;scheduling (computing);software bug;software testing	Michael Emmi;Shaz Qadeer;Zvonimir Rakamaric	2011		10.1145/1926385.1926432	program analysis;model checking;real-time computing;verification;concurrency;computer science;bicarbonate buffering system;sequential analysis;reliability;distributed computing;software testing;programming language;deterministic system;scheduling;static analysis;algorithm	PL	-23.21813498886758	32.205911005622454	122669
c9c24d9308c81c01905bb986e354a5e4c539cc47	meta-level constructs for concurrency among loosely-coupled course-grained knowledge sources				David G. Schwartz;Leon Sterling	1992			concurrency;database;computer science	DB	-22.557624764650296	20.96802952524715	122699
84ce0d662dbc036c82aa718734917a03eda3e0a2	multistage indexing algorithms for speeding prolog execution	multistage indexing algorithm;indexing logic programming code optimization pattern matching and unification;prolog execution;term structure;indexation;code optimization;pattern matching;spectrum	In a previous article we proposed a new and efficient indexing technique that utilizes all the functors in the clause-heads and the goal. The salient feature of this technique is that the selected clause-head unifies (modulo nonlinearity)with the goal. As a consequence, our technique results in sharper discrimination, fewer choice points and reduced backtracking. A naı̈ve and direct implementation of our indexing algorithms considerably slowed down the execution speeds of a wide range of programs typically seen in practice. This is because it handled deep and shallow terms, terms with few indexable arguments, small and large procedures uniformly. To beneficially extend the applicability of our algorithms we need mechanisms that are ‘sensitive’ to term structures and size and complexity of procedures. We accomplish this in theν-ALS compiler by carefully decomposing our indexing process intomultiple stages . The operations performed by these stages increase in complexity ranging from first argument indexing to unification (modulo nonlinearity). Further the indexing process can be terminated at any stage if it is not beneficial to continue further. We have now completed the design and implementation of ν-ALS. Using it we have enhanced the performance of a broad range of programs typically encountered in practice. Our experience strongly suggests that indexing based on unification (modulo nonlinearity) is a viable idea in practice and that a broad spectrum of useful programs can realize all of its benefits.	algorithm;backtracking;compile time;compiler;ibm notes;modulo operation;multistage amplifier;nonlinear system;prolog;unification (computer science);warren abstract machine	Ta Chen;I. V. Ramakrishnan;Ramadoss Ramesh	1992			search engine indexing;nonlinear system;computer science;theoretical computer science;unification;operating system;pattern matching;program optimization;database;programming language;prolog;logic programming;code;algorithm;backtracking	PL	-19.492645514697678	22.96997043231719	122880
3e2a69ca942187eb355b5f2b5bd56172033f65d9	combining aspect-oriented and strategic programming	strategic programming;dynamic typing;programming paradigm;perforation;rule based;journal article;term rewrite system;aspect oriented programming;rule based programming;language extension;exception handling;aspect oriented;data flow;term rewriting;unanticipated extension	Properties such as logging, persistence, debugging, tracing, distribution, performance monitoring and exception handling occur in most programming paradigms and are normally very difficult or even impossible to modularize with traditional modularization mechanisms because they are crosscutting. Recently, aspect-oriented programming has enjoyed recognition as a practical solution for separating these concerns. In this paper we describe an extension to the Stratego term rewriting language for capturing such properties. We show our aspect language offers a concise, practical and adaptable solution for dealing with unanticipated algorithm extension for forward data-flow propagation and dynamic type checking of terms. We briefly discuss some of the challenges faced when designing and implementing an aspect extension for and in a rule-based term rewriting system.	algorithm;aspect-oriented programming;dataflow;debugging;exception handling;logic programming;persistence (computer science);programming paradigm;rewriting;software propagation;tracing (software);type system	Karl Trygve Kalleberg;Eelco Visser	2006	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2005.06.035	rule-based system;very high-level programming language;aspect-oriented programming;programming domain;reactive programming;functional reactive programming;computer science;theoretical computer science;extensible programming;functional logic programming;programming paradigm;procedural programming;symbolic programming;inductive programming;fifth-generation programming language;programming language;algorithm	PL	-24.50712073019694	25.78227179345037	123067
88ae83c8129e1b1fb1d3fb410a638794ffa46c38	a method to formalize the semantics of programming languages			programming language;semantics (computer science)	Walter Issel	1975	Elektronische Informationsverarbeitung und Kybernetik		programming language;comparison of multi-paradigm programming languages;third-generation programming language;natural language processing;denotational semantics;mathematics;operational semantics;well-founded semantics;fifth-generation programming language;programming language theory;second-generation programming language;artificial intelligence	PL	-23.802660486923326	21.631567718310453	123120
5409ec594e78d847c0954074dd236495d46bfb92	computing array shapes in matlab	compilacion;optimisation;ombre;optimizacion;semantics;ejecucion programa;semantica;semantique;methode algebrique;program execution;sombra;forme en s;shadow;execution programme;uniformite;uniformidad;algebraic method;inferencia;s shape;uniformity;compilation;number;optimization;metodo algebraico;forma de una s;nombre;large classes;inference;numero	This paper deals with the problem of statically inferring the shape of an array in languages such as MATLAB. Inferring an array’s shape is desirable because it empowers better compilation and interpretation; specifically, knowing an array’s shape could permit reductions in the number of run-time array conformability checks, enable memory preallocation optimizations, and facilitate the in-lining of “scalarized” code. This paper describes how the shape of a MATLAB expression can be determined statically, based on a methodology of systematic matrix formulations. The approach capitalizes on the algebraic properties that underlie MATLAB’s shape semantics and exactly captures the shape that the MATLAB expression assumes at run time. Some of the highlights of the approach are its applicability to a large class of MATLAB functions and its uniformity. Our methods are compared with the previous shadow variable scheme, and we show how the algebraic view allows inferences not deduced by the traditional approach.	apl;algebraic equation;canonical account;circuit complexity;compile time;compiler;inline expansion;interpretation (logic);linear algebra;matlab;run time (program lifecycle phase)	Pramod G. Joisha;U. Nagaraj Shenoy;Prithviraj Banerjee	2001		10.1007/3-540-35767-X_26	shadow;numero sign;computer science;theoretical computer science;operating system;mathematics;semantics;programming language;grammatical number;algorithm	AI	-19.198057405124587	24.499558784805156	123165
c0328ee58a2b36f8d835050bc2666b671d27d45e	transition between different abstraction levels in an abstract state machine (asm) ground model	abstraction level stages intended transition process refinement patterns high level specification transition machine basic transition guidelines basic differences arbitrary data structures abstract rules abstract functions development life cycle service specification asm abstract state machine ground model;refinement calculus data structures finite state machines formal specification;formal specification;asm;formal methods;consistency asm ground model formal methods abstraction levels refinements;finite state machines;ground model;data structures;refinement calculus;abstraction levels;consistency;business abstracts concrete skeleton syntactics educational institutions software;refinements	Using Abstract State Machines (ASMs) we can specify a service or piece of software formally for the purpose of future implementation. During the development life-cycle the related specification evolves. Abstract functions and rules are defined, new functions and rules are introduced and arbitrary data structures are instantiated with the specification being refined towards implementation. Two main abstraction levels are introduced in this paper to show a possible classification of abstraction levels of a formal specification. Furthermore, the basic differences and basic transition guidelines between those levels are discussed. A small example of the proposed two main abstraction levels is shown on a high-level specification of a potential transition machine to outline the intended transition process, typical appearance of abstraction level stages and refinement patterns.	abstract state machines;abstraction layer;data structure;formal specification;high- and low-level;principle of abstraction;production (computer science);refinement (computing);selection rule	Jan Kubovy;Dagmar Auer;Josef Küng;Mariam Rady	2013	2013 24th International Workshop on Database and Expert Systems Applications	10.1109/DEXA.2013.29	refinement calculus;formal methods;computer science;formal specification;database;refinement;consistency;programming language;algorithm;language of temporal ordering specification;abstract state machines	SE	-31.242878778026306	29.60263527079011	123186
14ebb5cda052b102b5c3cbc9925ac355b253829c	gpss/360-norden, an improved system analysis tool	libraries;analytical models;debugging;printers;model generation;computational modeling;displays computational modeling computer simulation printers debugging central processing unit analytical models libraries character generation load modeling;character generation;displays;system analysis;load modeling;computer simulation;central processing unit	The General Purpose Simulation System (GPSS)/ 360 program used with the IBM System/360 computers has been modified to permit larger models, data libraries, and an interactive user service. The major improvements of the GPSS/360-Norden are the following. 1) Models of any size may be run by using direct access devices to store sections of the model. 2) The models can interact with data banks stored on direct access devices. 3) The model output can be presented to the user on a 2250 display it as well as on the conventional printer. 4) Model generation and debugging has been improved using the 2250 display unit. Complete compatibility has been maintained and normal use of GPSS/360 is unaffected by the changes and additions.	system analysis	William A. Walde;David Eig;Sherman R. Hunter	1968	IEEE Trans. Systems Science and Cybernetics	10.1109/TSSC.1968.300173	computer simulation;simulation;computer hardware;computer science;gpss;central processing unit;system analysis;debugging;computational model;computer graphics (images)	Logic	-33.35846016390281	27.43273037351554	123188
bf1c485db939456380955f8594d5b5d22d6e731d	formal definition of sdl-2000 - compiling and running sdl specifications as asm models				Robert Eschbach;Uwe Glässer;Reinhard Gotzhein;Martin von Löwis;Andreas Prinz	2001	J. UCS	10.3217/jucs-007-11-1024	data mining;computer science	NLP	-24.80359630503365	21.360801440579884	123265
0894a15bfcd563719c44f0ed747dc090820abfdd	modular reasoning in object-oriented programming	modular reasoning;object oriented programming;formal verification	Difficulties in reasoning about functional correctness and relational properties of object-oriented programs are reviewed. An approach using auxiliary state is briefly described, with emphasis on the author’s work. Some near term challenges are sketched. Formal verification depends on scientific theories of programming, which answer questions such as these: What are good models of computational behavior? What behavioral properties of components are needed for modular reasoning about a composed system? How can such properties be specified and a component be verified, or even derived from its specification? How can a program and justification of its correctness be revised in accord with small revision of its specification? Such questions have well developed answers that are adequate for small programs under strong simplifying assumptions. But many useful programs are quite large and built from complicated components that violate simplifying assumptions. The longstanding challenge of compositional reasoning remains substantially unsolved. Object-oriented programs pose several challenges that are the focus of my recent research, in which auxiliary state is being used to specify encapsulation boundaries and disciplined interdependence. Section 2, explains the approach, accomplishments, and challenges in terms of invariants for shared mutable objects. Section 3 addresses relational properties including data refinement and secure information flow. This line of research has been carried out for Java-like programming languages; I argue in Section 1 for the importance of such languages. Some additional challenges pertinent to objectoriented programming, but not tied to the main theme, are discussed in Section 4. A detailed tutorial on the state-based approach to encapsulation advocated here appears elsewhere [33]. Several near-term challenges (1–5 years) are presented here in the setting of sequential object-oriented programs. Because the approach taken here is based on the use of assertions, it is also quite relevant to verification of concurrent object-oriented programs and low level imperative code. 1 Why Java-like language? In order to develop theory for modular reasoning about large programs, we need a corpus of large programs and automated support for experiments. Since I would like to do ? Partially supported by the National Science Foundation under grants CCR-0208984 and CCF0429894 and by Microsoft Research. science that contributes to human good through improved engineering, the primary objects of study should be representative examples of large programs that are significantly deployed and used. This means confronting programs written in notations like C, Java, and C#—though not necessarily handling all of their features without restriction. Aside from obvious pragmatic reasons for interest in Java-like languages, there are technical reasons why such a language is a good point in the language design space. – The language is sufficiently rich to express higher order design patterns which are needed for well structured programs and used in common practice. – Despite the preceding item, the language is essentially “defunctionalized” [42, 4] owing to the binding of methods to classes rather than to instances. Thus relatively simple semantic models are adequate, at least for large fragments of the language. For example, my work discussed in Sections 2 and 3 has been done using a straightforward Scott-Strachey denotational semantics, for a fragment of Java including recursive types, inheritance, mutable objects, and other features without restriction; this model has been encoded in PVS [34]. Nipkow’s group and others have obtained strong results using straightforward operational models [26]. – The module system (packages, generic classes, public/private/protected visibility) embodies most of what current theory offers for scope-based encapsulation. – The Java type system is name-based; named types provide a convenient hook on which to hang specifications and encapsulation boundaries. In particular, it helps deal with inheritance, which is widely used if problemmatic. – Pointer arithmetic is absent. Parameter passing is by value and identifiers cannot alias. Method declarations are not nested, avoiding the semantic complexity of reference to variables in enclosing scopes other than global scope.1 These features are not without cost. Java programs make much use of global variables (“statics”)—global in that they are in outermost scopes; this is mitigated in that the scope of visibility may be a single class or package. Reflection, at least in full generality, is a feature I see as a very difficult and long-term challenge for verification. This is exacerbated in that reflection, like threads and permission-based access control, appears in the form of special libraries rather than being distinguished with separate syntax. Perhaps the highest cost is the ubiquity of aliasing in the sense of shared references to mutable objects in the heap. 2 Heap encapsulation using auxiliary state For modular reasoning in object-oriented programming there are several challenges. 1. Non-hierarchical control flow due to callbacks leads, even in sequential programs, to interference like that in concurrent programs. 2. The conventional notion of layered abstraction is also subverted by non-hierarchical control flow due to inheritance and method overriding. 1 Compare the complexity of Idealized Algol models [44] with Modula-3 and Oberon, where non-local references are restricted for those procedures that are passed as arguments or stored in variables [32]. 3. Design patterns that are essentially higher order are often used, but unlike in functional programming the encapsulation aspects are not explicit in the program text, owing to data representation based on shared heap objects. 4. Functional aspects of such patterns are also not specified formally, for lack of good models (compare “map” in functional programming with the “Visitor” pattern). The second challenge is addressed by the notion of behavioral subtyping which is well understood [29, 20] except that the extant theories do not fully deal with the first and third challenges. For the fourth challenge, which we discuss in Section 4, one might argue that at best we should aim for verifying simple safety properties. Indeed, in his VSTTE talk Bart Jacobs said that full functional verification of nontrivial Java programs is impractical. But for realistically complex systems, attempts to verify simple safety properties lead to the need for more general properties, especially object invariants. For the first and third challenges, progress is being made using auxiliary state to express encapsulation using assertions. That is the topic of this section, which focuses on object invariants. More extensive discussions and citations on these topics can be found in Müller’s VSTTE paper [31] and my survey paper [33]. Non-hierarchical control flow. As an example of the first challenge, consider a sensor playing the role of Subject in the Subject/Observer pattern [22]. The sensor maintains a set of registered Views: when the sensor value reaches the threshhold v.thresh of a given view v, the sensor invokes method v.notify and removes v from the set. This description is in terms of a set, part of the abstraction offered by the Subject; the implementation might store views in an array ordered by thresh values. The pattern cannot be seen simply as a client using an abstraction, because notify is what is known as an upcall to the client. The difficulty is that v.notify may make a reentrant callback to the sensor. Some callbacks are quite sensible, e.g., the view could query the sensor value. But trouble is likely if v.notify invokes a method to enumerate the current set of views. While notifications are under way, the array may be in an inconsistent state—is v in the set? in the array?—yet the enumeration method may assume as precondition the sensor’s invariant. Non-hierarchical control flow renders naive reasoning about object invariants unsound. The problem is similar to interference in shared-variable concurrency, for which there are several established and well understood solutions. For the reentrant callback problem, which already occurs in sequential code, the situation is less settled, although the probem is a frequent cause of insidious bugs. Various solutions have been proposed: – Establish caller’s invariant before every method call. But this is impractical in many cases: most calls do not result in reentrant callbacks and good use of abstraction in design leads to many calls to substructures while a super-structure’s invariant is temporarily violated. – Use concurrency locks. But this leads to deadlocks in the sequential case. – Use temporal specification of allowed calling sequences. This can be heavy handed and violates abstraction by making method calls visible. Moreover, verification of such properties requires the whole program in general. A more promising approach begins by making the invariant an explicit precondition on those methods that assume it, like the enumerator in the example. This precondition cannot be established by client v attempting a reentrant callback, unless in fact the sensor restores its invariant before invoking v.notify. An object invariant I ought not appear in the precondition of a public method, as that could expose the internal representation. Various techniques have been proposed to hide information, e.g., treating I in a precondition as an opaque predicate [14, 15], a typestate [19], a call to a pure method, or a model field [30, 25]. We advocate the approach of Leino et al [8], known as the Boogie methodology or the inv/own discipline. We give a simplified account sufficient for discussion. The discipline uses a ghost (auxiliary) field2 inv of type boolean which represents whether the invariant of o is in force, just as a programmer might do using an ordinary 	access control;aliasing;approximation;callback (computer programming);class invariant;client (computing);complex systems;concurrency (computer science);control flow;correctness (computer science);data (computing);data security;deadlock;denotational semantics;design pattern;encapsulation (networking);enumerated type;enumerator (computer science);experiment;formal verification;functional programming;global variable;identifier;immutable object;imperative programming;interdependence;interference (communication);invariant (computer science);java;library (computing);lock (computer science);method (computer programming);method overriding;microsoft research;modula-3;modular programming;oberon;observer pattern;opaque predicate;pointer (computer programming);precondition;programmer;programming language;recursion;reentrancy (computing);refinement (computing);relevance;rendering (computer graphics);semantic data model;shared variables;software bug;text corpus;theory;type system;verification and validation;visitor pattern	David A. Naumann	2005		10.1007/978-3-540-69149-5_13	qualitative reasoning;formal verification;computer science;theoretical computer science;reasoning system;programming language;object-oriented programming;deductive reasoning;algorithm	PL	-21.14507067603277	29.9320364630438	123439
450af4151c088cb21bc6f014c99ca486c65bbe98	procedural control in production systems	production system	This paper proposes a general production system architecture that allows procedural control knowledge to be directly represented and used. This architecture, called a controlled production ~vs'tem, is based on a separately specified control structure that effects control over production invocation and interaction independently of the search strategy. It is shown that a controlled production system provide~ a hasis for describing and implementing control constructs which, unlike most existing schemes, is formally adequate and retains all the properties desired of a knowledge based systenl--modularity, flexibility, extensibilit~, and explanat,rv capacity. We also show that this architecture provides for a uniform programming methodoh~gy--the procedural languages and the declarative languages turn out to be special cases of a controlled production system. Schemes for improving system efficiency and resolving nondeterminism are also exanlined It is ~hown that the separate representation of control prot, ides a basis for a theory Of efficiency transformations on production systents, and allows for more effective means of directing s'earch.	control flow;integrated development environment;nondeterministic algorithm;production system (computer science);systems architecture	Michael P. Georgeff	1982	Artif. Intell.	10.1016/0004-3702(82)90039-X	simulation;computer science;artificial intelligence;production system;algorithm	OS	-26.4758130137043	27.492528737017267	123474
0cd93c9380121fb90d4dc35fb307b19dcb23576c	a mapreduce based approach of scalable multidimensional anonymization for big data privacy preservation on cloud	cost effectiveness mapreduce cloud computing privacy protection data anonymization scalable multidimensional anonymization approach big data privacy preservation scalable median finding algorithm median of medians technique histogram technique recursion granularity;privacy preservation;big data;multidimensional anonymization;information management data handling data storage systems scalability data privacy cloud computing privacy;mapreduce;data protection big data cloud computing;data protection;conference proceeding;cloud computing;multidimensional anonymization big data cloud computing privacy preservation mapreduce	The massive increase in computing power and data storage capacity provisioned by cloud computing as well as advances in big data mining and analytics have expanded the scope of information available to businesses, government, and individuals by orders of magnitude. Meanwhile, privacy protection is one of most concerned issues in big data and cloud applications, thereby requiring strong preservation of customer privacy and attracting considerable attention from both IT industry and academia. Data anonymization provides an effective way for data privacy preservation, and multidimensional anonymization scheme is a widely-adopted one among existing anonymization schemes. However, existing multidimensional anonymization approaches suffer from severe scalability or IT cost issues when handling big data due to their incapability of fully leveraging cloud resources or being cost-effectively adapted to cloud environments. As such, we propose a scalable multidimensional anonymization approach for big data privacy preservation using Map Reduce on cloud. In the approach, a highly scalable median-finding algorithm combining the idea of the median of medians and histogram technique is proposed and the recursion granularity is controlled to achieve cost-effectiveness. Corresponding MapReduce jobs are dedicatedly designed, and the experiment evaluations demonstrate that with our approach, the scalability and cost-effectiveness of multidimensional scheme can be improved significantly over existing approaches.	big data;case preservation;cloud computing;computer data storage;consumer privacy;data anonymization;data mining;information privacy;mapreduce;median of medians;recursion;scalability;selection algorithm	Xuyun Zhang;Chi Yang;Surya Nepal;Chang Liu;Wan-Chun Dou;Jinjun Chen	2013	2013 International Conference on Cloud and Green Computing	10.1109/CGC.2013.24	computer science;data mining;database;internet privacy	DB	-30.69360471704413	20.254096762863334	123480
79f670ef7e154c606b7a4bc005b0ec7850463d24	infrawatch: data management of large systems for monitoring infrastructural performance	data management;data capture;data mining;sensor network;public domain;intelligent data analysis;data analysis;complex data	This paper introduces a new project, InfraWatch, that demonstrates the many challenges that a large complex data analysis application has to offer in terms of data capture, management, analysis and reporting. The project is concerned with the intelligent monitoring and analysis of large infrastructural projects in the public domain, such as public roads, highways, tunnels and bridges. As a demonstrator, the project includes the detailed measurement of traffic and weather load on one of the largest highway bridges in the Netherlands. As part of a recent renovation and re-enforcement effort, the bridge has been equipped with a substantial sensor network, which has been producing large amounts of sensor data for more than a year. The bridge is currently equipped with a multitude of vibration and stress sensors, a video camera and weather station. We propose this bridge as a challenging environment for intelligent data analysis research. In this paper we outline the reasons for monitoring infrastructural assets through sensors, the scientific challenges in for example data management and analysis, and we present a visualization tool for the data coming from the bridge. We think that the bridge can serve as a means to promote research and education in intelligent data analysis.	adaptive system;computer data storage;computer vision;data mining;data stream mining;data visualization;international standard book number;john bridges (software developer);knowledge representation and reasoning;machine learning;online and offline;sensor;sequential pattern mining;smart transducer;springer (tank);structure mining	Arno J. Knobbe;Hendrik Blockeel;Arne Koopman;Toon Calders;Bas Obladen;Carlos Bosma;Hessel Galenkamp;Eddy Koenders;Joost N. Kok	2010		10.1007/978-3-642-13062-5_10	public domain;simulation;wireless sensor network;data management;computer science;data mining;automatic identification and data capture;data analysis;computer security;complex data type	Robotics	-32.61904383180135	18.8224664305719	123593
22d7ab4fbb8997eb521aec3ed9fa61fe1ba3134e	algebraic operational semantics and occam	concurrent language;operational semantics	We generalize algebraic operational semantics from sequential languages to distributed, concurrent languages using Occam as an example. Elsewhere, we will discuss applications to the study of veri cation and transformation of programs.	abstract machine;distributed computing;mathematical model;operational semantics;production (computer science);programming language;occam	Yuri Gurevich;Lawrence S. Moss	1989		10.1007/3-540-52753-2_39	formal semantics;action semantics;computer science;theoretical computer science;communicating sequential processes;formal semantics;programming language;well-founded semantics;operational semantics;denotational semantics;algorithm;computational semantics	PL	-21.544285683259723	21.973025123483712	123738
0442ff9c4ce57e3b080523a170c4204fa83c9e55	self: the power of simplicity	exploratory programming;lexical scoping;ordinary object;conventional language;inheritance hierarchy;object-oriented language;concrete idea;new object-oriented language;object-oriented computation;new insight;object oriented language;object oriented	Self is a new object-oriented language for exploratory programming based on a small number of simple and concrete ideas: prototypes, slots, and behavior. Prototypes combine inheritance and instantiation to provide a framework that is simpler and more flexible than most object-oriented languages. Slots unite variables and procedures into a single construct. This permits the inheritance hierarchy to take over the function of lexical scoping in conventional languages. Finally, because Self does not distinguish state from behavior, it narrows the gaps between ordinary objects, procedures, and closures. Self's simplicity and expressiveness offer new insights into object-oriented computation.	computation;exploratory programming;linkage (software);scope (computer science);universal instantiation	David M. Ungar;Randall B. Smith	1987	Lisp and Symbolic Computation	10.1145/38765.38828	method;object model;computer science;programming language;object-oriented programming;object definition language	PL	-25.889252445891003	27.19587490886303	123882
86d2ffd8c8cfdd9f181966a07011d9dd542dec69	ecoop’ 87 european conference on object-oriented programming	programming language;object oriented programming;software engineering	The Smalltalk-80 system offers a language with a small and elegant conceptual core, and a highly interactive programming environment. We believe, however, that it could be made more learnable and usable by a relatively small set of changes. In this paper, we present the results of a series of empirical studies on learnability, and also some informal studies of large implementation projects. Based on these studies, we suggest a number of changes to the Smalltalk-80 language and system.	integrated development environment;interactive programming;learnability;smalltalk	Barstow;John C. Brauer;Brinch Hansen D. Gries D. Luckham;Moler;Stoer;Wirth	1987		10.1007/3-540-47891-4	first-generation programming language;declarative programming;very high-level programming language;programming domain;computer science;theoretical computer science;software development;extensible programming;software engineering;functional logic programming;computer programming;programming paradigm;symbolic programming;inductive programming;fifth-generation programming language;programming language theory;programming language;object-oriented programming;concurrent object-oriented programming;programming in the large and programming in the small	SE	-27.264571999936408	24.865883110376114	123925
bb4a85c5d5e23af5a831605c7391e1c3f2694b3a	classifying inheritance mechanisms in concurrent object oriented programming	herencia;distributed system;systeme reparti;concurrent object oriented programming;semantica formal;heritage;sistema informatico;simultaneidad informatica;computer system;object oriented programming;formal semantics;semantique formelle;concurrency;sistema repartido;object oriented;systeme informatique;formal analysis;inheritance;programmation orientee objet;simultaneite informatique	Inheritance is one of the key concepts in object-oriented programming. However, the usefulness of inheritance in concurrent objectoriented programming is greatly reduced by the problem of inheritance anomaly. Inheritance anomaly is manifested by undesirable re-definitions of inherited code. The problem is aggravated by the lack of a formal analysis, with a multitude of differing proposals and conflicting opinions causing the current state of research, and further directions, to be unclear. In this paper we present a formal analysis of inheritance anomaly in concurrent object-oriented programming. Starting from a formal definition of the problem we develop a taxonomy of the anomaly, and use it to classify the various proposals. As a result, the major ideas, trends and limitations of the various proposals are clearly exposed. Formal analysis of the anomaly and a thorough exposition of its causes and implications are the pre-requisites for a successful integration of inheritance and concurrency.	576i;acm transactions on programming languages and systems;agent-oriented programming;anomaly detection;bernard j. lechner;communicating sequential processes;concurrency (computer science);concurrent computing;distributed computing;exception handling;generic programming;hoare logic;lecture notes in computer science;parsing;prentice hall international series in computer science;programming language;programming paradigm;real-time transcription;software bug;springer (tank);switzerland;taxonomy (general);theory of computation;trinity	Lobel Crnogorac;Anand S. Rao;Kotagiri Ramamohanarao	1998		10.1007/BFb0054108	computer science;artificial intelligence;database;programming language;object-oriented programming;algorithm	PL	-24.39845063796626	29.74406527979671	124084
8e7d43eef23651d999ff4b66ba18bbff89395101	first-order logic according to harrison		We present a certified declarative first-order prover with equality based on John Harrison’s Handbook of Practical Logic and Automated Reasoning, Cambridge University Press, 2009. ML code reflection is used such that the entire prover can be executed within Isabelle as a very simple interactive proof assistant. As examples we consider Pelletier’s problems 1-46.	automated reasoning;declarative programming;first-order logic;first-order predicate;interactive proof system;isabelle;proof assistant;reflection (computer programming)	Alexander Birch Jensen;Anders Schlichtkrull;Jørgen Villadsen	2017	Archive of Formal Proofs		algorithm;first-order logic;mathematics	Logic	-20.452793973406365	20.092332033821794	124143
796751c9c530596df160456c998fcbf4d5a7a783	interactive high-level language direct-execution microprocessor system	computer program;high level languages microprocessors interactive systems writing natural languages debugging joining processes microcomputers computer science computer errors;interactive system;direct execution system;program debugging direct execution system high level language system interactive systems interpreter microprocessor system;program debugging;interpreter;microprocessor system;high level language;interactive systems;high level language system	It is our habit in writing an English composition that, as we write each word, each phrase, each sentence, and each paragraph, we consciously or unconsciously check the syntax and the semantics of the composition just written. Writing a computer program in a high-level language could be made similar to writing a composition in English. In this case, a highly interactive highlevel language system checks the syntax and the semantics of the highlevel language program as each symbol, each expression, and each statement are being entered at the terminal. When the source program is completely entered, the program could have been debugged and could have run once.	computer program;consciousness;debugging;high- and low-level;high-level programming language;microprocessor	Yaohan Chu;E. Raymond Cannon	1976	IEEE Transactions on Software Engineering	10.1109/TSE.1976.233802	natural language processing;computer architecture;interpreter;computer science;programming language;high-level programming language	SE	-27.458765747614027	23.228706526268716	124270
7d7ba7ac0007eb1dc8d3d6d9d861a5edb0e416ee	fluid updates: beyond strong vs. weak updates	state space explosion	We describe a symbolic heap abstraction that unifies reasoning about arrays, pointers, and scalars, and we define a fluid update operation on this symbolic heap that relaxes the dichotomy between strong and weak updates. Our technique is fully automatic, does not suffer from the kind of state-space explosion problem partition-based approaches are prone to, and can naturally express properties that hold for non-contiguous array elements. We demonstrate the effectiveness of this technique by evaluating it on challenging array benchmarks and by automatically verifying buffer accesses and dereferences in five Unix Coreutils applications with no annotations or false alarms.	benchmark (computing);state space;unix;verification and validation	Isil Dillig;Thomas Dillig;Alexander Aiken	2010		10.1007/978-3-642-11957-6_14	computer science;theoretical computer science;algorithm	PL	-20.76620853705197	31.438319968486557	124709
f6dc2fd4fd7e750df16996db680f9a47c919d222	c++ - the core language: a foundation for c programmers			c++;programmer	Gregory Satir;Doug Brown	1995			c++/cx;computer science;software engineering;programming language	PL	-29.531568560331795	26.440535546349825	124816
ac8aef4b47e748cbad7abf133b12a4d587e2f114	a database programming language for a deductive object-oriented database	thesis;computer software programming computer software		database;programming language	Maria Lusia Barja Goni	1995			fourth-generation programming language;first-generation programming language;computing;data manipulation language;programming domain;computer science;programming language implementation;theoretical computer science;software framework;component-based software engineering;software development;software construction;computer programming;database;programming paradigm;symbolic programming;inductive programming;programming language theory;programming language;computer network programming;system programming;software system;component-oriented database	DB	-25.07399054804863	22.59194503638976	124825
76384bc147eb5387c448ab1de75ce8fd9c2964e0	a translation of statecharts into signal approach of time, interoperability	formal specification;program interpreters;code generation;program verification;systems analysis;clock calculus reactive systems modeling languages statecharts specification signal specification signal tools verification efficient distributed compact code generation;reactive system;program interpreters systems analysis formal specification program verification;equations signal processing automata signal generators clocks calculus signal design circuits wiring control systems	The languages for modeling reactive systems can be divided in two styles: the imperative ones and the declarative ones. This paper shows a way to translate a Statecharts speci cation (imperative) to a Signal one (declarative, equational, synchronous). This translation gives access to the Signal tools from a Statecharts speci cation: veri cation, e cient / distributed / compact code generation using the clock calculus available in Signal.	code generation (compiler);declarative programming;imperative programming;interoperability	Jean-René Beauvais;Roland Houdebine;Paul Le Guernic;Éric Rutten;Thierry Gautier	1998		10.1109/CSD.1998.657539	systems analysis;computer architecture;real-time computing;reactive system;computer science;formal specification;programming language;code generation	SE	-32.940145213214144	32.17288705259232	125031
5b1c85fe7849c68e5388403234eb2a36e349b4e8	the meb and ceb static analysis for csp specifications	concurrent programming;control flow graph;concurrent programs;csp;program slicing;static analysis;data structure	This work presents a static analysis technique based on program slicing for CSP specifications. Given a particular event in a CSP specification, our technique allows us to know what parts of the specification must necessarily be executed before this event, and what parts of the specification could be executed before it in some execution. Our technique is based on a new data structure which extends the Synchronized Control Flow Graph (SCFG). We show that this new data structure improves the SCFG by taking into account the context in which processes are called and, thus, makes the slicing process more precise.	algorithm;control flow graph;data structure;experiment;program slicing;prototype;ssi ceb;static program analysis	Michael Leuschel;Marisa Llorens;Javier Oliver;Josep Silva;Salvador Tamarit	2008		10.1007/978-3-642-00515-2_8	program slicing;real-time computing;data structure;computer science;communicating sequential processes;database;programming language;static analysis;control flow graph	SE	-19.162325836085017	28.7995074109112	125110
cead44d7cbc286f00847ed129f5e6dc384dab483	using the bridge design pattern for osgi service update		In the OSGi framework, components cooperate by sharing service objects. The suggested way to replace a service by a newer version consists of updating its containing components which requires a temporary shutdown of the component. Special care must be taken to avoid dangling references to old service instances. As this appears to be an overly expensive strategy, we describe the use of the well-known Bridge design pattern to decouple service replacement from component updates. Instead of registering services only references to instances of automatically generated bridge classes are registered. This solves not only the problem of dangling references but also avoids stopping and starting dependent bundles.	bridge pattern;dangling pointer;design pattern;embedded system;osgi;run time (program lifecycle phase);shutdown (computing);whole earth 'lectronic link	Hans Werner Pohl;Jens Gerlach	2003			real-time computing;systems engineering;computer network;computer science;bridge pattern;shutdown	DB	-27.00008668635381	30.640924794785853	125114
b8489c7c5a7ae2600e3628e3225d01641560ab11	persistent first class procedures are enough	programming language;programming environment;information hiding;data type;modelling language;file system;separate compilation;data protection;database management system;abstract types	We describe how the provision of a persistent programming environment together with a language that supports first class procedures may be used to provide the semantic features of other object modelling languages. In particular the effects of information hiding, data protection and separate compilation are provided and a comparison of the method with more traditional techniques is examined.	compiler;first-class function;information privacy;integrated development environment;modeling language	Malcolm P. Atkinson;Ronald Morrison	1984		10.1007/3-540-13883-8_75	fourth-generation programming language;first-generation programming language;data definition language;dynamic compilation;language primitive;data manipulation language;programming domain;data type;reactive programming;data control language;computer science;programming language implementation;theoretical computer science;database;data protection act 1998;programming paradigm;procedural programming;symbolic programming;low-level programming language;fifth-generation programming language;programming language;information hiding;abstract data type;programming language specification;high-level programming language	PL	-26.207581948025044	26.869900978598952	125388
6e71df95573a4f50458ab4e551c000fb85117488	the use of hoare logic in the verification of horizontal microprograms	hardware verification;programming language;microprogrammation;logique hoare;programmation câblee;program verification;hoare logic;formal verification;axiomatisation;design and implementation;langage programmation;microprogramming;verification programme;correctness proof	In recent years, much effort has been devoted to the design and implementation of microprogramming languages that support the production of highly reliable yet efficient run time microcode. One of the goals for such languages is to facilitate the formal verification of microprograms using Hoare's inductive assertion method. Essential to the use of this method is an axiomatic definition of the microprogramming language. In this paper, we describe the axiomatization of a machine dependent microprogramming language called S*(QM-1)(1). This language is an instantiation of the machine independent language schema S*(2,3) based on the Nanodata QM-1 “nanolevel” architecture, and is designed for the development and specification ofhorizontal microprograms. We discuss the rationale underlying the design and axiomatization of this language and we show, using S*(QM-1) as a case study, some of the important points in which the verification of firmware differs from software verification.	assertion (software development);axiomatic system;design rationale;firmware;formal verification;hoare logic;machine-dependent software;microcode;run time (program lifecycle phase);software verification;universal instantiation	Subrata Dasgupta;Alan Wagner	1984	International Journal of Computer & Information Sciences	10.1007/BF00985823	separation logic;formal verification;computer science;theoretical computer science;microcode;high-level verification;hoare logic;programming language;axiomatic semantics;intelligent verification;algorithm;functional verification	Arch	-24.41930898943166	30.572540265735018	125487
364df8e7f45860e03f90aa903cd88b52a15b7af2	on the expressiveness of links in hypertext systems	machine abstraite;developpement logiciel;traitement texte;hipertexto;formal specification;automata estado finito;maquina abstracta;ingenieria logiciel;software engineering;abstract machine;specification formelle;especificacion formal;automate a pile;adaptive hypertext;desarrollo logicial;finite state automata;software development;genie logiciel;pushdown automata;tratamiento textos;generating function;finite automaton;automate fini;push down automaton;hypertexte;hypertext;word processing;automata a pila	In this paper, we study how linking mechanisms contribute to the expressiveness of hypertext systems. For this purpose, we formalize hypertext systems as abstract machines. As the primary benefit of hypertext systems is to be able to read documents non-linearly, their expressiveness is defined in terms of the ability to follow links. Then, we classify hypertext systems according to the power of the underlying automaton. The model allows us to compare embedded versus separate links and simple versus generic links. Then, we investigate history mechanisms, adaptive hypertexts and functional links. Our conclusion is that simple links, whether embedded or separate, generic links and some adaptive links all give hypertext systems the power of finite state automata. The history mechanism confers to them the power of pushdown automata, whereas the general functional links give them Turing completeness.	abstract machine;adaptive grammar;automata theory;carr–benkler wager;computation;embedded system;finite-state machine;hyperlink;hypertext;leslie speaker;multi-user;observable;pushdown automaton;stack (abstract data type);turing completeness;turing machine	Luc Moreau;Wendy Hall	1998	Comput. J.	10.1093/comjnl/41.7.459	generating function;hypertext;computer science;theoretical computer science;software development;operating system;formal specification;database;mathematics;finite-state machine;programming language;pushdown automaton;algorithm	OS	-24.40413613053822	31.593063357453705	125504
ed81211acd4969b71d0a20ac64635c8b59a0458e	object-oriented programming at john deere	object oriented programming	John Deere has taken an aggressive approach to th e adoption of object-oriented programming techniques . Object-oriented programming was selected in 1984 as th e foundation for a new development program on Unix-base d workstations, and a C-based language for object-oriente d programming was developed to provide portability back to a large corporate mainframe system . This language has bee n designed to provide as simple a transition as possible for programmers already trained in conventional programmin g techniques.	d programming language;mainframe computer;programmer;unix;workstation	Roger Burkhart	1989	DATA BASE	10.1145/71232.71235	computer science;programming language;object-oriented programming	PL	-29.030149376419633	26.80600934671935	125866
41c882369999611e1f30699a746977f6674b584e	design and implementation of a task-oriented robot language	lenguaje programacion;programmation;programming language;robot industriel;programacion;design and implementation;problem oriented language;object oriented;robot industrial;lenguaje orientado problema;langage programmation;source code;high level language;programming;langage oriente probleme;industrial robot	This paper describes a very high-level language processor for the block world, consisting of a task-oriented level and an object-oriented level. The command 'build arch' is taken as an example. First, the task-oriented-level processor translates the input (e.g. build arch) into an output description of the concrete goal state by referring to knowledge about the goal task in the concept model. The object-orientedlevel processor then translates the description oT the goal state into the source code of the motion-level processor AL/L by automatically making task plans and computing the position and orientation of each object. If the concept of a goal task is pre-defined in the concept model, these processors can expand the goal task into the AL/L source code. As a result, human programmers do not need to consider the details of the environment or of the robot.	blocks world;central processing unit;high- and low-level;natural language processing;programmer;robot;very high-level programming language	Akira Okano;Hitoshi Matsubara;Hirochika Inoue	1988	Advanced Robotics	10.1163/156855389X00082	programming;simulation;computer science;artificial intelligence;programming language;object-oriented programming;high-level programming language;source code	Robotics	-26.88343410650369	24.54136624489407	125895
f86188a76d3b2d077c37e2fa1bedf8c8f310a65f	a language support for exhaustive fault-injection in message-passing system models.		This paper presents an approach towards specifying and veri fying adaptive distributed systems. We here take fault-handling as an example of adaptive behavior nd propose a modeling language Sandal for describing fault-prone message-passing systems. One o f th unique mechanisms of the language is a linguistic support for abstracting typical faults such as unexpected termination of processes and random loss of messages. The Sandal compiler translates a mo del into a set of NuSMV modules. During the compilation process, faults specified in the mode l will be woven into the output. One can thus enjoy full-automatic exhaustive fault-injection wit hout writing faulty behaviors explicitly. We demonstrate the advantage of the language by verifying a mod el of the two-phase commit protocol under faulty environment.	adaptive behavior;compiler;distributed computing;fault injection;message passing;modeling language;nusmv;software bug;two-phase commit protocol;verification and validation	Masaya Suzuki;Takuo Watanabe	2014		10.4204/EPTCS.168.4	real-time computing;computer science;theoretical computer science;programming language;algorithm	AI	-22.261331871202152	29.471649895888373	126061
cc2c4771adab2ffb1805f152995610aecfc020a9	a practical method for syntactic error diagnosis and recovery	error recovery;right hand side;semantic information;code optimisation;error diagnosis;compilation;pascal;cray 1;vector processors;semantic analysis	Our goal is to develop a practical syntactic error recovery method applicable within the general framework of viable prefix parsing. Our method represents an attempt to accurately diagnose and report all syntax errors without reporting errors that are not actually present. Successful recovery depends upon accurate diagnosis of errors together with sensible “correction” or alteration of the text to put the parse back on track. The issuing of accurate and helpful diagnostics is achieved by indicating the nature of the recovery made for each error encountered. The error recovery is prior to and independent of any semantic analysis of the program. However, the method does not exclude the invocation of semantic actions while parsing or preclude the use of semantic information for error recovery.  The method assumes a framework in which an LR or LL parser, driven by the tables produced by a parser generator, maintains an input symbol buffer, state or prediction stack, and parse stack. The input symbol buffer contains part or all of the sequence of remaining input tokens, including the current token. The LR state stack is analogous to the LL prediction stack; except when restricting our attention to the LL case, prediction stack shall serve as a generic term indicating the LR state or LL prediction stack. The parse stack contains the symbols of the right hand sides that have not yet been reduced.	alphabet (formal languages);compiler-compiler;ll parser;lr parser;parsing;semantic analysis (compilers);syntax error	Michael G. Burke;Gerald A. Fisher	1982		10.1145/800230.806981	speech recognition;pascal;computer science;theoretical computer science;programming language;algorithm	NLP	-24.49825595929603	24.974516412037786	126222
479ef71d0276bddba32d652cad99b4f53f25dca1	a methodology for machine language decompilation	inverse translation;target language;program documentation;optimization;decompiling;machine language translation	Machine language decompilation is the translation of machine (assembly) language instruction sequences into statements in a high-level algebraic language such as PL/1. This process can be viewed as the inverse of compilation. Decompilation can be used as an aid for program conversion and program documentation. A general methodology for decompilation that is independent of a particular source and target language is presented. The basic approach is to map the source machine language to a high-level representation, which is relatively machine and language independent, and then translate to the chosen target language. An experimental decompiler was implemented to translate Knuth's MIXAL assembly language into PL/1.	assembly language;compiler;decompiler;documentation generator;high- and low-level;mix;machine code;pl/i	Barron C. Housel;Maurice H. Halstead	1974		10.1145/800182.810410	natural language processing;language identification;cache language model;first-generation programming language;speech recognition;transfer-based machine translation;universal networking language;interpreter;language primitive;object language;specification language;data control language;computer science;programming language implementation;common intermediate language;compiled language;machine translation;low-level programming language;programming language;machine translation software usability;high-level programming language;source code	PL	-26.69552199123135	23.787229491037674	126286
15dd5120c01b3b40737583d2da5a91d4dc6b8493	you say 'what', i hear 'where' and 'why'? (mis-)interpreting sql to derive fine-grained provenance		SQL declaratively specifies what the desired output of a query is. This work shows that a non-standard interpretation of the SQL semantics can, instead, disclose where a piece of the output originated in the input and why that piece found its way into the result. We derive such data provenance for very rich SQL dialects—including recursion, windowed aggregates, and user-defined functions—at the fine-grained level of individual table cells. The approach is non-invasive and implemented as a compositional source-level SQL rewrite: an input SQL query is transformed into its own interpreter that wields data dependencies instead of regular values. We deliberately design this transformation to preserve the shape of both data and query, which allows provenance derivation to scale to complex queries without overwhelming the underlying database system. 1. DATA PROVENANCE EXPLAINS COMPLEX SQL QUERIES A complex SQL query. In a hilly landscape, which marks are visible from your current location? That will depend on your position’s altitude and the height of the terrain around you: valleys are obscured by nearby ridges, while peaks, even if remote, may still be in view. The two-dimensional sketch of Figure 1 suggests one answer to the question: first, compute the running maximum (or: max scan) of view angles between our location and the ever farther hill tops before us. Second, a mark is visible iff its angle is at least as large as the maximum angle αi we have measured so far. We thus can spot the tree (its view angle α3 exceeds the current maximum of α2) while marks p1 and p2 are obscured. The max scan technique does apply in three dimensions, This paper is an extended version of an article published in the Proceedings of the VLDB Endowment (PVLDB, 11(11), August 2018). 200 m 300 m 400 m 500 m 600 m 700 m 0° 0° 0° α1 α2 α2 α2 α2 α2 α2 α3 α3 α3 α3 MAX(α) scan α1 α2 α3	data dependency;database;declarative programming;recursion;rewrite (programming);sql;select (sql);vldb;window function;winsock	Tobias Mueller;Benjamin Dietrich;Torsten Grust	2018	PVLDB	10.14778/3236187.3236204	database;semantics;recursion;sql;computer science;derivation;provenance;interpreter	DB	-21.182067872076605	24.759109581853856	126425
c111c75a318eb123af37f8f5c4cd1e44b131b50c	a unique formalism for specifying and designing objects in a parallel environment	object oriented language	When objects are put in a parallel environment, new problems appear. Constraints on methods may change, the composition and extension of objects is more difficult. We introduce a concurrent object concept to implement solutions of these problems in object-oriented languages. A formalization of this concept and proofs of properties are built up in a Multimodal Logic of Actions. We terminate this paper by a discussion about the proof of inheritance properties in a parallel environment.	semantics (computer science)	Jean Paul Bahsoun;Corinne Servieres;Christel Seguin	1993		10.1007/BFb0039714	computer science;theoretical computer science;distributed computing;programming language;object-oriented programming	HCI	-28.620317964801625	31.09185952726026	126444
3623261c7184141ea50a2dbee980c65223158a1b	self type constructors	herencia;theorie type;theory binary methods;securite;heritage;generics;langage java;mytype;binary methods;higher order;teoria de tipos;expressive power;polymorphism;type theory;safety;design;lenguaje java;polymorphisme;polimorfismo;inheritance;seguridad;languages;type constructor polymorphism;abstract types;type safety;java language	Bruce and Foster proposed the language LOOJ, an extension of Java with the notion of MyType, which represents the type of a self reference and changes its meaning along with inheritance. MyType is useful to write extensible yet type-safe classes for objects with recursive interfaces, that is, ones with methods that take or return objects of the same type as the receiver.  Although LOOJ has also generics, MyType has been introduced as a feature rather orthogonal to generics. As a result, LOOJ cannot express an interface that refers to the same generic class recursively but with different type arguments. This is a significant limitation because such an interface naturally arises in practice, for example, in a generic collection class with method map(), which converts a collection to the same kind of collection of a different element type. Altherr and Cremet and Moors, Piessens, and Odersky gave solutions to this problem but they used a highly sophisticated combination of advanced mechanisms such as abstract type members, higher-order type constructors, and F-bounded polymorphism.  In this paper, we give another solution by introducing self type constructors, which integrate MyType and generics so that MyType can take type arguments in a generic class. Self type constructors are tailored to writing recursive interfaces more concicely than previous solutions. We demonstrate the expressive power of self type constructors by means of examples, formalize a core language with self type constructors, and prove its type safety.	abstract type;generic programming;java;looj;recursion;self-reference;type constructor;type safety	Chieri Saito;Atsushi Igarashi	2009		10.1145/1640089.1640109	polymorphism;design;unit type;type safety;computer science;recursive data type;type constructor;programming language;kind;type theory;expressive power;algorithm	PL	-25.032909645499817	27.13968777914786	126491
e1c821216ee19d6da80863f37dfaf41e67c188c6	lazy and enforceable assertions for functional logic programs	prototypical implementation;demand-driven evaluation strategy;lazy assertion;o action;functional logic programming;strict assertion;enforceable assertion;program execution;functional logic language;important technique	Assertions or contracts are an important technique to improve the quality of software. Thus, assertions are also desirable for functional logic programming. Unfortunately, there is no established meaning of assertions in languages with a demand-driven evaluation strategy. Strict assertions are immediately checked but may influence the behavior of programs. Lazy assertions do not modify the behavior but may not be faithful since some assertions might not be checked at all. In order to avoid the disadvantages of strict and lazy assertions, we propose enforceable assertions that are delayed as lazy assertions but can be explicitly enforced at some point where faith is required, e.g., at the end of the program execution or before irrevocable I/O actions. We describe a prototypical implementation of this idea in the functional logic language Curry where the programmer can choose between lazy and enforceable assertions.	assertion (software development);compiler;computation;constraint programming;curry;declarative programming;flops;functional logic programming;functional programming;graphical user interface;hall effect;haskell;input/output;international conference on logic programming;jones calculus;lazy evaluation;lecture notes in computer science;library (computing);mera 300;object-oriented software construction;overhead (computing);programmer;programming paradigm;runtime error detection;runtime system;software testing;springer (tank);steiner tree problem;strict programming language;symposium on principles of programming languages;type system;web development	Michael Hanus	2010		10.1007/978-3-642-20775-4_5	computer science;theoretical computer science;programming language;algorithm	PL	-21.33046527387424	23.613925214717952	126705
6a4004b798bc9c00dd092aceaedca2ceff4eef46	a complete transformational toolkit for compilers	dependence analysis;lambda calculus;rewrite systems;program slicing;equational logic;constant propagation	"""In an earlier paper, one of the present authors presented a preliminary account of an equational logic called P ~ . PIM is intended to function as a """"transformational toolkit"""" to be used by compilers and analysis tools for imperative languages, and has been applied to such problems as program slicing, symbolic evaluation, conditional constant propagation, and dependence analysis. PIM consists of the untyped lambda calculus extended with an algebraic rewriting system that characterizes the behavior of lazy stores and generalized conditionals. A major question left open in the earlier paper was whether there existed a complete equational axiomatization of PtM's semantics. In this paper, we answer this question in the affirmative for Pnvl's core algebraic component, PIMt, under the assumption of certain reasonable restrictions on term formation. We systematically derive the complete PIM logic as the culmination of a sequence of increasingly powerful equational systems starting from a straightforward """"interpreter"""" for closed PIM terms."""	axiomatic system;compiler;constant folding;dependence analysis;imperative programming;lambda calculus;lazy evaluation;linear algebra;p (complexity);program slicing;rewriting;software propagation;symbolic execution	Jan A. Bergstra;T. B. Dinesh;John Field;Jan Heering	1996		10.1007/3-540-61055-3_31	program slicing;equational logic;computer science;lambda calculus;programming language;constant folding;algorithm;dependence analysis	Logic	-19.37975467626707	23.34236430135139	126839
da5eecdd4dddb786bd8a251cfc7b6a155c967bb9	subtype polymorphism à la carte via machine learning on dependent types		The essential rationale for subtype polymorphism is adherence to the u0027Open/Closed Principleu0027 [12]: the ability to write framework code in terms of superclasses and subsequently invoke it with any subclass that exhibits u0027proper subtypingu0027 via the Liskov Substitution Principle (LSP) [11]. Formally, the LSP states that if o(t : T) is a provable property of objects t of type T, then o(s) should be true for objects s of subtype S of T. In practice, such properties have typically been those expressible via u0027Design by Contractu0027 [12], specifically preconditions, postconditions and invariants. Such abstraction via subtype polymorphism is intended to insulate against requirements change. However, when new requirements do necessitate a change of contract, the maintenance consequences can be severe. In the (typical) absence of explicit language or tool support, enforcement of proper subtyping is laborious and error-prone: contractual changes typically require manual inspection/repair of the class hierarchy to determine/address violations of the LSP.		Jerry Swan;Colin G. Johnson;Edwin C. Brady	2018		10.1145/3236454.3236469	programming language;class hierarchy;liskov substitution principle;enforcement;design by contract;invariant (mathematics);subtyping;abstraction;computer science;computer programming	SE	-20.48715065594468	26.874755990413338	126898
3c8680d499247971a269157be3e5cedefe088804	visualizing program designs through pegasys	control systems;programming environments;lan interconnection;visual programming;visualization;logic programming;data structures;visualization documentation graphics lan interconnection data structures flowcharts programming environments logic programming control systems;flowcharts;graphics;documentation	This article is an introduction to many of the interesting features of PegaSys, an experimental system that encourages and facilitates extensive use of graphical images as formal, machine- processable documentation. Unlike most other systems that use graphics to describe programs, the main purpose of PegaSys is to facilitate the explanation of program designs. What is particularly interesting about PegaSys is its ability to: (1) check whether pictures are syntactically meaningful, (2) enforce design rules throughout the hierarchical decomposition of a design, and (3) determine whether a program meets its pictorial documentation. Much of the power of PegaSys stems from its ability to represent and reason about different kinds of pictures within a single logical framework. Excerpts from a working session with PegaSys are used to illustrate the basic style of interaction as well as the three PegaSys capabilities.	documentation;experimental system;graphical user interface;graphics;image;list of bbs software;logical framework	Mark Moriconi;Dwight F. Hare	1985	Computer	10.1109/MC.1985.1662979	visualization;data structure;documentation;computer science;control system;graphics;artificial intelligence;theoretical computer science;operating system;software engineering;programming language;logic programming;computer security	AI	-31.19074349688571	23.741252742210065	127064
0d2cc9ee73d9398fd273845a26b3ed79af5ebf29	observable sharing for functional circuit description	semantica operacional;preuve programme;program proof;operational semantics;program transformation;specification programme;transformation programme;functional programming;transformacion programa;semantique operationnelle;informatique theorique;prueba programa;programmation fonctionnelle;functional language;program specification;programacion funcional;functional programming language;especificacion programa;computer theory;informatica teorica	y Abstract Pure functional programming languages have been proposed as a vehicle to describe, simulate and manipulate circuit speciications. We propose an extension to Haskell to solve a standard problem when manipulating data types representing circuits in a lazy functional language. The problem is that circuits are nite graphs { but viewing them as an algebraic (lazy) datatype makes them indistinguishable from potentially innnite regular trees. However, implementations of Haskell do indeed represent cyclic structures by graphs. The problem is that the sharing of nodes that creates such cycles is not observable by any function which traverses such a structure. In this paper we propose an extension to call-by-need languages which makes graph sharing observable. The extension is based on non updatable reference cells and an equality test (sharing detection) on this type. We show that this simple and practical extension has well-behaved semantic properties, which means that many typical source-to-source program transformations, such as might be performed by a compiler, are still valid in the presence of this extension.	compiler;functional programming;haskell;lazy evaluation;observable;program transformation;programming language;simulation	Koen Claessen;David Sands	1999		10.1007/3-540-46674-6_7	computer science;artificial intelligence;theoretical computer science;operating system;database;mathematics;distributed computing;programming language;functional programming;operational semantics;algorithm	PL	-19.361673740080022	23.600375149160712	127253
d80e27b4215144919316d0e65c92d9ff9dc3fdd2	a framework for designing concurrent and recoverable abstract data types based on commutativity	abstract data type	In this paper, we try to focus the reader's interest on the problems that transactional systems have to resolve for taking advantage of commutativity in a serializable and recoverable way. Our framework is, (as others), based on the use of conditional commutativity on abstract date types. We present new features that have not been found in the literature hitherto, that both increase concurrency and simplify recovery.	abstract data type;concurrency (computer science);schedule (computer science);serializability	Carmelo Malta;José L Martínez	2010	CoRR		real-time computing;computer science;distributed computing;abstract data type;algorithm	PL	-21.05149134780785	30.24014224037	127408
c8c49ce0f68655767c2434b171674130898c245a	taming the pl/i syntax	language generator;generateur langage;generator;analyse syntaxique;pl1 language;syntactic analysis;pl1;parser;analyseur syntaxique;pl i	Ideally the syntactic part of a PL/I compiler would be generated directly from the semiformal definition of ANSI Standard PL/I. A practical approach to this is described, using finite state machines and an LALR parser generator. The parser uses a method due to Aoe which avoids list searching. Adapted for this method the PL/I grammar has 841 states. The parse table generator exploits the freedom to renumber states in a way that improves on previous algorithms for compacting the tables. The parser tables occupy less than 4K bytes.	algorithm;byte;compiler;finite-state machine;lalr parser generator;lr parser;line number;pl/i;parsing;table (database)	B. L. Marks	1984	Softw., Pract. Exper.	10.1002/spe.4380140807	natural language processing;compiler-compiler;lalr parser;canonical lr parser;computer science;parsing;glr parser;programming language;recursive descent parser;algorithm;simple lr parser	NLP	-24.56664258389937	24.390074017194188	127564
3c422799e505e988a0fb88d227039af2fe16d08c	genetic programming		We present a set of extensions to Montana’s popular Strongly Typed Genetic Programming system that introduce constraints on the structure of program trees. It is demonstrated that these constraints can be used to evolve programs with a naturally imperative structure, using common high-level imperative language constructs such as loops. A set of three problems including factorial and the general even-n-parity problem are used to test the system. Experimental results are presented which show success rates and required computational effort that compare favourably against other systems on these problems, while providing support for this imperative structure.	computation;control flow;genetic programming;high- and low-level;imperative programming;type system	Dimitrios Effraimidis	2012		10.1007/978-3-642-29139-5		PL	-22.062660613301055	25.548930673122733	127648
1b5cc53a3cd531907d5c7619d9a286a6289f062d	using horocol to program a society of agents or teams of robots	programming languages robot programming;multi agent system;programming language multi agent systems self reconfigurable robots robocup;programming language;robots programming horocol language;self reconfigurable robots;robot kinematics robot programming service robots hardware legged locomotion parallel robots computer languages multiagent systems trajectory hybrid power systems;programming languages;robot programming	In this paper we present an example of the use of the Horocol language for programming a society or teams of robots. This example shows the principal features of the Horocol language. This language has been developed to offer a solution to express the behaviours of a set of teams of robots or agents. We focus on the originality of this language which is in the instructions for programming the team coordination.	parallel computing;precondition;rewrite (programming);robot	Dominique Duhaut;Yann Le Guyadec;Michel Dubois	2005	2005 International Symposium on Computational Intelligence in Robotics and Automation	10.1109/CIRA.2005.1554324	simulation;reactive programming;functional reactive programming;computer science;programming language implementation;artificial intelligence;extensible programming;multi-agent system;robot control;programming paradigm;symbolic programming;inductive programming;fifth-generation programming language	Robotics	-28.512552439418087	20.94226515397548	127757
330ee11431fb2cde7084029580cd875b85eccec4	visualisations compactes: une approche déclarative pour la visualisation d'information	representation flexibility;algorithm description models;information visualization;generic visualization models;taxonomy of representations;dataflow architectures for information visualization;algorithmic complexity of information visualization	We introduce a descriptive model that allows the definition of a large class of information visualization algorithms with a small number of parameters. Compact visualizations, which we conjecture is equivalent to the class of visualizations that can be rendered in a time directly proportional to the size of the input data, are defined by a fixed dataflow architecture: clustering and subclustering of input data, sort, graphic primitives and graphic attributes generation. At each step, the parameters are expressions of the host programming language, which include input attribute names and local variable names. Local variables are a specific concept that allows us to extend the expressiveness of the dataflow architecture. We introduce our model formally, and then show its expressiveness with a few examples.	algorithm;cluster analysis;dataflow architecture;declarative programming;expressive power (computer science);information visualization;linear algebra;local variable;patent visualisation;programming language	Thomas Baudel	2002		10.1145/777005.777027	information visualization;computer science;theoretical computer science;algorithm	PL	-28.122790066800846	18.3203382532176	127858
a87d8c03bfbc7d231e334e03bc372c7249ae46b9	hint orchestration using acl2's simplifier		This paper describes a strategy for providing hints during an ACL2 proof, implemented in a utility called use-termhint. An extra literal is added to the goal clause and simplified along with the rest of the goal until it is stable under simplification, after which the simplified literal is examined and a hint extracted from it. This simple technique supports some commonly desirable yet elusive features. It supports providing different hints to different cases of a case split, as well as binding variables so as to avoid repeating multiply referenced subterms. Since terms used in these hints are simplified in the same way as the rest of the goal, this strategy is also more robust against changes in the rewriting normal form than hints in which terms from the goal are written out explicitly.	acl2;high-level programming language;literal (mathematical logic);name binding;programming idiom;rewriting;text simplification	Sol Swords	2018	CoRR	10.4204/EPTCS.280.13	orchestration (computing);algorithm;mathematics;acl2;rewriting	NLP	-21.38353048005994	25.300451745970662	127985
38d2bc1a325ecdfc404bc2ba0250cf031011681f	broadcast and broadband reception quality field experiment to validate the effectiveness of media-unifying platform		With the growing popularity of a variety of devices, such as smart phones and tablets, users now have more options to watch video content, and the same content is available on different media using either broadcast or broadband. To simplify video watching for users, we have proposed Media-Unifying platform that automatically selects the appropriate delivery media according to the device function and the user's situation. This paper describes a field experiment with measuring tools that record the reception quality of both broadcast and broadband. The experiment was conducted in an environment that simulates real-life situations, such as walking or riding on a train, to verify the validity of the proposed platform. Measured data showed that the platform can increase total viewable time and reduce the amount of broadband data reception.	digital video;experiment;real life;smartphone;tablet computer	Hiroki Endo;Shuhei Taguchi;Kinji Matsumura;Kazuya Fujisawa;Kenjiro Kai	2017	2017 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB)	10.1109/BMSB.2017.7986195	computer science;broadband networks;computer network;field experiment;popularity;multimedia;broadband;broadcasting	Mobile	-31.357104042551878	19.452744934538014	128125
30bc651747b442de20c704720364d45d7a48c931	efficient trace monitoring	runtime verification;software systems;aspect oriented programming;context free grammar;program monitoring;program analysis;regular expression	A wealth of recent research involves generating program monitors from declarative specifications. Doing this efficiently has proved challenging, and available implementations often produce infeasibly slow monitors. We demonstrate how to dramatically improve performance -- typically reducing overheads to within an order of magnitude of the program's normal runtime.		Pavel Avgustinov;Julian Tibble;Eric Bodden;Laurie J. Hendren;Ondrej Lhoták;Oege de Moor;Neil Ongkingco;Ganesh Sittampalam	2006		10.1145/1176617.1176673	program analysis;parallel computing;real-time computing;aspect-oriented programming;computer science;runtime verification;context-free grammar;programming language;regular expression;software system	Arch	-20.981448672059532	32.266240617422376	128205
454352d5dac27e1f4f62cd187f2e0029351daf90	formalizing a hierarchical file system	verification;proof assistant;flash memory;specification;refinement;theorem proving;model checking;levels of abstraction;file system;permission system;store	An abstract file system is defined here as a partial function from (absolute) paths to data. Such a file system determines the set of valid paths. It allows the file system to be read and written at a valid path, and it allows the system to be modified by the Unix operations for creation, removal, and moving of files and directories. We present abstract definitions (axioms) for these operations. This specification is refined towards a pointer implementation. The challenge is to have a natural abstraction function from the implementation to the specification, to define operations on the concrete store that behave exactly in the same way as the corresponding functions on the abstract store, and to prove these facts. To mitigate the problems attached to partial functions, we do this in two steps: first a refinement towards a pointer implementation with total functions, followed by one that allows partial functions. These two refinements are proved correct by means of a number of invariants. Indeed, the insights gained consist, on the one hand, of the invariants of the pointer implementation that are needed for the refinement functions, and on the other hand of the precise enabling conditions of the operations on the different levels of abstraction. Each of the three specification levels is enriched with a permission system for reading, writing, or executing, and the refinement relations between these permission systems are explored. Files and directories are distinguished from the outset, but this rarely affects our part of the specifications. All results have been verified with the proof assistant PVS, in particular, that the invariants are preserved by the operations, and that, where the invariants hold, the operations commute with the refinement functions.	acl2;automated theorem proving;concurrency control;coq (software);correctness (computer science);directory (computing);fault tolerance;function type;hol (proof assistant);hard link;higher-order function;human computer;identifier;invariant (computer science);isabelle;pointer (computer programming);principle of abstraction;proof assistant;prototype verification system;recursion;recursive definition;refinement (computing);requirement;rework (electronics);turing completeness;type system;unix;user experience	Wim H. Hesselink;Muhammad Ikram Lali	2010	Formal Aspects of Computing	10.1007/s00165-010-0171-2	model checking;verification;computer science;theoretical computer science;database;mathematics;refinement;automated theorem proving;proof assistant;programming language;specification;algorithm	PL	-21.753747276952385	26.509571933014893	128232
8f4e21a3d3a00bd30b8a7daf7b65320ad519d2c6	multi-dimensional sensor data aggregator for adaptive network management in m2m communications	compressed sensing;temperature sensors;servers;data restoration multidimensional sensor data aggregator adaptive network management m2m communications sensors buildings houses sensor gateway service providers multidimensional matrices kddi corporation;logic gates;xml;logic gates servers temperature sensors compressed sensing temperature measurement xml;temperature measurement;telecommunication network management client server systems sensors	This paper proposes a method of aggregating tempo-spatial data generated by sensors deployed in buildings or houses. The size of each sensor data such as temperature is usually small, but it often involves many additional data to represent its attribute values like time, location, data type and data precision. This would often increase the traffic volume between sensor gateway at building/home side and service providers at server side. In our method, such sensor data are packed into multidimensional matrices indexed by those attribute values for more compact representation, and the compressed sensing technique is adaptively applied to further reduce the data size. The method was applied to a field trial with KDDI corporation to collect data from 29 community facilities, and the traffic volume was reduced to 50% with reasonable precision of data restoration.	circuit restoration;compressed sensing;data aggregation;sensor;server-side;significant figures	Kenji Yoi;Hirozumi Yamaguchi;Akihito Hiromori;Akira Uchiyama;Teruo Higashino;Naohisa Yanagiya;Toshikazu Nakatani;Atsuo Tachibana;Teruyuki Hasegawa	2015	2015 IFIP/IEEE International Symposium on Integrated Network Management (IM)	10.1109/INM.2015.7140431	xml;logic gate;temperature measurement;computer science;operating system;data mining;database;compressed sensing;computer security;server;computer network	DB	-31.075578703253086	18.40120864732604	128260
0911e1a2349c5ede3bff482e1dc338b498db3c4e	the architecture of the utrecht haskell compiler	international organizations;compiler architecture;attribute grammar;intermediate language;aspect oriented;program analysis;haskell;aspect orientation	In this paper we describe the architecture of the Utrecht Haskell Compiler (UHC).  UHC is a new Haskell compiler, that supports most (but not all) Haskell 98 features, plus some experimental extensions. It targets multiple backends, including a bytecode interpreter backend and a whole-program analysis backend, both via C. The implementation is rigorously organized as stepwise transformations through some explicit intermediate languages. The tree walks of all transformations are expressed as an algebra, with the aid of an Attribute Grammar based preprocessor. The compiler is just one materialization of a framework that supports experimentation with language variants, thanks to an aspect-oriented internal organization.	aspect-oriented software development;attribute grammar;compiler;haskell;intermediate representation;interprocedural optimization;preprocessor;program analysis;stepwise regression	Atze Dijkstra;Jeroen Fokker;S. Doaitse Swierstra	2009		10.1145/1596638.1596650	program analysis;compiler;parallel computing;aspect-oriented programming;compiler correctness;computer science;compiler construction;bootstrapping;programming language;attribute grammar;intermediate language;functional compiler;algorithm	PL	-23.563115067879274	24.776428238184646	128911
7968adf5fa9d112ec6f72c2b154ee91b106c0cb1	a method using a set-theoretical formalism to describe the semantics of programming languages	semantics of programming languages	Without Abstract	semantics (computer science)	Walter Issel	1976		10.1007/3-540-07854-1_200	natural language processing;fourth-generation programming language;declarative programming;action semantics;type erasure;computer science;theoretical computer science;third-generation programming language;functional logic programming;formal semantics;mathematics;programming paradigm;abstract family of languages;inductive programming;fifth-generation programming language;programming language theory;programming language;well-founded semantics;operational semantics;second-generation programming language;comparison of multi-paradigm programming languages;denotational semantics;control flow analysis;semantics	PL	-23.969824281909005	21.659779538332288	129041
fee21b8cc916e6fafb1345bb6f5922f3d9109631	implementation strategies for diana attributes	abstract syntax tree;efficient implementation;compact representation;semantic analysis;intermediate representation	DIANA is a high-level intermediate representation for Ada programs [Goos83]. The DIANA representation of a source program has the basic form of an abstract syntax tree for the program, augmented by attributes representing the results of semantic analysis. Since it first appeared in 1981 [Goos81], some implementors have criticised DIANA as inefficient. In particulax, people often claim that it requires too much space to implement on a small machine. This paper refutes this claim by showing how several space-efficient implementation techniques fit into the DI ANA framework.	abstract syntax tree;ada;high- and low-level;intermediate representation	David Alex Lamb	1987	SIGPLAN Notices	10.1145/39305.39310	abstract syntax;computer science;theoretical computer science;database;programming language;intermediate language;abstract syntax tree	PL	-23.240930756135743	24.575557733836202	129132
26fe7b6586500b7c26471c56e9d6b0aadc0fd831	alarm trend catcher: projecting operating conditions in the electrical power grid domain with interactive alarm visualization		Electric power grid operation is a critical activity in which control room operators supervise the status of the grid infrastructure in order to avoid undesirable situations. One of the most important tasks of this operation is projecting ahead the infrastructure state in real time. This task is currently based on managing alarms, with the responsibility of the control room operator to estimate both their relevance and evolution in time. A review of traditional alarm visualization formats highlights some deficiencies, which may lead to operating inefficiencies or even critical operating problems. Driven by a field study, this paper presents Alarm Trend Catcher, an innovative alarm visualization approach aimed at assisting not only the perception of upcoming alarms but also the projection of future states of the electric power grid. The user study of this visualization approach shows supporting evidence of its benefits for gaining interesting operational insights from the alarm information.		Rosa Romero Gómez;David Díez Cebollero	2016			simulation;artificial intelligence;operating system;data mining;computer security	HPC	-32.48615531740558	20.17007671419678	129310
1b493831f22c84c5b220a3075f038db7df18af3c	knowledgesheet: a graphical spreadsheet interface for interactively developing a class of constraint programs	finite domain constraints;constraint programming;constraint satisfaction problem;large classes	We introduce a generalization of the spreadsheet paradigm, called Knowledgesheet, for solving a class of constraint satisfaction problems. The traditional spreadsheet paradigm is based on attaching arithmetic expressions to individual cells and then evaluating them; our Knowledgesheet interface instead allows finite domain constraints to be attached to individual cells that are then solved to obtain a solution. This extension provides an easy-to-use interface for solving a large class of constraint satisfaction problems--those whose specification and solution conforms to a 2-dimensional structure, e.g., scheduling problems, timetabling problems, etc. A prototype for the Knowledgesheet has been developed and applied to solve many different types of problems.	graphical user interface;interactivity;spreadsheet	Gopal Gupta;Shameem F. Akhter	2000		10.1007/3-540-46584-7_21	constraint logic programming;concurrent constraint logic programming;mathematical optimization;constraint programming;constraint satisfaction;constraint learning;computer science;constraint graph;theoretical computer science;constraint satisfaction dual problem;complexity of constraint satisfaction;constraint;programming language;constraint satisfaction problem;algorithm;hybrid algorithm;local consistency;backtracking	HCI	-30.163380675690632	23.38619540385075	129362
a634a2357ea8cc5e835f1d3d0bac3a29e38e659b	meta-generation of syntax-oriented editors		A method for automatic generation of syntax-oriented editors (SOE) for high level programming languages is presented. It is based on a special template definition metalanguage. The SOE functional environment including the operations with source files and internal representation of the programs in form of abstract syntax tree was implemented as an independent modular structure. As a result of target language metadescription processing the SOE for this language is generated by a special preprocessor. Depending on user's experience level (novice, advanced, etc.), generation of various SOE for the same language by changing a level of descriptions of phrases and templates is also possible.	abstract syntax tree;compiler;high-level programming language;parse tree;preprocessor;school of everything;syntax highlighting	N. Shvets;Constantin Ciubotaru	1995	The Computer Science Journal of Moldova		natural language processing;computer science;programming language	PL	-27.49333675446794	23.361955508752015	129412
2fb0587bedc0fa0ef724ea374a5b6d2b39280f61	the denotational semantics of dynamic networks of processes	informatica;parallelisme;lenguaje de programacion;mathematics;programming language;formal semantics;semantique formelle;denotational semantic;natuurwetenschappen;parallelism;ordered by external client;denotational semantics;langage programmation;landbouwwetenschappen;reseau dynamique;semantique denotationnelle;wiskunde en informatica wiin;dynamic networks	DNP (dynamic networks of processes) is a variant of the language introduced by Kahn and MacQueen [11, 12]. In the language it is possible to create new processes dynamically. We present a complete, formal denotational semantics for the language, along the lines sketched by Kahn and MacQueen. An informal explanation of the formal semantics is also given.	dnp3;denotational semantics;kahn process networks;semantics (computer science)	Arie de Bruin;A. P. Wim Böhm	1985	ACM Trans. Program. Lang. Syst.	10.1145/4472.4473	normalisation by evaluation;formal semantics;action semantics;computer science;theoretical computer science;formal semantics;programming language;denotational semantics of the actor model;operational semantics;denotational semantics;algorithm	PL	-21.080229420546264	22.326029669516544	129422
6f26cae070a00172d7f0b1b3db4b228ad52a9f89	val: automatic plan validation, continuous effects and mixed initiative planning using pddl	programming language semantics;mixed initiative;planning artificial intelligence;aerospace computing planning artificial intelligence programming language semantics;semantic interpretation;aerospace computing;international planning competition;electronic computers computer science;space operations project automatic plan validation tool val 3rd international planning competition pddl2 1 mixed initiative planning;humans power system modeling batteries differential equations information systems debugging visualization radio access networks process planning immune system	This work describes aspects of our plan validation tool, VAL. The tool was initially developed to support the 3rd International Planning Competition, but has subsequently been extended in order to exploit its capabilities in plan validation and development. In particular, the tool has been extended to include advanced features of PDDL2.1 which have proved important in mixed-initiative planning in a space operations project. Amongst these features, treatment of continuous effects is the most significant, with important effects on the semantic interpretation of plans. The tool has also been extended to keep abreast of developments in PDDL, providing critical support to participants and organisers of the 4th IPC.	approximation algorithm;automated planning and scheduling;beagle;bridging (networking);exploratory testing;mobile robot;numerical analysis;planning domain definition language;polynomial;rover (the prisoner);semantic interpretation;time complexity;variable assembly language	Richard Howey;Derek Long;Maria Fox	2004	16th IEEE International Conference on Tools with Artificial Intelligence	10.1109/ICTAI.2004.120	semantic interpretation;simulation;computer science;artificial intelligence;machine learning;management science;programming language	Robotics	-28.74663391429636	20.64133554722405	129532
93f1faacf7d83e7eff5302de5984841af4e606c5	a family of gödel machine implementations	artificial intelligence	The Gödel Machine is a universal problem solver encoded as a completely self-referential program capable of rewriting any part of itself, provided it can prove that the rewrite is useful according to some utility function, encoded within itself. Based on experience gained by constructing a virtual machine capable of running the first Gödel Machine implementation written in self-referential code, we discuss several important refinements of the original concept. We also show how different approaches to implementing the proof search leads to a family of possible Gödel Machine implementations.	gödel machine;rewrite (programming);rewriting;self-reference;solver;utility;virtual machine	Bas R. Steunebrink;Jürgen Schmidhuber	2011		10.1007/978-3-642-22887-2_29	computer science;artificial intelligence;theoretical computer science;machine learning;algorithm	PL	-21.456842786798116	18.68494031014757	129544
0191f7b2c10ae1c403432e45f33acd3dd932e30b	josh: an open aspectj-like language	extensibility;generic description;aspect oriented programming;pointcut	Although aspect-oriented programming (AOP) is becoming widely used, the design of the pointcut language and the generic and reusable description of advice are still research topics. To address these topics, this paper presents Josh, which is our new AspectJ-like language with an extensible pointcut language and a few mechanisms for generic description. The extensible pointcut language is based on the idea of open compiler. Since Josh allows defining a new pointcut designator in Java, the users can define a pointcut designator useful in a particular application domain. Also, Josh allows any Java expression to be included in the body of advice. This mechanism enables the generic and reusable description of advice.	advice (programming);application domain;aspect-oriented programming;aspectj;compiler;java;pointcut	Shigeru Chiba;Kiyoshi Nakagawa	2004		10.1145/976270.976284	aspect-oriented programming;extensibility;computer science;theoretical computer science;database;programming language	PL	-27.197496613239945	29.111039052273846	129556
0e2c8d71a3d1e5840cdf65e12fb5c718cb540d38	the transform - a new language construct			language construct	David Gries;Dennis M. Volpano	1990	Structured Programming		computer science;theoretical computer science;constructed language	Theory	-23.344532543064368	21.76110420609808	129625
c38d07315385c2bf37787e1dcb380a03aad7230e	λ-elimination in illative combinatory logic			combinatory logic	Martin W. Bunder	1979	Notre Dame Journal of Formal Logic	10.1305/ndjfl/1093882671		Logic	-22.008190123523324	20.386712442934947	129683
fb7e50dea1d752d8e28bd980d963c18ef078b7c1	visualization of chr through source-to-source transformation	004;source to source transformation constraint handling rules visualization	In this paper, we propose an extension of Constraint Handling Rules (CHR) with different visualization features. One feature is to visualize the execution of rules applied on a list of constraints. The second feature is to represent some of the CHR constraints as objects and visualize the effect of CHR rules on them. To avoid changing the compiler, our implementation is based on source-to-source transformation. 1998 ACM Subject Classification I.2.2 Automatic Programming, D.3.2 Language Classifications	automatic programming;compiler;constraint handling rules;source transformation	Slim Abdennadher;Nada Sharaf	2012		10.4230/LIPIcs.ICLP.2012.109	computer science;theoretical computer science;programming language;algorithm	PL	-22.421949262166375	24.52042589714771	129802
13b509cd812adb497e3802a1902f79df306e1948	pascal program development aids	expert judgement;judgement spaces;target language;fsm;pattern recognition;simulated human judgement;artificial intelligence;bridge bidding;game playing;program development	In this paper we describe a set of tools we have devised to aid in the development and maintenance of PASCAL programs. PASCAL has been chosen as a target language since it is gaining increasing interest as a language for use on mini- and microcomputers.	adobe coldfusion;compiler;microcomputer;pascal	Charles E. Hughes;Charles P. Pfleeger	1979		10.1145/503506.503514	simulation;computer science;artificial intelligence;algorithm	PL	-27.91929179044542	23.37878988240862	130173
2b3bd5bdf34dca27838e5356a76938bb812a9577	extension language automation of embedded system debugging	distributed debugging;debugger;software engineering;embedded system;computer network;tcl tk;input output;temporal constraints;object oriented;extension language;reflection	Embedded systems contain several layers of target processing abstraction. These layers include electronic circuit, binary machine code, mnemonic assembly code, and high-level procedural and object-oriented abstractions. Physical and temporal constraints and artifacts within physically embedded systems make it impossible for software engineers to operate at a single layer of processor abstraction. The Luxdbg embedded system debugger exposes these layers to debugger users, and it adds an additional layer, the extension language layer, that allows users to extend both the debugger and its target processor capabilities. Tcl is Luxdbg's extension language. Luxdbg users can apply Tcl to automate interactive debugging steps, to redirect and to interconnect target processor input-output facilities, to schedule multiple processor execution, to log and to react to target processing exceptions, to automate target system testing, and to prototype new debugging features. Inclusion of an extension language like Tcl in a debugger promises additional advantages for distributed debugging, where debuggers can pass extension language expressions across computer networks.	artifact (software development);assembly language;automation;debugger;debugging;electronic circuit;embedded system;high- and low-level;machine code;procedural programming;prototype;scripting language;software engineer;system testing;tcl	Dale E. Parson;Bryan Schlieder;Paul Beatty	2000	Automated Software Engineering	10.1023/A:1013276002164	input/output;parallel computing;real-time computing;reflection;computer science;software engineering;programming language;object-oriented programming	EDA	-30.746200907160954	27.535835038184306	130205
c85b67644525440f5636cced8065bb54f88270f6	an integrated environment for developing communication protocols	outil logiciel;software tool;protocole transmission;formalization;implementation;reseau ordinateur;conception;refinement;test;formal protocol development;computer network;ensayo;ejecucion;afinamiento;essai;protocolo transmision;formal description techniques;herramienta controlada por logicial;protocol validation;red ordenador;diseno;formalizacion;affinement;communication protocol;design;validation;formalisation;protocol engineering;communication protocols;integrated tools environment;technique description formelle;transmission protocol	Abstract   Due to technological advances and the international trends to open interoperable systems, development of communication protocols for computer networks and distributed systems is becoming increasingly complex and cost sensitive. Protocol engineers require an improved methodology, supported by powerful tools over the whole development process. In this paper, we introduce a development life-cycle based on formal methods. It is used to identify the different activities, from requirements definition to specification, implementation, and testing, together with the set of tools that apply to each phase. We then describe the architecture of an integrated tools environment for protocol engineering and report on a realization of the basic components.		Jürgen M. Schneider;Lothar F. Mackert;Georg Zörntlein;Roelof Jan Velthuys;Udo Bär	1992	Computer Networks and ISDN Systems	10.1016/0169-7552(92)90123-8	communications protocol;simulation;computer science;algorithm;computer network	Theory	-32.44940056072296	31.244589636383417	130374
086025e6545cae98a12748361b6a96e852ee759f	generics in java and c++: a comparative model	lenguaje programacion;langage oriente objet;object oriented model;langage c;generic algorithm;programming language;langage java;programmation generique;c language;comparative modeling;polymorphism;langage programmation;lenguaje java;polymorphisme;polimorfismo;object oriented languages;lenguaje c;java language;generic programming	All object oriented programming languages offer various degrees of support for generic programming. C++ offers parametric polymorphism through templates [12], Eiffel [11] offers parameterized classes, Haskell and ML also offers some form of parametric polymorphism. Till JDK 1.5 (beta), Java had no direct support for parameterized types – the user had to depend on the “generic idiom” [1], where variable types were replaced by their typed upper bounds (typically Object) to provide the façade of genericity, while introducing casts in referring to the specific uses of these types. The collection classes introduced in JDK 1.2, the Observer pattern implementation of JDK 1.1, all provided enough emphasis on the aspects of fostering genericity in the programming model, although direct language support was missing.	c++;eiffel;generic programming;generics in java;haskell;java development kit (jdk);java version history;observer pattern;parametric polymorphism;programming language;programming model;type conversion	Debasish Ghosh	2004	SIGPLAN Notices	10.1145/997140.997144	template;polymorphism;homology modeling;c++;genetic algorithm;computer science;theoretical computer science;c++11;policy-based design;programming language;object-oriented programming;generic programming;java syntax;algorithm;generics in java;typename	PL	-25.690248991444818	26.30209034230663	130592
191b84e8838a43a0705206384b97707d87dddadd	interface grammars for modular software model checking	software model checking;right hand side;specification language;automatic generation;model checking;context free grammar;enterprise javabean;modular verification;finite state machine;interface grammars;production rule	We propose an interface specification language based on grammars for modular software model checking. In our interface specification language, component interfaces are specified as context free grammars. An interface grammar for a component specifies the sequences of method invocations that are allowed by that component. Using interface grammars one can specify nested call sequences that cannot be specified using interface specification formalisms that rely on finite state machines. Moreover, our interface grammars allow specification of semantic predicates and actions, which are Java code segments that can be used to express additional interface constraints. We have built an interface compiler that takes the interface grammar for a component as input and generates a stub for that component. The resulting stub is a table-driven parser generated from the input interface grammar. Invocation of a method within the component becomes the lookahead symbol for the stub/parser. The stub/parser uses a parser stack, the lookahead, and a parse table to guide the parsing. The semantic predicates and semantic actions that appear in the right hand sides of the production rules are executed when they appear at the top of the stack. We conducted a case study by writing an interface grammar for the Enterprise JavaBeans (EJB) persistence interface. Using our interface compiler we automatically generated an EJB stub using the EJB interface grammar. We used the JPF model checker to check EJB clients using this automatically generated EJB stub. Our results show that EJB clients can be verified efficiently using our approach.	application programming interface;code segment;compiler;context-free grammar;decision table;enterprise javabeans;finite-state machine;input device;java;lr parser;model checking;modular programming;parsing;persistence (computer science);specification language	Graham Hughes;Tevfik Bultan	2007	IEEE Transactions on Software Engineering	10.1145/1273463.1273471	model checking;interface description language;specification language;parsing expression grammar;computer science;theoretical computer science;database;finite-state machine;context-free grammar;programming language	SE	-27.576977558193317	27.10987970686626	130645
c4ae78d4668da7044b05aefd0337a02f9138e2d9	zone-based indoor mobile noise monitoring	acoustic noise measurement;indoor radio;mobile handsets;noise (working environment);noise pollution;construction site;environmental data;factory;indoor appliance;indoor noise exposure;industrialization;mobile phone;noise emission level monitoring;outdoor sources;traffic;zone-based indoor mobile noise monitoring;mobile phone;noise monitoring;noise pollution;participatory sensing;sensor	High noise levels are increasing every day. The rapid development of most world economies has encouraged most countries to move towards industrialization. Some of the sources of noise are indoor appliances within our homes and offices. There are also outdoor sources of noise that cause discomfort examples include traffic, construction sites and factories. This paper looks at the new approaches adopted in the monitoring of noise emission levels. It adopts an approach which involved the use of mobile phones to measure and monitor indoor noise exposure in four vital areas. Mobile phones were used to improve the methods of collecting environmental data.	experiment;laptop;mobile phone;noise (electronics);vacuum cleaner;washing machine	Samah Almutlaq;Eiman Kanjo;Lilac Alsafadi	2014	6th International Conference on Mobile Computing, Applications and Services		electronic engineering;environmental engineering;telecommunications;engineering	Mobile	-30.05556205935456	18.96639927641485	130658
e7977737873cd13429f1858587ed8200961bb0ca	reachability under contextual locking	procedure return;procedure call;lock usage pattern;multi-threaded program;polynomial-time decidable;popular programming paradigm;new programming paradigm;pairwise reachability problem;new natural programming paradigm;lock usage	The pairwise reachability problem for a multi-threaded program asks, given control locations in two threads, whether they can be simultaneously reached in an execution of the program. The problem is important for static analysis and is used to detect statements that are concurrently enabled. This problem is in general undecidable even when data is abstracted and when the threads (with recursion) synchronize only using a finite set of locks. Popular programming paradigms that limit the lock usage patterns have been identified under which the pairwise reachability problem becomes decidable. In this paper, we consider a new natural programming paradigm, called contextual locking, which ties the lock usage to calling patterns in each thread: we assume that locks are released in the same context that they were acquired and that every lock acquired by a thread in a procedure call is released before the procedure returns. Our main result is that the pairwise reachability problem is polynomial-time decidable for this new programming paradigm as well.	algorithm;cns;concurrency (computer science);graph coloring;ibm notes;lock (computer science);microsoft customer care framework;model checking;p (complexity);polynomial;programming paradigm;reachability problem;recursion;stack (abstract data type);static program analysis;subroutine;thread (computing);time complexity;two-phase locking;undecidable problem	Rohit Chadha;P. Madhusudan;Mahesh Viswanathan	2012		10.1007/978-3-642-28756-5_30	real-time computing;computer science;distributed computing;programming language;algorithm	PL	-19.88743414189652	30.140657051153458	130886
421256beb06dc21fb64b2ea850466344ac06602b	fate: one step towards an automatic aging people fall detection service	aging people;gyroscopes;fall detector;reliability;pilot;detectors zigbee bluetooth computers mobile handsets batteries aging;health service fate automatic aging fall detection service european union cip ict psp program automatic fall detection indoors conditions outdoors conditions call center sensor subsystem accelerometers gyroscopes reliability zigbee bluetooth protocols mobile phone spain italy ireland;zigbee;zigbee accelerometers access protocols bluetooth gyroscopes mobile radio reliability;mobile radio;conference report;access protocols;accelerometer;bluetooth;accelerometers;long lye syndrome;automatic fall detection;fear or falling;pilot fall detector automatic fall detection fear or falling accelerometer aging people long lye syndrome	FATE is a project funded by the European Union under the program CIP/ICT-PSP with the main objective of organizing a big pilot on the automatic falls detection in aging people living at home. Automatic detection of falls is done in indoors and outdoors conditions, and in both cases the detection generates an alarm sent to a call center. The detection system is designed around a sensor sub-system based on accelerometers and gyroscopes able to detect falls with a high reliability. The complete system is based on a communications layer based in ZigBee and Bluetooth protocols. The gateway for sending the alarm to the call center is a mobile phone. Pilots are organized in three different countries (Spain, Italy and Ireland) where different models of health service and implemented call centers are available. Pilots duration will be one year, involving 175 users and one of the main final objectives is to gain experience with the integration of an automatic fall detection service in an already care/health existing service.	bluetooth;experiment;fate;mobile phone;organizing (structure);real-time transcription;software deployment;usability	Joan Cabestany;Juan Manuel Moreno;Carlos Perez;Albert Samà;Andreu Català;Alejandro Rodríguez-Molinero;Marc Arnal	2013	Proceedings of the 20th International Conference Mixed Design of Integrated Circuits and Systems - MIXDES 2013		embedded system;telecommunications;engineering;computer security	SE	-32.96148720038766	21.31185498530048	131197
3dd48548a016cce9d3ea78c61b6fcb270b998fb7	single-threaded objects in acl2	lenguaje programacion;microprocessor;diseno circuito;programming language;logique mathematique;circuit design;langage declaratif;logica matematica;program verification;mathematical logic;theorem proving;demonstration theoreme;verificacion programa;lisp language;declarative language;langage programmation;conception circuit;microprocesseur;lisp;computer hardware;demostracion teorema;verification programme;microprocesador;materiel informatique;material informatica	"""ACL2 is a rst-order applicative programming language based on Common Lisp. It is also a mathematical logic for which a mechanical theorem-prover has been implemented in the style of the Boyer-Moore theorem prover. The ACL2 system is used primarily in the modeling and veriica-tion of computer hardware and software, where the executability of the language allows models to be used as prototype designs or \simulators."""" To support eecient execution of certain kinds of models, especially models of microprocessors, ACL2 provides \single-threaded objects,"""" structures with the usual \copy on write"""" applicative semantics but for which writes are implemented destructively. Syntactic restrictions insure consistency between the formal semantics and the implementation. The design of single-threaded objects has been innuenced both by the need to make execution eecient and the need to make proofs about them simple. We discuss the issues. 1 Background \ACL2"""" stands for \A Computational Logic for Applicative Common Lisp."""" We use the name both for a mathematical logic based on applicative Common Lisp 24] and for a mechanized theorem proving system for that logic developed by Matt Kaufmann and author Moore. ACL2 is closely related to the Boyer-Moore logic and system and its interactive enhancement 2, 3, 4]. ACL2's primary use is in modeling microprocessors and proving theorems about those models. The key reason we abandoned the Nqthm logic and adopted applicative Common Lisp is that the latter can produce extremely eecient runtime code. Execution eeciency is important because our microprocessor models are often run as simulators."""	acl2;applicative programming language;automated theorem proving;common lisp;computation;computational logic;computer hardware;microprocessor;nqthm;prototype;semantics (computer science);simulation;theorem proving system;thread (computing)	Robert S. Boyer;J. Strother Moore	2002		10.1007/3-540-45587-6_3	computer science;artificial intelligence;theoretical computer science;operating system;lisp;database;mathematics;distributed computing;programming language;object-orientation;algorithm	Logic	-23.56676344480991	29.01931175030539	131221
be030ea2a62ba210a0f8323f6643a07efb4c4078	event-driven programming with logical execution times	circuit declenchement;circuito desenganche;time triggered;compilateur;race condition;execution time;programming language;asynchrone;real time;specification;langage evolue;ejecucion programa;logical programming;compiler;program execution;asynchronisation;sistema reactivo;run time system;asynchronism;especificacion;programmation logique;logical execution time;filter;execution programme;temps reel;reactive system;systeme reactif;filtre;tiempo real;trigger;temps execution;lenguaje evolucionado;tiempo ejecucion;programacion logica;high level language;filtro;asincronia;compilador;asincrono;asynchronous;hard real time	We present a new high-level programming language, called XGIOTTO, for programming applications with hard real-time constraints. Like its predecessor, XGIOTTO is based on the LET (logical execution time) assumption: the programmer specifies when the outputs of a task become available, and the compiler checks if the specification can be implemented on a given platform. However, while the predecessor language GIOTTO was purely time-triggered, XGIOTTO accommodates also asynchronous events. Indeed, through a mechanism called event scoping, events are the main structuring principle of the new language. The XGIOTTO compiler and run-time system implement event scoping through a treebased event filter. The compiler also checks programs for determinism (absence of race conditions).	compiler;event-driven programming;high- and low-level;high-level programming language;programmer;race condition;real-time clock;real-time computing;run time (program lifecycle phase);runtime system;scope (computer science)	Arkadeb Ghosal;Thomas A. Henzinger;Christoph M. Kirsch;Marco A. A. Sanvido	2004		10.1007/978-3-540-24743-2_24	compiler;parallel computing;real-time computing;compiler correctness;filter;reactive system;computer science;operating system;asynchronous communication;database;distributed computing;race condition;programming language;high-level programming language;functional compiler;specification;algorithm	PL	-24.272165515354537	32.15893164806261	131269
1c8fd4ce494520f90e21814be9a370a33853c06d	using php and html languages to create graphical interfaces and to remote control of programs		In this document we describe how to use PHP and HTML languages to create graphical interfaces to any program. The interface is used to send data and parameters to the program from a web page. We can choose different kinds of data and the format of received data. We show an example, in which a graphical interface controls a program written in Fortran 77.	c++;database;download;fortran;graphical user interface;html;php;remote control;web page	Bogdan Ksiezopolski;Pawel Luka	2003	Annales UMCS, Informatica		computer science;database;remote control	HCI	-30.540059703973743	26.17144502582731	131347
50a65be8721ace3c464a99aed9ecb5180ef7c3bd	a fixed-program machine for combinator expression evaluation	code generation;flow analysis	It is our purpose here to present an alternative evaluation mechanism for combinator expressions, namely a relatively straightforward compilation algorithm which translates combinators to fixed-program code for a stack machine. The resulting code is faithful to Turner's use of non-strict functions, in that it performs normal order evaluation. We show how this code can be made more efficient by performing call-by-need evaluation (without changing the semantics of the language) and consider some flow-analysis-based optimizations, as well.  The remainder of this paper is organized as follows: We first review (briefly) Turner's arguments in favor of using combinators and how to translate function definitions to combinator expressions in Section 2. Next, in Section 3, we present our translation of combinator expressions to call-by-name stack machine code and the evaluation of a simple program in our model. In Section 4 we show how to eliminate repeated evaluation of argument expressions (or, equivalently, optimize evaluation of S combinator expressions) by using call by need. In Section 5 we consider flow-analysis-based optimizations which can significantly improve the code generated by the naive algorithms of Sections 3 and 4. Finally, Section 6 presents conclusions and suggestions for further work in this area.	a new kind of science;algorithm;combinatory logic;compiler;lazy evaluation;machine code;stack machine;strict function	Steven S. Muchnick;Neil D. Jones	1982		10.1145/800068.802130	computer science;theoretical computer science;data-flow analysis;programming language;algorithm;code generation	PL	-22.158909098460313	25.511176478400245	131567
d5d8ddba28ea4cf67813b57ac3c35d747784c637	how to keep your neighbours in order	sorting;agda;ordering;binary trees;generic treatment;computer programming languages;balancing;dependent types;electronic computers computer science;learning experiences;containers	I present a datatype-generic treatment of recursive container types whose elements are guaranteed to be stored in increasing order, with the ordering invariant rolled out systematically. Intervals, lists and binary search trees are instances of the generic treatment. On the journey to this treatment, I report a variety of failed experiments and the transferable learning experiences they triggered. I demonstrate that a total element ordering is enough to deliver insertion and flattening algorithms, and show that (with care about the formulation of the types) the implementations remain as usual. Agda's instance arguments and pattern synonyms maximize the proof search done by the typechecker and minimize the appearance of proofs in program text, often eradicating them entirely. Generalizing to indexed recursive container types, invariants such as size and balance can be expressed in addition to ordering. By way of example, I implement insertion and deletion for 2-3 trees, ensuring both order and balance by the discipline of type checking.	agda;algorithm;experiment;invariant (computer science);recursion (computer science);type system	Conor McBride	2014		10.1145/2628136.2628163	dependent type;simulation;binary tree;order theory;computer science;sorting;theoretical computer science;programming language;algorithm	PL	-21.569874255609186	29.51728071834075	131661
2b2d6415f5febdce48b50c980e1b96ef7c590cc5	combining the robustness of checked exceptions with the flexibility of unchecked exceptions using anchored exception declarations	verification;handling mechanisms;ancrage;lenguaje programacion;debugging;instruccion declaracion;puesta a punto programa;adaptability;adaptabilite;reliability;exceptions;anchoring;programming language;semantica formal;information hiding;formal semantics;anchored exception declaration cappuccino;software engineering;adaptabilidad;debogage;semantique formelle;anclaje;object oriented systems;anchored exception declaration;cappuccino;issues;theory;error handling;declaration;traitement exception;genie logiciel;langage programmation;exception handling;static analysis;instruction declaration;programs;ingenieria informatica;languages;traitement erreur;java	Ever since their invention 30 years ago, checked exceptions have been a point of much discussion. On the one hand, they increase the robustness of software by preventing the manifestation of unanticipated checked exceptions at run-time. On the other hand, they decrease the adaptability of software because they must be propagated explicitly, and must often be handled even if they cannot be signalled.We show that the problems with checked exceptions are caused by a lack of expressiveness of the exceptional return type of a method, which currently dictates a copy & paste style. We add the required expressiveness by introducing anchored exception declarations, which allow the exceptional behavior of a method to be declared relative to that of others. We present the formal semantics of anchored exception declarations, along with the necessary rules for ensuring compile-time safety, and give a proof of soundness. We show that anchored exception declarations do not violate the principle of information hiding when used properly, and provide a guideline for when to use them.We have implemented anchored exception declarations in Cappuccino, which is an extension to the ClassicJava programming language.	cappuccino;compile time;compiler;cut, copy, and paste;exception handling;programming language;return type;robustness (computer science);semantics (computer science)	Marko van Dooren;Eric Steegmans	2005		10.1145/1094811.1094847	exception handling;computer science;database;programming language;algorithm	PL	-23.700125219789626	29.27159110716569	131822
e8025038877d756415babc4b046db680e206162d	design and implementation of an application interface for lotos processors	design and implementation	LOTOS has attracted a lot of attention as a suitable language for formal description techniques, and a number of LOTOS processors have been proposed and implemented, that include specification simulators, test case generators, property checkers, and structural editors. Those varieties of LOTOS processors should ideally be implemented based on a common  LOTOS kernel  interfaced with a set of well defined  interface library functions , the combination of which provides sufficiently powerful functionality for LOTOS system in a very flexible development environment. This approach is particularly advantageous because it yields  portable  systems. The purpose of this paper is to report our project of designing such a kernel,  LIpS (LOTOS Interpretation Server) , and the set of interface functions, which we call  service functions  of LIpS. We believe that our kernel together with these service functions can implement virtually all LOTOS processors, and therefore serves as a general purpose LOTOS processor development environment. Moreover, the grammar handled by our kernel contains extensions to standard LOTOS that allows separate compilation and treatment of non-determinism.		Kazuhito Ohmaki;Hirosato Tsuji;Kenjiroh Yamanaka;Yoshikazu Sato;Yoshinori Itabashi;Toshihiko Shimizu	1991		10.1016/B978-0-444-89402-1.50033-3	real-time computing;computer science;theoretical computer science;distributed computing;programming language;algorithm	EDA	-29.584016635087075	29.150704330342204	132212
3ef4202f49824807282509483fb7e44114b0e5f0	a study on the development of a web-based c compiler for c programming lab.		"""iii Contents Preface, v Slide set 1 Basics of machine, software, and program design, S1 Slide set 2 Fundamentals of C++, S16 Slide set 3 Modifying objects, S32 Slide set 4 If constructs, S55 Slide set 5 Iterative constructs, S69 Slide set 6 Libraries, S103 Slide set 7 Programmer-defined functions, S114 Slide set 8 Class construct, S139 Slide set 9 Abstract data types, S148 Slide set 10 Arrays, S171 Slide set 11 Vectors, S189 Slide set 12 EzWindows API, S213 iv Slide set 13 Pointers and dynamic objects, S226 Slide set 14 Inheritance, S248 Slide set 15 Templates and polymorphism, S265 Laboratory 1 Riding the wave of the future, 1 Laboratory 2 Attacking your first problem, 9 Laboratory 3 Inquiring minds want to know about the if statement, 17 Laboratory 4 Let's go looping now, everybody is learning how, 25 Laboratory 5 Taking a trip to the library, 37 Laboratory 6 Pass it on, 43 Laboratory 7 Functional living, 57 Laboratory 8 Getting classy, 69 Laboratory 9 Now that's classy, 77 Laboratory 10 EzWindows and event-based programming, 83 Laboratory 11 v Preface INTRODUCTION This revised laboratory manual and accompanying lecture slides are designed to be used in conjunction with a C++ introductory text providing broad integrated coverage of the ANSI C++ standard and its Standard Template Library (STL). In particular, the manual is a companion to the text C++ Program Design: An Introduction to Programming and Object-Oriented Design 3rd Edition. We have also revised our EzWindows Graphical API in response to user feedback. Unlike most laboratory manuals that are designed to be self-study aids for mastering syntax or to have students perform straightforward, self-guided activities in """" open """" laboratories at a time and place of the students' choosing, this laboratory manual is designed for use in a """" closed """" laboratory. A closed laboratory meets at an assigned time and place with a laboratory instructor, and, depending upon class size, laboratory assistants. Each closed laboratory activity typically illustrates concepts from lecture by using examples, implementations , and problems that are designed to challenge the student. A closed laboratory environment provides a student with the opportunity to try various options or approaches and to receive immediate feedback. Similarly, when mistakes are made or clarification is needed, help is immediately available. We have used closed laboratories in our introductory computer science course for the past nine years, and our department has …"""	abstract data type;application programming interface;bus mastering;c++;compiler;computer science;conditional (computer programming);event-driven programming;graphical user interface;pointer (computer programming);programmer;standard template library	KwanSun Choi;HeungGu Juan;Dongsik Kim;Sunheum Lee;Yong-Hae Kong	2003			computer architecture;compiler;parallel computing;bootstrapping;programming language	PL	-30.327408486817745	25.474738393938146	132220
93925872faf003b335386326f659d6c1fe32e552	cdl++ for the description of moving objects in cellular automata	simulation ordinateur;moving object;programacion paralela;program transformation;parallel programming;transformation programme;performance programme;transformacion programa;automate cellulaire;eficacia programa;simulacion computadora;program performance;cellular automata;conflict resolution;computer simulation;cellular automaton;programmation parallele;automata celular	We introduce a new model for objects which can move around on a cellular grid. The model consists of two phases, the movement phase and the con ict resolution phase. In the movement part of the description objects specify their desired direction. The con ict, which occurs when alternative objects want to move to the same free cell, is resolved in the con ict resolution part. The cellular description language CDL was extended to CDL++ in order to describe moving objects. This extension is automatically converted into a two{phased CDL program.	cellular automaton;compiler description language;freecell;naruto shippuden: clash of ninja revolution 3	Christian Hochberger;Rolf Hoffmann;Stefan Waldschmidt	1999		10.1007/3-540-48387-X_44	cellular automaton;simulation;computer science;artificial intelligence;algorithm	ML	-25.02795189509972	30.577259464587783	132291
b5203c4b78be1c6468b73fa45803c62f39ba3a3d	challenges of drive-by iot sensing for smart cities: city scanner case study		Fixed sensor stations are the primary means to collect environmental data in cities. Yet, their high cost of deployment and maintenance often result in an accurate but geographically-sparse monitoring. We advocate for a drive-by approach to urban sensing in which vehicles are used as sensors to scan the city with a high spatiotemporal resolution. We present City Scanner, a highly-customizable, self-sufficient platform that allows for cost-efficient drive-by sensing. City Scanner modules can be deployed on existing vehicles (e.g. busses and taxis) without interfering with their operations. We describe our first prototype that includes sensing modules for air quality, temperature, humidity, and thermal imaging. We discuss the challenges we encountered during an 8-months deployment of the platform on trash trucks in order to derive implications for the design of future drive-by sensing systems.	cost efficiency;prototype;sensor;smart city;software deployment;sparse matrix	Amin Andjomshoaa;Simone Mora;Philip Schmitt;Carlo Ratti	2018		10.1145/3267305.3274167	multimedia;smart city;environmental monitoring;real-time computing;software deployment;truck;scanner;environmental data;air quality index;computer science;taxis	HCI	-30.16549965006246	18.65112087911592	132481
f36811b9ac263e3915c09e9028ef66ddd8a5ca5c	po: an object model to express parallelism	object model		parallel computing	Antonio Corradi;Letizia Leonardi	1989	SIGPLAN Notices	10.1145/67387.67429	computer vision;object model;computer science;theoretical computer science;programming language	PL	-24.425934854090876	22.527921335546253	132801
c16a96a017284e3e74b1ffbb114d1ff8112f5632	threads as resource for concurrency verification	separation logic;threads as resource;first class threads;concurrency verification	"""In mainstream languages, threads are first-class in that they can be dynamically created, stored in data structures, passed as parameters, and returned from procedures. However, existing verification systems support reasoning about threads in a restricted way: threads are often represented by unique tokens that can neither be split nor shared. In this paper, we propose """"threads as resource"""" to enable more expressive treatment of first-class threads. Our approach allows the ownership of a thread (and its resource) to be flexibly split, combined, and (partially) transferred across procedure and thread boundaries. We illustrate the utility of our approach in handling three problems. First, we use """"threads as resource"""" to verify the multi-join pattern, i.e. threads can be shared among concurrent threads and joined multiple times in different threads. Second, using inductive predicates, we show how our approach naturally captures the threadpool idiom where threads are stored in data structures. Lastly, we present how thread liveness can be precisely tracked. To demonstrate the feasibility of our approach, we implemented it in a tool, called ThreadHIP, on top of an existing ParaHIP verifier. Experimental results show that ThreadHIP is more expressive than ParaHIP while achieving comparable verification performance."""	correctness (computer science);data structure;inductive reasoning;join-calculus;liveness;race condition;thread (computing)	Duy-Khanh Le;Wei-Ngan Chin;Yong Meng Teo	2015		10.1145/2678015.2682540	monitor;thread;parallel computing;real-time computing;separation logic;gang scheduling;java concurrency;computer science;distributed computing;green threads;readers–writers problem;concurrent data structure;programming language;process	PL	-20.452250432758646	30.971574508256282	133146
c59e074f9a34468ce36a1beee586e5ec10306d00	asynchronous communication model based on linear logic	modelizacion;regle inference;quantization;cuantificacion;phase semantics;asynchronous ccs;operational semantics;semantics;concurrent program;transmission message;natural extension;quantification;semantica;semantique;partage ressource;message transmission;informacion;inference rule;modelisation;computer programming languages;logic programming;programmation logique;mathematical programming;asynchronous communication;logique ordre 1;resource sharing;programa competidor;particion recursos;asynchronous π calculus;linear programming;programmation lineaire;transmission asynchrone;linear logic programming;modeling;linear logic;programmation mathematique;first order logic;information;asynchronous transfer mode;programme concurrent;regla inferencia;transmision mensaje;logica orden 1;langage programmation ordinateur;asynchronous concurrent computation	We propose a new framework called ACL for concurrent computation based on linear logic. ACL is a kind oflinear logic programming framework, where its operational semantics is described in terms ofproof construction in linear logic. We also give a model-theoretic semantics based onphase semantics, a model of linear logic. Our framework well captures concurrent computation based on asynchronous communication. It will, therefore, provide us with a new insight into other models of asynchronous concurrent computation from alogical point of view. We also expect ACL to become a formal framework for analysis, synthesis and transformation of concurrent programs by the use of techniques for traditional logic programming. ACL's attractive features for concurrent programming paradigms are also discussed.	computation;concurrency (computer science);concurrent computing;game semantics;linear logic;logic programming;operational semantics;programming paradigm;theory	Naoki Kobayashi;Akinori Yonezawa	1995	Formal Aspects of Computing	10.1007/BF01211602	concurrent constraint logic programming;shared resource;linear logic;systems modeling;information;horn clause;quantization;computer science;linear programming;theoretical computer science;asynchronous communication;asynchronous transfer mode;first-order logic;computational logic;distributed computing;semantics;programming language;well-founded semantics;logic programming;operational semantics;multimodal logic;algorithm;rule of inference;temporal logic of actions	PL	-25.03350435516801	31.718811978950086	133187
2995f2e3ef7c21178687561727a3b21ba07b2516	mark, a reasoning kit for mobility	formal methods;mobile computer;formal method;theorem proving;mobile computing	The experience gained to date in the development of network applications has shown the difficulties of using traditional software technologies: reasoning about network applications is subtly different from reasoning about ordinary programs because of stronger requirements on security, different forms of termination, and phenomena like mobility and network-awareness. There are currently no standard methods, techniques and tools to support specification, development and (property) certification of these applications. To support property certification of network applications, we propose to use the network-aware logic Mobadtl and its proof assistant, Mark (Mobadtl Reasoning Kit). In the paper we present the prototype implementation of Mark and, as a validating example, we consider applications where mobile components are allowed to carry some resources with them when moving around the network.	automated theorem proving;graphical user interface;interaction;isabelle;proof assistant;prototype verification system;requirement;vertex-transitive graph	Gian Luigi Ferrari;Carlo Montangero;Laura Semini;Simone Semprini	2002	Automated Software Engineering	10.1023/A:1014530313153	formal methods;computer science;engineering;theoretical computer science;operating system;software engineering;automated theorem proving;programming language;mobile computing;algorithm	SE	-32.309013825713315	29.86063727841584	133249
3fa4b267b16d5b1c33663e98203234162a694dee	early experience with recursion optimization in an extensible rewriter			recursion	Jerome Fessy;Béatrice Finance	1993			theoretical computer science;computer science;recursion;extensibility	PL	-23.42294105871768	22.341657684869507	133293
11a08a7a0442bfdb8f3f50f0b18096287b2daca3	structural recursion as a query language	query language;relation algebra;programming language;programming paradigm;expressive power;typed lambda calculus	"""We propose a programming paradigm that tries to get close to both the semantic simplicity of relational algebra, and the expressive power of unrestricted programming languages. Its main computational engine is structural recursion on sets. All programming is done within a """"nicely"""" typed lambda calculus, as in Machiavelli [OBB89]. A guiding principle is that how queries are implemented is as important as whether they can be implemented. As in relational algebra, the meaning of any relation transformer is guaranteed to be a total map taking finite relations to finite relations. A naturally restricted class of programs written with structural recursion has precisely the expressive power of the relational algebra. The same programming paradigm scales up, yielding query languages for the complex-object model [AB89]. Beyond that, there are, for example, efficient programs for transitive closure and we are also able to write programs that move out of sets, and then perhaps back to sets, as long as we stay within a (quite flexible) type system. The uniform paradigm of the language suggests positive expectations for the optimization problem. In fact, structural recursion yields finer grain programming. Therefore we expect that lower-level and therefore better optimizations will be feasible. Comments University of Pennsylvania Department of Computer and Information Science Technical Report No. MSCIS-92-17. This technical report is available at ScholarlyCommons: http://repository.upenn.edu/cis_reports/403 Structural Recursion As A Query Language MS-CIS-92-17 LOGIC & COMPUTATION 46 Val Breazu-Tannen Peter Buneman Shamim Naqvi University of Pennsylvania School of Engineering and Applied Science Computer and Information Science Department Philadelphia, PA 19104-6389"""	expect;information and computer science;information retrieval;information science;mathematical optimization;optimization problem;programming language;programming paradigm;query language;recursion;relational algebra;structural induction;transformer;transitive closure;type system;typed lambda calculus	Val Tannen;Peter Buneman;Shamim A. Naqvi	1991			fourth-generation programming language;typed lambda calculus;very high-level programming language;codd's theorem;programming domain;reactive programming;computer science;theoretical computer science;functional logic programming;relation algebra;database;programming paradigm;procedural programming;symbolic programming;inductive programming;fifth-generation programming language;programming language theory;programming language;expressive power;algorithm;query language	DB	-20.492723958657088	22.35819621180659	133297
5bfd5f05cf47fb44db4115b4d6682869f58cf41e	a programming language for simulating digital systems	programming language;digital systems	"""Systems 7'c2""""~s [~z.*Detmc~[s [ncorporated~ l)al~c~s, """"l'c.ras .iS.*D'acg. One of the major pr¢fl)lems iu the design of any but the simplest of digitM systems is debugging the logic (as opposed to noise problems, defective circuits, mistakes in wiring, etc.). To aid ia this t~sk, a compiler has bee~/written to translate the logieM design equations into a program (for the CDC 1604 computer) which t)erforms a bit-wise simulation of the system. In addition to the design eqm~tions, special commands are compiled which allows the simulated system to be exercised so that errors iu the logic equations may be eorreeted before committing the design to hardware. The debugging off a newly born digital system is tedious, time-cousuming, and usually very frustrating. The reason for this is that the multitudinous ills which befall digital systems all may produce similar symptoms. It is not easy, to be specific, to determine whether failure to operate correctly is due 1:o basic logical-design errors, trivial mistakes in the equations, mistakes in wiring, un-war,ted noise, defective circuits or components, or perhaps an occasional cold solder joint. Any experieneed digital engineer or logic desig~,_er can easily extend ~his list. This paper reports on an effort to accomplish ~he debugging of the logic design independently of the other potential difficulties by simulating the logic design on a digital computer. This is, of course, not a new idea [1, 2, 3]. In all cas(.s known to the author simulators were devised for a particular problem at hand. For these considerable effort is involved both in preparing the logic equa-t:io~s, due to restriction on the forms which they may take, and in preparing the data to initialize the simulation. The goal of the work reported here was to provide a general-purpose tool as an aid to logic designers and engineers working on a mm~ber of projects, both large and small. ()I~e of the major stinmli to this work is that in military systems work, an important portion of the work of Texas Instruments, there is continuing pressure to shorten the design cycle. This, plus the predominance of one-and two-of-a-kind items, indicates the necessity to provide all possible assistance for the design engineer. The major guidelines to this effort were the followir~g: (t) i\lemory elements (flipflops) were restricted to strictly sytmhronous (clocktd) types such as J-K, R-S, T and D, (2) Devices were idealized, i.e. fan-in, fan-out, delay and transilion times were ig~tored. (:-;) …"""	apl;cdc 1604;compiler;computer;debugging;digital electronics;fan-in;fan-out;general-purpose modeling;programming language;simulation;soldering;wiring	Robert M. McClure	1965	J. ACM	10.1145/321250.321252	fourth-generation programming language;first-generation programming language;natural language programming;computer architecture;very high-level programming language;language primitive;specification language;programming domain;reactive programming;computer science;programming language implementation;extensible programming;functional logic programming;programming paradigm;symbolic programming;low-level programming language;inductive programming;fifth-generation programming language;programming language theory;programming language;programming language specification;high-level programming language	EDA	-30.817339233594325	22.611350239478806	133365
805cd84ebbad5b6ab90eb8240796e983c673348a	a practical soft type system for scheme	dynamic typing;recursive types;soft typing;program optimization;type inference;data structure;type system;run time checks	A soft type system infers types for the procedures and data structures of dynamically typed programs. Like conventional static types, soft types express program invariants and thereby provide valuable information for program optimization and debugging. A soft type checker uses the types inferred by a soft type system to eliminate run-time checks that are provably unnecessary; any remaining run-time checks are flagged as potential program errors. Soft Scheme is a practical soft type checker for R4RS Scheme. Its underlying type system generalizes conventional Hindley-Milner type inference by incorporating recursive types and a limited form of union type. Soft Scheme accommodates all of R4RS Scheme including uncurried procedures of fixed and variable arity, assignment, and continuations.	continuation;data structure;debugging;hindley–milner type system;mathematical optimization;program optimization;recursion;scheme;type inference	Andrew K. Wright;Robert Cartwright	1997	ACM Trans. Program. Lang. Syst.	10.1145/239912.239917	real-time computing;type system;data structure;computer science;theoretical computer science;programming language;algorithm	PL	-21.02751210564812	29.163754490486347	133369
a366d16ff133a79738a87fa188f9298d257f79de	teaching recursion as a problem-solving tool using standard ml	first year;standard ml;polymorphism;data structure;functional programming language;problem solving	Standard ML is a state of the art functional programming language, with features that make it excellent for teaching recursion and problem solving at the introductory level. Among the many pedagogically interesting characteristics of ML are its simple and uniform syntax, its type polymorphism and type inferencing system, and datatype declaration facilities. With little formal ML instruction, after several weeks first year students were able to use recursively defined data structures and to define fairly powerful recursive functions in ML. Standard ML is highly recommended as a tool for teaching recursive problem solving in the context of a course on the foundations of computer science.	computer science;data structure;declaration (computer programming);functional programming;problem solving;programming language;recursion;recursive definition;standard ml	Peter B. Henderson;Francisco Javier Romero	1989		10.1145/65293.71190	polymorphism;data structure;computer science;mutual recursion;programming language;algorithm	PL	-26.22688349002208	25.079932117361093	133495
29bb6eba872a36aa59bce1d108b4db79e9fc01a7	compact recursive-descent parsing of exptressions	compilateur;compiler;compilers;expression parsing;analizador sintaxico;recursive descent parsing;parser;analyseur syntaxique;precedence;compilador	Compiler writing tools, such as parser generators, are.commonly used to build new operators. Nevertheless, constructing compilers by hand remains common. Such compilers are often built using recursive-descent parsing for most of the lwguage and operator-precedence parsing for expressions. This paper describes a simple technique for parsing expressions using recursive descent that avoids the usual proliferation of procedures that occurs when recursive descent is used to parse expressions. By taking advantage of the similarity of the productions describing expressions in most languages, the n + 1 procedures usually required to parse expressions with n precedence levels can be replaced with a table and two procedures.	compiler;expression (computer science);order of operations;parsing;recursion;recursive descent parser	David R. Hanson	1985	Softw., Pract. Exper.	10.1002/spe.4380151206	natural language processing;parser combinator;compiler;memoization;parsing expression grammar;top-down parsing language;computer science;bottom-up parsing;theoretical computer science;parsing;s-attributed grammar;programming language;top-down parsing	PL	-24.717176213287082	24.26964930689264	133533
27a7a125fc04a94470277eb195ff813b8e3260af	ownership confinement ensures representation independence for object-oriented programs	encapsulation;langage imperatif;methode recursive;visibilite;naming;visibilidad;recursive types;tecnologia electronica telecomunicaciones;computacion informatica;object oriented language;analyse statique;relational parametricity;simulation;imperative language;metodo recursivo;grupo de excelencia;recursive method;abstraction;encapsulacion;intelligence artificielle;object oriented programming;data refinement;program verification;satisfiability;abstraccion;analisis estatica;confinement;refinement method;verificacion programa;marcador;visibility;alias control;pointer;object oriented;ciencias basicas y experimentales;denomination;data abstraction;denominacion;confinamiento;pointeur;oriente objet;artificial intelligence;inteligencia artificial;lenguaje imperativo;methode raffinement;tecnologias;static analysis;verification programme;orientado objeto;metodo afinamiento	Representation independence formally characterizes the encapsulation provided by language constructs for data abstraction and justifies reasoning by simulation. Representation independence has been shown for a variety of languages and constructs but not for shared references to mutable state; indeed it fails in general for such languages. This article formulates representation independence for classes, in an imperative, object-oriented language with pointers, subclassing and dynamic dispatch, class oriented visibility control, recursive types and methods, and a simple form of module. An instance of a class is considered to implement an abstraction using private fields and so-called representation objects. Encapsulation of representation objects is expressed by a restriction, called confinement, on aliasing. Representation independence is proved for programs satisfying the confinement condition. A static analysis is given for confinement that accepts common designs such as the observer and factory patterns. The formalization takes into account not only the usual interface between a client and a class that provides an abstraction but also the interface (often called “protected”) between the class and its subclasses.	abstraction (software engineering);aliasing;client (computing);dynamic dispatch;immutable object;imperative programming;instance (computer science);pointer (computer programming);recursion;simulation;static program analysis	Anindya Banerjee;David A. Naumann	2005	J. ACM	10.1145/1101821.1101824	computer science;artificial intelligence;theoretical computer science;mathematics;programming language;object-oriented programming;algorithm	PL	-23.72175481103443	29.165460030101343	133545
7bb06c9f9ff283047123db70e7ab38183de39f62	cogent: certified compilation for a functional systems language.		We present a self-certifying compiler for the Cogent systems language. Cogent is a restricted, polymorphic, higher-order, and purely functional language with linear types and without the need for a trusted runtime or garbage collector. It compiles to efficient C code that is designed to interoperate with existing C functions. The language is suited for layered systems code with minimal sharing such as file systems or network protocol control code. For a well-typed Cogent program, the compiler produces C code, a high-level shallow embedding of its semantics in Isabelle/HOL, and a proof that the C code correctly implements this embedding. The aim is for proof engineers to reason about the full semantics of real-world systems code productively and equationally, while retaining the interoperability and leanness of C. We describe the formal verification stages of the compiler, which include automated formal refinement calculi, a switch from imperative update semantics to functional value semantics formally justified by the linear type system, and a number of standard compiler phases such as type checking and monomorphisation. The compiler certificate is a series of language-level meta proofs and per-program translation validation phases, combined into one coherent top-level theorem in Isabelle/HOL.	automated theorem proving;benchmark (computing);c standard library;coherence (physics);communications protocol;compile time;compiler;correctness (computer science);foreign function interface;formal proof;formal verification;garbage collection (computer science);hol (proof assistant);high- and low-level;higher-order programming;imperative programming;in-place algorithm;interoperability;isabelle;iterator;memory footprint;memory management;proof assistant;purely functional programming;refinement (computing);refinement calculus;requirement;software bug;substructural type system;tree-meta;type safety;value semantics;world-system	Liam O'Connor;Christine Rizkallah;Zilin Chen;Sidney Amani;Japheth Lim;Yutaka Nagashima;Thomas Sewell;Alex Hixon;Gabriele Keller;Toby C. Murray;Gerwin Klein	2016	CoRR		theoretical computer science;programming language;compiler;functional compiler;code generation;compiler correctness;hol;computer science;formal verification;compiler construction;object code	PL	-21.337143121717702	27.569727603765408	133571
556ee6be99dc5449996947f0f0c027e9877fa62a	nlomj - natural language object modal in java	object oriented programming oop;information retrieval;natural language object model in java nlomj;object oriented programming;natural language;natural language processing nlp;natural language processing;object model	In this paper we present NLOMJ—a natural language object model in Java with English as the experiment language. This model describes the grammar elements of any permissible expression in a natural language and their complicated relations with each other with the conc ept “Object” in OOP (Object Oriented Programming). Directly mapped to t he syntax and semantics of the natural language, it can be used in informat ion retrieval as a linguistic method. Around the UML diagram of the NLOMJ the imp ortant classes (Sentence, Clause and Phrase) and their sub classes re introduced and their syntactic and semantic meanings are explained.	diagram;java;natural language;unified modeling language	Jiyou Jia	2004		10.1007/0-387-23152-8_26	natural language processing;language identification;interface description language;first-generation programming language;natural language programming;method;very high-level programming language;universal networking language;language primitive;question answering;object-based language;object model;object language;data control language;computer science;object;linguistics;low-level programming language;natural language;programming language;object-oriented programming;high-level programming language;object constraint language;object definition language	AI	-25.99108013637503	19.67169238051048	133633
0b56e1dab4e5c7bf9981e200d8e0df4a3a0a17cc	generic programming of reusable, high performance container types using automatic type hierarchy inference and bidirectional antichain typing	programming language;efficient implementation;object oriented approach;type inference;high performance;data structure;type system;generic programming	We introduce a new compile–time notion of type subsumption based on type simulation. We show how to apply this static subsumption relation to support a more intuitive, object oriented approach to generic programming of reusable, high performance container types. As a first step towards an efficient implementation of the resulting type system in a compiler we present a novel algorithm for bidirectional type inference over arbitrary syntax graphs. The algorithm uses the new static type subsumption relation to compress the data that has to be stored for each node in the typeflow graph. During typeflow analysis this means that the set of types for a given node can be symbolically represented using antichains instead of using bitvectors or some other explicit set representation. This results in a typing algorithm that is both flexible and precise and shows good performance on representative instances.	compiler;generic programming;genetic algorithm;is-a;simulation;subsumption architecture;type inference;type system;typing	Wouter Kuijper;Michael Weber	2011	CoRR		type conversion;type system;unit type;data structure;data type;computer science;recursive data type;theoretical computer science;algebraic data type;type inference;programming language;generic programming;algorithm	PL	-22.73132724373067	24.290891865120486	133751
3288b181af3f5d9dce76ed1e5950705f692bec75	discrete event simulation on mini- and microcomputers: some experiments with the pascal language	natural language;simulation study;technical report;discrete event simulation	Currently, mini- and microcomputers are finding application in many areas of business and scientific work. However, the use of small computers for simulation studies is virtually nonexistent. In this paper, we discuss the language requirements for discrete event simulation and present the features of the Pascal language that make it a natural language to use for writing simulation programs. Finally, we discuss our experiences in developing several nontrivial simulations using Pascal.	computer;experiment;microcomputer;natural language;pascal;requirement;scientific literature;simulation	Andrew F. Seila;Der-Fa Robert Chen	1981			simulation;computer science;technical report;theoretical computer science;discrete event simulation;natural language;world wide web;simulation language	AI	-28.689151357501338	23.53793966242124	134128
aa69e40b81d280de2d3fafc692b419386d508bcc	party parameterized synthesis of token rings	linear temporal logic;parameterized specification;parameterized synthesis problem;bounded synthesis;arbitratry number;token ring;flexible method;state-of-the-art synthesis tool;current version;single process;parameterized synthesis	Synthesis is the process of automatically constructing an implementation from a specification. In parameterized synthesis, we construct a single process such that the distributed system consisting of an arbitratry number of copies of the process satisfies a parameterized specification. In this paper, we present Party, a tool for parameterized synthesis from specifications in indexed linear temporal logic. Our approach extends SMT-based bounded synthesis, a flexible method for distributed synthesis, to parameterized specifications. In the current version, Party can be used to solve the parameterized synthesis problem for token-ring architectures. The tool can also synthesize monolithic systems, for which we provide a comparison to other state-of-the-art synthesis tools.	distributed computing;linear temporal logic;logic synthesis;speech synthesis;token ring	Ayrat Khalimov;Swen Jacobs;Roderick Bloem	2013		10.1007/978-3-642-39799-8_66	real-time computing;computer science;theoretical computer science;distributed computing;programming language;algorithm	Logic	-33.35374383239042	31.61019767074836	134140
6f3e058012cf80fe0b565267d7b2b11a9102aa35	what every engineer should know about microcomputer systems and debugging: bill wray and bill crawfordmarcel dekker, new york, usa (1984) $28 pp 183		As microprocessor-based control systems find wider application it becomes necessary to have suitable textbooks available for use by designers and maintenance technicians who are already familiar with electromechanical controllers and who wish to familiarize themselves with the new technology. The authors have aimed their book at readers who are primarily nonelectronic engineers and have accordingly written a lengthy introduction which initiates them into the fundamental terminology of the subject. Unnecessary computer jargon has been avoided but, when appropriate, terms which enjoy wide usage have been used and these are defined in a 20-page glossary which ranges from ABORT to WRITE. This includes many commonplace words such as handshake, interrupt and stack that have acquired their own specialized meaning within the context of microprocessors. Basic information on TTL logic levels, binary and hexadecimal counting systems, bytes and other fundamentals are included. In the same way basic building blocks, or 'chips', such as PIAs and ACIAs, RAMs and ROMs, are briefly outlined so that when they are encountered later, in more detail, they are recognized as old friends. System design fundamentals are considered step by step and design decisions are examined in detail, with flowcharts to guide readers through the maze of options open to them. In particular, guidance is given in avoiding the pitfall for the unwary which the authors call a 'disaster sandwich'. This is the intractable problem which results from trying to run untested software on untested hardware to control a newly designed interface. The solution offered is to eliminate hardware problems by buying a single-board computer, and although this may seem to be begging the question it is sound commercial advice when only a limited number of boards will be required. As a bonus a very useful description is given of the Motorola M68MM17 microcomputer module which incorporates all the facilities needed for most control applications. Throughout the book reference is made to Motorola products, which is to be expected, since both authors work for Motorola Microsystems. It also has the advantage that specific pieces of hardware are discussed, rather than purely abstract black boxes, the designs being centred around the 6809, 6821,6840 and 6850 family. While reading about these devices the reader is frequently referred to the manufacturer's data sheets and it would have been a valuable addition if appropriate abstracts from these had been included, particularly when these readers are unlikely to have ready access to such information. People whose previous experience has been confined to BASIC on home computers will find chapter 5, 'Software development and debugging', a very informative introduction to programming in machine code and assembler. Debugging these programs requires different techniques from those written in high-level languages, and special strategies are needed to locate errors in program operation. As an aid to this process the EXORciser is introduced. This is the Motorola microcomputer development system appropriate for the 6809 family, and it facilitates both system design and verification. By the time appendix A, 'System software flowcharts', is reached the design of a boiler control system for a chemical plant has been examined, block by block. This appendix details by means of flowcharts the operation of the control software. Many useful techniques are illustrated which may be applied to readers' own designs. Appendix B, entitled 'Closedloop systems stability and dynamics', alerts the inexperienced designer to the problems of instability which he/she will encounter and describes methods for overcoming them. Finally the book is completed with a usefully detailed index which enables ready reference to particular topics. Although the stated aim of this book is to update plant engineers it succeeds in presenting complex material in a simple, easy-to-read manner. This should make it appeal to a much wider audience and it is anticipated that it will find its way onto the bookshelves of technology teachers and homecomputer enthusiasts who wish to communicate with experimental control systems.	assembly language;basic;bitwise operation;black box;byte;computational complexity theory;control system;datasheet;debugging;experience;flowchart;glossary;handbook;hexadecimal;high- and low-level;high-level programming language;information;instability;interrupt;jargon;logic level;machine code;microcomputer;microprocessor;motorola 68000;read-only memory;single-board computer;software development;systems design;transistor–transistor logic	J. A. Hardcastle	1985	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/0141-9331(85)90218-2	computer science		-31.297447138165644	22.053148955033155	134361
a50d8f89c3430eb98e8b13065356fd2beecc2c70	visual modeling for parallel programming based on dsl	model verification;dsl;dsl parallel programming visual modeling code generation;computer model;code generation;parallel programming;visualization computational modeling dsl parallel programming load modeling algorithm design and analysis;code generator parallel programming dsl parallel application visual modeling graphical user interface source code framework graphical construction blocks composition rules model checker model verification;visualization;computational modeling;formal verification;graphical user interfaces;visual modeling;graphical model;graphic user interface;source code;parallel programs;load modeling;parallel programming formal verification graphical user interfaces;algorithm design;algorithm design and analysis;parallel applications	Parallel Application Visual Modeling (PAVM) is a system that simplifies the development of parallel applications by providing a graphical user interface for visually modeling and generating corresponding source code framework according to the constructed model. The specification of graphical construction blocks and composition rules are proposed to help construct feasible visual models. Model checker is designed to conduct model verification, and code generator is implemented to translate models to source code framework. PAVM is implemented based on DSL tools. With the help of PAVM, programmers are able to focus on algorithm design and obtain source code framework from graphical models.	digital subscriber line;parallel computing;visual modeling	Mengmeng Wang;Ce Yu;Jizhou Sun;Chao Sun;Jinyan Chen	2011		10.1109/CloudCom.2011.96	computer simulation;algorithm design;computer architecture;computer science;theoretical computer science;graphical user interface;programming language	HPC	-32.06890934213811	24.174846736916304	134447
812c5ea8b7c3dbce3b5cfd0ecdc075a1d1e30d4a	on the completeness of a test suite reduction strategy			test suite	Tsong Yueh Chen;Man Fai Lau	1999	Comput. J.	10.1093/comjnl/42.5.430	theoretical computer science;completeness (statistics);test suite;computer science;reduction strategy	Logic	-19.476524224761157	20.123289351106393	134584
27db44a4b29970d33e3b3a5d0e00c248e96255c9	extending interactive data language with higher-order functions	dynamic typing;array processing;interaction analysis;image processing;scientific data;functional programming;idl;lines of code;language extension;higher order functions;interactive data language	The interactive data language (IDL) is a dynamically typed array processing language widely used for the analysis of images and other scientific data. It operates in two basic modes. The first is a command line mode for interactive analysis and visualization of scientific data. The second is as a development platform for end-user applications which process scientific data. This paper details the introduction of higher-order functions to the core IDL. The purpose of these constructs is to increase the productivity of the interactive IDL user. Historically, interactive users of IDL have been scientists and engineers engaged in the exploration of new data. The addition of functional constructs aids these users by allowing them to accomplish, in a few lines of code, what might otherwise require writing a custom function which is then compiled and used. The constructs described were implemented as C language extensions to core IDL. The IDL extensions themselves are available for download at http://www.ittvis.com/idl/hof/ .	apl;command-line interface;compiler;download;higher-order function;idl (interactive data language);source lines of code;type system	Ronald T. Kneusel	2008	Innovations in Systems and Software Engineering	10.1007/s11334-008-0068-x	embedded system;interface description language;type system;web idl;image processing;computer science;theoretical computer science;operating system;database;programming language;functional programming;higher-order function;source lines of code;data	PL	-28.844102521128338	26.884053528712844	134775
d826cd21476bc5a3bf1da7678a390a884ee3ad60	a contextual equivalence checker for imj ∗	qa76 electronic computers computer science computer software	We present CONEQCT: a contextual equivalence checking tool for terms of IMJ∗, a fragment of Interface Middleweight Java for which the problem is decidable. Given two, possibly open (containing free identifiers), terms of the language, the contextual equivalence problem asks if the terms can be distinguished by any possible IMJ context. Although there has been a lot of prior work describing methods for constructing proofs of equivalence by hand, ours is the first tool to decide equivalences for a non-trivial, object-oriented language, completely automatically. This is achieved by reducing the equivalence problem to the emptiness problem for fresh-register pushdown automata. An evaluation demonstrates that our tool works well on examples taken from the literature. A dedicated webpage for the tool is: http://bitbucket.org/sjr/coneqct.	automata theory;bitbucket;formal equivalence checking;identifier;java;observational equivalence;pushdown automaton;stack (abstract data type);turing completeness;web page	Andrzej S. Murawski;Steven J. Ramsay;Nikos Tzevelekos	2015		10.1007/978-3-319-24953-7_19	logical equivalence;discrete mathematics;computer science;theoretical computer science;mathematics;programming language;algorithm	Logic	-19.990414892324345	21.16762110318578	134874
5629c040d930a1cee4db624b423c0c692ff92204	solving the ttc fixml case with funnyqt		FunnyQT is a model querying and model transformation library for the functional Lisp-dialect Clojure providing a rich and efficient querying and transformation API. This paper describes the FunnyQT solution to the TTC 2014 FIXML transformation case. It solves the core task of generating Java, C#, C++, and C code for a given FIXML message. It also solves the extension tasks of determining reasonable types for the fields of classes.	application programming interface;c++;clojure;financial information exchange;java;lisp;model transformation	Tassilo Horn	2014			programming language;model transformation;computer science;clojure;java	PL	-27.41936906531867	28.01308252392807	135138
264e1837d04c5a7f0394ef3ff3616d27cf1d2b7d	flexible anonymization for privacy preserving data publishing: a systematic search based approach	optimization problem;empirical evidence	k-anonymity is a popular measure of privacy for data publishing: It measures the risk of identity-disclosure of individuals whose personal information are released in the form of published data for statistical analysis and data mining purposes(e.g. census data). Higher values of k denote higher level of privacy (smaller risk of disclosure). Existing techniques to achieve k-anonymity use a variety of “generalization” and “suppression” of cell values for multi-attribute data. At the same time, the released data needs to be as “information-rich” as possible to maximize its utility. Information loss becomes an even greater concern as more stringent privacy constraints are imposed [4]. The resulting optimization problems have proven to be computationally intensive for data sets with large attribute-domains. In this paper, we develop a systematic enumeration based branchand-bound technique that explores a much richer space of solutions than any previous method in literature. We further enhance the basic algorithm to incorporate heuristics that potentially accelerate the search process significantly.	algorithm;data anonymization;data mining;experiment;heuristic (computer science);image compression;mathematical optimization;parallel computing;personally identifiable information;priority queue;privacy;query optimization;search algorithm;zero suppression	Bijit Hore;Ravi Chandra Jammalamadaka;Sharad Mehrotra	2007		10.1137/1.9781611972771.51	data publishing;computer science;enumeration;information retrieval;data set;data mining;personally identifiable information;heuristics;optimization problem	ML	-30.65849880042274	20.278006955281313	135159
9b25e1b2ed8a4ffe87bf91d70e1901f2e2f19457	the larch shared language: some open problems	open problems;larch shared language	The Larch Shared Languagefor the specification of abstract data types has evolved over a number of years from a simple algebraic language to one that is both more complicated and more useful. This talk reviews some of its major design decisions and then discussessome of the design issues and remaining open problems—most of which are consequencesof the same decisions that contribute to LSL’s good properties.	abstract data type;larch family;linear algebra	James J. Horning	1995		10.1007/3-540-61629-2_36	natural language processing;forestry;communication	PL	-24.975481976459278	21.018857723875286	135191
f7480faf816927799587a509a849948a5884ba04	making c++ practical for business applications: 3 essential classes	lenguaje programacion;base donnee;langage c;programming language;implementation;application transactionnelle;database;base dato;ingenieria logiciel;software engineering;ejecucion;c language;object oriented;transactional application;aplicacion transaccional;genie logiciel;langage programmation;oriente objet;orientado objeto;lenguaje c	V~%en experienced business applications progranmlers first look at C/C++, they're shocked to discover that some data representations they routinely need are unavailable. To get equivalent results someone has to build new data types (C++ classes), but doing so can be a daunting and error-prone undertaking, it's hardly surprising, then, that many experienced professionals are resisting a switch to C++.	c++ classes;cobol;cognitive dimensions of notations;compiler;grams;pl/i;programmer;programming language;subroutine	Conrad Weisert	1995	SIGPLAN Notices	10.1145/219726.219729	computer science;database;programming language;object-oriented programming;implementation;algorithm	PL	-27.005566247971608	25.259744145605833	135277
417eef8ccb667e3604975c97be091f9d53d4d580	polya: true type polymorphism for mobile ambients	mobile ambients;upper bound;safe ambients;polymorphism;ambient calculus;mobile agent;type system	Previous type systems for mobility calculi (the original Mobile Ambients, its variants and descendants, e.g., Boxed Ambients and Safe Ambients, and other related systems) offer little support for generic mobile agents. Previous systems either do not handle communication at all or globally assign fixed communication types to ambient names that do not change as an ambient moves around or interacts with other ambients. This makes it hard to type examples such as a “messenger” ambient that uses communication primitives to collect a message of nonpredetermined type and deliver it to a non-predetermined destination. In contrast, we present our new type system PolyA. Instead of assigning communication types to ambient names, PolyA assigns a type to each process P that gives upper bounds on (1) the possible ambient nesting shapes of any process P ′ to which P can evolve, (2) the values that may be communicated at each location, and (3) the capabilities that can be used at each location. Because PolyA can type generic mobile agents, we believe PolyA is the first type system for a mobility calculus that provides type polymorphism comparable in power to polymorphic type systems for the λ-calculus. PolyA is easily extended to ambient calculus variants. A restriction of PolyA has principal typings.	ambient calculus;lambda calculus;mobile agent;type system	Torben Amtoft;Henning Makholm;Joe B. Wells	2004		10.1007/1-4020-8141-3_45	real-time computing;mathematics;distributed computing;algorithm	PL	-28.561029273777127	32.23557465830101	135320
88cb83c78c42a7c90f458a096091e33ebc60122b	reactive programming with rescala		This demo introduces Reactive Programming (Bainomugisha et al. 2013), a programming technique to implement reactive applications through reusable modular abstractions for events and constraints. To discuss the fundamental Reactive Programming abstractions, we present REScala (Salvaneschi et al. 2014), a language for Reactive Programming. REScala provides many concepts of Reactive Programming as an embedded DSL to Scala. The two core concepts REScala provides are: • Events allow users to build event-driven applications that react to external stimuli, e.g., mouse clicks or button presses. Events can be derived, combined and transformed into new events. Imperative reactions can be attached to events, e.g., printing some output or updating a GUI. • Signals provide an elegant way for users to build applications with changing state, e.g., the current time or a moving object’s position in space. Instead of using manually maintained variables from native Scala, REScala offers Vars, which have the same semantics of imperatively assignable variables, but also act as Signals. Like Events, Signals can be combined and transformed to define derived state. They provide a form of constraint programming, similar to formulae in spreadsheets, but embedded into the generic host programming language. On derived Signals, users can imperatively read the value just like from variables. They can, however, not imperatively write the value, because it is defined by a constraint over other Signals. Instead, the value is updated automatically when any	constraint programming;digital subscriber line;domain-specific language;embedded system;event-driven programming;graphical user interface;imperative programming;printing;programming language;reactive programming;scala;spreadsheet	Ragnar Mogk;Joscha Drechsler	2017		10.1145/3079368.3079384	programming language;functional reactive programming;inductive programming;programming paradigm;functional logic programming;fifth-generation programming language;programming domain;reactive programming;extensible programming;computer science	PL	-28.083311185891237	27.21108717232597	135321
07b381a5c6fa8b506a91118147d5bdfcb5e3e123	on fairness notions in distributed systems: ii. equivalence-completions and their hierarchies	distributed system;systeme reparti;partial order semantics;systeme interaction;equitabilite processus;interaction;semantics;satisfiability;semantica;semantique;equivalence;semantique ordre partiel;sistema repartido;strong interaction fairness;process fairness;scheduling;robustesse;complecion;robustness;ordonamiento;interaccion;sif;interaction system;completion;equivalencia;ordonnancement;robustez	This is the second part of a two-part paper in which we discuss the implementability of fairness notionsin distributed systems where asynchronous processes interact via multiparty interactions. We focus here on equivalence-robust fairness notions where equivalence computations are either all fair or all unfair. Francezet al. (1992,Formal Aspects Comput. 4, 582–591) propose a notion of completion to transform a non-equivalence-robust fairness notion to an equivalence-robust one while maintaining several properties of the source. However, a completion may not preserve strong feasibility—a necessary and sufficient condition for a completion to be implementable. In this paper, we study the system requirement for a completion to be strongly feasible and determine the strongest implementable completion for every given fairness notion. Moreover, for most systems we obtain a fairness notion, which we refer to as SG +, such that SG+ is the strongest fairness notion that is both implementable and equivalence-robust. We also provide a comprehensive comparison of SG + and several well-known fairness notions and their minimal and maximal completions. Finally, we show that if equivalence-robustness is dropped, then in general it is impossible to define a fairness notion that is implementable and stronger than all other implementable fairness notions, unless the system consists of only one interaction. This implies plenty of leeway in the design of fairness notions suitable for various applications. C © 2001 Academic Press	algorithm;centralized computing;computation;distributed computing;fairness measure;interaction;liveness;maximal set;robustness (computer science);scheduling (computing);sender policy framework;source input format;suicidegirls;system requirements;turing completeness	Yuh-Jzer Joung	2001	Inf. Comput.	10.1006/inco.2000.3015	equivalence;discrete mathematics;interaction;completion;computer science;mathematics;semantics;linguistics;scheduling;algorithm;robustness;satisfiability	Logic	-25.140935890238666	32.2451993288069	135365
cc923be9ab9335fc9d24f31dc588ea5c2f11c1f8	trace analysis for conformance and arbitration testing	distributed system;prueba;trace analysis;protocols;systeme reparti;formal specification;norme iso;global knowledge;protocole transmission;distributed test architectures;sistema informatico;norma iso;specification;computer system;osi;open systems interconnection;test;protocols conformance testing open systems program testing;iso standard;system testing formal specifications modular construction transport protocols councils decision support systems fault detection software testing;automated construction;input output;protocolo transmision;transport protocol conformance testing arbitration testing implementation under test iut communication protocol implementations distributed test architectures partial input output traces local observers error detection power global knowledge automated construction trace analysis modules formal specification reference specification osi open systems interconnection;sistema repartido;conformance testing;program testing;communication protocol implementations;especificacion;local observers;partial input output traces;implementation under test;protocol specification;transport protocol;error detection power;communication protocol;trace analysis modules;systeme informatique;error detection;iut;open systems;reference specification;arbitration testing;test oracle;transmission protocol	The authors explore a testing approach where the concern for selecting the appropriate test input provided to the implementation under test (IUT) is separated as much as possible from the analysis of the observed output. Particular emphasis is placed on the analysis of the observed interactions of the IUT in order to determine whether the observed input/output trace conforms to the IUT's specification. The authors consider this aspect of testing with particular attention to testing of communication protocol implementations. Various distributed test architectures are used for this purpose, where partial input/output traces are observable by local observers at different interfaces. The error-detection power of different test configurations is determined on the basis of the partial trace visible to each local observer and their global knowledge about the applied test case. The automated construction of trace analysis modules from the formal specification of the protocol is also discussed. Different transformations of the protocol specification may be necessary to obtain the reference specification, which can be used by a local or global observer for checking the observed trace. Experience with the construction of an arbiter for the OSI (open systems interconnection) transport protocol is described. >	conformance testing	Gregor von Bochmann;Rachida Dssouli;J. R. Zhao	1989	IEEE Trans. Software Eng.	10.1109/32.41328	reliability engineering;embedded system;communications protocol;real-time computing;computer science;operating system;software engineering	SE	-32.01879713826383	31.362714245965222	135424
02a7652e4400d4373781455b4b884ba98a534569	from control effects to typed continuation passing	parametric polymorphism;languages;continuation passing style;continuations;polymorphism	First-class continuations are a powerful computational effect, allowing the programmer to express any form of jumping. Types and effect systems can be used to reason about continuations, both in the source language and in the target language of the continuation-passing transform. In this paper, we establish the connection between an effect system for first-class continuations and typed versions of continuation-passing style. A region in the effect system determines a local answer type for continuations, such that the continuation transforms of pure expressions are parametrically polymorphic in their answer types. We use this polymorphism to derive transforms that make use of effect information, in particular, a mixed linear/non-linear continuation-passing transform, in which expressions without control effects are passed their continuations linearly.	continuation;continuation-passing style	Hayo Thielecke	2003		10.1145/640128.604144	polymorphism;parametric polymorphism;computer science;artificial intelligence;philosophy of language;continuation-passing style;continuation;reliability;programming language;algorithm	Logic	-19.799849267564355	23.824573701533264	135465
d6dfad5a5ee329422d16dabd84283c4badca29d3	from function level semantics to program transformation and optimization	program transformation;off the shelf	"""The software crisis results from our disorderly concepts of """"program"""". These make programming an art, rather than an engineering discipline. Such a discipline would at least require that we have stocks of useful off-the-shelf programs and collections of standard theorems that can be applied repeatedly. We have neither."""	program optimization;program transformation	John W. Backus	1985		10.1007/3-540-15198-2_5	computer science;database;programming language;algorithm	PL	-22.80783755286468	20.857945040262766	135868
eb74c50dd95461d11ee719ed39d3be69fca248ba	object inheritance without classes (artifact)	004;inheritance objects classes operational semantics plt redex	This artifact is a PLT Redex implementation of the operational semantics presented in Object Inheritance Without Classes. It defines the core syntax and runtime semantics of the Graceless language, and then extends it in multiple different ways to produce the various implementations of object inheritance, including single and multiple inheritance. The implementation makes the semantics runnable, and precisely defines some behaviour which is defined informally in the paper. 1998 ACM Subject Classification F.3.2 Semantics of Programming Languages	multiple inheritance;operational semantics;racket;reduction strategy (code optimization);semantics (computer science)	Timothy Jones;Michael Homer	2016	DARTS	10.4230/DARTS.2.1.6	multiple inheritance;computer science;database;composition over inheritance;programming language;algorithm	PL	-24.62325622406729	26.807722958033764	135899
1d031e0084257acfb296eefceeec08c385c0b1cc	a framework for certified program analysis and its applications to mobile-code safety	developpement logiciel;verificacion modelo;informatique mobile;automatic proving;agent mobile;langage type;securite;agente movil;verification modele;typed assembly language;analyse temporelle;demostracion automatica;interpretacion abstracta;program verification;analisis temporal;analisis programa;time analysis;demonstration automatique;verificacion programa;model checking;programa aplicacion;sistema ml;application program;desarrollo logicial;programme application;proof carrying code;typed language;software development;safety;mobile code;mobile software;program analysis;interpretation abstraite;analyse programme;mobile agent;abstract interpretation;mobile computing;verification programme;coq proof assistant;seguridad;ml system;code mobile;lenguaje tipado;systeme ml	A certified program analysis is an analysis whose implementation is accompanied by a checkable proof of soundness. We present a framework whose purpose is to simplify the development of certified program analyses without compromising the run-time efficiency of the analyses. At the core of the framework is a novel technique for automatically extracting Coq proof-assistant specifications from ML implementations of program analyses, while preserving to a large extent the structure of the implementation. We show that this framework allows developers of mobile code to provide to the code receivers untrusted code verifiers in the form of certified program analyses. We demonstrate efficient implementations in this framework of bytecode verification, typed assembly language, and proof-carrying code.	code mobility;program analysis;proof-carrying code;typed assembly language	Bor-Yuh Evan Chang;Adam Chlipala;George C. Necula	2006		10.1007/11609773_12	program analysis;model checking;computer science;theoretical computer science;software development;mobile agent;programming language;mobile computing;algorithm	PL	-23.13440686334097	30.07311346289332	135989
ff149fb5dc5709fb206051ec7ab55d8bfe7b0dbf	toward visual constraint programming	clp compiler;run time code execution;clausal syntax;constraint logic programs;geometric shapes;visual syntax;wires;run time code execution visual constraint programming visual constraint programming language visual syntax geometric shapes clausal syntax constraint logic programming clausal constraint logic programming format clp compiler;runtime;logic programming shape program processors runtime circuits decision making wires arithmetic;visual programming;visual languages;shape;logic programming;visual constraint programming;constraint programming;arithmetic;constraint handling;circuits;constraint logic programming;program compilers;visual languages constraint handling visual programming program compilers;clausal constraint logic programming format;program processors;visual constraint programming language	W e present a Visual Constraint Programming Language which is based o n a very szmple vzsual syn tax composed of a f e w basic geometrzc shapes that correspond to the traditional clausal (i.e. textual) syntax of Constraint Logic Programming. It is however powerful and e f ic ien t , as the m a i n idea of our approach os t o compile down the visual syntax in to the clausal Constraint Logic Programming f o r m a t and use a n e f i c i e n t C L P compiler t o execute the runt ime code.	artificial intelligence;compiler;constraint logic programming;constraint programming;programming language	Emmanuel Chailloux;Philippe Codognet	1997		10.1109/VL.1997.626614	constraint logic programming;concurrent constraint logic programming;abstract syntax;first-generation programming language;constraint programming;constraint satisfaction;programming domain;reactive programming;computer science;theoretical computer science;functional logic programming;syntax;programming paradigm;inductive programming;fifth-generation programming language;visual programming language;programming language;homoiconicity;logic programming;algorithm;abstract syntax tree;syntax error	AI	-26.438361088527856	23.80446834651231	136261
4924ecc75d2a30120a088740c53594065cae67ee	a simple technique for handling multiple polymorphism	object oriented language;object oriented programming;polymorphism	Certain situations arise in programming that lead to multiply polymorphic expressions, that is, expressions in which several terms may each be of variable type. In such situations, conventional object-oriented programming practice breaks down, leading to code which is not properly modular. This paper describes a simple approach to such problems which preserves all the benefits of good object-oriented programming style in the face of any degree of polymorphism. An example is given in Smalltalk-80 syntax, but the technique is relevant to all object-oriented languages.	exception handling;expression (computer science);programming style;smalltalk;software design pattern	Daniel H. H. Ingalls	1986		10.1145/28697.28732	polymorphism;declarative programming;programming domain;reactive programming;computer science;object;theoretical computer science;ad hoc polymorphism;programming paradigm;procedural programming;inductive programming;fifth-generation programming language;programming language;object-oriented programming;second-generation programming language;algorithm	PL	-25.27915120550688	27.08922947191946	136540
6e9e2059e2ba09047e8e0fc75009563859747ae8	implementing attribute grammars using conventional compiler construction tools	grammar;attribute grammars;computer languages;generators;formal specification;semantics grammar encoding equations program processors computer languages generators;bottom up language translator attribute grammar based specification conventional compiler construction tool structure preserving coding pattern syntax directed translation scheme bottom up parser generation tool bottom up oriented translation scheme primitive attribution operation demand driven strategy data driven strategy translation scheme driven tool;semantics;program compilers attribute grammars formal specification;program compilers;encoding;program processors	This article describes a straightforward and structure-preserving coding pattern to encode arbitrary non-circular attribute grammars as syntax-directed translation schemes for bottom-up parser generation tools. According to this pattern, a bottom-up oriented translation scheme is systematically derived from the original attribute grammar. Semantic actions attached to each syntax rule are written in terms of a small repertory of primitive attribution operations. By providing alternative implementations for these attribution operations, it is possible to plug in different semantic evaluation strategies in a seamlessly way (e.g., a demand-driven strategy, or a data-driven one). The pattern makes it possible the direct implementation of attribute grammar-based specifications using widely-used translation scheme-driven tools for the development of bottom-up language translators (e.g. YACC, BISON, CUP, etc.). As a consequence, this initial coding can be subsequently refined to yield final efficient implementations. Since these implementations still preserve the ability of being extended with new features described at the attribute grammar level, the advantages from the point of view of development and maintenance become apparent.	attribute grammar;bottom-up parsing;cups;compiler;encode;gnu bison;syntax-directed translation;yacc	Daniel Rodriguez-Cerezo;Antonio Sarasa Cabezuelo;José Luis Sierra	2011	2011 Federated Conference on Computer Science and Information Systems (FedCSIS)		grammar systems theory;natural language processing;context-sensitive grammar;tree-adjoining grammar;synchronous context-free grammar;l-attributed grammar;link grammar;operator-precedence grammar;computer science;affix grammar;theoretical computer science;parsing;syntax;formal specification;extended affix grammar;grammar;database;semantics;linguistics;definite clause grammar;context-free grammar;programming language;attribute grammar;adaptive grammar;algorithm;encoding	Vision	-25.34377510822489	24.843188531456185	136588
fb6c6c793129ad90b85553a1ae4efbab849239c5	model-based pricing for machine learning in a data marketplace		Data analytics using machine learning (ML) has become ubiquitous in science, business intelligence, journalism and many other domains. While a lot of work focuses on reducing the training cost, inference runtime and storage cost of ML models, little work studies how to reduce the cost of data acquisition, which potentially leads to a loss of sellers’ revenue and buyers’ affordability and efficiency. In this paper, we propose a model-based pricing (MBP) framework, which instead of pricing the data, directly prices ML model instances. We first formally describe the desired properties of the MBP framework, with a focus on avoiding arbitrage. Next, we show a concrete realization of the MBP framework via a noise injection approach, which provably satisfies the desired formal properties. Based on the proposed framework, we then provide algorithmic solutions on how the seller can assign prices to models under different market scenarios (such as to maximize revenue). Finally, we conduct extensive experiments, which validate that the MBP framework can provide high revenue to the seller, high affordability to the buyer, and also operate on low runtime cost.	algorithm;data acquisition;experiment;machine learning;million book project	Lingjiao Chen;Paraschos Koutris;Arun Kumar	2018	CoRR		data mining;business intelligence;computer science;revenue;inference;data acquisition;journalism;data analysis;machine learning;artificial intelligence;arbitrage	AI	-33.08018448417975	20.49660235917474	136614
86c6ab22eb81ac194447ba459e68382685901292	bat2xml: xml-based java bytecode representation	java bytecode;dataflow analysis;control flow;xml;static analysis	The creation, transformation and analysis of bytecode is widespread. Nevertheless, several problems related to the reusability and comprehensibility of the results and tools exist. In particular, the results of tools for bytecode analysis are usually represented in proprietary tool dependent ways, which makes it hard to build more sophisticated analysis on top of the results generated by tools for lower-level analysis.Furthermore, intermediate results, such as e.g., the results of basic control flow and dataflow analysis, are usually not explicitly represented at all; though, required by many more sophisticated analysis. This lack of a common format, for the well structured representation of the (intermediate) results of code analysis, makes the creation of new tools or the integration of the results generated by different tools costly and ineffective. To solve the highlighted problems, we propose a higher-level XML-based representation of Java bytecode which is designed as a common platform for the creation and transformation of bytecode and explicitly enables the integration of arbitrary information generated by different tools for static code analysis.	common platform;control flow;data-flow analysis;dataflow;java bytecode;static program analysis;whole earth 'lectronic link;xml	Michael Eichberg	2005	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2005.02.035	xml;computer science;theoretical computer science;database;programming language;control flow;static analysis	PL	-22.849144259580157	27.211805373512735	136683
18816da5a6b78079d0566e3cb2c2d9b5c4c58fd5	how understanding and restructuring differ from compiling - a rewriting perspective	compiler construction;abstract syntax program understanding program restructuring program compiling semantic analysis syntactic analysis large software systems grammar recovery grammar composition lexical syntax context free syntax relational calculus term rewriting semantics directed querying concrete syntax;context free grammars;software systems;context free;rewriting systems;context free grammars reverse engineering program compilers rewriting systems relational algebra;software systems calculus application software symbiosis concrete books optimizing compilers parallel architectures software standards power generation;program compilers;term rewriting;relational algebra;semantic analysis;reverse engineering	Syntactic and semantic analysis are established topics in the area of compiler construction. Their application to the understanding and restructuring of large software systems reveals, however, that they have various shortcomings that need to be addressed. In this paper, we study these shortcomings and propose several solutions. First, grammar recovery and grammar composition are discussed as well as the symbiosis of lexical syntax and context-free syntax. Next, it is shown how a relational calculus can be defined by way of term rewriting and how a fusion of term rewriting and this relational calculus can be obtained to provide semantics-directed querying and restructuring. Finally, we discuss how the distance between concrete syntax and abstract syntax can be minimized for the benefit of restructuring. In particular, we pay attention to origin tracking, a systematic technique to maintain a mapping between the output and the input of the rewriting process. Along the way, opportunities for further research will be indicated.	abstract syntax;compiler;context-free language;lexical analysis;parse tree;relational calculus;rewriting;semantic analysis (compilers);software system	Paul Klint	2003		10.1109/WPC.2003.1199184	abstract syntax;relational algebra;computer science;compiler construction;abstract semantic graph;database;context-free grammar;programming language;algorithm;abstract syntax tree;reverse engineering;software system	SE	-26.21514115225559	22.804390760041056	136732
7ca0018cfa7be0e5d6dc4e4495959d3fbaffdd7d	on objects and events	lenguaje programacion;linguistique;edicion;object oriented language;micro kernels;programming language;resource control;edition;publishing;langage java;concurrent program;object oriented programming;suscripcion;codificacion;linguistica;remote method invocation;publish subscribe;programa competidor;coding;bytecode rewriting;langage programmation;abonnement;lenguaje java;mobile object systems;programmation orientee objet;security;subscription;codage;programme concurrent;java language;java;linguistics	This paper presents linguistic primitives for publish/subscribe programming using events and objects. We integrate our primitives into a strongly typed object-oriented language through four mechnisms: (1) serialization, (2) multiple subtyping, (3)closures, and (4) deferred code evaluation. We illustrate our primitives through Java, showing how we have overcome its respective lacks. A precompiler transforms statements based on our publish/subscribe primitives into calls to specifically generated typed adapters, which resemble the typed stubs and skeletons by the rmic precompiler for remote method invocations in Java	java remote method invocation;preprocessor;publish–subscribe pattern;serialization	Patrick Th. Eugster;Rachid Guerraoui;Christian Heide Damm	2001		10.1145/504282.504301	computer science;information security;theoretical computer science;database;programming language;object-oriented programming	PL	-24.974208207653334	28.090765076834664	136735
4b354d2d039bd7d969704822bbc8322ce366e0b5	first-class extents		Adding environments as rst-class values to a language can greatly enhance its expressiveness. But rst-class environments do not mesh well into a lexically scoped language since they rely on identi ers (variable names). By distinguishing variables from identi ers and therefore extents from environments, we present an alternative: rst-class extents. First-class extents are de ned on variables rather than identi ers and are therefore immune to name capturing problems that plague rst-class environments. Then by distinguishing variables from locations and therefore extents from stores, our rst-class extents can coexist with imperative features and still allow tail-recursion to be properly implemented as iteration. To test our claims, we extend Scheme with a collection of features that are essential to rst-class extents, give a denotational semantics for the extension, and demonstrate that it can be fully embedded into Scheme albeit losing tail-recursiveness. Then we show how rst-class extents lead to a way of extending Scheme with object-oriented programming features.	capacitor plague;coexist (image);denotational semantics;embedded system;imperative programming;iteration;recursion (computer science);scheme;scope (computer science);tail call	Shinn-Der Lee;Daniel P. Friedman	1992	Lisp and Symbolic Computation		natural language processing;state;identifier;computer science;variable;philosophy of language;etendue;programming language;location;algorithm	PL	-23.29461190887659	25.697700743073916	137032
a06f4eede10bfa8f4ce41898b4732e8fe0dc79c0	a prototype of an interface bilder for the common lisp interface manager - clib	interface builder;clim;graphical user interface;development process;rapid prototyping;common lisp interface manager;graphic user interface;lisp	The Common Lisp Interface Manager (CLIM) is used to develop graphical user interfaces for Lisp-based applications. With the prototype of the CLIM Interface Builder (CLIB) the programmer can generate code for CLIM interactively. The developing process will be fast and less prone to errors. With this new tool, the interactive rapid prototyping reduces costs of a specification phase. Here we present the concept and first results of the prototype of CLIB.	common lisp;graphical user interface;interactivity;interface builder;programmer;prototype;rapid prototyping	Jan Hesse;Rainer König;Filippo Logi;Jens Herder	1993	SIGPLAN Notices	10.1145/163114.163116	computer architecture;computer science;operating system;graphical user interface;programming language;preprocessor	HCI	-31.632669351709875	27.601348224681605	137035
a9292d7d17a21f365feafdb5680dd85a220ed604	certification of compiled assembly code by invariant translation	compilacion;translation invariant;analyse statique;certification;logiciel a securite critique;safety properties;analisis programa;safety critical software;certified compilation;certificacion;compilation;program analysis;interpretation abstraite;analyse programme;point of view;static analysis;abstract interpretation;static program analysis	We present a method for analyzing assembly programs obtained by compilation and checking safety properties on compiled programs. It proceeds by analyzing the source program, translating the invariant obtained at the source level, and then checking the soundness of the translated invariant with respect to the assembly program. This process is especially adapted to the certification of assembly or other machine-level kinds of programs. Furthermore, the success of invariant checking enhances the level of confidence in the results of both the compilation and the static analysis. From a practical point of view, our method is generic in the choice of an abstract domain for representing sets of stores, and the process does not interact with the compilation itself. Hence a certification tool can be interfaced with an existing analyzer and designed so as to work with a class of compilers that do not need to be modified. Finally, a prototype was implemented to validate the approach.	assembly language;compiler;point of view (computer hardware company);prototype;static program analysis	Xavier Rival	2003	International Journal on Software Tools for Technology Transfer	10.1007/s10009-003-0125-6	program analysis;computer science;database;programming language;certification;static analysis;algorithm;static program analysis	SE	-21.897271557051656	27.90269404702941	137154
ed630e3b094019eadce94c0b7b83692350e6cd04	refinement and verification applied to an in-flight data acquisition unit	verification;micro common representation language;helicoptero;sistema adquisicion dato;verification modele;b method;refinement;program verification;data acquisition system;mathematical and computer sciences;refinement method;verificacion programa;helicoptere;formal verification;model checking;langage microcrl;verification formelle;µcrl;helicopter;functional requirement;methode raffinement;aida;verification programme;systeme acquisition donnee;metodo afinamiento;data acquisition;helicopters	In order to optimise maintenance and increase safety, the Royal Netherlands Navy initiated the development of a multi-channel on-board data acquisition system for its Lynx helicopters. This AIDA (Automatic In-flight Data Acquisition) system records usage and loads data on main rotor, engines and airframe. We used refinement in combination with model checking to arrive at a formally verified prototype implementation of the AIDA system, starting from the functional requirements.	data acquisition;formal verification;functional requirement;model checking;on-board data handling;prototype;r.o.t.o.r.;refinement (computing)	Wan Fokkink;Natalia Ioustinova;Ernst Kesseler;Jaco van de Pol;Yaroslav S. Usenko;Yuri A. Yushtein	2002		10.1007/3-540-45694-5_1	simulation;computer science;data acquisition;programming language;algorithm	Embedded	-23.476403203660407	31.512180081778926	137162
0d5f453d9aa88e0421aa540db0925a85e1a55037	using event-b to construct instruction set architectures	machine abstraite;software;lenguaje programacion;virtual machine;compilateur;event b;programming language;logiciel;binary image;abstract state machine;implementation;maquina abstracta;formal methods;b method;instruction set architecture;compiler;software engineering;abstract machine;formal method;informatique theorique;data abstraction;genie logiciel;langage programmation;logicial;source code;bri;c programming language;implementacion;high level language;ingenieria informatica;compilador;computer theory;informatica teorica	The instruction set architecture (ISA) of a computing machine is the definition of the binary instructions, registers, and memory space visible to an executable binary image. ISAs are typically implemented in hardware as microprocessors, but also in software running on a host processor, i.e. virtual machines (VMs). Despite there being many ISAs in existence, all share a set of core properties which have been tailored to their particular applications. An abstract model may capture these generic properties and be subsequently refined to a particular machine, providing a reusable template for development of robust ISAs by the formal construction of all normal and exception conditions for each instruction. This is a task to which the Event-B (Metayer et al. in Rodin deliverable 3.2 Event-B language, http://rodin.cs.ncl.ac.uk , 2005; Schneider in The B-method an introduction, Palgrave, Basingstoke, 2001) formal notation is well suited. This paper describes a project to use the Rodin tool-set (Abrial in Formal methods and software engineering, Springer, Berlin, 2006) to perform such a process, ultimately producing two variants of the MIDAS (Microprocessor Instruction and Data Abstraction System) ISA (Wright in Abstract state machines, B and Z, Springer, Berlin, 2007; Wright in MIDAS machine specification, Bristol University, http://www.cs.bris.ac.uk/Publications , 2009) as VMs. The abstract model is incrementally refined to variant models capable of automatic translation to C source code, which this is compiled to create useable VMs. These are capable of running binary executables compiled from high-level languages such as C (Kernighan and Ritchie in The C programming language, Prentice-Hall, Englewood Cliffs, 1988), and compilers targeted to each variant allow demonstration programs to be executed on them.	abstract state machines;b-method;binary image;compiler;computer hardware;dspace;exception handling;executable;formal methods;high- and low-level;jean-raymond abrial;machine translation;microprocessor;processor register;rodin tool;software engineering;springer (tank);the c programming language;the new palgrave dictionary of economics;usability;virtual machine	Stephen Wright;Kerstin Eder	2009	Formal Aspects of Computing	10.1007/s00165-009-0142-7	b-method;compiler;parallel computing;formal methods;binary image;computer science;basic rate interface;virtual machine;instruction set;mathematics;programming language;implementation;high-level programming language;source code;abstract state machines	Arch	-24.713394970257262	28.787520793504367	137746
8347786be9c07682a81a85e53d5bf6a17379154b	towards trustworthy refactoring in erlang	qa 76 software computer programming;qa76 76 computer software	Tool-assisted refactoring transformations must be trustworthy if programmers are to be confident in applying them on arbitrarily extensive and complex code in order to improve style or efficiency. We propose a simple, high-level but rigorous, notation for defining refactoring transformations in Erlang, and show that this notation provides an extensible, verifiable and executable specification language for refactoring. To demonstrate the applicability of our approach, we show how to define and verify a number of example refactorings in the system.	code refactoring;erlang (programming language);executable;formal verification;high- and low-level;programmer;specification language;trustworthy computing	Dániel Horpácsi;Judit Köszegi;Simon J. Thompson	2016		10.4204/EPTCS.216.5	computer science;database;programming language;code refactoring	PL	-21.8538946278453	28.09684701913655	137898
fafb13055534f1dd41d41659ff4d6d5c7a4ef4c6	certification of termination proofs using ceta	dependence graph;code generation;term rewrite system;theorem prover;term rewriting	There are many automatic tools to prove termination of term rewrite systems, nowadays. Most of these tools use a combination of many complex termination criteria. Hence generated proofs may be of tremendous size, which makes it very tedious (if not impossible) for humans to check those proofs for correctness. In this paper we use the theorem prover Isabelle/HOL to automatically certify termination proofs. To this end, we first formalized the required theory of term rewriting including three major termination criteria: dependency pairs, dependency graphs, and reduction pairs. Second, for each of these techniques we developed an executable check which guarantees the correct application of that technique as it occurs in the generated proofs. Moreover, if a proof is not accepted, a readable error message is displayed. Finally, we used Isabelle’s code generation facilities to generate a highly efficient and certified Haskell program, CeTA, which can be used to certify termination proofs without even having Isabelle installed.	automated theorem proving;code generation (compiler);correctness (computer science);error message;executable;hol (proof assistant);haskell;isabelle;rewrite (programming);rewriting	René Thiemann;Christian Sternagel	2009		10.1007/978-3-642-03359-9_31	computer science;automated theorem proving;programming language;algorithm;code generation	PL	-19.404379818334707	25.662478440747694	137943
e059c05a59decf282a4542fb9ec46f3adaa7edc2	transformation of programs for fault-tolerance	tolerancia falta;developpement logiciel;modelizacion;recuperacion;atomic action;fault tolerant;sistema informatico;semantics;program transformation;ejecucion programa;computer system;ingenieria logiciel;recovery;transformation programme;semantica;semantique;software engineering;specification language;program execution;qa76 electronic computers computer science computer software;modelisation;transformacion programa;reliable communication;desarrollo logicial;execution programme;fault tolerance;software development;genie logiciel;systeme informatique;recuperation;lenguaje especificacion;communication channels;modeling;langage specification;tolerance faute	In this paper we describe how a program constructed for afault-free system can be transformed into afault-tolerant program for execution on a system which is susceptible to failures. A program is described by a set of atomic actions which perform transformations from states to states. We assume that a fault environment is represented by a programF. Interference by the fault environmentF on the execution of a programP can then be described as afault-transformation ℱ which transformsP into a program ℱ(P). This is proved to be equivalent to the programP□P F , whereP F is derived fromP andF, and □ defines the union of the sets of actions ofP andF P . A recovery transformation ℛ transformsP into a program ℛ(P) =P□R by adding a set ofrecovery actions R, called arecovery program. If the system isfailstop and faults do not affect recovery actions, we have ℱ(ℛ(P))=ℱ(P)□R=P□P F □R We illustrate this approach to fault-tolerant programming by considering the problem of designing a protocol that guarantees reliable communication from a sender to a receiver in spite of faults in the communication channel between them.	channel (communications);fault tolerance;interference (communication);linearizability	Zhiming Liu;Mathai Joseph	1992	Formal Aspects of Computing	10.1007/BF01211393	fault tolerance;real-time computing;computer science;semantics;programming language;algorithm	PL	-23.84252475449703	30.88653786550687	137956
ac8bf75fef954dfab4dda459b5856a94d153f17a	programming languages	functional programming languages;meta languages;functional programming;functional languages;students;logic programming languages;educational courses;programming languages;c-program;common lisp;logical programming languages;declarative programming;standard meta languages;practicing engineers;computer engineering courses;haskell;lisp;computer science education;declarative programming languages	This article focuses on the strengths and shortcomings of application languages as they are used in general purpose computers, on the strategies for their use, and on how they impact laboratory and medical systems. The author describes the reason why we are where we are at the present stage of computer languages and suggests where we might expect to be in the foreseeable future.	computer language;computers;programming languages	Thomas L. Lincoln	1991	Clinics in laboratory medicine	10.1007/978-3-319-24012-1		NLP	-28.24587588337417	23.738911556494983	137994
920f74f146ee8d8f6eb2de87ab87ec2a7dcdb9c5	exercises in free syntax. syntax definition, parsing, and assimilation of language conglomerates	aspectj;object oriented language;embedded language;program transformation;context free;automatic generation;concrete object syntax;syntax embedding;scannerless parsing;general solution;parse table composition;metaborg;precedence rules;extensible syntax;software development;domain specific language;language extension;separate compilation;generalized lr parsing	Application programmer’s interfaces give access to domain knowledge encapsulated in class libraries without providing the appropriate notation for expressing domain composition. Since object-oriented languages are designed for extensibility and reuse, the language constructs are often sufficient for expressing domain abstractions at the semantic level. However, they do not provide the right abstractions at the syntactic level. In this chapter we describe MetaBorg, a method for providing concrete syntax for domain abstractions to application programmers. The method consists of embedding domain-specific languages in a general purpose host language and assimilating the embedded domain code into the surrounding host code. Instead of extending the implementation of the host language, the assimilation phase implements domain abstractions in terms of existing APIs leaving the host language undisturbed. Indeed, MetaBorg can be considered a method for promoting APIs to the language level. The method is supported by proven and available technology, i.e. the syntax definition formalism SDF and the program transformation language and toolset Stratego/XT. We illustrate the method with applications in three domains: code generation, XML generation, and user interface construction. 2.1 I N T R O D U C T I O N Class libraries encapsulate knowledge about the domain for which the library is written. The application programmer’s interface to a library is the means for programmers to access that knowledge. However, the generic language of method invocation provided by object-oriented languages does often not provide the right notation for expressing domain-specific composition. General purpose languages, particularly object-oriented languages, are designed for extensibility and reuse. That is, language concepts such as objects, interfaces, inheritance, and polymorphism support the construction of class hierarchies with reusable implementations that can easily be extended with variants. Thus, OO languages provide the flexibility to develop and evolve APIs according to growing insight into a domain. Although these facilities are often sufficient for expressing domain abstractions at the semantic level, they do not provide the right abstractions at the syntactic level. This is obvious when considering the domain of arithmetic or logical operations. Most modern languages provide infix operators using the well-known notation from mathematics. Programmers complain when they have to program in a language where arithmetic operations are made	application programming interface;class hierarchy;code generation (compiler);data assimilation;domain-specific language;embedded system;extensibility;library (computing);logical connective;parse tree;parsing;program transformation;programmer;semantics (computer science);stratego/xt;subroutine;syntax definition formalism;transformation language;user interface;whole earth 'lectronic link;xml	Martin Bravenboer	2003			natural language processing;compiler;l-attributed grammar;language primitive;parsing expression grammar;computer science;domain-specific language;theoretical computer science;parsing;s-attributed grammar;syntax;extended affix grammar;context-free language;formal grammar;low-level programming language;programming language;homoiconicity;second-generation programming language;high-level programming language;abstract syntax tree	PL	-28.08142515605408	27.351286656382648	138037
a0f54597a00626efe21c5d51e18bdb437ba04adb	xrobots: a flexible language for programming mobile robots based on hierarchical state machines	robot sensing systems hardware clocks actuators programming;robot sensing systems;template behavior xrobots flexible language mobile robot programming hierarchical state machines domain specific language brooks;clocks;mobile robot;state machine;actuators;mobile robots;robot programming control engineering computing mobile robots;domain specific language;control engineering computing;programming;language design;robot programming;hardware	This paper introduces a domain-specific language for programming mobile robots that is based on hierarchical state machines. Following Brooks, we refer to states as behaviors. A novelty of this language is that behaviors are treated as first class objects in the language and thus they can be passed as arguments to other parameterized behaviors. The language has template behaviors which allow generalized behaviors to be customized and instantiated. This makes the language quite flexible in terms of programming styles. An example of its flexibility are presented, followed by a description of the challenges in the language design.	domain-specific language;first-class function;mobile robot;programming paradigm;uml state machine	Steve Tousignant;Eric Van Wyk;Maria L. Gini	2012	2012 IEEE International Conference on Robotics and Automation	10.1109/ICRA.2012.6225145	control engineering;mobile robot;simulation;language primitive;specification language;programming domain;reactive programming;computer science;programming language implementation;artificial intelligence;programming paradigm;symbolic programming;low-level programming language;finite-state machine;fifth-generation programming language	Robotics	-30.471271014906893	30.416515545672823	138278
fcdcb982e93a2fa0819d1cdacb72232603ec2c64	preservation of proof obligations from java to the java virtual machine	verification condition generator;optimizing compiler;java programming;java virtual machine;program verification;program generation;proof carrying code	While program verification environments typically target source programs, there is an increasing need to provide strong guarantees for executable programs.#R##N##R##N#We establish that it is possible to reuse the proof that a source Java program meets its specification to show that the corresponding JVM program, obtained by non-optimizing compilation, meets the same specification. More concretely, we show that verification condition generators for Java and JVM programs generate the same set of proof obligations, when applied to a program  p and its compilation [[ p ]] respectively.#R##N##R##N#Preservation of proof obligations extends the applicability of Proof Carrying Code, by allowing certificate generation to rely on existing verification technology.		Gilles Barthe;Benjamin Grégoire;Mariela Pavlova	2008		10.1007/978-3-540-71070-7_7	real-time computing;jsr 94;java concurrency;computer science;operating system;java modeling language;strictfp;optimizing compiler;real time java;programming language;java;generics in java;scala;java applet;java annotation	PL	-21.341529847092993	27.689649903846043	138680
cd83403271b86fa7e97d4b7d1a7bbfcebfad3eab	an interactive programme for steiner trees		We introduce a fully written programmed code with a supervised method for generating Steiner trees. Our choice of the programming language, and the use of well-known theorems from Geometry and Complex Analysis, allowed this method to be implemented with only 747 lines of effective source code. This eases the understanding and the handling of this beta version for future developments.	bird's-eye view;endeavour (supercomputer);entity–relationship model;gilbert cell;multicast;programming language;regular grid;software release life cycle;steiner tree problem;supervised learning;undo;wavelength-division multiplexing	Marcelo Zanchetta do Nascimento;Valério Ramos Batista;Wendhel Raffa Coimbra	2012	CoRR		computer science;theoretical computer science;mathematics;algorithm	PL	-25.740576883735795	25.348956060973332	138708
d9f5e345131c4ba4a00ee545bba9ce722f654e06	fm’99 — formal methods		State Machines (ASM) and Algebraic Methods in Software Technology (AMAST) A Termination Detection Algorithm: Specification and Verification . . . . . .1720 R. Eschbach Logspace Reducibility via Abstract State Machines . . . . . . . . . . . . . . . . . . . .1738 E. Grädel and M. Spielmann Formal Methods for Extensions to CAS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .1758 M. N. Dunstan, T. Kelsey, U. Martin, and S. Linton An Algebraic Framework for Higher-Order Modules . . . . . . . . . . . . . . . . . . . .1778 R. Jiménez and F. Orejas	abstract state machines;algorithm;formal methods;higher-order function;l (complexity)	Jan van Leeuwen	1999		10.1007/3-540-48118-4	natural language processing;formal methods;artificial intelligence;computer science	Logic	-20.324043624015342	18.88893568218887	138829
15bbd054cbe747f63f516bdc996c533c7ee32804	a type discipline for authorization policies	verification;modelizacion;lenguaje programacion;distributed system;communication process;theorie type;systeme reparti;computacion informatica;programming language;licence procedure;autorizacion;interrogation base donnee;spi calcul;interrogacion base datos;signature electronique;coordination language;autorisation;logical programming;specification programme;type systems;calculo spi;proceso comunicacion;process calculus;modelisation;langage coordination;processus communication;datalog;sistema repartido;algebra proceso;lenguaje coordinacion;programmation logique;ciencias basicas y experimentales;digital signature;criptografia;cryptography;theory;type theory;algorithme reparti;algebre processus;langage programmation;cryptographie;algoritmo repartido;authorization;audicion;process model;dependent types;firma numerica;logic programs;grupo a;communication channels;process algebra;security;program specification;programacion logica;distributed algorithm;modeling;languages;audit;database query;especificacion programa;type system;spi calculus	Distributed systems and applications are often expected to enforce high-level authorization policies. To this end, the code for these systems relies on lower-level security mechanisms such as digital signatures, local ACLs, and encrypted communications. In principle, authorization specifications can be separated from code and carefully audited. Logic programs in particular can express policies in a simple, abstract manner.  We consider the problem of checking whether a distributed implementation based on communication channels and cryptography complies with a logical authorization policy. We formalize authorization policies and their connection to code by embedding logical predicates and claims within a process calculus. We formulate policy compliance operationally by composing a process model of the distributed system with an arbitrary opponent process. Moreover, we propose a dependent type system for verifying policy compliance of implementation code. Using Datalog as an authorization logic, we show how to type several examples using policies and present a general schema for compiling policies.	antivirus software;authorization;compiler;cryptography;datalog;dependent type;digital signature;distributed computing;encryption;high- and low-level;opponent process;process calculus;process modeling;type system;verification and validation	Cédric Fournet;Andrew D. Gordon;Sergio Maffeis	2007	ACM Trans. Program. Lang. Syst.	10.1145/1275497.1275500	distributed algorithm;process calculus;computer science;theoretical computer science;database;programming language;algorithm	Security	-23.35473653420969	29.85384533565685	138975
ac723815f570b8aafec539e611c8e279c5e52d4d	mining natural language programming directives with class-oriented bayesian networks	bayesian network;intelligent interfaces;programming language;visual basic;object oriented programming languages;natural language;learning from examples;source code	"""Learning a programming language is a painstaking process, as it requires knowledge of its syntax, apart from knowing the basic process of representing logical sequences to programming stages. This fact deteriorates the coding process and expels most users from programming. Particularly for novice users or persons with vision problems, learning of how to program and tracing the syntax errors could be improved dramatically by using the most natural of all interfaces, i.e. natural language. Towards this orientation, we suggest a wider framework for allowing programming using natural language. The framework can be easily extended to support different object-oriented programming languages such as C, C++, Visual Basic or Java. Our suggested model is named """"Language Oriented Basic"""" and it concerns an intelligent interface that supports code creation, modification and control in Visual Basic. Users can use simple-structured Greek sentences in natural language and the system can output the corresponding syntactic tree. When users declare end of input, the system transforms the syntactic trees to source code. Throughout the whole interaction process, users can check the under-development code in order to verify its correspondence to their expectations. Due to the fact that using natural language can cause a great degree of ambiguity, Bayesian networks and learning from examples have been utilized as an attempt to reason on the most probable programming representation, given a natural language input sentence. In order to enhance the classifier, we propose a novel variation of Bayesian networks that favor the classification process. Experimental results have depicted precision and recall measures in a range of 73% and 70% respectively."""	bayesian network;natural language programming	Manolis Maragoudakis;Nikolaos Cosmas;Aristogiannis Garbis	2008		10.1007/978-3-540-88192-6_4	natural language processing;language identification;fourth-generation programming language;first-generation programming language;natural language programming;very high-level programming language;language primitive;programming domain;reactive programming;computer science;third-generation programming language;machine learning;bayesian network;syntax;data mining;programming paradigm;symbolic programming;low-level programming language;inductive programming;natural language;fifth-generation programming language;visual programming language;programming language;object-oriented programming;programming language specification;second-generation programming language;high-level programming language;source code	AI	-27.024747463120885	22.020255777230677	139135
274baaea405ccdaf8138feb69a0f209e5fe079e7	hyper-code revisited: unifying program source, executable, and data	user interface;conference item;qa76 computer software	The technique of hyper-programming allows program representations held in a persistent store to contain embedded links to persistent code and data. In 1994, Connor et al proposed extending this to hyper-code, in which program source, executable code and data are all represented to the user in exactly the same form. Here we explore the concept of hyper-code in greater detail and present a set of abstract language-independent operations on which various concrete systems can be based. These operations (explode, implode, evaluate, root and edit) are provided by a single user interface tool that subsumes the functions of both an object browser and a program editor. We then describe a particular implementation using PJama (persistent Java) and examine the impact of several language features on the resulting system.	display resolution;embedded system;executable;java;language-independent specification;object browser;persistence (computer science);programmer;systems architecture;text simplification;user interface	Evangelos Zirintsis;Graham N. C. Kirby;Ronald Morrison	2000		10.1007/3-540-45498-5_21	computer science;theoretical computer science;database;programming language	PL	-28.675112363487667	26.46144760756609	139418
8203be803bbb5ebe314de39d5b9f6a48c7f2b9f3	grammar-based hardware synthesis from port-size independent specifications	grammar;hardware design languages;support construction;protocols;computer languages;entrada salida;metodologia;elektroteknik och elektronik;program;grammar based;maintenance;application software;grammar based language;etude experimentale;electrical engineering electronic engineering information engineering;port size independent specification;hardware synthesis;specification;port size independent;space exploration;exploracion;automaton;testing;indexing terms;test;appui;methodologie;books;transmision asincronica;ensayo;input output;automata;protocols automata high level synthesis computer languages hardware design languages space exploration testing application software books java;high level synthesis;essai;grammars;especificacion;system design;grammaire;system;system design hardware synthesis port size independent specification automaton grammar based language program communication protocol atm;mantenimiento;exploration;communication centric;communication protocol;asynchronous transmission;design;transmission asynchrone;design space exploration;computer hardware;methodology;atm;protocols grammars high level synthesis;apoyo;estudio experimental;materiel informatique;gramatica;operation and maintenance;material informatica;entree sortie;java	A protocol defines how systems communicate. There are two ways of specifying the protocol, the language of communication. One way is to specify the automaton that recognizes the language, and this is the approach taken by SDL, etc. The other more abstract way ss to specify the grammar of the language and let a tool synthesize the automaton. Directly specifying the automaton makes the specification implementation dependent in two ways: the time behavior is specified in terms of states, and the width of the inputs and outputs is fixed. By specifying the grammar, the specification is potentially independent of both these implementation details and allows design space exploration in these dimensions. This paper presents a grammar-based language, called Program, that supports a port-size independent specifications methodology and its application to parts of the Operation and Maintenance protocol, a typical application from the ATM world. The methodology has also been applied to another test set of example designs and compared to standard RTL synthesis and HLS in order to evaluate the quality of the produced designs.		Johnny Öberg;Anshul Kumar;Ahmed Hemani	2000	IEEE Trans. VLSI Syst.	10.1109/92.831438	embedded system;communications protocol;electronic engineering;computer science;engineering;theoretical computer science;operating system;automaton;software testing;programming language;attribute grammar;algorithm	EDA	-31.537990688459423	30.57460412302871	139502
60922daff700e8344dac566b13d073b3d2021f63	selective tail call elimination	langage fonctionnel;approximation asymptotique;stack;lenguaje funcional;pila;asymptotic approximation;functional language;pile memoire;aproximacion asintotica	Tail calls are expected not to consume stack space in most functional languages. However, there is no support for tail calls in some environments. Even in such environments, proper tail calls can be implemented with a technique called a trampoline. To reduce the overhead of trampolining while preserving stack space asymptotically we propose selective tail call elimination based on an effect system. The effect system infers the number of successive tail calls generated by the execution of an expression, and trampolines are introduced only when they are necessary.	effect system;functional programming;overhead (computing);tail call	Yasuhiko Minamide	2003		10.1007/3-540-44898-5_9	stack;computer science;calculus;mathematics;programming language;functional programming;algorithm	PL	-22.13928673322465	29.51695309060504	139666
a1f934c994a8bf12008d86f456b5d97e2f3e0492	mapping and visiting in functional and object-oriented programming	object oriented programming	"""class Exp{ public abstract T Accept<T,D>(Visitor<T,D> v, D x); } class PlusExp : Exp { private Exp e1, e2; public PlusExp(Exp a, Exp b){ e1 = a; e2 = b; } public Exp LeftOperand{get {return e1;}} public Exp RightOperand{get {return e2;}} public override T Accept<T,D>(Visitor<T,D> v, D x){ return v.Visit(this, x); } } class TimesExp : Exp { private Exp e1, e2; public TimesExp(Exp a, Exp b){ e1 = a; e2 = b; } public Exp LeftOperand{get {return e1;}} public Exp RightOperand{get {return e2;}} public override T Accept<T,D>(Visitor<T,D> v, D x){ return v.Visit(this, x); } } class Identifier : Exp { private string name; public Identifier(string s) {name = s;} public string Name{get {return name;} } public override T Accept<T,D>(Visitor<T,D> v, D x){ return v.Visit(this, x); } } class Literal : Exp { private string litString; public string LiteralString{get {return litString;}} public Literal(string s) {litString = s; } 98 JOURNAL OF OBJECT TECHNOLOGY VOL 7, NO. 7 B THE VISITOR DESIGN PATTERN public override T Accept<T,D>(Visitor<T,D> v, D x){ return v.Visit(this, x); } } class Environment: Dictionary<String, int>{}; class Interpreter: Visitor<int, Environment> { public int Visit(PlusExp e, Environment env){ return (e.LeftOperand.Accept(this, env) + e.RightOperand.Accept(this, env)); } public int Visit(TimesExp e, Environment env){ return (e.LeftOperand.Accept(this, env) * e.RightOperand.Accept(this, env)); } public int Visit(Identifier e, Environment env){ return env[e.Name]; } public int Visit(Literal e, Environment env){ return Int32.Parse(e.LiteralString); } } class ReversePolishConverter: Visitor<string, None> { private const string SPACE = """" """"; public string Visit(PlusExp e, None x){ return e.LeftOperand.Accept(this, x) + SPACE + e.RightOperand.Accept(this, x) + SPACE + """"+""""; } public string Visit(TimesExp e, None x){ return e.LeftOperand.Accept(this, x) + SPACE + e.RightOperand.Accept(this, x) + SPACE + """"*""""; } public string Visit(Identifier e, None x){ return e.Name; } public string Visit(Literal e, None x){ return e.LiteralString; } } class None {}; class Program { VOL 7, NO. 7 JOURNAL OF OBJECT TECHNOLOGY 99 MAPPING AND VISITING IN FUNCTIONAL AND OBJECT-ORIENTED PROGRAMMING public static void Main() { TimesExp ast = new TimesExp(new PlusExp (new Literal(""""1""""), new Identifier(""""x"""")), new Literal(""""2"""")); // (1 + x) * 2 Environment env = new Environment(); env.Add(""""x"""",9); Interpreter interpreter = new Interpreter(); ReversePolishConverter reversPolishConverter = new ReversePolishConverter(); None nothing = new None(); Console.WriteLine(interpreter.Visit(ast, env)); Console.WriteLine(reversPolishConverter.Visit(ast, nothing)); } } } C DOUBLE DISPATCH VISITING IN CLOS ;;; Visitor in CLOS Common Lisp Object System. ;; Expression Classes (defclass Expression () ()) (defclass PlusExpression (Expression) ((a1 :initarg :firstAddend :accessor firstAddend) (a2 :initarg :secondAddend :accessor secondAddend))) (defclass MinusExpression (Expression) ((m :initarg :minuend :accessor minuend) (s :initarg :subtrahend :accessor subtrahend))) (defclass TimesExpression (Expression) ((f1 :initarg :firstFactor :accessor firstFactor) (f2 :initarg :secondFactor :accessor secondFactor))) (defclass DivideExpression (Expression) ((dividend :initarg :dividend :accessor dividend) (divisor :initarg :divisor :accessor divisor))) (defclass Identifier (Expression) ((name :initarg :name :accessor name))) (defclass IntegerLiteral (Expression) ((literal :initarg :literal :accessor literal))) ;; Visitor classes (defclass Visitor () ()) 100 JOURNAL OF OBJECT TECHNOLOGY VOL 7, NO. 7 C DOUBLE DISPATCH VISITING IN CLOS (defclass Interpreter (Visitor) ()) (defclass ReversePolish (Visitor) ()) ;; Visit multi methods. ;; Notice that the visit method specializes on both an Expression ;; and a (kind of) of visitor. (defmethod visit ((e PlusExpression) (v Interpreter) &optional extra) (+ (visit (firstAddend e) v extra) (visit (secondAddend e) v extra))) (defmethod visit ((e MinusExpression) (v Interpreter) &optional extra) ((visit (minuend e) v extra) (visit (subtrahend e) v extra))) (defmethod visit ((e TimesExpression) (v Interpreter) &optional extra) (* (visit (firstFactor e) v extra) (visit (secondFactor e) v extra))) (defmethod visit ((e DivideExpression) (v Interpreter) &optional extra) (/ (visit (dividend e) v extra) (visit (divisor e) v extra))) (defmethod visit ((e Identifier) (v Interpreter) &optional extra) (lookup-identifier extra (name e))) (defmethod visit ((e IntegerLiteral) (v Interpreter) &optional extra) (convert-string-to-integer (literal e))) (defmethod visit ((e PlusExpression) (v ReversePolish) &optional extra) (concatenate ’string (visit (firstAddend e) v extra) """" """" (visit (SecondAddend e) v extra) """" """" """"+"""")) (defmethod visit ((e MinusExpression) (v ReversePolish) &optional extra) (concatenate ’string (visit (minuend e) v extra) """" """" (visit (subtrahend e) v extra) """" """" """"-"""")) (defmethod visit ((e TimesExpression) (v ReversePolish) &optional extra) (concatenate ’string (visit (firstFactor e) v extra) """" """" (visit (SecondFactor e) v extra) """" """" """"*"""")) (defmethod visit ((e DivideExpression) (v ReversePolish) &optional extra) (concatenate ’string (visit (dividend e) v extra) """" """" (visit (divisor e) v extra) """" """" """"/"""")) (defmethod visit ((e Identifier) (v ReversePolish) &optional extra) (name e)) (defmethod visit ((e IntegerLiteral) (v ReversePolish) &optional extra) (literal e)) VOL 7, NO. 7 JOURNAL OF OBJECT TECHNOLOGY 101 MAPPING AND VISITING IN FUNCTIONAL AND OBJECT-ORIENTED PROGRAMMING ;; Auxiliary stuff. (defun convert-string-to-integer (str &optional (radix 10)) """"Given a digit string and optional radix, return an integer."""" ; Details not relevant for this paper ) (defun lookup-identifier (env name) (cdr (assoc name env :test (function equal)))) ;; Some sample visiting: (defun main () (let* ((expr1 (make-instance ’PlusExpression :firstAddend (make-instance ’IntegerLiteral :literal """"3"""") :secondAddend (make-instance ’IntegerLiteral :literal """"2""""))) (expr2 (make-instance ’TimesExpression :firstFactor (make-instance ’IntegerLiteral :literal """"3"""") :secondFactor (make-instance ’Identifier :name """"var""""))) (expr3 (make-instance ’MinusExpression :minuend expr1 :subtrahend expr2)) (interpretation (make-instance ’Interpreter)) (reverse-polish (make-instance ’ReversePolish)) (env ’((""""x"""" . 5) (""""var"""" . 7))) ) (list (visit expr1 interpretation env) (visit expr2 interpretation env) (visit expr3 interpretation env) (visit expr1 reverse-polish) (visit expr2 reverse-polish) (visit expr3 reverse-polish) )))"""	clos network;common lisp;concatenation;const (computer programming);defun;dictionary;distribution (mathematics);double dispatch;exptime;emoticon;env;identifier;integer literal;literal (mathematical logic);lookup table;mutator method;reverse polish notation;shared nothing architecture;the journal of object technology;visit;visitor pattern	Kurt Nørmark;Bent Thomsen;Lone Leth Thomsen	2008	Journal of Object Technology	10.5381/jot.2008.7.7.a2	method;programming domain;computer science;object;object-relational mapping;programming paradigm;programming language;object-oriented programming	Vision	-27.31067324557543	20.351690691377865	139706
523dd9e40f228e983218ff511a50a320dc6796f1	using polymorphism to improve expert system maintainability	knowledge base polymorphism expert system maintainability object oriented programming clasp production rules terminological definitions common loops common lisp operating system modularity reusability;expert systems;rule based;object oriented programming;object oriented;polymorphism;expert systems displays control systems object oriented programming application software software maintenance large scale systems software systems artificial intelligence dispatching;object oriented programming expert systems;expert system	The problems encountered in applying object-oriented programming to expert systems are described. A production system called Clasp, which addresses these difficulties, is presented. Clasp integrates methods, production rules, and terminological definitions for classes. The approach is a further generalization of Common Loops and the Common Lisp Operating System, which have all extended notion of methods in which all argument types can describe the applicability of methods. The system was designed to improve the modularity and reusability of the rule base, to support the development of a more consistent and homogeneous knowledge base, and to enhance the predictability of rules.<<ETX>>	algorithm;common lisp;commonloops;control theory;expert system;free variables and bound variables;generic function;heuristic;knowledge base;knowledge engineer;logic programming;monkey's audio;naruto shippuden: clash of ninja revolution 3;operating system;production system (computer science);programming paradigm;rule-based system;subsumption architecture;vii	John Yen;Hsiao-Lei Juang;Robert M. MacGregor	1991	IEEE Expert	10.1109/64.79709	legal expert system;computer science;artificial intelligence;database;subject-matter expert;object-oriented programming;expert system	AI	-27.04404526867776	20.672339896398313	139810
a4ee57e283b17c2cc925575084ebf21bfeb388e5	the dynamic frames theory	specification language;frame problem;formal verification;specification languages;design pattern;program correctness	The theory of Dynamic Frames has been invented to deal with the frame problem in the presence of encapsulation and pointers. It has proved more flexible and conceptually simpler than previous approaches that tackled the problem. It is now being actively used both for theoretical and for practical purposes related to the formal verification of program correctness. This paper presents the full theory of Dynamic Frames, together with its reasoning laws and exemplifies the use of these laws in proving correct several common design patterns. It also discusses the ongoing research on the topic.	correctness (computer science);design pattern;encapsulation (networking);formal verification;frame problem;pointer (computer programming)	Ioannis T. Kassios	2010	Formal Aspects of Computing	10.1007/s00165-010-0152-5	correctness;frame problem;specification language;formal verification;computer science;theoretical computer science;design pattern;programming language;algorithm	PL	-32.17235950263924	29.6967826273988	139825
e3875bc2465568da2a358175d87ad355666a3cd2	hide and show: using real compiler code for teaching	code generation;sscli;compilers;parsing;lexical analysis	In this paper, we present a novel approach that enables students in graduate compiler courses to examine and experiment with a real compiler without becoming overwhelmed by complexity. The key to the idea is the use of a debugger directly on a compiler during the compilation process. By providing instructions on breakpoints and variables of interest, the student is only shown the relevant portions of the compiler; the rest is hidden. We describe our strategy of using exercise sessions targeted toward illustration of core compiler concepts such as lexical analysis, parsing and code generation.	breakpoint;code generation (compiler);compiler;debugger;lexical analysis;parsing	Elizabeth L. White;Ranjan Sen;Nina Stewart	2005		10.1145/1047344.1047365	compile time;compiler;parallel computing;dynamic compilation;loop-invariant code motion;object code;lexical analysis;compiler correctness;interprocedural optimization;computer science;loop optimization;superoptimization;theoretical computer science;operating system;compiler construction;parsing;dead code elimination;optimizing compiler;bootstrapping;compilation error;programming language;inline expansion;intrinsic function;functional compiler;code generation	PL	-26.64366316652986	25.275531544056737	139883
77d9cad8915f0f97dbc01e7798c60b7ceab8eb8c	methods for specifying static semantics	static semantics;formal specification;context sensitive	The formal specification of a programming language involves the specification of three types of rules: syntax, static semantics and semantics. Various methods have been proposed for specifying the static semantic rules of programming languages, but as yet no method has received general acceptance. This paper looks at several different specification techniques and attempts to isolate the basic mechanisms used by each of them and explain the pattern of development of specification techniques for static semantics.	denotational semantics;programming language	M. Howard Williams	1981	Comput. Lang.	10.1016/0096-0551(81)90045-X	formal methods;formal semantics;action semantics;specification language;formal verification;computer science;theoretical computer science;formal semantics;formal specification;database;programming language;well-founded semantics;operational semantics;programming language specification;denotational semantics;language of temporal ordering specification;specification pattern;context-sensitive language	Logic	-21.126391406699877	26.416578668489034	140101
c36ff13c201aa3caaa8ed1179b206023fdd194ed	a machine-checked, type-safe model of java concurrency: language, virtual machine, memory model, and verified compiler		Klein and Nipkow’s formalisation Jinja [83] of a Java-like programming language was the first that unifies source code, bytecode, and a compiler, is executable, and has been shown type safe – with Isabelle/HOL [128] having mechanically checked all definitions and proofs. In this thesis, I extend Jinja to JinjaThreads with concurrency in the form of Java threads and the Java memory model (JMM). Moreover, I transfer the existing theorems of type safety and compiler correctness, and prove the important JMM guarantee that data-race free programs behave like under interleaving semantics. Furthermore, I present the first formally-verified compiler for multithreaded Java. JinjaThreads splits in two dimensions. On the one hand, like in Jinja, the compiler connects source code with bytecode on the level of languages. On the other hand, the semantics spans across different layers ranging from the implementation of the shared memory via the formalisation of the languages to the interleaving of threads and the axiomatic JMM. JinjaThreads is more than the sum of its parts, because it is their integration in a unified model that permits to correctly capture their interaction and to make reliable statements about the theory of the Java programming language. Jinja has simplified Java in many places for clarity. In contrast, JinjaThreads investigates concurrency as described in the Java language specification in detail. On the language level, JinjaThreads covers dynamic thread creation, synchronisation via locks and monitors, wait and notify, interruption, and joining on threads. To obtain a tractable model, I have structured JinjaThreads in modules which encapsulate language-independent parts and which source code and bytecode share. For example, the interleaving semantics is parametrised over the singlethreaded semantics and responsible for managing the thread pool, locks, interrupts, wait sets and notifications. By instantiating the parameters, I directly obtain the semantics for source code and bytecode. This modularity allows to formally define deadlock caused by synchronisation, which the type safety proof has to account for. The second aspect of concurrency is the JMM. In this thesis, I connect its axiomatic specification with an operational semantics of Java for the	cobham's thesis;compiler correctness;concurrency (computer science);correctness (computer science);deadlock;executable;formal verification;forward error correction;hol (proof assistant);interrupt;isabelle;java concurrency;java memory model;jinja (template engine);language-independent specification;lock (computer science);monitor (synchronization);operational semantics;programming language specification;race condition;shared memory;thread (computing);thread pool;type safety;unified model;virtual machine	Andreas Lochbihler	2012				PL	-25.779282216112314	29.78654480928627	140152
8b95625f517d891c25a3bdbc2dfba4bc7106fe77	implementing lazy functional languages on stock hardware: the spineless tagless g-machine	abstract machine;higher order functions;functional language		lazy evaluation	Simon L. Peyton Jones	1992	J. Funct. Program.	10.1017/S0956796800000319	computer science;theoretical computer science;abstract machine;programming language;functional programming;higher-order function;algorithm	PL	-21.70288096890275	22.635333642933457	140162
8c11d385d1eb63a7d7af7b6d75d4f25648c95532	an implementation and semantics for transactional memory introspection in haskell	reference monitors;transactional memory	Transactional Memory Introspection (TMI) is a novel reference monitor architecture that provides complete mediation, freedom from time of check to time of use bugs and improved failure handling for authorization. TMI builds on and integrates with implementations of the Software Transactional Memory (STM) architecture [Harris and Fraser 2003]. In this paper we present a formal definition of TMI and a concrete implementation over the Haskell STM. We find that this specification and reference implementation establishes clear semantics for the TMI architecture. In particular, they help identify and resolve ambiguities that apply to implementations such in our prior work [Birgisson et al. 2008].	authorization;harris affine region detector;haskell;introspection;reference implementation;reference monitor;software bug;software transactional memory;time of check to time of use	Arnar Birgisson;Úlfar Erlingsson	2009		10.1145/1554339.1554350	parallel computing;real-time computing;computer science;software transactional memory;database	PL	-21.948562931980042	31.22571480693505	140189
8572bf1556de73e41a3129fd4feadab11852f89a	tvla: a system for generating abstract interpreters	alias analysis;pointer analysis;java programming;operational semantics;shape analysis;transition systems;constraint solving;static analysis;abstract interpretation;data structure	The course will present TVLA (Three-Valued-Logic Analyzer). TVLA is a ”‘YACC’”’-like framework for automatically constructing abstract interpreters from an operational semantics. The operational semantics is specified as a generic transition system based on first order logic. TVLA has been implemented in Java and was successfully used to prove interesting properties of (concurrent) Java programs manipulating linked dynamically allocated data structures and to verify the partial correctness of several sorting programs.	correctness (computer science);data structure;first-order logic;java;logic analyzer;operational semantics;sorting;transition system	Tal Lev-Ami;Roman Manevich;Shmuel Sagiv	2004		10.1007/978-1-4020-8157-6_28	computer science;theoretical computer science;programming language;algorithm	PL	-19.180110800132	25.632790527785787	140198
49e82bd6615e2b468186ba7856e90559d39d74a3	totally verified systems: linking verified software to verified hardware	verified systems;linking verified software;verified hardware;higher order logic	We describe exploratory efforts to design and verify a compiler for a formally verified microprocessor as one aspect of the eventual goal of building totally verified systems. Together with a formal proof of correctness for the microprocessor, this yields a precise and rigorously established link between the semantics of the source language and the execution of compiled code by the fabricated microchip. We describe, in particular: (1) how the limitations of real hardware influenced this proof; and (2) how the general framework provided by higher-order logic was used to formalize the compiler correctness problem for a hierarchically structured language.		Jeffrey J. Joyce	1989		10.1007/0-387-97226-9_29	computer science;theoretical computer science;programming language;algorithm	OS	-20.567060232472517	27.177479558666814	140245
4650658001bf3e88886a6a28e09e329a0222f246	a parallel logic simulator based on concurrent prolog	parallel logic simulator;concurrent prolog	Using the framework of the parallel logical language. Concurrent Prolog, the concepts and the strategies for the parallel logic simulation are defined. Some types of a prototypical parallel logic simulator are implemented by Concurrent Prolog, and they are used for logic simulations of some sample objects. The performance and the applicability of Concurrent Prolog in the parallel logic simulation will be discussed.	logic programming;logic simulation;prolog	Yasunori Noda;Tetsuo Kinoshita;Akira Okumura;Tatsuro Hirano;Tadashi Hiruta	1985		10.1007/3-540-16479-0_25	computer architecture simulator;computer science;theoretical computer science;programming language;algorithm	EDA	-24.30534427101496	22.960001381115255	140556
7831820ef9d7e692f07c8f3059b2471f1658b4be	jpure: a modular purity system for java	side effect;extended static checking	Purity Analysis is the problem of determining whether or not a method may have side-effects. This has many applications, including automatic parallelisation, extended static checking, and more. We present a novel algorithm for inferring the purity of methods in Java. Our algorithm exploits two properties, called freshness and locality, which, when combined together, enable more precise purity analysis. Our algorithm also differs from the majority of previous attempts at purity analysis, in that it is modularly checkable. That is, the algorithm produces annotations which can be checked without the need for an expensive and costly interprocedural analysis. We evaluate our analysis against several packages from the Java Standard Library. Our results indicate that it is possible to uncover significant amounts of purity efficiently.	algorithm;extended static checking;interprocedural optimization;java;locality of reference;parallel computing;pure function;purity (quantum mechanics);replay attack;standard library	David J. Pearce	2011		10.1007/978-3-642-19861-8_7	real-time computing;computer science;theoretical computer science;programming language;side effect;algorithm	PL	-20.165539940152964	31.763396906498762	140746
88962fd7ca00c2eb19df59498d93ab3cff56d9ea	guided grammar convergence		Relating formal grammars is a hard problem that balances between language equivalence (which is known to be undecidable) and grammar identity (which is trivial). In this paper, we investigate several milestones between those two extremes and propose a methodology for inconsistency management in grammar engineering. While conventional grammar convergence is a practical approach relying on human experts to encode differences as transformation steps, guided grammar convergence is a more narrowly applicable technique that infers such transformation steps automatically by normalising the grammars and establishing a structural equivalence relation between them. This allows us to perform a case study with automatically inferring bidirectional transformations between 11 grammars (in a broad sense) of the same artificial functional language: parser specifications with different combinator libraries, definite clause grammars, concrete syntax definitions, algebraic data types, metamodels, XML schemata, object models.	algebraic data type;bidirectional transformation;combinator library;definite clause grammar;encode;formal grammar;functional programming;library (computing);metamodeling;parse tree;turing completeness;undecidable problem;xml	Vadim Zaytsev	2013	CoRR		grammar systems theory;natural language processing;context-sensitive grammar;tree-adjoining grammar;generative grammar;indexed grammar;synchronous context-free grammar;l-attributed grammar;link grammar;parsing expression grammar;computer science;affix grammar;regular tree grammar;extended affix grammar;emergent grammar;definite clause grammar;context-free grammar;programming language;attribute grammar;ambiguous grammar;adaptive grammar;mildly context-sensitive grammar formalism;combinatory categorial grammar;algorithm	PL	-22.771468952176395	21.791982911195003	140996
35896df227a18bcb9d3a923334acf201c672bc30	typeless programming in java 5.0	observer design;program design;code generation;type inference;language design;intersection types;type system;program design and implementation	With the introduction of Java 5.0 [9] the type system has been extended by parameterized types, type variables, type terms, and wildcards. As a result very complex types can arise. The termVector<Vector<AbstractList<Integer>>>is for example a correct type in Java 5.0.Considering all that, it is often rather difficult for a programmer to recognize whether such a complex type is the correct one for a given method or not. Furthermore there are methods whose principle types would be intersection types. But intersection types are not implemented in Java 5.0. This means that Java 5.0 methods often don't have the principle type which is contradictive to the OOP-Principle of writing re-usable code.This has caused us to develop a Java 5.0 type inference system which assists the programmer by calculating types automatically. This type inference system allows us, to declare method parameters and local variables without type annotations. The type inference algorithm calculates the appropriate and principle types.We implement the algorithm in Java using the observer design pattern.	algorithm;design pattern;inference engine;java version history;local variable;observer pattern;principal type;programmer;type inference;type system;wildcard character	Martin Plümicke;Jörg Bäuerle	2006		10.1145/1168054.1168079	type signature;type conversion;type system;unit type;type family;java concurrency;data type;type safety;computer science;recursive data type;theoretical computer science;java modeling language;type inference;interface;strictfp;program design language;programming language;algorithm;code generation;generics in java;scala;java annotation	PL	-25.3629660831302	27.884086801960336	141087
9ba5f6457b60969c5b08fddb1cc8bac54c9c16c5	modeling software product lines using color-blind transition systems	developpement logiciel;modelizacion;sistema transicion;alarm;language class;product lines;labelled transition system;calculateur embarque;cecite;maquina estado finito;labeled transition systems;blind;product line;relativized simulation;systeme transition etiquete;transition system;modelisation;blindness;systeme transition;ceguera;desarrollo logicial;classe langage;alarme;software development;boarded computer;transition systems;estructura producto;alarma;machine etat fini;software product line;modeling;structure produit;calculador embarque;finite state machine;ciego;product structure;sistema transicion marcada;embedded software;clase lenguaje;aveugle	Families of embedded discrete finite state programs are modeled using input-enabled alternating transition systems. One model describes all functionality, while each variant is defined by an environment, describing its possible uses. The environments show both the inputs that a system can receive and indicate which of the system’s responses are relevant for the environment. The latter trait, called color-blindness, creates new possibilities for system transformations in the specialization process. We demonstrate the use of the framework by applying it to two classes of realistic design languages. An example of a product line of alarm clocks is used throughout the article.	code generation (compiler);context-sensitive language;embedded system;mathematical optimization;model checking;partial template specialization;precondition;program analysis;prototype;refinement (computing);software product line;specification language	Kim G. Larsen;Ulrik Nyman;Andrzej Wasowski	2007	International Journal on Software Tools for Technology Transfer	10.1007/s10009-007-0046-x	simulation;systems modeling;embedded software;computer science;software development;finite-state machine;algorithm	SE	-23.6275493582821	30.748574295412194	141100
dc3a70c5c69836ece2dfdc51b0b8034e89567c67	eager and delayed contract monitoring for call-by-value and call-by-name evaluation	study design;software development	Article history: Available online 15 July 2010	effect system;haskell;idempotence;monad (functional programming);precondition;sorting algorithm	Markus Degen;Peter Thiemann;Stefan Wehr	2010	J. Log. Algebr. Program.	10.1016/j.jlap.2010.07.006	real-time computing;computer science;software development;clinical study design	PL	-22.37033444205662	21.431676468867092	141128
255b12a5646baaa7632c289823fa04eb8c5fec6f	extensible and modular generics for the masses	data type;polymorphism;generating function;functional requirement;generic programming	A generic function is a function that is defined on the structure of data types: with a single definition, we obtain a function that works for many data types. In contrast, an ad-hoc polymorphic function requires a separate implementation for each data type. Previous work by Hinze on lightweight generic programming has introduced techniques that allow the definition of generic functions directly in Haskell. A severe drawback of these approaches is that generic functions, once defined, cannot be extended with ad-hoc behaviour for new data types, precluding the design of an extensible and modular generic programming library based on these techniques. In this paper, we present a revised version of Hinze’s Generics for the masses approach that overcomes this limitation. Using our new technique, writing an extensible and modular generic programming library in Haskell 98 is possible.	boilerplate code;compile time;compiler;extensibility;first variation;functional programming;generic function;generic programming;haskell;hoc (programming language);jeremy gibbons;library (computing);programming language;type class	Bruno C. d. S. Oliveira;Ralf Hinze;Andres Löh	2006			polymorphism;generating function;data type;computer science;theoretical computer science;programming language;generic programming;functional requirement;algorithm	PL	-23.79176276293452	26.956752148844092	141328
cf210e59afa81e337e87052337364bf1ec366b8e	analysis of meta-programs: an example	program understanding;debugging;software engineering;program queries;program analysis;meta programs;meta programming	Meta-programs are generic, incomplete, adaptable programs that are instantiated at construction time to meet specific requirements. Templates and generative techniques are examples of meta-programming techniques. Understanding of meta-programs is more difficult than understanding of concrete, executable programs. Static and dynamic analysis methods have been applied to ease understanding of programs — can similar methods be used for meta-programs? In our projects, we build meta-programs with a meta-programming technique called XVCL. Meta-programs in XVCL are organized into a hierarchy of meta-components from which the XVCL processor generates concrete, executable programs that meet specific requirements. We developed an automated system that analyzes XVCL meta-programs, and presents developers with information that helps them work with meta-programs more effectively. Our system conducts both static and dynamic analysis of a meta-program. An integral part of our solution is a query language, FQL in which we formulate questions about meta-program properties. An FQL query processor automatically answers a class of queries. The analysis method described in the paper is specific to XVCL. However, the principle of our approach can be applied to other meta-programming systems. We believe readers interested in meta-programming in general will find some of the lessons from our experiment interesting and useful.		Stan Jarzabek;Hongyu Zhang;Ru Shen;Vu Tung Lam;Zhenxin Sun	2006	International Journal of Software Engineering and Knowledge Engineering	10.1142/S0218194006002689	program analysis;metaprogramming;computer science;artificial intelligence;theoretical computer science;software engineering;data mining;programming language;debugging	SE	-28.001172850881986	24.425718163077995	141351
f2eef3868f62f19e6777bc337c142185d06273be	compiler technology for object-oriented languages	object oriented language		compiler	Jens Palsberg	1996	ACM Comput. Surv.	10.1145/242224.242428	natural language processing;computer architecture;compiler;object-based language;object code;object language;compiler correctness;computer science;compiler construction;metacompiler;low-level programming language;bootstrapping;fifth-generation programming language;programming language;object-oriented programming;language technology;second-generation programming language;intrinsic function;functional compiler	PL	-25.021866169158752	22.5699437237232	141463
143765711f09db3a714ea06e8689b77a4d30d9c6	domains and expressions: an interface between two approaches to computer algebra	root finding;user interface;computer model;quotient algebra;multivariate polynomial;normal form;computer algebra;language design;symbolic numeric computation	This paper describes a method to use compiled, strongly typed Aldor domains in the interpreted, expression-oriented Maple environment. This represents a non-traditional approach to structuring computer algebra software: using an efficient, compiled language, designed for writing large complex mathematical libraries, together with a top-level system based on user-interface priorities and ease of scripting.We examine what is required to use Aldor libraries to extend Maple in an effective and natural way. Since the computational models of Maple and Aldor differ significantly, new run-time code must implement a non-trivial semantic correspondence. Our solution allows Aldor functions to run tightly coupled to the Maple environment, able to directly and efficiently manipulate Maple data objects. We call the overall system Alma.	aldor;compiled language;compiler;computational model;computer algebra system;foreign function interface;garbage collection (computer science);high- and low-level;library (computing);maple;mathematical optimization;problem solving;requirement;symbolic computation;user interface	Cosmin E. Oancea;Stephen M. Watt	2005		10.1145/1073884.1073921	computer simulation;quotient algebra;symbolic computation;theoretical computer science;root-finding algorithm;symbolic-numeric computation;mathematics;user interface;algorithm;algebra	PL	-23.293081679484917	25.728928146379282	141577
53caad225b0c10dbc2303da0c6625c598f2d3ff4	towards accessible integration and deployment of formal tools and techniques	software tools cloud computing computer aided instruction computer science education formal verification integrated software interactive systems program compilers program interpreters;computer aided instruction;program interpreters;syntactics visualization inference algorithms libraries cognition metals abstracts;computer science education;formal verification;integrated software;software tools;program compilers;cloud based web application accessible integration accessible deployment formal tools formal techniques computer science researchers programming languages formal verification communities automated assistance tools automated verification tools system designs formal arguments aartifact domain specific languages alloy spin z3 type checking congruence closure third party formal modeling web based interactive formal reasoning;interactive systems;cloud computing	Computer science researchers in the programming languages and formal verification communities, among others, have produced a variety of automated assistance and verification tools and techniques for formal reasoning. While there have been notable successes in utilizing these tools on the development of safe and secure software and hardware, these leading-edge advances remain largely underutilized by large populations of potential users that may benefit from them. In particular, we consider researchers, instructors, students, and other end users that may benefit from instant feedback from lightweight modeling and verification capabilities when exploring system designs or formal arguments. We describe Aartifact, a supporting infrastructure that makes it possible to quickly and easily assemble interacting collections of small domain-specific languages, as well as translations between those languages and existing tools (e.g., Alloy, SPIN, Z3) and techniques (e.g., evaluation, type checking, congruence closure); the infrastructure also makes it possible to compile and deploy these translators in the form of a cloud-based web application with an interface that runs inside a standard browser. This makes more manageable the process of exposing a limited, domain-specific, and logistically accessible subset of the capabilities of existing tools and techniques to end users. This infrastructure can be viewed as a collection of modules for defining interfaces that turn third-party formal modeling and verification tools and techniques into plug-ins that can be integrated within web-based interactive formal reasoning environments.	alloy (specification language);cloud computing;compiler;computer science;domain-specific language;formal verification;interaction;population;programming language;reason;spin;software deployment;system integration;type system;web application;z3 (computer);zeller's congruence	Andrei Lapets;Richard W. Skowyra;Azer Bestavros;Assaf J. Kfoury	2013	2013 3rd International Workshop on Developing Tools as Plug-Ins (TOPI)	10.1109/TOPI.2013.6597189	formal methods;formal verification;computer science;theoretical computer science;formal specification;refinement;programming language;computer engineering	SE	-32.543782943320956	26.481786058559226	142000
d1a2d3eaa25459ede1b117f03ec44e267eda3e8a	semantically transparent selective reset for and parallel interpreters based on the origin of failures				William H. Winsborough	1987			theoretical computer science;discrete mathematics;interpreter;computer science	HPC	-22.945584639554006	21.401167575854565	142217
15c9afcfc66a5f0ac68bce91b6e15cedb7ba6475	java server pages - servlets, inside out	java reflection	In our last column we discussed how Java Servlets address the two basic problems of developing web-based applications: program invocation and session state maintenance. The invocation problem is solved by defining a base class, HttpServlet, with methods for initialization, servicing requests, etc. that can be extended by application developers. The application developer's class can be registered with the web server which, in turn, invokes the appropriate method at the appropriate time in a multi-threaded fashion. Session state is maintained by the servlet implementation in a web-server-specific fashion (usually through I-I'ITP cookies) freeing the application developer from cookie or URL manipulation and providing a session object is used to store session-specific information. While servlets are well suited for writing efficient applications, they fall a bit short in addressing the needs of web page creators that expect easy HTML page formatting using their favorite tools. In particular, the Servlet API forces the application writer to write out HTML using Java print statements, a cumbersome approach at best. Java Server Pages (JSP) is a technology built on top of servlets meant to address the needs of web page designers. JSPs extend the HTML language with additional tags that are translated into Java Servlet code before the page is used. In addition, JSPs can be extended with custom tags where the translation is specified in Java macro expansion code. This extension capability is particularly interesting from a programming language implementation and translation perspective.	apl;asp.net;application programming interface;html;java servlet;javaserver pages;programming language implementation;server (computing);thread (computing);web application;web page;web server	Brent W. Benson	2000	SIGPLAN Notices	10.1145/369928.369934		PL	-30.398356852563303	26.20883055253611	142316
b7c7efa4e0c0a26eb3f47f9d4cf4890087075ec9	composing classes - roles vs traits	composition;code reuse;modularity;inheritance;traits;article;roles	Code replication has significant drawbacks in system maintenance. Code replication can have its origins in the composition limitations of the language. Several proposals have tried to overcome these limitations. A popular one is traits. However, traits do not support state or visibility control. Static roles are also a way of composing classes that has the benefits of traits and offers state, visibility control and other advantages as block renaming. We compare both approaches on how they are used to compose classes, and how they can be used to reduce code replication caused by composition limitations. As a case study we will compare how both approaches can reduce code replication by detecting and removing code clones within the JHotDraw framework. Results show that roles are capable of reducing a larger amount of replicated code than traits.	duplicate code;multiple inheritance;sensor	Fernando Sérgio Barbosa;Ademar Aguiar	2013		10.5220/0004424000630073	composition;computer science;bioinformatics;systems engineering;modularity	PL	-26.827075504573315	29.642426894132615	142392
97ae4f637e992ac1b4e3da90429be6d2f848a536	eliom: a core ml language for tierless web programming	web;client server;ml;functional;eliom;ocaml	Eliom is a dialect of OCaml for Web programming in which server and client pieces of code can be mixed in the same file using syntactic annotations. This allows to build a whole application as a single distributed program, in which it is possible to define in a composable way reusable widgets with both server and client behaviors. Our language also enables simple and type-safe communication. Eliom matches the specificities of the Web by allowing the programmer to interleave client and server code while maintaining efficient one-way server-to-client communication. The Eliom language is both sufficiently small to be implemented on top of an existing language and sufficiently powerful to allow expressing many idioms of Web programming. In this paper, we present a formalization of the core language of Eliom. We provide a type system, the execution model and a compilation scheme.	compiler;ocaml;one-way function;programmer;server (computing);type safety;type system;web development;world wide web	Gabriel Radanne;Jérôme Vouillon;Vincent Balat	2016		10.1007/978-3-319-47958-3_20	web service;first-generation programming language;computer science;web api;web log analysis software;web page;database;fat client;programming language;world wide web;application server;client–server model;server side includes;remote evaluation	PL	-26.877058073337164	28.618180858932767	142465
a9520f2e84bbe41b02aee8974fdd979327d900a7	gaussianscripteditor: an editor for gaussian scripting language for grid environment	software;chemicals;chemist oriented gaussian scripting language editing environment;programming environments;virtualization;human computer interaction;dsl;dltk technology;user interface;chemistry computing;virtualization editor grid antlr dltk;user interfaces authoring languages chemistry computing grid computing human computer interaction program compilers programming environments text editing;editor;authoring languages;maintenance engineering;data mining;computational chemistry;grid;user friendly application;graphical user interfaces;dltk technology gaussianscripteditor chemist oriented gaussian scripting language editing environment grid environment scientific research computational chemistry software user interface software oriented scripting language user friendly application domain specific language dsl compiling technology antlr technology;compiling technology;dltk;grid environment;domain specific language;antlr;computational chemistry software;program compilers;grid computing;scientific research;user interfaces;gaussian processes chemistry grid computing open source software user interfaces domain specific languages dsl chemical technology real time systems systems engineering education;gaussianscripteditor;buildings;scripting language;text editing;antlr technology;software oriented scripting language	More and more chemists carry out scientific research using computation. In this process, computational chemistry software has played a very important role. Among these computational chemistry software, Gaussian is very prominent. The most essential user interface of Gaussian is scripting language, but software-oriented scripting language is a great burden to chemists. Meanwhile, chemists are increasingly using grid environment to do scientific research. So, it is significant to build a user-friendly and chemist-oriented Gaussian scripting language editing environment in grid. In this paper, we introduce GaussianScriptEditor to solve these problems. GaussianScriptEditor is on the basis of grid platform and its implementation is related to Domain Specific Language (DSL), knowledge in computational chemistry, technology of compiling, ANTLR and DLTK.	antlr;compiler;computation;computational chemistry;dltk;digital subscriber line;domain-specific language;scripting language;usability;user interface	Tongming Wei;Ruisheng Zhang;Xianrong Su;Shilin Chen;Lian Li	2009	2009 Eighth International Conference on Grid and Cooperative Computing	10.1109/GCC.2009.49	maintenance engineering;human–computer interaction;computer science;theoretical computer science;operating system;database;scripting language;programming language;user interface	HPC	-32.255536107424646	26.62986502334051	142586
d2902bf4ea79cfdf1d14897beec71bd1fb5217e8	modular abstractions for verifying real-time distributed systems	distributed system;temporal logic;real time;satisfiability;specification language;model checking;modular decomposition;requirement specification	In this work we present a verification methodology for real-time distributed systems, based on their modular decomposition into processes. Given a distributed system, each of its components is reduced by abstracting away from details that are irrelevant for the required specification. The abstract components are then composed to form an abstract system to which a model checking proceddre is applied. The abstraction relation and the specification language guarantee that if the abstract system satisfies a specification, then the original system satisfies it as well. The specification language RTL is a branching-time version of the real-time temporal logic TPTL presented in Alur and Henzinger [1]. Its model checking is linear in the size of the system and exponential in the size of the formula. Two notions of abstraction for real-time systems are introduced, each preserving a sublanguage of RTL.		Hana De-Leon;Orna Grumberg	1993	Formal Methods in System Design	10.1007/BF01383942	model checking;real-time computing;specification language;temporal logic;computer science;theoretical computer science;modular decomposition;programming language;language of temporal ordering specification;satisfiability	Logic	-29.24556634630948	31.727388193312517	142828
2f84204ca2820456baedd4f58149052c0cfc1626	a process-oriented simulation package based on modula-2	programming environment;data type;technical report;discrete event simulation	SIMOD is a process-oriented, discrete-event simulation package, implemented as a set of precompiled modules written in Modula-2. It is not a new language; basically, a SIMOD program is simply a Modula-2 program. The package offers predefined data types and procedures, and runtime support facilities to manage the clock, event list, processes, resource acquisitions, etc. Using these tools, a programmer is able to express his model quickly and concisely, in a readable language. In this paper, we describe SIMOD, give some programming examples, and summarize the positive and negative aspects of our experience with Modula-2 as a host language for building a discrete-event simulation programming environment.	integrated development environment;modula-2;programmer;simulation	Pierre L'Ecuyer;Nataly Giroux	1987		10.1145/318371.318401	first-generation programming language;real-time computing;simulation;data type;computer science;technical report;theoretical computer science;discrete event simulation;programming language;world wide web	PL	-32.0744331920681	26.640449691310238	142872
a03ae81e847e2ef6d27758ea03331ba62c35fdfd	a multi-level approach to program synthesis	program synthesis;theorem proving;algorithm design	N. Fuchs, editor, 7 International Workshop on Logic Program Synthesis and Transformation (LOPSTR’97), LNAI 1463, pp. 1–25, c ©Springer Verlag, 1998. Abstract. We present an approach to a coherent program synthesis system which integrates a variety of interactively controlled and automated techniques from theorem proving and algorithm design at different levels of abstraction. Besides providing an overall view we summarize the individual research results achieved in the course of this development.	algorithm design;automated reasoning;automated theorem proving;coherence (physics);coherent;database schema;first-order logic;first-order predicate;functional programming;high- and low-level;imperative programming;interactivity;lecture notes in computer science;local search (optimization);mathematical induction;nuprl;principle of abstraction;program synthesis;programming language;rewrite (programming);springer (tank)	Wolfgang Bibel;Daniel S. Korn;Christoph Kreitz;F. Kurucz;Jens Otten;Stephen Schmitt;G. Stolpmann	1997		10.1007/3-540-49674-2_1	algorithm design;computer science;theoretical computer science;automated theorem proving;algorithm	Logic	-22.018857084050186	20.048595066916754	142927
0f5d4cc7ecacbb558df0965a601797afb4577fb6	fault tolerant functional reactive programming (functional pearl)		Session: Compilation and ConcurrencyHighly critical application domains, like medicine and aerospace, require the use of strict design, implementation and validation techniques. Functional languages have been used in these domains to develop synchronous dataflow programming languages for reactive systems. Causal stream functions and Functional Reactive Programming capture the essence of those languages in a way that is both elegant and robust.rnTo guarantee that critical systems can operate under high stress over long periods of time, these applications require clear specifications of possible faults and hazards, and how they are being handled. Modeling failure is straightforward in functional languages, and many Functional Reactive abstractions incorporate support for failure or termination. However, handling emph{unknown types of faults}, and incorporating emph{fault tolerance} into Functional Reactive Programming, requires a different construction and remains an open problem.rnThis work presents extensions to an existing functional reactive abstraction to facilitate tagging reactive transformations with hazard tags or confidence levels. We present a prototype framework to quantify the reliability of a reactive construction, by means of numeric factors or probability distributions, and demonstrate how to aid the design of fault-tolerant systems, by constraining the allowed reliability to required boundaries. By applying type-level programming, we show that it is possible to improve static analysis and have compile-time guarantees of key aspects of fault tolerance. Our approach is powerful enough to be used in systems with realistic complexity, and flexible enough to be used to guide their analysis and design, to test system properties, to verify fault tolerance properties, to perform runtime monitoring, to implement fault tolerance during execution and to address faults during runtime. We present implementations in Haskell and in Idris.	application domain;causal filter;compile time;compiler;dataflow programming;fault tolerance;functional programming;functional reactive programming;haskell;hazard (computer architecture);idris;programming language;prototype;static program analysis;termination analysis	Ivan Perez	2018	PACMPL	10.1145/3236791	functional reactive programming;computer science;compiler;programming language;dataflow programming;functional programming;haskell;static analysis;fault tolerance;reactive system	PL	-21.02795928323633	31.573142298531437	143015
1bb1b16478d18fe7c1ea9314b27738b0f71883cd	precisely deciding control state reachability in concurrent traces with limited observability		We propose a new algorithm for precisely deciding a control state reachability (CSR) problem in runtime verification of concurrent programs, where the trace provides only limited observability of the execution. Under the assumption of limited observability, we know only the type of each event (read, write, lock, unlock, etc.) and the associated shared object, but not the concrete values of these objects or the control/data dependency among these events. Our method is the first sound and complete method for deciding such CSR in traces that involve more than two threads, while handling both standard synchronization primitives and ad hoc synchronizations implemented via shared memory accesses. It relies on a new polygraph based analysis, which is provably more accurate than existing methods based on lockset analysis, acquisition history, universal causality graph, and a recently proposed method based the causally-precedes relation. We have implemented the method in an offline data-race detection tool and demonstrated its effectiveness on multithreaded C/C++ applications.	algorithm;c++;causality;concurrency control;data dependency;digital footprint;experiment;hoc (programming language);library (computing);multithreading (computer architecture);online and offline;race condition;reachability;runtime verification;sim lock;shared memory;thread (computing);tracing (software)	Chao Wang;Kevin Hoang	2014		10.1007/978-3-642-54013-4_21	real-time computing;computer science;theoretical computer science;distributed computing;programming language;algorithm	Logic	-20.30263293293184	30.90532671620607	143042
42843ddbe462e3b0ca73dd475ff64a443a07af22	i/o guided detection of list catamorphisms: towards problem specific use of program templates in ip	higherorder functions;search space;program schemes;input output;inductive programming;igorii;higher order functions;haskell	Inductive programming (IP), usually defined as a search in a space of candidate programs, is an inherent exponentially complex problem. To constrain the search space, program templates have ever been one of the first choices. In previous approaches to incorporate program schemes, either an (often very well) informed expert user has to provide a template in advance, or templates are used simply on suspicion, regardless whether they are target-aiming or not. Instead of rather fit the data to the template, we present an approach to fit a template to the data. We propose to utilise universal properties of higher-order functions to detect the appropriateness of a certain template in the input/output examples. We use this technique to introduce catamorphisms on lists in our IP system Igor2.	catamorphism;higher-order function;inductive programming;input/output;norm (social)	Martin Hofmann;Emanuel Kitzelmann	2010		10.1145/1706356.1706375	input/output;real-time computing;computer science;theoretical computer science;inductive programming;programming language;higher-order function;algorithm	PL	-19.838439727353627	27.722431336816097	143090
3dd3ce9f7152cc6077f347c30abba0c6c368e68f	abi compatibility through a customizable language	legacy software;compilateur;langage c;customization;personnalisation;indice aptitud;compatibilidad;compiler;macros;indice aptitude;organizacion memoria;logiciel patrimonial;c language;software evolution;capability index;logicial herencia;abi;organisation memoire;compatibility;personalizacion;design;compatibilite;binary compatibility;memory organization;c;languages;compilador;lenguaje c	ZL is a C++-compatible language in which high-level constructs, such as classes, are defined using macros over a C-like core language. This approach makes many parts of the language easily customizable. For example, since the class construct can be defined using macros, a programmer can have complete control over the memory layout of objects. Using this capability, a programmer can mitigate certain problems in software evolution such as fragile ABIs (Application Binary Interfaces) due to software changes and incompatible ABIs due to compiler changes. In this paper, we outline the problem of fragile and incompatible ABIs and show how ZL can be used to solve them.	application binary interface;compiler;high- and low-level;programmer;software evolution	Kevin Atkinson;Matthew Flatt;Gary Lindstrom	2010		10.1145/1868294.1868316	design;compiler;parallel computing;computer science;software evolution;programming language;compatibility;algorithm	PL	-25.389085725247696	28.283350906190794	143181
f87e04d478da2017b634619278ed1a3ad5e176fb	how can java be made blind-friendly	java programming;accessibility for the blind	The widely used and highly popular Java programming language is proved to be a great tool for developing platform independent applications. Everyday users mostly encounter them when using portable devices (mobile phones, PDAs, etc). However, the ordinary Java applications are inaccessible for the blind in general. Even the most used screen readers can only be enabled to handle GUI elements of a Java application by an additional adaptation package (e.g. access bridge for Jaws). Even with this, only a portion of existing Java programs that use swing classes may be made partially accessible for the blind. The solution offered eliminates the need of any screen reader.	graphical user interface;jaws scripting language;java;lecture notes in computer science;mobile phone;netware file system;operating system;personal digital assistant;programmer;programming language;springer (tank);swing (java)	Norbert Markus;Zoltan Juhasz;Gábor Bognár;András Arató	2008		10.1007/978-3-540-70540-6_75	embedded system;java card;java concurrency;computer science;operating system;strictfp;embedded java;real time java;java;world wide web;java applet;java annotation	PL	-30.567576525895987	26.26296073464605	143249
176dc1a4938c8337b7fde383ae5de0b04d3b9359	representation of complex objects: multiple facets with part-whole hierarchies	complex objects;semantic network;smalltalk;production rule	We have presented a working system that integrates in a homogeneous way multi-facets and part-whole hierarchies. Of course  many improvements are in order, notably some form of compilation to gain speed.    The main direction to be explored, in our opinion, is the meta-knowledge needed to implement reasoning about the system. Our  first attempt was to couple Systalk with our version of OPUS [10], a Smalltalk-80 interpretation of OPS-5 (see Pachet [11]) and to have OPUS production rules  control a Systalk robot. The next will be to integrate a powerful semantic network. Work is going on in this way.      		Francis Wolinski;Jean-François Perrot	1991		10.1007/BFb0057028	natural language processing;computer science;machine learning;semantic network;programming language	ML	-25.65363088140032	21.88325640777987	143455
a1589678953cd8e6137b6e86c449a1c5850527dc	etude et mise en œuvre de techniques de validation à l'exécution. (study and implementation of runtime validation techniques)		This thesis deals with three dynamic validation techniques : runtime verification (monitoring), runtime enforcement, and testing from property. We consider these approaches in the absence of complete behavioral specification of the system under scrutiny. Our study is done in the context of the Safety-Progress classification of properties. This framework offers several advantages for specifying properties on systems. We adapt the results on this classification, initially dedicated to infinite sequences, to take into account finite sequences. Those sequences may be considered as abstract representations of a system execution. Relying on this general framework, we study the applicability of dynamic validation methods. We characterize the classes of monitorable, enforceable, and testable properties. Then, we proposed three generic approaches for runtime verification, enforcement, and testing. We show how it is possible to obtain, from a property expressed in the Safety-Progress framework, some verification, enforcement, and testing mechanisms for the property under consideration. Finally, we propose the tools J-VETO and J-POST implementing all the aforementioned results on Java programs.		Yliès Falcone	2009				SE	-19.423155241620425	28.30988937744541	143551
427292cb8c9014b98e31541322217d3cb5636226	functional prototypes for generic c++ libraries: a transformational approach based on higher-order, typed signatures	concepts;program transformation;higher order functions;type constructors;defunctionalization	This paper presents a method for developing generic C++ software libraries from functional prototypes, based on program transformation. More precisely, the type signatures of generic functions, i.e., functions parameterized on types, are transformed. This transformation maps type-level expressions from functional higher-order, typed languages to type-level expressions in C++. In particular, type-level functional constructs such as higher-order functions and type constructors, are mapped to type parameters of generics that are constrained with appropriate concepts. The core of the transformation is a novel form of “defunctionalization” at the level of types. To make the transformation retargetable, we based it on two kernel languages that can be bound to different functional and object-oriented languages. For this paper, we use bindings to Haskell as front end and C++ with concepts as back end. Our transformational approach presents an alternative to a language extension and is useful particularly for functional prototyping where functional features are employed at specification time. We illustrate our approach by a case study: we show how we developed a generic C++ library for vulnerability modeling in the context of global change from a functional prototype in Haskell.	antivirus software;c++;defunctionalization;front and back ends;generic function;generic programming;global change;haskell;higher-order function;library (computing);map;program transformation;prototype;type constructor;type signature	Daniel Lincke;Sibylle Schupp;Cezar Ionescu	2014	International Journal on Software Tools for Technology Transfer	10.1007/s10009-014-0299-0	defunctionalization;concepts;computer science;theoretical computer science;programming language;generic programming;higher-order function;concept;algorithm	PL	-23.7346689312647	27.14714143795552	143563
83a8c4e1e7ad7fc4fd1ccd4d984bd2080706fb16	verifying and testing asynchronous circuits using lotos	hardware verification;digital logic;speed independent;asynchronous circuit;conformance testing;hardware design	It isshownhow DILL (Digital Logic in LOTOS) canbeusedtospecify,verify andtestasynchronous hardwaredesigns.Asynchronous(unclocked)circuitsarea topic of active researchin thehardware community. It is illustratedhow DILL canaddresssomeof the key challenges.New relationsfor (strong)conformancearedefinedfor assessingacircuit implementationagainstits specification.An algorithmisalsopresentedfor generatingandapplyingimplementationtestsbasedonaspecification. Toolshave beendevelopedfor automatedverificationof conformanceandgenerationof tests.The approachis illustratedwith threecasestudiesthatexplorespeedindependence, delaysensiti vity and testingof sampleasynchronouscircuit designs.	armatix ip1;automata theory;boolean algebra;correctness (computer science);semiconductor industry;verification and validation	Ji He;Kenneth J. Turner	2000		10.1007/978-0-387-35533-7_17	boolean algebra;real-time computing;asynchronous circuit;computer science;theoretical computer science;conformance testing	EDA	-33.50454687306882	32.27918251012073	143603
3bce75cc44e26b9586423f53932edb0bcfc21440	on application of self-similar pictures in education	unfold;functional programing;traversal;co induction;anamorphism;level order;breadth first;fold;program calculation	A set of basic functions for drawing self-similar pictures in a purely functional programming language (extended with turtle graphics abilities [l]) has been implemented. The implementation is based on creation of a library of several functions, convenient for usage with self-similar drawings. The solution is clear and simple and can easily be translated into any other (functional or other) programming language with limited graphics abilities. The main educational value of this implementation lies in ability of introduction of more inventive thinking, in offering more natural way for uniflcation of a program with data structures and in serving as a ready-made prototyping tool. Besides, graphics features especially turtle graphics are excellent instrument for introduction of higher-order functions and functional programming style in general. Purely functional programming language (LispKit) LISP has been developed at the Institute of Mathematics several years ago 21 and was at first extended with limited graphic abilities I [3 , then with turtle graphics abilities [4] and [5]. It is also worth mentioning that it has been used as a part of a CSl course at the Institute called “Programming languages” as a successful tool for introduction of basic programming principles [6] and [7]. For gaining a full control of a turtle, one basically needs only a few functions: an ability to change a direction of a turtle, an ability to draw a line (forward and backward) and an ability to move a turtle. These functions may be created in any language with only limited graphics abilities and later used for educational purposes. So, a library module that extends (LispKit) LISP into a graphical programming language, consists of the following functions: forward, backward, up, down and turn. An additional set of turtle graphics functions enabling more convenient work, consists of the following: angle, home, drawto and jumpto, with all functions having their usual meaning. Focusing on explanation of recursion, it proved to be very useful to use self-similar pictures in lessons. Pictures are naturally explained through recursion, while their drawing process can be followed on the screen and compared with the “calculation” process given by a teacher. Still, it is fairly easy to extend the set of turtle functions, to fulfill this request. There are basically two kinds of self-similar pictures those created by drawing from one point to several directions and those creating “connected” self-similar drawings (drawings with no nodes). These two may be created using the following functions: myself a function for drawing from one point, and self a function for drawing connected drawings. Besides, a few simple functions enabling easier work are added, like repunt (or, repeat...until), shrink (shortening the arguments of a function), advance, return, left and right (for moving and turning a turtle for a default value). Usage of turtle graphics in a functional language has twofold educational value. From one point of view, it shows the immediate results of a function evaluation, helping the user to understand recursion. On the other hand, it can be even used later, for “older” students, showing how a purely functional language, without the notion of a state, side-effects and global storage, can handle problems exclusively based on those features.	basic programming;cr rao advanced institute of mathematics, statistics and computer science;data structure;higher-order function;image;lispkit lisp;programming style;purely functional programming;recursion;self-similarity;state (computer science);turtle graphics;visual programming language	Zoran Putnik	1998		10.1145/289423.289470	breadth-first search;computer science;theoretical computer science;fold;programming language;functional programming;tree traversal;algorithm	Graphics	-29.287965017654102	21.71156515157923	143913
e959e1cdc1388dc6aca3d92e3ae5727d4e86ed0f	open cezeri library: a novel java based matrix and computer vision framework	visualization tool;matrix library;facade design pattern;method chain;fluent interface	In this paper we introduce the Open Cezeri Library (OCL) framework as a domain specific language (DSL) for researchers, scientists, and engineering students to enable them to develop basic linear algebra operations via simple matrix calculations, image processing, computer vision, and machine learning applications in JAVA programming language. OCL provides a strong intuition of coding for the developer while implementing by means of a fluent interface. The significant aspect of the OCL is to combine the methods of well-known platforms; MATLAB and JAVA, accordingly. Moreover, OCL supports a fluent interface so that users can extend a single line of codes by putting a dot between the methods because all the methods implemented actually return the host class. It was observed that the learning curve of the OCL is lower than the MATLAB and the native JAVA languages, and makes coding more readable, understandable, traceable, and enjoyable. In addition to this, the experiments revealed that the running performance of the OCL is quite comparable and can be used in a variety of diverse applications. © 2016 Wiley Periodicals, Inc. Comput Appl Eng Educ; View this article online at wileyonlinelibrary.com/journal/cae; DOI 10.1002/cae.21745		Musa Atas	2016	Comp. Applic. in Engineering Education	10.1002/cae.21745	facade pattern;computer science;multimedia;programming language;computer graphics (images)	Vision	-31.850380012073988	25.52075790664875	143955
3382264b868dc1fd6d1d87525bb0a7be8c88cd71	incrementalization across object abstraction	lenguaje programacion;incrementalization;object oriented language;blow up;programming language;securite informatique;program transformation;abstraction;query optimization;role based access control;object oriented programming;program verification;transformation programme;abstraccion;analisis programa;program optimization;invariants;computer security;verificacion programa;transformacion programa;efficient implementation;complex system;object oriented;seguridad informatica;langage programmation;oriente objet;design;optimisation programme;program analysis;analyse programme;verification programme;orientado objeto;optimizacion programa	"""Object abstraction supports the separation of what operations are provided by systems and components from how the operations are implemented, and is essential in enabling the construction of complex systems from components. Unfortunately, clear and modular implementations have poor performance when expensive query operations are repeated, while efficient implementations that incrementally maintain these query results are much more difficult to develop and to understand, because the code blows up significantly, and is no longer clear or modular.This paper describes a powerful and systematic method that first allows the """"what"""" of each component to be specified in a clear and modular fashion and implemented straightforwardly in an object-oriented language; then analyzes the queries and updates, across object abstraction, in the straightforward implementation; and finally derives the sophisticated and efficient """"how"""" of each component by incrementally maintaining the results of repeated expensive queries with respect to updates to their parameters. Our implementation and experimental results for example applications in query optimization, role-based access control, etc. demonstrate the effectiveness and benefit of the method."""	complex systems;mathematical optimization;query optimization;role-based access control	Yanhong A. Liu;Scott D. Stoller;Michael Gorbovitski;Tom Rothamel;Yanni Ellen Liu	2005		10.1145/1094811.1094848	complex systems;simulation;computer science;database;programming language;object-oriented programming;algorithm	PL	-22.995699013060943	28.0409343671137	144207
5657523052dc191191561dca5f62a2e7f9d0cb19	alloy: a lightweight object modelling notation	object models;z specification language;specification language;first order logic;structural properties;semantic analysis;object model	Alloy is a little language for describing structural properties. It offers a declaration syntax compatible with graphical object models, and a set-based formula syntax powerful enough to express complex constraints and yet amenable to a fully automatic semantic analysis. Its meaning is given by translation to an even smaller (formally defined) kernel. This paper presents the language in its entirety, and explains its motivation, contributions and deficiencies.	declaration (computer programming);domain-specific language;graphical user interface;semantic analysis (compilers)	Daniel O Jackson	2002	ACM Trans. Softw. Eng. Methodol.	10.1145/505145.505149	natural language processing;object model;object language;specification language;computer science;first-order logic;programming language;algorithm;object constraint language;object definition language	PL	-26.29561630912598	21.735428255385404	144488
b71e2aea52609b20c645fccbd9ab3188aacd785a	here, there and everywhere - on the recurring use of turtle graphics in cs1	learning difficulties;programming language;constructivist learning theory;abstraction;object oriented programming;object oriented;control flow;concept map;message passing;object diagram;object oriented modelling;object identification	The Logo programming language implements a virtual drawing machine—the turtle machine. The turtle machine is well-known for giving students an intuitive understanding of fundamental procedural programming principles. In this paper we present our experiences with resurrecting the Logo turtle in a new object-oriented way and using it in an introductory object-oriented programming course. While, at the outset, we wanted to achieve the same qualities as the original turtle (understanding of state, control flow, instructions) we realized that the concept of turtles is well suited for teaching a whole range of fundamental principles. We have successfully used turtles to give students an intuitive understanding of central object-oriented concepts and principles such as object, class, message passing, behaviour, object identification, subclasses and inheritance; an intuitive understanding of recursion; and to show students the use of abstraction in practice as the turtles at a late stage in the course becomes a handy graphics library used in a context otherwise unrelated to the turtles.	control flow;graphics library;handy board;java package;library (computing);logo;message passing;procedural programming;programming language;recursion;state (computer science);turtle (robot);turtle graphics	Michael E. Caspersen;Henrik Bærbak Christensen	2000		10.1145/359369.359375	method;computer science;artificial intelligence;theoretical computer science;god object	PL	-29.0767528207432	24.44658469523349	144563
60d70f040801072fd94d3631aee9945f04705eae	programmation d'ordre supérieur en lambda-prolog			prolog;λprolog	Yves Bekkers	1993			programming language;lambda;mathematics;prolog	Logic	-22.49631215952973	20.23035072711965	144588
263f6bd5a1c612a54e2c0c47a545ccf6bfe55d15	managing the evolution of aspect-oriented software with model-based pointcuts	institutional repositories;modelizacion;fedora;structure programme;desacoplamiento;punto ejecucion;orientado aspecto;conceptual analysis;semantics;conceptual model;decouplage;semantica;semantique;analisis conceptual;vital;modelisation;estructura programa;aspect oriented programming;decoupling;aspect oriented;point execution;analyse conceptuelle;vtls;modeling;program structure;ils;oriente aspect;pointcut	In spite of the more advanced modularisation mechanisms, aspect-oriented programs still suffer from evolution problems. Due to the fragile pointcut problem, seemingly safe modifications to the base code of an aspect-oriented program can have an unexpected impact on the semantics of the pointcuts defined in that program. This can lead to broken aspect functionality due to accidental join point misses and unintended join point captures. We tackle this problem by declaring pointcuts in terms of a conceptual model of the base program, rather than defining them directly in terms of how the base program is structured. As such, we achieve an effective decoupling of the pointcuts from the base program’s structure. In addition, the conceptual model provides a means to verify where and why potential fragile pointcut conflicts occur, by imposing structural and semantic constraints on the conceptual model, that can be verified when the base program evolves. To validate our approach we implemented a model-based pointcut mechanism, which we used to define some aspects on SmallWiki, a medium-sized application, and subsequently detected and resolved occurrences of the fragile pointcut problem when this application evolved.	aspect-oriented software development;coupling (computer programming);entity;error-tolerant design;evolution;formal system;high- and low-level;intensional logic;join point;pointcut;rendering (computer graphics);semantics (computer science);smalltalk;tom;universal instantiation	Andy Kellens;Kim Mens;Johan Brichau;Kris Gybels	2006		10.1007/11785477_28	simulation;aspect-oriented programming;computer science;semantics;programming language;algorithm	SE	-23.91697557798135	29.20848384682292	144625
2c59267eb6ce9ed9b661008a4dade21def5afcd4	teaching oop in c++ using an artificial life framework	object oriented programming;student learning;artificial life	This paper explores the framework method for teaching object-oriented programming. Specifically, it describes a hierarchy of C++ classes that implement the framework for an Artificial Life simulator. Students learn how to read these classes and extend them via inheritance: they design and implement subclasses that encapsulate the behavior and state of environments and the entities that inhabit them. The simulator constructs an artificial world from objects of these subclasses; then, it animates these objects as they interact during the simulation. This paper includes one sample project and both subclasses that implement its solution.	artificial life framework;c++ classes;entity;simulation	Richard E. Pattis	1997		10.1145/268084.268097	simulation;computer science;artificial intelligence;theoretical computer science;programming language;object-oriented programming;artificial life	PL	-29.131219318849134	24.349671118198312	144637
793bd598f39089e2451e07aee25ae764c3c1cd54	absl: an actor-based specification language for office automation	formal specification;software engineering;specification language;object oriented;levels of abstraction;active objects;office automation	Applications for use in an office environment are often very difficult to implement and/or prototype. One reason for such difficulty is the unavailability of an appropriate specification methodology through which an office analyst can specify the functional behavior of office applications at a high-level of abstraction and in a non-procedural fashion to the implementors. As a result, a great deal of effort, time, and money is often spent on “re-inventing the wheel” whenever a new office system concept is to be developed and/or prototyped. In this paper, we address the above problem by introducing a formal specification methodology, called ABSL, to be used for the specification of applications for offices. In this new methodology, which is based on the actor model, every office entity is uniformly viewed as an active computing component, or an active object. Each active object is viewed as a self-contained entity that models a logical or physical component appearing in an office environment. An example is provided to show the expressiveness of ABSL. Plans for future research in this area are given at the end of paper.	active object;actor model;automation;formal specification;high- and low-level;money;procedural programming;prototype;reinventing the wheel;specification language;unavailability	Hossein Saiedian;Elizabeth A. Unger	1990		10.1145/100348.100387	simulation;specification language;computer science;artificial intelligence;software engineering;formal specification;database;programming language;object-oriented programming	DB	-29.002911186716215	30.42448980461224	144785
5a03795659d6b36ec0b84e59d26f35aed5c19c07	on translation of boolean expressions	computers;mathematics;program synthesis;boolean algebra;functional analysis;translations;algol;programming;mathematics and computers	A program which translates an algorithmic language such as ALGOL into the machine language of an electronic computer performs the following functions:<list><item><italic>Analysis</italic>. From the program in algorithmic language are determined the operations which the computer must perform in the execution of the target program and the logical interdependence of these. </item><item><italic>Optimization</italic>. Of the many possibilities for optimization that exist, two are pertinent to this note: (2a) the elimination of superfluous operations, and (2b) the execution at translation time of those operations which do not depend on results produced by the target program. </item><item><italic>Synthesis</italic>. The sequence of operations which arise from steps 1 and 2 is expressed in the language of the computer and placed into the target program.	algol;boolean expression;computer;generalized context-free grammar;interdependence;machine code;mathematical optimization;relevance	H. H. Bottenbruch;A. A. Grau	1962	Commun. ACM	10.1145/368273.368414	functional analysis;boolean algebra;programming;computer science;theoretical computer science;mathematics;programming language;algorithm	PL	-19.50882862895287	22.174974228104638	144917
7908f0a10bc9ec4274223a0beed4dcf37c582a2e	run your research: on the effectiveness of lightweight mechanization	formal model;life cycle;programming language;selected works;semantic model;domain specific language;bepress;program analysis;lightweight semantics engineering;language design;formal language	Formal models serve in many roles in the programming language community. In its primary role, a model communicates the idea of a language design; the architecture of a language tool; or the essence of a program analysis. No matter which role it plays, however, a faulty model doesn't serve its purpose.  One way to eliminate flaws from a model is to write it down in a mechanized formal language. It is then possible to state theorems about the model, to prove them, and to check the proofs. Over the past nine years, PLT has developed and explored a lightweight version of this approach, dubbed Redex. In a nutshell, Redex is a domain-specific language for semantic models that is embedded in the Racket programming language. The effort of creating a model in Redex is often no more burdensome than typesetting it with LaTeX; the difference is that Redex comes with tools for the semantics engineering life cycle.	domain-specific language;embedded system;formal language;latex;program analysis;programming language;racket;reduction strategy (code optimization)	Casey Klein;John Clements;Christos Dimoulas;Carl Eastlund;Matthias Felleisen;Matthew Flatt;Jay A. McCarthy;Jon Rafkind;Sam Tobin-Hochstadt;Robert Bruce Findler	2012		10.1145/2103656.2103691	semantic data model;program analysis;biological life cycle;first-generation programming language;natural language programming;formal language;very high-level programming language;universal networking language;language primitive;object language;specification language;data control language;computer science;domain-specific language;low-level programming language;programming language;programming language specification;high-level programming language;algorithm	PL	-21.10521728421485	25.745047460943898	144928
bcea15d836d0e5430af596d10682d62a548f12c5	paragon specifications: structure, analysis and implementation	programmation;implementation;estructura;specification;hardware description languages;lenguaje;langage;object oriented programming;analyse;programacion;ejecucion;concurrency;descripcion;especificacion;object oriented;reecriture;oriente objet;analysis;language;rewriting;description;orientado objeto;programming;materiel informatique;structure;material informatica;reescritura;structure analysis;hardware;analisis	Paragon is a notation for specifying object behaviours using sets of rewrite rules, where rewriting is controlled by synchronous and asynchronous message passing, and where objects may be dynamically created as a rewriting side-eeect. This paper overviews Paragon, and introduces a simple classiication scheme for analysis of Paragon speciications. Restrictions on speciications are discussed in consideration of implementation feasibility and eeciency constraints. Implementation schemes based on the analysis and restrictions are deened. In particular, a translation strategy for static systems is detailed and motivated with a worked example. To reinforce the low-level nature of the derived implementation the translation is deened in terms of a digital hardware description language. Schemes for the implementation of general dynamic systems are also considered.	digital electronics;dynamical system;hardware description language;high- and low-level;message passing;rewrite (programming);rewriting	Paul Anderson;David Bolton;Paul H. J. Kelly	1994	Future Generation Comp. Syst.	10.1016/0167-739X(94)90054-X	parallel computing;computer science;theoretical computer science;operating system;analysis;database;distributed computing;programming language;object-oriented programming;algorithm	EDA	-25.3615458776628	31.55091599412649	145031
645dcc2de160209d6d547c557348ff9d61b9b619	classes: an abstract data type facility for the c language	postludes;domain tuning;double exit loop;building block;structured programs;abstract data type;data type;loop realizability;function;goal invariant;range tuning;termination assertion;data structure	Language constructs for definition and use of abstract data types ease the design and maintenance of large programs. This paper describes the C class concept, an extension to the C language providing such constructs. A class is defined using standard C data types and functions, and it can itself be used as a building block for new classes. A class provides a way of restricting access to a data structure to a specific set of functions associated with it, without incurring significant overheads at compile time or at run time.The C class concept is introduced by small examples of its use, and familiarity with the C language [2] is assumed. Appendix A is a complete small C program using classes.Classes have been in use for more than a year on a dozen PDP11 and VAX UNIX systems [1], and they are currently used for a diverse set of projects on more than 30 systems. Classes are currently implemented by an intermediate pass of the cc compiler, called the class pre-processor, which is invoked when the directive #class is found in a C source file. The class pre-processor is easily ported to a system with a version of the portable C compiler. A Motorola68000 version is in use.	ansi c;abstract data type;c data types;compile time;data structure;directive (programming);pdp-11;portable c compiler;run time (program lifecycle phase);small-c;unix;vax	Bjarne Stroustrup	1982	SIGPLAN Notices	10.1145/947886.947893	c++ classes;composite data type;real-time computing;data structure;data type;computer science;const;programming language;abstract data type;function;algorithm	PL	-24.544546363758855	27.51995867942822	145035
adef50283edf7107fdfd448bb49500db5990879a	encoding dcc in haskell		The seminal work on the Dependency Core Calculus (DCC) shows how monads not only can be used for embedding effects in purely functional languages but also to statically track data dependencies. Such types of analysis have applications in research areas like security, partial evaluation, and slicing, where DCC plays the role of a unifying formalism. For a Haskell programmer, putting DCC into practice raises many interesting conceptual and implementation concerns. Specifically, DCC uses a non-standard bind operator, i.e., with a different type signature than that provided by monads. In fact, embedding such non-standard bind operator opens the door for many design decisions. Furthermore, it is unclear if DCC extends to traditional methods used by Haskell programmers to handle effects (such as monad transformers). In this work, we describe a novel encoding of DCC in Haskell, with a focus on its use for security-although our results also apply to the other domains. We address the concerns mentioned above and show how our implementation of DCC can be seen as a direct translation from its typing rules via the use of closed type families and type classes-two advanced type-system features of Haskell. We also analyze what kind of effects DCC is compatible with and which ones it cannot secure. We also derive an alternative formulation of DCC based on fmap and a corresponding non-standard join.	computer security;data dependency;functional programming;haskell;monad (functional programming);monad transformer;partial evaluation;programmer;semantics (computer science);transformers;type class;type family;type rule;type signature	Maximilian Algehed;Alejandro Russo	2017		10.1145/3139337.3139338	programming language;operator (computer programming);slicing;haskell;type signature;embedding;formalism (philosophy);monad (functional programming);partial evaluation;computer science	PL	-22.74283034541345	25.59104487629884	145133
15a897a4b5141a63a265d203d62c2949f587b2a6	yet another formalisation of kads conceptual models	algebraic specification;conceptual model;formal language	This paper presents the use of the VITAL conceptual modeling language to formalise KADS conceptual models. This language is a formal language based on algebraic specification and order sorted predicate logic.	yet another	Willem Jonker;Jan Willem Spee	1992		10.1007/3-540-55546-3_42	formal language;computer science;conceptual model;programming language	Logic	-25.67137035908344	19.423831687275424	145191
5e18e92beb246ce720caf3f1cd3989aeaacfa30e	type-based publish/subscribe: concepts and experiences	distribution;encapsulation;developpement logiciel;lenguaje programacion;type;computacion informatica;programming language;behavioral analysis;securite;componente logicial;generics;design abstraction;distributed programs;abstraction;simultaneidad informatica;composant logiciel;langage java;encapsulacion;abstraccion;intergiciel publication souscription;large scale;concurrency;intergicial editor suscriptor;generic point;ciencias basicas y experimentales;desarrollo logicial;analyse comportementale;publish subscribe;software development;safety;software component;langage programmation;concurrent programs;lenguaje java;analisis conductual;distributed generators;grupo a;seguridad;simultaneite informatique;publish subscribe middleware;languages;reflection;type safety;java language;java	A continuously increasing number of interconnected computer devices makes the requirement for programming abstractions for remote one-to-many interaction yet more stringent. The publish/subscribe paradigm has been advocated as a candidate abstraction for such one-to-many interaction at large scale. Common practices in publish/subscribe, however, include low-level abstractions which hardly leverage type safety, and provide only poor support for object encapsulation. This tends to put additional burden on software developers; guarantees such as the aforementioned type safety and object encapsulation become of increasing importance with an accrued number of software components, which modern applications also involve, besides an increasing number of hardware components.Type-based publish/subscribe (TPS) is a high-level variant of the publish/subscribe paradigm which aims precisely at providing guarantees such as type safety and encapsulation. We present the rationale and principles underlying TPS, as well as two implementations in Java: the first based on a specific extension of the Java language, and a second novel implementation making use of recent general-purpose features of Java, such as generics and behavioral reflection. We compare the two approaches, thereby evaluating the aforementioned features---as well as additional features which have been included in the most recent Java 1.5 release---in the context of distributed and concurrent programming. We discuss the benefits of alternative programming languages and features for implementing TPS. By revisiting alternative abstractions for distributed programming, including “classic” and recent ones, we extend our investigations to programming language support for distributed programming in general, pointing out that overall, the support in current mainstream programming languages is still insufficient.	component-based software engineering;concurrent computing;design rationale;distributed computing;encapsulation (networking);general-purpose markup language;generic programming;high- and low-level;java;one-to-many (data model);programming language;programming paradigm;publish–subscribe pattern;software developer;type safety	Patrick Th. Eugster	2007	ACM Trans. Program. Lang. Syst.	10.1145/1180475.1180481	distribution;real-time computing;reflection;concurrency;encapsulation;type safety;computer science;component-based software engineering;software development;generic point;database;abstraction;programming paradigm;publish–subscribe pattern;programming language;generic programming;java;type	PL	-26.546348169572404	29.69060802687237	145224
5a597d0effdb943e4aa00833f042fab499ffa863	visual constraint rules	rewrite rule;rule based;visual programming;building simulation;constraint programming;rule based programming	Visual rule-based languages have been used quite successfully to program graphical simulations. They all use rewrite rules, which have the often-mentioned advantage that a program can supposedly be extended simply by adding a few more rules. In practice however, the rules tend to depend on each other, and instead of just adding rules, existing rules need to be changed. Visual constraint rules combine ideas from rule-based programming and constraint programming to create declarative forward-chaining-like rules that can be used in a more modular way to support iterative programming. Libraries of visual descriptions can be built and reused to compose complex behavior, which makes exploring the space of possible descriptions of simulations easier. This is valuable for the intended educational use of Cartoonist, a visual programming environment to build simulations. Furthermore, constraint rules also provide a way to describe a variety of parallel behaviors that are important in simulations, yet are not supported by similar systems.	algorithm;constraint programming;forward chaining;graphical user interface;graphics;integrated development environment;iteration;library (computing);logic programming;rewrite (programming);rewriting;rule-based system;self-replication;simulation;subsumption architecture;visual programming language	Roland Hübscher	1997	J. Vis. Lang. Comput.	10.1006/jvlc.1997.0055	rule-based system;constraint logic programming;concurrent constraint logic programming;constraint programming;constraint satisfaction;reactive programming;computer science;theoretical computer science;machine learning;programming paradigm;inductive programming;fifth-generation programming language;visual programming language;programming language;algorithm	HCI	-28.39745333653782	24.55085625735363	145262
3ed652ff4974c4dbe6019e37b49ee3b036917233	smart asset management for electric utilities: big data and future		This paper discusses about needs and ways to improve predictive maintenance in the future while facilitating the electric utilities to make smarter decisions about when and where maintenance should be performed. Utilities have been collecting data in large amounts but they are hardly utilized because they are huge in amount and also there is uncertainty associated with it. Condition monitoring of assets collects large amounts of data during daily operations. The question arises “How to extract information from this large chunk of data?” The concept of “rich data and poor information” is being challenged by big data analytics. Along with technological advancements like Internet of Things (IoT), big data analytics will play an important role for electric utilities. The aim will be to make the current asset management more smarter than it was, and this work describes some pathways.	algorithm;big data;chunking (computing);downtime;elegant degradation;holism;internet of things;mathematical optimization;pascal;scheduling (computing);software deployment	Swasti R. Khuntia;José L. Rueda;Mart A. M. M. van der Meijden	2017	CoRR		current asset;condition monitoring;big data;asset management;emerging technologies;business;marketing;internet of things	ML	-33.20500183515389	19.21113660015794	145458
3a27790f0b4bb8a65d6f04e8abcc9969c7923d2d	decision procedures		ion-Based Methods Predicate abstraction was introduced by Susanne Graf and Hassen Säıdi [135] in the context of interactive theorem proving. It was later used by Microsoft’s SLAM [12], the tool BLAST [139], and to a limited degree also by Java Path Finder [280], all of which can be considered the pioneers of software verifiers that support industrial programming languages. SLAM has evolved into SDV (for Static Driver Verifier) [13], which is now part of Microsoft’s Windows Driver Kit. SDV is used to verify device drivers, and is considered to be the first wide-scale industrial application of formal software verification. In the decade that followed, at least ten further software verifiers based on predicate abstraction were introduced. Both Magic [65] and SatAbs [73] 12.6 Bibliographic Notes 307 can also verify concurrent programs. The BFC Model Checker for Boolean programs can verify the parametric case, i.e., programs in which the number of concurrent threads is not bounded apriori [161]. With the Windows 8.1 release, SDV now uses Corral as verifier [177, 178], which uses techniques described in Sect. 12.3. The annual competition for propositional SAT solvers has resulted in a remarkable surge in the performance and quality of the solvers. To this end, the Competition on Software Verification (SV-COMP) was founded in 2012, and is held annually in association with the conference TACAS. The benchmarks are split into numerous categories according to particular language features that are exercised, including “Bit-Vectors”, “Concurrency”, and “HeapManipulation”. Beyond Safety Checking This chapter focuses on methods for checking reachability properties. However, decision procedures have applications beyond reachability checking in program analysis, and we will give a few examples. Termination checkers attempt to answer the question “does this program run forever, or will it eventually terminate?” Proving program termination is typically done by finding a ranking function for the program states, and the generation of these ranking functions relies on a decision procedure for the appropriate theory. Typical instances use linear arithmetic over the rationals (e.g., [76]) and the bit vectors [74, 86, 68]. A further applications of program analysis that goes beyond reachability checking is the computation of quantitative properties of programs, e.g., information leakage [141]. A.1 The Satisfiability-Modulo-Theory Library and Standard (SMT-LIB) A bit of history: The growing interest and need for decision procedures such as those described in this book led to the SMT-LIB initiative (short for Satisfiability-Modulo-Theory Library). The main purpose of this initiative was to streamline the research and tool development in the field to which this book is dedicated. For this purpose, the organizers developed the SMT-LIB standard [239], which formally specifies the theories that attract enough interest in the research community, and that have a sufficiently large set of publicly available benchmarks. As a second step, the organizers started collecting benchmarks in this format, and today (2016) the SMT-LIB repository includes more than 100 000 benchmarks in the SMT-LIB 2.5 format, classified into dozens of logics. A third step was to initiate SMT-COMP, an annual competition for SMT solvers, with a separate track for each division. These three steps have promoted the field dramatically: only a few years back, it was very hard to get benchmarks, every tool had its own language standard and hence the benchmarks could not be migrated without translation, and there was no good way to compare tools and methods. These problems have mostly been solved because of the above initiative, and, consequently, the number of tools and research papers dedicated to this field is now steadily growing. The SMT-LIB initiative was born at FroCoS 2002, the fourth Workshop on Frontiers of Combining Systems, after a proposal by Alessandro Armando. At the time of writing this appendix, it is co-led by Clark Barrett, Pascal Fontaine, and Cesare Tinelli. Clark Barrett, Leonardo de Moura, and Cesare Tinelli currently manage the SMT-LIB benchmark repository. 1 In fact, it was reported in [94] that each tool tended to be the best on its own set of benchmarks. A SMT-LIB: a Brief Tutorial © Springer-Verlag Berlin Heidelberg 2016 D. Kroening and O. Strichman, Decision Procedures, Texts in Theoretical Computer Science. An EATCS Series, DOI 10.1007/978-3-662-50497-0 309 310 A SMT-LIB: a Brief Tutorial The current state: The current SMT-LIB standard is at version 2.5 (as of 2015). It supports the theories that are presented in Fig. A.1. The symbols should be interpreted as follows: • QF for the restriction to quantifier-free formulas • A or AX for arrays without or with extensionality • BV for fixed-size bit-vectors • FP for Floating-Point • IA for integer arithmetic • RA for real arithmetic • IRA for mixed integer arithmetic • IDL for integer difference logic • RDL for rational difference logic • L before IA, RA, or IRA for the linear fragment of those arithmetics • N before IA, RA, or IRA for the nonlinear fragment of those arithmetics • UF for the extension allowing free sort and function symbols Fig. A.1. The theories supported by the SMT-LIB standard have associated benchmarks and at least one tool that (attempts to) solve them. An arrow (T1, T2) means that T1 is a special case of T2. The greyed nodes are theories that are covered in this book. The figure is copied (with permission) from the SMT-LIB web-site A.2 The SMT-LIB File Interface The SMT-LIB standard defines a file format for describing decision problems. The benefit of a standardized file format is that it is easy to experiment with A.2 The SMT-LIB File Interface 311 a range of solvers, and to replace the solver used in case better solvers are developed. The description below refers to ver. 2.0 of the standard, but ver. 2.5 is backward-compatible. SMT-LIB files are ASCII text files, and as a consequence can be written with any text editor that can save plain text files. The syntax is derived from that of Common Lisp’s S-expressions. All popular solvers are able to read formulas from files or the standard input of the program, which permits the use of POSIX pipes to communicate with the solver. We will refrain from giving a formal syntax and semantics for SMT-LIB files, and will instead give examples for the most important theories. A.2.1 Propositional Logic We will begin with an example in propositional logic. Suppose we wanted to check the satisfiability of (a ∨ b) ∧ ¬a . We first need to declare the Boolean variables a and b. The SMT-LIB syntax offers the command declare-fun for declaring functions, i.e., mappings from some sequence of function arguments to the domain of the function. Variables are obtained by creating a function without arguments. Thus, we will write 1 (declare-fun a () Bool) 2 (declare-fun b () Bool) to obtain two Boolean variables named a and b. Note the empty sequence of arguments after the name of the variable. We can now write constraints over these variables. The syntax for the usual Boolean constants and connectives is as follows: true true false false ¬a (not a) a =⇒ b (=> a b) a ∧ b (and a b) a ∨ b (or a b) a⊕ b (xor a b) Using the operators in the table, we can write the formula above as follows: 1 (and (or a b) (not a)) Constraints are given to the SMT solver using the command assert. We can add the formula above as a constraint by writing 1 (assert (and (or a b) (not a))) As our formula is a conjunction of two constraints, we could have equivalently written 312 A SMT-LIB: a Brief Tutorial 1 (assert (or a b)) 2 (assert (not a)) After we have passed all constraints to the solver, we can check satisfiability of the constraint system by issuing the following command:	adobe streamline;apriori algorithm;automated theorem proving;blast;backward compatibility;barrett reduction;base one foundation component library (bfc);benchmark (computing);bit array;boolean satisfiability problem;common lisp;computation;decision problem;denotational semantics;device driver;driver verifier;emoticon;european association for theoretical computer science;european joint conferences on theory and practice of software;exclusive or;formal grammar;formal verification;information leakage;java;logic programming;logical connective;microsoft windows;model checking;modulo operation;nonlinear system;posix;pascal;predicate abstraction;program analysis;programming language;proof assistant;propositional calculus;quantifier (logic);ranking (information retrieval);reachability;report definition language;s-expression;smt placement equipment;simultaneous localization and mapping;simultaneous multithreading;software verification;solver;spectral leakage;springer (tank);standard streams;systemverilog;terminate (software);termination analysis;text editor;traffic collision avoidance system;ver (command);windows driver kit;xfig	Daniel Kroening;Ofer Strichman	2016		10.1007/978-3-662-50497-0		Logic	-20.26632359820233	20.280571690743717	145747
1be9dbef25b122bdce14d0fd5cab193e8f066c44	foundational certified code in the twelf metalogical framework	foundational certified code;operational semantics;typed assembly language;satisfiability;logic programming;metalogic;logic programs	Foundational certified code systems seek to prove untrusted programs to be safe relative to safety policies given in terms of actual machine architectures, thereby improving the systems' flexibility and extensibility. Using the Twelf metalogical framework, we have constructed a safety policy for the IA-32 architecture with a trusted runtime library. The safety policy is based on a formalized operational semantics. We have also developed a complete, foundational proof that a fully expressive typed assembly language satisfies that safety policy.	extensibility;formal verification;ia-32;operational semantics;runtime library;twelf;typed assembly language	Karl Crary;Susmit Sarkar	2008	ACM Trans. Comput. Log.	10.1145/1352582.1352584	metalogic;computer science;theoretical computer science;programming language;logic programming;operational semantics;algorithm;satisfiability	PL	-20.96924046878116	27.06488033380444	145807
258486040123ced08ce3f07ab08b51ea9fe13696	programming by non-programmers	computer programs;programing languages;media research;computer science;programers	Non-programmers were asked to organize natural English commands of a laboratory programming language into programs for solving name-sorting problems. The problems differed in the sort concept to be programmed (conjunction vs. disjunction) and in the form of expression of the letter tests to be made on the names (affirmation vs. negation.)  Programming performance was found to be impaired with disjunctive concepts and with letter tests involving negation. Different classes of program structure were identified and were associated with certain problem conditions and error measures. An influence of prior experience with procedures on performance was suggested. Program debugging and testing performance was characterized.		Lance A. Miller	1974	International Journal of Man-Machine Studies	10.1016/S0020-7373(74)80004-0	computer science;artificial intelligence;programming language;algorithm	Arch	-27.856433496467567	23.03654894745706	145886
5d713db39b4e81d33043393783ad31ed6b82e295	frances: a tool for understanding code generation	programming language;code generation;compilers;frances;visualization;target language;high level language	Compiler and programming language implementation courses are integral parts of many computer science curricula. However, the range of topics necessary to teach in such a course are difficult for students to understand and time consuming to cover. In particular, code generation is a confusing topic for students unfamiliar with low level target languages. We present Frances, a tool for helping students understand code generation and low level languages. The key idea is to graphically illustrate the relationships between high level language constructs and low level (assembly) language code. By illustrating these relationships, we take advantage of the students existing understanding of some high level language. We have used Frances in a compiler design course and received highly positive feedback. Students conveyed to us that Frances significantly helped them to understand the concepts necessary to implement code generation in a compiler project.	assembly language;code generation (compiler);compiler;computer science;high- and low-level;high-level programming language;language code;positive feedback;programming language implementation	Tyler Sondag;Kian L. Pokorny;Hridesh Rajan	2010		10.1145/1734263.1734269	compile time;compiler;visualization;computer science;programming language implementation;theoretical computer science;software engineering;compiler construction;low-level programming language;programming language;high-level programming language;code generation;assembly language;source code	PL	-27.951325546720508	23.743121456561088	145887
8d60e24cb47d6d191e2c9032cb510b002adf62da	improving eiffel assertions using quantiefied iterators			eiffel;iterator	Miguel Katrib;Jesüs Coira	1997	JOOP		eiffel;database;programming language;computer science	NLP	-22.805547142693772	21.75727774246274	146363
d0ae6c1ec4338be04b17da318e25fb0d816da6fa	module interconnection languages and prolog		Module interconnection languages are used during system design to represen t the architecture of a software system. The requirements for such language s are outlined and Prolog is proposed as a candidate which satisfies thes e requirements. The system design phase of the software life-cycle consists of takin g the functional specification of a piece of software and devising a gros s architecture for the software which satisfies this specification. It ha s been argued [1] that system design is a separate and distinct activity fro m the detailed design which follows it in the software life-cycle. The forme r is concerned with establishing an overall system architecture ; the latter i s concerned with the processing that occurs in each component of the syste m architecture. Because of the nature of system design it has been argued that a special-purpose language should be used during this phase of the softwar e life-cycle. The requirements of such a module interconnection language an d its processing system are : 9 It should encourage and record the stepwise development of a software system. It should adequately represent the hierarchic structure of softwar e systems. It should be able to represent information required for the automati c • calculation of design metrics such as those outlined in [2] an d It should be possible to query representations expressed in th e • module interconnection language in order to derive maintenanc e information. A system for processing module interconnection languages should b e q able to interface with operating system utilities such as linkers an d loaders. This paper briefly outlines how Prolog [4] is able to meet thes e requirements. Representing Module Interconnection in Prolo g In order to describe how Prolog can be used as a module interconnectio n language it will be necessary to describe how it can be applied to a system expressed in one specific language. The language chosen in this paper i s _3 _	functional specification;interconnection;linker (computing);operating system;programming in the large and programming in the small;prolog;requirement;software release life cycle;software system;stepwise regression;systems architecture;systems design	Darrel C. Ince	1984	SIGPLAN Notices	10.1145/988241.988249	computer science;theoretical computer science;programming language;algorithm	Arch	-32.82332175824743	25.818386569034466	146419
868a34ae87c1bfe371f05c0ec5703a60ec79153e	information-relational semantics of the fifth system		This describes work in progress on the semantics of Fifth, a declarative programming language which obtains efficiency without relying on domain-specific assumptions by means of an adaptive evaluation strategy. The adaptive evaluation strategy demands a consistent way of measuring the progress of a computation in terms of the information yielded by different control paths explored in the course of attempting to evaluate it. The Fifth system’s semantics are situated at the intersection of Icon, Prolog, Radul’s and Sussman’s information propagation networks and Codd’s relational model as developed by Date et. al. which in turn places it somewhere between abstract execution systems for program analysis and existing logic programming tools, and promises to enrich declarative programming with the semantics of relations on answer sets, and may through the consistent use of relations achieve the combination of expressivity and efficiency characteristic of the APL family and its consistent reliance on arrays.	apl;complex adaptive system;computation;declarative programming;fifth-generation programming language;kripke semantics;logic programming;program analysis;programming tool;prolog;relational model;situated;software propagation	Anthony Di Franco	2018				PL	-20.303859591271074	22.623950549273154	146430
5aa170634717db385ed535f9c42b6f6bc7f2a947	recursivity, sequence recursivity, stack recursivity and semantics of programs		Without Abstract	recursion	Giorgio Germano;Andrea Maggiolo-Schettini	1976		10.1007/3-540-07854-1_161	algorithm	Logic	-22.805609836923626	20.757816696368074	146475
d5c9317b88489cf1fc7b0b9e34ec0364346d8e4d	static transition compression	langage imperatif;redundancia;temps lineaire;specification;imperative language;transition;tiempo lineal;lenguaje cs;inference rule;transicion;langage cs;redundancy;especificacion;linear time;compresion;structured programming;programmation structuree;lenguaje imperativo;compression;programmation non structuree;redondance;context sensitive language;programacion estructurada;programme avec saut	Starting from an operational specification of a translation from a structured to an unstructured imperative language, we point out how a compositional and context-insensitive translation gives rise to static chains of jumps. Taking an inspiration from the notion of continuation, we state a new compositional and context-sensitive specification that provably gives rise to no static chains of jumps, no redundant labels, and no unused labels. It is defined with one inference rule per syntactic construct and operates in linear time and space on the size of the source program (indeed it operates in one pass).	basic block;code generation (compiler);context-sensitive grammar;continuation;control flow;functional programming;imperative programming;just-in-time compilation;optimizing compiler;r. kent dybvig;static single assignment form;structured programming;tail call;time complexity	Daniel Damian;Olivier Danvy	2001		10.1007/3-540-44806-3_6	time complexity;imperative programming;computer science;artificial intelligence;transition;redundancy;programming language;structured programming;compression;specification;algorithm;context-sensitive language;rule of inference	PL	-19.4641493393793	23.96475367703126	146763
0d22d7c1f295900f08b3cd00e094304497921753	prototype system for improving manually collected data quality		The starting point of the research was to map out areas and activities of the public sector in which savings could be achieved by controlling, optimizing and intensifying operations. This research is a part of the ongoing two-year (2013-2014) Kiiaudata (Kiinteistöjärjestelmien datan älykäs analysointi – smart analysis of property systems data) project funded by Tekes [2014], where one of the main aims was to study potential new technologies for managing and controlling conditions in buildings in a smart way. In collaboration with the City of Pori, a survey was made about the points where measurement data is collected and also how said data is utilized. As the result of this mapping, it was decided to focus on the upgrading of measurement data collection and the new swimming pool was chosen as the research subject, as it is the city’s most expensive individual building in terms of energy consumption. The idea was that the maintenance staff would continue checking the physical measuring devices to ensure their conditions, but the collected data would be recorded with the developed system in contrast to the fully manual record keeping used in the past (i.e. pen and paper). The measurements produce information that can be used, for example, in consumption and condition tracking. For instance, analyses of alteration in energy consumption can be made by means of inclusive measurement and usage tracking based on it. Electricity, heat and water are examples for different measured energy currents. In many cases, the aforementioned currents can be tracked and anomalous situations can be reported automatically using modern computer controlled systems, but there still remain situations where manual work is required, especially when dealing with legacy systems.	data quality;legacy system;prototype	Jari Soini;Pekka Sillberg;Petri Rantanen	2014			measuring instrument;operations management;data collection;energy consumption;public sector;legacy system;business;data quality	SE	-31.898717212192675	20.112595226858033	146832
537056505f172ae394c93ecb5d09d8c975bd8faf	numeric activex components	numerical optimisation.;microsoft office;activex components;numeric activex component;internet;numerical computation;microsoft windows	This paper is concerned with the use of ActiveX components for numerical computations from within the Microsoft Windows environment. Detailed information is provided concerning the use of these components from Microsoft Excel, Microsoft Visual Basic, Microsoft Web browsers and Inprise Delphi. The examples in the paper are based on the following ActiveX components: a fast Fourier transform ActiveX control, a financial derivative pricing ActiveX control, and a numerical optimisation ActiveX control.	activex;computation;embarcadero delphi;fast fourier transform;mathematical optimization;microsoft windows;numerical analysis;visual basic	G. F. Levy	2001	Softw., Pract. Exper.	10.1002/1097-024X(200102)31:2%3C147::AID-SPE360%3E3.0.CO;2-V	capicom;fast fourier transform;microsoft windows;microsoft visual studio;the internet;visual basic for applications;computer hardware;microsoft office;computer science;derivative;operating system;component object model;vbscript;computer graphics (images)	Robotics	-32.18653970078392	28.138936486809563	147147
d266ed5ac428104f9dbd39cdc81face41451a27a	features of the concurrent programming language aldwych	functional programming;it value;concurrency;object oriented;declarative;single writer multiple reader;concurrent programs;actors	This paper describes a concurrent programming language, Aldwych. The language has a simple operational model where any variable has exactly one process that can write to it but any number that can read it. Once a variable has been written to its value cannot be changed, but a tuple value can be written to a variable, and variables within the tuple written to later. Processes consist of a set of rules which are triggered when variables are given values.We show how this underlying model can be extended using a number of derived forms so that Aldwych has features of imperative, object-oriented and functional programming.	concurrent computing;functional programming;imperative programming;programming language	Matthew M. Huntbach	2003		10.1145/952532.952738	concurrency;computer science;theoretical computer science;database;programming language;object-oriented programming;functional programming;algorithm;concurrent object-oriented programming	PL	-25.75135056376885	26.895769133873756	147350
17adce6e9965e4c7f4b0f9aa0b2bf68297e2444a	a first analysis of string apis: the case of pharo	api;style;strings;design;library	Most programming languages natively provide an abstraction of character strings. However, it is difficult to assess the design or the API of a string library. There is no comprehensive analysis of the needed operations and their different variations. There are no real guidelines about the different forces in presence and how they structure the design space of string manipulation. In this article, we harvest and structure a set of criteria to describe a string API. We propose an analysis of the Pharo 4 String library as a first experience on the topic.	application programming interface;pharo;programming language;string (computer science)	Damien Pollet;Stéphane Ducasse	2015		10.1145/2811237.2811298	string operations;string interpolation;string literal;string;string interning;computer science;theoretical computer science;connection string;c string handling;scanf format string;world wide web;algorithm	SE	-27.372935440250284	22.116301967546658	147550
4e585a9f422b10c5f7b22d0a12d150c982be542f	designa and implementation of simple object description language	application development;design and implementation;incremental language design;domain specific;languages;language design;domain specificity;attrubute grammars	In the paper a design and implementation of Simple Object Description Language SODL for automatic interface creation are presented. First, problem domain – developing network applications and reasons for developing new domain-specific language are described. Since the cross network method calls slow down performance of our applications the solution was Tier to Tier Object Transport (TTOT). However, with this approach the network application development time has been increased. To enhance our productivity a new domain-specific SODL language has been designed. Syntax and semantics of SODL language are formally defined in an incremental way by special kind of attribute grammars that allows extensions and modifications in an easy way. From formal specifications SODL compiler is automatically generated using compiler/interpreter generator tool LISA. Finally, the benefits of our approach have been discussed.	attribute grammar;compiler;domain-specific language;lisa;multitier architecture;object description language;problem domain	Marjan Mernik;Uros Novak;Enis Avdicausevic;Viljem Zumer	2001		10.1145/372202.372468	natural language processing;universal networking language;language primitive;object language;specification language;idef4;computer science;programming language implementation;domain-specific language;design language;low-level programming language;programming language;rapid application development;management	PL	-27.630008243269007	26.49909499309929	147568
87b7b4a595c6a9f9407c16a7bd3936950b888ee4	runtime assertion checking and its combinations with static and dynamic analyses - tutorial synopsis		Among various static and dynamic software verification techniques, runtime assertion checking traditionally holds a particular place. Commonly used by most software developers, it can provide a fast feedback on the correctness of a property for one or several concrete executions of the program. Quite easy to realize for simple program properties, it becomes however much more complex for complete program contracts written in an expressive specification language. This paper presents a one-hour tutorial on runtime assertion checking in which we give an overview of this popular dynamic verification technique, present its various combinations with other verification techniques (such as static analysis, deductive verification, test generation, etc.) and emphasize the benefits and difficulties of these combinations. They are illustrated on concrete examples of C programs within the Frama-C software analysis framework using the executable specification language E-ACSL.	a new kind of science;ansi/iso c specification language;assertion (software development);correctness (computer science);deductive database;executable;frama-c;software developer;software verification;static program analysis;video synopsis	Nikolai Kosmatov;Julien Signoles	2014		10.1007/978-3-319-09099-3_13	programming language	SE	-19.221468010693965	27.844390124672287	147687
5459cd31ff0f182da81d5c58026546b995118676	the s lam project: debugging system software via static analysis	automated deduction;debugging;puesta a punto programa;interconnection;analisis sistema;integrated circuit;programa control;automatisation;circuito integrado;automatizacion;specification language;analisis programa;debogage;interconexion;refinement method;model checking;projet slam;interconnexion;checking program;programme controle;system analysis;analyse systeme;lenguaje especificacion;program analysis;methode raffinement;analyse programme;static analysis;metodo afinamiento;langage specification;device driver;circuit integre;automation	"""The goal of the SLAM project is to check whether or not a program obeys """"API usage rules"""" that specify what it means to be a good client of an API. The SLAM toolkit statically analyzes a C program to determine whether or not it violates given usage rules. The toolkit has two unique aspects: it does not require the programmer to annotate the source program (invariants are inferred); it minimizes noise (false error messages) through a process known as """"counterexample-driven refinement"""". SLAM exploits and extends results from program analysis, model checking and automated deduction. We have successfully applied the SLAM toolkit to Windows XP device drivers, to both validate behavior and find defects in their usage of kernel APIs."""	application programming interface;automated theorem proving;debugging;device driver;error message;invariant (computer science);microsoft windows;model checking;natural deduction;programmer;refinement (computing);slam project;simultaneous localization and mapping;static program analysis;tag (game)	Thomas Ball;Sriram K. Rajamani	2002		10.1145/503272.503274	program analysis;model checking;embedded system;specification language;computer science;operating system;integrated circuit;interconnection;automation;system analysis;programming language;debugging;static analysis;algorithm	SE	-23.642781728365087	31.195829653170435	147734
e19d0f2124531a1353e3f21a2923cef354364aae	computer aided verification		ion and Refinement Model-Checking with Formula-Dependent Abstract Models . . . . . . . . . . . . . . 155 Alexander Asteroth, Christel Baier, Ulrich Aßmann Verifying Network Protocol Implementations by Symbolic Refinement Checking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169 Rajeev Alur, Bow-Yaw Wang Automatic Abstraction for Verification of Timed Circuits and Systems . . . . 182 Hao Zheng, Eric Mercer, Chris Myers	algorithm;automata theory;computer aided verification;long division;method of analytic tableaux;modal μ-calculus;model checking;proof calculus;rajeev alur;requirement;symbolic linguistic representation;termination analysis;yaws;eric	Abhik Roychoudhury;I. V. Ramakrishnan	2001		10.1007/3-540-44585-4		Logic	-20.274629620542985	18.845320875056256	147834
ed2f8e942584a16bf0fd9d954502bcab370708c9	bridging csp and c++ with selective formalism and executable specifications	algebraic specification;hardware software codesign;executable specification;programming language;communicating sequential process;c language algebraic specification communicating sequential processes;formal method;c language;communicating sequential processes;formal verification;model checking;concurrent systems;hardware software codesign c language selective formalism executable specification communicating sequential processes algebraic notation hierarchical behavioral specification concurrent system formal interprocess synchronization communication semantics csp specification simulation formal verification model checking tool programming language system control specification c module abstract event csp automatic translation formal method;computer languages hardware communication system control automatic control spine concurrent computing information science computational modeling formal verification control systems	"""CSP (Communicating Sequential Processes) is a usefulalgebraic notation for creating a hierarchical behaviouralspecification for concurrent systems, due to its formalinterprocess synchronization and communication semantics.CSP specifications are amenable to simulation andformal verification by model-checking tools. To overcomethe drawback that CSP is neither a full-featured nor popularprogramming language, an approach called """"selectiveformalism"""" allows the use of CSP to be limited to specifyingthe control portion of a system, while the rest of itsfunctionality is supplied in the form of C++ modules.These are activated through association with abstractevents in the CSP specification. The target system is constructedusing a framework called CSP++, which automaticallytranslates CSP specifications into C++, therebymaking CSP directly executable. Thus a bridge is built thatallows a formal method to be combined with a popularprogramming language. It is believed that this methodologycan be extended to hardware/software codesign."""	bridging (networking);c++;communicating sequential processes;concurrency (computer science);executable;formal methods;model checking;semantics (computer science);simulation	William B. Gardner	2003	First ACM and IEEE International Conference on Formal Methods and Models for Co-Design, 2003. MEMOCODE '03. Proceedings.	10.1109/MEMCOD.2003.1210108	model checking;real-time computing;formal methods;specification language;formal verification;computer science;theoretical computer science;communicating sequential processes;formal specification;programming language;programming language specification;language of temporal ordering specification	SE	-32.81520386016636	32.02088565109097	147872
e0e5266f97d3a960b96baac8eda53200a81403cd	short paper: towards information flow reasoning about real-world c code		Strangely, despite much recent success proving information flow control (IFC) security for C programs, little work has investigated how to prove IFC security directly against C code, as opposed to over an abstract specification. We consider what a suitable IFC logic for C might look like, and propose a suitable continuation-passing style IFC security definition for C code. We discuss our ongoing work implementing these ideas in the context of an existing full-featured, sound program verification framework for C, the Verified Software Toolchain, supported by the verified C complier CompCert.	compcert;continuation;continuation-passing style;formal verification;information flow (information theory);non-interference (security);toolchain	Samuel Grütter;Toby C. Murray	2017		10.1145/3139337.3139345	toolchain;theoretical computer science;information flow (information theory);software;separation logic;computer science	Security	-20.668008655488936	27.45837600726105	148004
a7be64de0129f5d6188dfe21c71c48e5ad6dcef0	implementations of the csp notation for concurrent systems	proceso secuencial comunicante;sistema operativo;lenguaje de programacion;programming language;communicating sequential process;concurrent program;systeme non deterministe;synchronisation;non deterministic system;concurrent systems;operating system;synchronization;programa competidor;langage programmation;systeme exploitation;sincronizacion;sistema no determinista;processus sequentiel communiquant;programme concurrent		concurrency (computer science)	M. Elizabeth C. Hull	1986	Comput. J.	10.1093/comjnl/29.6.500	synchronization;real-time computing;computer science;programming language;algorithm	Logic	-24.275010499555083	31.791642291730305	148015
83aa346ee07c1870a0322db2abd7bdc3d2569b1d	generalization in hierarchies of online program specialization systems	formal specification;program transformation;logical programming;transformation programme;functional programming;analisis programa;program specialization;specification formelle;especificacion formal;transformacion programa;programmation logique;metaprogrammation;programmation fonctionnelle;program analysis;analyse programme;metaprogramming;functional language;programacion logica;programacion funcional;metaprogramacion	In recent work, we proposed a simple functional language S-graph-n to study metaprogramming aspects of self-applicable online program specialization. The primitives of the language provide support for multiple encodings of programs. An important component of online program specialization is the termination strategy. In this paper we show that such a representation has the great advantage of simplifying generalization of multiply encoded data. After developing and formalizing the basic metaprogramming concepts, we extend two basic methods to multiply encoded data: most speciic generalization and the homeomorphic embedding relation. Examples and experiments with the initial design of an online specializer illustrate their use in hierarchies of online program specializers.	experiment;functional programming;metaprogramming;partial template specialization;s-graph	Robert Glück;John Hatcliff;Jesper Jørgensen	1998		10.1007/3-540-48958-4_10	program analysis;metaprogramming;computer science;artificial intelligence;theoretical computer science;formal specification;mathematics;programming language;functional programming;algorithm	ML	-20.768059856785356	23.455379207548944	148208
fa7de8aa78e359ccd698cd7c04f9bbedbb0d651e	introducting data decomposition into vdm for tractable development of programs	development strategy;persistence;tagged types;object oriented programming;ada 9x;dynamic binding;eiffel;classes;c;inheritance;program development	"""Formal program development by VDM comprises not only data reifications from abstract model to concrete model but also suitable operation decompositions. Usually, these two kinds of development steps are done independently with the order of """"operation decompositions after data reifications"""". Furthermore, data reifications and operation decompositions are based on the whole model because no model split is allowed. If, however, larger specifications are aimed at, it is very important to provide support for model split and suitably interweave it with data reification and operation decomposition. In this paper, we introduce a new concept---data decomposition which is based on the ideas of model split, modularisation and operation decomposition, and combine it with VDM to form a more general formal development method DD-VDM. As a result, a more flexible development strategy can be adopted and the development complexity can be effectively controlled."""	cobham's thesis;dd (unix);formal methods;precondition;reification (knowledge representation);vienna development method	Jian Lu	1995	SIGPLAN Notices	10.1145/214448.214460	persistence;vienna development method;computer science;eiffel;theoretical computer science;class;programming language;object-oriented programming;algorithm	SE	-26.94162143407994	21.262531743354174	148229
8544f8a747d6c4816f63dda1d22d30818fdc39bb	linear objects: logical processes with built-in inheritance	developpement logiciel;dynamic change;lenguaje programacion;concurrent logic programming;programming language;programming paradigm;simultaneidad informatica;ingenieria logiciel;logical programming;object oriented programming;software engineering;expressive power;search trees;concurrency;side effect;programmation logique;object oriented;desarrollo logicial;software development;genie logiciel;langage programmation;oriente objet;classical logic;logic programs;programacion logica;linear logic;simultaneite informatique;orientado objeto;logical process;dynamic behavior	We present a new framework for amalgamating two successful programming paradigms: logic programming and object-oriented programming. From the former, we keep the delarative reading of programs. From the latter, we select two crucial notions: (i) the ability for objects to dynamically change their internal state during the computation; (ii) the structured representation of knowledge, generally obtained via inheritance graphs among classes of objects. We start with the approach, introduced in concurrent logic programming languages, which identifies objects with proof processes and object states with arguments occurring in the goal of a given process. This provides a clean, side-effect free account of the dynamic behavior of objects in terms of the search tree—the only dynamic entity in logic programming languages. We integrate this view of objects with an extension of logic programming, which we call Linear Objects, based on the possibility of having multiple literals in the head of a program clause. This contains within itself the basis for a flexible form of inheritance, and maintains the constructive property of Prolog of returning definite answer substitutions as output of the proof of non-ground goals. The theoretical background for Linear Objects is Linear Logic, a logic recently introduced to provide a theoretical basis for the study of concurrency. We also show that Linear Objects can be considered as constructive restriction of full Classical Logic. We illustrate the expressive power of Linear Objects compared to Prolog by several examples from the object-oriented domain, but we also show that it can be used to provide elegant solutions for problems arising in the standard style of logic programming.	answer set programming;canonical account;computation;concurrency (computer science);concurrent logic programming;expressive power (computer science);linear logic;programming language;programming paradigm;prolog;search tree	Jean-Marc Andreoli;Remo Pareschi	1991	New Generation Computing	10.1007/BF03037173	concurrent constraint logic programming;linear logic;constraint programming;protocol;declarative programming;chain-of-responsibility pattern;horn clause;computer science;artificial intelligence;theoretical computer science;computational logic;class-based programming;subject-oriented programming;programming paradigm;inductive programming;fifth-generation programming language;programming language;object-oriented programming;prolog;logic programming;algorithm;concurrent object-oriented programming	PL	-21.013823490671637	22.79152364765751	148393
1c533f06bf02977e2fac69c44f55ac0f0f7a8475	visualisation of device datasets to assist digital forensic investigation		The increasing use of digital devices in our everyday lives, and their ever-increasing storage capacities places digital forensics investigatory resources under significant pressure. The workload for investigators is increasing, and the time required to analyse the datasets is not decreasing to compensate. This research looks at the potential for utilising information visualisation techniques to increase investigative efficiency with a view to decreasing the overall time taken to investigate a case, while still maintaining a high level of accuracy. It is envisaged that this may have the potential to lead to a reduced backlog of cases for law enforcement agencies, and expedited processing of criminal cases involving digital evidence.		Gavin Hales	2017	2017 International Conference On Cyber Situational Awareness, Data Analytics And Assessment (Cyber SA)	10.1109/CyberSA.2017.8073402	workload;visualization;law enforcement;data mining;digital forensics;information visualization;digital evidence;engineering	DB	-32.512261184784016	20.16317063904746	148498
355e1e985a3607cd8c45f909f64b718d5d1a5a6f	one-pass code generation using continuations	lenguaje programacion;optimisation;compilateur;optimizacion;langage c;programming language;generation code;generacion codigo;code generation;contextual information;tratamiento lenguaje;compiler;boolean expression;c language;language processing;expresion booleana;expression booleenne;traitement langage;langage programmation;optimization;modula 2;compilador;lenguaje c	Translation schema are described, using the functional notation of Peyton Jones, which make possible the generation of high quality code for conditional formulas and boolean expressions. These use contextual information describing the continuation of evaluation after that of the formula being translated. The technique can be used for the incremental improvement of a code generator, leading eventually to one that never generates jumps to jumps and making it possible to optimise target machine instructions that have been described in the literature as 'logically adjacent'. The method is efficient and can be applied to conventional programming languages such as C and Modula 2.	boolean expression;code generation (compiler);continuation;display resolution;jones calculus;modula;modula-2;programming language	Keith Clarke	1989	Softw., Pract. Exper.		modula-2;compiler;boolean expression;computer science;theoretical computer science;programming language;algorithm;code generation	PL	-24.64263049128879	24.362343746162377	148767
742a4e12c39005ac794812202972eed974279f84	a framework for higher-order functions in c++	object oriented;higher order functions;functional programming language;support function	C and C++ allow passing functions as arguments to other functions in the form of function pointers. However, since function pointers can refer only to existing functions declared at global or file scope, these function arguments cannot capture local environments. This leads to the common misconception that C and C++ do not support function closures. In fact, function closures can be modeled directly in C++ by enclosing a function inside an object such that the local environment is captured by data members of the object. This idiom is described in advanced C++ texts and is used, for example, to implement callbacks. The purpose of this paper is twofold: First, we demonstrate how this idiom can be generalized to a type-safe framework of C++ class templates for higher-order functions that support composition and partial application. Second, we explore the expressiveness of the framework and compare it with that of existing functional programming languages. We illustrate by means of various examples that object-oriented and functional idioms can coexist productively and can be used to enhance the functionality of common classes, for example, of nonlinear collections such as trees. A C++ implementation of the framework is available on request.	algol;c++ classes;callback (computer programming);closure (computer programming);coexist (image);compiler;function pointer;functional programming;functoid;garbage collection (computer science);generic programming;higher-order function;local variable;machine-dependent software;memory management;nonlinear system;on the fly;operator overloading;operators in c and c++;partial application;pascal;programmer;programming language;scope (computer science);thunk;type safety	Konstantin Läufer	1995			run-time type information;computer science;theoretical computer science;c mathematical functions;inline function;policy-based design;programming language;algorithm;reference	PL	-24.215900508682722	27.365004759082687	148903
4c9dd5c75569d7bc0531d71aa309504ba965ac76	typestate-like analysis of multiple interacting objects	sensibilidad contexto;modele reference;lenguaje programacion;semantica operacional;theorie type;context aware;mise a jour;tracematches;analyse statique;programming language;reference model;operational semantics;individual object;program verification;analisis estatica;actualizacion;verificacion programa;enrejado;semantique operationnelle;treillis;type theory;control flow;langage programmation;sensibilite contexte;static analysis;verification programme;verification typestate;updating;typestate;lattice;modelo referencia	This paper presents a static analysis of typestate-like temporal specifications of groups of interacting objects, which are expressed using tracematches. Whereas typestate expresses a temporal specification of one object, a tracematch state may change due to operations on any of a set of related objects bound by the tracematch. The paper proposes a lattice-based operational semantics equivalent to the original tracematch semantics but better suited to static analysis. The paper defines a static analysis that computes precise local points-to sets and tracks the flow of individual objects, thereby enabling strong updates of the tracematch state. The analysis has been proved sound with respect to the semantics. A context-sensitive version of the analysis has been implemented as instances of the IFDS and IDE algorithms. The analysis was evaluated on tracematches used in earlier work and found to be very precise. Remaining imprecisions could be eliminated with more precise modeling of references from the heap and of exceptional control flow.	algorithm;context-sensitive grammar;control flow;interaction;operational semantics;static program analysis	Nomair A. Naeem;Ondrej Lhoták	2008		10.1145/1449764.1449792	reference model;computer science;artificial intelligence;lattice;programming language;control flow;operational semantics;type theory;static analysis;algorithm	PL	-19.870041182044623	28.604800885098506	148965
4662ad7f06a14486ae3cd4aacf230e4dc4b776e2	one-pass, optimal tree parsing - with or without trees	bottom up;code generation	This paper describes the theory behind and implementation of wburg, a code-generator generator that accepts tree grammars as input and produces a code generator that emits an optimal parse of an IR tree in just a single bottom-up pass. Furthermore, wburg eliminates the need for an explicit IR tree altogether. The grammars that wburg-generated parsers can parse are a proper subset of those that two-pass systems can handle. However, analysis indicates that wburg can optimally handle grammars for most instruction sets (e.g., SPARC, MIPS R3000, and x86).	bottom-up parsing;code generation (compiler);r3000;sparc;x86	Todd A. Proebsting;Benjamin R. Whaley	1996		10.1007/3-540-61053-7_69	tree rotation;computer science;top-down and bottom-up design;programming language;code generation	NLP	-24.34136377121951	24.566915834714354	148980
7943b1d9217f85916fb87556140fb15a90d3647a	a tutorial implementation of a dependently typed lambda calculus	lambda calculus;data type;dependent types;electronic computers computer science;typed lambda calculus	We present the type rules for a dependently-typed core calculus together with a straightforward implementation in Haskell. We explicitly highlight the changes necessary to shift from a simplytyped lambda calculus to the dependently-typed lambda calculus. We also describe how to extend our core language with data types and write several small example programs. The paper is accompanied by an executable interpreter and example code that allows immediate experimentation with the system we describe.	dependent type;executable;haskell;simply typed lambda calculus;type rule	Andres Löh;Conor McBride;Wouter Swierstra	2010	Fundam. Inform.	10.3233/FI-2010-304	lambda lifting;system f;deductive lambda calculus;fixed-point combinator;calculus of constructions;typed lambda calculus;dependent type;binary lambda calculus;normalisation by evaluation;pure type system;data type;computer science;theoretical computer science;lambda calculus;simply typed lambda calculus;curry–howard correspondence;hindley–milner type system;programming language;church encoding;lambda cube;higher-order function;generalized algebraic data type;type inhabitation;algorithm	PL	-22.424191850572974	23.697850805801	149080
40ce74a4d3626ec18309b71e700a7fce4c015bd7	signal: a declarative language for synchronous programming of real-time systems	condition dependence;synchronous programming;dependence analysis;signal processing;declarative languages;multiprocessor architecture;point of view;data flow;real time systems	We present an applicative language, SIGNAL, designed to program real-time systems. The language is based on a synchronous notion of time. We assume the execution of operations to have a zero logical time duration; then, the sequence of communication events determines entirely a temporal reference. The ordering of the runable operations is limited only by the dependencies between the calculi : this is the point of view of data flow languages. SIGNAL is a data flow language (where the potential parallelism is implicit), which permits a structural description of interconnected processes. SIGNAL handles possibly infinite sequences of values (called signals) characterized by an implicit clock which specifies the relative instants (with respect to other signals) at which these values are available. Specific operators, such as delay, undersampling, deterministic merge, are designed to express temporal relations between different signals : in this way, a SIGNAL program expresses both functional and temporal relationships between all the involved signals. The language is semantically sound, and its declarative style allows to derive, by a simple projection on the commutative field Z/3Z, a complete static calculus of the timing of any SIGNAL process, called its clock calculus. Hence, the language SIGNAL is also a formal system to reason about timing and concurrency. The clock calculus is completed together with the dependency analysis of a given program. This leads to a conditional dependence graph in which the edges may be labelled by the involved clocks. From this graph, we generate code for a sequential machine, but it appears to be the suitable level to study the implementation on a multiprocessor architecture.	declarative programming;real-time computing;real-time transcription;synchronous programming language	Thierry Gautier;Paul Le Guernic	1987		10.1007/3-540-18317-5_15	data flow diagram;parallel computing;real-time computing;declarative programming;computer science;signal programming;signal processing;programming paradigm;inductive programming;fifth-generation programming language;programming language;dependence analysis	Embedded	-26.49460854805425	32.1265621025145	149211
902d7782f06ef084ee2d2bbe8592315c0e1e492e	safe structural conformance for java	virtual machine;conformal structure;java programming;object oriented programming;remote method invocation;java development kit	In Java, an interface specifies public abstract methods and associated public constants. Conformance of a class to an interface is by name. We propose to allow structural conformance to interfaces: Any class or interface that declares or implements each method in a target interface conforms structurally to the interface, and any expression of the source class or interface type can be used where a value of the target interface type is expected. We argue that structural conformance results in a major gain in flexibility in situations that require retroactive abstraction over types. Structural conformance requires no additional syntax and only small modifications to the Java compiler and optionally, for performance reasons, the virtual machine, resulting in a minor performance penalty. Our extension is type-safe: A cast-free program that compiles without errors will not have any type errors at run time. Our extension is conservative: Existing Java programs still compile and run in the same manner as under the original language definition. Finally, structural conformance works well with recent extensions such as Java remote method invocation. We have implemented our extension of Java with structural interface conformance by modifying the Java Developers Kit 1.1.5 source release for Solaris and Windows 95/NT. We have also created a test suite for the extension. Konstantin Läufer* Gerald Baumgartner** Vincent F. Russo*** Safe Structural Conformance for Java Page 2 of 19	carder.su;compile time;conformance testing;indirection;java classloader;java compiler;java remote method invocation;microsoft windows;overhead (computing);run time (program lifecycle phase);software development kit;subroutine;test suite;thomas baumgartner;type safety;type system;virtual machine;windows 95	Konstantin Läufer;Gerald Baumgartner;Vincent F. Russo	2000	Comput. J.	10.1093/comjnl/43.6.469	java data objects;java card;java api for xml-based rpc;method;real-time computing;jsr 94;java concurrency;application programming interface;jar;computer science;virtual machine;operating system;java modeling language;interface;strictfp;embedded java;real time java;reflection;programming language;object-oriented programming;java;generics in java;scala;java applet;java annotation;non-blocking i/o	PL	-26.188198537080332	29.88404495780017	149261
6988acb9d2302b40e2eb75008277d7385e52b5b1	application of theorem proving methods for automatic program synthesis for n. c. machine tools.	automatic programming;theorem proving;machine tool			Andrzej M. Goscinski;Tadeusz Szuba	1981	Angewandte Informatik		computer science;theoretical computer science;machine tool;automated theorem proving;algorithm	Logic	-20.151667883405697	19.44767075014624	149480
5dd030feeea68eb4e782ef3d24db404d144da3cb	an optimising compiler for a modern functional language	langage fonctionnel;lenguaje programacion;optimisation;compilateur;optimizacion;programming language;lenguaje funcional;tratamiento lenguaje;compiler;language processing;traitement langage;langage programmation;parallel machines;optimization;functional language;compilador	ion (detailed summary). In Proceedings 1986 ACM Conference on LISP and Functional Programming, pp. 351-363. ACM (August, 1986). 13. P. Hudak, A. Bloss and J. Young, Code optimizations for lazy evaluation. Lisp and Symbolic Computation: An International Journal 1 (2), 147-1 64 (1988). And a s always, we thank the 'grapplers' a t Yale for their never-ending support. 14. P. Hudak and J. Young, A collecting interpretation of expressions (without power domains). In Proceedings of ACM Symposium on Principles of Programming Languages, pp. 107-1 18 (January, 1988). 15. P. Hudak and J. Young, Higher-order strictness analysis for untyped lambda calculus. In 12th ACM Symposium on Principles of Programming Languages, pp. 97-1 09 (January, 1986). 16. R. J. M. Hughes, Super-combinators: a new implementation method for applicative languages. In Proceedings 1982 ACM Conference on LISP and Functional Programming, pp. 1-10. ACM (August, 1982). 17. D. Kranz, ORBIT: an optimizing compiler for scheme. Ph.D. thesis, Yale University, Department of Computer Science (1988). Available as technical report YALEUI	applicative programming language;combinatory logic;computer science;higher-order and symbolic computation;international conference on functional programming;lambda calculus;lazy evaluation;lisp;optimizing compiler;power domains;strictness analysis;symposium on principles of programming languages	Adrienne G. Bloss;Paul Hudak;Jonathan Young	1989	Comput. J.	10.1093/comjnl/32.2.152	compiler;computer science;theoretical computer science;compiler construction;programming language;functional programming;second-generation programming language;algorithm	PL	-23.902145676625842	22.37090286291497	149696
c0d2a9151f61b41da4838ace879c7b8e660c43f7	internal versus external dsls for trace analysis - (extended abstract)	trace analysis;domain specific language dsl;tracecontract;internal dsl;scala;external dsl;run time verification	This tutorial explores the design and implementation issues arising in the development of domain-specific languages for trace analysis. It introduces the audience to the general concepts underlying such special-purpose languages building upon the authors’ own experiences in developing both external domainspecific languages and systems, such as EAGLE, HAWK, RULER and LOGSCOPE, and the more recent internal domain-specific language and system TRACECONTRACT within the SCALA language.	domain-specific language;scala	Howard Barringer;Klaus Havelund	2011		10.1007/978-3-642-29860-8_1	real-time computing;computer science;programming language;scala	PL	-28.355595624433086	29.621597069929564	149763
a647c80405b0446d22b8de0114bb585b84f14eaa	eaop: an aspect oriented programming framework for erlang		Aspect oriented programming (AOP) is a paradigm ideal for defining cross-cutting concerns within an existing application. Although several AOP frameworks exist for more renowned languages such as Java and C#, little to no frameworks have been developed for actor oriented languages such as Erlang. We thus present eAOP, a new AOP framework specifically designed to instrument actor-oriented constructs in Erlang such as message sends and receives, along with other traditional constructs such as function calls.	application programming interface;aspect-oriented programming;aspectj;compiler;cross-cutting concern;dynamorio;erlang (programming language);exception handling;instrumentation (computer programming);java;malware;open-source software;pointcut;programming paradigm;prototype;software development;surround sound;visual intercept	Ian Cassar;Adrian Francalanza;Luca Aceto;Anna Ingólfsdóttir	2017		10.1145/3123569.3123570	programming language;erlang (programming language);aspect-oriented programming;concurrency;java;computer science;instrumentation (computer programming)	SE	-27.10900146006781	30.180265198959127	149818
a7cedef768092eb5c706ec4ccc3cf4d81284cb33	a formal framework for feature interaction with emphasis on testing		In this paper we present a formal framework for feature interaction. The framework consists of three levels: the requirement level, the speciication level, and the implementation level. At the requirement level we deene interaction as inconsistency between feature requirements. At the speciication and implementation level we deene interaction as a relative notion. Interaction at the specii-cation level is deened relative to the requirements the feature speciications are expected to satisfy. Interaction at the implementation level is deened relative to both the requirements and the speciications the implementations of the features are supposed to realize. We identify an interaction as originating from one (and only one) of the three levels. Also, we show that interactions may be inherited from the requirement level to the speciication level and from the speciication level to the implementation level. The nal part of the paper contains an outline of how absence of feature interaction can be tested for at the implementation level.	feature interaction problem;requirement	Jens Chr. Godskesen	1995			machine learning;computer science;artificial intelligence	SE	-33.04625838439516	30.3918596596218	149826
e4ef4c830ecbe3511c5b415bffb7e96fef49c8cf	a semantic model of types for applicative languages	prolog;lambda calculus;functional programming;higher order;semantic model;extensible languages;logic programming;equational programming;polymorphism	If integer constants are added to the syntax of the pure lambda calculus, then primitive integer values have to be added to the underlying domain V of values. Unlike functions, primitive values should not be applied; we want a run-time error to occur if an attempt is made to apply them as functions. Expressions that might lead to run-time errors are separated out by imposing a “type” structure on expressions. A systematic model of types is developed, in which types are formalized as “ideals” (sets with a certain structure). Polymorphic functions are handled by introducing a quantifier for taking conjunctions of types. Operations for constructing new types from old lead to the consideration of higher-order or meta types, which are called “kinds” to avoid confusion with types. Finally, the semantic model of types is applied to show the soundness of a proof system for inferring the types of expressions.	applicative programming language;lambda calculus;proof calculus;quantifier (logic);run time (program lifecycle phase)	David B. MacQueen;Ravi Sethi	1982		10.1145/800068.802156	semantic data model;polymorphism;typed lambda calculus;higher-order logic;computer science;lambda calculus;type constructor;programming language;functional programming;prolog;logic programming;algorithm;product type	PL	-22.739995100913962	25.339257764178754	149828
5ad934e9461b5973dc63430de00538158209805a	tapir: a language for verified os kernel probes	type systems;dependent types;kernel probes	Kernel probes allow code to be inserted into a running operating system kernel to gather information for debugging or profiling. Inserting code into the kernel raises a number of safety issues. Current solutions follow one of the two paths: a VM-based approach, where safety properties are checked dynamically by an interpreter, or a static-analysis approach, where probe code is guaranteed to be safe statically. While more attractive, existing static solutions depend on ad-hoc and error-prone analysis. We propose to explore enforcing safety properties using a type system, thus building our analysis on top of the well-studied ground of type theory.	operating system	Ilya Yanok;Nathaniel Nystrom	2015		10.1145/2818302.2818303	embedded system;real-time computing;computer science;theoretical computer science;kernel preemption	OS	-21.979094106567054	31.920004933061307	150042
295075303b3943407014a817cb6b585a82936a4a	efficient compilation of linear recursive functions into object level loops	hierarchical structure;functional form;programming language;derived equivalence;linear functionals;functional programming language;large classes	Whilst widely recognised as an excellent means for solving problems and for designing software, functional programming languages have suffered from their inefficient implementations on conventional computers. A route to improved runtime performance is to transform recursively defined functions into programs which execute more quickly and/or consume less space. We derive equivalent imperative programming language loops for a large class of linear recursive functions of which the tail-recursive functions form a very small subset. We first identify a small set of primitive function defining expressions for which we determine the corresponding loop-expressions. We then determine the loop-expressions for linear functions defined by any expressions which are formed from those primitives. In this way, a very general class of linear functions can be transformed automatically into loops in the parsing phase of a compiler, since the parser has in any case to determine the hierarchical structure of function definitions. Further transformation may involve specific properties of particular defining expressions, and adopt previous schemes. In addition, equivalent linear functions can be found for many non-linear ones which can therefore also be transformed into loops.	bil herd;common look and feel;compile time;compiler;computer;functional programming;hope (programming language);imperative programming;linear algebra;linear function;nonlinear system;parsing;predicate transformer semantics;programming language;recursion (computer science);recursive definition;regular expression;rewriting;run time (program lifecycle phase);tail call;transformers;while	Peter G. Harrison;Hessam Khoshnevisan	1986		10.1145/12276.13332	linear-fractional programming;computer science;theoretical computer science;programming language;functional programming;higher-order function;algorithm	PL	-22.03203610465146	25.464498722524656	150069
8935ba7dc287f63650c385b83e050a8cb42b0417	deep semantics of visual languages	shallow and deep semantics of schemes;higher order;semantics of visual languages;visual language;software synthesis;dynamic evaluation of attributes;higher order attribute models	Visual specification of software is gaining popularity, but its usage is restricted by the lack of precise semantics of visual languages. In the present work, precise semantics of visual languages is defined. Three kinds of deep semantics of schemes are presented as different ways of usage of attribute models of schemes. Attribute models of a wide class of schemes are defined and higher-order attribute models are introduced. Dynamic evaluation of attributes is used in defining semantics of schemes.	algorithm;attribute grammar;attribute-value system;computational problem;consistency model;interpreter (computing);programming tool;scalability;software propagation;time complexity	Pavel Grigorenko;Enn Tyugu	2006			natural language processing;higher-order logic;formal semantics;failure semantics;computer science;formal semantics;database;programming language;well-founded semantics;operational semantics;denotational semantics;computational semantics	SE	-26.488760249297773	21.60475756485643	150378
3d12b19d14d12508115414793cad27c2d6d9be1c	abstract domains for type juggling	type conversions;php;static analysis;abstract interpretation	Abstract Web scripting languages, such as PHP and JavaScript, provide a wide range of dynamic features that make them both flexible and error-prone. In order to prevent bugs in web applications, there is a sore need for powerful static analysis tools. In this paper, we investigate how Abstract Interpretation may be leveraged to provide a precise value analysis providing rich typing information that can be a useful component for such tools. In particular, we define the formal semantics for a core of PHP that illustrates type juggling , the implicit type conversions typical of PHP, and investigate the design of abstract domains and operations that, while still scalable, are expressive enough to cope with type juggling. We believe that our approach can also be applied to other languages with implicit type conversions.	abstract interpretation;aliasing;computer science;php;semantics (computer science);string (computer science)	Vincenzo Arceri;Sergio Maffeis	2017	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2017.02.003	real-time computing;simulation;computer science;static analysis;algorithm	Logic	-26.551894704955302	28.212252738545608	150401
a0a6301a239519f117ca6c0398d5230ff15c67ff	tutorial: introdução à visão computacional usando opencv		Resumo: Este tutorial apresenta conceitos introdutórios de processamento de imagens (filtros) e de visão computacional (segmentação, classificação, reconhecimento de padrão e rastreamento). Estes conceitos serão introduzidos utilizando a biblioteca OpenCV, que é distribuída gratuitamente e possui documentação farta na internet, com exemplos e aplicações práticas. Será mostrado como obter e como instalar a ferramenta para diversos tipos de plataformas e linguagens de desenvolvimento. Os conceitos de processamento de imagens e visão computacional serão discutidos não apenas no aspecto teórico, mas também serão apresentados exemplos de implementação para que os leitores possam entender e utilizar os exemplos apresentados neste tutorial. Abstract: This tutorial presents basic concepts in Image Processing (filtering) and Computer Vision (segmentation, classification, pattern recognition and tracking). These concepts are presented using the OpenCV library, which is free and has large documentation in the Internet including examples and practical applications. It will be shown how to get and install the library for different operating systems and development environments. This tutorial presents theoretical aspects and their implementations using OpenCV so readers can understand and use the library afterwords.	computer vision;documentation;image processing;multi-agent system;numerical aperture;opencv;operating system;pattern recognition;web typography	Maurício Marengoni;Denise Stringhini	2009	RITA		simulation;operating system;computer science	Vision	-33.43321298778364	24.215907502028728	150432
50f2bbe56e88fd2f8b48a41e4af56d567a8d0d69	extending context-oriented programming to new application domains: run-time adaption support for java			java	Malte Appeltauer	2012				HCI	-30.02403503019734	27.77387101465689	150447
f4ab91a26061dcb5ec3fa4983250a39f480a485f	increasing the efficiency of prolog lexical databases with n-gram boolean cubes	lexical database;high level language;natural language processing	PROLOG has been shown to be an effective tool for expressing the logic of many problems dealing with parsing, natural language processing, and spelling verification [1,7,8,9,12]. As a class, these problems deal with the manipulation of lexical databases as Horn clauses. Since PROLOG does not generally differentiate between program clauses and data clauses, the internal representation and manipulation of data may not be optimal for a particular application. This paper discusses an alternative method of representing and manipulating lexical databases through the use of N-gram analysis, prefiltering, and integration with another high level language.	cubes;database;high-level programming language;horn clause;n-gram;natural language processing;parsing;prolog	Richard Rankin	1988		10.1145/62453.62488	natural language processing;lexical item;computer science;linguistics;lexical choice;programming language	AI	-25.152832557235783	19.5468539836681	150601
2a501e4c4bed1c69685bc25b2de11996f2476fd5	cobol vs. pl/1: some performance comparisons	cobol;performance comparison;natural language query processing;model mangegment systems	The benchmark program was implemented in both languages and wri t ten so as to use identical data types and language facilities where they exist and close approximations elsewhere. Measured results from eleven separate tests were then used to evaluate the relative merits of code generated for each language in that part icular area. The results in some cases were quite surprising and required the running of addit ional experiments to explain the outcome. The original benchmark was writ ten in Cobol and is called BNC1. Subsequently, three PL/1 versions were created: BNC6, BNC7 and BNC8. BNC6 is the original translation; Cobol COMP-3 items were interpreted as PIC '9' items and PERFORMs as PROCEDURE calls. BNC7 interprets Cobol COMP-3 items as FIXED DECIMAL and enumerates mult iple occurrences of sections of code PERFORMed in Cobol rather than use PROCEDURES. Finally, BNC8 is similar to BNC7 but uses PROCEDURE calls to implement PERFORM. BNC6 can be thought of as a Cobol programmer 's conservative translation whereas BNC7 and BNC8 are translations more favorable to PL/1. In all three PL/1 versions, data-i tems were translated exactly wherever possible, e.g., COMP to FIXED BINARY, PIC to PIC, etc. The benchmark breaks down into 11 separate tests each of which is t imed separately using an assembler	approximation;assembly language;benchmark (computing);cobol;experiment;pl/i;programmer	Paul J. Jalics	1984	Commun. ACM	10.1145/357994.358019	computer science;database;cobol;programming language	PL	-25.494270141589293	26.24786352464158	150612
73334478f65682bbb27d2ae428a86532d5856b02	aspect-oriented programming using reflection and metaobject protocols	aspect oriented programming;metaobject protocol	Some of the original inspiration for Aspect-Oriented Programming (AOP) [KLM97] draws from the research in dynamic, reflective object-oriented languages and metaobject protocols (MOPs) [KdB91]. A MOP lets the programmer delve “under the covers” and programmatically affect basic language mechanisms such as dynamic method dispatch and class instantiation. These powerful facilities enable the sort of crosscutting metaprogramming that AOP strives to deliver. We believe that there are two reasons why research on providing aspect-oriented programming features to the programmer has strayed from its metaobject protocol roots:	aspect-oriented programming;dynamic dispatch;metaobject;metaprogramming;programmer;reflection (computer programming);universal instantiation	Gregory T. Sullivan	2001	Commun. ACM	10.1145/383845.383865	real-time computing;aspect-oriented programming;computer science;distributed computing;programming language	PL	-26.998482235811576	29.872466065634804	150807
04926893c7f3984c128c44354ab69e4e7af51e65	mostly modular compilation of crosscutting concerns by contextual predicate dispatch	aspectj;compilacion separada;separtated compilation;compilation separee;systeme modulaire;orientado aspecto;concern separation;langage java;sistema modular;object oriented programming;object oriented;aspect oriented programming;separation preoccupation;modular system;separacion preocupacion;oriente objet;lenguaje java;crosscutting concerns;aspect oriented;orientado objeto;languages;oriente aspect;java language;java	The modularity of aspect-oriented programming (AOP) has been a controversial issue. To investigate this issue compared with object-oriented programming (OOP), we propose a simple language providing AOP mechanisms, which are enhanced traditional OOP mechanisms. We also present its formal system and then show that programs in this language can be only mostly modularly (i.e. separately) typechecked and compiled.We mention a source of this unmodularity and discuss whether or not it is appropriate to claim that AOP breaks modularity compared with OOP.	aspect-oriented programming;compiler;cross-cutting concern;dynamic dispatch;formal system;predicate dispatch;tag (game)	Shigeru Chiba;Atsushi Igarashi;Salikh Zakirov	2010		10.1145/1869459.1869503	real-time computing;aspect-oriented programming;computer science;programming language;object-oriented programming;algorithm	PL	-24.72685786955755	28.38606280451788	150870
95fa2e92948c4fa7586da73385af358f0034cef0	corecursive featherweight java	object oriented language;regular terms;programming paradigm;operational semantics;featherweight java;expressive power;object oriented;programming paradigms;object oriented programming languages;coinduction;theoretical foundation;data structure;java like languages	Despite cyclic data structures occur often in many application domains, object-oriented programming languages provide poor abstraction mechanisms for dealing with cyclic objects.  Such a deficiency is reflected also in the research on theoretical foundation of object-oriented languages; for instance, Featherweigh Java (FJ), which is one of the most widespread object-oriented calculi, does not allow creation and manipulation of cyclic objects.  We propose an extension to Featherweight Java, called COFJ, where it is possible to define cyclic objects, abstractly corresponding to regular terms, and where an abstraction mechanism, called regular corecursion, is provided for supporting implementation of coinductive operations on cyclic objects.  We formally define the operational semantics of COFJ, and provide a handful of examples showing the expressive power of regular corecursion; such a mechanism promotes a novel programming style particularly well-suited for implementing cyclic data structures, and for supporting coinductive reasoning.	angular defect;application domain;coinduction;corecursion;data structure;java platform, enterprise edition;operational semantics;programming language;programming style	Davide Ancona;Elena Zucca	2012		10.1145/2318202.2318205	computer science;theoretical computer science;programming language;algorithm;generics in java	PL	-24.936750658455843	25.6570806490772	150913
022c9acad46aa38c884a7ddc91cd9dea82a1ba38	debugging haskell by observing intermediate data structures	functional programming;data structure	Haskell has long needed a debugger. Although there has been much research into the topic of debugging lazy functional programs, no robust tool has yet come from the Haskell community that can help debug full Haskell until now. This paper describes a portable debugger for full Haskell, building only on commonly implemented extensions. It is based on the concept of observation of intermediate data structures, rather than the more traditional stepping and variable examination paradigm used by imperative debuggers.	combinatory logic;command-line interface;compiler;concurrent versions system;data structure;debugger;debugging;graphical user interface;hood method;haskell;imperative programming;lazy evaluation;observable;programming paradigm;stepping level;the glorious glasgow haskell compilation system;type class;web page	Andy Gill	2000	Electr. Notes Theor. Comput. Sci.	10.1016/S1571-0661(05)80538-9	computer architecture;parallel computing;data structure;computer science;programming language;functional programming	PL	-26.386898914829036	25.388917457832754	150940
4497fd3bdcf2c5057d46467eee42f9454c7b3476	re-classification and multi-threading: ficklemt	static checking;type and effect systems;multi threading;object oriented language;operational semantics;object oriented languages	In this paper we consider re-classification in the presence of multi-threading. To this aim we define a multi-threaded extension of the language Fickle, that we call FickleMT. We define an operational semantics and a type and effect system for the language. Each method signature carries the information on the possible effects of the method execution. The type and effect system statically checks this information. The operational semantics uses this information in order to delay the execution of some threads when this could cause messageNotUnderstood errors. We prove that in the execution of a well-typed expression such delays do not produce deadlock.	deadlock;effect system;multithreading (computer architecture);operational semantics;programmer;state (computer science);superuser;synchronization (computer science);thread (computing);type signature	Ferruccio Damiani;Mariangiola Dezani-Ciancaglini;Paola Giannini	2004		10.1145/967900.968163	real-time computing;computer science;database;distributed computing;programming language;object-oriented programming	PL	-22.515126010560778	32.20442671853817	151133
e34af4d58bcd3da3c438f24b89e6568a11185064	minimizing finite automata with graph programs	finite automata	GP (for Graph Programs) is a rule-based, nondeterministic programming language for solving graph problems at a high level of abstraction, freeing programmers from dealing with low-level data structures. In this case study, we present a graph program which minimizes finite automata. The program represents an automaton by its transition diagram, computes the state equivalence relation, and merges equivalent states such that the resulting automaton is minimal and equivalent to the input automaton. We illustrate how the program works by a running example and argue that it correctly implements the minimization algorithm of Hopcroft, Motwani and Ullman.	automata theory;automaton;dfa minimization;data structure;diagram;finite-state machine;high- and low-level;high-level programming language;logic programming;nondeterministic programming;programmer;sethi–ullman algorithm;turing completeness	Detlef Plump;Robin Suri;Ambuj Singh	2011	ECEASST	10.14279/tuj.eceasst.39.658	powerset construction;nondeterministic finite automaton with ε-moves;nested stack automaton;büchi automaton;nondeterministic finite automaton;state diagram;continuous spatial automaton;null graph;quantum finite automata;computer science;theoretical computer science;two-way deterministic finite automaton;deterministic finite automaton;probabilistic automaton;continuous automaton;deterministic automaton;ω-automaton;graph algebra;finite-state machine;dfa minimization;programming language;mobile automaton;timed automaton;directed acyclic word graph;algorithm	Logic	-21.38887591737991	22.46966823853518	151141
59538daae61c76e654f092ad612ab563e64975e3	compositional minimization of finite state systems	communication system;observational equivalence;transition systems;reactive system;state explosion;parallel processing;structural properties;intermediate representation	20 Indeed, a way to obtain interface speciications is by using the property to be veriied as interface speciication. This is what Clarke et al. CLM89] had in mind. However, their approach only exploits the alphabet of the property under consideration. A reened treatment of property constraints using our notion of interface speciication is under investigation. Finally, it should be mentioned that our method can easily be implemented. In fact, an implementation in the Edinburgh Concurrency Workbench CPS89] and in the Aldebaran veriication tool Fer88] is planned.	edmund m. clarke;exploit (computer security);workbench	Susanne Graf;Bernhard Steffen	1990		10.1007/BFb0023732	parallel processing;reactive system;computer science;programming language;intermediate language;algorithm;communications system	Logic	-21.303835224294097	20.868080352431573	151177
1bb9f1237b5b6cfea62fc3edb4127a6cec1bcc13	game: a framework for programming genetic algorithms applications	libraries;parallel programming genetic algorithms optimisation programming environments object oriented programming data structures software reusability;optimisation;genetic operator;programming environments;concurrent computing;object oriented design;graphical interface;programming environment;major european project;genetic algorithm manipulation environment;multiple parallel computation models;parallel programming;object oriented programming;genetics;parameterised libraries;c classes;programming interfaces;virtual computing environment;computational modeling;real world application;monitoring;levels of abstraction;data structures;programming profession;parallel object oriented programming environment genetic algorithms applications programming game system programming environment major european project sequential application parallel applications concurrent applications object oriented design genetic oriented data structures programming interfaces data structures monitoring virtual computing environment multiple parallel computation models parameterised libraries c classes graphic interface genetic algorithm manipulation environment;genetic oriented data structures;software reusability;parallel computer;genetic algorithms applications;graphic interface;parallel object oriented programming environment;sequential application;genetic algorithm;genetic algorithms;genetic algorithms data structures concurrent computing libraries programming environments educational institutions parallel programming computational modeling object oriented modeling programming profession;concurrent applications;programming;data structure;object oriented modeling;parallel applications;game system	The GAME system is a programming environment being developed at University College London as part of the major European project, aimed at promoting and demonstrating the use of genetic algorithms in real world applications. GAME is target at the development and execution of complex sequential, concurrent or parallel applications, based on the genetic algorithm (GA) paradigm. Its object-oriented design and implementation provide the required levels of abstraction to describe and configure applications for a broad range of domains. GAME addresses the basic requirements involved in the design cycle of a GA application; it offers a set of genetic-oriented data structures, objects and straightforward programming interfaces that permit the implementation of a variety of GAs and parallel GAs. The underlying infrastructure provides the mechanisms for problem independent manipulation of data structures, monitoring, and execution on a virtual computing environment supporting multiple parallel computation models. Applications are constructed from parameterised libraries containing algorithms and genetic operators modules. GAME is highly customisable and its libraries can be easily expanded with the inclusion of new parameterised modules. Novice users can rapidly configure and execute pre-defined applications by simply setting up few parameters. Programmers can create new applications by combining pre-defined algorithms and genetic operators, or by directly programming new algorithms using the set of C++ classes provided. A graphic interface and monitoring facilities are also available in GAME. >	genetic algorithm	José L. Ribeiro Filho;Philip C. Treleaven	1994		10.1109/ICEC.1994.349945	genetic algorithm;data structure;computer science;theoretical computer science;machine learning;graphical user interface;distributed computing;programming language;game programming	HPC	-30.42950814770318	28.631979318679793	151203
9ef22c8d2d098468b4547aeef016af5977a73b58	a comparative study of task communication in ada	ada;non determinism;synchronization;tasks;communication	Abstract#R##N##R##N#A previous paper compared the mechanisms for process communication in Hoare's communicating sequential processes and in Brinch Hansen's distributed processes, by both qualitative and quantitative analyses. This paper extends these analyses to the corresponding features for communication between tasks in Ada. The similarity between Ada's features and Hoare's proposals is confirmed, but some limitations on non-determinism in Ada are noted.	ada	Jim Welsh;Andrew M. Lister	1981	Softw., Pract. Exper.	10.1002/spe.4380110305	synchronization;real-time computing;ada;computer science;programming language;algorithm	SE	-25.984352767048936	32.095234123006435	151590
50967e7d9ab223741a82e5b362d1e5d7861234aa	a formal specification and verification framework for time warp-based parallel simulation	formal specifications time warp simulation;time warp;parallel programming time warp simulation formal specification program verification theorem proving;formal specification;parallel programming;causal ordering;program verification;theorem proving;theorem prover;formal verification;type checking;specification and verification;parallel discrete event simulation;prototype verification system;time warp simulation;distributed simulation;article;formal specification and verification;parallel discrete event simulation formal specification formal verification framework time warp based parallel simulation prototype verification system distributed simulation kernels time warp paradigm common formal base domain specific simulators correctness conditions causal ordering event processing correct rollback processing pvs theorem prover type check condition system;domain specificity;parallel simulation	ÐThis paper describes a formal framework developed using the Prototype Verification System (PVS) to model and verify distributed simulation kernels based on the Time-Warp paradigm. The intent is to provide a common formal base from which domain specific simulators can be modeled, verified, and developed. PVS constructs are developed to represent basic Time-Warp constructs. Correctness conditions for Time-Warp simulation are identified describing causal ordering of event processing and correct rollback processing. The PVS theorem prover and type-check condition system are then used to verify all correctness conditions. In addition, the paper discusses the framework's reusability and extensibility properties in support of specification and verification of Time-Warp extensions and optimizations. Index TermsÐframework has been used to specify and verify extensions to the time-warp algorithm. Specifically, the framework was extended to 1) represent and ae 1 INTRODUCTION	algorithm;automated theorem proving;causal filter;complex event processing;correctness (computer science);exception handling;extensibility;formal specification;formal verification;programming paradigm;prototype verification system;simulation	Peter Frey;Radharamanan Radhakrishnan;Harold W. Carter;Philip A. Wilsey;Perry Alexander	2002	IEEE Trans. Software Eng.	10.1109/32.979989	real-time computing;computer science;theoretical computer science;automated theorem proving;programming language	Logic	-29.868047706232716	31.50498560413732	151616
2851d5ab72782234de50e23a32830873d558a8ff	derivation of recursive algorithms for cs2	recursive algorithm	CS2 is the second course in our computing major. Our intent in this course is to teach students to derive procedures for manipulating dynamic data structures such as lists and trees. We have recently changed our implement ation language from Pascal to OberonF. This allows us to encapsulate the abstract data structures and their operations into abstract data types. Oberon/F is a cross-platform compiler and runtime system which allows concurrent development for both Windows and the Macintosh operating system. This programming framework makes it possible to build applications which have the “look and feel” of the host operating system; that is, applications with dialog boxes, pulldown menus, etc. OberonF also allows type extension and type bound procedures, which provide a structured way to introduce inheritance and objects, respectively. Reusability of code is facilitated by a simple import/export mechanism, which is efficiently implemented by use of dynamic linking. The course begins with the introduction of proof rules for the procedure declaration and the procedure call, in the absence of recursion. These rules are used to derive algorithms for the abstract data type list as implemented in an array. These algorithms include deleting, sorting, loading, and storing. In addition, the searching algorithms derived in the first course are transformed into procedures. This part of the course culminates in a derivation of the Heapsort algorithm. After this, proof rules are given for the recursive procedure definition, which are then used to derive the Quicksort algorithm. Recursive procedures are used extensively in the derivation of algorithms for manipulating lists and trees with a linked implementation. This is quite natural given the recursive definition of these data structures. The derivations of these algorithms are very straightforward because the reappearance of a specification similar to the original (with different parameters) leads naturally to a recursive call. This unit includes a derivation of Mergesort on a linked list, and binary tree sort. A later section of this paper provides an example of such a derivation. The formulation of a specification from an informal description is a key step in program derivation. A programming task can of course be specified in many clifferent ways, and the choice of specification can effect both the ease of derivation and the efficiency of the final program. The students are taught to formulate specifications and explore alternatives.	abstract data type;compiler;declaration (computer programming);dynamic data;dynamic linker;dynamization;heapsort;linked data structure;linked list;look and feel;macintosh operating systems;merge sort;microsoft windows;oberon;operating system;program derivation;quicksort;recursion (computer science);recursive definition;runtime system;search algorithm;sorting;subroutine;tree sort;dialog	Richard T. Denman	1996		10.1145/236452.236477	recursive bayesian estimation;recursive definition;computer science;primitive recursive function;programming language;μ operator;recursive partitioning;recursion	PL	-27.37404502456952	26.038747844063554	151697
4b27785ca969f307932061bdeaa8e71960524619	some patterns of component and language integration		aspect FragSplitObject { static Frag frag; ... protected static void makeSplitObject( JoinPoint jp, Object o) {...} protected static Object invokeSplitObject( JoinPoint jp, Object o) {...} abstract pointcut splitObjectClasses(Object obj); pointcut theConstructors(Object obj): splitObjectClasses(obj) && execution(new(..)); pointcut theMethods(Object obj): splitObjectClasses(obj) && execution(* *(..)) && !execution(String toString());pointcut splitObjectClasses(Object obj); pointcut theConstructors(Object obj): splitObjectClasses(obj) && execution(new(..)); pointcut theMethods(Object obj): splitObjectClasses(obj) && execution(* *(..)) && !execution(String toString()); before(Object obj): theConstructors(obj) { makeSplitObject(thisJoinPoint, obj); } Object around(Object obj) : theMethods(obj) && !cflow(execution(* invokeSplitObject(..))) { return invokeSplitObject(thisJoinPoint, obj); } } Note that this aspect excerpt is simplified. The actual aspec t implementation in the Frag distribution is more complex because it treats all Java prim itive types by separate pointcuts and advices. The aspect is defined as an abstract aspect. In concrete aspec ts the pointcut splitObjectClasses is refined. Here, the classes can be defined by the user to which the aspect is applied. For instance, we can apply the split objects to the classes Circle andSquare:	fragmentation (computing);java;obj (programming language);pattern language;pointcut;prim's algorithm;software architecture;wavefront .obj file	Uwe Zdun	2004			aspectj;implementation;systems engineering;programming language;computer science;terminology;weaving;pointcut;command language;primitive data type;java	SE	-25.870317283871643	29.42595493485163	151707
10614f8ca60d41610bd1d96f4e91cdc7cb22e095	three tools for smalltalk programmers: task oriented views, information cards, and scrc card			programmer;smalltalk	Ivan Tomek	1994			smalltalk;programming language;computer science	HCI	-29.500049062528674	26.589811065467888	151910
9a2d698d44bd71f2f89ab6383eb6546a9057582d	what's on your hard drive?		The tools on our hard drives vary widely in how directly they allow us to access the underlying hardware. Accordingly, many of our readers are most comfortable working as close to the machine as possible. Sometimes called “bare metal” programmers, they prefer the fewest possible layers of abstraction separating their code from the hardware. Others prefer tools that specialize in tasks less directly tied to the machine, creating models or even coding in Java with nary a thought of bits and bytes. Despite their fundamental differences, both types of tools inspire equal amounts of love and hate in their users, some of whom love tools for the same reasons (such as having a command line interface) that others hate them. We would like to hear more about which ones you love and hate, whatever your bias, at www.acmqueue.com.	abstraction layer;bare machine;byte;command-line interface;hard disk drive;java;programmer	Charlene O'Hanlon	2005	ACM Queue	10.1145/1105664.1105670		Arch	-30.705177946110577	26.946015759358964	151987
bfaedeae7a254289f985964f5578fa49be5de9dc	the visible web browser	web based applications;http;web browser;message passing;world wide web	As an aid to the study of the World-Wide Web, we have developed a software application that allows a user to observe the messages passed between a Web browser and a Web server. The application is based on the Mozilla Web Browser, and displays the HTTP headers sent and received by the browser. The program could be used by students in courses studying the Web, by researchers interested in the behavior of Web servers, and by developers to debug Web-based applications.	http cookie;hypertext transfer protocol;list of http header fields;server (computing);surface web;web application;web server;world wide web	Atticus Gifford;Benjamin J. Menasha;David Finkel	1999		10.1145/305786.305959	web service;ajax;web application security;hypertext transfer protocol;static web page;web development;web application;message passing;web modeling;framing;web mapping;web-based simulation;web design;comet;computer science;web api;web navigation;web page;database;internet privacy;client-side scripting;programming language;web 2.0;world wide web;web server;web testing	Web+IR	-31.36576885988314	25.863525925884325	152411
ae57632bb10e68a9fa43db609c4a9f5aa833f799	towards a dynamic-update-enabled jvm	virtual machine;dynamic software updating;java virtual machine;module system;virtual machine research;dynamic software updates;java	This paper advocates that de facto dynamic updates of Java applications will eventually require a dynamic-update-enabled Java virtual machine. We argue that our approach for dynamic updates of component-based Java applications complements the new module system planned for upcoming Java releases. We conclude that simple extensions to an existing JVM can bring full flexibility and transparency to dynamic updates in Java.	component-based software engineering;java version history;java virtual machine;maxine;modular programming;z/vm	Allan Raundahl Gregersen;Douglas Simon;Bo Nørregaard Jørgensen	2009		10.1145/1562860.1562862	java card;java api for xml-based rpc;real-time computing;plug-in;jsr 94;java concurrency;jar;computer science;operating system;cross-platform;strictfp;embedded java;real time java;programming language;java;generics in java;scala;java applet;java annotation	PL	-26.56135171991369	30.088760351465595	152525
11c54d5f989007cf0337895ed32155a220c2f4b4	declutter your r workflow with tidy tools		The R language has withstood the test of time. Forty years after it was initially developed (in the form of the S language) R is being used by millions of programmers on workflows the inventors of the language could never have imagined. Although base R packages perform well in most settings, workflows can be made more efficient by developing packages with more consistent arguments, inputs and outputs and emphasizing constantly improving code over historical code consistency. The universe of R packages known as the tidyverse, including dplyr, tidyr and others, aim to improve workflows and make data analysis as smooth as possible by applying a set of core programming principles in package development.	html tidy;programmer;r language;s (programming language)	Zev Ross;Hadley Wickham;David Robinson	2017	PeerJ PrePrints	10.7287/peerj.preprints.3180v1	data mining;piping;ggplot2;workflow;biology	PL	-29.960683749867137	21.295198414608112	152594
511521f7d097ecb8c16f0d9d2caaca3ada666b0a	meta-predicate semantics	predicate based module systems;objects;meta predicates	We describe and compare design choices for meta-predicate semantics, as found in representative Prolog predicate-based module systems and in Logtalk. We look at the consequences of these design choices from a pragmatic perspective, discussing explicit qualification semantics, computational reflection support, expressiveness of meta-predicate directives, meta-predicate definitions safety, portability of meta-predicate definitions, and meta-predicate performance. We also describe how to extend the usefulness of meta-predicate definitions. Our aim is to provide useful insights to discuss meta-predicate semantics and portability issues based on actual implementations and common usage patterns.	backward compatibility;ciao;compiler;control flow;derived object;directive (programming);eclipse;experience;information;interface description language;library (computing);logtalk;modular programming;programmer;prolog;reflection (computer programming);richard o'keefe;software portability;xsb	Paulo Moura	2011		10.1007/978-3-642-32211-2_11	computer science;object;database;programming language;operational semantics;algorithm	PL	-26.65428328642507	28.604966932118064	152672
91a79cade73b01b2d4b3f505a8037f3b813c362e	analyzing a csma/cd protocol through a systems of communicating machines specification	machine communicante;modelizacion;communications society;liveness properties;acceso multiple;acces multiple;carrier sense multiple access with collision detection;communicating process;ethernet bus;access protocol;computer networks;proceso comunicante;modelisation;finite state machines carrier sense multiple access;automata;csma cd protocol;finite state machines;collision detection;shared variables;multiaccess communication automata access protocols computer science safety ethernet networks computer networks communications society nasa;processus communicant;safety;access protocols;communication protocol;ethernet bus csma cd protocol systems of communicating machines specification carrier sense multiple access with collision detection safety liveness properties finite state machines shared variables;carrier detection;computer science;protocole acces;multiple access;systems of communicating machines specification;machine etat fini;ethernet networks;modeling;nasa;acceso protocolo;carrier sense multiple access;finite state machine;multiaccess communication;detection porteuse	A model for the specification and analysis of communication protocols called Systems of Communicating Machines is used to specify a C S W C D protocol, and to analyze it for safety and certain restricted liveness properties. The model uses a combination of finite state machines and variables in the specification of each machine, and the communication between machines is accomplished through shared variables. The ethernet bus is modeled as a single variable shared by all communicating processes. Collisions are modeled by simultaneous writes to this variable.	finite-state machine;liveness;shared variables	Gilbert M. Lundy;Raymond E. Miller	1993	IEEE Trans. Communications	10.1109/26.221072	communications protocol;systems modeling;telecommunications;computer science;distributed computing;automaton;finite-state machine;computer security;collision detection;computer network	Networks	-31.472958080479323	32.2355852213446	152703
a6a25fa78948e5e80dd4540f261ab7b4fad1ed57	an incremental ll(1) parsing algorithm	design algorithm;language processor;design of algorithms;editeur base langage;language theory;incremental parsing;teoria lenguaje;parsing;conception algorithme;language based editor;analizador sintaxico;language processors;parser;programa tratamiento lenguaje;programme traitement langage;analyseur syntaxique;article;theorie langage	Given a parse tree for a sentence xzy and a string z̄, an in cremental parser builds the parse tree for the sentence xz̄y by reusing as much of the parse tree for xzy as possible. The incremental LL(1) parsing algorithm in this paper makes use of a break-point table to identify reusable subtrees of the original parse tree in building the new parse tree. The break-point table may be computed from the grammar.	algorithm;parsing	Wuu Yang	1993	Inf. Process. Lett.	10.1016/0020-0190(93)90179-D	natural language processing;parser combinator;computer science;bottom-up parsing;parsing;incremental decision tree;search tree;programming language;top-down parsing;algorithm	NLP	-24.728401129653836	24.308747835477874	153134
06411d990c206c07040ab4158914892b51324e49	state constraints and pathwise decomposition of programs	verification;developpement logiciel;program understanding;simplification;test programa;decomposition;programmation;state constraint;program simplification;execution paths;pathwise decomposition;coaccion;contrainte;subprograms;testing;ingenieria logiciel;indexing terms;software engineering;analisis programa;programacion;software engineering computational complexity program testing;program pathwise;constraint;program testing;formal basis;verification state constraint programming construct program pathwise subprograms control flow execution paths formal basis pathwise decomposition program simplification testing;computational complexity;aggregates;desarrollo logicial;state constraints;software development;control flow;simplificacion;genie logiciel;program analysis;testing computer science aggregates capacitive sensors;computer science;descomposicion;analyse programme;test programme;programming construct;program test;programming;capacitive sensors	A state constraint is a new programming construct designed to restrict the domain of definition of a program. It can be used to decompose a program pathwise, i.e., to divide the program into subprograms along the control flow, as opposed to divide the program across the control flow when the program is decomposed into functions and procedures. As the result one can now construct and manipulate a program consisting of one or more execution paths of another program. This paper describes the idea involved, examines the properties of state constraints, establishes a formal basis for pathwise decomposition, and discusses their utilities in program simplification, testing, and verification.	control flow;program transformation;subroutine;text simplification	J. C. Huang	1990	IEEE Trans. Software Eng.	10.1109/32.57625	program analysis;programming;verification;index term;computer science;theoretical computer science;software development;operating system;software engineering;capacitive sensing;software testing;constraint;decomposition;programming language;computational complexity theory;control flow;management;simplification;algorithm	SE	-23.266726270561204	28.309231457260672	153190
02ba8bf9015dd4d1cb45ba41958906e284a10ac3	computation of summaries using net unfoldings	004;net unfoldings concurrent systems petri nets	We study the following summarization problem: given a parallel composition A = A1 ‖ . . . ‖ An of labelled transition systems communicating with the environment through a distinguished component Ai, efficiently compute a summary Si such that E ‖ A and E ‖ Si are trace-equivalent for every environment E. While Si can be computed using elementary automata theory, the resulting algorithm suffers from the state-explosion problem. We present a new, simple but subtle algorithm based on net unfoldings, a partial-order semantics, give some experimental results using an implementation on top of Mole, and show that our algorithm can handle divergences and compute weighted summaries with minor modifications.	acta informatica;algorithm;altran praxis;automata theory;bernhard steffen (computer scientist);causality;communicating sequential processes;computation;computer aided verification;computing the permanent;concurrency (computer science);correctness (computer science);deadlock;formal methods;hoare logic;informatics;international conference on computer-aided design;international joint conference on artificial intelligence;jason;model checking;orna berry;partial order reduction;petri nets;petri net;prentice hall international series in computer science;proceedings of the ieee;star trek generations;state space search;the computer journal;thomas baumgartner;tracing (software);unfolding (dsp implementation);verification and validation;victor animatograph corporation	Javier Esparza;Loïg Jezequel;Stefan Schwoon	2013		10.4230/LIPIcs.FSTTCS.2013.225	discrete mathematics;computer science;theoretical computer science;mathematics;algorithm	Logic	-19.84556205193599	19.38609464817326	153262
f85cfec3f7c4aba87e4dc6d0fbb953c1851d2021	paradigms for design and implementation in ada	compilation separee;ada;implementation;program design;langage evolue;conception programme;ada computer program language;computer programming;design and implementation;paradigme;ada language;high level language	An examination of the respective advantages and disadvantages of three characteristic paradigms of design and implementation in Ada illustrates the importance of choosing the appropriate paradigm for a given set of circumstances.	ada;programming paradigm	Václav Rajlich	1985	Commun. ACM	10.1145/3894.3898	computer architecture;parallel computing;ada;computer science;computer programming;programming language;implementation	OS	-25.691941157902768	24.4365241721258	153265
045257de386dcf13103dddea8ab2a4106d20d447	'von großen böcken und einer menge staub' - zur maschinellen verarbeitung modifizierter idiome mit semantisch-autonomen komponenten	formal semantics	In the following we deal with a special type of phrasal idiom, called gurative-compositional idiom. Idioms of this class have semantically autonomous components that shows a certain degree of independence. We propose an adequate manner of formal semantic representation of their idiomatic meaning: we map the referents of the semantically autonomous components into the formal representation. The advantage of our approach will be shown by the means of Discourse Representation Theory (DRT). Furthermore we show an implemented mechanism based on-calculus and DRT, which allows to process sentences in which a combination of both literal and gurative language use occurs.	autonomous robot;literal (mathematical logic);programming idiom	Ingrid Fischer;Martina Keil	1996			linguistics;semantics of logic;philosophy	NLP	-25.1130462363139	18.709752096565726	153402
5900fedbf8f0c0f804c05a7ea043687cfcc27caf	spine-local type inference		We present spine-local type inference, a partial type inference system for inferring omitted type annotations for System F terms based on local type inference. Local type inference relies on bidirectional inference rules to propagate type information into and out of adjacent nodes of the AST and restricts type-argument inference to occur only within a single node. Spine-local inference relaxes the restriction on type-argument inference by allowing it to occur only within an application spine and improves upon it by using contextual type-argument inference. As our goal is to explore the design space of local type inference, we show that, relative to other variants, spine-local type inference enables desirable features such as first-class curried applications, partial type applications, and the ability to infer types for some terms not otherwise possible. Our approach enjoys usual properties of a bidirectional system of having a specification for our inference algorithm and predictable requirements for typing annotations, and in particular maintains some the advantages of local type inference such as a relatively simple implementation and a tendency to produce good-quality error messages when type inference fails.	algorithm;currying;error message;inference engine;requirement;system f;type inference	Christopher Jenkins;Aaron Stump	2018	CoRR		theoretical computer science;type inference;computer science;inference;system f	PL	-22.623435621342036	25.40708335118268	153458
9961c606c92250da315a35fce501b85516104874	collecting graphical abstract views of mercury program executions	front end;programming language;call graph;control flow graph;software engineering;expressive power;lines of code;software visualization	A program execution monitor is a program that collects and abstracts information about program executions. The collect operator is a high level, general purpose primitive which lets users implement their own monitors. Collect is built on top of the Mercury trace. In previous work, we have demonstrated how this operator can be used to efficiently collect various kinds of statistics about Mercury program executions. In this article we further demonstrate the expressive power and effectiveness of collect by providing more monitor examples. In particular, we show how to implement monitors that generate graphi-cal abstractions of program executions such as proof trees, control flow graphs and dynamic call graphs. We show how those abstractions can be easily modified and adapted, since those monitors only require several dozens of lines of code. Those abstractions are intended to serve as front-ends of software visualization tools. Although collect is currently implemented on top of the Mercury trace, none of its underlying concepts depend of Mercury and it can be implemented on top of any tracer for any programming language.	control flow graph;high-level programming language;mercury;software visualization;source lines of code	Erwan Jahier	2000			call graph;software visualization;real-time computing;computer science;theoretical computer science;front and back ends;software engineering;programming language;source lines of code;expressive power;control flow graph	PL	-21.97466110596425	31.432748172791214	153535
425542e4b8330145b0d5419b98bf90eda2c5ef2a	exception safety: concepts and techniques	lenguaje programacion;langage c;programming language;securite;design technique;c language;safety;langage programmation;exception handling;gestion exception;seguridad;high performance	This paper presents a set of concepts and design techniques that has proven successful in implementing and using C++ libraries intended for applications that simultaneously require high reliability and high performance. The notion of exception safety is based on the basic guarantee that maintains basic invariants and avoids resource leaks and the strong guarantee that ensures that a failed operation has no effect.	c++;exception safety;library (computing)	Bjarne Stroustrup	2000		10.1007/3-540-45407-1_4	exception handling;computer science;database;programming language;algorithm	Embedded	-23.59435526258452	30.068495101081155	153581
ba530190faa1efc50a6dbcbb1a384e8cd319e52d	executable tile specifications for process calculi	semantica operacional;process calculi;executable specification;maude;operational semantics;simultaneidad informatica;logical programming;rewriting strategies;concurrency;semantique operationnelle;programmation logique;rewriting systems;tile logic;informatique theorique;rewriting logic;rest of the world;pmeqtl;programacion logica;simultaneite informatique;systeme reecriture;computer theory;informatica teorica	Tile logic extends rewriting logic by taking into account side-effects and rewriting synchronization. These aspects are very important when we model process calculi, because they allow us to express the dynamic interaction between processes and the rest of the world. Since rewriting logic is the semantic basis of several language implementation efforts, we can define an executable specification of tile systems by mapping tile logic back into rewriting logic. In particular, this implementation requires the development of a metalayer to control rewritings, i.e., to discard computations that do not correspond to any deduction in tile logic. Our methodology is applied to term tile systems that cover and extend a wide-class of SOS formats for the specification of process calculi. The case study of full CCS, where the term tile format is needed to deal with recursion (in the form of the replicator operator), is discussed in detail.	executable;process calculus	Roberto Bruni;José Meseguer;Ugo Montanari	1999		10.1007/978-3-540-49020-3_5	concurrency;rewriting;computer science;theoretical computer science;database;programming language;operational semantics;algorithm	PL	-21.08463554891208	22.546322888146186	153736
937da2a99d851832d5811a2c54b47edf561e92aa	efficient symbolic computation of process expressions	symbolic computation;trace based specifications;information systems;optimization technique;lazy evaluation;operational semantics;execution environment;black box specifications;information system;interpreter;process algebra	This paper describes three optimization techniques for the eb3 process algebra. The optimizations are expressed in a new deterministic operational semantics which is shown to be trace-equivalent to a traditional non-deterministic operational semantics. Internal action transitions are eliminated by an efficient preruntime analysis of the structure of a process expression. Execution environments are used to optimize variable instantiation using lazy evaluation. Non-determinism is eliminated by returning a choice between possible transitions. This new operational semantics is implemented in the eb3 pai process algebra interpreter to support the eb3 method. The goal of this method is to automate the development of information systems using, among other mechanisms, efficient symbolic computation of process expressions. © 2009 Elsevier B.V. All rights reserved.	agile software development;analysis of algorithms;asynchronous i/o;automata theory;book;computational complexity theory;computer data storage;documentation;entity;executable;full scale;hands-on computing;information system;lazy evaluation;linear algebra;mathematical optimization;operational semantics;overhead (computing);persistence (computer science);process calculus;programmer;recursion;requirement;response time (technology);state diagram;state transition table;substitution (logic);symbolic computation;tail call;throughput;true quantified boolean formula	Benoît Fraikin;Marc Frappier	2009	Sci. Comput. Program.	10.1016/j.scico.2009.02.002	symbolic computation;computer science;theoretical computer science;programming language;operational semantics;information system;algorithm	PL	-21.410322018105383	22.868102396490542	153794
7771d06152d3002d48e2a306270c4148c0f4ac11	facilitating modular property-preserving extensions of programming languages	programming language;formal methods;software engineering;modular programming languages;language design	We will explore an approach to modular programming language descriptions and extensions in a denotational style. Based on a language core, language features are added stepwise on the core. Language features can be described separated from each other in a self-contained, orthogonal way. We present an extension semantics framework consisting of mechanisms to adapt semantics of a basic language to new structural requirements in an extended language preserving the behaviour of programs of the basic language. Common templates of extension are provided. These can be collected in extension libraries accessible to and extendible by language designers. Mechanisms to extend these libraries are provided. A notation for describing language features embedding these semantics extensions is presented.	abstract machine;book;compiler;computation;definition;denotational semantics;extensibility;formal language;formal methods;imperative programming;java;library (computing);modular programming;modularity (networks);parallel extensions;parallel computing;perl;programming language;requirement;rewriting;schmidt decomposition;stepwise regression;technical support;theory	Claus Pahl	1998			natural language processing;fourth-generation programming language;first-generation programming language;natural language programming;very high-level programming language;formal methods;universal networking language;language primitive;object language;specification language;programming domain;data control language;computer science;programming language implementation;domain-specific language;theoretical computer science;third-generation programming language;low-level programming language;fifth-generation programming language;programming language theory;programming language;programming language specification;high-level programming language;semantics	PL	-25.22474042638484	22.938928169223438	153833
6be7f5a101d4764911937c5305caa3a182ae5c70	adaptive constraint handling with chr in java	lenguaje programacion;calcul repere;semantica operacional;syntax;programming language;benchmark problem;operational semantics;code generation;object oriented programming;syntaxe;constraint satisfaction;calculo de referencia;run time system;satisfaction contrainte;constraint handling rules;semantique operationnelle;object oriented programming languages;benchmark calculation;langage programmation;constraint handling;satisfaccion restriccion;sintaxis;process simulation;programmation orientee objet;data structure	The most advanced implementation of adaptive constraint processing with Constraint Handling Rules (CHR) is introduced in the imperative object-oriented programming language Java. The presented Java implementation consists of a compiler and a run-time system, all implemented in Java. The run-time system implements data structures like sparse bit vectors, logical variables and terms as well as an adaptive unification and an adaptive entailment algorithm. Approved technologies like attributed variables for constraint storage and retrieval as well as code generation for each head constraint are used. Also implemented are theoretically sound algorithms for adapting of rule derivations and constraint stores after arbitrary constraint deletions. The presentation is rounded off with some novel applications of CHR in constraint processing: simulated annealing for the n queens problem and intelligent backtracking for some SAT benchmark problems.	aim alliance;adaptive grammar;algorithm;backtracking;benchmark (computing);bit array;code generation (compiler);compiler;constraint handling rules;constraint logic programming;constraint programming;constraint satisfaction problem;data structure;diagram;goto;imperative programming;java;kathleen antonelli;programming language;runtime system;simulated annealing;sparse matrix;unification (computer science)	Armin Wolf	2001		10.1007/3-540-45578-7_18	constraint logic programming;concurrent constraint logic programming;constraint programming;binary constraint;process simulation;data structure;constraint satisfaction;constraint learning;computer science;constraint graph;artificial intelligence;theoretical computer science;operating system;machine learning;database;distributed computing;programming language;object-oriented programming;algorithm;hybrid algorithm;local consistency;backtracking	PL	-21.233988394878907	23.0722716539824	153956
8336ac998b4a76f447db8d51074930788e94c02c	a reverse engineering process for design level document production from ada code	reverse engineering	OR SUBSYSTEM • from the task table the abstractor subsystem abstracts the documents M I M 4 and the related graphic representations. The overall architecture of the prototype is shown in Figure 7, which highlights the two subsystems and the major modules in each subsystem: the PREPROCESSOR and MODEL COMPILER for the extractor; the FORM COMPILER and the GRAPH ORGANIZER for the abstractor. Both the PREPROCESSOR and the MODEL COMPILER were implemented with LEX and YACC :9, two standard UNIX facilities for lexical analyser and parser generation. Figure 8 shows the detailed architecture of the extractor. The PREPROCESSOR, written using LEX and C, accepts an ADA source file as input and translates it into the corresponding ADA tasking profile. The MODEL COMPILER, written using LEX, YACC and C, accepts an ADA tasking profile as input and produces the related task table. More specifically, the MODEL COMPILER uses the lexical analyser generated by LEX to analyse the input ADA tasking profile and partition it into categories, while the parser generated by YACC uses the categories identified by the lexical analyser and the set of grammar rules describing the ADA tasking profile to recognize and classify the input phrases. Whenever an input phrase matches one of the grammar rules, the parser calls the associated C routine in order to generate and/or update by ADA syntax and since the purpose of CDT is essentially to make experimentation possible, we have not considered all the characteristics of the language. Advanced features such as generics and tasks created by allocator evaluation are not considered. Furthermore, all tasks are assumed to be at the same level. Consequently, the extractor only analyses ADA programs composed of (i) a set of separately compiled packages which declare one or more tasks in the visible part of their specification; (ii) a main procedure that declares a set of tasks in its declarative section and/or uses (i.e. contains a context clause naming) these packages. As far as the abstractor is concerned, both the FORM COMPILER and GRAPH ORGANIZER modules are written in C. The FORM COMPILER accepts a task table as input and, according to user requirements, produces one or more of the M I M 4 documents. The Ada Source Code Fi l ter ing Rules 1 Ada Tasklng Profi le Lexlca] Rules and User Routines	ada;compiler;eclipse;experiment;extractor (mathematics);form;generic programming;lex (software);lexical analysis;preprocessor;prototype;randomness extractor;requirement;reverse engineering;unix;user requirements document;yacc	Gerardo Canfora;Aniello Cimitile;Ugo de Carlini	1993	Information & Software Technology	10.1016/0950-5849(93)90026-Y	computer science;systems engineering;database;programming language;reverse engineering	SE	-32.78949892446677	25.793420973017597	154176
6310199a4ed1ab9662112eda61f9333ec83b1af7	formal specification of protocols and its independence of communication mechanism	formal specification	In specifying and designing protocols, it is convenient to use an abstract synchronous communication mechanism. But in practice, asynchronous communication mechanisms cannot be avoided. This paper presents a formal approach based on CSP, and some theoretical results on the specification and design of protocols such that the correctness of the protocols is independent of the communication mechanism used in implementation.		Sun Yong	1987		10.1007/978-3-642-71655-3_60	formal methods;specification language;formal verification;computer science;system requirements specification;formal specification;refinement;language of temporal ordering specification	Theory	-32.87227238102625	31.65395134815914	154202
5849203a9b200f595892f10d88c747dca146bfac	observational congruences for dynamically reconfigurable tile systems	reconfiguration;bisimilarite;reconfiguracion;reconfigurable system;closure;dynamique;dynamic reconfiguration;bisimulacion;specification;duality;bisimulation;sos format;systeme ouvert;dinamica;congruencia;dualite;extremite;especificacion;end;dynamics;tile logic;informatique theorique;logique tile;extremidad;format sos;dualidad;cerradura;π calculus;open systems;sistema abierto;congruence;fermeture;sos formats;computer theory;calcul pi;dynamic bisimulation;informatica teorica	The SOSformats that ensure that bisimilarity is a congruence fail in the presence of structural axioms on states. Dynamic bisimulation, introduced to characterize the coarsest congruence for CCS which is also aweakbisimulation, reconciles the ‘bisimilarity is a congruence’ propertywith structural axioms and also with the specification of open ended systems, where states can be reconfigured at runtime. We show that the compositional framework offered by tile logic handles structural axioms and specifications of reconfigurable systems successfully. This allows for a finitary presentation of dynamic context closure, as internalized in the tile language. The case study of the -calculus illustrates the main features of our approach. Moreover, duality is exploited to model a second kind of reconfiguration: dynamic specialization. © 2005 Elsevier B.V. All rights reserved.	bisimulation;congruence of squares;partial template specialization;reconfigurability;run time (program lifecycle phase)	Roberto Bruni;Ugo Montanari;Vladimiro Sassone	2005	Theor. Comput. Sci.	10.1016/j.tcs.2004.10.044	end;dynamics;combinatorics;discrete mathematics;duality;π-calculus;bisimulation;control reconfiguration;pure mathematics;closure;congruence;mathematics;open system;programming language;specification;algorithm	Logic	-25.168326334314113	31.358419825616632	154224
a1cd1b158e7544b1bad427c09c4750c43cac477a	agreement between conformance and composition		In our previous paper [1], a new model of a Labeled Transition System (LTS)-type implementation was proposed. In ordinary LTSs, transitions are labeled by actions; therefore, they can be called LTSs of actions. The new model is an LTS of observations; in this model, observations and test actions (buttons) are used instead of actions. This model generalizes many testing semantics that are based on the LTS of actions but use additional observations (refusals, ready sets, etc.). Moreover, systems with priority, which are not described by the LTS of actions, are simulated uniformly. In the present paper, we develop this approach by focusing on the composition of systems. The point is that, on observation traces, one cannot define a composition with respect to which a composition of LTSs would possess the property of additivity: the set of traces of a composition of LTSs coincides with the set of all pairwise compositions of traces of LTS operands. This is explained by the fact that an observation in a composition state is not calculated based on observations in states-operands. In this paper, we propose an approach that eliminates this drawback. To this end, we label the transitions of LTSs by symbols (events) that, on the one hand, can be composed to guarantee the property of additivity, and, on the other hand, can be used to generate observations under testing: a transition by an event gives rise to an observation related to this event. This model is called an LTS of events. In this paper, we define (1) a transformation of an LTS of events into an LTS of observations to conform with the principles of our previous paper [1]; (2) a composition of LTSs of events; (3) a composition of specifications that preserves conformance: a composition of conformal implementations is conformal to a composition of specifications; and (4) a uniform simulation of LTSs of actions in terms of the LTSs of events, which allows one to consider an implementation in any interaction semantics admissible for LTSs of actions. In this case, a composition of LTSs of events obtained as a result of simulation of the original LTSs of actions is equivalent to the LTS of events obtained as a result of simulation of the composition of these LTSs of actions.	conformance testing;operand;simulation;tracing (software);transition system	Igor B. Bourdonov;Alexander S. Kossatchev	2013	Programming and Computer Software	10.1134/S0361768813060029	artificial intelligence;mathematics;algorithm	AI	-31.380915266740068	30.549113501973526	154412
263e583d5810746751f08e4e752ddb69adc66168	mutally exclusive rules in logic programming		A technique to detect that pairs of rules are “mutually exclusive” in a logic program is described. In contrast to previous work our algorithm derives mutual exclusion by looking not only on built-in, but also user-defined predicates. This technique has applications to optimization of the execution of programs containing these rules. Additionally, the programmer is less dependent on non-logical language features, such as Prolog’s “cut”, thus creating more opportunities for parallel execution strategies.	abstract interpretation;algorithm;approximation;bottom-up proteomics;compile time;compiler;constraint inference;ibm notes;language binding;local variable;logic programming;mathematical optimization;mutual exclusion;programmer;prolog;software propagation;undecidable problem	Kjell Post	1994				PL	-19.676769389258563	24.764258279343622	154521
4c044a1af73e0a2a6fc304bb09eeb633b29dd887	gec: a toolkit for generic rapid prototyping of type safe interactive applications	user interface;data type;higher order;rapid prototyping;real world application;interactive application;interactive system;article in monograph or in proceedings	GUI programming is notoriously tedious. By using generic functions that create Graphical Editor Components (GECs), it becomes possible to define user interfaces without any knowledge of low level I/O handling. A GEC editor can automatically be derived for values of any (user-defined) monomorphic (first order) type. With an editor the application user can create new values of the type the editor is created for. In this way one obtains an editor for free for almost any type. Such a free editor may not look fancy, but one can change the default look by defining specialized versions of the generic function for certain types, e.g. representing buttons, pull-down menus and the like. Furthermore, with GECs the programmer can create an abstraction level separating the view type and the domain type completely. As a result, the programmer can easily make a library of suited graphical representations and freely choose which representation to use. Consequently, working with GECs a programmer can focus on the data type representing the user interaction instead of on the nasty graphical details. Editors can be easily combined to ensure that a change in one editor has effects on the contents of others. One can combine editors by hand or use an arrow library of editor combinators. It is even possible to create editors for higher order types which enables the creation of user interfaces in which functions can be typed in by the user or read from disk at run-time. In the latter case, functions are actually compiled functions that are dynamically linked into the running application. GECs are suited for rapid prototyping of real world applications, for teaching and for debugging. This paper focuses on the use of the GEC toolkit for functional programmers, only briefly explaining its inner workings and underlying principles.	acm sigact;abstraction layer;automatic programming;clean;combinatory logic;compiler;computer architecture;computer programming;de bruijn graph;debugging;edmund m. clarke;electronic notes in theoretical computer science;fudgets;generic function;generic programming;graphical user interface;haskell;hoc (programming language);ifip working group 2.1;input/output;integrated facility for linux;integrated development environment;interactivity;international conference on functional programming;international standard book number;jones calculus;koopmans' theorem;lazy evaluation;lecture notes in computer science;linker (computing);matthew flatt;mixin;model–view–controller;monad (functional programming);programmer;programming language;programming paradigm;rapid prototyping;smalltalk;software testing;springer (tank);symposium on principles of programming languages;test automation;type safety;type system;xml editor	Peter Achten;Marko C. J. D. van Eekelen;Marinus J. Plasmeijer;Arjen van Weelden	2004		10.1007/11546382_5	human–computer interaction;computer science;theoretical computer science	PL	-28.49826537803879	26.525667434373087	154672
b28755275e2b1818e4cdd83ba44837ab9d0f7740	stateful behavioral types for active objects		It is notoriously hard to correctly implement a multiparty protocol which involves asynchronous/concurrent interactions and constraints on states of multiple participants. To assist developers in implementing such protocols, we propose a novel specification language to specify interactions within multiple object-oriented actors and the side-effects on heap memory of those actors. A behavioral-type-based analysis is presented for type checking. Our specification language formalizes a protocol as a global type, which describes the procedure of asynchronous method calls, the usage of futures, and the heap side-effects with a first-order logic. To characterize runs of instances of types, we give a model-theoretic semantics for types and translate them into logical constraints over traces. We prove protocol adherence: If a program is well-typed w.r.t. a protocol, then every trace of the program adheres to the protocol, i.e., every trace is a model for the formula of the protocol’s type.	state (computer science);stateful firewall	Eduard Kamburjan;Tzu-Chun Chen	2018		10.1007/978-3-319-98938-9_13	programming language;memory management;computer science;asynchronous method invocation;specification language;stateful firewall;semantics;asynchronous communication;heap (data structure)	HCI	-20.742497937837058	28.716847760821146	154756
d5faba32a7e6115d29d3be806fa3df5127534f28	operational and denotational semantics of prolog	denotational semantic	Abstract   A Vienna Definition Language operational semantics of PROLOG, which includes the cut, the database, and the extra-logical operations, is presented. This semantics serves as the basis for deriving a denotational-continuation-style semantics of PROLOG through a systematic transformation of the operational semantics by a method described by Berry.	denotational semantics;operational semantics;prolog	Bijan Arbab;Daniel M. Berry	1987	J. Log. Program.	10.1016/0743-1066(87)90008-2	normalisation by evaluation;formal semantics;action semantics;computer science;theoretical computer science;formal semantics;programming language;denotational semantics of the actor model;well-founded semantics;operational semantics;denotational semantics;algorithm;computational semantics	PL	-24.033798219690283	20.484576759185167	154815
f01b579d84e1fb15c4d2a61efb0fd013020e83f9	translating b to tla+ for validation with tlc	tool support;b method;model checking;animation;tla	The state-based formal methods B and TLA share the common base of predicate logic, arithmetic and set theory. However, there are still considerable differences, such as the way to specify state transitions, the different approaches to typing, and the available tool support. In this paper, we present a translation from B to TLA to validate B specifications using the model checker TLC. The translation includes many adaptations and optimizations to allow efficient checking by TLC. Moreover, we present a way to validate liveness properties for B specifications under fairness conditions. Our implemented translator, Tlc4B, automatically translates a B specification to TLA, invokes the model checker TLC, and translates the results back to B. We use ProB to double check the counter examples produced by TLC and replay them in the ProB animator. Tlc4B can also transmit constant values, precalculated by ProB to TLC. This allows the user to combine the strength of both tools, i.e. ProB’s constraint solving abilities and TLC’s highly tuned model checking core. Furthermore, we demonstrate an approach to optimize the model checking process by encoding proof information in the translated TLAspecification. We also present a series of case studies and benchmark tests comparing Tlc4B and ProB.	algorithm;benchmark (computing);canonical account;constraint satisfaction problem;fairness measure;formal methods;high- and low-level;liveness;model checking;multi-level cell;set theory;tla+;tcl;user interface	Dominik Hansen;Michael Leuschel	2016	Sci. Comput. Program.	10.1016/j.scico.2016.04.014	b-method;model checking;anime;real-time computing;computer science;programming language;algorithm	SE	-20.351665247286135	26.30406951019949	154845
f1f4e531bb5a0000ed0ed1cb237564baf1753a58	finite models for formal security proofs	inductionless induction;formal security proof;dolev yao model;security proof;first order;common criteria;tree automaton;h_1;finite model;first order logic;security protocol	First-order logic models of security for cryptographic pro tocols, based on variants of the Dolev-Yao model, are now well-established tools . Given that we have checked a given security protocol π using a given first-order prover, how hard is it to extract a formally checkable proof of it, as required in, e .g., common criteria at the highest evaluation level (EAL7)? We demonstrate that th is is surprisingly hard in the general case: the problem is non-recursive. Nonethel ss, we show that we can instead extract finite models M from a setS of clauses representing π, automatically, and give two ways of doing so. We then define a model -ch cker testing M |= S, and show how we can instrument it to output a formally checka ble proof, e.g., in Coq. Experience on a number of protocols shows that t his is practical, and that even complex (secure) protocols modulo equational the ories have small finite models, making our approach suitable. Partially supported by project PFC (“plateforme de confianc e”), pôle de comṕetitivité System@tic Parisrégion Ile-de-France. Part of this work was done during RNTL p roject EVA, 2000-2003.	automata theory;automaton;common criteria;coq (software);cryptographic protocol;cryptography;dolev–yao model;eva conferences;evaluation assurance level;experiment;first-order logic;first-order predicate;image scaling;model checking;modulo operation;needham–schroeder protocol;paradox (database);powerbuilder foundation classes;public-key cryptography;recursion (computer science);sizeof;verification and validation;yao graph	Jean Goubault-Larrecq	2010	Journal of Computer Security	10.3233/JCS-2009-0395	computer science;theoretical computer science;first-order logic;programming language;algorithm	Crypto	-19.39980158192359	26.09958729742407	155091
2f293f146fe48ec80a9de3336f727c9267e2cb0c	retrospective: a retrospective on the warp machines		The Warp system was installed and almost immediately used for application development. Additional software demands were the price of user acceptance our collaborators wanted to use the system over the network [3], they demanded optimized code [7] and a debugger. When the project started, the plan was to hand microcode a set of core vision library routines. When a compiler effort proved feasible, the application developers stated that programs would be “simple functions, about l/2 a page of code”. When the compiler was done, programs with a length of 10s of pages were written; the right hand side of one assignment alone contained 11,000 characters. (This statement had been generated by another tool. The compiler translated the statement correctly, but it took 30 minutes.)	compiler;debugger;microcode	Thomas R. Gross;Monica S. Lam	1998		10.1145/285930.285950	parallel computing;computer science;computer architecture	Arch	-30.09057803663283	21.91526630431017	155207
6199cb816ec60f65c6611558b2c81792d2ef2397	efficient support for mode-directed tabling in the yaptab tabling system		Mode-directed tabling is an extension to the tabling technique that supports the definition of mode operators for specifying how answers are inserted into the table space. In this paper, we focus our discussion on the efficient support for mode directed-tabling in the YapTab tabling system. We discuss 7 different mode operators and explain how we have extended and optimized YapTab’s table space organization to support them. Initial experimental results show that our implementation compares favorably with the B-Prolog and XSB state-of-the-art Prolog tabling systems.	b-prolog;benchmark (computing);declaration (computer programming);memoization;prolog;scheduling (computing);thread (computing);virtual 8086 mode;xsb	João Santos;Ricardo Rocha	2013	CoRR		real-time computing;computer science;artificial intelligence;algorithm	AI	-20.913199678029486	23.950956339931494	155323
148a2a506f9de04c71830998ed8883a0d9823a3f	polymorphic subtyping for effect analysis: the dynamic semantics	dynamic semantics;effect analysis;polymorphic subtyping;operational semantics;software development;object oriented;type system;high level language;polymorphism;synchronous communication;control flow analysis;type inference;programming language	We study an annotated type and effect system that integrates let-polymorphism, effects, and subtyping into an annotated type and effect system for a fragment of Concurrent ML. First a small-step operational semantics is defined and next the annotated type and effect system is proved semantically sound. This provides insights into the rule for generalisation in the annotated type and effect system.	concurrent ml;effect system;hindley–milner type system;operational semantics;programming language	Torben Amtoft;Flemming Nielson;Hanne Riis Nielson;Jürgen Ammann	1996		10.1007/3-540-62503-8_9	subtyping;type erasure;type safety;computer science;database;system f-sub;programming language;operational semantics;algorithm	PL	-22.892918383557877	22.483645524110408	155344
07c6d4bbd2c011e42210f46b817249179324a892	an embedded language approach to teaching hardware compilation	langage fonctionnel;lenguaje programacion;compilacion;essai materiel informatique;programming language;embedded language;lenguaje funcional;testing;optimizacion compiladora;functional programming;synthese materiel informatique;compiler optimization;compilation materiel informatique;langage programmation;compilation;advanced technology;programmation fonctionnelle;erlang;enseignement;computer hardware;technologie avancee;functional language;programacion funcional;functional programming language;materiel informatique;material informatica;optimisation compilateur;circuit synthesis;tecnologia avanzada;teaching;ensenanza	This paper describes a course in hardware description and synthesis (hardware compilation), taught as an introductory graduate course at Chalmers University of Technology, and as an advanced undergraduate course at the University of Malta. The functional programming language Haskell was used both to describe circuits and circuit synthesis schemes.	compiler;embedded system;functional programming;haskell;logic synthesis;programming language	Koen Claessen;Gordon J. Pace	2002	SIGPLAN Notices	10.1145/636517.636526	erlang;computer science;optimizing compiler;software testing;programming language;functional programming;algorithm	PL	-23.924995613838096	30.22606106242322	155391
ae84af9d0ef2d41dc8db810457eadfc029a09e07	creating and using domain-specific language features	language composition;editor generation;graphical modeling languages	"""The value that domain-specific languages provide to their users is the domain-specific language features they contain. These features provide notations from the domain of interest, as well as domain-specific analysis and optimizations. But domain-specific languages are sometimes a poor means of delivering these valuable features to their users. A challenge arises when a problem crosses multiple domains and whose programming or modeling solution could benefit from language features from all domains of interest. Using multiple domain-specific languages can become cumbersome, perhaps outweighing their benefits in the first place.  An alternative approach, advocated by this position paper, is to provide domain-specific language features to programmers and modelers as composable language extensions that they can import into their general-purpose programming or modeling language. In our view, there are three requirements for a language extension framework to be widely usable. First, language extensions should be developed independently, by domain-experts, as libraries or domain-specific languages are now. Second, extensions should be automatically composable so that programmers and modelers can pick the language extensions they want, and direct tools to compose them, without the need for writing """"glue-code."""" Third, this composition process should not fail to yield a working compiler (or other tools) for the custom extended language. Thus, the programmer has some assurance that the extensions that they pick will work together.  We briefly describe how this vision of extensible language frameworks is supported by the Silver and Copper metaprogramming tools."""	compiler;domain-specific language;extensible programming;general-purpose modeling;general-purpose programming language;glue code;library (computing);metaprogramming;modeling language;programmer;requirement	Ted Kaminski;Eric Van Wyk	2013		10.1145/2489812.2489817	natural language processing;fourth-generation programming language;first-generation programming language;very high-level programming language;universal networking language;language primitive;object-based language;object language;specification language;data control language;computer science;domain-specific language;third-generation programming language;low-level programming language;modeling language;fifth-generation programming language;programming language;programming language specification;second-generation programming language;high-level programming language	PL	-28.273199187274038	28.049898967892133	155448
da620344439bc3754c267daa1f02c518f529bb3b	object-oriented language processing	modelizacion;lenguaje programacion;object oriented language;compilateur;programming language;programmation modulaire;tratamiento lenguaje;programacion modular;internal structure;compiler;modelisation;analyse syntaxique;language processing;analisis sintaxico;object oriented;syntactic analysis;estructura datos;traitement langage;analizador sintaxico;langage programmation;modular programming;oriente objet;structure donnee;parser;modeling;orientado objeto;analyseur syntaxique;data structure;compilador	Compiler architecture often follows an imperative layout. Different actions in the compiler are modeled as functions that operate over defined data structures. In this work, we present existing methodologies for writing objectoriented language-processing tools. As a contribution, we explore possibilities of writing a compiler based on recursive descent parsing in an object-oriented way. As a proof of the concept, we present a parser generator that employs the presented constructs both in its internal structure and in generated output.	abstract syntax tree;application programming interface;central processing unit;compiler;compiler-compiler;data structure;human-readable medium;imperative programming;language technology;parse tree;parsing;parsing expression grammar;recursion;recursive descent parser	Pietu Pohjalainen	2006		10.1007/11860990_8	natural language processing;compiler;data structure;compiler correctness;computer science;compiler construction;parsing;programming language;object-oriented programming;functional compiler;algorithm	PL	-25.127205274487	24.10499103024503	155451
2a96e26e529e96b4304e05ac4d28cf61657a6104	hobit: programming lenses without using lens combinators		We propose HOBiT, a higher-order bidirectional programming language, in which users can write bidirectional programs in the familiar style of conventional functional programming, while enjoying the full expressiveness of lenses. A bidirectional transformation, or a lens, is a pair of mappings between source and view data objects, one in each direction. When the view is modified, the source is updated accordingly with respect to some laws—a pattern that is found in databases, model-driven development, compiler construction, and so on. The most common way of programming lenses is with lens combinators, which are lens-to-lens functions that compose simpler lenses to form more complex ones. Lens combinators preserve the bidirectionality of lenses and are expressive; but they compel programmers to a specialised point-free style—i.e., no naming of intermediate computation results—limiting the scalability of bidirectional programming. To address this issue, we propose a new bidirectional programming language HOBiT, in which lenses are represented as standard functions, and combinators are mapped to language constructs with binders. This design transforms bidirectional programming, enabling programmers to write bidirectional programs in a flexible functional style and at the same time access the full expressiveness of lenses. We formally define the syntax, type system, and the semantics of the language, and then show that programs in HOBiT satisfy bidirectionality. Additionally, we demonstrate HOBiT ’s programmability with examples.	combinatory logic	Kazutaka Matsuda;Meng Wang	2018		10.1007/978-3-319-89884-1_2	theoretical computer science;programming language;combinatory logic;semantics;scalability;functional programming;syntax;language construct;compiler construction;computer science;lens (optics)	HCI	-24.83181019604892	26.66484196969768	155865
08e6ca11b8771e5f5913f4a25b2cd9af1625f3f7	rewriting in practice		We discuss applications of rewriting in three different areas: design and analysis of algorithms, theorem proving and term rewriting, and modeling and analysis of biological processes. 1998 ACM Subject Classification F.4.2 [Mathematical Logic and Formal Languages] Grammars and Other Rewriting Systems–Decision problems; I.6.5 [Simulation and Modeling] Model Development–Modeling Methodologies; I.1.2 [Symbolic and Algebraic Manipulation] Algorithms– Algebraic Algorithms	algebraic equation;algorithm;analysis of algorithms;automated theorem proving;rewriting;simulation;symbolic computation	Ashish Tiwari	2011		10.1007/978-3-642-21691-6_3	prefix grammar;computer science;theoretical computer science;semi-thue system;confluence;algorithm;graph rewriting	Logic	-20.035063591211824	19.21227167039093	155895
4161dc3c17a9232d52c77a006e90941f4b1f6f4f	how to write system-specific, static checkers in metal	multi threaded programming;race conditions;design intent;static analysis;java	This paper gives an overview of the metal language, which we have designed to make it easy to construct systemspecific, static analyses. We call these analyses extensions because they act as the input to a generic analysis engine that runs the static analysis over a given source base. We also interchangeably refer to them as checkers because they check that a user-specified property holds in the source base and report any violations of that property. Note that checkers may not detect all violations of a specified property. Their goal is to find as many violations as possible with a minimum of false positives. We describe seven checkers in total; most are less than 50 lines of code, but, in aggregate, find hundreds of errors in a typical large system. In addition to language details, we give a feel for how to build good checkers: exploiting highlevel knowledge, ranking errors, suppressing false positives. Further, we show how to write checkers that extract rules to check from the source code itself without the assistance of the programmer. The common thread among these analyses is that they all exploit the fact that many abstract program restrictions map clearly to source code actions [5]. While metal extensions are executed much like a traditional dataflow analysis, they can easily be augmented in ways outside the scope of traditional approaches, such as using statistical analysis to discover rules [6]. To check a rule, an extension does two things: (1) recognizes interesting source code actions relevant to a given rule and (2) checks that these actions satisfy some rule-specific	aggregate data;context-sensitive grammar;data-flow analysis;dataflow;finite-state machine;general-purpose modeling;interprocedural optimization;programmer;source lines of code;static program analysis;weitao yang	Benjamin Chelf;Dawson R. Engler;Seth Hallem	2002		10.1145/586094.586097	parallel computing;real-time computing;computer science;race condition;programming language;java;static analysis	PL	-21.718994193646406	31.0397077360423	155897
ba6e468339bbbad267bc269b04de1d9ac9d68c9a	an extension of agentspeak(l) and jason tailored to programming and software development	jona;jason;agent program ming languages;agent oriented programming;agentspeak l	Agent programming languages like AgentSpeak(L) -- and Jason, as its modern extension/implementation -- have features that make them interesting for software development and general-purpose programming, besides AI problems. A main one is the level of abstraction, that eases the design and development of concurrent, reactive, distributed applications. At the same time, being not developed for general-purpose programming and software development in mind, they typically miss elements that are important for that purpose. These weaknesses can negatively impact on their adoption beyond the agent-oriented programming community. Accordingly, in this paper we discuss some features extending the basic model provided by AgentSpeak(L)/Jason, with the purpose of improving its adoption for programming and software development. Such extensions are shown in practice using a Jason extension called Jona.	agent-oriented programming;agentspeak;distributed computing;encapsulation (networking);functional programming;general-purpose modeling;general-purpose programming language;gradual typing;jason;java;multi-agent system;operational semantics;programmer;seamless3d;smart system;software development;type system	Angelo Croatti;Alessandro Ricci	2016		10.1145/3001886.3001887	reactive programming;computer science;artificial intelligence;software development;functional logic programming;programming paradigm;algorithm	SE	-27.674808271478128	29.008673790663988	155974
5dbadf14ec9a793313395375f560ed0e09ce3610	low-level language subroutines for use within fortran	fortran	"""The use of symbolically coded subroutines within FowraAN is described by IBM in the relevant coding manuals [1]. A number of SAP and FAP coded subroutines have been written it ! different installations to increase the usefulness of 704, 709, and 7090 FOUTttAN; and have been reported individually. Most accounts have appeared in the SHA~E literature, or have been informal. One or two accounts have appeared in tile open literature [2]. Considerable use has been made of such subroutines by the author and his associates during the past few years, in connection with conwmtional scientific computing, symbol manipulation, expanded character set input-output, visual display, and so forth. This note contains some systematic comments concerning these subroutines. Section II deals with some subroutines that perform """"special arithmetic"""" (for example, multi-precision and full-word integer arithmetic). Section III deals with some subroutines that are used in symbol manipulation. Bit manipulation and expanded character set input-output at~ considered briefly in Seetion IV. Visual displayqs discussed in Section V. It, is suggested that subroutines such as those described here make coding much easier, expand the variety of processes that can be coded without any knowledge of symbolic programming, and help in maintaining compatibility of programs between different computers and in easing the transition from one computer to another."""	arbitrary-precision arithmetic;bit manipulation;character encoding;computational science;computer;face animation parameter;fortran;ibm 7090;subroutine;symbolic programming	Michael P. Barnett	1961	Commun. ACM	10.1145/366813.366826	computer science	Graphics	-29.5546317182432	22.40321407670645	155998
1738d9472d4fc8f804786f81f5ec8bb4d05f50cd	a transformation system combining partial evaluation with term rewriting	higher order;first order;partial evaluation;term rewriting	This paper presents a new approach to optimizing functional programs based on com bining partial evaluation and rewriting Programs are composed of higher order primitives Partial evaluation is used to eliminate higher order functions First order rewriting is used to process the transformation Laws about the higher order primitives that are relevant for the optimizations are automatically extracted from a library and transformed into rst order terms using partial evaluation Such a combination of a partial evaluation system and an intrinsically rst order rewriting tool allows a form of higher order rewriting at a rst order level This way it is possible to automate deforestation of higher order programs	higher-order function;partial evaluation;rewriting	Françoise Bellegarde	1993		10.1007/3-540-58233-9_3	discrete mathematics;theoretical computer science;mathematics;confluence;algorithm	Logic	-20.034138184795264	24.511110394221113	156092
52f70b441c89817992f017c9ef1dad4cafe4d9f4	intelligent program analysis	program analysis	"""In order to exanaine the possibilities o f using a computer as an aid to teaching programming, a prototype intelligent program analyzer has been constructed, lts design assumes that a system cannot analyze a program unless it can """"understand"""" it; understanding being based on a knowledge o f what must be accomplished and haw code is used to express the intentions. It was found that a one-page description o f two common sorting algorithms or o f some common approximation problent~"""" was su.Oicient for the computer to understand and analyze a wide variety ofprograms and idenlify and describe almost all errors."""	approximation;program analysis;prototype;sorting algorithm	Gregory R. Ruth	1976	Artif. Intell.	10.1016/0004-3702(76)90022-9	program analysis;simulation;computer science;theoretical computer science	AI	-27.338747776244162	23.9742696908861	156166
70a844674b7c7e9307f2eb1c3f1bd2a10b60f282	automatically refining partial specifications for heap-manipulating programs	constraint abstraction;partial specification refinement;separation logic;semi automatic software verification;numerical analysis;article;static program analysis	Automatically verifying heap-manipulating programs is a challenging task, especially when dealing with complex data structures with strong invariants, such as sorted lists and AVL/red-black trees. The verification process can greatly benefit from human assistance through specification annotations, but this process requires intellectual effort from users and is error-prone. In this paper, we propose a new approach to program verification that allows users to provide only partial specification to methods. Our approach will then refine the given annotation into a more complete specification by discovering missing constraints. The discovered constraints may involve both numerical and multi-set properties that could be later confirmed or revised by users. We further augment our approach by requiring partial specification to be given only for primary methods. Specifications for loops and auxiliary methods can then be systematically discovered by our augmented mechanism, with the help of information propagated from the primary methods. Our work is aimed at verifying beyond shape properties, with the eventual goal of analysing full functional properties for pointer-based data structures. Initial experiments have confirmed that we can automatically refine partial specifications with non-trivial constraints, thus making it easier for users to handle specifications with richer properties.	avl tree;cognitive dimensions of notations;color;correctness (computer science);data structure;emoticon;experiment;formal verification;like button;numerical analysis;pointer (computer programming);prototype;red–black tree;verification and validation	Shengchao Qin;Guanhua He;Chenguang Luo;Wei-Ngan Chin;Hongli Yang	2014	Sci. Comput. Program.	10.1016/j.scico.2013.03.004	separation logic;numerical analysis;computer science;database;programming language;algorithm;static program analysis	SE	-19.16120145744242	26.95587906442597	156379
844ea8977f42eb7bfc97e6d42219e85dcd330ef0	dataflow lambda-calculus	lambda calculus		dataflow;lambda calculus	Gabriel Ciobanu	1995	Sci. Ann. Cuza Univ.		binary lambda calculus;dataflow;system f;programming language;fixed-point combinator;process calculus;computer science;lambda calculus;typed lambda calculus	Logic	-22.297001363891827	21.6908201684021	156408
a3b472754ab44b208dcf5a72439ca6f0c8cf5709	from state machines to temporal logic: specification methods for protocol standards	distributed system;temporal logic;state machine;spectrum;specification language;communication protocol;state transition diagram	This paper attempts to lend perspective to several different methods that have been employed for specifying computer communication protocols by comparing a spectrum of specification techniques. The paper characterizes specification languages such as state transition diagrams, variants of temporal logic approaches, and sequence expressions by the extent to Which information is encoded as properties of a single state versus properties of a history of the entire computation state sequence. Taking the prototypical alternating bit protocol as an example, each method is used to specify the requirements for the send process of the distributed system.	temporal logic	Richard L. Schwartz;P. M. Melliar-Smith	1982		10.1007/3-540-16047-7_35	real-time computing;interval temporal logic;computer science;theoretical computer science;finite-state machine;programming language;language of temporal ordering specification;abstract state machines	Theory	-31.110813097884172	32.19765184086298	156553
6aabb19bc4e2ee642075bfc83f7126a741bbd819	trace-based verification of imperative programs with i/o	application development;separation logic;ynot;program verification;hoare logic;modular verification;imperative programming;dependent types;coq proof assistant;data structure;traces	In this paper we demonstrate how to prove the correctness of systems implemented using lowlevel imperative features like pointers, files, and socket I/O with respect to high level I/O protocol descriptions by using the Coq proof assistant. We present a web-based course gradebook application developed with Ynot, a Coq library for verified imperative programming. We add a dialog-based I/O system to Ynot, and we extend Ynot’s underlying Hoare logic with event traces to reason about I/O and protocol behavior. Expressive abstractions allow the modular verification of both high level specifications like privacy guarantees and low level properties like data structure pointer invariants.	coq (software);correctness (computer science);data structure;high-level programming language;hoare logic;imperative programming;input/output;pointer (computer programming);proof assistant;tracing (software);web application;dialog	Gregory Malecha;J. Gregory Morrisett;Ryan Wisnesky	2011	J. Symb. Comput.	10.1016/j.jsc.2010.08.004	imperative programming;dependent type;separation logic;data structure;theoretical computer science;hoare logic;rapid application development;algorithm	PL	-21.0401687373454	28.20706455793575	156660
c303c563026b11de91d2b91fc5ff293b8c0d338a	an overview of fortran 2003	lenguaje programacion;compilacion;compilateur;programming language;interoperabilite;interoperabilidad;compiler;marcador;pointer;polymorphism;traitement exception;langage programmation;compilation;pointeur;exception handling;polymorphisme;polimorfismo;fortran;interoperability;compilador	There is plenty happening just now with respect to Fortran. Two sets of features (for exception handling and for enhancements to allocatable arrays) were defined in Technical Reports as extensions to Fortran 95 and have become widely available in compilers.  The features of Fortran 2003 have been chosen and the standard is essentially complete. As well as the contents of the two Technical Reports, this adds interoperability with C, parameterized derived types, procedure pointers, type extension and polymorphism, access to the computing environment, support of international character sets, and many other enhancements.  A new Technical Report is essentially complete. This enhances the module features and avoids the 'compilation cascade' that can mar the development of very large programs. It is written as an extension of Fortran 2003, but is expected to be widely implemented as an extension to Fortran 95 compilers.  We will summarize all these developments, which will make Fortran even more suitable for large numerically-demanding applications.	character encoding;code;command-line interface;compiler;exception handling;excited state;fortran;function pointer;interoperability;multiple inheritance;numerical analysis	John K. Reid	2004	SIGPLAN Notices	10.1145/1026474.1026480	exception handling;interoperability;polymorphism;compiler;parallel computing;pointer;computer science;programming language;algorithm	HPC	-24.7252445060613	27.770976974133923	156824
2c38baabc29010ca853e462e6016fc4f1a8dc557	the list introduction strategy for the derivation of logic programs	lenguaje programacion;evaluation performance;derivacion;performance evaluation;programming language;programacion automatica;evaluacion prestacion;program derivation;derivation programme;program transformation;estrategia;logical programming;automatic programming;transformation programme;strategy;bypass;transformacion programa;programming theory;programmation logique;derivation;estructura datos;langage programmation;theorie programmation;structure donnee;logic programs;programacion logica;strategie;data structure;programmation automatique	We present a new program transformation strategy based on the introduction of lists. This strategy is an extension of the tupling strategy which is based on the introduction of tuples of fixed length. The list introduction strategy overcomes some of the limitations of the tupling strategy and, in particular, it makes it possible to transform general recursive programs into linear recursive ones also in cases when this transformation cannot be performed by the tupling strategy. The linear recursive programs we derive by applying the list introduction strategy have in most cases very good time and space performance because they avoid repeated evaluations of goals and unnecessary constructions of data structures.	data structure;logic programming;program transformation;recursion;μ-recursive function	Alberto Pettorossi;Maurizio Proietti	2002	Formal Aspects of Computing	10.1007/s001650200011	data structure;strategy;computer science;artificial intelligence;programming language;derivation;program derivation;algorithm	AI	-20.019437864286274	23.22869921449724	156852
83807c46235364f543a36dc0523f9aa67e1e6a37	application of methods for syntax analysis of context-free languages to query evaluation of logic programs		My research goal is to employ a parser generation algorithm based on the Earley parsing algorithm to the evaluation and compilation of queries to logic programs, especially to deductive databases. By means of partial deduction, from a query to a logic program a parameterized automaton is to be generated that models the evaluation of this query. This automaton can be compiled to executable code; thus we expect a speedup in runtime of query evaluation. An extended abstract/ full version of a paper accepted to be presented at the Doctoral Consortium of the 30th International Conference on Logic Programming (ICLP 2014), July 19-22, Vienna, Austria	algorithm;automaton;compiler;consortium;deductive database;earley parser;executable;international conference on logic programming;natural deduction;parsing;speedup	Heike Stephan	2014	CoRR		natural language processing;abstract syntax;query optimization;computer science;syntax;ontology language;programming language;homoiconicity;algorithm;query language;abstract syntax tree;syntax error	Logic	-23.027656453706474	18.65982583276429	157217
dcac25d9432a68cbf47ef1a9bf6a75a2e22e36a9	a new solution to the hidden copy problem	langage fonctionnel;lenguaje programacion;big step operational sementics;optimisation;compilateur;alias analysis;optimizacion;programming language;time complexity;language theory;lenguaje funcional;operational semantics;semantics;ingenieria logiciel;teoria lenguaje;compiler;semantica;semantique;software engineering;analisis programa;must alias analysis;algorithme;algorithm;interpretacion;complexite temps;genie logiciel;langage programmation;interpretation;optimization;program analysis;analyse programme;complejidad tiempo;abstract interpretation;functional language;copy optimization;lenguaje formal;theorie langage;formal language;compilador;algoritmo;langage formel	We consider the well-known problem of avoiding unnecessary costly copying that arises in languages with copy/value semantics and large aggregate structures such as arrays, sets, or les. The origins of many recent studies focusing on avoiding copies of at arrays in functional languages may be traced back to SETL copy optimization Schwartz 75]. The problem is hard, and progress is slow, but a successful solution is crucial to achieving a pointer-free style of programming envisioned by Hoare 75]. We give a new solution to copy optimization that uses dynamic reference counts and lazy copying to implement updates eeciently in an imperative language with arbitrarily nested nite sets and maps (which can easily model arrays, records and other aggregate datatypes). Big step operational semantics and abstract interpretations are used to prove the soundness of the analysis and the correctness of the transformation. An eecient algorithm to implement the analysis is presented. The approach is supported by realistic empirical evidence. Our solution anticipates the introduction of arbitrarily nested polymor-phic sets and maps into JAVA. It may also provide a new eecient strategy for implementing object cloning in Java and object assigment in C++. We illustrate how our methods might improve the recent approach of Wand and Clinger 98] to avoid copies of at arrays in a language of rst-order recursion equations.	aggregate data;algorithm;array data structure;c++;clinger–cohen act;correctness (computer science);functional programming;hoare logic;imperative programming;java;lazy evaluation;map;mathematical optimization;operational semantics;pointer (computer programming);recursion;setl;value semantics;whole earth 'lectronic link	Deepak Goyal;Robert Paige	1998		10.1007/3-540-49727-7_20	program analysis;time complexity;compiler;formal language;alias analysis;interpretation;computer science;artificial intelligence;philosophy of language;theoretical computer science;operating system;database;mathematics;semantics;programming language;functional programming;operational semantics;algorithm	PL	-20.18386560098505	23.63946134900789	157379
3bd6f81ff90acb22cd720bb219f495c4117f6efa	functional representation and program debugging	debugging;computer program;programming language semantics;computer languages;debugging artificial intelligence computer languages strips laboratories military computing information science reasoning about programs electric breakdown vocabulary;information science;vocabulary;electric breakdown;reasoning about programs;function representation;artificial intelligence;program debugging;strips;military computing	In this paper, we propose a method for representing and debugging computer programs that combines the best features of two streams of research in AI. The representation adopted here combines features normally found in only in plan representations with features normally found only an device representations. Not only does this combined approach solve problems that have not been solved b y strictly plan-based or proofbased debuggers, but the formal nature of the programming domain has helped us to clarify the semantics of functional representations. We then introduce the notion of a context-rich semantics that we call the functional semantics of a program, and show how reasoning with the functional semantics has certain advantages over reasoning with traditional programming language semantics.	computer program;debugger;debugging;function representation;mental representation;programming domain;semantics (computer science)	Dean Allemang;B. Chandrasekaran	1991		10.1109/KBSE.1991.638030	natural language processing;strips;information science;computer science;artificial intelligence;theoretical computer science;software engineering;function representation;algorithmic program debugging;programming language;well-founded semantics;debugging;operational semantics;denotational semantics;semantics;computational semantics	AI	-26.691109875898793	20.803728857133226	157472
12b7c9042c91f415af655f97e75b316375fb6f11	reducing the number of annotations in a verification-oriented imperative language	programming language;software verification;polymorphism;language design	Automated software verification is a very active field of research which has made enormous progress both in theoretical and practical aspects. Recently, an important amount of research effort has been put into applying these techniques on top of mainstream programming languages. These languages typically provide powerful features such as reflection, aliasing and polymorphism which are handy for practitioners but, in contrast, make verification a real challenge. In this work we present Pest, a simple experimental, while-style, multiprocedural, imperative programming language which was conceived with verifiability as one of its main goals. This language forces developers to concurrently think about both the statements needed to implement an algorithm and the assertions required to prove its correctness. In order to aid programmers, we propose several techniques to reduce the number and complexity of annotations required to successfully verify their programs. In particular, we show that high-level iteration constructs may alleviate the need for providing complex loop annotations.	algorithm;aliasing;correctness (computer science);formal verification;handy board;high- and low-level;imperative programming;iteration;programmer;programming language;reflection (computer programming);software verification	Guido de Caso;Diego Garbervetsky;Daniel Gorín	2009	CoRR		polymorphism;language primitive;software verification;computer science;theoretical computer science;fifth-generation programming language;programming language;algorithm	PL	-19.48603450722032	26.755548197893802	157650
8e40df1df03ca224e639d8c495482cf8a715f1b8	automated deployment of argumentation protocols	verification;automated synthesis;argumentation;model checking;interaction models;dialogue games	The objective of this paper is to try to fill the gap between: argumentation, electronic institutions and protocols by using a combination of automated synthesis and model checking methods. More precisely, this paper proposes a means of moving rapidly from argument specification to protocol implementation, using an extension of the Argument Interchange Format as the specification language and the Lightweight Coordination Calculus as an implementation language.	argument map;bridging (networking);model checking;multi-agent system;object language;sequent calculus;software deployment;specification language	Ashwag Maghraby;Dave Robertson;María Adela Grando;Michael Rovatsos	2012		10.3233/978-1-61499-111-3-197	model checking;verification;computer science;artificial intelligence;theoretical computer science;automated proof checking;algorithm	Logic	-33.354917598580656	30.61035507764669	157922
7071ca0ef904a3a36f2d7940f019ccf5bbd004f8	a new mixing programming method		MATLAB, developed by Mathworks co., is a high-performance language for technical computing. It integrates computation, visualization, and programming in an easy-to-use environment where problems and solutions are expressed in familiar mathematical notation. Because of powerful matrix computation, it is called Matrix Lab. Matlab Language is similar with natural language. The usage of Matlab is extremely convenient. It also have plenty functions that can be called easy. But, it also has some shortcomings. Because the MATLAB language is a kind of explanation execution script language, it is very slow regarding the loops sentence execution. Under the same condition, compared to some high level languages such as Vc and VB, It executes loops sentences at the slow speed. The graphic user interface (GUI) of The MATLAB is not very friendly, and the parameter input and the output is not convenient. VB, as a kind of high level computer language, executes loops sentences more quickly, and its GUI is user-friendly. By incorporating visual programming interface of VB with the powerful function of MATLAB in numerical calculation and graphic display, we can take their advantages and avoid their weaknesses.	application programming interface;computation;computer language;graphical user interface;high-level programming language;matlab;natural language;numerical analysis;numerical linear algebra;scripting language;usability;visual programming language	Yuan Cui	2008	Computer and Information Science		computational science;programming domain;reactive programming;functional reactive programming;computer science;theoretical computer science;procedural programming;symbolic programming;inductive programming;programming language	PL	-28.89416542576287	21.835258449715692	157931
3653b22809221fe263cf436a4c2fe4cd7e8532da	design and implementation of the intelligent plant factory system based on ubiquitous computing		Abnormal climate events have caused agricultural production decrease and its supply insecurity. These problems, coupled with population growth, have led to food shortage. As a solution to such a situation, plant factories capable of producing farm products regardless of climate or location have garnered broader attention. In this research, we propose a ubiquitous computing-based intelligent plant factory system designed by improving the previous plant factory system with just a simple function of On/Off. The proposed system collects information on necessary environmental factors for plants growth, nutrient solution, etc. through sensors installed in a plant factory and infers contextual information through the ontology based on the collected data to provide corresponding appropriate services to users. In addition, this system supports various application platforms such as web PDA, smart phones, etc. so that users can access its service on the plant factory anywhere and anytime.	ubiquitous computing	Jeong Hwan Hwang;Hoseok Jeong;Hyun Yoe	2014		10.1007/978-3-319-07596-9_10	embedded system;systems engineering;ubiquitous robot;computer engineering	Robotics	-30.551658487892603	18.954883329252784	158063
2b37e08af7795f12d3a83f5352b589da95edda80	comments on f. de cindio, g. de michelis, l. pomello and c. simone		The authors advocate the use of Superposed Automata Nets (SA nets), a subclass of Predicate Transition Nets, during the requirements definition and design phases. SA nets are taken as the basis for contracts between the various persons and organizations involved in the system design process.		Christiane Floyd;Reinhard Keil	1982		10.1007/978-3-642-69208-6_23	subclass;discrete mathematics;requirement;predicate (grammar);systems design;requirements analysis;mathematics	Logic	-24.391045812427087	19.001698680368616	158166
389f1df415703a46af73a2b18f059efd9f68ef61	students' mental models of recursion at wits	formal specification;first year;programming language;mental models;recursion;recurrence relation;recursive algorithm;mental model;group 3	"""Recursion is a concept which all computer scientists should understand and be able to use but novices find it difficult to master. In the School of Computer Science at the University of the Witwatersrand (Wits) we have for a long time been concerned about how we can assist our students with recursion [4, 1, 3]. One thrust of our research is the study of the mental models of recursion (c.f. Kahney [2]) which our first year students develop.  Most of our students encounter recursion for the first time in our Fundamental Algorithmic Concepts (FAC) course. When we originally investigated the mental models of our students we noted that although many of them seem to develop the viable copies model there are still many that develop models which are non-viable (i.e. that cannot be relied on to lead to a correct result) [1]. Thus we adapted the way in which recursion was introduced in FAC in 2003, 2004 and 2005 by introducing more complex recursive algorithms earlier to help in the development of the copies mental model. We then compared the mental models developed by the 2003, 2004 and 2005 students to those developed by the earlier group [3]. The results indicate that more of the students were developing viable mental models of recursion and thus that the changes to our teaching were benefitting our students.  In 2006 we changed the programing language in which our students implement algorithms to Python (from Scheme). In essence the programming language was the only change made as the course was still taught in a """"functional"""" style to emphasize the link between the formal specification of a problem, the solution to the problem and the program. We did, however, feel it was important to assess the impact of the change on our students' mental models of recursion. We thus did a similar study on the 2006 students to that on earlier cohorts.  The students' traces from two recursive algorithms were categorised into the mental models previously observed [1,3] by identifying how the student deals with the active flow, base case and passive flow in their trace and then by combining this information into an overall categorisation of the trace for that algorithm. Overall the results are in line with our previous results which showed that the copies model is the dominant model for a recurrence relation type of recursive function but that for list manipulation problems some students showed an active or looping model. These results indicate that our teaching approach, even with the switch to Python, is assisting our students in developing a viable copies mental model of recursion. Such a mental model is more likely to lead to correct traces of recursive algorithms.  An interesting new result was the emergence of a passive mental model. Here the students recognised that the recursive algorithm would somehow get to the base case and then used the base case plus the implicit definition of the function in the algorithm to build up the required solution. This model may have arisen because the students were given a recurrence in Tutorial 1 and asked to calculate what value would be returned. Solving the recurrence essentially meant working up from the value where the result is defined directly until the desired answer is found. Some students may have adopted this as their model of recursion."""	categorization;computer science;computer scientist;emergence;fly-by-wire;formal specification;mental model;programming language;python;recurrence relation;recursion (computer science);scheme;super-recursive algorithm;thrust;tracing (software)	Ian Douglas Sanders;Vashti Galpin	2007		10.1145/1268784.1268883	recursion;simulation;recurrence relation;computer science;artificial intelligence;formal specification;programming language;algorithm;recursion	AI	-30.223441166147747	22.66897218289895	158189
c84993be97697123146cd502903a914e8ab68c90	extracting symbolic transitions from tla ^+ + specifications		In TLA, a system specification is written as a logical formula that restricts the system behavior. As a logic, TLA does not have assignments and other imperative statements that are used by model checkers to compute the successor states of a system state. Model checkers compute successors either explicitly — by evaluating program statements — or symbolically — by translating program statements to an SMT formula and checking its satisfiability. To efficiently enumerate the successors, TLA’s model checker TLC introduces side effects. For instance, an equality x ′ = e is interpreted as an assignment of e to the yet unbound variable x . Inspired by TLC, we introduce an automatic technique for discovering expressions in TLA formulas such as x ′ = e and x ′ ∈ {e1, . . . , ek} that can be provably used as assignments. In contrast to TLC, our technique does not explicitly evaluate expressions, but it reduces the problem of finding assignments to the satisfiability of an SMT formula. Hence, we give a way to slice a TLA formula in symbolic transitions, which can be used as an input to a symbolic model checker. Our prototype implementation successfully extracts symbolic transitions from a few TLA benchmarks.	enumerated type;free variables and bound variables;imperative programming;model checking;multi-level cell;prototype;well-formed formula	Jure Kukovec;Thanh-Hai Tran;Igor V. Konnov	2018		10.1007/978-3-319-91271-4_7		Logic	-20.373854123792274	26.236500532786653	158261
9681996114e546243769b7b537e84c1bdf50ec99	implementing polymorphic typing in a logic programming language	lenguaje programacion;type;tipo;programming language;implementation;run time type checking;ejecucion programa;logical programming;program execution;recherche;ejecucion;logic programming;typing;programmation logique;informatique theorique;execution programme;polymorphism;langage programmation;polymorphisme;polimorfismo;logic programs;programacion logica;investigacion;computer theory;informatica teorica	Introducing types into a logic programming language leads to the need for typed uniication within the computation model. In the presence of polymorphism and higher-order features, this aspect forces analysis of types at run-time. We propose extensions to the Warren Abstract Machine (WAM) that permit such analysis to be done with reasonable eeciency. Much information about the structures of types is present at compile-time, and we show that this information can be used to considerably reduce the work during execution. We illustrate our ideas in the context of a typed version of Prolog. We describe a modiied representation for terms, new instructions and additional data areas that in conjunction with existing WAM structures suuce to implement this language. The nature of compiled code is illustrated through examples, and the kind of run-time overheads that are incurred for processing types is analyzed, especially in those cases where others have shown that type checking can be eliminated during execution. The ideas presented here are being used in an implementation of the higher-order language called Prolog.	compile time;compiler;emulator;ibm notes;logic programming;model of computation;open road tolling;point of sale;programming language;prolog;run time (program lifecycle phase);scope (computer science);type system;typing;warren abstract machine	Keehang Kwon;Gopalan Nadathur;Debra Sue Wilson	1994	Comput. Lang.	10.1016/0096-0551(94)90012-4	polymorphism;computer science;programming language;implementation;logic programming;type;algorithm	PL	-20.695692055581606	23.468528511878127	158385
59f2bed94f2a40e9224f333abed8eea00e32917f	abstract data types, subtypes and data independence	abstract data type		abstract data type	F. Warren Burton;Brian Lings	1981	Comput. J.	10.1093/comjnl/24.4.308	computer science;programming language;abstract data type	DB	-23.143875243176716	21.03973179184253	158401
f5de4025a5f5bb4f79c9d793d676d0c3e8853fe4	conc2seq: a frama-c plugin for verification of parallel compositions of c programs	concurrent computing;radiation detectors;indexes;programming;context;instruction sets	Frama-C is an extensible modular framework for analysis of C programs that offers different analyzers in the form of collaborating plugins. Currently, Frama-C does not support the proof of functional properties of concurrent code. We present Conc2Seq, a new code transformation based tool realized as a Frama-C plugin and dedicated to the verification of concurrent C programs. Assuming the program under verification respects an interleaving semantics, Conc2Seq transforms the original concurrent C program into a sequential one in which concurrency is simulated by interleavings. User specifications are automatically reintegrated into the new code without manual intervention. The goal of the proposed code transformation technique is to allow the user to reason about a concurrent program through the interleaving semantics using existing Frama-C analyzers.	ansi/iso c specification language;concurrency (computer science);concurrent computing;control flow;coq (software);formal proof;forward error correction;frama-c;local variable;plug-in (computing);postcondition;proof assistant;runtime verification;satisfiability modulo theories;simulation;touchstone file;undefined behavior	Allan Blanchard;Nikolai Kosmatov;Matthieu Lemerre;Frédéric Loulergue	2016	2016 IEEE 16th International Working Conference on Source Code Analysis and Manipulation (SCAM)	10.1109/SCAM.2016.18	database index;programming;concurrent computing;computer science;theoretical computer science;software engineering;instruction set;database;programming language;particle detector	Logic	-21.29291965095008	27.446500060621638	158547
9483d4131496cd5c7886e919e042950dc658abe6	communicating processes with value-passing and assignments	parallelisme;lenguaje programacion;programa paralelo;occam;programming language;communicating process;proceso comunicante;parallelism;paralelismo;processus communicant;mathematical model;langage programmation;csp;parallel program;programme parallele	A semantic theory of an imperative language which allows value-passing and assignments as a simple action prefixing is described. Three different semantic approaches are given: denotational based on the mathematical model Acceptance Trees, axiomatic based on inequations and behavioural in terms of testing. The equivalence of these different approaches is shown. The results are compared with similar results for other languages such asCSP andOccam.	imperative programming;mathematical model;turing completeness	Matthew Hennessy;Anna Ingólfsdóttir	1993	Formal Aspects of Computing	10.1007/BF01212486	computer science;theoretical computer science;communicating sequential processes;mathematical model;mathematics;programming language;algorithm;occam	PL	-21.12459877755154	22.022945665414447	158726
d73f810c4084c7182ee37c73abd2523e418fdda1	"""""""lambda"""" prolog: interpretador e unificação de ordem superior"""			prolog;λprolog	Artemio Ludwig	1992				Arch	-22.565614350129845	19.987657033865386	158761
